---
title: Datacore Data
format:
  html:
    toc: true
    number-sections: true
jupyter: python3
execute:
  echo: true
  warning: false
  message: false
---

This chapter demonstrates how to connect to [Datacore](https://datacore.vn/), Vietnam's premier provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock prices and firm characteristics, including historical trading data and company fundamentals. While Datacore requires a subscription, most students and researchers typically have access through their university libraries or research institutions. For those without access, Datacore provides [demo datasets](https://datacore.vn/demo/dataset-groups) that allow you to run the code examples in this book with sample data.

The chapter is organized as follows. We first establish the connection to Datacore's cloud storage infrastructure. Then, we download and prepare company fundamentals data, including balance sheet items, income statement variables, and derived metrics essential for asset pricing research. Next, we retrieve and process stock price data, computing returns, market capitalizations, and excess returns. We conclude by merging these datasets and providing descriptive statistics that characterize the Vietnamese equity market.

## Setting Up the Environment

We begin by loading the Python packages used throughout this chapter. The core packages include `pandas` for data manipulation, `numpy` for numerical operations, and `sqlite3` for local database management. We also import visualization libraries for creating publication-quality figures.

```{python}
import pandas as pd
import numpy as np
import sqlite3
from datetime import datetime
from io import BytesIO

from plotnine import *
from mizani.formatters import comma_format, percent_format
```

We establish a connection to our local SQLite database, which serves as the central repository for all processed data. This database was introduced in the previous chapter and will store the cleaned datasets for use in subsequent analyses.

```{python}
tidy_finance = sqlite3.connect(database="data/tidy_finance_python.sqlite")
```

We define the date range for our data collection. The Vietnamese stock market began operations in July 2000 with the establishment of the Ho Chi Minh City Stock Exchange (HOSE), so our sample period starts from 2000 and extends through the end of 2024.

```{python}
start_date = "2000-01-01"
end_date = "2024-12-31"
```

## Connecting to Datacore

Datacore delivers data through a cloud-based object storage system built on MinIO, an S3-compatible storage infrastructure. This architecture enables efficient, programmatic access to large datasets without the limitations of traditional database connections. To access the data, you need credentials provided by Datacore upon subscription: an endpoint URL, access key, and secret key.

The following class establishes the connection to Datacore's storage system. The credentials are stored as environment variables for security, following best practices for credential management in research computing environments.

```{python}
#| eval: false
import os
import boto3
from botocore.client import Config

class DatacoreConnection:
    """
    Connection handler for Datacore's MinIO-based storage system.
    
    This class manages authentication and provides methods for
    accessing financial datasets stored in Datacore's cloud infrastructure.
    
    Attributes
    ----------
    s3 : boto3.client
        S3-compatible client for interacting with Datacore storage
    
    Examples
    --------
    >>> conn = DatacoreConnection()
    >>> conn.test_connection()
    Connected. Buckets:
     - financial-data
     - fundamentals
    """
    
    def __init__(self):
        """Initialize connection using environment variables."""
        self.MINIO_ENDPOINT = os.environ["MINIO_ENDPOINT"]
        self.MINIO_ACCESS_KEY = os.environ["MINIO_ACCESS_KEY"]
        self.MINIO_SECRET_KEY = os.environ["MINIO_SECRET_KEY"]
        self.REGION = os.getenv("MINIO_REGION", "us-east-1")
        
        self.s3 = boto3.client(
            "s3",
            endpoint_url=self.MINIO_ENDPOINT,
            aws_access_key_id=self.MINIO_ACCESS_KEY,
            aws_secret_access_key=self.MINIO_SECRET_KEY,
            region_name=self.REGION,
            config=Config(signature_version="s3v4"),
        )
    
    def test_connection(self):
        """Verify connection by listing available buckets."""
        response = self.s3.list_buckets()
        print("Connected successfully. Available buckets:")
        for bucket in response.get("Buckets", []):
            print(f"  - {bucket['Name']}")
    
    def list_objects(self, bucket_name, prefix=""):
        """List objects in a bucket with optional prefix filter."""
        response = self.s3.list_objects_v2(
            Bucket=bucket_name, 
            Prefix=prefix
        )
        return [obj["Key"] for obj in response.get("Contents", [])]
    
    def read_excel(self, bucket_name, key):
        """Read an Excel file from Datacore storage."""
        obj = self.s3.get_object(Bucket=bucket_name, Key=key)
        return pd.read_excel(BytesIO(obj["Body"].read()))
    
    def read_csv(self, bucket_name, key, **kwargs):
        """Read a CSV file from Datacore storage."""
        obj = self.s3.get_object(Bucket=bucket_name, Key=key)
        return pd.read_csv(BytesIO(obj["Body"].read()), **kwargs)
```

With the connection class defined, we can establish a connection and verify access to Datacore's data repositories.

```{python}
#| eval: false
# Initialize connection
conn = DatacoreConnection()
conn.test_connection()

# Get bucket name from environment
bucket_name = os.environ["MINIO_BUCKET"]
```

## Company Fundamentals Data

Firm accounting data are essential for portfolio analyses, factor construction, and valuation studies. Datacore hosts comprehensive fundamentals data for Vietnamese listed companies, including annual and quarterly financial statements prepared according to Vietnamese Accounting Standards (VAS).

### Understanding Vietnamese Financial Statements

Before processing the data, it is important to understand the structure of Vietnamese financial reports. Vietnamese companies follow VAS, which shares similarities with International Financial Reporting Standards (IFRS) but has notable differences:

1. **Fiscal Year**: Most Vietnamese companies use a calendar fiscal year ending December 31, though some companies (particularly in retail and agriculture) use different fiscal year-ends.

2. **Reporting Frequency**: Listed companies must publish quarterly financial statements within 20 days of quarter-end and annual audited statements within 90 days of fiscal year-end.

3. **Industry-Specific Formats**: Companies in banking, insurance, and securities sectors follow specialized reporting formats that differ from the standard industrial format.

4. **Currency**: All figures are reported in Vietnamese Dong (VND). Given the large nominal values (millions to trillions of VND), we often scale figures to millions or billions for readability.

### Downloading Fundamentals Data

Datacore organizes fundamentals data in Excel files partitioned by time period for efficient access. We download and concatenate these files to create a comprehensive dataset spanning our sample period.

```{python}
#| eval: false
# Define paths to fundamentals data files
fundamentals_paths = [
    "fundamental_annual/fundamental_annual_1.xlsx",
    "fundamental_annual/fundamental_annual_2.xlsx",
    "fundamental_annual/fundamental_annual_3.xlsx",
]

# Download and combine all files
fundamentals_list = []
for path in fundamentals_paths:
    df_temp = conn.read_excel(bucket_name, path)
    fundamentals_list.append(df_temp)
    print(f"Downloaded: {path} ({len(df_temp):,} rows)")

df_fundamentals_raw = pd.concat(fundamentals_list, ignore_index=True)
print(f"\nTotal observations: {len(df_fundamentals_raw):,}")
```

### Cleaning and Standardizing Fundamentals

The raw fundamentals data requires several cleaning steps to ensure consistency and usability. We standardize variable names, handle missing values, and create derived variables commonly used in asset pricing research.

```{python}
#| eval: false
def clean_fundamentals(df):
    """
    Clean and standardize company fundamentals data.
    
    Parameters
    ----------
    df : pd.DataFrame
        Raw fundamentals data from Datacore
    
    Returns
    -------
    pd.DataFrame
        Cleaned fundamentals with standardized column names
    """
    df = df.copy()
    
    # Standardize identifiers
    df["symbol"] = df["symbol"].astype(str).str.upper().str.strip()
    df["year"] = pd.to_numeric(df["year"], errors="coerce").astype("Int64")
    
    # Drop rows with missing identifiers
    df = df.dropna(subset=["symbol", "year"])
    
    # Define columns that should be numeric
    numeric_columns = [
        "total_asset", "total_equity", "total_liabilities",
        "total_current_asset", "total_current_liabilities",
        "is_net_revenue", "is_cogs", "is_manage_expense",
        "is_interest_expense", "is_eat", "is_net_business_profit",
        "na_tax_deferred", "nl_tax_deferred", "e_preferred_stock",
        "capex", "total_cfo", "ca_cce", "ca_total_inventory",
        "ca_acc_receiv", "cfo_interest_expense", "basic_eps",
        "is_shareholders_eat", "cl_loan", "cl_finlease",
        "cl_due_long_debt", "nl_loan", "nl_finlease",
        "is_cos_of_sales", "e_equity"
    ]
    
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    
    # Handle duplicates: keep row with most non-missing values
    df["_completeness"] = df.notna().sum(axis=1)
    df = (df
        .sort_values(["symbol", "year", "_completeness"])
        .drop_duplicates(subset=["symbol", "year"], keep="last")
        .drop(columns="_completeness")
        .reset_index(drop=True)
    )
    
    return df

df_fundamentals = clean_fundamentals(df_fundamentals_raw)
print(f"After cleaning: {len(df_fundamentals):,} firm-year observations")
print(f"Unique firms: {df_fundamentals['symbol'].nunique():,}")
```

### Creating Standardized Variables

To facilitate comparison with international studies and ensure compatibility with standard asset pricing methodologies, we create variables following conventions established in the academic literature. We map Vietnamese financial statement items to their Compustat equivalents where possible.

```{python}
#| eval: false
def create_standard_variables(df):
    """
    Create standardized financial variables for asset pricing research.
    
    This function maps Vietnamese financial statement items to standard
    variable names used in the academic finance literature, following
    conventions from Fama and French (1992, 1993, 2015).
    
    Parameters
    ----------
    df : pd.DataFrame
        Cleaned fundamentals data
    
    Returns
    -------
    pd.DataFrame
        Fundamentals with standardized variables added
    """
    df = df.copy()
    
    # Fiscal date (assume December year-end)
    df["datadate"] = pd.to_datetime(df["year"].astype(str) + "-12-31")
    
    # === Balance Sheet Items ===
    df["at"] = df["total_asset"]                    # Total assets
    df["lt"] = df["total_liabilities"]              # Total liabilities
    df["seq"] = df["total_equity"]                  # Stockholders' equity
    df["act"] = df["total_current_asset"]           # Current assets
    df["lct"] = df["total_current_liabilities"]     # Current liabilities
    
    # Common equity (fallback to total equity if not available)
    df["ceq"] = df.get("e_equity", df["seq"])
    
    # === Deferred Taxes ===
    df["txditc"] = df.get("na_tax_deferred", 0).fillna(0)  # Deferred tax assets
    df["txdb"] = df.get("nl_tax_deferred", 0).fillna(0)    # Deferred tax liab.
    df["itcb"] = 0  # Investment tax credit (rare in Vietnam)
    
    # === Preferred Stock ===
    pref = df.get("e_preferred_stock", 0)
    if isinstance(pref, pd.Series):
        pref = pref.fillna(0)
    df["pstk"] = pref
    df["pstkrv"] = pref  # Redemption value
    df["pstkl"] = pref   # Liquidating value
    
    # === Income Statement Items ===
    df["sale"] = df["is_net_revenue"]                        # Net sales/revenue
    df["cogs"] = df.get("is_cogs", 0).fillna(0)              # Cost of goods sold
    df["xsga"] = df.get("is_manage_expense", 0).fillna(0)    # SG&A expenses
    df["xint"] = df.get("is_interest_expense", 0).fillna(0)  # Interest expense
    df["ni"] = df.get("is_eat", np.nan)                      # Net income
    df["oibdp"] = df.get("is_net_business_profit", np.nan)   # Operating income
    
    # === Cash Flow Items ===
    df["oancf"] = df.get("total_cfo", np.nan)  # Operating cash flow
    df["capx"] = df.get("capex", np.nan)       # Capital expenditures
    
    return df

df_fundamentals = create_standard_variables(df_fundamentals)
```

### Computing Book Equity and Profitability

Book equity is a crucial variable for value investing strategies and the construction of HML (High Minus Low) factor portfolios. We follow the definition from Kenneth French's data library, which accounts for deferred taxes and preferred stock.

```{python}
#| eval: false
def compute_book_equity(df):
    """
    Compute book equity following Fama-French conventions.
    
    Book equity = Stockholders' equity 
                  + Deferred taxes and investment tax credit
                  - Preferred stock
    
    Negative or zero book equity is set to missing, as book-to-market
    ratios are undefined for such firms.
    
    Parameters
    ----------
    df : pd.DataFrame
        Fundamentals with standardized variables
    
    Returns
    -------
    pd.DataFrame
        Fundamentals with book equity (be) added
    """
    df = df.copy()
    
    # Primary measure: stockholders' equity
    # Fallback 1: common equity + preferred stock
    # Fallback 2: total assets - total liabilities
    seq_measure = (df["seq"]
        .combine_first(df["ceq"] + df["pstk"])
        .combine_first(df["at"] - df["lt"])
    )
    
    # Add deferred taxes
    deferred_taxes = (df["txditc"]
        .combine_first(df["txdb"] + df["itcb"])
        .fillna(0)
    )
    
    # Subtract preferred stock (use redemption value as primary)
    preferred = (df["pstkrv"]
        .combine_first(df["pstkl"])
        .combine_first(df["pstk"])
        .fillna(0)
    )
    
    # Book equity calculation
    df["be"] = seq_measure + deferred_taxes - preferred
    
    # Set non-positive book equity to missing
    df["be"] = df["be"].where(df["be"] > 0, np.nan)
    
    return df

df_fundamentals = compute_book_equity(df_fundamentals)

# Summary statistics for book equity
print("Book Equity Summary Statistics (in million VND):")
print(df_fundamentals["be"].describe().round(2))
```

Operating profitability, introduced by @Fama2015, measures a firm's profits relative to its book equity. Firms with higher operating profitability tend to have higher expected returns.

```{python}
#| eval: false
def compute_profitability(df):
    """
    Compute operating profitability following Fama-French (2015).
    
    Operating profitability = (Revenue - COGS - SG&A - Interest) / Book Equity
    
    Parameters
    ----------
    df : pd.DataFrame
        Fundamentals with book equity computed
    
    Returns
    -------
    pd.DataFrame
        Fundamentals with operating profitability (op) added
    """
    df = df.copy()
    
    # Operating profit before taxes
    operating_profit = (
        df["sale"] 
        - df["cogs"].fillna(0) 
        - df["xsga"].fillna(0) 
        - df["xint"].fillna(0)
    )
    
    # Scale by book equity
    df["op"] = operating_profit / df["be"]
    
    # Winsorize extreme values (outside 1st and 99th percentiles)
    lower = df["op"].quantile(0.01)
    upper = df["op"].quantile(0.99)
    df["op"] = df["op"].clip(lower=lower, upper=upper)
    
    return df

df_fundamentals = compute_profitability(df_fundamentals)
```

### Computing Investment

Investment, measured as asset growth, captures firms' investment behavior. @Fama2015 document that firms with high asset growth (aggressive investment) tend to have lower future returns.

```{python}
#| eval: false
def compute_investment(df):
    """
    Compute investment (asset growth) following Fama-French (2015).
    
    Investment = (Total Assets_t / Total Assets_{t-1}) - 1
    
    Parameters
    ----------
    df : pd.DataFrame
        Fundamentals data
    
    Returns
    -------
    pd.DataFrame
        Fundamentals with investment (inv) added
    """
    df = df.copy()
    
    # Create lagged assets
    df_lag = (df[["symbol", "year", "at"]]
        .assign(year=lambda x: x["year"] + 1)
        .rename(columns={"at": "at_lag"})
    )
    
    # Merge lagged values
    df = df.merge(df_lag, on=["symbol", "year"], how="left")
    
    # Compute investment (asset growth)
    df["inv"] = df["at"] / df["at_lag"] - 1
    
    # Set to missing if lagged assets non-positive
    df["inv"] = df["inv"].where(df["at_lag"] > 0, np.nan)
    
    return df

df_fundamentals = compute_investment(df_fundamentals)
```

### Computing Total Debt

In Vietnamese financial statements, total liabilities include non-interest-bearing items such as accounts payable and tax payables. For leverage analysis, we compute total interest-bearing debt by aggregating loan and lease obligations.

```{python}
#| eval: false
def compute_total_debt(df):
    """
    Compute total interest-bearing debt.
    
    Total Debt = Short-term loans + Finance leases (current)
                 + Current portion of long-term debt
                 + Long-term loans + Finance leases (non-current)
    
    Parameters
    ----------
    df : pd.DataFrame
        Fundamentals data
    
    Returns
    -------
    pd.DataFrame
        Fundamentals with total_debt added
    """
    df = df.copy()
    
    df["total_debt"] = (
        df.get("cl_loan", 0).fillna(0) +           # Short-term bank loans
        df.get("cl_finlease", 0).fillna(0) +       # Current finance leases
        df.get("cl_due_long_debt", 0).fillna(0) +  # Current portion LT debt
        df.get("nl_loan", 0).fillna(0) +           # Long-term bank loans
        df.get("nl_finlease", 0).fillna(0)         # Non-current finance leases
    )
    
    return df

df_fundamentals = compute_total_debt(df_fundamentals)
```

### Applying Filters and Final Preparation

We apply standard filters to ensure data quality: requiring positive assets, non-negative sales, and presence of core variables needed for portfolio construction.

```{python}
#| eval: false
# Keep only observations with required variables
required_vars = ["at", "lt", "seq", "sale"]
comp_vn = df_fundamentals.dropna(subset=required_vars)

# Apply quality filters
comp_vn = comp_vn.query("at > 0")      # Positive assets
comp_vn = comp_vn.query("sale >= 0")   # Non-negative sales

# Keep last observation per firm-year (in case of restatements)
comp_vn = (comp_vn
    .sort_values("datadate")
    .groupby(["symbol", "year"])
    .tail(1)
    .reset_index(drop=True)
)

# Diagnostic summary
print(f"Final sample: {len(comp_vn):,} firm-year observations")
print(f"Unique firms: {comp_vn['symbol'].nunique():,}")
print(f"Sample period: {comp_vn['year'].min()} - {comp_vn['year'].max()}")
```

### Storing Fundamentals Data

We store the prepared fundamentals data in our local SQLite database for use in subsequent chapters.

```{python}
#| eval: false
comp_vn.to_sql(
    name="comp_vn",
    con=tidy_finance,
    if_exists="replace",
    index=False
)

print("Company fundamentals saved to database.")
```

## Stock Price Data

Stock price data forms the foundation of return-based analyses in empirical finance. Datacore provides comprehensive historical price data for all securities traded on HOSE, HNX, and UPCoM, including adjusted prices that account for corporate actions.

### Downloading Price Data

We download the historical price data from Datacore's storage system. The data includes daily observations with open, high, low, close prices, trading volume, and adjustment factors.

```{python}
#| eval: false
# Download historical price data
prices_raw = conn.read_csv(
    bucket_name,
    "historical_price/dataset_historical_price.csv",
    low_memory=False
)

print(f"Downloaded {len(prices_raw):,} daily price observations")
print(f"Date range: {prices_raw['date'].min()} to {prices_raw['date'].max()}")
```

### Processing Price Data

We clean the price data and compute adjusted prices that account for stock splits, stock dividends, and other corporate actions.

```{python}
#| eval: false
def process_price_data(df):
    """
    Process raw price data from Datacore.
    
    Parameters
    ----------
    df : pd.DataFrame
        Raw price data
    
    Returns
    -------
    pd.DataFrame
        Processed price data with standardized columns
    """
    df = df.copy()
    
    # Parse dates
    df["date"] = pd.to_datetime(df["date"])
    
    # Standardize column names
    df = df.rename(columns={
        "open_price": "open",
        "high_price": "high",
        "low_price": "low",
        "close_price": "close",
        "vol_total": "volume"
    })
    
    # Compute adjusted close price
    df["adjusted_close"] = df["close"] * df["adj_ratio"]
    
    # Standardize symbol
    df["symbol"] = df["symbol"].astype(str).str.upper().str.strip()
    
    # Sort for return calculation
    df = df.sort_values(["symbol", "date"])
    
    # Add year for merging with fundamentals
    df["year"] = df["date"].dt.year
    
    return df

prices = process_price_data(prices_raw)
```

### Computing Returns

We compute daily returns using adjusted closing prices to ensure returns correctly reflect total shareholder returns including dividends and corporate actions.

```{python}
#| eval: false
def compute_returns(df):
    """
    Compute daily returns from adjusted prices.
    
    Returns are computed as simple percentage changes in adjusted
    closing prices. Extreme negative returns beyond -99% are capped
    to handle potential data errors.
    
    Parameters
    ----------
    df : pd.DataFrame
        Price data with adjusted_close column
    
    Returns
    -------
    pd.DataFrame
        Price data with returns added
    """
    df = df.copy()
    
    # Compute daily returns
    df["ret"] = (df
        .groupby("symbol")["adjusted_close"]
        .pct_change()
    )
    
    # Cap extreme negative returns (data quality safeguard)
    df["ret"] = df["ret"].clip(lower=-0.99)
    
    return df

prices = compute_returns(prices)

# Summary statistics
print("Daily Return Summary Statistics:")
print(prices["ret"].describe().round(4))
```

### Computing Market Capitalization

Market capitalization is computed as the product of price and shares outstanding. Since Datacore provides earnings per share and net income, we can infer shares outstanding from these variables.

```{python}
#| eval: false
def compute_market_cap(prices_df, fundamentals_df):
    """
    Compute market capitalization by inferring shares outstanding.
    
    Shares outstanding = Net income attributable to shareholders / Basic EPS
    
    Parameters
    ----------
    prices_df : pd.DataFrame
        Daily price data
    fundamentals_df : pd.DataFrame
        Fundamentals with EPS and earnings data
    
    Returns
    -------
    pd.DataFrame
        Price data with market capitalization added
    """
    # Compute shares outstanding from fundamentals
    shares = fundamentals_df.copy()
    shares["shrout"] = shares["is_shareholders_eat"] / shares["basic_eps"]
    shares = shares[["symbol", "year", "shrout"]].dropna()
    
    # Merge with prices
    df = prices_df.merge(shares, on=["symbol", "year"], how="left")
    
    # Compute market cap (in million VND)
    df["mktcap"] = (df["close"] * df["shrout"]) / 1_000_000
    
    # Set zero or negative market cap to missing
    df["mktcap"] = df["mktcap"].where(df["mktcap"] > 0, np.nan)
    
    return df

prices = compute_market_cap(prices, df_fundamentals)
```

### Converting to Monthly Frequency

Most asset pricing studies use monthly returns to reduce noise and align with the frequency of fundamental data releases. We aggregate daily data to monthly frequency, keeping the last observation of each month.

```{python}
#| eval: false
def convert_to_monthly(df):
    """
    Convert daily price data to monthly frequency.
    
    We use month-end observations for prices and market cap,
    and compute monthly returns from adjusted prices.
    
    Parameters
    ----------
    df : pd.DataFrame
        Daily price data
    
    Returns
    -------
    pd.DataFrame
        Monthly price data
    """
    # Resample to monthly, keeping last observation
    monthly = (df
        .sort_values(["symbol", "date"])
        .groupby("symbol")
        .resample("ME", on="date")
        .last()
        .drop(columns=["symbol", "date"], errors="ignore")
        .reset_index()
    )
    
    # Compute monthly returns
    monthly["ret"] = (monthly
        .groupby("symbol")["adjusted_close"]
        .pct_change()
    )
    
    # Compute lagged market cap for portfolio weighting
    monthly["mktcap_lag"] = (monthly
        .groupby("symbol")["mktcap"]
        .shift(1)
    )
    
    # Add year-month identifier
    monthly["year"] = monthly["date"].dt.year
    monthly["month"] = monthly["date"].dt.month
    
    return monthly

prices_monthly = convert_to_monthly(prices)
print(f"Monthly observations: {len(prices_monthly):,}")
```

### Computing Excess Returns

Excess returns are computed by subtracting the risk-free rate from total returns. For Vietnam, we use a proxy based on government bond yields, as a dedicated monthly risk-free rate series analogous to the US Treasury bill rate is not readily available.

```{python}
#| eval: false
def compute_excess_returns(df, annual_rf=0.04):
    """
    Compute excess returns over the risk-free rate.
    
    We use an annualized risk-free rate of 4%, which approximates
    the yield on Vietnamese government bonds during our sample period.
    
    Parameters
    ----------
    df : pd.DataFrame
        Monthly price data with returns
    annual_rf : float
        Annualized risk-free rate (default: 4%)
    
    Returns
    -------
    pd.DataFrame
        Monthly data with excess returns
    """
    df = df.copy()
    
    # Convert annual rate to monthly
    monthly_rf = annual_rf / 12
    df["risk_free"] = monthly_rf
    
    # Compute excess returns
    df["ret_excess"] = df["ret"] - df["risk_free"]
    
    # Cap at -100% (cannot lose more than investment)
    df["ret_excess"] = df["ret_excess"].clip(lower=-1.0)
    
    return df

prices_monthly = compute_excess_returns(prices_monthly)

# Remove observations with missing essential variables
prices_monthly = prices_monthly.dropna(
    subset=["ret_excess", "mktcap", "mktcap_lag"]
)

print(f"Final monthly sample: {len(prices_monthly):,} observations")
```

### Storing Monthly Price Data

We store the processed monthly price data in our database.

```{python}
#| eval: false
prices_monthly.to_sql(
    name="prices_monthly",
    con=tidy_finance,
    if_exists="replace",
    index=False
)

print("Monthly price data saved to database.")
```

## Descriptive Statistics

Before proceeding to asset pricing analyses, we examine the characteristics of our sample to understand the Vietnamese equity market's evolution and composition.

### Market Evolution Over Time

We first examine how the number of listed securities has grown over time.

```{python}
#| eval: false
securities_over_time = (prices_monthly
    .groupby("date")
    .agg(
        n_securities=("symbol", "nunique"),
        total_mktcap=("mktcap", "sum")
    )
    .reset_index()
)
```

```{python}
#| label: fig-securities-over-time
#| eval: false
#| fig-cap: "The figure shows the monthly number of securities in the Vietnamese stock market sample."
#| fig-alt: "Line chart showing the growth in number of listed securities over time."
securities_figure = (
    ggplot(securities_over_time, aes(x="date", y="n_securities"))
    + geom_line(color="steelblue", size=1)
    + labs(
        x="",
        y="Number of Securities",
        title="Growth of Vietnamese Stock Market"
    )
    + scale_x_datetime(date_breaks="2 years", date_labels="%Y")
    + scale_y_continuous(labels=comma_format())
    + theme_minimal()
)
securities_figure.show()
```

### Market Capitalization Evolution

The aggregate market capitalization reflects the overall size and development of the Vietnamese equity market.

```{python}
#| label: fig-market-cap-over-time
#| eval: false
#| fig-cap: "The figure shows the total market capitalization of Vietnamese listed companies over time."
#| fig-alt: "Line chart showing total market capitalization growth."
mktcap_figure = (
    ggplot(securities_over_time, aes(x="date", y="total_mktcap / 1000"))
    + geom_line(color="darkgreen", size=1)
    + labs(
        x="",
        y="Market Cap (Trillion VND)",
        title="Total Market Capitalization of Vietnamese Equities"
    )
    + scale_x_datetime(date_breaks="2 years", date_labels="%Y")
    + scale_y_continuous(labels=comma_format())
    + theme_minimal()
)
mktcap_figure.show()
```

### Return Distribution

Understanding the distribution of monthly returns helps identify potential data quality issues and characterize market risk.

```{python}
#| label: fig-return-distribution
#| eval: false
#| fig-cap: "Distribution of monthly excess returns for Vietnamese stocks."
#| fig-alt: "Histogram showing the distribution of monthly excess returns."
return_distribution = (
    ggplot(prices_monthly, aes(x="ret_excess"))
    + geom_histogram(
        binwidth=0.02, 
        fill="steelblue", 
        color="white",
        alpha=0.7
    )
    + labs(
        x="Monthly Excess Return",
        y="Frequency",
        title="Distribution of Monthly Excess Returns"
    )
    + scale_x_continuous(limits=(-0.5, 0.5))
    + theme_minimal()
)
return_distribution.show()
```

### Coverage of Book Equity

Book equity is essential for constructing value portfolios. We examine what fraction of our sample has book equity data available over time.

```{python}
#| label: fig-book-equity-coverage
#| eval: false
#| fig-cap: "Share of securities with available book equity data by year."
#| fig-alt: "Line chart showing the coverage of book equity data over time."
# Merge prices with fundamentals
coverage_data = (prices_monthly
    .assign(year=lambda x: x["date"].dt.year)
    .groupby(["symbol", "year"])
    .tail(1)
    .merge(comp_vn[["symbol", "year", "be"]], 
           on=["symbol", "year"], 
           how="left")
)

# Compute coverage by year
be_coverage = (coverage_data
    .groupby("year")
    .apply(lambda x: pd.Series({
        "share_with_be": x["be"].notna().mean()
    }))
    .reset_index()
)

coverage_figure = (
    ggplot(be_coverage, aes(x="year", y="share_with_be"))
    + geom_line(color="darkorange", size=1)
    + geom_point(color="darkorange", size=2)
    + labs(
        x="Year",
        y="Share with Book Equity",
        title="Coverage of Book Equity Data"
    )
    + scale_y_continuous(labels=percent_format(), limits=(0, 1))
    + theme_minimal()
)
coverage_figure.show()
```

## Daily Stock Data

While monthly data suffices for most asset pricing applications, some research questions require daily frequency data. Given the large size of daily data, we provide a batch download approach that processes stocks in groups to manage memory efficiently.

```{python}
#| eval: false
def download_daily_data_batch(conn, bucket_name, batch_size=100):
    """
    Download and process daily stock data in batches.
    
    This function handles large datasets by processing stocks in
    smaller batches to avoid memory issues.
    
    Parameters
    ----------
    conn : DatacoreConnection
        Connection to Datacore storage
    bucket_name : str
        Name of the data bucket
    batch_size : int
        Number of stocks to process per batch
    
    Returns
    -------
    None
        Data is saved directly to SQLite database
    """
    # Get list of all symbols
    symbols = prices_monthly["symbol"].unique().tolist()
    n_batches = int(np.ceil(len(symbols) / batch_size))
    
    print(f"Processing {len(symbols)} symbols in {n_batches} batches...")
    
    for i in range(n_batches):
        batch_symbols = symbols[i*batch_size : (i+1)*batch_size]
        
        # Filter daily data for batch
        batch_data = prices[prices["symbol"].isin(batch_symbols)].copy()
        
        # Compute excess returns
        batch_data["ret_excess"] = batch_data["ret"] - (0.04 / 252)
        batch_data["ret_excess"] = batch_data["ret_excess"].clip(lower=-1.0)
        
        # Select columns to save
        batch_data = batch_data[["symbol", "date", "ret_excess"]]
        
        # Save to database
        if_exists = "replace" if i == 0 else "append"
        batch_data.to_sql(
            name="prices_daily",
            con=tidy_finance,
            if_exists=if_exists,
            index=False
        )
        
        print(f"Batch {i+1}/{n_batches} complete ({len(batch_data):,} rows)")
    
    print("Daily data download complete.")
```

## Merging Stock and Fundamental Data

The final step links price data with fundamental data using the stock symbol as the common identifier. This merged dataset forms the basis for constructing portfolios sorted on firm characteristics.

```{python}
#| eval: false
# Example: Create merged dataset for end-of-June each year
merged_data = (prices_monthly
    .query("month == 6")
    .merge(
        comp_vn[["symbol", "year", "be", "op", "inv", "at", "mktcap"]],
        on=["symbol", "year"],
        how="left",
        suffixes=("", "_fundamental")
    )
)

# Compute book-to-market ratio
merged_data["bm"] = merged_data["be"] / merged_data["mktcap"]

print(f"Merged observations: {len(merged_data):,}")
print(f"With book-to-market: {merged_data['bm'].notna().sum():,}")
```

## Key Takeaways

1. **Datacore provides unified access** to Vietnamese financial data through a modern cloud-based infrastructure, eliminating the need to aggregate data from multiple fragmented sources.

2. **Company fundamentals** from Datacore include comprehensive balance sheet, income statement, and cash flow data prepared according to Vietnamese Accounting Standards, which we map to standard variables used in international research.

3. **Book equity computation** follows the Fama-French methodology, accounting for deferred taxes and preferred stock to ensure comparability with US-based studies.

4. **Stock price data** includes adjustment factors for corporate actions, enabling accurate return calculations over long horizons.

5. **Monthly frequency** is standard for asset pricing research, reducing noise while maintaining sufficient observations for statistical inference.

6. **Risk-free rate approximation** uses Vietnamese government bond yields as a proxy, given the absence of a standardized short-term rate series comparable to US Treasury bills.

7. **Data quality validation** through descriptive statistics and visualization helps identify potential issues before conducting formal analyses.

8. **Batch processing** enables efficient handling of large daily datasets that would otherwise exceed memory constraints.
