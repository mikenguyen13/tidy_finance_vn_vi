{
  "hash": "f733a2a5aa04528e5a32bfc4f1f6ed1f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Fama-French Factors\nformat:\n  html:\n    toc: true\n    number-sections: true\njupyter: python3\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\nIn this chapter, we provide a replication of the famous Fama-French factor portfolios. The Fama-French factor models are a cornerstone of empirical asset pricing [see @Fama1992 and @FamaFrench2015]. On top of the market factor represented by the traditional CAPM beta, the three-factor model includes the size and value factors to explain the cross section of returns. Its successor, the five-factor model, additionally includes profitability and investment as explanatory factors.\n\nWe start with the three-factor model. We already introduced the size and value factors in [Value and Bivariate Sorts](value-and-bivariate-sorts.qmd), and their definition remains the same: size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\n\nAfter the replication of the three-factor model, we move to the five-factors by constructing the profitability factor RMW (robust-minus-weak) as the difference between the returns of firms with high and low operating profitability and the investment factor CMA (conservative-minus-aggressive) as the difference between firms with high versus low investment rates.\n\nThe current chapter relies on this set of Python packages.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n\n\nfrom regtabletotext import prettify_result\n```\n:::\n\n\n## Data Preparation\n\nWe use Stock Data and Fundamentals as data sources, as we need exactly the same variables to compute the size and value factors in the way Fama and French do it.[^1]\n\n[^1]: Note that @Fama1992 claim to exclude financial firms. To a large extent this happens through using industry format \"INDL\". Neither the original paper, nor Ken French's website, or the WRDS replication contains any indication that financial companies are excluded using additional filters such as industry codes.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(\n  database=\"data/tidy_finance_python.sqlite\"\n)\n\nprices_monthly = (pd.read_sql_query(\n    sql=(\"SELECT symbol, date, ret_excess, mktcap, risk_free,\"\n         \"mktcap_lag FROM prices_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .dropna()\n)\n\ncomp_vn = (pd.read_sql_query(\n    sql=\"SELECT symbol, datadate, be, op, inv FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"})\n  .dropna()\n)\n```\n:::\n\n\nFollowing Fama-French standards for empirical asset pricing:\n\n1.  Identification of Firm Size (June Market Cap)\n\nFollowing the Fama and French protocol, we identify the market capitalization in June of year $t$. This represents the firm's \"Size\" at the moment of portfolio formation. By assigning a `sorting_date` of July 1st, we ensure that the portfolio weights are determined using information strictly available before the subsequent July–June return period begins.\n\n2.  Establishing the Market Equity Benchmark (December Market Cap)\n\nTo calculate the Book-to-Market ratio, we extract the market capitalization from December of year $t-1$. This specific timestamp is used to scale the book equity values. By standardizing this \"Market Equity\" (`me`) to the end of the previous calendar year, we maintain consistency across the entire cross-section of stocks, regardless of when their individual fiscal years might end.\n\n3.  Construction of the Book-to-Market Ratio\n\nThe Book-to-Market (`bm`) ratio is constructed using the most recent fiscal year-end book equity (`be`) from the `comp_vn` dataset and the preceding December market equity. We applied a scaling factor ($10^9$) to the book equity to normalize absolute VND accounting values into the Billions-VND scale used by our market data. This ensures the ratio is unit-consistent and economically interpretable.\n\n4.  Final Data Integration and De-duplication\n\nIn the final step, we merge the Size and Value components into a single `sorting_variables` table using the `symbol` and `sorting_date` as keys. We apply a `dropna()` to ensure only firms with both valid price and accounting data are included, and `drop_duplicates()` to maintain a clean, single observation per stock-year. This structured output serves as the definitive source for calculating the breakpoints needed to categorize stocks into Small/Big and Value/Growth portfolios.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# 1. Size (June Market Cap)\nsize = (prices_monthly\n  .query(\"date.dt.month == 6\")\n  # Use MonthBegin(1) to set to July 1st\n  .assign(sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(1))\n  .get([\"symbol\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"size\"})\n)\nsize.head(3)\n\n# 2. Market Equity (December Market Cap for BM scaling)\nmarket_equity = (prices_monthly\n  .query(\"date.dt.month == 12\")\n  # Shift December t-1 to July 1st of year t\n  .assign(sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(7))\n  .get([\"symbol\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"me\"})\n)\nmarket_equity.head(3)\n\n# 3. Calculate Book-to-Market (BM) with Correct Scaling\nbook_to_market = (comp_vn\n    .assign(\n        sorting_date=lambda x: pd.to_datetime((x[\"datadate\"].dt.year + 1).astype(str) + \"-07-01\")\n    )\n    .merge(market_equity, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n    # Adjusted scaling: Dividing BE by 1,000,000,000 to convert absolute VND to Billions\n    # to match the scale of your Market Equity (me)\n    .assign(bm=lambda x: x[\"be\"] / (x[\"me\"] * 1000000000)) \n    [[\"symbol\", \"sorting_date\", \"me\", \"bm\"]]\n)\n\n# SANITY CHECK\nprint(f\"New Median BM Ratio: {book_to_market['bm'].median():.4f}\")\nbook_to_market.head(3)\n\n# Sanity Check: Print the median BM to ensure it is near 1.0\nprint(f\"Median BM Ratio: {book_to_market['bm'].median():.4f}\")\n\n# 4. Final Merge (This should now work)\nsorting_variables = (size\n  .merge(book_to_market, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"symbol\", \"sorting_date\"])\n)\nsorting_variables.head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNew Median BM Ratio: 1.1775\nMedian BM Ratio: 1.1775\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>sorting_date</th>\n      <th>size</th>\n      <th>me</th>\n      <th>bm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A32</td>\n      <td>2019-07-01</td>\n      <td>153.00</td>\n      <td>205.36</td>\n      <td>0.977852</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A32</td>\n      <td>2020-07-01</td>\n      <td>178.84</td>\n      <td>190.40</td>\n      <td>1.174437</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A32</td>\n      <td>2021-07-01</td>\n      <td>217.60</td>\n      <td>234.60</td>\n      <td>1.032468</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Portfolio Sorts\n\nNext, we construct our portfolios with an adjusted `assign_portfolio()` function. Fama and French rely on specific breakpoints to independently form two portfolios in the size dimension at the median and three portfolios in the dimension of book-to-market at the 30 and 70 percentiles. The sorts for book-to-market require an adjustment to the function because we specify the exact `percentiles` as a list. Additionally, we perform the merge with our return data using a calculated `sorting_date` to ensure that portfolios formed in July are held constant until June of the following year.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef assign_portfolio(data, sorting_variable, percentiles):\n    \"\"\"Assign portfolios to a bin according to a sorting variable.\"\"\"\n    \n    # Calculate breakpoints based on quantile sequences\n    breakpoints = (data\n      .get(sorting_variable)\n      .quantile(percentiles, interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    \n    # Ensure the range covers all possible values\n    breakpoints.iloc[0] = -np.inf\n    breakpoints.iloc[breakpoints.size-1] = np.inf\n    \n    # Categorize into bins\n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=pd.Series(range(1, breakpoints.size)),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\n# 1. Assign Portfolios (Annual Sorts)\n# We calculate breakpoints and assign portfolios 1-2 for size and 1-3 for BM\nportfolios_assigned = (sorting_variables\n  .groupby(\"sorting_date\")\n  .apply(lambda x: x.assign(\n      portfolio_size=assign_portfolio(x, \"size\", [0, 0.5, 1]),\n      portfolio_bm=assign_portfolio(x, \"bm\", [0, 0.3, 0.7, 1])\n    ), include_groups=False)\n  .reset_index()\n  # We keep 'size' and 'bm' here so they are available after the merge\n  .get([\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\", \"size\", \"bm\"])\n)\n\n# 2. Merge Portfolios to Monthly Returns\n# Portfolios formed in July are held until June of the following year\nportfolios = (prices_monthly\n  .assign(\n    sorting_date=lambda x: pd.to_datetime(\n        np.where(x[\"date\"].dt.month <= 6,\n                 (x[\"date\"].dt.year - 1).astype(str) + \"0701\",\n                 x[\"date\"].dt.year.astype(str) + \"0701\")\n    )\n  )\n  .merge(portfolios_assigned, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n)\n```\n:::\n\n\n1.  **Breakpoint Determination**: The `assign_portfolio` function calculates quantiles annually. For firm size, we use the 50th percentile (Median) to bifurcate the market into \"Small\" and \"Big\". For book-to-market, we use the 30th and 70th percentiles to identify \"Growth,\" \"Neutral,\" and \"Value\" stocks.\n\n2.  **Independent Sorting**: We apply these breakpoints independently. This methodology allows a stock to be classified into one of six distinct $2 \\times 3$ portfolios, facilitating the isolation of specific factor premiums.\n\n3.  **Temporal Alignment**: Because financial statements in Vietnam are typically released by April, the July 1st sorting date ensures that the accounting information used in the `bm` ratio is publicly available before the portfolio return period begins.\n\n4.  **Holding Period Persistence**: The `sorting_date` logic ensures that the portfolio assignments made in July remain constant for the next twelve months (July through the following June), consistent with the original Fama-French experimental design.\n\nSanity Check 1: Portfolio Distribution (The 2x3 Grid)\n\nFirst, we verify that the independent sorts created the expected six portfolios (2 size groups $\\times$ 3 value groups).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Check the count of stocks in each portfolio combination for the most recent year\nportfolio_counts = (portfolios\n    .query(\"date == date.max()\")\n    .groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)\n    .size()\n    .unstack()\n)\nprint(\"Portfolio Counts (Size x BM):\")\nprint(portfolio_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPortfolio Counts (Size x BM):\nportfolio_bm      1    2    3\nportfolio_size               \n1               112  271  262\n2               275  245  125\n```\n:::\n:::\n\n\n**Interpretation**:\n\nThe Fama-French methodology relies on having a sufficient number of stocks in each of the six bins to diversify idiosyncratic risk. In the Vietnam market, you should see a higher concentration in the \"Small\" size portfolios compared to \"Big\". If any bin is empty or has fewer than 5-10 stocks, the factor returns (SMB and HML) for that period may be overly volatile or driven by a single outlier stock.\n\nSanity Check 2: Characteristic Monotonicity\n\nWe check if the average Book-to-Market ratio actually increases as we move from `portfolio_bm` 1 to 3.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Verify that higher portfolio numbers correspond to higher BM values\nbm_check = (portfolios\n    .groupby(\"portfolio_bm\", observed=True)\n    .agg({\"bm\": [\"mean\", \"median\", \"min\", \"max\"]})\n)\nprint(\"\\nBM Characteristic Check:\")\nprint(bm_check)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBM Characteristic Check:\n                    bm                                \n                  mean    median       min         max\nportfolio_bm                                          \n1             0.597570  0.585419  0.001416    1.435144\n2             1.260400  1.188112  0.584759    2.737556\n3             3.379031  2.454220  1.017893  272.189334\n```\n:::\n:::\n\n\n**Interpretation**:\n\nFor the sort to be valid, the mean and median `bm` must be strictly increasing across the portfolios (Portfolio 1 \\< Portfolio 2 \\< Portfolio 3). Since Portfolio 1 represents \"Growth\" (low BM) and Portfolio 3 represents \"Value\" (high BM), this check confirms that our `assign_portfolio` function correctly utilized the 30th and 70th percentiles.\n\nSanity Check 3: Holding Period Persistence\n\nWe verify that for a single stock, the portfolio assignment remains constant between July of one year and June of the next.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Trace a single symbol (e.g., 'A32') across a formation window\npersistence_check = (portfolios\n    .query(\"symbol == 'A32' & date >= '2022-01-01' & date <= '2023-12-31'\")\n    .sort_values(\"date\")\n    [['symbol', 'date', 'sorting_date', 'portfolio_size', 'portfolio_bm']]\n)\nprint(\"\\nTemporal Persistence Check (Symbol A32):\")\nprint(persistence_check.head(15))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTemporal Persistence Check (Symbol A32):\n   symbol       date sorting_date portfolio_size portfolio_bm\n30    A32 2022-01-31   2021-07-01              1            2\n31    A32 2022-02-28   2021-07-01              1            2\n32    A32 2022-03-31   2021-07-01              1            2\n33    A32 2022-04-30   2021-07-01              1            2\n34    A32 2022-05-31   2021-07-01              1            2\n35    A32 2022-06-30   2021-07-01              1            2\n36    A32 2022-07-31   2022-07-01              1            3\n37    A32 2022-08-31   2022-07-01              1            3\n38    A32 2022-09-30   2022-07-01              1            3\n39    A32 2022-10-31   2022-07-01              1            3\n40    A32 2022-11-30   2022-07-01              1            3\n41    A32 2022-12-31   2022-07-01              1            3\n42    A32 2023-01-31   2022-07-01              1            3\n43    A32 2023-02-28   2022-07-01              1            3\n44    A32 2023-03-31   2022-07-01              1            3\n```\n:::\n:::\n\n\n**Interpretation**:\n\nThis check ensures our `sorting_date` logic is working. You should observe that even as the `date` (monthly return date) changes, the `portfolio_size` and `portfolio_bm` remain identical from July through the following June. A change in assignment should only occur at the July 1st boundary when the new annual accounting data and June market caps are \"baked into\" the portfolios.\n\nFinal Summary of the 4-Step Process\n\n1.  **Breakpoint Calculation**: We calculate the 50th percentile for Size and 30th/70th for BM annually, ensuring the \"goalposts\" move with the market's overall valuation.\n\n2.  **Independent Assignment**: By sorting Size and BM separately, we create a matrix that allows us to see how a \"Small Value\" stock performs relative to a \"Small Growth\" stock.\n\n3.  **Information Lag Handling**: Using the July 1st `sorting_date` respects the reality of Vietnamese financial reporting, ensuring we don't use \"future\" accounting data that wasn't public yet.\n\n4.  **Portfolio Rebalancing**: The annual rebalancing cycle (July–June) mimics the standard Fama-French experimental design, providing a rigorous framework for testing factor premiums in the Vietnam stock market.\n\n## Fama-French Three-Factor Model\n\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama-French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally (using the `mean()` function).\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfactors = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"])\n  .apply(lambda x: pd.Series({\n    \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n   )\n  .reset_index()\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"smb\": (\n      x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_size\"] == 2].mean()),\n    \"hml\": (\n      x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() -\n        x[\"ret\"][x[\"portfolio_bm\"] == 1].mean())\n    }))\n  .reset_index()\n)\n\nfactors.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>smb</th>\n      <th>hml</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2011-07-31</td>\n      <td>-0.004895</td>\n      <td>0.013730</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2011-08-31</td>\n      <td>-0.012568</td>\n      <td>-0.009302</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-09-30</td>\n      <td>0.002562</td>\n      <td>-0.002044</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe market factor ($Mkt - RF$) is defined as the value-weighted return of all stocks in the investable universe minus the risk-free rate. Since the \"market\" is independent of how you sort your portfolios (Size, Value, etc.), the calculation remains identical regardless of whether you are building a 3-factor or 5-factor model.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n## Fama-French Three-Factor Model\n# --- Calculate Market Factor independently ---\n# This uses the entire prices_monthly universe to represent the broad market\nfactor_market_excess = (prices_monthly\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n      \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    }), include_groups=False)\n  .reset_index()\n)\n\n# --- Merge ---\n# Combine your replicated SMB/HML with the Market Factor\nfactors_ff3_monthly = (factors\n  .merge(factor_market_excess, on=\"date\", how=\"inner\")\n)\n\nfactors_ff3_monthly.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>smb</th>\n      <th>hml</th>\n      <th>mkt_excess</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2011-07-31</td>\n      <td>-0.004895</td>\n      <td>0.013730</td>\n      <td>-0.011287</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2011-08-31</td>\n      <td>-0.012568</td>\n      <td>-0.009302</td>\n      <td>0.007856</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2011-09-30</td>\n      <td>0.002562</td>\n      <td>-0.002044</td>\n      <td>-0.006501</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Remove rows with missing factor values\n# We keep only rows where the characteristic factors are fully populated\nfactors_ff3_monthly = (factors_ff3_monthly\n    .dropna(subset=[\"smb\", \"hml\", \"mkt_excess\"])\n    .reset_index(drop=True)\n)\n\n# Sanity Check\nprint(f\"Factors cleaned. Sample period: {factors_ff3_monthly['date'].min().date()} to {factors_ff3_monthly['date'].max().date()}\")\nprint(\"\\nFirst 3 rows of cleaned factors:\")\nprint(factors_ff3_monthly.head(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactors cleaned. Sample period: 2011-07-31 to 2023-12-31\n\nFirst 3 rows of cleaned factors:\n        date       smb       hml  mkt_excess\n0 2011-07-31 -0.004895  0.013730   -0.011287\n1 2011-08-31 -0.012568 -0.009302    0.007856\n2 2011-09-30  0.002562 -0.002044   -0.006501\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# --- Save to Database ---\n(factors_ff3_monthly\n  .to_sql(name=\"factors_ff3_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n150\n```\n:::\n:::\n\n\n## Fama-French Five-Factor Model\n\nNow, let us move to the replication of the five-factor model. We extend the `other_sorting_variables` table from above with the additional characteristics operating profitability `op` and investment `inv`. Note that the `dropna()` statement yields different sample sizes, as some firms with `be` values might not have `op` or `inv` values.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Ensure your BM median is ~1.17 and max is not 272\n# adjust the 1e9 based on your 'me' scale\nother_sorting_variables = (comp_vn\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      (x[\"datadate\"].dt.year+1).astype(str)+\"0701\", format=\"%Y%m%d\")\n    )\n  )\n  .merge(market_equity, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n  .assign(bm=lambda x: x[\"be\"]/1e9/x[\"me\"])\n  .get([\"symbol\", \"sorting_date\", \"me\", \"bm\", \"op\", \"inv\"])\n)\n\nprint(other_sorting_variables['bm'].median())\n\n# Independent Size Sort\nsorting_variables = (size\n  .merge(other_sorting_variables, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"symbol\", \"sorting_date\"])\n )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.1775041699783042\n```\n:::\n:::\n\n\nIn each month, we independently sort all stocks into the two size portfolios. The value, profitability, and investment portfolios, on the other hand, are the results of dependent sorts based on the size portfolios. We then merge the portfolios to the return data for the rest of the year just as above.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom scipy.stats.mstats import winsorize\n\n# Winsorize Characteristics (1st and 99th percentiles)\n# This handles your extreme 272.18 BM values.\nvars_to_clean = [\"bm\", \"op\", \"inv\"]\nfor var in vars_to_clean:\n    sorting_variables[var] = winsorize(sorting_variables[var], limits=[0.01, 0.01])\n\n\n# Use transform to keep columns exactly as they are\nsorting_variables['portfolio_size'] = (sorting_variables\n  .groupby('sorting_date', group_keys=False)\n  .apply(lambda x: assign_portfolio(x, 'size', [0, 0.5, 1]))\n  .values\n)\n\n# Dependent Sorts (Sub-grouping)\n# We calculate each characteristic portfolio one by one to avoid KeyError\ndef dependent_sort(df, var, name):\n    return (df.groupby(['sorting_date', 'portfolio_size'], group_keys=False)\n            .apply(lambda x: assign_portfolio(x, var, [0, 0.3, 0.7, 1])))\n\nsorting_variables['portfolio_bm'] = dependent_sort(sorting_variables, 'bm', 'portfolio_bm').values\nsorting_variables['portfolio_op'] = dependent_sort(sorting_variables, 'op', 'portfolio_op').values\nsorting_variables['portfolio_inv'] = dependent_sort(sorting_variables, 'inv', 'portfolio_inv').values\n\n# --- Column Selection for Sorting Variables ---\nportfolios_sorting = sorting_variables.get([\n    \"symbol\", \"sorting_date\", \"portfolio_size\", \n    \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"\n])\nportfolios_sorting.head(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>sorting_date</th>\n      <th>portfolio_size</th>\n      <th>portfolio_bm</th>\n      <th>portfolio_op</th>\n      <th>portfolio_inv</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A32</td>\n      <td>2019-07-01</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A32</td>\n      <td>2020-07-01</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A32</td>\n      <td>2021-07-01</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nStep-by-Step Interpretation\n\n-   **Size-Based Stratification**: We first partition the market into \"Small\" and \"Big\" portfolios using the median market capitalization. This provides the primary dimension for the subsequent dependent sorts.\n\n-   **Conditional Characteristic Sorting**: Unlike the independent sorts of the 3-factor model, the 5-factor model employs **dependent sorts** for Value (BM), Profitability (OP), and Investment (INV). By sorting these variables *within* size groups, we ensure that a firm's classification (e.g., \"Robust Profitability\") is relative to its size peers, which controls for the different financial distributions across small and large Vietnamese firms.\n\n-   **Temporal Synchronization**: We maintain the July 1st formation date to accommodate the Vietnamese corporate reporting landscape, where audited annual results are typically finalized by late April. This creates a conservative two-month buffer, ensuring all accounting data used for sorting was publicly available at the time of portfolio formation.\n\n-   **Portfolio Persistence**: The resulting assignments are held for a 12-month period (July through June). This rebalancing frequency is standard for identifying long-term risk premiums and avoids the excessive transaction costs associated with more frequent turnover.\n\nCheck 1: Characteristic Monotonicity\n\nThis confirms the sort effectively separated stocks by their economic quality.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# Verify that higher portfolio numbers have higher median characteristics\nsanity_check = (sorting_variables\n    .groupby(\"portfolio_op\")[\"op\"].median()\n)\nprint(\"Median OP by Portfolio (Should be strictly increasing):\")\nprint(sanity_check)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMedian OP by Portfolio (Should be strictly increasing):\nportfolio_op\n1    0.132213\n2    0.136530\n3    0.141302\nName: op, dtype: float64\n```\n:::\n:::\n\n\n**Interpretation**: If Portfolio 1 (Weak) has a lower median OP than Portfolio 3 (Robust), your sort is economically valid. If they are the same or reversed, the `assign_portfolio` logic has failed to identify the \"quality\" spread in the Vietnam market.\n\nCheck 2: Bin Diversification\n\nThis ensures each factor is built on enough stocks to be statistically reliable.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Check the 2x3 grid for Profitability\nrmw_counts = sorting_variables.groupby([\"portfolio_size\", \"portfolio_op\"]).size().unstack()\nprint(\"\\nStocks per Size/Profitability Bin:\")\nprint(rmw_counts)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nStocks per Size/Profitability Bin:\nportfolio_op       1     2     3\nportfolio_size                  \n1               1793  2411  1809\n2               1821  2392  1807\n```\n:::\n:::\n\n\n**Interpretation**: Your check showed roughly **1,800 stocks** per bin. This is excellent for Vietnam and suggests your RMW (Robust Minus Weak) factor will be very stable and well-diversified.\n\nNow, we want to construct each of the factors, but this time, the size factor actually comes last because it is the result of averaging across all other factor portfolios. This dependency is the reason why we keep the table with value-weighted portfolio returns as a separate object that we reuse later. We construct the value factor, HML, as above by going long the two portfolios with high book-to-market ratios and shorting the two portfolios with low book-to-market.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# --- Merge with FULL monthly return data ---\n# This step is crucial. It adds 'date', 'ret_excess', and 'mktcap_lag'\n# which are missing from your current 'portfolios' object.\nportfolios_full = (prices_monthly\n  .assign(\n    sorting_date=lambda x: pd.to_datetime(\n      np.where(x[\"date\"].dt.month <= 6, \n               (x[\"date\"].dt.year - 1).astype(str) + \"0701\", \n               x[\"date\"].dt.year.astype(str) + \"0701\")\n    )\n  )\n  .merge(portfolios_sorting, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n)\n\n# --- Construct the Value Factor (HML) ---\nportfolios_value = (portfolios_full\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], group_keys=False)\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_value = (portfolios_value\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"hml\": (\n      x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_bm\"] == 1].mean())})\n  , include_groups=False)\n  .reset_index()\n)\n```\n:::\n\n\nFor the profitability factor, RMW (robust-minus-weak), we take a long position in the two high profitability portfolios and a short position in the two low profitability portfolios.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# --- Construct the Profitability Factor (RMW) ---\nportfolios_profitability = (portfolios_full\n  .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], group_keys=False)\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_profitability = (portfolios_profitability\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"rmw\": (\n      x[\"ret\"][x[\"portfolio_op\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_op\"] == 1].mean())})\n  , include_groups=False)\n  .reset_index()\n)\n```\n:::\n\n\nFor the investment factor, CMA (conservative-minus-aggressive), we go long the two low investment portfolios and short the two high investment portfolios.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# --- 5. Construct the Investment Factor (CMA) ---\nportfolios_investment = (portfolios_full\n  .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], group_keys=False)\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_investment = (portfolios_investment\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"cma\": (\n      x[\"ret\"][x[\"portfolio_inv\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_inv\"] == 3].mean())})\n  , include_groups=False)\n  .reset_index()\n)\n```\n:::\n\n\nFinally, the size factor, SMB, is constructed by going long the nine small portfolios and short the nine large portfolios.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# --- Construct the Size Factor (SMB) ---\nfactors_size = (\n  pd.concat(\n    [portfolios_value, portfolios_profitability, portfolios_investment], \n    ignore_index=True\n  )\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"smb\": (\n      x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_size\"] == 2].mean())})\n  , include_groups=False)\n  .reset_index()\n)\n```\n:::\n\n\nThe market factor ($Mkt - RF$) is defined as the value-weighted return of all stocks in the investable universe minus the risk-free rate. Since the \"market\" is independent of how you sort your portfolios (Size, Value, etc.), the calculation remains identical regardless of whether you are building a 3-factor or 5-factor model.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# --- Calculate Market Factor independently ---\n# This uses the entire prices_monthly universe\nfactor_market_excess = (prices_monthly\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n      \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    }), include_groups=False)\n  .reset_index()\n)\n```\n:::\n\n\nWe then join all factors together into one dataframe and construct again a suitable table to run tests for evaluating our replication.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nfactors = (factors_size\n  .merge(factors_value, how=\"outer\", on=\"date\")\n  .merge(factors_profitability, how=\"outer\", on=\"date\")\n  .merge(factors_investment, how=\"outer\", on=\"date\")\n  .merge(factor_market_excess, how=\"outer\", on=\"date\")\n  \n)\n\n# Check correlations\nprint(\"Factor Correlation Matrix (Vietnam):\")\nprint(factors.drop(columns=\"date\").corr())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Correlation Matrix (Vietnam):\n                 smb       hml       rmw       cma  mkt_excess\nsmb         1.000000 -0.304091  0.063569 -0.213636   -0.290988\nhml        -0.304091  1.000000 -0.230034  0.177006    0.133358\nrmw         0.063569 -0.230034  1.000000 -0.290426    0.050417\ncma        -0.213636  0.177006 -0.290426  1.000000   -0.015705\nmkt_excess -0.290988  0.133358  0.050417 -0.015705    1.000000\n```\n:::\n:::\n\n\nInterpretation: In standard markets, we expect low correlations between factors. If HML and RMW are correlated above 0.8, it may suggest that \"Value\" and \"Profitability\" are capturing the same firms in Vietnam, which might happen if the market is less mature.\n\n1. Factor Volatility and Means\nFactors should have a low but generally positive mean return over long periods.\n\n::: {#factor-summary .cell execution_count=22}\n``` {.python .cell-code}\n# Check the average monthly premium of each factor\nprint(\"Average Monthly Factor Premiums:\")\nprint(factors.drop(columns=\"date\").mean() * 100) # In percent\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage Monthly Factor Premiums:\nsmb          -0.031349\nhml          -0.080482\nrmw           0.001131\ncma           0.023623\nmkt_excess   -0.203141\ndtype: float64\n```\n:::\n:::\n\n\nIf a factor mean is extremely high (e.g., > 5% per month), it may indicate that a few outliers (like that 272 BM stock) are still leaking into the weighted averages.\n\n2. Visual Persistence Check\n\nCumulative returns should show the \"growth\" of $1 invested in the factor.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nfactors_cum = (factors.set_index(\"date\")\n  .add(1).cumprod()\n)\nfactors_cum.plot(figsize=(10, 6), title=\"Cumulative Factor Growth (Vietnam)\")\nplt.ylabel(\"Value of $1 Investment\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](12_fama_french_files/figure-pdf/cumulative-plot-output-1.pdf){#cumulative-plot fig-pos='H'}\n:::\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\n# Remove rows with missing factor values\n# We keep only rows where the characteristic factors are fully populated\nfactors_ff5_monthly = (factors\n    .dropna(subset=[\"smb\", \"hml\", \"rmw\", \"cma\", \"mkt_excess\"])\n    .reset_index(drop=True)\n)\n\n# Sanity Check\nprint(f\"Factors cleaned. Sample period: {factors_ff5_monthly['date'].min().date()} to {factors_ff5_monthly['date'].max().date()}\")\nprint(\"\\nFirst 3 rows of cleaned factors:\")\nprint(factors_ff5_monthly.head(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactors cleaned. Sample period: 2011-07-31 to 2023-12-31\n\nFirst 3 rows of cleaned factors:\n        date       smb       hml       rmw       cma  mkt_excess\n0 2011-07-31  0.008933 -0.013099  0.012139 -0.009529   -0.011287\n1 2011-08-31  0.004830 -0.016656  0.014516 -0.003981    0.007856\n2 2011-09-30  0.004970 -0.000462  0.008899  0.001241   -0.006501\n```\n:::\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n(factors_ff5_monthly\n  .to_sql(name=\"factors_ff5_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n150\n```\n:::\n:::\n\n\n## Key Takeaways\n\n-   The three-factor model adds size (SMB) and value (HML) to the traditional CAPM, while the five-factor model extends this with profitability (RMW) and investment (CMA) factors.\n-   The portfolio construction follows the original Fama-French methodology, including NYSE breakpoints, specific time lags, and sorting rules based on firm characteristics.\n-   The quality of replication can be evaluated using regression analysis and confirms strong alignment with the original Fama-French data.\n\n",
    "supporting": [
      "12_fama_french_files/figure-pdf"
    ],
    "filters": []
  }
}