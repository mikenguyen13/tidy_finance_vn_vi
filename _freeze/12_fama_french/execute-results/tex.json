{
  "hash": "f5b94f395c2252ad78b03141e021a7ef",
  "result": {
    "engine": "jupyter",
    "markdown": "# Fama-French Factors\n\nThis chapter provides a replication of the Fama-French factor portfolios for the Vietnamese stock market. The Fama-French factor models represent a cornerstone of empirical asset pricing, originating from the seminal work of @Fama1992 and later extended in @FamaFrench2015. These models have transformed how academics and practitioners understand the cross-section of expected stock returns, moving beyond the single-factor Capital Asset Pricing Model to incorporate multiple sources of systematic risk.\n\nWe construct both the three-factor and five-factor models at monthly and daily frequencies. The monthly factors serve as the foundation for most asset pricing tests and portfolio analyses, while the daily factors enable higher-frequency applications including short-horizon event studies, market microstructure research, and daily beta estimation. By constructing factors at both frequencies, we create a complete toolkit for empirical finance research in the Vietnamese market.\n\nThe chapter proceeds as follows. We first discuss the theoretical motivation for each factor and the economic intuition behind the Fama-French methodology. We then prepare the necessary data, merging stock returns with accounting characteristics. Next, we implement the portfolio sorting procedures that form the basis of factor construction, carefully following the original Fama-French protocols while adapting them for Vietnamese market characteristics. We construct the three-factor model (market, size, and value) before extending to the five-factor model (adding profitability and investment). Finally, we construct daily factors and validate our replicated factors through various diagnostic checks.\n\n## Theoretical Background\n\n### The Evolution from CAPM to Multi-Factor Models\n\nThe Capital Asset Pricing Model of @Sharpe1964 posits that a single factor—the market portfolio—should explain all cross-sectional variation in expected returns. However, decades of empirical research have documented persistent patterns that CAPM cannot explain. @Fama1992 demonstrated that two firm characteristics—size and book-to-market ratio—capture substantial variation in average returns that the market beta leaves unexplained.\n\nSmall firms tend to earn higher returns than large firms, a pattern known as the size effect. Similarly, firms with high book-to-market ratios (value stocks) tend to outperform firms with low book-to-market ratios (growth stocks), known as the value premium. The three-factor model formalizes these observations by constructing tradeable factor portfolios:\n\n$$\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i^{MKT}(r_{m,t} - r_{f,t}) + \\beta_i^{SMB} \\cdot SMB_t + \\beta_i^{HML} \\cdot HML_t + \\varepsilon_{i,t}\n$$ {#eq-ff3}\n\nwhere:\n\n- $r_{i,t} - r_{f,t}$ is the excess return on asset $i$\n- $r_{m,t} - r_{f,t}$ is the market excess return\n- $SMB_t$ (Small Minus Big) is the size factor\n- $HML_t$ (High Minus Low) is the value factor\n\n### The Five-Factor Extension\n\n@FamaFrench2015 extended the model to include two additional factors motivated by the dividend discount model. Firms with higher profitability should have higher expected returns (all else equal), and firms with aggressive investment policies should have lower expected returns:\n\n$$\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i^{MKT}MKT_t + \\beta_i^{SMB}SMB_t + \\beta_i^{HML}HML_t + \\beta_i^{RMW}RMW_t + \\beta_i^{CMA}CMA_t + \\varepsilon_{i,t}\n$$ {#eq-ff5}\n\nwhere:\n\n- $RMW_t$ (Robust Minus Weak) is the profitability factor\n- $CMA_t$ (Conservative Minus Aggressive) is the investment factor\n\n### Factor Construction Methodology\n\nThe Fama-French methodology constructs factors through double-sorted portfolios:\n\n1. **Size sorts**: Stocks are divided into Small and Big groups based on median market capitalization.\n\n2. **Characteristic sorts**: Within each size group, stocks are sorted into terciles based on book-to-market (for HML), operating profitability (for RMW), or investment (for CMA).\n\n3. **Factor returns**: Factors are computed as the difference between average returns of portfolios with high versus low characteristic values, averaging across size groups to neutralize size effects.\n\n4. **Timing**: Portfolios are formed at the end of June each year using accounting data from the prior fiscal year, ensuring all information was publicly available at formation.\n\n## Setting Up the Environment\n\nWe load the required Python packages for data manipulation, statistical analysis, and visualization.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nfrom scipy.stats.mstats import winsorize\nimport matplotlib.pyplot as plt\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\n```\n:::\n\n\nWe connect to our SQLite database containing the processed Vietnamese financial data.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n```\n:::\n\n\n## Data Preparation\n\n### Loading Stock Returns\n\nWe load the monthly stock returns data, which includes excess returns, market capitalization, and the risk-free rate. These variables are essential for computing value-weighted portfolio returns and factor premiums.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprint(f\"Monthly returns: {len(prices_monthly):,} observations\")\nprint(f\"Unique stocks: {prices_monthly['symbol'].nunique():,}\")\nprint(f\"Date range: {prices_monthly['date'].min():%Y-%m} to {prices_monthly['date'].max():%Y-%m}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMonthly returns: 165,499 observations\nUnique stocks: 1,457\nDate range: 2010-02 to 2023-12\n```\n:::\n:::\n\n\n### Loading Company Fundamentals\n\nWe load the company fundamentals data containing book equity, operating profitability, and investment—the characteristics needed for constructing the Fama-French factors.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ncomp_vn = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, datadate, be, op, inv\n        FROM comp_vn\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n).dropna()\n\nprint(f\"Fundamentals: {len(comp_vn):,} firm-year observations\")\nprint(f\"Unique firms: {comp_vn['symbol'].nunique():,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFundamentals: 18,108 firm-year observations\nUnique firms: 1,496\n```\n:::\n:::\n\n\n### Constructing Sorting Variables\n\nFollowing Fama-French conventions, we construct the sorting variables with careful attention to timing. The key principles are:\n\n1. **Size (June Market Cap)**: We use market capitalization at the end of June of year $t$ to sort stocks into size groups. This ensures we capture the firm's size at the moment of portfolio formation.\n\n2. **Book-to-Market Ratio**: We use book equity from fiscal year $t-1$ divided by market equity at the end of December $t-1$. This creates a six-month gap between the accounting data and portfolio formation, ensuring the information was publicly available.\n\n3. **Portfolio Formation Date**: Portfolios are formed on July 1st and held for twelve months until the following June.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef construct_sorting_variables(prices_monthly, comp_vn):\n    \"\"\"\n    Construct sorting variables following Fama-French methodology.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns with market cap\n    comp_vn : pd.DataFrame\n        Company fundamentals with book equity, profitability, investment\n        \n    Returns\n    -------\n    pd.DataFrame\n        Sorting variables aligned with July 1st formation dates\n    \"\"\"\n    \n    # 1. Size: June market capitalization\n    # Portfolio formation is July 1st, so we use June market cap\n    size = (prices_monthly\n        .query(\"date.dt.month == 6\")\n        .assign(\n            sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(1)\n        )\n        [[\"symbol\", \"sorting_date\", \"mktcap\"]]\n        .rename(columns={\"mktcap\": \"size\"})\n    )\n    \n    print(f\"Size observations: {len(size):,}\")\n    \n    # 2. Market Equity: December market cap for B/M calculation\n    # December t-1 market cap is used with fiscal year t-1 book equity\n    # This is then used for July t portfolio formation\n    market_equity = (prices_monthly\n        .query(\"date.dt.month == 12\")\n        .assign(\n            # December year t-1 maps to July year t formation\n            sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(7)\n        )\n        [[\"symbol\", \"sorting_date\", \"mktcap\"]]\n        .rename(columns={\"mktcap\": \"me\"})\n    )\n    \n    print(f\"Market equity observations: {len(market_equity):,}\")\n    \n    # 3. Book-to-Market and other characteristics\n    # Fiscal year t-1 data is used for July t portfolio formation\n    book_to_market = (comp_vn\n        .assign(\n            # Fiscal year-end + 6 months = July formation\n            sorting_date=lambda x: pd.to_datetime(\n                (x[\"datadate\"].dt.year + 1).astype(str) + \"-07-01\"\n            )\n        )\n        .merge(market_equity, on=[\"symbol\", \"sorting_date\"], how=\"inner\")\n        .assign(\n            # Scale book equity to match market equity units\n            # BE is in VND, ME is in millions VND\n            bm=lambda x: x[\"be\"] / (x[\"me\"] * 1e9)\n        )\n        [[\"symbol\", \"sorting_date\", \"me\", \"bm\", \"op\", \"inv\"]]\n    )\n    \n    print(f\"Book-to-market observations: {len(book_to_market):,}\")\n    \n    # 4. Merge size with characteristics\n    sorting_variables = (size\n        .merge(book_to_market, on=[\"symbol\", \"sorting_date\"], how=\"inner\")\n        .dropna()\n        .drop_duplicates(subset=[\"symbol\", \"sorting_date\"])\n    )\n    \n    return sorting_variables\n\nsorting_variables = construct_sorting_variables(prices_monthly, comp_vn)\n\nprint(f\"\\nFinal sorting variables: {len(sorting_variables):,} stock-years\")\nprint(f\"Sorting date range: {sorting_variables['sorting_date'].min():%Y-%m} to {sorting_variables['sorting_date'].max():%Y-%m}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSize observations: 13,756\nMarket equity observations: 14,286\nBook-to-market observations: 13,389\n\nFinal sorting variables: 12,046 stock-years\nSorting date range: 2011-07 to 2023-07\n```\n:::\n:::\n\n\n### Validating Sorting Variables\n\nBefore proceeding, we validate that our sorting variables have reasonable distributions. The book-to-market ratio should center around 1.0 for a typical market, though emerging markets may differ.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(\"Sorting Variable Summary Statistics:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\n# Check for extreme values that might indicate data issues\nprint(f\"\\nB/M Median: {sorting_variables['bm'].median():.4f}\")\nprint(f\"B/M 1st percentile: {sorting_variables['bm'].quantile(0.01):.4f}\")\nprint(f\"B/M 99th percentile: {sorting_variables['bm'].quantile(0.99):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSorting Variable Summary Statistics:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.7033      0.1852      0.1322\nstd     14680.4225      3.8683      0.2782      2.5410\nmin         0.4864      0.0014     -0.7529     -0.9569\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817    272.1893      1.4256    261.3355\n\nB/M Median: 1.1849\nB/M 1st percentile: 0.1710\nB/M 99th percentile: 8.0262\n```\n:::\n:::\n\n\n### Handling Outliers\n\nExtreme values in sorting characteristics can distort portfolio assignments and factor returns. We apply winsorization to limit the influence of outliers while preserving the general ranking of stocks.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Check BEFORE winsorization\nprint(\"BEFORE Winsorization:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\n# Apply winsorization\ndef winsorize_characteristics(df, columns, limits=(0.01, 0.99)):\n    \"\"\"\n    Apply winsorization using pandas clip.\n    \"\"\"\n    df = df.copy()\n    for col in columns:\n        if col in df.columns:\n            lower = df[col].quantile(limits[0])\n            upper = df[col].quantile(limits[1])\n            df[col] = df[col].clip(lower=lower, upper=upper)\n            print(f\"  {col}: clipped to [{lower:.4f}, {upper:.4f}]\")\n    return df\n\nsorting_variables = winsorize_characteristics(\n    sorting_variables,\n    columns=[\"bm\", \"op\", \"inv\"],  # Don't winsorize size\n    limits=(0.01, 0.99)\n)\n\n# Check AFTER winsorization\nprint(\"\\nAFTER Winsorization:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBEFORE Winsorization:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.7033      0.1852      0.1322\nstd     14680.4225      3.8683      0.2782      2.5410\nmin         0.4864      0.0014     -0.7529     -0.9569\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817    272.1893      1.4256    261.3355\n  bm: clipped to [0.1710, 8.0262]\n  op: clipped to [-0.7319, 1.2192]\n  inv: clipped to [-0.3990, 1.5195]\n\nAFTER Winsorization:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.5544      0.1837      0.0894\nstd     14680.4225      1.2843      0.2705      0.2721\nmin         0.4864      0.1710     -0.7319     -0.3990\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817      8.0262      1.2192      1.5195\n```\n:::\n:::\n\n\n## Portfolio Assignment Functions\n\n### The Portfolio Assignment Function\n\nWe create a flexible function for assigning stocks to portfolios based on quantile breakpoints. This function handles both independent sorts (where breakpoints are computed across all stocks) and dependent sorts (where breakpoints are computed within subgroups).\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef assign_portfolio(data, sorting_variable, percentiles):\n    \"\"\"Assign portfolios to a bin according to a sorting variable.\"\"\"\n    \n    # Get the values\n    values = data[sorting_variable].dropna()\n    \n    if len(values) == 0:\n        return pd.Series([np.nan] * len(data), index=data.index)\n    \n    # Calculate breakpoints\n    breakpoints = values.quantile(percentiles, interpolation=\"linear\")\n    \n    # Handle duplicate breakpoints by using unique values\n    unique_breakpoints = np.unique(breakpoints)\n    \n    # If all values are the same, assign all to portfolio 1\n    if len(unique_breakpoints) <= 1:\n        return pd.Series([1] * len(data), index=data.index)\n    \n    # Set boundaries to -inf and +inf\n    unique_breakpoints.iloc[0] = -np.inf\n    unique_breakpoints.iloc[unique_breakpoints.size-1] = np.inf\n    \n    # Assign to bins\n    assigned = pd.cut(\n        data[sorting_variable],\n        bins=unique_breakpoints,\n        labels=pd.Series(range(1, breakpoints.size)),\n        include_lowest=True,\n        right=False\n    )\n    \n    return assigned\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Check the distribution of characteristics BEFORE portfolio assignment\nprint(\"Operating Profitability Distribution:\")\nprint(sorting_variables[\"op\"].describe())\nprint(f\"\\nUnique OP values: {sorting_variables['op'].nunique()}\")\n\nprint(\"\\nInvestment Distribution:\")\nprint(sorting_variables[\"inv\"].describe())\nprint(f\"\\nUnique INV values: {sorting_variables['inv'].nunique()}\")\n\n# Check breakpoints for a specific date\ntest_date = sorting_variables[\"sorting_date\"].iloc[0]\ntest_data = sorting_variables.query(\"sorting_date == @test_date\")\n\nprint(f\"\\nBreakpoints for {test_date}:\")\nprint(f\"OP 30th percentile: {test_data['op'].quantile(0.3):.4f}\")\nprint(f\"OP 70th percentile: {test_data['op'].quantile(0.7):.4f}\")\nprint(f\"INV 30th percentile: {test_data['inv'].quantile(0.3):.4f}\")\nprint(f\"INV 70th percentile: {test_data['inv'].quantile(0.7):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOperating Profitability Distribution:\ncount    12046.000000\nmean         0.183738\nstd          0.270509\nmin         -0.731888\n25%          0.030913\n50%          0.136675\n75%          0.295185\nmax          1.219223\nName: op, dtype: float64\n\nUnique OP values: 11804\n\nInvestment Distribution:\ncount    12046.000000\nmean         0.089388\nstd          0.272147\nmin         -0.399042\n25%         -0.049497\n50%          0.034157\n75%          0.158155\nmax          1.519474\nName: inv, dtype: float64\n\nUnique INV values: 11805\n\nBreakpoints for 2019-07-01 00:00:00:\nOP 30th percentile: 0.0541\nOP 70th percentile: 0.2566\nINV 30th percentile: -0.0343\nINV 70th percentile: 0.1116\n```\n:::\n:::\n\n\n### Assigning Portfolios for Three-Factor Model\n\nFor the three-factor model, we perform independent double sorts on size and book-to-market. Size is split at the median (2 groups), and book-to-market is split at the 30th and 70th percentiles (3 groups), creating 6 portfolios.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef assign_ff3_portfolios(sorting_variables):\n    \"\"\"\n    Assign portfolios for Fama-French three-factor model.\n    Independent 2x3 sort on size and book-to-market.\n    \"\"\"\n    df = sorting_variables.copy()\n    \n    # Independent size sort (median split)\n    df[\"portfolio_size\"] = df.groupby(\"sorting_date\")[\"size\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.5, 1], labels=[1, 2], duplicates='drop')\n    )\n    \n    # Independent B/M sort (30/70 split)\n    df[\"portfolio_bm\"] = df.groupby(\"sorting_date\")[\"bm\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    return df\n\n# Assign portfolios\nportfolios_ff3 = assign_ff3_portfolios(sorting_variables)\n\n# Validate\nprint(\"FF3 Book-to-Market by Portfolio (should be INCREASING):\")\nprint(portfolios_ff3.groupby(\"portfolio_bm\", observed=True)[\"bm\"].median().round(4))\n\n\nprint(\"Three-Factor Portfolio Assignments:\")\nprint(portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]].head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFF3 Book-to-Market by Portfolio (should be INCREASING):\nportfolio_bm\n1    0.5836\n2    1.1891\n3    2.4552\nName: bm, dtype: float64\nThree-Factor Portfolio Assignments:\n  symbol sorting_date portfolio_size portfolio_bm\n0    A32   2019-07-01              1            2\n1    A32   2020-07-01              1            2\n2    A32   2021-07-01              1            2\n3    A32   2022-07-01              1            3\n4    A32   2023-07-01              1            2\n5    AAA   2011-07-01              2            2\n6    AAA   2012-07-01              2            3\n7    AAA   2013-07-01              2            3\n8    AAA   2014-07-01              2            2\n9    AAA   2015-07-01              2            3\n```\n:::\n:::\n\n\n### Validating Portfolio Assignments\n\nWe verify that the portfolio assignments create the expected 2×3 grid with reasonable stock counts in each cell.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Check portfolio distribution for most recent year\nlatest_date = portfolios_ff3[\"sorting_date\"].max()\n\nportfolio_counts = (portfolios_ff3\n    .query(\"sorting_date == @latest_date\")\n    .groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)\n    .size()\n    .unstack(fill_value=0)\n)\n\nprint(f\"Portfolio Counts for {latest_date:%Y-%m}:\")\nprint(portfolio_counts)\n\n# Verify characteristic monotonicity\nprint(\"\\nBook-to-Market by Portfolio (should be increasing):\")\nprint(portfolios_ff3.groupby(\"portfolio_bm\", observed=True)[\"bm\"].median().round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPortfolio Counts for 2023-07:\nportfolio_bm      1    2    3\nportfolio_size               \n1               113  271  263\n2               275  246  125\n\nBook-to-Market by Portfolio (should be increasing):\nportfolio_bm\n1    0.5836\n2    1.1891\n3    2.4552\nName: bm, dtype: float64\n```\n:::\n:::\n\n\nWe verify that for a single stock, the portfolio assignment remains constant between July of one year and June of the next.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Trace a single symbol (e.g., 'A32') across a formation window\npersistence_check = (portfolios_ff3\n    .query(\"symbol == 'A32' & sorting_date >= '2022-01-01' & sorting_date <= '2023-12-31'\")\n    .sort_values(\"sorting_date\")\n    [['symbol', 'sorting_date', 'portfolio_size', 'portfolio_bm']]\n)\nprint(\"\\nTemporal Persistence Check (Symbol A32):\")\nprint(persistence_check.head(15))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTemporal Persistence Check (Symbol A32):\n  symbol sorting_date portfolio_size portfolio_bm\n3    A32   2022-07-01              1            3\n4    A32   2023-07-01              1            2\n```\n:::\n:::\n\n\n## Fama-French Three-Factor Model (Monthly)\n\n### Merging Portfolios with Returns\n\nWe merge the portfolio assignments with monthly returns. The key insight is that portfolios formed in July of year $t$ are held through June of year $t+1$. We implement this by computing a `sorting_date` for each monthly return observation.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef merge_portfolios_with_returns(prices_monthly, portfolio_assignments):\n    \"\"\"\n    Merge portfolio assignments with monthly returns.\n    \n    Portfolios formed in July t are held through June t+1.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns\n    portfolio_assignments : pd.DataFrame\n        Portfolio assignments with sorting_date\n        \n    Returns\n    -------\n    pd.DataFrame\n        Returns merged with portfolio assignments\n    \"\"\"\n    portfolios = (prices_monthly\n        .assign(\n            # Map each return month to its portfolio formation date\n            sorting_date=lambda x: pd.to_datetime(\n                np.where(\n                    x[\"date\"].dt.month <= 6,\n                    (x[\"date\"].dt.year - 1).astype(str) + \"-07-01\",\n                    x[\"date\"].dt.year.astype(str) + \"-07-01\"\n                )\n            )\n        )\n        .merge(\n            portfolio_assignments,\n            on=[\"symbol\", \"sorting_date\"],\n            how=\"inner\"\n        )\n    )\n    \n    return portfolios\n\nportfolios_monthly_ff3 = merge_portfolios_with_returns(\n    prices_monthly,\n    portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]]\n)\n\n\nprint(f\"Merged observations: {len(portfolios_monthly_ff3):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMerged observations: 136,444\n```\n:::\n:::\n\n\n### Computing Value-Weighted Portfolio Returns\n\nWe compute value-weighted returns for each of the six portfolios. Value-weighting uses lagged market capitalization to avoid look-ahead bias.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ndef compute_portfolio_returns(data, grouping_vars):\n    \"\"\"\n    Compute value-weighted portfolio returns.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Returns data with portfolio assignments and mktcap_lag\n    grouping_vars : list\n        Variables defining portfolio groups\n        \n    Returns\n    -------\n    pd.DataFrame\n        Value-weighted returns for each portfolio-date\n    \"\"\"\n    portfolio_returns = (data\n        .groupby(grouping_vars + [\"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]),\n            \"n_stocks\": len(x)\n        }))\n        .reset_index()\n    )\n    \n    return portfolio_returns\n\n\n# Compute portfolio returns\nportfolio_returns_ff3 = compute_portfolio_returns(\n    portfolios_monthly_ff3,\n    [\"portfolio_size\", \"portfolio_bm\"]\n)\n\nprint(\"Portfolio Returns Summary:\")\nprint(portfolio_returns_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)[\"ret\"].describe().round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPortfolio Returns Summary:\n                             count    mean     std     min     25%     50%  \\\nportfolio_size portfolio_bm                                                  \n1              1             150.0 -0.0052  0.0379 -0.1268 -0.0262 -0.0058   \n               2             150.0 -0.0025  0.0414 -0.1080 -0.0236 -0.0053   \n               3             150.0  0.0039  0.0601 -0.1612 -0.0269 -0.0004   \n2              1             150.0 -0.0124  0.0594 -0.2222 -0.0449 -0.0107   \n               2             150.0 -0.0021  0.0671 -0.1701 -0.0403 -0.0046   \n               3             150.0  0.0024  0.0879 -0.2359 -0.0527 -0.0012   \n\n                                75%     max  \nportfolio_size portfolio_bm                  \n1              1             0.0161  0.0849  \n               2             0.0180  0.1195  \n               3             0.0314  0.2015  \n2              1             0.0210  0.1741  \n               2             0.0317  0.1770  \n               3             0.0437  0.2124  \n```\n:::\n:::\n\n\n### Constructing SMB and HML Factors\n\nWe now construct the SMB and HML factors from the portfolio returns.\n\n**SMB (Small Minus Big)**: Average return of three small portfolios minus average return of three big portfolios.\n\n**HML (High Minus Low)**: Average return of two high B/M portfolios minus average return of two low B/M portfolios.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef construct_ff3_factors(portfolio_returns):\n    \"\"\"\n    Construct Fama-French three factors from portfolio returns.\n    \n    Parameters\n    ----------\n    portfolio_returns : pd.DataFrame\n        Value-weighted returns for 2x3 portfolios\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly SMB and HML factors\n    \"\"\"\n    factors = (portfolio_returns\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            # SMB: Small minus Big (average across B/M groups)\n            \"smb\": (\n                x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean()\n            ),\n            # HML: High minus Low B/M (average across size groups)\n            \"hml\": (\n                x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean()\n            )\n        }))\n        .reset_index()\n    )\n    \n    return factors\n\nfactors_smb_hml = construct_ff3_factors(portfolio_returns_ff3)\n\nprint(\"SMB and HML Factors:\")\nprint(factors_smb_hml.head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSMB and HML Factors:\n        date       smb       hml\n0 2011-07-31 -0.007768  0.002754\n1 2011-08-31 -0.067309  0.011474\n2 2011-09-30  0.014884  0.022854\n3 2011-10-31 -0.003743  0.001631\n4 2011-11-30  0.063234  0.009103\n5 2011-12-31  0.014571  0.015280\n6 2012-01-31 -0.026080  0.009672\n7 2012-02-29 -0.035721  0.005474\n8 2012-03-31 -0.002344  0.032477\n9 2012-04-30 -0.033391  0.074191\n```\n:::\n:::\n\n\n### Computing the Market Factor\n\nThe market factor is the value-weighted return of all stocks minus the risk-free rate. We compute this independently from the sorted portfolios.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndef compute_market_factor(prices_monthly):\n    \"\"\"\n    Compute value-weighted market excess return.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns with mktcap_lag\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly market excess return\n    \"\"\"\n    market_factor = (prices_monthly\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]),\n            \"n_stocks\": len(x)\n        }), include_groups=False)\n        .reset_index()\n    )\n    \n    return market_factor\n\nmarket_factor = compute_market_factor(prices_monthly)\n\nprint(\"Market Factor Summary:\")\nprint(market_factor[\"mkt_excess\"].describe().round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarket Factor Summary:\ncount    167.0000\nmean      -0.0123\nstd        0.0595\nmin       -0.2149\n25%       -0.0394\n50%       -0.0106\n75%        0.0200\nmax        0.1677\nName: mkt_excess, dtype: float64\n```\n:::\n:::\n\n\n### Combining Three Factors\n\nWe combine SMB, HML, and the market factor into the complete three-factor dataset.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nfactors_ff3_monthly = (factors_smb_hml\n    .merge(market_factor[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"inner\")\n)\n\n# Add risk-free rate for completeness\nrf_monthly = (prices_monthly\n    .groupby(\"date\")[\"risk_free\"]\n    .first()\n    .reset_index()\n)\n\nfactors_ff3_monthly = factors_ff3_monthly.merge(rf_monthly, on=\"date\", how=\"left\")\n\nprint(\"Fama-French Three Factors (Monthly):\")\nprint(factors_ff3_monthly.head(10))\n\nprint(\"\\nFactor Summary Statistics:\")\nprint(factors_ff3_monthly[[\"mkt_excess\", \"smb\", \"hml\"]].describe().round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFama-French Three Factors (Monthly):\n        date       smb       hml  mkt_excess  risk_free\n0 2011-07-31 -0.007768  0.002754   -0.078748   0.003333\n1 2011-08-31 -0.067309  0.011474    0.029906   0.003333\n2 2011-09-30  0.014884  0.022854   -0.002173   0.003333\n3 2011-10-31 -0.003743  0.001631   -0.014005   0.003333\n4 2011-11-30  0.063234  0.009103   -0.179410   0.003333\n5 2011-12-31  0.014571  0.015280   -0.094802   0.003333\n6 2012-01-31 -0.026080  0.009672    0.081273   0.003333\n7 2012-02-29 -0.035721  0.005474    0.069655   0.003333\n8 2012-03-31 -0.002344  0.032477    0.029005   0.003333\n9 2012-04-30 -0.033391  0.074191    0.048791   0.003333\n\nFactor Summary Statistics:\n       mkt_excess       smb       hml\ncount    150.0000  150.0000  150.0000\nmean      -0.0101    0.0027    0.0120\nstd        0.0586    0.0420    0.0535\nmin       -0.2149   -0.1599   -0.1284\n25%       -0.0380   -0.0175   -0.0160\n50%       -0.0095    0.0070    0.0043\n75%        0.0214    0.0261    0.0340\nmax        0.1677    0.1175    0.1618\n```\n:::\n:::\n\n\n### Saving Three-Factor Data\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nfactors_ff3_monthly.to_sql(\n    name=\"factors_ff3_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Three-factor monthly data saved to database.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThree-factor monthly data saved to database.\n```\n:::\n:::\n\n\n## Fama-French Five-Factor Model (Monthly)\n\n### Portfolio Assignments with Dependent Sorts\n\nFor the five-factor model, we use dependent sorts: size is sorted independently, but profitability and investment are sorted within size groups. This controls for the correlation between size and these characteristics.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndef assign_ff5_portfolios(sorting_variables):\n    \"\"\"\n    Assign portfolios for Fama-French five-factor model.\n    \"\"\"\n    df = sorting_variables.copy()\n    \n    # Independent size sort\n    df[\"portfolio_size\"] = df.groupby(\"sorting_date\")[\"size\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.5, 1], labels=[1, 2], duplicates='drop')\n    )\n    \n    # Dependent sorts within size groups\n    df[\"portfolio_bm\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"bm\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    df[\"portfolio_op\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"op\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    df[\"portfolio_inv\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"inv\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    return df\n\n# Run\nportfolios_ff5 = assign_ff5_portfolios(sorting_variables)\n```\n:::\n\n\n### Validating Five-Factor Portfolios\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# Check characteristic monotonicity for each dimension\nprint(\"Profitability by Portfolio (should be increasing):\")\nprint(portfolios_ff5.groupby(\"portfolio_op\", observed=True)[\"op\"].median().round(4))\n\nprint(\"\\nInvestment by Portfolio (should be increasing):\")\nprint(portfolios_ff5.groupby(\"portfolio_inv\", observed=True)[\"inv\"].median().round(4))\n\n# Check portfolio counts\nprint(\"\\nStocks per Size/Profitability Bin:\")\nprint(portfolios_ff5.groupby([\"portfolio_size\", \"portfolio_op\"], observed=True).size().unstack(fill_value=0))\n\n# Check number of unique firms per year\n(portfolios_ff5\n .groupby(\"sorting_date\")[\"symbol\"]\n .nunique())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProfitability by Portfolio (should be increasing):\nportfolio_op\n1   -0.0053\n2    0.1366\n3    0.4098\nName: op, dtype: float64\n\nInvestment by Portfolio (should be increasing):\nportfolio_inv\n1   -0.1012\n2    0.0329\n3    0.2568\nName: inv, dtype: float64\n\nStocks per Size/Profitability Bin:\nportfolio_op       1     2     3\nportfolio_size                  \n1               1812  2403  1811\n2               1811  2401  1808\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\nsorting_date\n2011-07-01     556\n2012-07-01     632\n2013-07-01     643\n2014-07-01     650\n2015-07-01     669\n2016-07-01     737\n2017-07-01     842\n2018-07-01    1073\n2019-07-01    1183\n2020-07-01    1224\n2021-07-01    1258\n2022-07-01    1286\n2023-07-01    1293\nName: symbol, dtype: int64\n```\n:::\n:::\n\n\n### Merging and Computing Portfolio Returns\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# Merge with returns\nportfolios_monthly_ff5 = merge_portfolios_with_returns(\n    prices_monthly,\n    portfolios_ff5[[\"symbol\", \"sorting_date\", \"portfolio_size\", \n                    \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"]]\n)\n\nprint(f\"Five-factor merged observations: {len(portfolios_monthly_ff5):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFive-factor merged observations: 136,444\n```\n:::\n:::\n\n\n### Constructing All Five Factors\n\nWe construct each factor from the appropriate portfolio sorts.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ndef construct_ff5_factors(portfolios_monthly):\n    \"\"\"\n    Construct Fama-French five factors from portfolio data.\n    \n    Parameters\n    ----------\n    portfolios_monthly : pd.DataFrame\n        Monthly returns with all portfolio assignments\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly five-factor returns\n    \"\"\"\n    \n    # HML: Value factor from B/M sorts\n    portfolios_bm = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_hml = (portfolios_bm\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # RMW: Profitability factor from OP sorts\n    portfolios_op = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_rmw = (portfolios_op\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"rmw\": (x.loc[x[\"portfolio_op\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_op\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # CMA: Investment factor from INV sorts\n    # Note: CMA is Conservative minus Aggressive (low inv - high inv)\n    portfolios_inv = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_cma = (portfolios_inv\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"cma\": (x.loc[x[\"portfolio_inv\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_inv\"] == 3, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # SMB: Size factor (average across all characteristic portfolios)\n    all_portfolios = pd.concat([portfolios_bm, portfolios_op, portfolios_inv])\n    \n    factors_smb = (all_portfolios\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # Combine all factors\n    factors = (factors_smb\n        .merge(factors_hml, on=\"date\", how=\"outer\")\n        .merge(factors_rmw, on=\"date\", how=\"outer\")\n        .merge(factors_cma, on=\"date\", how=\"outer\")\n    )\n    \n    return factors\n\nfactors_ff5 = construct_ff5_factors(portfolios_monthly_ff5)\n\n# Add market factor\nfactors_ff5_monthly = (factors_ff5\n    .merge(market_factor[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"inner\")\n    .merge(rf_monthly, on=\"date\", how=\"left\")\n)\n\nprint(\"Fama-French Five Factors (Monthly):\")\nprint(factors_ff5_monthly.head(10))\n\nprint(\"\\nFactor Summary Statistics:\")\nprint(factors_ff5_monthly[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].describe().round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFama-French Five Factors (Monthly):\n        date       smb       hml       rmw       cma  mkt_excess  risk_free\n0 2011-07-31 -0.015907 -0.002812  0.060525  0.045291   -0.078748   0.003333\n1 2011-08-31 -0.061842  0.006189 -0.022700 -0.023177    0.029906   0.003333\n2 2011-09-30  0.014387  0.024301 -0.006005  0.003588   -0.002173   0.003333\n3 2011-10-31 -0.006958 -0.006940  0.026694  0.003649   -0.014005   0.003333\n4 2011-11-30  0.074369  0.015617 -0.058766  0.044214   -0.179410   0.003333\n5 2011-12-31  0.006687  0.022494  0.062655  0.052444   -0.094802   0.003333\n6 2012-01-31 -0.016254  0.010513 -0.042191 -0.067170    0.081273   0.003333\n7 2012-02-29 -0.026606  0.024465 -0.030849 -0.036383    0.069655   0.003333\n8 2012-03-31  0.005096  0.050930 -0.018441  0.043488    0.029005   0.003333\n9 2012-04-30  0.000712  0.058214 -0.061434  0.009233    0.048791   0.003333\n\nFactor Summary Statistics:\n       mkt_excess       smb       hml       rmw       cma\ncount    150.0000  150.0000  150.0000  150.0000  150.0000\nmean      -0.0101    0.0077    0.0115   -0.0047    0.0083\nstd        0.0586    0.0419    0.0518    0.0477    0.0335\nmin       -0.2149   -0.1522   -0.1283   -0.2126   -0.0814\n25%       -0.0380   -0.0137   -0.0126   -0.0308   -0.0131\n50%       -0.0095    0.0104    0.0046    0.0010    0.0067\n75%        0.0214    0.0316    0.0323    0.0178    0.0289\nmax        0.1677    0.1284    0.1510    0.1297    0.1331\n```\n:::\n:::\n\n\n### Factor Correlations\n\nWe examine correlations between factors, which should generally be low for the factors to capture distinct sources of risk.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nprint(\"Factor Correlation Matrix:\")\ncorrelation_matrix = factors_ff5_monthly[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].corr()\nprint(correlation_matrix.round(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFactor Correlation Matrix:\n            mkt_excess    smb    hml    rmw    cma\nmkt_excess       1.000 -0.712  0.230 -0.006 -0.104\nsmb             -0.712  1.000  0.256 -0.373  0.246\nhml              0.230  0.256  1.000 -0.694  0.479\nrmw             -0.006 -0.373 -0.694  1.000 -0.352\ncma             -0.104  0.246  0.479 -0.352  1.000\n```\n:::\n:::\n\n\n### Saving Five-Factor Data\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nfactors_ff5_monthly.to_sql(\n    name=\"factors_ff5_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Five-factor monthly data saved to database.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFive-factor monthly data saved to database.\n```\n:::\n:::\n\n\n## Daily Fama-French Factors\n\n### Motivation for Daily Factors\n\nDaily factors are essential for several applications:\n\n1. **Daily beta estimation**: CAPM regressions using daily data require daily market excess returns.\n2. **Event studies**: Measuring abnormal returns around corporate events requires daily factor adjustments.\n3. **High-frequency research**: Market microstructure studies need daily or intraday factor data.\n\nThe construction methodology mirrors the monthly approach, but we compute portfolio returns at daily frequency while maintaining the same annual portfolio formation dates.\n\n### Loading Daily Returns\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n# Load daily price data\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess\n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Daily returns: {len(prices_daily):,} observations\")\nprint(f\"Date range: {prices_daily['date'].min():%Y-%m-%d} to {prices_daily['date'].max():%Y-%m-%d}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaily returns: 3,462,157 observations\nDate range: 2010-01-05 to 2023-12-29\n```\n:::\n:::\n\n\n### Adding Market Cap for Daily Weighting\n\nFor value-weighted daily returns, we need market capitalization. We use the most recent monthly market cap as the weight for daily returns within that month.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# Get monthly market cap to use as weights for daily returns\nmktcap_monthly = (prices_monthly\n    [[\"symbol\", \"date\", \"mktcap_lag\"]]\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n)\n\n# Add year_month to daily data for merging\nprices_daily = (prices_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .merge(\n        mktcap_monthly[[\"symbol\", \"year_month\", \"mktcap_lag\"]],\n        on=[\"symbol\", \"year_month\"],\n        how=\"left\"\n    )\n    .dropna(subset=[\"ret_excess\", \"mktcap_lag\"])\n)\n\nprint(f\"Daily returns with weights: {len(prices_daily):,} observations\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaily returns with weights: 3,443,815 observations\n```\n:::\n:::\n\n\n### Merging Daily Returns with Portfolios\n\nWe use the same portfolio assignments (formed annually in July) for daily returns.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n# Step 1: Ensure portfolios_ff3 has correct format\nportfolios_ff3_clean = portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]].copy()\nportfolios_ff3_clean[\"sorting_date\"] = pd.to_datetime(portfolios_ff3_clean[\"sorting_date\"])\n\nprint(\"Portfolio sorting dates:\")\nprint(portfolios_ff3_clean[\"sorting_date\"].unique()[:5])\n\n# Step 2: Create sorting_date for daily data\nprices_daily_with_sort = prices_daily.copy()\nprices_daily_with_sort[\"sorting_date\"] = prices_daily_with_sort[\"date\"].apply(\n    lambda x: pd.Timestamp(f\"{x.year}-07-01\") if x.month > 6 else pd.Timestamp(f\"{x.year - 1}-07-01\")\n)\n\nprint(\"\\nDaily sorting dates:\")\nprint(prices_daily_with_sort[\"sorting_date\"].unique()[:5])\n\n# Step 3: Merge\nportfolios_daily_ff3 = prices_daily_with_sort.merge(\n    portfolios_ff3_clean,\n    on=[\"symbol\", \"sorting_date\"],\n    how=\"inner\"\n)\n\nprint(f\"\\nMerged daily observations: {len(portfolios_daily_ff3):,}\")\nprint(f\"Unique dates: {portfolios_daily_ff3['date'].nunique():,}\")\n\n# Step 4: Verify portfolio distribution\nprint(\"\\nPortfolio distribution in daily data:\")\nprint(portfolios_daily_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPortfolio sorting dates:\n<DatetimeArray>\n['2019-07-01 00:00:00', '2020-07-01 00:00:00', '2021-07-01 00:00:00',\n '2022-07-01 00:00:00', '2023-07-01 00:00:00']\nLength: 5, dtype: datetime64[us]\n\nDaily sorting dates:\n<DatetimeArray>\n['2018-07-01 00:00:00', '2019-07-01 00:00:00', '2020-07-01 00:00:00',\n '2021-07-01 00:00:00', '2022-07-01 00:00:00']\nLength: 5, dtype: datetime64[us]\n\nMerged daily observations: 2,843,570\nUnique dates: 3,126\n\nPortfolio distribution in daily data:\nportfolio_bm         1       2       3\nportfolio_size                        \n1               218040  585561  617327\n2               636152  552114  234376\n```\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n# Diagnostic: Check the daily portfolio merge\nprint(\"=\"*50)\nprint(\"DIAGNOSTIC: Daily Portfolio Merge\")\nprint(\"=\"*50)\n\nprint(f\"\\nDaily prices rows: {len(prices_daily):,}\")\nprint(f\"Daily FF3 portfolios rows: {len(portfolios_daily_ff3):,}\")\nprint(f\"Match rate: {len(portfolios_daily_ff3)/len(prices_daily)*100:.1f}%\")\n\n# Check portfolio distribution in daily data\nprint(\"\\nDaily portfolio distribution:\")\nprint(portfolios_daily_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\n# Check a specific date\ntest_date = portfolios_daily_ff3[\"date\"].iloc[1000]\nprint(f\"\\nSample date: {test_date}\")\nprint(portfolios_daily_ff3.query(\"date == @test_date\").groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n==================================================\nDIAGNOSTIC: Daily Portfolio Merge\n==================================================\n\nDaily prices rows: 3,443,815\nDaily FF3 portfolios rows: 2,843,570\nMatch rate: 82.6%\n\nDaily portfolio distribution:\nportfolio_bm         1       2       3\nportfolio_size                        \n1               218040  585561  617327\n2               636152  552114  234376\n\nSample date: 2023-06-28 00:00:00\nportfolio_bm      1    2    3\nportfolio_size               \n1                93  232  312\n2               291  280   67\n```\n:::\n:::\n\n\n### Computing Daily Three Factors\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ndef compute_daily_ff3_factors(portfolios_daily):\n    \"\"\"\n    Compute daily Fama-French three factors.\n    \n    Parameters\n    ----------\n    portfolios_daily : pd.DataFrame\n        Daily returns with portfolio assignments\n        \n    Returns\n    -------\n    pd.DataFrame\n        Daily SMB and HML factors\n    \"\"\"\n    # Compute daily portfolio returns\n    portfolio_returns = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    # Compute factors\n    factors = (portfolio_returns\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean()),\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    return factors\n\nfactors_daily_smb_hml = compute_daily_ff3_factors(portfolios_daily_ff3)\n\nprint(f\"Daily factor observations: {len(factors_daily_smb_hml):,}\")\nprint(factors_daily_smb_hml.head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaily factor observations: 3,126\n        date       smb       hml\n0 2011-07-01  0.008587  0.000967\n1 2011-07-04  0.005099 -0.001099\n2 2011-07-05 -0.009088  0.010152\n3 2011-07-06  0.004875 -0.003918\n4 2011-07-07 -0.011239 -0.000584\n5 2011-07-08  0.005636 -0.008003\n6 2011-07-11  0.003940  0.006172\n7 2011-07-12  0.003205  0.006543\n8 2011-07-13 -0.000097 -0.001134\n9 2011-07-14 -0.001248  0.001669\n```\n:::\n:::\n\n\n### Computing Daily Market Factor\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\ndef compute_daily_market_factor(prices_daily):\n    \"\"\"\n    Compute daily value-weighted market excess return.\n    \n    Parameters\n    ----------\n    prices_daily : pd.DataFrame\n        Daily returns with mktcap_lag\n        \n    Returns\n    -------\n    pd.DataFrame\n        Daily market excess return\n    \"\"\"\n    market_daily = (prices_daily\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    return market_daily\n\nmarket_factor_daily = compute_daily_market_factor(prices_daily)\n\nprint(f\"Daily market factor: {len(market_factor_daily):,} days\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaily market factor: 3,474 days\n```\n:::\n:::\n\n\n### Combining Daily Three Factors\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nfactors_ff3_daily = (factors_daily_smb_hml\n    .merge(market_factor_daily, on=\"date\", how=\"inner\")\n)\n\n# Add risk-free rate (use monthly rate / 21 as daily approximation, or load actual daily rate)\nfactors_ff3_daily[\"risk_free\"] = 0.04 / 252  # Approximate daily risk-free\n\nprint(\"Daily Fama-French Three Factors:\")\nprint(factors_ff3_daily.head(10))\n\nprint(\"\\nDaily Factor Summary Statistics:\")\nprint(factors_ff3_daily[[\"mkt_excess\", \"smb\", \"hml\"]].describe().round(6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaily Fama-French Three Factors:\n        date       smb       hml  mkt_excess  risk_free\n0 2011-07-01  0.008587  0.000967   -0.019862   0.000159\n1 2011-07-04  0.005099 -0.001099   -0.000633   0.000159\n2 2011-07-05 -0.009088  0.010152    0.013314   0.000159\n3 2011-07-06  0.004875 -0.003918   -0.008045   0.000159\n4 2011-07-07 -0.011239 -0.000584    0.003391   0.000159\n5 2011-07-08  0.005636 -0.008003    0.000218   0.000159\n6 2011-07-11  0.003940  0.006172   -0.013393   0.000159\n7 2011-07-12  0.003205  0.006543   -0.017505   0.000159\n8 2011-07-13 -0.000097 -0.001134    0.000767   0.000159\n9 2011-07-14 -0.001248  0.001669   -0.000695   0.000159\n\nDaily Factor Summary Statistics:\n        mkt_excess          smb          hml\ncount  3126.000000  3126.000000  3126.000000\nmean     -0.000479     0.000236     0.000594\nstd       0.011269     0.008488     0.008585\nmin      -0.070268    -0.032671    -0.039418\n25%      -0.005074    -0.004882    -0.003941\n50%       0.000350    -0.000106     0.000522\n75%       0.005531     0.004307     0.005233\nmax       0.043386     0.042686     0.083889\n```\n:::\n:::\n\n\n### Computing Daily Five Factors\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\n# Step 1: Clean portfolios\nportfolios_ff5_clean = portfolios_ff5[[\"symbol\", \"sorting_date\", \"portfolio_size\", \n                                        \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"]].copy()\nportfolios_ff5_clean[\"sorting_date\"] = pd.to_datetime(portfolios_ff5_clean[\"sorting_date\"])\n\n# Step 2: Merge with daily prices\nportfolios_daily_ff5 = prices_daily_with_sort.merge(\n    portfolios_ff5_clean,\n    on=[\"symbol\", \"sorting_date\"],\n    how=\"inner\"\n)\n\nprint(f\"FF5 Daily merged observations: {len(portfolios_daily_ff5):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFF5 Daily merged observations: 2,843,570\n```\n:::\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\ndef compute_daily_ff5_factors(portfolios_daily):\n    \"\"\"Compute daily Fama-French five factors.\"\"\"\n    \n    # HML from B/M sorts\n    portfolios_bm = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_hml = (portfolios_bm\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # RMW from OP sorts\n    portfolios_op = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_rmw = (portfolios_op\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"rmw\": (x.loc[x[\"portfolio_op\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_op\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # CMA from INV sorts (note: low minus high)\n    portfolios_inv = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_cma = (portfolios_inv\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"cma\": (x.loc[x[\"portfolio_inv\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_inv\"] == 3, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # SMB from all sorts\n    all_portfolios = pd.concat([portfolios_bm, portfolios_op, portfolios_inv])\n    \n    factors_smb = (all_portfolios\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # Combine\n    factors = (factors_smb\n        .merge(factors_hml, on=\"date\", how=\"outer\")\n        .merge(factors_rmw, on=\"date\", how=\"outer\")\n        .merge(factors_cma, on=\"date\", how=\"outer\")\n    )\n    \n    return factors\n\n# Compute daily FF5 factors\nfactors_daily_ff5 = compute_daily_ff5_factors(portfolios_daily_ff5)\n\n# Add market factor\nfactors_ff5_daily = (factors_daily_ff5\n    .merge(market_factor_daily, on=\"date\", how=\"inner\")\n)\nfactors_ff5_daily[\"risk_free\"] = 0.04 / 252\n\nprint(\"Daily Fama-French Five Factors:\")\nprint(factors_ff5_daily.head(10))\n\nprint(\"\\nDaily Five-Factor Summary Statistics:\")\nprint(factors_ff5_daily[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].describe().round(6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaily Fama-French Five Factors:\n        date       smb       hml       rmw       cma  mkt_excess  risk_free\n0 2011-07-01  0.006295  0.002515  0.013140  0.007680   -0.019862   0.000159\n1 2011-07-04  0.002880 -0.002875  0.006560 -0.004886   -0.000633   0.000159\n2 2011-07-05 -0.004260  0.009864 -0.012158 -0.004470    0.013314   0.000159\n3 2011-07-06  0.001544 -0.009847  0.012977  0.006286   -0.008045   0.000159\n4 2011-07-07 -0.009789 -0.003988 -0.000197 -0.006995    0.003391   0.000159\n5 2011-07-08  0.001537 -0.006700  0.010841 -0.007661    0.000218   0.000159\n6 2011-07-11  0.005396  0.004747  0.000655  0.013375   -0.013393   0.000159\n7 2011-07-12  0.004759  0.007367  0.001989  0.014669   -0.017505   0.000159\n8 2011-07-13 -0.000009  0.001110 -0.002052 -0.001633    0.000767   0.000159\n9 2011-07-14 -0.001668  0.002916 -0.005427  0.005388   -0.000695   0.000159\n\nDaily Five-Factor Summary Statistics:\n        mkt_excess          smb          hml          rmw          cma\ncount  3126.000000  3126.000000  3126.000000  3126.000000  3126.000000\nmean     -0.000479     0.000484     0.000549    -0.000136     0.000413\nstd       0.011269     0.008033     0.008312     0.008538     0.006756\nmin      -0.070268    -0.036283    -0.039155    -0.154013    -0.047698\n25%      -0.005074    -0.004122    -0.003681    -0.004212    -0.003364\n50%       0.000350     0.000105     0.000384     0.000030     0.000162\n75%       0.005531     0.004358     0.004819     0.004036     0.003800\nmax       0.043386     0.060307     0.086269     0.102001     0.089907\n```\n:::\n:::\n\n\n### Saving Daily Factors\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nfactors_ff3_daily.to_sql(\n    name=\"factors_ff3_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nfactors_ff5_daily.to_sql(\n    name=\"factors_ff5_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Daily factor data saved to database.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaily factor data saved to database.\n```\n:::\n:::\n\n\n## Factor Validation and Diagnostics\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\n# Verify all tables are in database\nprint(\"\\n\" + \"=\"*50)\nprint(\"DATABASE SUMMARY\")\nprint(\"=\"*50)\n\ntables = [\"factors_ff3_monthly\", \"factors_ff5_monthly\", \n          \"factors_ff3_daily\", \"factors_ff5_daily\"]\n\nfor table in tables:\n    df = pd.read_sql_query(f\"SELECT COUNT(*) as n FROM {table}\", con=tidy_finance)\n    print(f\"{table}: {df['n'].iloc[0]:,} observations\")\n\n# Correlation check: Monthly vs Daily (aggregated)\nprint(\"\\n\" + \"=\"*50)\nprint(\"MONTHLY VS DAILY CONSISTENCY CHECK\")\nprint(\"=\"*50)\n\nfactors_daily_agg = (factors_ff3_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .groupby(\"year_month\")[[\"mkt_excess\", \"smb\", \"hml\"]]\n    .sum()\n    .reset_index()\n)\n\nfactors_monthly_check = (factors_ff3_monthly\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n)\n\ncomparison = factors_monthly_check.merge(\n    factors_daily_agg, on=\"year_month\", suffixes=(\"_monthly\", \"_daily\")\n)\n\nfor factor in [\"mkt_excess\", \"smb\", \"hml\"]:\n    corr = comparison[f\"{factor}_monthly\"].corr(comparison[f\"{factor}_daily\"])\n    print(f\"{factor}: Monthly-Daily correlation = {corr:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n==================================================\nDATABASE SUMMARY\n==================================================\nfactors_ff3_monthly: 150 observations\nfactors_ff5_monthly: 150 observations\nfactors_ff3_daily: 3,126 observations\nfactors_ff5_daily: 3,126 observations\n\n==================================================\nMONTHLY VS DAILY CONSISTENCY CHECK\n==================================================\nmkt_excess: Monthly-Daily correlation = 0.9980\nsmb: Monthly-Daily correlation = 0.9953\nhml: Monthly-Daily correlation = 0.9936\n```\n:::\n:::\n\n\n### Cumulative Factor Returns\n\nWe visualize the cumulative performance of each factor to assess whether the factors generate meaningful premiums over time.\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\n# Compute cumulative returns\nfactors_cumulative = (factors_ff5_monthly\n    .set_index(\"date\")\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .add(1)\n    .cumprod()\n)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nfactors_cumulative.plot(ax=ax)\nax.set_title(\"Cumulative Factor Returns (Vietnam)\")\nax.set_xlabel(\"\")\nax.set_ylabel(\"Growth of $1\")\nax.legend(title=\"Factor\")\nax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Cumulative returns of Fama-French factors for the Vietnamese market. The figure shows the growth of $1 invested in each factor portfolio.](12_fama_french_files/figure-pdf/fig-cumulative-factors-output-1.pdf){#fig-cumulative-factors fig-align='center' fig-alt='Line chart showing cumulative factor returns over time.' fig-pos='H'}\n:::\n:::\n\n\n### Average Factor Premiums\n\nWe compute annualized average factor premiums and their statistical significance.\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\n# Annualized average returns (monthly returns * 12)\nfactor_premiums = (factors_ff5_monthly\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .mean() * 12 * 100  # Annualized percentage\n)\n\n# Standard errors\nfactor_se = (factors_ff5_monthly\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .std() / np.sqrt(len(factors_ff5_monthly)) * np.sqrt(12) * 100\n)\n\n# T-statistics\nfactor_tstat = factor_premiums / factor_se\n\nprint(\"Annualized Factor Premiums (%):\")\nprint(factor_premiums.round(2))\n\nprint(\"\\nT-Statistics:\")\nprint(factor_tstat.round(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnnualized Factor Premiums (%):\nmkt_excess   -12.09\nsmb            9.19\nhml           13.75\nrmw           -5.69\ncma            9.94\ndtype: float64\n\nT-Statistics:\nmkt_excess    -7.30\nsmb            7.76\nhml            9.38\nrmw           -4.22\ncma           10.49\ndtype: float64\n```\n:::\n:::\n\n\n### Comparing Monthly and Daily Factors\n\nWe verify consistency between monthly and daily factors by computing correlations.\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\n# Aggregate daily factors to monthly for comparison\nfactors_daily_monthly = (factors_ff5_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .groupby(\"year_month\")\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .sum()  # Sum daily returns to get monthly\n    .reset_index()\n)\n\n# Merge with actual monthly factors\ncomparison = (factors_ff5_monthly\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .merge(\n        factors_daily_monthly,\n        on=\"year_month\",\n        suffixes=(\"_monthly\", \"_daily\")\n    )\n)\n\n# Correlations\nfor factor in [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]:\n    corr = comparison[f\"{factor}_monthly\"].corr(comparison[f\"{factor}_daily\"])\n    print(f\"{factor}: Monthly-Daily correlation = {corr:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmkt_excess: Monthly-Daily correlation = 0.9980\nsmb: Monthly-Daily correlation = 0.9950\nhml: Monthly-Daily correlation = 0.9948\nrmw: Monthly-Daily correlation = 0.9929\ncma: Monthly-Daily correlation = 0.9884\n```\n:::\n:::\n\n\n## Key Takeaways\n\n1. **Factor Models Explained**: The Fama-French three-factor model adds size (SMB) and value (HML) factors to the CAPM, while the five-factor model further includes profitability (RMW) and investment (CMA) factors.\n\n2. **Construction Methodology**: Factors are constructed through double-sorted portfolios with careful attention to timing. Portfolios are formed in July using accounting data from the prior fiscal year to ensure information was publicly available.\n\n3. **Independent vs. Dependent Sorts**: The three-factor model uses independent sorts on size and book-to-market, creating a 2×3 grid. The five-factor model uses dependent sorts where characteristics are sorted within size groups.\n\n4. **Value-Weighted Returns**: Portfolio returns are computed using value-weighting with lagged market capitalization to avoid look-ahead bias.\n\n5. **Daily Factors**: Daily factors use the same annual portfolio assignments but compute returns at daily frequency, enabling higher-frequency applications like daily beta estimation.\n\n6. **Market Factor**: The market factor is computed independently as the value-weighted return of all stocks minus the risk-free rate.\n\n7. **Validation**: Factor quality can be assessed through characteristic monotonicity, portfolio diversification, cumulative returns, and consistency between daily and monthly frequencies.\n\n8. **Vietnamese Market Adaptation**: While following the original Fama-French methodology, we adapt for Vietnamese market characteristics including VAS accounting standards, reporting timelines, and currency units.\n\n",
    "supporting": [
      "12_fama_french_files/figure-pdf"
    ],
    "filters": []
  }
}