{
  "hash": "d19da34cce089f7c40e4f9274b104351",
  "result": {
    "engine": "jupyter",
    "markdown": "# Factor Zoo and Multiple Testing\n\n::: callout-note\nIn this chapter, we confront the multiple testing problem in empirical asset pricing. We replicate a broad set of published anomalies in the Vietnamese market, apply formal corrections for the number of tests conducted, estimate the proportion of false discoveries, and develop a framework for deciding which factors deserve credence.\n:::\n\nBy 2016, academic finance had documented over 300 variables that purportedly predict the cross-section of stock returns. @cochrane2011presidential memorably called this proliferation a \"zoo of factors.\" @harvey2016and cataloged over 300 published factors and argued that the standard significance threshold of $t > 2.0$ was far too low given the sheer number of hypotheses tested, either explicitly or implicitly, across decades of research. Their proposed threshold of $t > 3.0$ (and in some cases $t > 3.78$) would eliminate the majority of published anomalies.\n\nThe problem is not merely academic. Every researcher who sorts stocks on a new variable and finds $t > 2$ is implicitly testing a hypothesis that has already been tested hundreds of times with different variables. The probability that *at least one* of 300 independent tests produces $t > 2$ by chance (even when no true effect exists) is essentially 1.0. This is the multiple testing problem, and failing to account for it means that a substantial fraction of the \"factor zoo\" consists of false discoveries.\n\nFor Vietnamese equity research, the problem is arguably worse. The cross-section is smaller (600-800 stocks vs. 4,000+ in the U.S.), the time series is shorter (2008- vs. 1963-), and the data are noisier (illiquidity, zero-trading days, accounting irregularities). All of these amplify the risk of both Type I errors (finding something that isn't there) and Type II errors (missing something that is). This chapter provides the statistical tools to navigate this landscape rigorously.\n\n## The Scale of the Problem {#sec-factor-zoo-scale}\n\n### How Many Factors Exist?\n\n@harvey2016and counted over 300 factors published in top journals through 2012. @chen2022open constructed and made publicly available over 300 \"clearly significant\" anomalies from the U.S. literature. @hou2020replicating attempted to replicate 452 anomalies and found that 64% (with VW returns) failed to achieve $t > 1.96$ in their sample. @jensen2023there took a more optimistic view, finding that most factors replicate in direction if not always in magnitude.\n\nThe key insight is that the number of factors *tested* far exceeds the number *published*. For every published factor with $t > 2$, there may be dozens of unpublished tests with $t < 2$ that never saw print, which is the classic file drawer problem. The true number of tests conducted collectively by the profession is unknowable, but certainly in the thousands.\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code code-summary=\"Import libraries and configure environment\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.stats.multitest import multipletests\nfrom itertools import product\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n```\n:::\n\n\n::: {#data-load .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Load returns and characteristics panel\"}\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Monthly returns (survivorship-bias-free)\nmonthly = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'exchange', 'volume_avg_20d', 'turnover_value_avg_20d',\n        'n_zero_volume_days'\n    ]\n)\n\n# Pre-computed characteristic panel (from factor construction chapter)\ncharacteristics = client.get_characteristics_panel(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True\n)\n\n# Factor returns for spanning tests\nfactors_ff = client.get_factor_returns(\n    market='vietnam',\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    factors=['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml']\n)\n\nmonthly['month_end'] = pd.to_datetime(monthly['month_end'])\n\nprint(f\"Monthly returns: {len(monthly):,}\")\nprint(f\"Characteristics: {characteristics.shape}\")\nprint(f\"Available signals: {[c for c in characteristics.columns if c not in ['ticker', 'month_end']]}\")\n```\n:::\n\n\n## Building the Anomaly Zoo {#sec-factor-zoo-zoo}\n\n### Signal Definitions\n\nWe construct a comprehensive set of anomaly variables spanning the major categories from the literature. Each signal is lagged appropriately to avoid look-ahead bias (accounting signals use point-in-time alignment; price signals use the prior month's value).\n\n::: {#signal-construction .cell execution_count=3}\n``` {.python .cell-code code-summary=\"Construct the full set of anomaly signals\"}\n# Merge returns with characteristics\npanel = monthly.merge(characteristics, on=['ticker', 'month_end'], how='left')\npanel = panel.sort_values(['ticker', 'month_end'])\n\n# ============================================================\n# Category 1: VALUE (8 signals)\n# ============================================================\n# Already in characteristics panel from PIT merge:\n# bm, ep, cfp, sp, dp, ev_ebitda\n# Add earnings yield variants\nvalue_signals = ['bm', 'ep', 'cfp', 'sp', 'dp']\n\n# ============================================================\n# Category 2: SIZE (3 signals)\n# ============================================================\npanel['log_mcap'] = np.log(panel['market_cap'].clip(lower=1))\npanel['log_assets'] = np.log(panel['total_assets'].clip(lower=1))\npanel['log_revenue'] = np.log(panel['revenue'].clip(lower=1))\nsize_signals = ['log_mcap', 'log_assets', 'log_revenue']\n\n# ============================================================\n# Category 3: PROFITABILITY (8 signals)\n# ============================================================\n# GP/A (Novy-Marx 2013), OP (FF 2015), ROE, ROA, etc.\nprof_signals = ['gp_at', 'op', 'roe', 'roa', 'profit_margin',\n                 'asset_turnover', 'gross_margin', 'oper_margin']\n\n# ============================================================\n# Category 4: INVESTMENT / GROWTH (6 signals)\n# ============================================================\ninv_signals = ['asset_growth', 'capex_at', 'investment',\n                'sales_growth', 'equity_issuance', 'debt_issuance']\n\n# ============================================================\n# Category 5: MOMENTUM / REVERSAL (6 signals)\n# ============================================================\n# Compute from returns\nfor lag_start, lag_end, name in [\n    (2, 12, 'ret_12_2'), (2, 6, 'ret_6_2'), (7, 12, 'ret_12_7'),\n    (1, 1, 'ret_1'), (1, 3, 'ret_3_1')\n]:\n    if name not in panel.columns:\n        panel[name] = (\n            panel.groupby('ticker')['monthly_return']\n            .transform(lambda x: x.shift(lag_start).rolling(lag_end - lag_start + 1)\n                       .apply(lambda r: (1 + r).prod() - 1, raw=True))\n        )\n# Earnings momentum (SUE) should already be in characteristics\nmom_signals = ['ret_12_2', 'ret_6_2', 'ret_12_7', 'ret_1', 'ret_3_1', 'sue']\n\n# ============================================================\n# Category 6: RISK (5 signals)\n# ============================================================\nrisk_signals = ['beta', 'ivol', 'tvol', 'max_ret', 'skewness']\n\n# ============================================================\n# Category 7: LIQUIDITY / TRADING (6 signals)\n# ============================================================\nliq_signals = ['amihud', 'zero_return_pct', 'log_turnover',\n                'roll_spread', 'cs_spread', 'volume_cv']\n\n# ============================================================\n# Category 8: QUALITY / FINANCIAL HEALTH (5 signals)\n# ============================================================\nqual_signals = ['leverage', 'current_ratio', 'accruals',\n                 'earnings_variability', 'piotroski_f']\n\n# ============================================================\n# Category 9: DIVIDEND / PAYOUT (3 signals)\n# ============================================================\ndiv_signals = ['div_yield', 'payout_ratio', 'net_repurchase']\n\n# Combine all\nall_signals = (value_signals + size_signals + prof_signals +\n               inv_signals + mom_signals + risk_signals +\n               liq_signals + qual_signals + div_signals)\n\n# Remove signals with too little coverage\nsignal_coverage = {}\nfor sig in all_signals:\n    if sig in panel.columns:\n        cov = panel[sig].notna().mean()\n        signal_coverage[sig] = cov\n\nsignal_coverage = pd.Series(signal_coverage).sort_values(ascending=False)\nvalid_signals = signal_coverage[signal_coverage > 0.30].index.tolist()\n\nprint(f\"Total signals defined: {len(all_signals)}\")\nprint(f\"Signals with >30% coverage: {len(valid_signals)}\")\nprint(f\"\\nSignal categories:\")\nfor cat_name, sigs in [\n    ('Value', value_signals), ('Size', size_signals),\n    ('Profitability', prof_signals), ('Investment', inv_signals),\n    ('Momentum', mom_signals), ('Risk', risk_signals),\n    ('Liquidity', liq_signals), ('Quality', qual_signals),\n    ('Dividend', div_signals)\n]:\n    n_valid = sum(1 for s in sigs if s in valid_signals)\n    print(f\"  {cat_name:<15}: {n_valid}/{len(sigs)}\")\n```\n:::\n\n\n### Mass Factor Construction\n\nWe run the factor construction engine (from the previous chapter) across all valid signals to produce a \"zoo\" of factor returns:\n\n::: {#mass-construction .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Construct long-short portfolios for every valid signal\"}\n# Import the factor engine from previous chapter\n# (In practice, this would be imported from a shared module)\n\ndef construct_factor_simple(panel_df, signal_col, long_high=True,\n                              n_groups=5, weighting='value',\n                              min_stocks=20):\n    \"\"\"\n    Simplified factor construction: quintile sort on signal,\n    long-short = Q5 - Q1 (or reversed if long_high=False).\n    Returns a Series of monthly long-short returns.\n    \"\"\"\n    df = panel_df[['ticker', 'month_end', 'monthly_return',\n                    'market_cap', signal_col]].dropna().copy()\n    df = df.sort_values(['ticker', 'month_end'])\n    \n    # Lag the signal by one month\n    df['signal_lag'] = df.groupby('ticker')[signal_col].shift(1)\n    df = df.dropna(subset=['signal_lag', 'monthly_return'])\n    \n    results = []\n    for month, group in df.groupby('month_end'):\n        if len(group) < min_stocks:\n            continue\n        \n        group['quintile'] = pd.qcut(\n            group['signal_lag'].rank(method='first'),\n            n_groups, labels=False, duplicates='drop'\n        )\n        \n        if weighting == 'value':\n            def wret(g):\n                if g['market_cap'].sum() > 0:\n                    return np.average(g['monthly_return'],\n                                       weights=g['market_cap'])\n                return g['monthly_return'].mean()\n            port_ret = group.groupby('quintile').apply(wret)\n        else:\n            port_ret = group.groupby('quintile')['monthly_return'].mean()\n        \n        if 0 in port_ret.index and (n_groups - 1) in port_ret.index:\n            if long_high:\n                ls = port_ret[n_groups - 1] - port_ret[0]\n            else:\n                ls = port_ret[0] - port_ret[n_groups - 1]\n            results.append({'month_end': month, 'ls_return': ls})\n    \n    if not results:\n        return None\n    return pd.DataFrame(results).set_index('month_end')['ls_return']\n\n# Determine expected sign for each signal\n# (positive = high signal predicts high returns)\nsignal_directions = {\n    # Value: high = higher returns\n    'bm': True, 'ep': True, 'cfp': True, 'sp': True, 'dp': True,\n    # Size: small = higher returns (low signal = high returns)\n    'log_mcap': False, 'log_assets': False, 'log_revenue': False,\n    # Profitability: high = higher returns\n    'gp_at': True, 'op': True, 'roe': True, 'roa': True,\n    'profit_margin': True, 'asset_turnover': True,\n    'gross_margin': True, 'oper_margin': True,\n    # Investment: low growth = higher returns\n    'asset_growth': False, 'capex_at': False, 'investment': False,\n    'sales_growth': False, 'equity_issuance': False,\n    'debt_issuance': False,\n    # Momentum: high = higher returns (except reversal)\n    'ret_12_2': True, 'ret_6_2': True, 'ret_12_7': True,\n    'ret_1': False, 'ret_3_1': False, 'sue': True,\n    # Risk: low risk = higher returns (anomaly direction)\n    'beta': False, 'ivol': False, 'tvol': False,\n    'max_ret': False, 'skewness': False,\n    # Liquidity: illiquid = higher returns\n    'amihud': True, 'zero_return_pct': True,\n    'log_turnover': False, 'roll_spread': True,\n    'cs_spread': True, 'volume_cv': True,\n    # Quality: high quality = higher returns\n    'leverage': False, 'current_ratio': True, 'accruals': False,\n    'earnings_variability': False, 'piotroski_f': True,\n    # Dividend: high = higher returns\n    'div_yield': True, 'payout_ratio': True, 'net_repurchase': False,\n}\n\n# Construct all factors\nzoo_results = {}\nfor sig in valid_signals:\n    long_high = signal_directions.get(sig, True)\n    ls = construct_factor_simple(\n        panel, sig, long_high=long_high,\n        n_groups=5, weighting='value'\n    )\n    if ls is not None and len(ls) >= 60:\n        mean_ret = ls.mean()\n        se = ls.std() / np.sqrt(len(ls))\n        t = mean_ret / se if se > 0 else 0\n        \n        zoo_results[sig] = {\n            'mean_monthly': mean_ret,\n            'ann_return': mean_ret * 12,\n            'ann_vol': ls.std() * np.sqrt(12),\n            'sharpe': mean_ret / ls.std() * np.sqrt(12) if ls.std() > 0 else 0,\n            't_stat': t,\n            'abs_t': abs(t),\n            'p_value': 2 * (1 - stats.t.cdf(abs(t), df=len(ls) - 1)),\n            'n_months': len(ls),\n            'direction': 'Long High' if long_high else 'Long Low',\n            'category': next(\n                (cat for cat, sigs in [\n                    ('Value', value_signals), ('Size', size_signals),\n                    ('Profitability', prof_signals),\n                    ('Investment', inv_signals),\n                    ('Momentum', mom_signals), ('Risk', risk_signals),\n                    ('Liquidity', liq_signals),\n                    ('Quality', qual_signals),\n                    ('Dividend', div_signals)\n                ] if sig in sigs), 'Other'\n            ),\n            'returns': ls  # Store for later use\n        }\n\nzoo_df = pd.DataFrame({k: {kk: vv for kk, vv in v.items() if kk != 'returns'}\n                         for k, v in zoo_results.items()}).T\nzoo_df = zoo_df.sort_values('abs_t', ascending=False)\n\nprint(f\"Factors successfully constructed: {len(zoo_df)}\")\nprint(f\"Factors with |t| > 2.0: {(zoo_df['abs_t'] > 2.0).sum()} \"\n      f\"({(zoo_df['abs_t'] > 2.0).mean():.1%})\")\nprint(f\"Factors with |t| > 3.0: {(zoo_df['abs_t'] > 3.0).sum()} \"\n      f\"({(zoo_df['abs_t'] > 3.0).mean():.1%})\")\n```\n:::\n\n\n::: {#fig-t-stat-distribution .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Visualize the distribution of t-statistics across all anomalies\"}\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Panel A: Histogram\nt_vals = zoo_df['t_stat'].values\naxes[0].hist(t_vals, bins=30, color='#2C5F8A', alpha=0.7,\n             edgecolor='white', density=True)\n\n# Overlay normal distribution under null\nx_range = np.linspace(-5, 5, 200)\naxes[0].plot(x_range, stats.norm.pdf(x_range), 'k--', linewidth=1.5,\n             label='N(0,1) null')\n\n# Threshold lines\naxes[0].axvline(x=1.96, color='#E67E22', linestyle='--', linewidth=1.5,\n                label='|t| = 1.96')\naxes[0].axvline(x=-1.96, color='#E67E22', linestyle='--', linewidth=1.5)\naxes[0].axvline(x=3.0, color='#C0392B', linestyle='--', linewidth=1.5,\n                label='|t| = 3.0 (HLZ)')\naxes[0].axvline(x=-3.0, color='#C0392B', linestyle='--', linewidth=1.5)\naxes[0].set_xlabel('t-statistic')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Panel A: t-Statistic Distribution')\naxes[0].legend(fontsize=9)\n\n# Panel B: Ranked bar chart\ntop_n = min(40, len(zoo_df))\ntop = zoo_df.head(top_n).copy()\n\ncategory_colors = {\n    'Value': '#2C5F8A', 'Size': '#1ABC9C', 'Profitability': '#27AE60',\n    'Investment': '#E67E22', 'Momentum': '#C0392B', 'Risk': '#8E44AD',\n    'Liquidity': '#3498DB', 'Quality': '#F1C40F', 'Dividend': '#95A5A6'\n}\nbar_colors = [category_colors.get(top.iloc[i]['category'], '#BDC3C7')\n              for i in range(len(top))]\n\naxes[1].barh(range(top_n), top['abs_t'].values,\n             color=bar_colors, alpha=0.85, edgecolor='white')\naxes[1].set_yticks(range(top_n))\naxes[1].set_yticklabels(top.index, fontsize=7)\naxes[1].axvline(x=1.96, color='#E67E22', linestyle='--', linewidth=1)\naxes[1].axvline(x=3.0, color='#C0392B', linestyle='--', linewidth=1)\naxes[1].set_xlabel('|t-statistic|')\naxes[1].set_title('Panel B: Anomalies Ranked by |t|')\naxes[1].invert_yaxis()\n\n# Legend for categories\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=c, label=cat)\n                    for cat, c in category_colors.items()]\naxes[1].legend(handles=legend_elements, fontsize=7,\n                loc='lower right', ncol=2)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## The Multiple Testing Problem {#sec-factor-zoo-multiple-testing}\n\n### Why Single-Test p-Values Are Misleading\n\nSuppose you test $M$ independent hypotheses, each at significance level $\\alpha = 0.05$. Under the global null (no true effects), the expected number of false rejections is $M \\times \\alpha$. For $M = 50$ anomalies, you expect 2.5 \"significant\" results by pure chance.\n\nThe probability of *at least one* false rejection is:\n\n$$\nP(\\text{at least one false discovery}) = 1 - (1 - \\alpha)^M\n$$ {#eq-family-error}\n\nFor $M = 50$ and $\\alpha = 0.05$, this is $1 - 0.95^{50} = 0.923$. For $M = 300$, it is essentially 1. The single-test p-value is useless for deciding which of many tested factors are genuine.\n\n### Two Error Rate Concepts\n\nMultiple testing corrections control one of two error rates:\n\n**Family-Wise Error Rate (FWER).** The probability of making *at least one* Type I error across all tests. This is the most conservative criterion. If FWER = 0.05, you can be 95% confident that *every* rejected null is a true discovery. Methods: @bonferroni1936teoria correction, Holm step-down, @romano2005exact.\n\n**False Discovery Rate (FDR).** The expected *proportion* of rejected hypotheses that are false. This is less conservative. If FDR = 0.05, you expect that 5% of your discoveries are false, but the other 95% are real. Methods: @benjamini1995controlling (BH), @storey2003positive.\n\nIn asset pricing, FDR control is generally more appropriate than FWER because we are not trying to guarantee zero false discoveries, we are trying to identify a set of factors where *most* are genuine. A researcher who controls FDR at 5% and discovers 10 factors expects approximately 0.5 of them to be false, which is acceptable for building a factor model.\n\n::: {#fig-error-rates .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Visualize how error rates scale with number of tests\"}\nM_range = np.arange(1, 301)\nalpha = 0.05\n\nfwer_unadj = 1 - (1 - alpha) ** M_range\nfwer_bonf = np.minimum(1, M_range * alpha)  # Not actual FWER, just threshold\n\n# Expected false discoveries at alpha = 0.05\nexpected_false = M_range * alpha\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(M_range, fwer_unadj, color='#C0392B', linewidth=2,\n             label='Unadjusted FWER')\naxes[0].axhline(y=0.05, color='gray', linestyle='--', linewidth=1,\n                label='α = 0.05 target')\naxes[0].set_xlabel('Number of Tests (M)')\naxes[0].set_ylabel('P(≥1 False Discovery)')\naxes[0].set_title('Panel A: Family-Wise Error Rate')\naxes[0].legend()\n\naxes[1].plot(M_range, expected_false, color='#2C5F8A', linewidth=2)\naxes[1].set_xlabel('Number of Tests (M)')\naxes[1].set_ylabel('Expected False Discoveries')\naxes[1].set_title('Panel B: Expected False Discoveries at α = 0.05')\naxes[1].axhline(y=1, color='gray', linestyle='--', linewidth=0.8)\naxes[1].text(250, 1.5, '1 false discovery', color='gray', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Correction Methods {#sec-factor-zoo-corrections}\n\n### Bonferroni Correction (FWER Control)\n\nThe simplest and most conservative correction: reject hypothesis $i$ only if its p-value is below $\\alpha / M$, where $M$ is the total number of tests:\n\n$$\n\\text{Reject } H_{0,i} \\text{ if } p_i < \\frac{\\alpha}{M}\n$$ {#eq-bonferroni}\n\nThis controls FWER at $\\alpha$ regardless of the dependence structure among tests. The cost is severe loss of power: for $M = 50$ tests at $\\alpha = 0.05$, the per-test threshold becomes $0.001$, requiring $|t| > 3.29$.\n\n::: {#bonferroni .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Apply Bonferroni correction to the anomaly zoo\"}\nM = len(zoo_df)\nalpha = 0.05\n\n# Bonferroni threshold\nbonf_threshold = alpha / M\nbonf_t_threshold = stats.norm.ppf(1 - bonf_threshold / 2)\n\nzoo_df['bonf_reject'] = zoo_df['p_value'] < bonf_threshold\n\nprint(f\"Number of tests: {M}\")\nprint(f\"Bonferroni p-value threshold: {bonf_threshold:.6f}\")\nprint(f\"Equivalent t-statistic threshold: {bonf_t_threshold:.2f}\")\nprint(f\"Rejections: {zoo_df['bonf_reject'].sum()} / {M}\")\nprint(f\"\\nSurviving anomalies:\")\nsurvivors = zoo_df[zoo_df['bonf_reject']]\nif len(survivors) > 0:\n    print(survivors[['ann_return', 't_stat', 'category']].to_string())\nelse:\n    print(\"  None survive Bonferroni correction\")\n```\n:::\n\n\n### Holm Step-Down Procedure (FWER Control)\n\nThe Holm procedure is uniformly more powerful than Bonferroni while still controlling FWER. It ranks p-values from smallest to largest and applies progressively less stringent thresholds:\n\n1.  Sort p-values: $p_{(1)} \\leq p_{(2)} \\leq \\ldots \\leq p_{(M)}$\n2.  For $i = 1, 2, \\ldots$, reject $H_{0,(i)}$ if $p_{(i)} < \\alpha / (M - i + 1)$\n3.  Stop at the first $i$ where $H_{0,(i)}$ is not rejected\n\n::: {#holm .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Apply the Holm step-down procedure\"}\np_values = zoo_df['p_value'].values\nsignal_names = zoo_df.index.values\n\n# Using statsmodels implementation\nreject_holm, pvals_corrected_holm, _, _ = multipletests(\n    p_values, alpha=0.05, method='holm'\n)\n\nzoo_df['holm_reject'] = reject_holm\nzoo_df['holm_p_corrected'] = pvals_corrected_holm\n\nprint(f\"Holm rejections: {reject_holm.sum()} / {M}\")\nholm_survivors = zoo_df[zoo_df['holm_reject']]\nif len(holm_survivors) > 0:\n    print(\"\\nSurviving anomalies (Holm):\")\n    print(holm_survivors[['ann_return', 't_stat', 'holm_p_corrected',\n                           'category']].to_string())\n```\n:::\n\n\n### Benjamini-Hochberg Procedure (FDR Control)\n\nThe @benjamini1995controlling (BH) procedure controls the False Discovery Rate (i.e., the expected proportion of rejections that are false) at level $q$:\n\n1.  Sort p-values: $p_{(1)} \\leq p_{(2)} \\leq \\ldots \\leq p_{(M)}$\n2.  Find the largest $k$ such that $p_{(k)} \\leq \\frac{k}{M} q$\n3.  Reject all $H_{0,(i)}$ for $i = 1, \\ldots, k$\n\nThe BH procedure is valid under independence or positive dependence (PRDS), a condition that is generally satisfied for financial factor tests where factors tend to be positively correlated under the null.\n\n::: {#bh-fdr .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Apply the Benjamini-Hochberg FDR procedure\"}\n# BH procedure at q = 0.05 (expect 5% false discoveries)\nreject_bh, pvals_corrected_bh, _, _ = multipletests(\n    p_values, alpha=0.05, method='fdr_bh'\n)\n\nzoo_df['bh_reject'] = reject_bh\nzoo_df['bh_p_corrected'] = pvals_corrected_bh\n\n# Also at q = 0.10 (more liberal)\nreject_bh10, _, _, _ = multipletests(\n    p_values, alpha=0.10, method='fdr_bh'\n)\nzoo_df['bh10_reject'] = reject_bh10\n\nprint(f\"BH rejections (q=0.05): {reject_bh.sum()} / {M}\")\nprint(f\"BH rejections (q=0.10): {reject_bh10.sum()} / {M}\")\n\nbh_survivors = zoo_df[zoo_df['bh_reject']].sort_values('abs_t', ascending=False)\nif len(bh_survivors) > 0:\n    print(f\"\\nSurviving anomalies (BH, q=0.05):\")\n    print(bh_survivors[['ann_return', 't_stat', 'bh_p_corrected',\n                          'category']].head(20).to_string())\n```\n:::\n\n\n::: {#fig-bh-procedure .cell execution_count=10}\n``` {.python .cell-code code-summary=\"Plot the BH procedure graphically\"}\nsorted_pvals = np.sort(p_values)\nk = np.arange(1, M + 1)\nbh_line_05 = k / M * 0.05\nbh_line_10 = k / M * 0.10\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.scatter(k, sorted_pvals, s=15, color='#2C5F8A', alpha=0.7,\n           label='Sorted p-values', zorder=3)\nax.plot(k, bh_line_05, color='#C0392B', linewidth=2,\n        label='BH line (q=0.05)')\nax.plot(k, bh_line_10, color='#E67E22', linewidth=2,\n        linestyle='--', label='BH line (q=0.10)')\n\n# Highlight rejections\nn_reject_05 = reject_bh.sum()\nif n_reject_05 > 0:\n    ax.scatter(k[:n_reject_05], sorted_pvals[:n_reject_05],\n               s=40, color='#C0392B', zorder=4, label=f'Rejected (q=0.05)')\n\nax.set_xlabel('Rank (k)')\nax.set_ylabel('p-value')\nax.set_title('Benjamini-Hochberg Procedure')\nax.legend()\nax.set_ylim([-0.01, 0.15])\nax.set_xlim([0, min(M, 60)])\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### The Harvey-Liu-Zhu Threshold\n\n@harvey2016and take a different approach: rather than applying a formal multiple-testing correction to a specific set of tests, they estimate the hurdle t-statistic that accounts for the *cumulative* number of factors tested by the entire profession. Using a Bayesian framework calibrated to the 316 published factors, they derive:\n\n-   For a single new factor tested in isolation: $|t| > 2.0$ (conventional)\n-   Accounting for 100 prior tests: $|t| > 2.8$\n-   Accounting for 316 prior tests: $|t| > 3.0$\n-   Accounting for all conceivable tests: $|t| > 3.78$\n\nThe logic is that even if *your* study tests only one factor, the fact that hundreds of researchers have tested hundreds of signals before you means the prior probability that your specific signal is a true predictor is low.\n\n::: {#hlz-threshold .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Apply the Harvey-Liu-Zhu t-statistic thresholds\"}\nhlz_thresholds = {\n    'Conventional (|t| > 2.0)': 2.0,\n    'HLZ 100 tests (|t| > 2.8)': 2.8,\n    'HLZ 316 tests (|t| > 3.0)': 3.0,\n    'HLZ conservative (|t| > 3.78)': 3.78\n}\n\nprint(\"Anomalies Surviving Different t-Thresholds:\")\nprint(f\"{'Threshold':<35} {'Surviving':>10} {'% of Zoo':>10}\")\nprint(\"-\" * 55)\n\nfor name, thresh in hlz_thresholds.items():\n    n_survive = (zoo_df['abs_t'] > thresh).sum()\n    pct = n_survive / len(zoo_df) * 100\n    print(f\"{name:<35} {n_survive:>10} {pct:>10.1f}%\")\n```\n:::\n\n\n## Comparison of Methods {#sec-factor-zoo-comparison}\n\n::: {#fig-correction-comparison .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Compare all correction methods side by side\"}\ncomparison = {\n    'Unadjusted (|t| > 2)': (zoo_df['abs_t'] > 2.0).sum(),\n    'Bonferroni': zoo_df['bonf_reject'].sum(),\n    'Holm': zoo_df['holm_reject'].sum(),\n    'BH (q=0.05)': zoo_df['bh_reject'].sum(),\n    'BH (q=0.10)': zoo_df['bh10_reject'].sum(),\n    'HLZ (|t| > 3.0)': (zoo_df['abs_t'] > 3.0).sum(),\n    'HLZ (|t| > 3.78)': (zoo_df['abs_t'] > 3.78).sum(),\n}\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nnames = list(comparison.keys())\ncounts = list(comparison.values())\ncolors = ['#BDC3C7', '#C0392B', '#E74C3C', '#27AE60', '#2ECC71',\n          '#2C5F8A', '#1A3C5A']\n\nbars = ax.barh(range(len(names)), counts, color=colors,\n               alpha=0.85, edgecolor='white')\nax.set_yticks(range(len(names)))\nax.set_yticklabels(names)\nax.set_xlabel('Number of Surviving Anomalies')\nax.set_title('Anomalies Surviving Multiple Testing Corrections')\n\nfor i, (name, count) in enumerate(zip(names, counts)):\n    ax.text(count + 0.3, i, str(count), va='center', fontsize=10)\n\nax.invert_yaxis()\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n::: {#fig-survival-by-category .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Plot survival rates by anomaly category\"}\ncat_survival = (\n    zoo_df.groupby('category')\n    .agg(\n        n_total=('abs_t', 'count'),\n        n_survive_bh=('bh_reject', 'sum'),\n        n_survive_hlz=('abs_t', lambda x: (x > 3.0).sum()),\n        mean_abs_t=('abs_t', 'mean')\n    )\n    .sort_values('mean_abs_t', ascending=False)\n)\ncat_survival['survival_rate_bh'] = (\n    cat_survival['n_survive_bh'] / cat_survival['n_total']\n)\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\nx = np.arange(len(cat_survival))\nax.bar(x, cat_survival['n_total'], color='#BDC3C7', alpha=0.5,\n       label='Total tested', edgecolor='white')\nax.bar(x, cat_survival['n_survive_bh'], color='#27AE60', alpha=0.85,\n       label='Survive BH (q=0.05)', edgecolor='white')\n\nax.set_xticks(x)\nax.set_xticklabels(cat_survival.index, rotation=30, fontsize=9)\nax.set_ylabel('Number of Anomalies')\nax.set_title('Anomaly Survival by Category')\nax.legend()\n\n# Add survival rate text\nfor i, (_, row) in enumerate(cat_survival.iterrows()):\n    if row['n_total'] > 0:\n        ax.text(i, row['n_total'] + 0.2,\n                f\"{row['survival_rate_bh']:.0%}\",\n                ha='center', fontsize=9, color='#27AE60')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Estimating the False Discovery Proportion {#sec-factor-zoo-fdp}\n\n### The Storey Approach\n\nRather than controlling FDR at a pre-specified level, we can *estimate* the proportion of tested hypotheses that are truly null ($\\pi_0$) and the implied false discovery proportion (FDP) at any given threshold. @storey2003positive proposes estimating $\\pi_0$ from the distribution of p-values: under the null, p-values are uniformly distributed on $[0, 1]$, so the density of p-values in the \"flat\" region (away from zero) reveals $\\pi_0$.\n\n$$\n\\hat{\\pi}_0(\\lambda) = \\frac{\\#\\{p_i > \\lambda\\}}{M(1 - \\lambda)}\n$$ {#eq-pi0}\n\nfor a tuning parameter $\\lambda$. The estimated false discovery proportion at threshold $t^*$ is then:\n\n$$\n\\widehat{\\text{FDP}}(t^*) = \\frac{\\hat{\\pi}_0 \\cdot M \\cdot P(|t| > t^* | H_0)}{\\#\\{|t_i| > t^*\\}}\n$$ {#eq-fdp}\n\n::: {#storey-pi0 .cell execution_count=14}\n``` {.python .cell-code code-summary=\"Estimate the proportion of true nulls using Storey's method\"}\ndef estimate_pi0(p_values, lambdas=np.arange(0.05, 0.95, 0.05)):\n    \"\"\"\n    Estimate pi_0 (proportion of true nulls) using Storey (2003).\n    Uses bootstrap to select optimal lambda.\n    \"\"\"\n    M = len(p_values)\n    pi0_estimates = []\n    \n    for lam in lambdas:\n        n_above = (p_values > lam).sum()\n        pi0 = n_above / (M * (1 - lam))\n        pi0_estimates.append({'lambda': lam, 'pi0': min(pi0, 1.0)})\n    \n    pi0_df = pd.DataFrame(pi0_estimates)\n    \n    # Use the median of estimates for robustness\n    # (alternatives: bootstrap, spline smoothing)\n    pi0_hat = pi0_df['pi0'].median()\n    \n    return pi0_hat, pi0_df\n\npi0_hat, pi0_df = estimate_pi0(zoo_df['p_value'].values)\n\nprint(f\"Estimated π₀ (proportion of true nulls): {pi0_hat:.3f}\")\nprint(f\"Implied proportion of true factors: {1 - pi0_hat:.3f}\")\nprint(f\"Estimated number of true factors: {(1 - pi0_hat) * len(zoo_df):.0f} \"\n      f\"out of {len(zoo_df)}\")\n```\n:::\n\n\n::: {#fig-pvalue-distribution .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Plot p-value distribution and pi_0 estimation\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: p-value histogram\naxes[0].hist(zoo_df['p_value'], bins=20, density=True,\n             color='#2C5F8A', alpha=0.7, edgecolor='white')\naxes[0].axhline(y=pi0_hat, color='#C0392B', linewidth=2, linestyle='--',\n                label=f'π₀ = {pi0_hat:.2f} (null density)')\naxes[0].axhline(y=1, color='gray', linewidth=1, linestyle=':',\n                label='Uniform (all null)')\naxes[0].set_xlabel('p-value')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Panel A: p-Value Distribution')\naxes[0].legend()\n\n# Panel B: pi_0 estimates across lambda\naxes[1].plot(pi0_df['lambda'], pi0_df['pi0'],\n             color='#2C5F8A', linewidth=2, marker='o', markersize=4)\naxes[1].axhline(y=pi0_hat, color='#C0392B', linestyle='--',\n                linewidth=1.5, label=f'Median π₀ = {pi0_hat:.2f}')\naxes[1].set_xlabel('λ')\naxes[1].set_ylabel('π₀(λ)')\naxes[1].set_title('Panel B: π₀ Estimates by λ')\naxes[1].legend()\naxes[1].set_ylim([0, 1.05])\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### FDP Curve\n\nUsing $\\hat{\\pi}_0$, we compute the estimated FDP at every possible t-threshold:\n\n::: {#fig-fdp-curve .cell execution_count=16}\n``` {.python .cell-code code-summary=\"Compute and plot the FDP curve\"}\nt_thresholds = np.arange(1.0, 5.01, 0.1)\nfdp_curve = []\n\nfor t_thresh in t_thresholds:\n    n_rejected = (zoo_df['abs_t'] > t_thresh).sum()\n    if n_rejected == 0:\n        fdp_curve.append({'t_threshold': t_thresh, 'n_rejected': 0,\n                           'fdp': 0})\n        continue\n    \n    # Expected false discoveries under null\n    p_null_reject = 2 * (1 - stats.norm.cdf(t_thresh))\n    expected_false = pi0_hat * len(zoo_df) * p_null_reject\n    \n    fdp = min(expected_false / n_rejected, 1.0)\n    fdp_curve.append({'t_threshold': t_thresh, 'n_rejected': n_rejected,\n                       'fdp': fdp})\n\nfdp_df = pd.DataFrame(fdp_curve)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: FDP curve\naxes[0].plot(fdp_df['t_threshold'], fdp_df['fdp'] * 100,\n             color='#C0392B', linewidth=2)\naxes[0].axhline(y=5, color='gray', linestyle='--', linewidth=1,\n                label='5% FDP')\naxes[0].axhline(y=10, color='gray', linestyle=':', linewidth=1,\n                label='10% FDP')\naxes[0].axvline(x=2.0, color='#E67E22', linestyle='--',\n                linewidth=1, label='|t| = 2.0')\naxes[0].axvline(x=3.0, color='#2C5F8A', linestyle='--',\n                linewidth=1, label='|t| = 3.0')\naxes[0].set_xlabel('|t| Threshold')\naxes[0].set_ylabel('Estimated FDP (%)')\naxes[0].set_title('Panel A: False Discovery Proportion')\naxes[0].legend(fontsize=8)\naxes[0].set_ylim([-2, 80])\n\n# Panel B: Number of rejections\naxes[1].plot(fdp_df['t_threshold'], fdp_df['n_rejected'],\n             color='#2C5F8A', linewidth=2)\naxes[1].set_xlabel('|t| Threshold')\naxes[1].set_ylabel('Number of Discoveries')\naxes[1].set_title('Panel B: Discoveries vs Threshold')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Bootstrap-Based Multiple Testing {#sec-factor-zoo-bootstrap}\n\n### The White Reality Check\n\nAnalytical corrections assume known distributions. When return distributions are non-normal (heavy tails, skewness, serial correlation), as they are in Vietnam, bootstrap methods provide more reliable inference.\n\n@white2000reality proposes a bootstrap reality check that accounts for data snooping across multiple strategies. The null hypothesis is that the best strategy has zero expected return. The procedure:\n\n1.  Generate $B$ bootstrap samples of the time series (block bootstrap to preserve serial dependence).\n2.  For each bootstrap sample, compute the t-statistic for every factor.\n3.  The p-value for factor $i$ is the proportion of bootstrap samples in which the maximum t-statistic across all factors exceeds the observed t-statistic of factor $i$.\n\n::: {#bootstrap-rc .cell execution_count=17}\n``` {.python .cell-code code-summary=\"Implement White's Reality Check via block bootstrap\"}\ndef white_reality_check(factor_returns_dict, n_bootstrap=1000,\n                          block_size=6, seed=42):\n    \"\"\"\n    White (2000) Reality Check for data snooping.\n    \n    Parameters\n    ----------\n    factor_returns_dict : dict\n        {factor_name: pd.Series of monthly returns}\n    n_bootstrap : int\n        Number of bootstrap replications\n    block_size : int\n        Block length for circular block bootstrap\n    \n    Returns\n    -------\n    DataFrame with original t-stats and bootstrap p-values\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Align all factor returns to common dates\n    all_names = list(factor_returns_dict.keys())\n    common_dates = None\n    for name in all_names:\n        dates = set(factor_returns_dict[name].index)\n        common_dates = dates if common_dates is None else common_dates & dates\n    common_dates = sorted(common_dates)\n    T = len(common_dates)\n    \n    # Build return matrix (T x M)\n    M = len(all_names)\n    return_matrix = np.column_stack([\n        factor_returns_dict[name].reindex(common_dates).values\n        for name in all_names\n    ])\n    \n    # Observed t-statistics\n    means = np.nanmean(return_matrix, axis=0)\n    ses = np.nanstd(return_matrix, axis=0) / np.sqrt(T)\n    t_obs = means / np.where(ses > 0, ses, np.nan)\n    \n    # Block bootstrap\n    n_blocks = int(np.ceil(T / block_size))\n    max_t_bootstrap = np.zeros(n_bootstrap)\n    \n    for b in range(n_bootstrap):\n        # Circular block bootstrap\n        block_starts = rng.integers(0, T, size=n_blocks)\n        indices = np.concatenate([\n            np.arange(start, start + block_size) % T\n            for start in block_starts\n        ])[:T]\n        \n        boot_returns = return_matrix[indices, :]\n        \n        # Center under null (subtract original mean)\n        boot_centered = boot_returns - means[np.newaxis, :]\n        \n        # Compute t-stats under null\n        boot_means = np.nanmean(boot_centered, axis=0)\n        boot_ses = np.nanstd(boot_centered, axis=0) / np.sqrt(T)\n        boot_t = boot_means / np.where(boot_ses > 0, boot_ses, np.nan)\n        \n        max_t_bootstrap[b] = np.nanmax(np.abs(boot_t))\n    \n    # Bootstrap p-values (one-sided: is obs_t extreme relative to max?)\n    boot_pvals = np.array([\n        np.mean(max_t_bootstrap >= abs(t)) if np.isfinite(t) else 1.0\n        for t in t_obs\n    ])\n    \n    results = pd.DataFrame({\n        'factor': all_names,\n        't_stat_obs': t_obs,\n        'boot_pval': boot_pvals\n    }).set_index('factor')\n    \n    return results, max_t_bootstrap\n\n# Collect factor return series\nfactor_returns_dict = {\n    name: zoo_results[name]['returns']\n    for name in zoo_results\n    if 'returns' in zoo_results[name]\n}\n\nboot_results, max_t_dist = white_reality_check(\n    factor_returns_dict, n_bootstrap=1000, block_size=6\n)\n\nboot_results = boot_results.sort_values('t_stat_obs', ascending=False)\n\nprint(\"White Reality Check Results (top 20):\")\nprint(boot_results.head(20).round(4).to_string())\nprint(f\"\\nFactors surviving (boot p < 0.05): \"\n      f\"{(boot_results['boot_pval'] < 0.05).sum()}\")\n```\n:::\n\n\n::: {#fig-bootstrap-distribution .cell execution_count=18}\n``` {.python .cell-code code-summary=\"Plot the bootstrap null distribution\"}\nfig, ax = plt.subplots(figsize=(12, 5))\n\nax.hist(max_t_dist, bins=40, density=True, color='#BDC3C7',\n        alpha=0.7, edgecolor='white', label='Bootstrap null\\n(max |t| across all factors)')\n\n# 95th percentile\np95 = np.percentile(max_t_dist, 95)\nax.axvline(x=p95, color='#C0392B', linewidth=2, linestyle='--',\n           label=f'95th percentile = {p95:.2f}')\n\n# Observed top factor t-statistics\ntop_5 = boot_results.head(5)\nfor i, (name, row) in enumerate(top_5.iterrows()):\n    ax.axvline(x=abs(row['t_stat_obs']),\n               color=plt.cm.Set1(i), linewidth=1.5, linestyle=':',\n               label=f'{name}: |t| = {abs(row[\"t_stat_obs\"]):.2f}')\n\nax.set_xlabel('Maximum |t-statistic|')\nax.set_ylabel('Density')\nax.set_title(\"White's Reality Check: Bootstrap Null Distribution\")\nax.legend(fontsize=8, loc='upper right')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Romano-Wolf Step-Down Bootstrap\n\nThe @romano2005exact step-down procedure improves on the White Reality Check by iteratively removing rejected hypotheses and re-testing the remaining ones, gaining power at each step:\n\n::: {#romano-wolf .cell execution_count=19}\n``` {.python .cell-code code-summary=\"Implement the Romano-Wolf step-down bootstrap procedure\"}\ndef romano_wolf_stepdown(factor_returns_dict, n_bootstrap=1000,\n                           block_size=6, alpha=0.05, seed=42):\n    \"\"\"\n    Romano-Wolf (2005) step-down procedure for FWER control.\n    More powerful than single-step methods because rejected\n    hypotheses are removed before re-testing.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    all_names = list(factor_returns_dict.keys())\n    common_dates = None\n    for name in all_names:\n        dates = set(factor_returns_dict[name].index)\n        common_dates = dates if common_dates is None else common_dates & dates\n    common_dates = sorted(common_dates)\n    T = len(common_dates)\n    M = len(all_names)\n    \n    return_matrix = np.column_stack([\n        factor_returns_dict[name].reindex(common_dates).values\n        for name in all_names\n    ])\n    \n    means = np.nanmean(return_matrix, axis=0)\n    ses = np.nanstd(return_matrix, axis=0) / np.sqrt(T)\n    t_obs = np.abs(means / np.where(ses > 0, ses, np.nan))\n    \n    # Sort by observed t-stat (descending)\n    order = np.argsort(-t_obs)\n    t_sorted = t_obs[order]\n    names_sorted = [all_names[i] for i in order]\n    \n    # Step-down procedure\n    rejected = set()\n    remaining = set(range(M))\n    \n    for step in range(M):\n        if not remaining:\n            break\n        \n        remaining_idx = sorted(remaining)\n        \n        # Bootstrap max t-stat among remaining\n        n_blocks = int(np.ceil(T / block_size))\n        max_t_boot = np.zeros(n_bootstrap)\n        \n        for b in range(n_bootstrap):\n            block_starts = rng.integers(0, T, size=n_blocks)\n            indices = np.concatenate([\n                np.arange(start, start + block_size) % T\n                for start in block_starts\n            ])[:T]\n            \n            boot_returns = return_matrix[indices][:, remaining_idx]\n            boot_centered = boot_returns - means[remaining_idx]\n            \n            boot_means = np.nanmean(boot_centered, axis=0)\n            boot_ses = np.nanstd(boot_centered, axis=0) / np.sqrt(T)\n            boot_t = np.abs(boot_means / np.where(boot_ses > 0, boot_ses, np.nan))\n            \n            max_t_boot[b] = np.nanmax(boot_t)\n        \n        # Critical value\n        cv = np.percentile(max_t_boot, (1 - alpha) * 100)\n        \n        # Test the most significant remaining factor\n        # Find the factor with highest t among remaining\n        best_remaining = max(remaining, key=lambda j: t_obs[j])\n        \n        if t_obs[best_remaining] > cv:\n            rejected.add(best_remaining)\n            remaining.remove(best_remaining)\n        else:\n            break  # Cannot reject any more\n    \n    results = pd.DataFrame({\n        'factor': all_names,\n        't_stat': t_obs,\n        'rejected': [i in rejected for i in range(M)]\n    }).sort_values('t_stat', ascending=False)\n    \n    return results\n\nrw_results = romano_wolf_stepdown(\n    factor_returns_dict, n_bootstrap=500, block_size=6\n)\n\nprint(f\"Romano-Wolf rejections: {rw_results['rejected'].sum()} / {len(rw_results)}\")\nprint(\"\\nRejected factors:\")\nprint(rw_results[rw_results['rejected']][['factor', 't_stat']].to_string())\n```\n:::\n\n\n## Out-of-Sample Validation {#sec-factor-zoo-oos}\n\n### Pre-Publication vs. Post-Publication Decay\n\n@mclean2016does find that anomaly returns decline by 32% after publication in the U.S. market, suggesting that roughly one-third of the premium was due to mispricing that was corrected by informed trading. For Vietnam, we use a time-series split as a proxy for out-of-sample testing:\n\n::: {#oos-split .cell execution_count=20}\n``` {.python .cell-code code-summary=\"Compare factor premia in-sample vs out-of-sample\"}\n# Split at midpoint of available data\nall_dates = sorted(panel['month_end'].unique())\nmid_date = all_dates[len(all_dates) // 2]\n\noos_comparison = []\n\nfor name in zoo_results:\n    ls = zoo_results[name]['returns']\n    \n    in_sample = ls[ls.index <= mid_date]\n    out_sample = ls[ls.index > mid_date]\n    \n    if len(in_sample) < 36 or len(out_sample) < 36:\n        continue\n    \n    is_mean = in_sample.mean() * 12\n    is_t = in_sample.mean() / (in_sample.std() / np.sqrt(len(in_sample)))\n    \n    oos_mean = out_sample.mean() * 12\n    oos_t = out_sample.mean() / (out_sample.std() / np.sqrt(len(out_sample)))\n    \n    oos_comparison.append({\n        'signal': name,\n        'is_return': is_mean,\n        'is_t': is_t,\n        'oos_return': oos_mean,\n        'oos_t': oos_t,\n        'decay_pct': (1 - oos_mean / is_mean) * 100 if is_mean != 0 else np.nan,\n        'same_sign': np.sign(is_mean) == np.sign(oos_mean),\n        'category': zoo_results[name].get('category', 'Other')\n    })\n\noos_df = pd.DataFrame(oos_comparison)\n\nprint(f\"Split date: {mid_date.strftime('%Y-%m')}\")\nprint(f\"Factors with same sign IS and OOS: \"\n      f\"{oos_df['same_sign'].mean():.1%}\")\nprint(f\"Average OOS decay: {oos_df['decay_pct'].median():.1f}%\")\n```\n:::\n\n\n::: {#fig-is-oos .cell execution_count=21}\n``` {.python .cell-code code-summary=\"Scatter in-sample vs out-of-sample returns\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Scatter\ncategory_colors = {\n    'Value': '#2C5F8A', 'Size': '#1ABC9C', 'Profitability': '#27AE60',\n    'Investment': '#E67E22', 'Momentum': '#C0392B', 'Risk': '#8E44AD',\n    'Liquidity': '#3498DB', 'Quality': '#F1C40F', 'Dividend': '#95A5A6'\n}\n\nfor cat in oos_df['category'].unique():\n    subset = oos_df[oos_df['category'] == cat]\n    axes[0].scatter(subset['is_return'] * 100, subset['oos_return'] * 100,\n                     color=category_colors.get(cat, '#BDC3C7'),\n                     s=50, alpha=0.7, label=cat, edgecolors='white')\n\nlim = max(abs(oos_df['is_return'].max()), abs(oos_df['oos_return'].max())) * 100 + 2\naxes[0].plot([-lim, lim], [-lim, lim], 'k--', linewidth=1, alpha=0.5)\naxes[0].plot([-lim, lim], [0, 0], color='gray', linewidth=0.5)\naxes[0].plot([0, 0], [-lim, lim], color='gray', linewidth=0.5)\naxes[0].set_xlabel('In-Sample Return (% ann.)')\naxes[0].set_ylabel('Out-of-Sample Return (% ann.)')\naxes[0].set_title('Panel A: IS vs OOS Factor Returns')\naxes[0].legend(fontsize=7, ncol=2)\n\n# Panel B: Decay by category\ncat_decay = (\n    oos_df.groupby('category')\n    .agg(\n        median_decay=('decay_pct', 'median'),\n        pct_same_sign=('same_sign', 'mean'),\n        n=('signal', 'count')\n    )\n    .sort_values('median_decay')\n)\n\ncolors_bar = [category_colors.get(cat, '#BDC3C7') for cat in cat_decay.index]\naxes[1].barh(range(len(cat_decay)), cat_decay['median_decay'],\n             color=colors_bar, alpha=0.85, edgecolor='white')\naxes[1].set_yticks(range(len(cat_decay)))\naxes[1].set_yticklabels(cat_decay.index)\naxes[1].set_xlabel('Median OOS Decay (%)')\naxes[1].set_title('Panel B: OOS Decay by Category')\naxes[1].axvline(x=0, color='gray', linewidth=0.8)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Which Factors Survive? {#sec-factor-zoo-survivors}\n\n### A Multi-Hurdle Filter\n\nWe now combine all the evidence, including multiple testing corrections, out-of-sample performance, and economic plausibility, to identify the factors that deserve credence in the Vietnamese market.\n\n::: {#multi-hurdle .cell execution_count=22}\n``` {.python .cell-code code-summary=\"Apply a multi-hurdle filter to identify robust factors\"}\n# Merge all test results\nfinal = zoo_df.copy()\n\n# Add OOS results\noos_lookup = oos_df.set_index('signal')\nfor col in ['oos_return', 'oos_t', 'same_sign']:\n    final[col] = final.index.map(lambda x: oos_lookup.loc[x, col]\n                                  if x in oos_lookup.index else np.nan)\n\n# Add bootstrap results\nboot_lookup = boot_results\nfinal['boot_pval'] = final.index.map(\n    lambda x: boot_lookup.loc[x, 'boot_pval']\n    if x in boot_lookup.index else np.nan\n)\n\n# Multi-hurdle criteria\nfinal['pass_t2'] = final['abs_t'] > 2.0\nfinal['pass_t3'] = final['abs_t'] > 3.0\nfinal['pass_bh'] = final['bh_reject']\nfinal['pass_boot'] = final['boot_pval'] < 0.05\nfinal['pass_oos_sign'] = final['same_sign'] == True\nfinal['pass_oos_t'] = final['oos_t'].abs() > 1.5  # Relaxed OOS threshold\n\n# Count hurdles passed\nhurdle_cols = ['pass_t2', 'pass_t3', 'pass_bh', 'pass_boot',\n                'pass_oos_sign', 'pass_oos_t']\nfinal['n_hurdles'] = final[hurdle_cols].sum(axis=1)\n\n# \"Robust\" = passes at least 5 of 6 hurdles\nfinal['robust'] = final['n_hurdles'] >= 5\n\nrobust_factors = final[final['robust']].sort_values('abs_t', ascending=False)\n\nprint(f\"Multi-Hurdle Filter Results:\")\nprint(f\"  Pass |t| > 2.0:        {final['pass_t2'].sum()}\")\nprint(f\"  Pass |t| > 3.0:        {final['pass_t3'].sum()}\")\nprint(f\"  Pass BH (q=0.05):      {final['pass_bh'].sum()}\")\nprint(f\"  Pass bootstrap:        {final['pass_boot'].sum()}\")\nprint(f\"  Pass OOS sign:         {final['pass_oos_sign'].sum()}\")\nprint(f\"  Pass OOS |t| > 1.5:    {final['pass_oos_t'].sum()}\")\nprint(f\"\\n  ROBUST (≥5/6 hurdles): {final['robust'].sum()}\")\n\nif len(robust_factors) > 0:\n    print(f\"\\nRobust Factors:\")\n    display_cols = ['ann_return', 't_stat', 'bh_p_corrected',\n                     'oos_return', 'oos_t', 'n_hurdles', 'category']\n    available_cols = [c for c in display_cols if c in robust_factors.columns]\n    print(robust_factors[available_cols].round(3).to_string())\n```\n:::\n\n\n::: {#fig-hurdle-heatmap .cell execution_count=23}\n``` {.python .cell-code code-summary=\"Heatmap of multi-hurdle results\"}\ntop25 = final.sort_values('abs_t', ascending=False).head(25)\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nhurdle_matrix = top25[hurdle_cols].astype(float).values\n\nsns.heatmap(hurdle_matrix, ax=ax,\n            xticklabels=['|t|>2', '|t|>3', 'BH', 'Boot', 'OOS sign', 'OOS |t|'],\n            yticklabels=top25.index,\n            cmap=['#E74C3C', '#27AE60'],\n            cbar=False, linewidths=0.5, linecolor='white',\n            annot=np.where(hurdle_matrix == 1, '✓', '✗'),\n            fmt='')\n\n# Add hurdle count column\nfor i, (_, row) in enumerate(top25.iterrows()):\n    ax.text(len(hurdle_cols) + 0.3, i + 0.5,\n            f\"{int(row['n_hurdles'])}/6\",\n            va='center', fontsize=9,\n            fontweight='bold' if row['n_hurdles'] >= 5 else 'normal',\n            color='#27AE60' if row['n_hurdles'] >= 5 else '#C0392B')\n\nax.set_title('Multi-Hurdle Test Results (Top 25 Anomalies)')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Implications for Vietnamese Factor Research {#sec-factor-zoo-implications}\n\nThe analysis in this chapter yields several practical implications:\n\n**The conventional threshold is insufficient.** With $M \\approx 50$ anomalies tested simultaneously, requiring only $|t| > 2.0$ virtually guarantees false discoveries. Vietnamese researchers should adopt a minimum threshold of $|t| > 3.0$ for individual factors, consistent with @harvey2016and, and should report BH-adjusted p-values for any study testing multiple signals.\n\n**The small cross-section amplifies noise.** With 600-800 stocks, quintile portfolios contain only 120-160 stocks each. This creates noisier portfolio returns and wider confidence intervals than in U.S. studies with 4,000+ stocks. Strategies that appear significant in the U.S. at $|t| = 2.5$ may have $|t| < 1.5$ in Vietnam simply due to the smaller sample. This is not evidence that the factor doesn't exist in Vietnam, it is evidence that the Vietnamese data lack power to detect it.\n\n**Out-of-sample validation is essential.** The time-series split reveals substantial decay for many anomalies, consistent with @mclean2016does. Factors that survive both in-sample and out-of-sample with consistent sign and magnitude are rare and valuable.\n\n**Category matters.** Momentum and profitability signals tend to have the highest replication rates across markets [@fama2012size; @jacobs2020anomalies]. Value and investment signals are more country-specific [@griffin2002fama]. Liquidity signals are likely to be strongest in emerging markets like Vietnam, where trading frictions are largest.\n\n**Report the full battery.** Any study that presents a new factor for the Vietnamese market should report: (i) the raw t-statistic, (ii) the BH-adjusted p-value given the number of tests in the study, (iii) out-of-sample performance in a held-out period, and (iv) sensitivity to construction choices (from the previous chapter). This reporting standard would dramatically improve the credibility of Vietnamese factor research.\n\n## Summary {#sec-factor-zoo-summary}\n\n| Method | Controls | Threshold (approx.) | Surviving | Philosophy |\n|---------------|---------------|---------------|---------------|---------------|\n| Unadjusted | Single test | \\|t\\| \\> 2.0 | Many | No correction |\n| Bonferroni | FWER | \\|t\\| \\> 3.3--4.0 | Few | Zero false positives |\n| Holm | FWER | Stepwise | Few | Slightly less conservative |\n| BH | FDR at q | Adaptive | Moderate | Tolerates some false positives |\n| HLZ | Profession-wide | \\|t\\| \\> 3.0--3.78 | Moderate | Prior over all tested factors |\n| White/RW Bootstrap | FWER (data-driven) | Bootstrap CV | Few | Accounts for non-normality |\n| Storey FDP | FDP estimate | Continuous | Diagnostic | Estimates true null proportion |\n| Multi-hurdle | Combined | Multiple tests | Most robust | Requires convergent evidence |\n\n: Summary of multiple testing methods and their properties. {#tbl-summary}\n\nThe factor zoo is not a uniquely American phenomenon. The temptation to test many signals and report the best-performing ones exists in every market. In Vietnam, where the data are shorter, noisier, and the academic community is growing rapidly, the risk of false discovery is high. The tools developed in this chapter, including BH adjustment, bootstrap reality checks, Storey's $\\pi_0$ estimation, out-of-sample splits, and the multi-hurdle filter, provide a principled framework for separating genuine factors from statistical noise.\n\nThe honest conclusion is likely to be humbling: of the dozens of anomalies documented in developed markets, only a handful survive rigorous multiple testing in Vietnamese data. Those survivors (probably concentrated in momentum, profitability, and liquidity) form the foundation for credible asset pricing research in this market.\n\n<!-- ## Exercises {#sec-factor-zoo-exercises}\n\n1.  **Bayesian approach.** Implement the @harvey2021lucky Bayesian framework for distinguishing lucky factors from genuine ones. Assign a prior probability $\\pi$ that any given factor is a true predictor, and compute the posterior probability of being genuine given its t-statistic and the number of tests. How does varying $\\pi$ affect which factors survive?\n\n2.  **Dependence-adjusted BH.** The standard BH procedure assumes independent (or PRDS) test statistics. Estimate the correlation structure of factor returns and implement the Benjamini-Yekutieli (BY) procedure, which controls FDR under arbitrary dependence. How many fewer factors survive compared to BH?\n\n3.  **Publication bias simulation.** Simulate a researcher who tests 100 random signals on Vietnamese data, publishes only those with $|t| > 2$, and reports the average published t-statistic. Repeat 1,000 times. What is the distribution of the \"published\" t-statistic? How does it compare to the true effect size (which is zero by construction)?\n\n4.  **Replication across exchanges.** Test whether factors that are significant on HOSE also replicate on HNX. If the same factor is significant on both exchanges independently, the probability of a false discovery is substantially lower (because the two samples are partially independent).\n\n5.  **Transaction cost hurdle.** For each factor that survives the multi-hurdle filter, compute the minimum round-trip trading cost that would eliminate the net-of-cost premium. Factors whose breakeven costs are below realistic Vietnamese transaction costs are not implementable, regardless of their statistical significance.\n\n6.  **Machine learning feature importance.** Use a random forest or gradient-boosted tree model to predict next-month returns from all available signals simultaneously. Extract feature importance measures. Compare the machine learning ranking of signals to the univariate t-statistic ranking. Do they agree?\n\n7.  **International replication.** Select the top 10 factors from the Vietnamese zoo. Test whether the same factors are significant in Thailand (SET), Indonesia (IDX), or the Philippines (PSE) using equivalent data. Factors that replicate across multiple ASEAN markets are less likely to be country-specific data artifacts.\n\n8.  **Rolling FDP monitoring.** Implement a rolling-window version of the Storey $\\pi_0$ estimator. Compute $\\hat{\\pi}_0$ for each 60-month window. Does the proportion of true factors change over time? An increasing $\\pi_0$ (more true nulls) might indicate that anomalies are being arbitraged away as the Vietnamese market matures. -->\n\n",
    "supporting": [
      "25_factor_zoo_and_multiple_testing_files/figure-pdf"
    ],
    "filters": []
  }
}