{
  "hash": "32e9a1e1ffc7b282e1fe78f6ae770f97",
  "result": {
    "engine": "jupyter",
    "markdown": "# Tail Risk and Extreme Events\n\nFinance is fundamentally a discipline of extremes. The central objects of concern (e.g., asset pricing, portfolio allocation, capital regulation, systemic stability) are all disproportionately shaped by rare but severe events. A portfolio manager who correctly estimates the mean and variance of returns but ignores the possibility of a $-20\\%$ daily move is, in the language of @taleb2010black, \"picking up pennies in front of a steamroller.\" The 1997 Asian Financial Crisis, the 2008 Global Financial Crisis, and the 2020 COVID-19 crash each demonstrated that the most consequential realizations in financial markets are precisely those that conventional Gaussian models assign near-zero probability.\n\nThis chapter develops the statistical and econometric toolkit for measuring, modeling, and diagnosing tail risk in Vietnamese equity markets. We focus on three interconnected layers. First, at the individual stock level, we estimate crash risk measures that capture the asymmetry and thickness of the left tail of return distributions. Second, at the portfolio or regulatory level, we implement Value at Risk (VaR) and Expected Shortfall (ES) under multiple estimation approaches and evaluate their accuracy via backtesting. Third, at the system level, we apply Extreme Value Theory (EVT) and tail dependence measures to characterize joint crash behavior across sectors and assess financial contagion.\n\nVietnamese markets present a particularly rich laboratory for studying tail phenomena. Daily price limits mechanically censor the tails of observed return distributions, creating latent tail risk that is not directly observable but must be inferred. State ownership concentrations, thin liquidity, and herding behavior among retail investors amplify crash dynamics. The absence of a developed derivatives market means that tail hedging instruments familiar in more developed markets (e.g., deep out-of-the-money puts, variance swaps) are largely unavailable, making accurate tail risk measurement all the more important for risk management.\n\n::: {#setup .cell message='false' execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\nimport plotnine as p9\nfrom mizani.formatters import percent_format, comma_format\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n\n## Crash Risk in Equity Markets\n\n### The Left Tail Problem\n\nThe return distribution of individual stocks and market indices is not symmetric. Decades of evidence, beginning with @mandelbrot1963variation and formalized in the asset pricing context by @harvey2000conditional, establish that equity returns exhibit negative skewness (i.e., large negative returns occur more frequently than a Gaussian model predicts) and excess kurtosis (i.e., the tails are thicker than normal in both directions, but the left tail is of primary economic concern).\n\nLet $r_{i,t}$ denote the daily log return of stock $i$ on day $t$. The standardized third central moment (skewness) is:\n\n$$\n\\text{Skew}(r_i) = \\frac{E\\left[(r_{i,t} - \\mu_i)^3\\right]}{\\sigma_i^3}\n$$ {#eq-skewness}\n\nNegative skewness implies that the distribution has a longer or fatter left tail relative to the right. In economic terms, negative skewness means that large losses are more likely than large gains of equal magnitude. The standardized fourth moment (excess kurtosis) is:\n\n$$\n\\text{Kurt}(r_i) = \\frac{E\\left[(r_{i,t} - \\mu_i)^4\\right]}{\\sigma_i^4} - 3\n$$ {#eq-kurtosis}\n\nPositive excess kurtosis indicates heavier tails than the normal distribution. Both moments have direct asset pricing implications: @harvey2000conditional and @kraus1976skewness argue that investors demand compensation for holding negatively skewed assets, implying that crash-prone stocks should earn higher expected returns, ceteris paribus.\n\n### Crash Risk Measures\n\nThe literature has developed several firm-specific crash risk measures. We implement the three most widely used, following @chen2001forecasting and @hutton2009opaque.\n\n**Measure 1: Negative Coefficient of Skewness (NCSKEW)**\n\nFor each firm $i$ in fiscal year $y$, compute firm-specific weekly returns $W_{i,\\tau}$ as residuals from a market model augmented with lead and lag market returns to account for nonsynchronous trading:\n\n$$\nr_{i,t} = \\alpha_i + \\beta_{1i} r_{m,t-1} + \\beta_{2i} r_{m,t} + \\beta_{3i} r_{m,t+1} + \\varepsilon_{i,t}\n$$ {#eq-crash-model}\n\nwhere $r_{m,t}$ is the value-weighted market return on day $t$. The firm-specific weekly return is $W_{i,\\tau} = \\ln(1 + \\sum_{t \\in \\tau} \\hat{\\varepsilon}_{i,t})$, where $\\tau$ indexes weeks. NCSKEW is then:\n\n$$\n\\text{NCSKEW}_{i,y} = -\\frac{n(n-1)^{3/2} \\sum W_{i,\\tau}^3}{(n-1)(n-2)\\left(\\sum W_{i,\\tau}^2\\right)^{3/2}}\n$$ {#eq-ncskew}\n\nwhere $n$ is the number of firm-specific weekly returns in the year. The negative sign ensures that higher NCSKEW corresponds to greater crash risk.\n\n**Measure 2: Down-to-Up Volatility (DUVOL)**\n\nPartition the firm-specific weekly returns into \"down weeks\" ($W_{i,\\tau} < \\bar{W}_i$) and \"up weeks\" ($W_{i,\\tau} \\geq \\bar{W}_i$), where $\\bar{W}_i$ is the annual mean. Then:\n\n$$\n\\text{DUVOL}_{i,y} = \\ln\\left(\\frac{(n_u - 1)\\sum_{\\text{down}} W_{i,\\tau}^2}{(n_d - 1)\\sum_{\\text{up}} W_{i,\\tau}^2}\\right)\n$$ {#eq-duvol}\n\nwhere $n_u$ and $n_d$ are the number of up and down weeks, respectively. Higher DUVOL indicates greater crash risk. @chen2001forecasting argue that DUVOL is less sensitive to outliers than NCSKEW because it does not involve the cube of returns.\n\n**Measure 3: Crash Count (COUNT)**\n\nDefine a crash event as a firm-specific weekly return that falls below $k$ standard deviations of its annual distribution:\n\n$$\n\\text{CRASH}_{i,\\tau} = \\mathbb{1}\\left(W_{i,\\tau} < \\bar{W}_i - k \\cdot \\hat{\\sigma}_{W_i}\\right)\n$$ {#eq-crash-event}\n\nThe standard threshold in the literature is $k = 3.09$, corresponding to the 0.1% tail of a standard normal distribution. The crash count for year $y$ is $\\text{COUNT}_{i,y} = \\sum_{\\tau \\in y} \\text{CRASH}_{i,\\tau}$.\n\n::: {#load-data .cell execution_count=3}\n``` {.python .cell-code}\n# DataCore.vn API\nfrom datacore import DataCore\ndc = DataCore()\n\n# Load daily stock returns and market returns\ndaily_returns = dc.get_daily_returns(\n    start_date=\"2010-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Market return: value-weighted index\nmarket_returns = dc.get_market_returns(\n    start_date=\"2010-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\n# Merge\ndf = daily_returns.merge(\n    market_returns[[\"date\", \"mkt_ret\"]],\n    on=\"date\",\n    how=\"left\"\n)\n\nprint(f\"Universe: {df['ticker'].nunique()} firms, \"\n      f\"{df['date'].nunique()} trading days\")\n```\n:::\n\n\n::: {#firm-specific-returns .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\n\ndef compute_firm_specific_returns(group):\n    \"\"\"\n    Estimate firm-specific daily returns from augmented market model\n    with lead and lag market returns (Chen, Hong, Stein 2001).\n    \"\"\"\n    g = group.sort_values(\"date\").copy()\n    g[\"mkt_lag1\"] = g[\"mkt_ret\"].shift(1)\n    g[\"mkt_lead1\"] = g[\"mkt_ret\"].shift(-1)\n    g = g.dropna(subset=[\"ret\", \"mkt_ret\", \"mkt_lag1\", \"mkt_lead1\"])\n\n    if len(g) < 30:\n        return pd.DataFrame()\n\n    X = g[[\"mkt_lag1\", \"mkt_ret\", \"mkt_lead1\"]].values\n    y = g[\"ret\"].values\n\n    model = LinearRegression().fit(X, y)\n    g[\"resid\"] = y - model.predict(X)\n\n    return g[[\"ticker\", \"date\", \"resid\"]]\n\n# Estimate by firm-year\ndf[\"year\"] = df[\"date\"].dt.year\nfirm_resids = (\n    df.groupby([\"ticker\", \"year\"], group_keys=False)\n    .apply(compute_firm_specific_returns)\n    .reset_index(drop=True)\n)\n```\n:::\n\n\n::: {#weekly-aggregation .cell execution_count=5}\n``` {.python .cell-code}\n# Aggregate daily residuals to weekly firm-specific returns\nfirm_resids[\"date\"] = pd.to_datetime(firm_resids[\"date\"])\nfirm_resids[\"week\"] = firm_resids[\"date\"].dt.isocalendar().week.astype(int)\nfirm_resids[\"year\"] = firm_resids[\"date\"].dt.year\n\nweekly_returns = (\n    firm_resids.groupby([\"ticker\", \"year\", \"week\"])\n    .agg(W=(\"resid\", lambda x: np.log(1 + x.sum())))\n    .reset_index()\n)\n```\n:::\n\n\n::: {#crash-risk-measures .cell execution_count=6}\n``` {.python .cell-code}\ndef compute_crash_measures(group):\n    \"\"\"Compute NCSKEW, DUVOL, and CRASH count for one firm-year.\"\"\"\n    W = group[\"W\"].values\n    n = len(W)\n\n    if n < 26:  # Require at least 26 weeks\n        return pd.Series({\n            \"ncskew\": np.nan, \"duvol\": np.nan,\n            \"crash_count\": np.nan, \"n_weeks\": n\n        })\n\n    # NCSKEW\n    W_demean = W - W.mean()\n    num = n * (n - 1)**1.5 * np.sum(W_demean**3)\n    den = (n - 1) * (n - 2) * (np.sum(W_demean**2))**1.5\n    ncskew = -(num / den) if den != 0 else np.nan\n\n    # DUVOL\n    mean_W = W.mean()\n    down = W[W < mean_W]\n    up = W[W >= mean_W]\n    n_d, n_u = len(down), len(up)\n\n    if n_d > 1 and n_u > 1:\n        duvol = np.log(\n            ((n_u - 1) * np.sum(down**2)) /\n            ((n_d - 1) * np.sum(up**2))\n        )\n    else:\n        duvol = np.nan\n\n    # Crash count (k = 3.09)\n    threshold = W.mean() - 3.09 * W.std()\n    crash_count = int(np.sum(W < threshold))\n\n    return pd.Series({\n        \"ncskew\": ncskew,\n        \"duvol\": duvol,\n        \"crash_count\": crash_count,\n        \"n_weeks\": n\n    })\n\ncrash_risk = (\n    weekly_returns.groupby([\"ticker\", \"year\"])\n    .apply(compute_crash_measures)\n    .reset_index()\n)\n\ncrash_risk = crash_risk.dropna(subset=[\"ncskew\", \"duvol\"])\nprint(f\"Crash risk panel: {len(crash_risk)} firm-years\")\n```\n:::\n\n\n### Cross-Sectional Distribution of Crash Risk\n\n@tbl-crash-summary presents summary statistics for the three crash risk measures across the Vietnamese equity market.\n\n::: {#tbl-crash-summary .cell tbl-cap='Summary Statistics of Crash Risk Measures' execution_count=7}\n``` {.python .cell-code}\nsummary = crash_risk[[\"ncskew\", \"duvol\", \"crash_count\"]].describe(\n    percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]\n).T.round(4)\n\nsummary.columns = [\"N\", \"Mean\", \"Std\", \"Min\", \"5%\", \"25%\",\n                    \"Median\", \"75%\", \"95%\", \"Max\"]\nsummary\n```\n:::\n\n\n::: {#fig-ncskew-distribution .cell execution_count=8}\n``` {.python .cell-code}\nplot_data = crash_risk[crash_risk[\"year\"].between(2012, 2024)].copy()\n\n(\n    p9.ggplot(plot_data, p9.aes(x=\"ncskew\"))\n    + p9.geom_histogram(bins=50, fill=\"#2E5090\", alpha=0.7)\n    + p9.facet_wrap(\"~year\", scales=\"free_y\", ncol=4)\n    + p9.geom_vline(xintercept=0, linetype=\"dashed\", color=\"red\", size=0.5)\n    + p9.labs(\n        x=\"NCSKEW\",\n        y=\"Count\",\n        title=\"Distribution of Firm-Level Crash Risk (NCSKEW)\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 8))\n)\n```\n:::\n\n\nThe rightward shift of the NCSKEW distribution during crisis episodes, particularly in 2020 and 2022, indicates a market-wide increase in crash risk. The red dashed line at zero separates firms with negative skewness (right of zero, i.e., high NCSKEW since the measure is negated) from those with positive skewness.\n\n### Interpretation in Asset Pricing\n\nCrash risk has first-order implications for asset pricing. @harvey2000conditional demonstrate that coskewness (i.e., the contribution of an individual asset to the skewness of the aggregate market portfolio) is priced in the cross-section of expected returns. Specifically, assets that tend to crash when the market crashes (negative coskewness) should command a risk premium.\n\nThe coskewness of stock $i$ with the market is:\n\n$$\n\\text{CoSkew}_i = \\frac{E\\left[(r_i - \\mu_i)(r_m - \\mu_m)^2\\right]}{\\sigma_i \\cdot \\sigma_m^2}\n$$ {#eq-coskewness}\n\nStocks with more negative coskewness exhibit disproportionately poor performance during market downturns. In Vietnam, this is especially relevant because: (1) retail investor herding amplifies co-movement during selloffs; (2) price limits delay crash completion, spreading crash risk across multiple days; and (3) state-owned enterprises, which constitute a large fraction of market capitalization, may exhibit coordinated crash behavior driven by common policy shocks.\n\n::: {#coskewness-estimation .cell execution_count=9}\n``` {.python .cell-code}\ndef compute_coskewness(group):\n    \"\"\"Estimate coskewness for a firm-year.\"\"\"\n    r_i = group[\"ret\"].values\n    r_m = group[\"mkt_ret\"].values\n\n    if len(r_i) < 60:\n        return np.nan\n\n    r_i_dm = r_i - r_i.mean()\n    r_m_dm = r_m - r_m.mean()\n\n    coskew = (\n        np.mean(r_i_dm * r_m_dm**2) /\n        (np.std(r_i) * np.std(r_m)**2)\n    )\n    return coskew\n\ndf_coskew = (\n    df.groupby([\"ticker\", \"year\"])\n    .apply(lambda g: pd.Series({\"coskewness\": compute_coskewness(g)}))\n    .reset_index()\n    .dropna()\n)\n```\n:::\n\n\n::: {#fig-coskew-time .cell execution_count=10}\n``` {.python .cell-code}\nmedian_coskew = (\n    df_coskew.groupby(\"year\")\n    .agg(\n        median_coskew=(\"coskewness\", \"median\"),\n        q25=(\"coskewness\", lambda x: x.quantile(0.25)),\n        q75=(\"coskewness\", lambda x: x.quantile(0.75))\n    )\n    .reset_index()\n)\n\n(\n    p9.ggplot(median_coskew, p9.aes(x=\"year\"))\n    + p9.geom_ribbon(\n        p9.aes(ymin=\"q25\", ymax=\"q75\"),\n        fill=\"#2E5090\", alpha=0.2\n    )\n    + p9.geom_line(p9.aes(y=\"median_coskew\"), color=\"#2E5090\", size=1)\n    + p9.geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\")\n    + p9.labs(\n        x=\"Year\",\n        y=\"Coskewness\",\n        title=\"Cross-Sectional Coskewness: Median and Interquartile Range\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n```\n:::\n\n\n### Downside Tail Concentration\n\nBeyond skewness, the concentration of returns in the extreme left tail provides additional diagnostic information. We define the downside tail concentration ratio as the fraction of total return variance attributable to observations below the $p$-th percentile:\n\n$$\n\\text{TCR}_p = \\frac{\\sum_{r_{i,t} < q_p} (r_{i,t} - \\bar{r}_i)^2}{\\sum_t (r_{i,t} - \\bar{r}_i)^2}\n$$ {#eq-tail-concentration}\n\nwhere $q_p$ is the $p$-th percentile of the firm's return distribution. Under a symmetric distribution, $\\text{TCR}_{5\\%}$ would be close to $5\\%$ (with a slight upward bias due to the squaring). Values substantially exceeding $5\\%$ indicate that the left tail contributes disproportionately to total risk.\n\n::: {#tail-concentration .cell execution_count=11}\n``` {.python .cell-code}\ndef tail_concentration_ratio(returns, p=0.05):\n    \"\"\"Compute tail concentration ratio at percentile p.\"\"\"\n    q = np.quantile(returns, p)\n    r_dm = returns - returns.mean()\n    total_var = np.sum(r_dm**2)\n    tail_var = np.sum(r_dm[returns < q]**2)\n    return tail_var / total_var if total_var > 0 else np.nan\n\ntcr = (\n    df.groupby([\"ticker\", \"year\"])\n    .agg(\n        tcr_5=(\"ret\", lambda x: tail_concentration_ratio(x.values, 0.05)),\n        tcr_1=(\"ret\", lambda x: tail_concentration_ratio(x.values, 0.01)),\n        n_obs=(\"ret\", \"count\")\n    )\n    .reset_index()\n    .query(\"n_obs >= 120\")\n)\n\nprint(\"Median TCR(5%):\", round(tcr[\"tcr_5\"].median(), 4))\nprint(\"Median TCR(1%):\", round(tcr[\"tcr_1\"].median(), 4))\n```\n:::\n\n\n## Value at Risk and Expected Shortfall\n\n### Definitions\n\nValue at Risk (VaR) and Expected Shortfall (ES) are the two principal quantile-based risk measures used in financial regulation and internal risk management. For a portfolio return $r$ with the cumulative distribution function $F$, the VaR at confidence level $\\alpha$ is:\n\n$$\n\\text{VaR}_\\alpha = -F^{-1}(\\alpha) = -\\inf\\{x : F(x) \\geq \\alpha\\}\n$$ {#eq-var-def}\n\nThis is simply the negative of the $\\alpha$-quantile of the return distribution. For $\\alpha = 0.01$, $\\text{VaR}_{1\\%}$ answers: \"What is the loss that is exceeded with only 1% probability?\"\n\nExpected Shortfall (also called Conditional VaR or CVaR) is the expected loss conditional on the loss exceeding VaR:\n\n$$\n\\text{ES}_\\alpha = -\\frac{1}{\\alpha}\\int_0^\\alpha F^{-1}(u) \\, du = -E\\left[r \\mid r \\leq -\\text{VaR}_\\alpha\\right]\n$$ {#eq-es-def}\n\nFor continuous distributions, ES simplifies to the conditional expectation in @eq-es-def. ES is always at least as large as VaR ($\\text{ES}_\\alpha \\geq \\text{VaR}_\\alpha$) and is strictly larger whenever the distribution has any mass beyond the VaR quantile.\n\n### Why Expected Shortfall Dominates VaR\n\n@tbl-tail-riskvar-vs-es summarizes the fundamental differences.\n\n| Property | VaR | Expected Shortfall |\n|-------------------|-------------------|----------------------------------|\n| Definition | Quantile of the loss distribution | Conditional mean loss beyond VaR |\n| Tail information | None beyond the quantile | Captures severity of tail losses |\n| Subadditivity | Not subadditive in general | Subadditive (coherent risk measure) |\n| Regulatory status | Basel II primary measure | Basel III/IV primary measure |\n| Backtesting | Binary: breach or no breach | Continuous: requires severity assessment |\n| Sensitivity to tail shape | None | Directly sensitive |\n\n: Comparison of VaR and Expected Shortfall {#tbl-tail-riskvar-vs-es}\n\nThe critical deficiency of VaR is the absence of subadditivity. A risk measure $\\rho$ is subadditive if $\\rho(X + Y) \\leq \\rho(X) + \\rho(Y)$ for all portfolios $X, Y$. VaR violates this condition for non-elliptical distributions, meaning that diversification can appear to increase risk under VaR, which is a perverse result. @artzner1999coherent established the axiomatic framework for coherent risk measures, and ES satisfies all four axioms: monotonicity, translation invariance, positive homogeneity, and subadditivity. VaR satisfies only three.\n\n### Estimation Methods\n\nWe implement four approaches to VaR and ES estimation, each with distinct assumptions and data requirements.\n\n**Method 1: Historical Simulation**\n\nThe simplest nonparametric approach uses the empirical distribution directly:\n\n$$\n\\widehat{\\text{VaR}}_\\alpha^{\\text{HS}} = -\\hat{F}_n^{-1}(\\alpha) = -r_{(\\lceil n\\alpha \\rceil)}\n$$ {#eq-var-hs}\n\nwhere $r_{(k)}$ is the $k$-th order statistic of the return sample. ES is the average of all returns below the VaR:\n\n$$\n\\widehat{\\text{ES}}_\\alpha^{\\text{HS}} = -\\frac{1}{\\lfloor n\\alpha \\rfloor}\\sum_{k=1}^{\\lfloor n\\alpha \\rfloor} r_{(k)}\n$$ {#eq-es-hs}\n\n**Method 2: Parametric (Gaussian)**\n\nAssume $r_t \\sim N(\\mu, \\sigma^2)$. Then:\n\n$$\n\\text{VaR}_\\alpha^{\\text{Gauss}} = -(\\hat{\\mu} + \\hat{\\sigma} \\cdot \\Phi^{-1}(\\alpha))\n$$ {#eq-var-gauss}\n\n$$\n\\text{ES}_\\alpha^{\\text{Gauss}} = -\\hat{\\mu} + \\hat{\\sigma} \\cdot \\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha}\n$$ {#eq-es-gauss}\n\nwhere $\\Phi$ and $\\phi$ are the standard normal CDF and PDF, respectively. The Gaussian assumption is known to underestimate tail risk for equity returns, but provides a useful baseline.\n\n**Method 3: Parametric (Student-**$t$)\n\nThe Student-$t$ distribution with $\\nu$ degrees of freedom captures excess kurtosis. The VaR and ES are:\n\n$$\n\\text{VaR}_\\alpha^{t} = -\\left(\\hat{\\mu} + \\hat{\\sigma}\\sqrt{\\frac{\\nu - 2}{\\nu}} \\cdot t_\\nu^{-1}(\\alpha)\\right)\n$$ {#eq-var-t}\n\n$$\n\\text{ES}_\\alpha^{t} = -\\hat{\\mu} + \\hat{\\sigma}\\sqrt{\\frac{\\nu-2}{\\nu}} \\cdot \\frac{f_{t_\\nu}(t_\\nu^{-1}(\\alpha))}{\\alpha} \\cdot \\frac{\\nu + (t_\\nu^{-1}(\\alpha))^2}{\\nu - 1}\n$$ {#eq-es-t}\n\nwhere $t_\\nu^{-1}$ and $f_{t_\\nu}$ are the quantile function and PDF of the Student-$t$ distribution.\n\n**Method 4: GARCH-Filtered EVT (Semi-Parametric)**\n\nThis approach, developed by @mcneil2000estimation, first filters returns through a GARCH(1,1) model to obtain standardized residuals $z_t = \\hat{\\varepsilon}_t / \\hat{\\sigma}_t$, then fits a Generalized Pareto Distribution (GPD) to the lower tail of the standardized residuals. We detail the EVT component in @sec-evt.\n\n::: {#var-es-estimation .cell execution_count=12}\n``` {.python .cell-code}\ndef estimate_var_es(returns, alpha=0.01):\n    \"\"\"\n    Estimate VaR and ES at level alpha using four methods.\n\n    Parameters\n    ----------\n    returns : array-like\n        Return series.\n    alpha : float\n        Tail probability (e.g., 0.01 for 1%).\n\n    Returns\n    -------\n    dict : VaR and ES estimates for each method.\n    \"\"\"\n    r = np.array(returns)\n    r = r[~np.isnan(r)]\n    n = len(r)\n    mu, sigma = r.mean(), r.std(ddof=1)\n\n    results = {}\n\n    # Method 1: Historical simulation\n    sorted_r = np.sort(r)\n    idx = int(np.ceil(n * alpha))\n    results[\"var_hs\"] = -sorted_r[idx - 1]\n    results[\"es_hs\"] = -sorted_r[:idx].mean()\n\n    # Method 2: Gaussian\n    z_alpha = stats.norm.ppf(alpha)\n    results[\"var_gauss\"] = -(mu + sigma * z_alpha)\n    results[\"es_gauss\"] = -mu + sigma * stats.norm.pdf(z_alpha) / alpha\n\n    # Method 3: Student-t (MLE for degrees of freedom)\n    try:\n        nu, loc, scale = stats.t.fit(r)\n        t_alpha = stats.t.ppf(alpha, df=nu)\n        results[\"var_t\"] = -(loc + scale * t_alpha)\n        results[\"es_t\"] = (\n            -loc + scale *\n            (stats.t.pdf(t_alpha, df=nu) / alpha) *\n            ((nu + t_alpha**2) / (nu - 1))\n        )\n        results[\"nu_hat\"] = nu\n    except Exception:\n        results[\"var_t\"] = np.nan\n        results[\"es_t\"] = np.nan\n        results[\"nu_hat\"] = np.nan\n\n    return results\n```\n:::\n\n\n::: {#market-var-es .cell execution_count=13}\n``` {.python .cell-code}\n# Compute VaR and ES for the aggregate market index\nmarket_daily = dc.get_market_returns(\n    start_date=\"2010-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\n# Rolling 1-year VaR/ES\nwindow = 252\nresults_list = []\n\nfor end_idx in range(window, len(market_daily)):\n    window_returns = market_daily[\"mkt_ret\"].iloc[end_idx - window:end_idx].values\n    date = market_daily[\"date\"].iloc[end_idx]\n\n    est = estimate_var_es(window_returns, alpha=0.01)\n    est[\"date\"] = date\n    results_list.append(est)\n\nvar_es_ts = pd.DataFrame(results_list)\n```\n:::\n\n\n::: {#fig-var-comparison .cell execution_count=14}\n``` {.python .cell-code}\nplot_var = var_es_ts.melt(\n    id_vars=\"date\",\n    value_vars=[\"var_hs\", \"var_gauss\", \"var_t\"],\n    var_name=\"method\",\n    value_name=\"var\"\n)\n\nmethod_labels = {\n    \"var_hs\": \"Historical Simulation\",\n    \"var_gauss\": \"Gaussian\",\n    \"var_t\": \"Student-t\"\n}\nplot_var[\"method\"] = plot_var[\"method\"].map(method_labels)\n\n(\n    p9.ggplot(plot_var, p9.aes(x=\"date\", y=\"var\", color=\"method\"))\n    + p9.geom_line(alpha=0.8, size=0.6)\n    + p9.labs(\n        x=\"\", y=\"1% VaR (Loss)\",\n        title=\"Rolling 252-Day Value at Risk Estimates\",\n        color=\"Method\"\n    )\n    + p9.scale_color_manual(\n        values=[\"#2E5090\", \"#C0392B\", \"#27AE60\"]\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5), legend_position=\"top\")\n)\n```\n:::\n\n\n::: {#fig-es-comparison .cell execution_count=15}\n``` {.python .cell-code}\nplot_es = var_es_ts.melt(\n    id_vars=\"date\",\n    value_vars=[\"es_hs\", \"es_gauss\", \"es_t\"],\n    var_name=\"method\",\n    value_name=\"es\"\n)\n\nmethod_labels_es = {\n    \"es_hs\": \"Historical Simulation\",\n    \"es_gauss\": \"Gaussian\",\n    \"es_t\": \"Student-t\"\n}\nplot_es[\"method\"] = plot_es[\"method\"].map(method_labels_es)\n\n(\n    p9.ggplot(plot_es, p9.aes(x=\"date\", y=\"es\", color=\"method\"))\n    + p9.geom_line(alpha=0.8, size=0.6)\n    + p9.labs(\n        x=\"\", y=\"1% ES (Loss)\",\n        title=\"Rolling 252-Day Expected Shortfall Estimates\",\n        color=\"Method\"\n    )\n    + p9.scale_color_manual(\n        values=[\"#2E5090\", \"#C0392B\", \"#27AE60\"]\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5), legend_position=\"top\")\n)\n```\n:::\n\n\n### VaR Backtesting\n\nA VaR model is only useful if its predictions are accurate. Backtesting evaluates whether the realized frequency of VaR breaches is consistent with the nominal confidence level. If VaR is estimated at the $\\alpha = 1\\%$ level, we expect approximately 1% of days to exhibit losses exceeding VaR.\n\nThe Kupiec unconditional coverage test [@kupiec1995techniques] evaluates whether the total number of breaches $x$ in $n$ observations is consistent with $\\alpha$:\n\n$$\n\\text{LR}_{\\text{UC}} = -2\\ln\\left(\\frac{\\alpha^x (1-\\alpha)^{n-x}}{\\hat{p}^x (1-\\hat{p})^{n-x}}\\right) \\sim \\chi^2(1)\n$$ {#eq-kupiec}\n\nwhere $\\hat{p} = x/n$ is the empirical breach frequency. The Christoffersen conditional coverage test [@christoffersen1998evaluating] additionally evaluates whether breaches are independent (no clustering):\n\n$$\n\\text{LR}_{\\text{CC}} = \\text{LR}_{\\text{UC}} + \\text{LR}_{\\text{IND}}\n$$ {#eq-christoffersen}\n\nwhere $\\text{LR}_{\\text{IND}}$ tests the null of independence using a first-order Markov chain model for breach indicators.\n\n::: {#backtesting .cell execution_count=16}\n``` {.python .cell-code}\ndef kupiec_test(returns, var_estimates, alpha=0.01):\n    \"\"\"\n    Kupiec unconditional coverage test for VaR backtesting.\n\n    Returns\n    -------\n    dict : breach count, expected, breach rate, LR statistic, p-value.\n    \"\"\"\n    breaches = returns < -var_estimates\n    x = breaches.sum()\n    n = len(returns)\n    p_hat = x / n\n\n    if p_hat == 0 or p_hat == 1:\n        return {\"breaches\": x, \"expected\": n * alpha,\n                \"breach_rate\": p_hat, \"lr_stat\": np.nan, \"p_value\": np.nan}\n\n    lr = -2 * (\n        x * np.log(alpha) + (n - x) * np.log(1 - alpha)\n        - x * np.log(p_hat) - (n - x) * np.log(1 - p_hat)\n    )\n    p_value = 1 - stats.chi2.cdf(lr, df=1)\n\n    return {\n        \"breaches\": int(x),\n        \"expected\": round(n * alpha, 1),\n        \"breach_rate\": round(p_hat, 4),\n        \"lr_stat\": round(lr, 4),\n        \"p_value\": round(p_value, 4)\n    }\n\n# Backtest each method\nactual_returns = market_daily[\"mkt_ret\"].iloc[window:].values\n\nbacktest_results = {}\nfor method, col in [(\"Historical\", \"var_hs\"),\n                     (\"Gaussian\", \"var_gauss\"),\n                     (\"Student-t\", \"var_t\")]:\n    backtest_results[method] = kupiec_test(\n        actual_returns, var_es_ts[col].values, alpha=0.01\n    )\n\nbt_df = pd.DataFrame(backtest_results).T\nbt_df.index.name = \"Method\"\nbt_df\n```\n:::\n\n\n### Estimation Under Limited Data {#sec-limited-data}\n\nIn Vietnamese equity markets, many stocks have short trading histories, thin liquidity, and censored returns due to price limits. These features create challenges for tail risk estimation that are less severe in developed markets.\n\n**Price limit censoring.** When a stock hits its daily price limit, the observed return is truncated. The true latent return may extend well beyond the limit, but is unobservable. This means that historical simulation mechanically understates VaR and ES for stocks that frequently hit limits. A Tobit-type correction can partially address this:\n\n$$\nr_{i,t}^{\\text{latent}} \\sim N(\\mu_i, \\sigma_i^2), \\quad r_{i,t}^{\\text{obs}} = \\max(\\underline{L}, \\min(\\bar{L}, r_{i,t}^{\\text{latent}}))\n$$ {#eq-tobit-var}\n\nThe parameters $(\\mu_i, \\sigma_i^2)$ can be estimated via maximum likelihood on the censored sample, and VaR/ES can then be computed from the estimated latent distribution.\n\n**Short histories.** For recently listed stocks, the available sample may be too short for reliable nonparametric estimation. In these cases, shrinkage toward a cross-sectional prior (e.g., the sector median VaR) or Bayesian approaches with informative priors can improve stability.\n\n## Extreme Value Theory {#sec-evt}\n\n### Motivation: What EVT Captures That GARCH Does Not\n\nGARCH models capture volatility clustering (i.e., the tendency of large returns to be followed by large returns), but they make specific distributional assumptions about standardized residuals (typically Gaussian or Student-$t$). The tail behavior of the conditional distribution is thus determined by the parametric innovation assumption, not learned from the data.\n\nExtreme Value Theory (EVT) takes a fundamentally different approach. It provides a statistical framework for estimating the distribution of extreme observations without specifying the entire return distribution. The key theoretical results (i.e., the Fisher-Tippett-Gnedenko theorem and the Pickands-Balkema-de Haan theorem) establish that the distribution of properly normalized maxima (or exceedances over high thresholds) converges to specific parametric families regardless of the parent distribution. This universality is what makes EVT powerful: the tail can be estimated even when the center of the distribution is misspecified.\n\n### Tail Index Estimation\n\nThe tail index $\\xi$ (also denoted $\\alpha^{-1}$ in some formulations) governs the rate of tail decay. For heavy-tailed distributions, the survival function satisfies:\n\n$$\nP(X > x) \\sim L(x) \\cdot x^{-1/\\xi}, \\quad x \\to \\infty\n$$ {#eq-tail-decay}\n\nwhere $L(x)$ is a slowly varying function. The tail index $\\xi > 0$ implies a Pareto-type tail; larger $\\xi$ means heavier tails. Financial return distributions typically have $\\xi \\in (0.2, 0.5)$, corresponding to tail indices $\\alpha = 1/\\xi \\in (2, 5)$, which implies finite variance but potentially infinite higher moments.\n\nThe Hill estimator [@hill1975simple] provides a simple nonparametric estimate of $\\xi$ from the upper order statistics:\n\n$$\n\\hat{\\xi}_{\\text{Hill}}(k) = \\frac{1}{k}\\sum_{j=1}^{k} \\ln X_{(n-j+1)} - \\ln X_{(n-k)}\n$$ {#eq-hill}\n\nwhere $X_{(1)} \\leq \\cdots \\leq X_{(n)}$ are the order statistics and $k$ is the number of upper order statistics used. The choice of $k$ involves a bias-variance tradeoff: small $k$ yields high variance; large $k$ introduces bias from non-tail observations.\n\n::: {#hill-estimator .cell execution_count=17}\n``` {.python .cell-code}\ndef hill_estimator(data, k_values=None):\n    \"\"\"\n    Compute Hill estimator of tail index for a range of k values.\n\n    Parameters\n    ----------\n    data : array-like\n        Positive values (use absolute values of losses).\n    k_values : array-like, optional\n        Number of upper order statistics to use.\n\n    Returns\n    -------\n    DataFrame with columns [k, xi_hat, se].\n    \"\"\"\n    x = np.sort(np.abs(data))[::-1]  # Descending order\n    n = len(x)\n\n    if k_values is None:\n        k_values = range(10, min(n // 2, 500), 5)\n\n    results = []\n    for k in k_values:\n        if k >= n or k < 2:\n            continue\n        log_excess = np.log(x[:k]) - np.log(x[k])\n        xi_hat = log_excess.mean()\n        se = xi_hat / np.sqrt(k)\n        results.append({\"k\": k, \"xi_hat\": xi_hat, \"se\": se})\n\n    return pd.DataFrame(results)\n```\n:::\n\n\n::: {#fig-hill-plot .cell execution_count=18}\n``` {.python .cell-code}\n# Use negative market returns (losses)\nlosses = -market_daily[\"mkt_ret\"].dropna().values\nlosses_positive = losses[losses > 0]\n\nhill_results = hill_estimator(losses_positive)\n\n(\n    p9.ggplot(hill_results, p9.aes(x=\"k\", y=\"xi_hat\"))\n    + p9.geom_ribbon(\n        p9.aes(ymin=\"xi_hat - 1.96 * se\", ymax=\"xi_hat + 1.96 * se\"),\n        fill=\"#2E5090\", alpha=0.2\n    )\n    + p9.geom_line(color=\"#2E5090\", size=0.8)\n    + p9.geom_hline(yintercept=0.33, linetype=\"dashed\", color=\"red\")\n    + p9.annotate(\n        \"text\", x=hill_results[\"k\"].max() * 0.7, y=0.35,\n        label=\"ξ = 1/3 (cubic tail)\", color=\"red\", size=8\n    )\n    + p9.labs(\n        x=\"Number of Order Statistics (k)\",\n        y=\"Hill Estimate ξ̂\",\n        title=\"Hill Plot for Market Return Left Tail\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n```\n:::\n\n\nThe Hill plot is a standard diagnostic in EVT. A stable region of the Hill estimate across a range of $k$ suggests a reliable tail index estimate. The red dashed line at $\\xi = 1/3$ corresponds to a tail index of $\\alpha = 3$, which implies that the third moment (skewness) is infinite, which is a finding consistent with much of the empirical finance literature [@cont2001empirical; @gabaix2003theory].\n\n### Block Maxima and the GEV Distribution\n\nThe Fisher-Tippett-Gnedenko theorem establishes that if the maximum of $n$ i.i.d. random variables, after proper normalization, converges in distribution, the limit must be a Generalized Extreme Value (GEV) distribution:\n\n$$\nG_\\xi(x) = \\exp\\left\\{-\\left(1 + \\xi \\cdot \\frac{x - \\mu}{\\sigma}\\right)^{-1/\\xi}\\right\\}\n$$ {#eq-gev}\n\nfor $1 + \\xi(x - \\mu)/\\sigma > 0$, where $\\mu$ is the location, $\\sigma > 0$ the scale, and $\\xi$ the shape parameter. The three sub-families are in @tbl-tail-riskgev-families.\n\n| Shape $\\xi$ | Distribution | Tail Type | Finance Example |\n|------------------|------------------|------------------|-------------------|\n| $\\xi > 0$ | Fréchet | Heavy (polynomial decay) | Equity returns, credit losses |\n| $\\xi = 0$ | Gumbel | Light (exponential decay) | Normal/lognormal models |\n| $\\xi < 0$ | Weibull | Bounded upper tail | Bounded loss distributions |\n\n: GEV Sub-Families and Financial Interpretation {#tbl-tail-riskgev-families}\n\nFor the block maxima approach, we divide the return series into non-overlapping blocks (typically months or quarters) and extract the minimum return (maximum loss) from each block. The GEV is then fitted to these block minima.\n\n::: {#block-maxima .cell execution_count=19}\n``` {.python .cell-code}\nfrom scipy.stats import genextreme\n\n# Monthly block minima (maximum losses)\nmarket_daily[\"month\"] = market_daily[\"date\"].dt.to_period(\"M\")\nblock_minima = (\n    market_daily.groupby(\"month\")\n    .agg(min_ret=(\"mkt_ret\", \"min\"))\n    .reset_index()\n)\n\n# Fit GEV to negated block minima (so we model maxima of losses)\nblock_losses = -block_minima[\"min_ret\"].values\nxi_hat, loc_hat, scale_hat = genextreme.fit(block_losses)\n\nprint(f\"GEV fit to monthly block maxima of losses:\")\nprint(f\"  Shape (ξ):    {xi_hat:.4f}\")\nprint(f\"  Location (μ): {loc_hat:.4f}\")\nprint(f\"  Scale (σ):    {scale_hat:.4f}\")\n```\n:::\n\n\n::: {#fig-gev-fit .cell execution_count=20}\n``` {.python .cell-code}\nx_grid = np.linspace(\n    block_losses.min() * 0.8,\n    block_losses.max() * 1.2,\n    200\n)\ngev_pdf = genextreme.pdf(x_grid, xi_hat, loc=loc_hat, scale=scale_hat)\n\nplot_data = pd.DataFrame({\n    \"x\": np.concatenate([block_losses, x_grid]),\n    \"source\": ([\"Empirical\"] * len(block_losses) +\n               [\"GEV Fit\"] * len(x_grid)),\n    \"density\": np.concatenate([\n        np.full(len(block_losses), np.nan),\n        gev_pdf\n    ]),\n    \"value\": np.concatenate([block_losses, np.full(len(x_grid), np.nan)])\n})\n\nempirical = plot_data[plot_data[\"source\"] == \"Empirical\"]\nfitted = plot_data[plot_data[\"source\"] == \"GEV Fit\"]\n\n(\n    p9.ggplot()\n    + p9.geom_histogram(\n        data=empirical, mapping=p9.aes(x=\"value\", y=\"..density..\"),\n        bins=30, fill=\"#2E5090\", alpha=0.5\n    )\n    + p9.geom_line(\n        data=fitted, mapping=p9.aes(x=\"x\", y=\"density\"),\n        color=\"#C0392B\", size=1\n    )\n    + p9.labs(\n        x=\"Monthly Maximum Loss\",\n        y=\"Density\",\n        title=\"GEV Distribution Fit to Monthly Block Maxima\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n```\n:::\n\n\n### Peaks Over Threshold and the GPD {#sec-gpd}\n\nThe Peaks Over Threshold (POT) approach is generally preferred over block maxima because it uses tail data more efficiently. The Pickands-Balkema-de Haan theorem establishes that for a sufficiently high threshold $u$, the distribution of exceedances $Y = X - u \\mid X > u$ converges to a Generalized Pareto Distribution (GPD):\n\n$$\nH_{\\xi,\\beta}(y) = \\begin{cases}\n1 - \\left(1 + \\frac{\\xi y}{\\beta}\\right)^{-1/\\xi} & \\text{if } \\xi \\neq 0 \\\\\n1 - \\exp\\left(-\\frac{y}{\\beta}\\right) & \\text{if } \\xi = 0\n\\end{cases}\n$$ {#eq-gpd}\n\nfor $y > 0$ and $1 + \\xi y / \\beta > 0$, where $\\beta > 0$ is the scale parameter and $\\xi$ is the shape parameter (identical to the GEV shape). The POT-based VaR and ES at level $\\alpha$ are:\n\n$$\n\\text{VaR}_\\alpha^{\\text{GPD}} = u + \\frac{\\hat{\\beta}}{\\hat{\\xi}}\\left[\\left(\\frac{n}{N_u} \\cdot \\alpha\\right)^{-\\hat{\\xi}} - 1\\right]\n$$ {#eq-var-gpd}\n\n$$\n\\text{ES}_\\alpha^{\\text{GPD}} = \\frac{\\text{VaR}_\\alpha^{\\text{GPD}}}{1 - \\hat{\\xi}} + \\frac{\\hat{\\beta} - \\hat{\\xi} u}{1 - \\hat{\\xi}}\n$$ {#eq-es-gpd}\n\nwhere $N_u$ is the number of observations exceeding $u$ and $n$ is the total sample size. These formulae are valid for $\\hat{\\xi} < 1$.\n\n::: {#gpd-estimation .cell execution_count=21}\n``` {.python .cell-code}\nfrom scipy.stats import genpareto\n\ndef fit_gpd_pot(returns, threshold_quantile=0.95):\n    \"\"\"\n    Fit GPD to exceedances over threshold using POT approach.\n\n    Parameters\n    ----------\n    returns : array-like\n        Return series (losses should be positive).\n    threshold_quantile : float\n        Quantile for threshold selection.\n\n    Returns\n    -------\n    dict : Threshold, GPD parameters, VaR, ES estimates.\n    \"\"\"\n    losses = -np.array(returns)\n    losses = losses[~np.isnan(losses)]\n    n = len(losses)\n\n    u = np.quantile(losses, threshold_quantile)\n    exceedances = losses[losses > u] - u\n    N_u = len(exceedances)\n\n    # Fit GPD\n    xi_hat, _, beta_hat = genpareto.fit(exceedances, floc=0)\n\n    # VaR and ES at 1%\n    alpha = 0.01\n    var_gpd = u + (beta_hat / xi_hat) * (\n        (n / N_u * alpha)**(-xi_hat) - 1\n    )\n    es_gpd = var_gpd / (1 - xi_hat) + (beta_hat - xi_hat * u) / (1 - xi_hat)\n\n    return {\n        \"threshold\": u,\n        \"n_exceedances\": N_u,\n        \"xi_hat\": xi_hat,\n        \"beta_hat\": beta_hat,\n        \"var_1pct\": var_gpd,\n        \"es_1pct\": es_gpd\n    }\n\n# Fit GPD to market losses\ngpd_results = fit_gpd_pot(market_daily[\"mkt_ret\"].dropna().values, 0.95)\n\nprint(\"GPD-POT Results (1% level):\")\nfor k, v in gpd_results.items():\n    if isinstance(v, float):\n        print(f\"  {k}: {v:.6f}\")\n    else:\n        print(f\"  {k}: {v}\")\n```\n:::\n\n\n### Threshold Selection\n\nThe choice of threshold $u$ is the key practical decision in POT analysis. Too low a threshold violates the asymptotic approximation; too high wastes data. The mean residual life plot provides a graphical diagnostic: if the GPD approximation holds above $u_0$, the mean excess function $e(u) = E[X - u \\mid X > u]$ is linear in $u$ for $u > u_0$:\n\n$$\ne(u) = \\frac{\\beta + \\xi u}{1 - \\xi}\n$$ {#eq-mean-excess}\n\n::: {#fig-mean-residual-life .cell execution_count=22}\n``` {.python .cell-code}\nlosses = -market_daily[\"mkt_ret\"].dropna().values\nlosses_sorted = np.sort(losses)\n\nthresholds = np.quantile(losses, np.linspace(0.80, 0.99, 50))\nmean_excess = []\n\nfor u in thresholds:\n    exceedances = losses[losses > u] - u\n    if len(exceedances) > 5:\n        mean_excess.append({\n            \"threshold\": u,\n            \"mean_excess\": exceedances.mean(),\n            \"se\": exceedances.std() / np.sqrt(len(exceedances)),\n            \"n_exceed\": len(exceedances)\n        })\n\nmrl_df = pd.DataFrame(mean_excess)\n\n(\n    p9.ggplot(mrl_df, p9.aes(x=\"threshold\", y=\"mean_excess\"))\n    + p9.geom_ribbon(\n        p9.aes(\n            ymin=\"mean_excess - 1.96 * se\",\n            ymax=\"mean_excess + 1.96 * se\"\n        ),\n        fill=\"#2E5090\", alpha=0.2\n    )\n    + p9.geom_line(color=\"#2E5090\", size=0.8)\n    + p9.geom_point(color=\"#2E5090\", size=1.5)\n    + p9.labs(\n        x=\"Threshold (u)\",\n        y=\"Mean Excess e(u)\",\n        title=\"Mean Residual Life Plot\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n```\n:::\n\n\nA threshold in the region where the mean residual life plot is approximately linear is appropriate. In practice, we also compare GPD parameter stability across a range of thresholds and select the lowest threshold at which estimates stabilize.\n\n### What EVT Captures That GARCH Does Not\n\n@tbl-tail-riskevt-vs-garch summarizes the complementary roles of these two modeling frameworks.\n\n| Feature | GARCH | EVT |\n|-----------------------------|------------------------|-------------------|\n| What is modeled | Conditional variance dynamics | Unconditional tail distribution |\n| Distributional assumption | Full distribution (Gaussian or $t$) | Only the tail (GPD or GEV) |\n| Time dependence | Explicit (volatility clustering) | None (i.i.d. or filtered) |\n| Tail shape | Determined by innovation distribution | Estimated from data |\n| Extreme quantiles | Extrapolation from parametric assumption | Principled extrapolation via EVT theorems |\n| Best use | Short-horizon risk forecasting | Extreme quantile estimation, stress testing |\n\n: GARCH vs. EVT: Complementary Frameworks {#tbl-tail-riskevt-vs-garch}\n\nThe GARCH-filtered EVT approach of @mcneil2000estimation combines both: GARCH captures the time-varying volatility, and EVT models the residual tail. This hybrid approach is considered best practice for financial risk management [@mcneil2015quantitative].\n\n## Tail Dependence and Contagion\n\n### Why Correlation Breaks Down in Crises\n\nPerhaps the most important practical fact about multivariate tail risk is that correlations estimated from the body of the distribution systematically understate dependence in the tails. @longin2001extreme demonstrate this using extreme value theory: for bivariate normal returns, the correlation of extreme realizations converges to zero as the threshold moves further into the tail. Yet empirically, the correlation of crash returns remains large and often increases (a phenomenon incompatible with multivariate normality).\n\nFormally, the coefficient of lower tail dependence between two return series $X$ and $Y$ is:\n\n$$\n\\lambda_L = \\lim_{q \\to 0^+} P\\left(Y \\leq F_Y^{-1}(q) \\mid X \\leq F_X^{-1}(q)\\right)\n$$ {#eq-tail-dependence}\n\nIf $\\lambda_L > 0$, the two series exhibit asymptotic tail dependence (i.e., extreme losses tend to occur jointly). For the bivariate normal distribution, $\\lambda_L = 0$ for all $|\\rho| < 1$ (asymptotic tail independence). This means that any model built on multivariate normality will structurally underestimate the probability of joint crashes. The Student-$t$ copula, by contrast, has $\\lambda_L > 0$ for $\\rho > -1$, which is why it has become the workhorse model for joint tail risk in finance [@demarta2005t].\n\n### Co-Crash Measures\n\nWe implement several empirical measures of joint crash behavior.\n\n**Exceedance Correlation.** Following @longin2001extreme, compute the correlation of returns conditional on both returns falling below a threshold $\\theta$:\n\n$$\n\\rho^-(\\theta) = \\text{Corr}(r_i, r_j \\mid r_i < \\theta, r_j < \\theta)\n$$ {#eq-exceedance-corr}\n\nIf returns are bivariate normal, $\\rho^-(\\theta) \\to 0$ as $\\theta \\to -\\infty$. Empirically, $\\rho^-(\\theta)$ typically increases as $\\theta$ decreases, indicating tail dependence beyond what the normal model implies.\n\n**Co-Exceedance Count.** For each day $t$, count the number of stocks or sectors whose returns fall below the $q$-th percentile of their respective marginal distributions:\n\n$$\nC_t(q) = \\sum_{i=1}^{N} \\mathbb{1}\\left(r_{i,t} < F_i^{-1}(q)\\right)\n$$ {#eq-coexceedance}\n\nDays with high $C_t(q)$ represent system-wide stress events.\n\n::: {#tail-dependence-estimation .cell execution_count=23}\n``` {.python .cell-code}\ndef empirical_tail_dependence(x, y, quantiles=None):\n    \"\"\"\n    Estimate lower tail dependence coefficient at various quantiles.\n\n    Uses the empirical copula approach: transform to uniform margins,\n    then estimate P(V <= q | U <= q) for decreasing q.\n    \"\"\"\n    if quantiles is None:\n        quantiles = [0.20, 0.15, 0.10, 0.05, 0.03, 0.01]\n\n    # Transform to uniform margins via empirical CDF\n    from scipy.stats import rankdata\n    n = len(x)\n    u = rankdata(x) / (n + 1)\n    v = rankdata(y) / (n + 1)\n\n    results = []\n    for q in quantiles:\n        mask_u = u <= q\n        mask_both = mask_u & (v <= q)\n\n        n_u = mask_u.sum()\n        n_both = mask_both.sum()\n\n        lambda_hat = n_both / n_u if n_u > 0 else np.nan\n\n        results.append({\n            \"quantile\": q,\n            \"lambda_L\": lambda_hat,\n            \"n_below_x\": int(n_u),\n            \"n_co_crash\": int(n_both)\n        })\n\n    return pd.DataFrame(results)\n```\n:::\n\n\n::: {#sector-tail-dependence .cell execution_count=24}\n``` {.python .cell-code}\n# Load sector-level daily returns\nsector_returns = dc.get_sector_returns(\n    start_date=\"2010-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\n# Compute pairwise tail dependence for major sectors\nsectors = sector_returns[\"sector\"].unique()[:8]  # Top 8 sectors\n\nimport itertools\n\npairwise_td = []\nfor s1, s2 in itertools.combinations(sectors, 2):\n    r1 = sector_returns.loc[\n        sector_returns[\"sector\"] == s1, [\"date\", \"ret\"]\n    ].set_index(\"date\")[\"ret\"]\n    r2 = sector_returns.loc[\n        sector_returns[\"sector\"] == s2, [\"date\", \"ret\"]\n    ].set_index(\"date\")[\"ret\"]\n\n    # Align dates\n    aligned = pd.concat([r1, r2], axis=1, join=\"inner\").dropna()\n    if len(aligned) < 500:\n        continue\n\n    td = empirical_tail_dependence(\n        aligned.iloc[:, 0].values,\n        aligned.iloc[:, 1].values,\n        quantiles=[0.05]\n    )\n\n    pairwise_td.append({\n        \"sector_1\": s1,\n        \"sector_2\": s2,\n        \"lambda_L_5pct\": td[\"lambda_L\"].iloc[0],\n        \"pearson_corr\": aligned.iloc[:, 0].corr(aligned.iloc[:, 1])\n    })\n\ntd_df = pd.DataFrame(pairwise_td)\n```\n:::\n\n\n::: {#tbl-tail-dependence .cell tbl-cap='Pairwise Tail Dependence vs. Linear Correlation Across Sectors' execution_count=25}\n``` {.python .cell-code}\ntd_display = td_df.copy()\ntd_display[\"lambda_L_5pct\"] = td_display[\"lambda_L_5pct\"].round(4)\ntd_display[\"pearson_corr\"] = td_display[\"pearson_corr\"].round(4)\ntd_display[\"ratio\"] = (\n    td_display[\"lambda_L_5pct\"] / td_display[\"pearson_corr\"]\n).round(3)\n\ntd_display.sort_values(\"lambda_L_5pct\", ascending=False).head(15)\n```\n:::\n\n\nThe ratio column in @tbl-tail-dependence is particularly informative. When $\\lambda_L / \\rho \\gg 1$, the sectors exhibit disproportionately high co-crash frequency relative to their overall correlation. This is the hallmark of tail dependence that Gaussian models miss.\n\n### System-Wide Stress Transmission\n\nTo assess system-wide contagion dynamics, we construct daily co-exceedance counts and examine their time-series properties.\n\n::: {#coexceedance .cell execution_count=26}\n``` {.python .cell-code}\n# Compute daily co-exceedance count at the 5% level\nsector_wide = sector_returns.pivot_table(\n    index=\"date\", columns=\"sector\", values=\"ret\"\n)\n\n# Rolling 252-day quantile thresholds\nquantile_thresholds = sector_wide.rolling(252, min_periods=60).quantile(0.05)\n\n# Indicator: return below rolling 5th percentile\nexceedance_indicators = (sector_wide < quantile_thresholds).astype(int)\nco_exceedance = exceedance_indicators.sum(axis=1)\nco_exceedance.name = \"co_exceedance\"\n\nco_exceed_df = co_exceedance.reset_index()\nco_exceed_df.columns = [\"date\", \"co_exceedance\"]\nco_exceed_df[\"n_sectors\"] = exceedance_indicators.notna().sum(axis=1).values\n```\n:::\n\n\n::: {#fig-coexceedance-ts .cell execution_count=27}\n``` {.python .cell-code}\n(\n    p9.ggplot(co_exceed_df.dropna(), p9.aes(x=\"date\", y=\"co_exceedance\"))\n    + p9.geom_line(color=\"#2E5090\", alpha=0.5, size=0.3)\n    + p9.geom_smooth(method=\"lowess\", color=\"#C0392B\", size=1, se=False)\n    + p9.labs(\n        x=\"\",\n        y=\"Number of Sectors in Left Tail\",\n        title=\"System-Wide Stress: Daily Co-Exceedance Count\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5))\n)\n```\n:::\n\n\nPeaks in the co-exceedance series correspond to system-wide stress episodes. The smoothed trend (red line) reveals the evolving baseline level of systemic fragility.\n\n### Financial Contagion Mechanisms\n\nThe literature distinguishes several channels through which stress propagates across assets, sectors, and markets [@forbes2002no; @kaminsky2002unholy]:\n\n**Direct linkages.** Balance sheet interconnections, interbank lending, and cross-holdings create direct transmission channels. When institution $A$ holds assets issued by institution $B$, losses at $B$ directly impair $A$'s balance sheet.\n\n**Information contagion.** Adverse news about one firm triggers reassessment of similarly exposed firms. @king1990transmission and @kodres2002rational formalize this via Bayesian updating: investors cannot distinguish idiosyncratic from systematic shocks, so a crash in one asset leads them to update beliefs about common risk factors.\n\n**Liquidity contagion.** Fire sales by distressed institutions depress prices of commonly held assets, creating losses for other holders. @brunnermeier2009deciphering model this as a liquidity spiral: declining prices trigger margin calls, which force further sales, further depressing prices. In Vietnamese markets, the absence of circuit breakers beyond price limits and the concentration of holdings among a few large institutional investors amplify this channel.\n\n**Behavioral contagion.** Herding behavior (i.e., investors mimicking others' trades) generates correlated selling pressure even in the absence of fundamental linkages. @bikhchandani1992theory and @banerjee1992simple provide theoretical foundations. @choe2005domestic document herding in emerging markets specifically.\n\nWe can test for contagion (as distinct from interdependence) using the @forbes2002no framework. The null hypothesis is that the co-movement observed during a crisis period is fully explained by the increase in volatility (which mechanically inflates correlations). The adjusted correlation is:\n\n$$\n\\rho^* = \\frac{\\rho_{\\text{crisis}}}{\\sqrt{1 + \\delta[1 - \\rho_{\\text{crisis}}^2]}}\n$$ {#eq-forbes-rigobon}\n\nwhere $\\delta = \\sigma_{\\text{crisis}}^2 / \\sigma_{\\text{tranquil}}^2 - 1$ is the relative increase in variance. If $\\rho^* > \\rho_{\\text{tranquil}}$, the increase in co-movement exceeds what volatility adjustment alone predicts, which is evidence of contagion.\n\n::: {#contagion-test .cell execution_count=28}\n``` {.python .cell-code}\ndef forbes_rigobon_test(returns_df, crisis_start, crisis_end):\n    \"\"\"\n    Forbes-Rigobon adjusted correlation test for contagion.\n\n    Parameters\n    ----------\n    returns_df : DataFrame\n        Columns are sector returns, index is date.\n    crisis_start, crisis_end : str\n        Crisis period boundaries.\n\n    Returns\n    -------\n    DataFrame : Pairwise contagion test results.\n    \"\"\"\n    tranquil = returns_df[returns_df.index < crisis_start].dropna()\n    crisis = returns_df[\n        (returns_df.index >= crisis_start) &\n        (returns_df.index <= crisis_end)\n    ].dropna()\n\n    sectors = returns_df.columns.tolist()\n    results = []\n\n    for s1, s2 in itertools.combinations(sectors, 2):\n        if s1 not in tranquil.columns or s2 not in crisis.columns:\n            continue\n\n        rho_t = tranquil[s1].corr(tranquil[s2])\n        rho_c = crisis[s1].corr(crisis[s2])\n\n        var_t = tranquil[s1].var()\n        var_c = crisis[s1].var()\n\n        if var_t == 0:\n            continue\n\n        delta = var_c / var_t - 1\n        rho_adj = rho_c / np.sqrt(1 + delta * (1 - rho_c**2))\n\n        results.append({\n            \"sector_1\": s1,\n            \"sector_2\": s2,\n            \"rho_tranquil\": round(rho_t, 4),\n            \"rho_crisis\": round(rho_c, 4),\n            \"rho_adjusted\": round(rho_adj, 4),\n            \"contagion\": rho_adj > rho_t\n        })\n\n    return pd.DataFrame(results)\n```\n:::\n\n\n::: {#contagion-covid .cell execution_count=29}\n``` {.python .cell-code}\n# Test for contagion during COVID-19 crash\ncontagion_covid = forbes_rigobon_test(\n    sector_wide,\n    crisis_start=\"2020-02-01\",\n    crisis_end=\"2020-04-30\"\n)\n\nn_pairs = len(contagion_covid)\nn_contagion = contagion_covid[\"contagion\"].sum()\nprint(f\"COVID-19 contagion test: {n_contagion}/{n_pairs} sector pairs \"\n      f\"show evidence of contagion ({100*n_contagion/n_pairs:.1f}%)\")\n```\n:::\n\n\n::: {#tbl-contagion-results .cell tbl-cap='Forbes-Rigobon Contagion Test: COVID-19 Crisis Period' execution_count=30}\n``` {.python .cell-code}\ncontagion_covid.sort_values(\n    \"rho_adjusted\", ascending=False\n).head(15)\n```\n:::\n\n\n## Applications to Financial Stability\n\n### Market-Wide Stress Indicators\n\nWe construct several market-wide stress indicators that aggregate the individual-level and sectoral tail risk measures developed above into a comprehensive system-level diagnostic.\n\n**Aggregate Crash Risk Index.** The cross-sectional average of firm-level NCSKEW provides a market-wide measure of crash risk that is forward-looking (it captures the buildup of negative skewness before a crash materializes):\n\n$$\n\\text{ACRI}_y = \\frac{1}{N_y}\\sum_{i=1}^{N_y} \\text{NCSKEW}_{i,y}\n$$ {#eq-acri}\n\n**Market Tail Risk (MTR).** Following @kelly2014tail, we estimate the time-varying tail risk of the market portfolio using option-implied or realized tail measures. In the absence of a liquid options market in Vietnam, we use the realized version based on the Hill tail index estimated from rolling windows:\n\n$$\n\\text{MTR}_t = \\hat{\\xi}_t^{\\text{Hill}} \\cdot \\hat{\\sigma}_t\n$$ {#eq-mtr}\n\nwhere $\\hat{\\xi}_t^{\\text{Hill}}$ is the Hill estimator computed from a trailing 252-day window and $\\hat{\\sigma}_t$ is the conditional volatility (e.g., from GARCH). This interaction captures both the thickness of the tail and the current scale of volatility.\n\n::: {#stability-indicators .cell execution_count=31}\n``` {.python .cell-code}\n# Aggregate Crash Risk Index\nacri = (\n    crash_risk.groupby(\"year\")\n    .agg(\n        acri=(\"ncskew\", \"mean\"),\n        median_ncskew=(\"ncskew\", \"median\"),\n        pct_high_crash=(\"ncskew\", lambda x: (x > x.quantile(0.75)).mean()),\n        n_firms=(\"ncskew\", \"count\")\n    )\n    .reset_index()\n)\n\n# Market Tail Risk: rolling Hill estimator x rolling volatility\nmkt_returns = market_daily[\"mkt_ret\"].dropna().values\ndates = market_daily[\"date\"].dropna().values\n\nmtr_list = []\nfor end_idx in range(504, len(mkt_returns)):\n    window_ret = mkt_returns[end_idx - 252:end_idx]\n    losses_pos = -window_ret[window_ret < 0]\n\n    if len(losses_pos) < 30:\n        continue\n\n    # Hill estimator at k = 50\n    sorted_losses = np.sort(losses_pos)[::-1]\n    k = min(50, len(sorted_losses) - 1)\n    if k < 10:\n        continue\n    log_excess = np.log(sorted_losses[:k]) - np.log(sorted_losses[k])\n    xi_hat = log_excess.mean()\n\n    sigma_hat = window_ret.std()\n    mtr = xi_hat * sigma_hat\n\n    mtr_list.append({\n        \"date\": dates[end_idx],\n        \"xi_hat\": xi_hat,\n        \"sigma_hat\": sigma_hat,\n        \"mtr\": mtr\n    })\n\nmtr_df = pd.DataFrame(mtr_list)\nmtr_df[\"date\"] = pd.to_datetime(mtr_df[\"date\"])\n```\n:::\n\n\n::: {#fig-stability-dashboard .cell execution_count=32}\n``` {.python .cell-code}\n(\n    p9.ggplot(mtr_df, p9.aes(x=\"date\", y=\"mtr\"))\n    + p9.geom_line(color=\"#2E5090\", alpha=0.5, size=0.4)\n    + p9.geom_smooth(method=\"lowess\", color=\"#C0392B\", size=1, se=False)\n    + p9.labs(\n        x=\"\",\n        y=\"MTR (ξ × σ)\",\n        title=\"Market Tail Risk: Rolling Hill Index × Conditional Volatility\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5))\n)\n```\n:::\n\n\n### Sectoral Fragility\n\nNot all sectors are equally vulnerable to tail events. We decompose system-wide tail risk into sectoral contributions to identify the most fragile parts of the market.\n\n::: {#sectoral-fragility .cell execution_count=33}\n``` {.python .cell-code}\n# Sector-level tail statistics\nsector_tail_stats = []\n\nfor sector in sectors:\n    sector_data = sector_returns[sector_returns[\"sector\"] == sector][\"ret\"].dropna()\n\n    if len(sector_data) < 500:\n        continue\n\n    s_data = sector_data.values\n\n    # Skewness and kurtosis\n    skew = stats.skew(s_data)\n    kurt = stats.kurtosis(s_data)\n\n    # VaR and ES at 1%\n    var_1 = -np.quantile(s_data, 0.01)\n    es_1 = -s_data[s_data <= np.quantile(s_data, 0.01)].mean()\n\n    # Hill tail index (k=50)\n    losses_pos = -s_data[s_data < 0]\n    sorted_l = np.sort(losses_pos)[::-1]\n    k = min(50, len(sorted_l) - 1)\n    if k >= 10:\n        log_exc = np.log(sorted_l[:k]) - np.log(sorted_l[k])\n        xi = log_exc.mean()\n    else:\n        xi = np.nan\n\n    sector_tail_stats.append({\n        \"sector\": sector,\n        \"skewness\": round(skew, 3),\n        \"excess_kurtosis\": round(kurt, 3),\n        \"var_1pct\": round(var_1, 4),\n        \"es_1pct\": round(es_1, 4),\n        \"tail_index_xi\": round(xi, 3) if not np.isnan(xi) else np.nan,\n        \"n_obs\": len(s_data)\n    })\n\nfragility_df = pd.DataFrame(sector_tail_stats)\n```\n:::\n\n\n::: {#tbl-sectoral-fragility .cell tbl-cap='Sectoral Tail Risk Profile' execution_count=34}\n``` {.python .cell-code}\nfragility_df.sort_values(\"es_1pct\", ascending=False)\n```\n:::\n\n\n::: {#fig-sector-scatter .cell execution_count=35}\n``` {.python .cell-code}\n(\n    p9.ggplot(\n        fragility_df.dropna(),\n        p9.aes(x=\"tail_index_xi\", y=\"es_1pct\", label=\"sector\")\n    )\n    + p9.geom_point(color=\"#2E5090\", size=3)\n    + p9.geom_text(nudge_y=0.002, size=7)\n    + p9.labs(\n        x=\"Tail Index (ξ, Hill)\",\n        y=\"1% Expected Shortfall\",\n        title=\"Sector Fragility Map\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 7))\n)\n```\n:::\n\n\nSectors in the upper-right quadrant of @fig-sector-scatter exhibit both high expected shortfall (large average losses in the tail) and high tail index (heavier-than-typical tails). These represent the most fragile components of the market from a systemic stability perspective.\n\n### Crisis Diagnostics\n\nWe construct a comprehensive crisis diagnostic framework that combines the indicators developed above to identify, characterize, and compare stress episodes in Vietnamese financial markets.\n\n::: {#crisis-diagnostics .cell execution_count=36}\n``` {.python .cell-code}\n# Identify extreme co-exceedance days (system-wide stress events)\nthreshold_high_stress = co_exceed_df[\"co_exceedance\"].quantile(0.99)\n\nstress_events = co_exceed_df[\n    co_exceed_df[\"co_exceedance\"] >= threshold_high_stress\n].copy()\n\n# Add market return on stress days\nstress_events = stress_events.merge(\n    market_daily[[\"date\", \"mkt_ret\"]], on=\"date\", how=\"left\"\n)\n\nprint(f\"System-wide stress days (top 1%): {len(stress_events)}\")\nprint(f\"Average market return on stress days: \"\n      f\"{stress_events['mkt_ret'].mean():.4f}\")\nprint(f\"Average co-exceedance on stress days: \"\n      f\"{stress_events['co_exceedance'].mean():.1f}\")\n```\n:::\n\n\n::: {#tbl-crisis-episodes .cell tbl-cap='Major Stress Episodes: Clustering of System-Wide Tail Events' execution_count=37}\n``` {.python .cell-code}\n# Cluster stress days into episodes (within 10 trading days)\nstress_events = stress_events.sort_values(\"date\").reset_index(drop=True)\nstress_events[\"gap\"] = stress_events[\"date\"].diff().dt.days\nstress_events[\"episode\"] = (stress_events[\"gap\"] > 15).cumsum()\n\nepisode_summary = (\n    stress_events.groupby(\"episode\")\n    .agg(\n        start_date=(\"date\", \"min\"),\n        end_date=(\"date\", \"max\"),\n        n_stress_days=(\"date\", \"count\"),\n        avg_market_return=(\"mkt_ret\", \"mean\"),\n        min_market_return=(\"mkt_ret\", \"min\"),\n        avg_coexceedance=(\"co_exceedance\", \"mean\")\n    )\n    .reset_index(drop=True)\n    .sort_values(\"min_market_return\")\n)\n\nepisode_summary[\"avg_market_return\"] = episode_summary[\"avg_market_return\"].round(4)\nepisode_summary[\"min_market_return\"] = episode_summary[\"min_market_return\"].round(4)\nepisode_summary[\"avg_coexceedance\"] = episode_summary[\"avg_coexceedance\"].round(1)\n\nepisode_summary.head(10)\n```\n:::\n\n\n<!-- ## Exercises\n\n1.  **Crash Risk Determinants.** Estimate a panel regression of NCSKEW on lagged firm characteristics: size, book-to-market, leverage, return volatility, trading volume, state ownership dummy, and lagged NCSKEW. Include industry and year fixed effects. Cluster standard errors by firm. Which characteristics predict future crash risk? Does state ownership amplify or attenuate crash risk?\n\n2.  **VaR Model Comparison.** Implement a GARCH(1,1)-$t$ model for the market return and compute the one-day-ahead VaR at the 1% level. Compare its backtesting performance (Kupiec and Christoffersen tests) against the four static methods implemented in this chapter. Which model performs best in terms of unconditional coverage? Which in terms of independence?\n\n3.  **Threshold Sensitivity.** Re-estimate the GPD model from @sec-gpd using thresholds at the 90th, 92.5th, 95th, and 97.5th percentiles. Plot the resulting VaR and ES estimates as a function of the threshold. At what threshold do the estimates stabilize? Relate your findings to the mean residual life plot.\n\n4.  **Tail Dependence Across Market Regimes.** Split the sample into bull and bear regimes (e.g., above/below the 12-month moving average of the market index). Estimate pairwise tail dependence coefficients separately for each regime. Is tail dependence symmetric across regimes, or is it asymmetrically higher during bear markets?\n\n5.  **Contagion During Multiple Crises.** Apply the Forbes-Rigobon contagion test to at least three distinct crisis episodes in Vietnam (e.g., 2011 banking stress, 2018 trade war, 2020 COVID-19, 2022 Vạn Thịnh Phát/corporate bond crisis). Compare the fraction of sector pairs showing contagion across episodes. Which crises were most contagious? What structural features of each crisis might explain the differences?\n\n6.  **Price Limit Bias in Tail Estimation.** Simulate a stock return series from a Student-$t$ distribution with known parameters, then censor it at Vietnamese daily price limits ($\\pm 7\\%$ for HOSE, $\\pm 10\\%$ for HNX). Compare VaR, ES, and Hill tail index estimates from the censored and uncensored series. Quantify the underestimation bias as a function of the true tail thickness and price limit width. -->\n\n## Summary\n\nThis chapter developed a comprehensive toolkit for measuring and diagnosing tail risk in Vietnamese equity markets. The key objects estimated (crash risk measures, VaR and Expected Shortfall, tail indices, and tail dependence coefficients) capture distinct but complementary aspects of extreme event behavior.\n\nAt the individual stock level, the NCSKEW, DUVOL, and crash count measures quantify asymmetry in firm-specific return distributions, providing forward-looking indicators of crash vulnerability. At the portfolio level, EVT-based approaches to VaR and ES estimation avoid the parametric misspecification that plagues Gaussian and even Student-$t$ models in the deep tails. At the system level, tail dependence and co-exceedance measures reveal the extent to which crashes propagate across sectors, and the Forbes-Rigobon contagion test distinguishes genuine contagion from the mechanical increase in co-movement driven by higher volatility.\n\nSeveral features of Vietnamese markets deserve emphasis. Price limits censor the observed return distribution, causing systematic underestimation of true tail risk. The concentration of trading among retail investors amplifies herding-driven crash dynamics. State ownership of large firms creates correlated exposure to policy shocks that is not captured by standard diversification. And the absence of deep derivatives markets limits the hedging instruments available for tail risk management, making accurate measurement all the more critical.\n\n",
    "supporting": [
      "61_tail_risk_extreme_events_files/figure-pdf"
    ],
    "filters": []
  }
}