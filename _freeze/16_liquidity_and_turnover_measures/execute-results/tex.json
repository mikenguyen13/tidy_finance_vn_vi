{
  "hash": "e6c202f47dfc9d38340f31fb9bde8510",
  "result": {
    "engine": "jupyter",
    "markdown": "# Liquidity and Turnover Measures\n\n::: callout-note\nIn this chapter, we construct a comprehensive suite of liquidity measures for the Vietnamese equity market, validate them against each other and against known benchmarks, test whether liquidity is priced in the cross-section of stock returns, examine commonality in liquidity, and analyze how liquidity conditions vary over time and across market regimes.\n:::\n\nLiquidity (i.e., the ability to trade quickly at low cost without moving the price) is arguably the single most important practical consideration for anyone working with Vietnamese equity data. A factor premium that looks attractive in a frictionless backtest may be completely unimplementable if the long and short legs load on illiquid stocks whose prices move against you when you trade. Conversely, a genuine liquidity premium (i.e., compensation for bearing the risk that a stock will be hard to sell when you need to) is one of the most robust and theoretically grounded anomalies in asset pricing.\n\nThe challenge is that liquidity is inherently multidimensional and difficult to measure. In developed markets with continuous limit order books and sub-second trade reporting, researchers can observe bid-ask spreads, market depth, and price impact directly. In Vietnam, microstructure data at this granularity are limited: HOSE operates a periodic call auction at open and close with continuous matching in between, the tick size is coarse relative to price levels, and many stocks trade so infrequently that the concept of a \"quoted spread\" is meaningful only on days when the stock actually trades. This forces researchers to rely on low-frequency proxies computed from daily price and volume data.\n\nThis chapter constructs the major liquidity proxies used in the academic literature, validates them in the Vietnamese context, and demonstrates their use in asset pricing and portfolio construction.\n\n## Theoretical Foundations {#sec-theory}\n\n### Why Liquidity Matters\n\nLiquidity affects asset prices through at least three channels:\n\n1.  **Level effect.** Investors demand compensation for the expected cost of trading. @amihud1986asset show that stocks with higher bid-ask spreads earn higher expected returns, with the premium being an increasing function of the investor's holding period. In equilibrium, illiquid stocks must offer higher expected returns to compensate for higher round-trip trading costs.\n2.  **Risk effect.** Liquidity is time-varying and co-moves across stocks. @pastor2003liquidity show that stocks whose returns are more sensitive to aggregate liquidity shocks earn higher expected returns. @acharya2005asset formalize this in a liquidity-adjusted CAPM where the required return includes a premium for bearing liquidity risk—the risk that the stock becomes illiquid precisely when the investor needs to sell.\n3.  **Commonality effect.** @chordia2000commonality document that individual stock liquidity co-moves strongly with market-wide liquidity. @brunnermeier2009market explain this through a \"liquidity spiral\": when asset values fall, margin constraints tighten, forcing leveraged investors to sell, which reduces market liquidity, which depresses prices further. This mechanism is particularly relevant in Vietnam, where retail investors with margin accounts are the dominant trading population.\n\n### Liquidity Dimensions\n\n@kyle1985continuous identifies three dimensions of liquidity:\n\n1.  **Tightness.** The cost of turning around a position quickly—measured by the bid-ask spread.\n2.  **Depth.** The volume that can be traded without moving the price—related to price impact.\n3.  **Resiliency.** The speed at which prices recover from uninformative order flow shocks.\n\nNo single measure captures all three dimensions. @goyenko2009liquidity and @fong2017best systematically evaluate which low-frequency proxies best capture each dimension by benchmarking against high-frequency measures. Their key finding: the @amihud2002illiquidity measure best captures the price impact dimension, the @roll1984simple estimator and @corwin2012simple spread best capture tightness, and the @lesmond1999new zero-return measure captures a blend of transaction costs and information asymmetry. For emerging markets specifically, @fong2017best recommend the Amihud measure and the Closing Percent Quoted Spread as the most reliable proxies.\n\n## Data Construction {#sec-data}\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code code-summary=\"Import libraries and configure environment\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom linearmodels.panel import PanelOLS\nfrom linearmodels.asset_pricing import LinearFactorModel\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n```\n:::\n\n\n::: {#data-load .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Load daily and monthly data\"}\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Daily trading data\ndaily = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'date', 'open', 'high', 'low', 'close',\n        'adjusted_close', 'volume', 'turnover_value',\n        'market_cap', 'shares_outstanding', 'free_float_pct',\n        'bid', 'ask', 'foreign_buy_volume', 'foreign_sell_volume'\n    ]\n)\n\n# Monthly aggregates\nmonthly = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'volume_avg_20d', 'turnover_value_avg_20d',\n        'n_trading_days', 'n_zero_volume_days'\n    ]\n)\n\n# Firm characteristics for cross-sectional tests\nfundamentals = client.get_fundamentals(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    frequency='annual',\n    fields=[\n        'ticker', 'fiscal_year', 'total_assets', 'total_equity',\n        'net_income', 'revenue', 'book_equity'\n    ]\n)\n\n# Factor returns for asset pricing tests\nfactors = client.get_factor_returns(\n    market='vietnam',\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    factors=['mkt_excess', 'smb', 'hml', 'wml']\n)\n\ndaily['date'] = pd.to_datetime(daily['date'])\ndaily = daily.sort_values(['ticker', 'date'])\n\nprint(f\"Daily observations: {daily.shape[0]:,}\")\nprint(f\"Monthly observations: {monthly.shape[0]:,}\")\nprint(f\"Unique tickers: {daily['ticker'].nunique()}\")\n```\n:::\n\n\n::: {#daily-returns .cell execution_count=3}\n``` {.python .cell-code code-summary=\"Compute daily returns and basic trading statistics\"}\n# Daily returns\ndaily['daily_return'] = (\n    daily.groupby('ticker')['adjusted_close'].pct_change()\n)\ndaily['abs_return'] = daily['daily_return'].abs()\ndaily['log_return'] = np.log(\n    daily['adjusted_close'] / daily.groupby('ticker')['adjusted_close'].shift(1)\n)\n\n# Turnover ratio (shares traded / shares outstanding)\ndaily['turnover_ratio'] = daily['volume'] / daily['shares_outstanding']\n\n# Zero indicators\ndaily['zero_return'] = (daily['daily_return'] == 0).astype(int)\ndaily['zero_volume'] = (daily['volume'] == 0).astype(int)\n\n# VND turnover (in billions)\ndaily['turnover_vnd_bn'] = daily['turnover_value'] / 1e9\n\nprint(\"Daily Return Summary:\")\nprint(daily['daily_return'].describe().round(6))\nprint(f\"\\nZero-return days: {daily['zero_return'].mean():.1%}\")\nprint(f\"Zero-volume days: {daily['zero_volume'].mean():.1%}\")\n```\n:::\n\n\n## Constructing Liquidity Measures {#sec-measures}\n\nWe construct seven liquidity proxies that span the dimensions of tightness, depth, and resiliency. Each is computed at the firm-month level, producing a panel that can be merged with monthly return data for cross-sectional tests.\n\n### Amihud Illiquidity Ratio\n\nThe @amihud2002illiquidity illiquidity measure is the ratio of absolute daily return to daily volume (in VND):\n\n$$\n\\text{ILLIQ}_{i,m} = \\frac{1}{D_{i,m}} \\sum_{d=1}^{D_{i,m}} \\frac{|R_{i,d}|}{\\text{DVOL}_{i,d}}\n$$ {#eq-amihud}\n\nwhere $|R_{i,d}|$ is the absolute daily return, $\\text{DVOL}_{i,d}$ is VND trading volume on day $d$, and $D_{i,m}$ is the number of trading days with positive volume in month $m$. Higher values indicate greater illiquidity—a given amount of volume moves the price more.\n\nThe Amihud measure captures the price impact dimension of liquidity. It is grounded in the @kyle1985continuous model where the parameter $\\lambda$ (Kyle's lambda) measures the price impact of order flow: $\\Delta p = \\lambda \\cdot Q$. The Amihud ratio is a daily-frequency analog of $\\lambda$.\n\n::: {#amihud .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Compute monthly Amihud illiquidity ratio\"}\ndef compute_amihud(daily_df, min_days=10):\n    \"\"\"\n    Compute the Amihud (2002) illiquidity ratio at the firm-month level.\n    \n    Excludes zero-volume days. Requires at least min_days observations\n    with positive volume per firm-month.\n    \"\"\"\n    df = daily_df[daily_df['volume'] > 0].copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    # |Return| / VND Volume\n    df['illiq_daily'] = df['abs_return'] / df['turnover_value']\n    \n    # Remove extreme outliers (top 0.1% within each month)\n    df['illiq_daily'] = df.groupby('month')['illiq_daily'].transform(\n        lambda x: x.clip(upper=x.quantile(0.999))\n    )\n    \n    # Aggregate to firm-month\n    amihud = (\n        df.groupby(['ticker', 'month'])\n        .agg(\n            amihud_raw=('illiq_daily', 'mean'),\n            n_positive_vol_days=('illiq_daily', 'count')\n        )\n        .reset_index()\n    )\n    \n    # Filter: require minimum trading days\n    amihud = amihud[amihud['n_positive_vol_days'] >= min_days]\n    \n    # Log transform (raw Amihud is heavily right-skewed)\n    amihud['amihud'] = np.log(1 + amihud['amihud_raw'] * 1e6)\n    \n    # Convert period to timestamp for merging\n    amihud['month_end'] = amihud['month'].dt.to_timestamp('M')\n    \n    return amihud[['ticker', 'month_end', 'amihud', 'amihud_raw',\n                    'n_positive_vol_days']]\n\namihud_monthly = compute_amihud(daily)\nprint(f\"Amihud observations: {len(amihud_monthly):,}\")\nprint(f\"\\nLog Amihud distribution:\")\nprint(amihud_monthly['amihud'].describe().round(3))\n```\n:::\n\n\n### Zero-Return Days (Lesmond Measure)\n\n@lesmond1999new propose using the proportion of zero-return days as a measure of transaction costs. The intuition is that if the true value change on a given day is smaller than the round-trip transaction cost, a rational marginal investor will not trade, and the observed return will be zero. Thus, the zero-return proportion is an increasing function of effective transaction costs.\n\n@lesmond2005liquidity validates this measure for emerging markets and finds it strongly correlated with explicit cost measures. In Vietnam, where zero-return days are common (as documented in the previous chapter), this measure has particular relevance.\n\n$$\n\\text{ZeroRet}_{i,m} = \\frac{\\text{Number of days with } R_{i,d} = 0}{D_{i,m}}\n$$ {#eq-zero-return}\n\n::: {#zero-return .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Compute monthly zero-return proportion\"}\ndef compute_zero_return(daily_df):\n    \"\"\"\n    Compute the Lesmond et al. (1999) zero-return measure\n    at the firm-month level.\n    \"\"\"\n    df = daily_df.copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    zero_ret = (\n        df.groupby(['ticker', 'month'])\n        .agg(\n            n_days=('daily_return', 'count'),\n            n_zero_return=('zero_return', 'sum'),\n            n_zero_volume=('zero_volume', 'sum')\n        )\n        .reset_index()\n    )\n    \n    zero_ret['zero_return_pct'] = (\n        zero_ret['n_zero_return'] / zero_ret['n_days']\n    )\n    zero_ret['zero_volume_pct'] = (\n        zero_ret['n_zero_volume'] / zero_ret['n_days']\n    )\n    \n    zero_ret['month_end'] = zero_ret['month'].dt.to_timestamp('M')\n    \n    return zero_ret[['ticker', 'month_end', 'zero_return_pct',\n                      'zero_volume_pct', 'n_days']]\n\nzero_monthly = compute_zero_return(daily)\nprint(f\"Zero-return observations: {len(zero_monthly):,}\")\nprint(f\"\\nZero-return proportion distribution:\")\nprint(zero_monthly['zero_return_pct'].describe().round(3))\n```\n:::\n\n\n### Turnover Ratio\n\nShare turnover—daily volume divided by shares outstanding—measures trading activity rather than trading cost. @datar1998liquidity use turnover as a liquidity proxy and document a negative cross-sectional relationship between turnover and expected returns, consistent with the liquidity premium hypothesis.\n\n$$\n\\text{Turn}_{i,m} = \\frac{1}{D_{i,m}} \\sum_{d=1}^{D_{i,m}} \\frac{\\text{Volume}_{i,d}}{\\text{SharesOut}_{i,d}}\n$$ {#eq-turnover}\n\n::: {#turnover .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Compute monthly average turnover ratio\"}\ndef compute_turnover(daily_df):\n    \"\"\"Compute average daily turnover ratio at the firm-month level.\"\"\"\n    df = daily_df.copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    turnover = (\n        df.groupby(['ticker', 'month'])\n        .agg(\n            turnover_mean=('turnover_ratio', 'mean'),\n            turnover_sum=('turnover_ratio', 'sum'),\n            volume_mean=('volume', 'mean'),\n            dvol_mean=('turnover_value', 'mean')\n        )\n        .reset_index()\n    )\n    \n    # Log transform for cross-sectional normality\n    turnover['log_turnover'] = np.log(\n        turnover['turnover_mean'].clip(lower=1e-8)\n    )\n    turnover['log_dvol'] = np.log(\n        turnover['dvol_mean'].clip(lower=1)\n    )\n    \n    turnover['month_end'] = turnover['month'].dt.to_timestamp('M')\n    \n    return turnover[['ticker', 'month_end', 'turnover_mean',\n                      'log_turnover', 'log_dvol']]\n\nturnover_monthly = compute_turnover(daily)\nprint(f\"Turnover observations: {len(turnover_monthly):,}\")\nprint(f\"\\nLog turnover distribution:\")\nprint(turnover_monthly['log_turnover'].describe().round(3))\n```\n:::\n\n\n### Roll Spread Estimator\n\n@roll1984simple derives an implicit bid-ask spread from the serial covariance of price changes. Under the assumptions that the true value follows a random walk and that observed prices bounce between the bid and ask:\n\n$$\n\\text{Roll}_{i,m} = \\begin{cases}\n2\\sqrt{-\\text{Cov}(\\Delta P_{i,d}, \\Delta P_{i,d-1})} & \\text{if } \\text{Cov} < 0 \\\\\n0 & \\text{if } \\text{Cov} \\geq 0\n\\end{cases}\n$$ {#eq-roll}\n\nwhere $\\Delta P_{i,d} = P_{i,d} - P_{i,d-1}$. The measure is intuitive: the bid-ask bounce creates negative serial correlation in transaction prices, and the magnitude of this negative correlation reflects the spread.\n\n::: {#roll-spread .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Compute monthly Roll (1984) effective spread estimator\"}\ndef compute_roll_spread(daily_df, min_days=15):\n    \"\"\"\n    Compute the Roll (1984) effective spread from serial\n    covariance of daily price changes.\n    \"\"\"\n    df = daily_df.copy()\n    df['month'] = df['date'].dt.to_period('M')\n    df['price_change'] = df.groupby('ticker')['adjusted_close'].diff()\n    df['price_change_lag'] = df.groupby('ticker')['price_change'].shift(1)\n    \n    def roll_estimate(group):\n        if len(group) < min_days:\n            return np.nan\n        cov = group['price_change'].cov(group['price_change_lag'])\n        if cov < 0:\n            spread = 2 * np.sqrt(-cov)\n            # Normalize by average price\n            avg_price = group['adjusted_close'].mean()\n            return spread / avg_price if avg_price > 0 else np.nan\n        else:\n            return 0.0\n    \n    roll = (\n        df.dropna(subset=['price_change', 'price_change_lag'])\n        .groupby(['ticker', 'month'])\n        .apply(roll_estimate)\n        .reset_index(name='roll_spread')\n    )\n    \n    roll['month_end'] = roll['month'].dt.to_timestamp('M')\n    \n    return roll[['ticker', 'month_end', 'roll_spread']]\n\nroll_monthly = compute_roll_spread(daily)\nprint(f\"Roll spread observations: {len(roll_monthly):,}\")\nprint(f\"\\nRoll spread distribution:\")\nprint(roll_monthly['roll_spread'].describe().round(4))\n```\n:::\n\n\n### Corwin-Schultz High-Low Spread\n\n@corwin2012simple estimate the effective spread from daily high and low prices. The key insight is that daily high and low prices contain information about both volatility and the spread—the high is typically a buy and the low a sell, so the high-low range reflects both true volatility and the bid-ask spread. By comparing one-day and two-day high-low ranges, the method separates the two components:\n\n$$\n\\hat{S}_{i,m} = \\frac{2(e^{\\hat{\\alpha}} - 1)}{1 + e^{\\hat{\\alpha}}}\n$$ {#eq-corwin-schultz}\n\nwhere:\n\n$$\n\\hat{\\alpha} = \\frac{\\sqrt{2\\hat{\\beta}} - \\sqrt{\\hat{\\beta}}}{3 - 2\\sqrt{2}} - \\sqrt{\\frac{\\hat{\\gamma}}{3 - 2\\sqrt{2}}}\n$$ {#eq-cs-alpha}\n\nwith $\\hat{\\beta}$ and $\\hat{\\gamma}$ computed from one-day and two-day log high-low ratios.\n\n::: {#corwin-schultz .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Compute the Corwin-Schultz (2012) high-low spread estimator\"}\ndef compute_corwin_schultz(daily_df, min_days=15):\n    \"\"\"\n    Compute the Corwin and Schultz (2012) bid-ask spread\n    estimator from daily high and low prices.\n    \"\"\"\n    df = daily_df.copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    # Log high-low ratio\n    df['log_hl'] = np.log(df['high'] / df['low'])\n    df['log_hl_sq'] = df['log_hl'] ** 2\n    \n    # Two-day high and low\n    df['high_2d'] = df.groupby('ticker')['high'].transform(\n        lambda x: x.rolling(2).max()\n    )\n    df['low_2d'] = df.groupby('ticker')['low'].transform(\n        lambda x: x.rolling(2).min()\n    )\n    df['log_hl_2d'] = np.log(df['high_2d'] / df['low_2d'])\n    df['log_hl_2d_sq'] = df['log_hl_2d'] ** 2\n    \n    def cs_estimate(group):\n        if len(group) < min_days:\n            return np.nan\n        \n        beta = group['log_hl_sq'].mean() + group['log_hl_sq'].shift(1).mean()\n        beta = group[['log_hl_sq']].rolling(2).sum().mean().values[0]\n        gamma = group['log_hl_2d_sq'].mean()\n        \n        k = np.sqrt(2) - 1\n        denom = 3 - 2 * np.sqrt(2)\n        \n        term1 = np.sqrt(max(beta, 0)) / denom\n        if beta > 0:\n            alpha_est = (np.sqrt(2 * beta) - np.sqrt(beta)) / denom\n            alpha_est -= np.sqrt(max(gamma / denom, 0))\n        else:\n            alpha_est = 0\n        \n        # Spread estimate\n        if alpha_est > 0:\n            spread = 2 * (np.exp(alpha_est) - 1) / (1 + np.exp(alpha_est))\n        else:\n            spread = 0\n        \n        return min(spread, 0.20)  # Cap at 20% (sanity check)\n    \n    cs = (\n        df.dropna(subset=['log_hl', 'log_hl_2d'])\n        .groupby(['ticker', 'month'])\n        .apply(cs_estimate)\n        .reset_index(name='cs_spread')\n    )\n    \n    cs['month_end'] = cs['month'].dt.to_timestamp('M')\n    \n    return cs[['ticker', 'month_end', 'cs_spread']]\n\ncs_monthly = compute_corwin_schultz(daily)\nprint(f\"Corwin-Schultz observations: {len(cs_monthly):,}\")\nprint(f\"\\nCS spread distribution:\")\nprint(cs_monthly['cs_spread'].describe().round(4))\n```\n:::\n\n\n### Quoted Bid-Ask Spread\n\nWhen bid and ask quotes are available, the quoted percentage spread provides a direct measure of tightness:\n\n$$\n\\text{PQSPR}_{i,m} = \\frac{1}{D_{i,m}} \\sum_{d=1}^{D_{i,m}} \\frac{\\text{Ask}_{i,d} - \\text{Bid}_{i,d}}{(\\text{Ask}_{i,d} + \\text{Bid}_{i,d})/2}\n$$ {#eq-quoted-spread}\n\n::: {#quoted-spread .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Compute monthly average quoted percentage spread\"}\ndef compute_quoted_spread(daily_df):\n    \"\"\"Compute average quoted percentage spread at the firm-month level.\"\"\"\n    df = daily_df[\n        (daily_df['bid'] > 0) & (daily_df['ask'] > 0) &\n        (daily_df['ask'] >= daily_df['bid'])\n    ].copy()\n    \n    df['month'] = df['date'].dt.to_period('M')\n    df['pqspr'] = (\n        (df['ask'] - df['bid']) / ((df['ask'] + df['bid']) / 2)\n    )\n    \n    # Winsorize extreme values\n    df['pqspr'] = df['pqspr'].clip(upper=df['pqspr'].quantile(0.999))\n    \n    spread = (\n        df.groupby(['ticker', 'month'])\n        .agg(\n            quoted_spread=('pqspr', 'mean'),\n            n_quotes=('pqspr', 'count')\n        )\n        .reset_index()\n    )\n    \n    spread['month_end'] = spread['month'].dt.to_timestamp('M')\n    \n    return spread[['ticker', 'month_end', 'quoted_spread', 'n_quotes']]\n\nquoted_monthly = compute_quoted_spread(daily)\nprint(f\"Quoted spread observations: {len(quoted_monthly):,}\")\nprint(f\"\\nQuoted spread distribution:\")\nprint(quoted_monthly['quoted_spread'].describe().round(4))\n```\n:::\n\n\n### Kyle's Lambda (Price Impact Regression)\n\nWe estimate Kyle's lambda—the price impact per unit of signed order flow—using a daily regression:\n\n$$\nR_{i,d} = \\alpha_i + \\lambda_i \\cdot \\text{Sign}(R_{i,d}) \\cdot \\sqrt{\\text{Volume}_{i,d}} + \\varepsilon_{i,d}\n$$ {#eq-kyle-lambda}\n\nThis is an adaptation of the @hasbrouck2009trading effective cost measure. The coefficient $\\lambda_i$ measures how much prices move per unit of (unsigned, square-rooted) volume.\n\n::: {#kyle-lambda .cell execution_count=10}\n``` {.python .cell-code code-summary=\"Estimate Kyle's lambda from daily price-volume regressions\"}\ndef compute_kyle_lambda(daily_df, min_days=15):\n    \"\"\"\n    Estimate Kyle's lambda (price impact per unit order flow)\n    from daily return-on-signed-volume regressions.\n    \"\"\"\n    df = daily_df[daily_df['volume'] > 0].copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    # Signed square-root volume (sign inferred from return)\n    df['signed_sqrt_vol'] = (\n        np.sign(df['daily_return']) * np.sqrt(df['volume'])\n    )\n    \n    def estimate_lambda(group):\n        if len(group) < min_days:\n            return np.nan\n        y = group['daily_return'].values\n        x = group['signed_sqrt_vol'].values\n        x = sm.add_constant(x)\n        try:\n            model = sm.OLS(y, x).fit()\n            lam = model.params[1]\n            return max(lam, 0)  # Lambda should be non-negative\n        except Exception:\n            return np.nan\n    \n    kyle = (\n        df.groupby(['ticker', 'month'])\n        .apply(estimate_lambda)\n        .reset_index(name='kyle_lambda')\n    )\n    \n    kyle['log_kyle'] = np.log(kyle['kyle_lambda'].clip(lower=1e-10))\n    kyle['month_end'] = kyle['month'].dt.to_timestamp('M')\n    \n    return kyle[['ticker', 'month_end', 'kyle_lambda', 'log_kyle']]\n\nkyle_monthly = compute_kyle_lambda(daily)\nprint(f\"Kyle lambda observations: {len(kyle_monthly):,}\")\nprint(f\"\\nLog Kyle lambda distribution:\")\nprint(kyle_monthly['log_kyle'].describe().round(3))\n```\n:::\n\n\n## Assembling the Liquidity Panel {#sec-panel}\n\nWe merge all seven measures into a single firm-month panel for comparative analysis.\n\n::: {#merge-panel .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Merge all liquidity measures into a unified panel\"}\n# Start with monthly returns as the base\npanel = monthly[['ticker', 'month_end', 'monthly_return',\n                  'market_cap']].copy()\n\n# Merge each liquidity measure\nfor name, df, key_col in [\n    ('Amihud', amihud_monthly, 'amihud'),\n    ('Zero Return', zero_monthly, 'zero_return_pct'),\n    ('Turnover', turnover_monthly, 'log_turnover'),\n    ('Roll', roll_monthly, 'roll_spread'),\n    ('Corwin-Schultz', cs_monthly, 'cs_spread'),\n    ('Quoted Spread', quoted_monthly, 'quoted_spread'),\n    ('Kyle Lambda', kyle_monthly, 'log_kyle'),\n]:\n    panel = panel.merge(\n        df[['ticker', 'month_end', key_col]],\n        on=['ticker', 'month_end'],\n        how='left'\n    )\n\n# Add log market cap\npanel['log_mcap'] = np.log(panel['market_cap'].clip(lower=1))\n\n# Add fundamentals (lagged)\nfund_lagged = fundamentals.copy()\nfund_lagged['merge_year'] = fund_lagged['fiscal_year'] + 1\npanel = panel.merge(\n    fund_lagged[['ticker', 'merge_year', 'book_equity']].rename(\n        columns={'merge_year': 'year'}),\n    left_on=['ticker', panel['month_end'].dt.year],\n    right_on=['ticker', 'year'],\n    how='left'\n)\npanel['bm'] = panel['book_equity'] / panel['market_cap']\n\nprint(f\"Unified panel: {len(panel):,} firm-months\")\nprint(f\"\\nCoverage by measure:\")\nliquidity_cols = ['amihud', 'zero_return_pct', 'log_turnover',\n                   'roll_spread', 'cs_spread', 'quoted_spread', 'log_kyle']\nfor col in liquidity_cols:\n    pct = panel[col].notna().mean()\n    print(f\"  {col:<20}: {pct:.1%}\")\n```\n:::\n\n\n## Cross-Sectional Properties of Liquidity {#sec-cross-section}\n\n### Summary Statistics by Size Quintile\n\nLiquidity varies enormously across the size distribution. Small-cap Vietnamese stocks can be orders of magnitude less liquid than large-caps.\n\n::: {#liquidity-by-size .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Compute liquidity statistics by market cap quintile\"}\n# Assign size quintiles within each month\npanel['size_quintile'] = (\n    panel.groupby('month_end')['market_cap']\n    .transform(lambda x: pd.qcut(x, 5, labels=['Q1 (Small)', 'Q2',\n                                                  'Q3', 'Q4',\n                                                  'Q5 (Large)'],\n                                   duplicates='drop'))\n)\n\n# Average liquidity by quintile\nliq_by_size = (\n    panel.groupby('size_quintile')[liquidity_cols]\n    .mean()\n    .round(4)\n)\n\nprint(\"Average Liquidity by Market Cap Quintile:\")\nprint(liq_by_size.to_string())\n```\n:::\n\n\n::: {#fig-liquidity-by-size .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Bar charts of liquidity measures by size quintile\"}\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\naxes = axes.flatten()\n\nmeasures_to_plot = [\n    ('amihud', 'Amihud (log)', '#2C5F8A'),\n    ('zero_return_pct', 'Zero-Return %', '#C0392B'),\n    ('log_turnover', 'Log Turnover', '#27AE60'),\n    ('roll_spread', 'Roll Spread', '#E67E22'),\n    ('cs_spread', 'Corwin-Schultz Spread', '#8E44AD'),\n    ('quoted_spread', 'Quoted Spread', '#1ABC9C')\n]\n\nfor i, (col, label, color) in enumerate(measures_to_plot):\n    data = panel.groupby('size_quintile')[col].mean()\n    axes[i].bar(range(len(data)), data.values,\n                color=color, alpha=0.85, edgecolor='white')\n    axes[i].set_xticks(range(len(data)))\n    axes[i].set_xticklabels(data.index, fontsize=8)\n    axes[i].set_ylabel(label)\n    axes[i].set_title(label)\n\nplt.suptitle('Liquidity Measures by Market Cap Quintile', fontsize=14)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Correlation Structure\n\nHow strongly do the different liquidity measures correlate? If they capture the same underlying dimension, we expect high correlations. If they capture different dimensions (tightness vs. depth vs. activity), correlations will be moderate.\n\n::: {#fig-liquidity-correlations .cell execution_count=14}\n``` {.python .cell-code code-summary=\"Compute and plot cross-sectional rank correlations\"}\n# Rank correlations (Spearman) among liquidity measures\n# Reverse turnover sign so higher = less liquid (consistent direction)\npanel_corr = panel[liquidity_cols].copy()\npanel_corr['neg_log_turnover'] = -panel_corr['log_turnover']\ncorr_cols = ['amihud', 'zero_return_pct', 'neg_log_turnover',\n              'roll_spread', 'cs_spread', 'quoted_spread', 'log_kyle']\ncorr_labels = ['Amihud', 'Zero-Return', 'Neg. Turnover', 'Roll',\n                'Corwin-Schultz', 'Quoted Spread', 'Kyle λ']\n\nrank_corr = panel_corr[corr_cols].corr(method='spearman')\nrank_corr.index = corr_labels\nrank_corr.columns = corr_labels\n\nfig, ax = plt.subplots(figsize=(9, 8))\nmask = np.triu(np.ones_like(rank_corr, dtype=bool), k=1)\nsns.heatmap(\n    rank_corr, mask=mask, annot=True, fmt='.2f',\n    cmap='YlOrRd', vmin=0, vmax=1, square=True,\n    linewidths=0.5, ax=ax,\n    cbar_kws={'label': 'Spearman Rank Correlation'}\n)\nax.set_title('Cross-Sectional Rank Correlations Among Liquidity Measures')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Principal Component Analysis of Liquidity\n\nGiven the multidimensionality of liquidity, we extract a composite liquidity factor using PCA:\n\n::: {#pca-liquidity .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Extract principal components from standardized liquidity measures\"}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Standardize each measure within each month (cross-sectional)\nliq_data = panel[liquidity_cols].copy()\nliq_data['neg_log_turnover'] = -liq_data['log_turnover']\n\npca_cols = ['amihud', 'zero_return_pct', 'neg_log_turnover',\n             'roll_spread', 'cs_spread', 'log_kyle']\n\n# Drop rows with any missing liquidity measure\nliq_complete = panel.dropna(subset=pca_cols).copy()\n\n# Cross-sectional standardization by month\ndef standardize_within_month(df, cols):\n    for col in cols:\n        df[col + '_z'] = (\n            df.groupby('month_end')[col]\n            .transform(lambda x: (x - x.mean()) / x.std())\n        )\n    return df\n\nliq_complete = standardize_within_month(liq_complete, pca_cols)\nz_cols = [c + '_z' for c in pca_cols]\n\n# Pool all months for PCA\npca_input = liq_complete[z_cols].dropna()\npca = PCA(n_components=3)\npca.fit(pca_input)\n\nprint(\"PCA Explained Variance Ratios:\")\nfor i, (var, cumvar) in enumerate(zip(\n    pca.explained_variance_ratio_,\n    np.cumsum(pca.explained_variance_ratio_)\n)):\n    print(f\"  PC{i+1}: {var:.3f} (cumulative: {cumvar:.3f})\")\n\nprint(\"\\nPC1 Loadings:\")\nfor col, loading in zip(pca_cols, pca.components_[0]):\n    print(f\"  {col:<20}: {loading:.3f}\")\n\n# Assign PC1 as composite illiquidity\nliq_complete['illiq_pc1'] = pca.transform(\n    liq_complete[z_cols].values\n)[:, 0]\n```\n:::\n\n\n## Aggregate Liquidity and Market Conditions {#sec-aggregate}\n\n### Time Series of Market Liquidity\n\nAggregate liquidity—the average illiquidity across all stocks—varies substantially over time. @chordia2001market document that market-wide liquidity declines during periods of high volatility and negative market returns.\n\n::: {#fig-aggregate-liquidity .cell execution_count=16}\n``` {.python .cell-code code-summary=\"Plot aggregate liquidity time series\"}\n# Compute monthly cross-sectional aggregates\nagg_liquidity = (\n    panel.groupby('month_end')\n    .agg(\n        amihud_median=('amihud', 'median'),\n        zero_ret_median=('zero_return_pct', 'median'),\n        turnover_median=('log_turnover', 'median'),\n        roll_median=('roll_spread', 'median'),\n        cs_median=('cs_spread', 'median'),\n        n_stocks=('ticker', 'nunique')\n    )\n    .reset_index()\n)\n\n# Standardize for plotting\nfor col in ['amihud_median', 'zero_ret_median', 'roll_median']:\n    agg_liquidity[col + '_z'] = (\n        (agg_liquidity[col] - agg_liquidity[col].mean())\n        / agg_liquidity[col].std()\n    )\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 9), height_ratios=[2, 1])\n\n# Panel A: Aggregate illiquidity\naxes[0].plot(agg_liquidity['month_end'], agg_liquidity['amihud_median_z'],\n             color='#2C5F8A', linewidth=1.5, label='Amihud')\naxes[0].plot(agg_liquidity['month_end'], agg_liquidity['zero_ret_median_z'],\n             color='#C0392B', linewidth=1.5, label='Zero-Return')\naxes[0].plot(agg_liquidity['month_end'], agg_liquidity['roll_median_z'],\n             color='#27AE60', linewidth=1.5, label='Roll Spread')\naxes[0].axhline(y=0, color='gray', linewidth=0.5)\naxes[0].set_ylabel('Standardized Illiquidity')\naxes[0].set_title('Panel A: Aggregate Illiquidity Over Time')\naxes[0].legend(fontsize=9)\n\n# Shade crisis periods\ncrisis_periods = [\n    ('2008-06-01', '2009-03-31', 'GFC'),\n    ('2011-01-01', '2011-12-31', 'Tightening'),\n    ('2020-02-01', '2020-05-31', 'COVID')\n]\nfor start, end, label in crisis_periods:\n    axes[0].axvspan(pd.Timestamp(start), pd.Timestamp(end),\n                     alpha=0.15, color='gray')\n    mid = pd.Timestamp(start) + (pd.Timestamp(end) - pd.Timestamp(start)) / 2\n    axes[0].text(mid, axes[0].get_ylim()[1] * 0.9, label,\n                 ha='center', fontsize=8, color='gray')\n\n# Panel B: Market return\nmarket_monthly = factors[['month_end', 'mkt_excess']].copy()\nmarket_monthly['month_end'] = pd.to_datetime(market_monthly['month_end'])\naxes[1].bar(market_monthly['month_end'],\n            market_monthly['mkt_excess'] * 100,\n            width=25,\n            color=['#27AE60' if r > 0 else '#C0392B'\n                   for r in market_monthly['mkt_excess']],\n            alpha=0.6)\naxes[1].set_ylabel('Market Excess Return (%)')\naxes[1].set_xlabel('Date')\naxes[1].set_title('Panel B: VN-Index Monthly Excess Return')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Commonality in Liquidity\n\n@chordia2000commonality find that individual stock liquidity co-moves with market liquidity, even after controlling for firm-specific factors. We test for commonality in Vietnam by regressing changes in firm-level liquidity on changes in market-level liquidity:\n\n$$\n\\Delta L_{i,m} = \\alpha_i + \\beta_i \\Delta L_{M,m} + \\gamma_i \\Delta L_{M,m-1} + \\delta_i \\Delta L_{M,m+1} + \\varepsilon_{i,m}\n$$ {#eq-commonality}\n\nwhere $\\Delta L_{i,m}$ is the change in firm $i$'s illiquidity, $\\Delta L_{M,m}$ is the change in market-average illiquidity (excluding firm $i$), and the lead/lag terms capture non-synchronous adjustment. The coefficient $\\beta_i$ measures the sensitivity of firm $i$'s liquidity to market-wide liquidity shocks.\n\n::: {#commonality .cell execution_count=17}\n``` {.python .cell-code code-summary=\"Test for commonality in liquidity across Vietnamese stocks\"}\n# Compute monthly changes in Amihud for each firm and the market\npanel_common = panel[['ticker', 'month_end', 'amihud']].dropna().copy()\npanel_common = panel_common.sort_values(['ticker', 'month_end'])\npanel_common['d_amihud'] = (\n    panel_common.groupby('ticker')['amihud'].diff()\n)\n\n# Market-level illiquidity change (equal-weighted, excluding firm i)\nmkt_liq = (\n    panel_common.groupby('month_end')['amihud']\n    .mean()\n    .diff()\n    .to_frame('d_amihud_mkt')\n)\nmkt_liq['d_amihud_mkt_lag'] = mkt_liq['d_amihud_mkt'].shift(1)\nmkt_liq['d_amihud_mkt_lead'] = mkt_liq['d_amihud_mkt'].shift(-1)\n\npanel_common = panel_common.merge(mkt_liq, on='month_end', how='left')\n\n# Estimate commonality for each firm\ndef estimate_commonality(group, min_obs=24):\n    g = group.dropna(subset=['d_amihud', 'd_amihud_mkt'])\n    if len(g) < min_obs:\n        return None\n    y = g['d_amihud']\n    X = sm.add_constant(g[['d_amihud_mkt', 'd_amihud_mkt_lag',\n                            'd_amihud_mkt_lead']])\n    try:\n        model = sm.OLS(y, X).fit()\n        return pd.Series({\n            'beta_mkt': model.params['d_amihud_mkt'],\n            'beta_t': model.tvalues['d_amihud_mkt'],\n            'r_squared': model.rsquared\n        })\n    except Exception:\n        return None\n\ncommonality = (\n    panel_common\n    .groupby('ticker')\n    .apply(estimate_commonality)\n    .dropna()\n)\n\nprint(\"Commonality in Liquidity (Amihud):\")\nprint(f\"  Mean beta_mkt: {commonality['beta_mkt'].mean():.3f}\")\nprint(f\"  Median beta_mkt: {commonality['beta_mkt'].median():.3f}\")\nprint(f\"  % significant at 5%: \"\n      f\"{(commonality['beta_t'].abs() > 1.96).mean():.1%}\")\nprint(f\"  Mean R-squared: {commonality['r_squared'].mean():.3f}\")\n```\n:::\n\n\n## Is Liquidity Priced? {#sec-pricing}\n\n### Portfolio Sorts\n\nWe test whether illiquidity predicts future returns by sorting stocks into quintile portfolios based on lagged liquidity measures and comparing average returns across quintiles.\n\n::: {#portfolio-sorts .cell execution_count=18}\n``` {.python .cell-code code-summary=\"Sort stocks into quintiles by lagged illiquidity and compute returns\"}\ndef liquidity_portfolio_sort(panel_df, liq_col, n_groups=5):\n    \"\"\"\n    Compute quintile portfolio returns sorted on lagged liquidity.\n    Lag the sorting variable by one month to avoid look-ahead bias.\n    \"\"\"\n    df = panel_df[['ticker', 'month_end', 'monthly_return',\n                    'market_cap', liq_col]].dropna().copy()\n    df = df.sort_values(['ticker', 'month_end'])\n    \n    # Lag the sorting variable\n    df['liq_lag'] = df.groupby('ticker')[liq_col].shift(1)\n    df = df.dropna(subset=['liq_lag', 'monthly_return'])\n    \n    # Assign quintiles within each month\n    df['quintile'] = (\n        df.groupby('month_end')['liq_lag']\n        .transform(lambda x: pd.qcut(x, n_groups, labels=False,\n                                       duplicates='drop'))\n    )\n    \n    # EW portfolio returns by quintile-month\n    port_returns = (\n        df.groupby(['month_end', 'quintile'])['monthly_return']\n        .mean()\n        .unstack()\n    )\n    \n    # Long-short (Q5 - Q1)\n    if n_groups - 1 in port_returns.columns and 0 in port_returns.columns:\n        port_returns['long_short'] = (\n            port_returns[n_groups - 1] - port_returns[0]\n        )\n    \n    return port_returns\n\n# Run sorts for each illiquidity measure\nsort_measures = {\n    'Amihud': 'amihud',\n    'Zero-Return': 'zero_return_pct',\n    'Neg. Turnover': 'log_turnover',  # Will reverse below\n    'Roll Spread': 'roll_spread',\n    'Corwin-Schultz': 'cs_spread',\n}\n\n# For turnover, negate so higher = less liquid\npanel_sorts = panel.copy()\npanel_sorts['neg_turnover'] = -panel_sorts['log_turnover']\nsort_measures_actual = {\n    'Amihud': 'amihud',\n    'Zero-Return': 'zero_return_pct',\n    'Neg. Turnover': 'neg_turnover',\n    'Roll Spread': 'roll_spread',\n    'Corwin-Schultz': 'cs_spread',\n}\n\nprint(\"Liquidity Premium (EW, Quintile Sorts):\")\nprint(f\"{'Measure':<18} {'Q1 (Liquid)':>12} {'Q5 (Illiquid)':>14} \"\n      f\"{'Q5-Q1':>10} {'t-stat':>8}\")\nprint(\"-\" * 62)\n\nsort_results = {}\nfor name, col in sort_measures_actual.items():\n    ports = liquidity_portfolio_sort(panel_sorts, col)\n    sort_results[name] = ports\n    \n    q1 = ports[0].mean() * 12\n    q5 = ports[4].mean() * 12 if 4 in ports.columns else np.nan\n    ls = ports['long_short'].mean() * 12 if 'long_short' in ports else np.nan\n    ls_se = ports['long_short'].std() / np.sqrt(len(ports)) * np.sqrt(12) if 'long_short' in ports else np.nan\n    t = ls / ls_se if ls_se and ls_se > 0 else np.nan\n    \n    print(f\"{name:<18} {q1:>12.4f} {q5:>14.4f} {ls:>10.4f} {t:>8.2f}\")\n```\n:::\n\n\n::: {#fig-liquidity-premium .cell execution_count=19}\n``` {.python .cell-code code-summary=\"Bar chart of quintile returns by liquidity measure\"}\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\naxes = axes.flatten()\n\ncolors_quintile = ['#27AE60', '#2ECC71', '#F1C40F', '#E67E22', '#C0392B']\n\nfor i, (name, ports) in enumerate(sort_results.items()):\n    if i >= 5:\n        break\n    quintile_means = [ports[q].mean() * 12 * 100 for q in range(5)\n                       if q in ports.columns]\n    axes[i].bar(range(len(quintile_means)), quintile_means,\n                color=colors_quintile[:len(quintile_means)],\n                alpha=0.85, edgecolor='white')\n    axes[i].set_xticks(range(len(quintile_means)))\n    axes[i].set_xticklabels([f'Q{q+1}' for q in range(len(quintile_means))])\n    axes[i].set_ylabel('Annualized Return (%)')\n    axes[i].set_title(name)\n    axes[i].axhline(y=0, color='gray', linewidth=0.5)\n\n# Hide unused subplot\nif len(sort_results) < 6:\n    axes[5].set_visible(False)\n\nplt.suptitle('Average Returns by Illiquidity Quintile', fontsize=14)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Fama-MacBeth Cross-Sectional Regressions\n\nPortfolio sorts are informative but cannot control for multiple characteristics simultaneously. We use @fama1993common-style cross-sectional regressions to test whether liquidity predicts returns after controlling for size, value, and momentum:\n\n$$\nR_{i,m+1} = \\gamma_{0,m} + \\gamma_{1,m} \\text{ILLIQ}_{i,m} + \\gamma_{2,m} \\ln(\\text{MCap}_{i,m}) + \\gamma_{3,m} \\text{BM}_{i,m} + \\gamma_{4,m} R_{i,m-12:m-1} + \\varepsilon_{i,m+1}\n$$ {#eq-fama-macbeth}\n\nThe time-series average of the monthly coefficient $\\bar{\\gamma}_1$ estimates the illiquidity premium, and its t-statistic uses the @fama1993common standard error.\n\n::: {#fama-macbeth .cell execution_count=20}\n``` {.python .cell-code code-summary=\"Fama-MacBeth regressions of returns on lagged liquidity and controls\"}\ndef fama_macbeth(panel_df, illiq_col, controls=['log_mcap', 'bm'],\n                  min_stocks=50):\n    \"\"\"\n    Run Fama-MacBeth cross-sectional regressions of\n    next-month returns on lagged illiquidity and controls.\n    \"\"\"\n    df = panel_df.copy()\n    df = df.sort_values(['ticker', 'month_end'])\n    \n    # Lag the illiquidity measure\n    df['illiq_lag'] = df.groupby('ticker')[illiq_col].shift(1)\n    \n    # Lag controls\n    for c in controls:\n        df[c + '_lag'] = df.groupby('ticker')[c].shift(1)\n    \n    regressors = ['illiq_lag'] + [c + '_lag' for c in controls]\n    df = df.dropna(subset=['monthly_return'] + regressors)\n    \n    # Month-by-month cross-sectional regressions\n    months = sorted(df['month_end'].unique())\n    gamma_list = []\n    \n    for month in months:\n        cross = df[df['month_end'] == month]\n        if len(cross) < min_stocks:\n            continue\n        \n        y = cross['monthly_return'].values\n        X = sm.add_constant(cross[regressors].values)\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            gammas = {'month_end': month, 'intercept': model.params[0]}\n            for j, reg in enumerate(regressors):\n                gammas[reg] = model.params[j + 1]\n            gamma_list.append(gammas)\n        except Exception:\n            pass\n    \n    gamma_df = pd.DataFrame(gamma_list)\n    \n    # Time-series averages and t-statistics\n    results = {}\n    for col in ['intercept'] + regressors:\n        mean = gamma_df[col].mean()\n        se = gamma_df[col].std() / np.sqrt(len(gamma_df))\n        t = mean / se if se > 0 else np.nan\n        results[col] = {'Coefficient': mean, 'SE': se, 't-stat': t}\n    \n    return pd.DataFrame(results).T, gamma_df\n\n# Run for each illiquidity measure\nprint(\"Fama-MacBeth Regressions: R_{i,m+1} on ILLIQ_{i,m} + controls\")\nprint(\"=\" * 70)\n\nfor name, col in [('Amihud', 'amihud'),\n                    ('Zero-Return', 'zero_return_pct'),\n                    ('Roll Spread', 'roll_spread'),\n                    ('Corwin-Schultz', 'cs_spread')]:\n    results, gammas = fama_macbeth(panel, col)\n    print(f\"\\n{name}:\")\n    print(results[['Coefficient', 't-stat']].round(4).to_string())\n```\n:::\n\n\n### Factor-Adjusted Liquidity Premium\n\nThe liquidity premium may be partially or fully explained by existing risk factors (size, value, momentum). We test this by regressing the long-short liquidity portfolio returns on the Fama-French-Carhart factors:\n\n::: {#factor-adjusted .cell execution_count=21}\n``` {.python .cell-code code-summary=\"Regress liquidity long-short returns on FF+Carhart factors\"}\nprint(\"Factor-Adjusted Liquidity Premium:\")\nprint(f\"{'Measure':<18} {'Alpha (ann.)':>12} {'Alpha t':>10} \"\n      f\"{'MKT':>8} {'SMB':>8} {'HML':>8} {'R2':>6}\")\nprint(\"-\" * 72)\n\nfor name, ports in sort_results.items():\n    if 'long_short' not in ports.columns:\n        continue\n    \n    ls_series = ports['long_short'].to_frame('ls')\n    ls_series.index = pd.to_datetime(ls_series.index)\n    \n    merged = ls_series.merge(factors, left_index=True,\n                              right_on='month_end', how='inner')\n    \n    if len(merged) < 24:\n        continue\n    \n    y = merged['ls']\n    X = sm.add_constant(merged[['mkt_excess', 'smb', 'hml', 'wml']])\n    \n    model = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n    \n    alpha_ann = model.params['const'] * 12\n    alpha_t = model.tvalues['const']\n    mkt_b = model.params['mkt_excess']\n    smb_b = model.params['smb']\n    hml_b = model.params['hml']\n    r2 = model.rsquared\n    \n    print(f\"{name:<18} {alpha_ann:>12.4f} {alpha_t:>10.2f} \"\n          f\"{mkt_b:>8.3f} {smb_b:>8.3f} {hml_b:>8.3f} {r2:>6.3f}\")\n```\n:::\n\n\n## Liquidity and Transaction Cost Estimation {#sec-implementation}\n\n### Translating Measures to Trading Costs\n\nFor practitioners, the key question is: what does a given Amihud or spread value *mean* in terms of actual VND cost per trade? We calibrate the relationship between our low-frequency proxies and explicit trading costs.\n\n::: {#cost-calibration .cell execution_count=22}\n``` {.python .cell-code code-summary=\"Calibrate liquidity proxies to estimated round-trip trading costs\"}\ndef estimate_round_trip_cost(row):\n    \"\"\"\n    Estimate total round-trip trading cost (in %) from\n    multiple liquidity proxies.\n    \n    Components:\n    1. Explicit: commission + tax (~0.35% round-trip)\n    2. Spread cost: half-spread each way\n    3. Price impact: function of trade size\n    \"\"\"\n    explicit = 0.0035  # 35 bps round-trip\n    \n    # Use Corwin-Schultz or quoted spread as spread estimate\n    spread = row.get('cs_spread', row.get('quoted_spread', 0.005))\n    spread_cost = spread  # Full spread = round-trip cost\n    \n    # Price impact (approximate from Amihud)\n    # For a trade of 1% of daily volume\n    amihud_raw = row.get('amihud_raw', 0)\n    impact = amihud_raw * 0.01  # Rough approximation\n    \n    return explicit + spread_cost + impact\n\npanel['estimated_rtc'] = panel.apply(estimate_round_trip_cost, axis=1)\n\n# Distribution by size quintile\nrtc_by_size = (\n    panel.groupby('size_quintile')['estimated_rtc']\n    .agg(['mean', 'median', 'std'])\n    .round(4)\n)\nprint(\"Estimated Round-Trip Cost by Size Quintile (%):\")\nprint((rtc_by_size * 100).round(2).to_string())\n```\n:::\n\n\n::: {#fig-cost-distribution .cell execution_count=23}\n``` {.python .cell-code code-summary=\"Plot estimated trading costs by size quintile\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Distribution\nfor q, color in zip(['Q1 (Small)', 'Q3', 'Q5 (Large)'],\n                     ['#C0392B', '#F1C40F', '#27AE60']):\n    subset = panel[panel['size_quintile'] == q]['estimated_rtc'].dropna()\n    subset = subset[subset < 0.15]  # Trim extreme\n    axes[0].hist(subset * 100, bins=50, density=True, alpha=0.5,\n                  color=color, label=q, edgecolor='white')\naxes[0].set_xlabel('Estimated Round-Trip Cost (%)')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Panel A: Cost Distribution by Size')\naxes[0].legend()\n\n# Panel B: Time series of median cost\ncost_ts = (\n    panel.groupby('month_end')['estimated_rtc']\n    .median()\n    .reset_index()\n)\naxes[1].plot(pd.to_datetime(cost_ts['month_end']),\n             cost_ts['estimated_rtc'] * 100,\n             color='#2C5F8A', linewidth=1.5)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Median Round-Trip Cost (%)')\naxes[1].set_title('Panel B: Aggregate Trading Costs Over Time')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Strategy Implementability\n\nA critical application of liquidity measurement is testing whether a given anomaly strategy remains profitable after accounting for realistic trading costs. We compute net-of-cost returns for the liquidity-sorted portfolios themselves—an inherently conservative test because the illiquid long leg carries the highest costs.\n\n::: {#net-of-cost .cell execution_count=24}\n``` {.python .cell-code code-summary=\"Compute net-of-cost returns for illiquidity-sorted portfolios\"}\n# For each quintile, estimate average monthly turnover and cost\n# and subtract from gross returns\nfor name, ports in sort_results.items():\n    if 'long_short' not in ports.columns:\n        continue\n    \n    gross_ann = ports['long_short'].mean() * 12\n    \n    # Estimate costs: illiquid quintile has higher costs\n    # Assume monthly turnover of ~15% for long-short with monthly rebalancing\n    turnover = 0.15\n    cost_q1 = 0.003  # 30 bps per trade for liquid stocks\n    cost_q5 = 0.015  # 150 bps for illiquid stocks\n    avg_cost = (cost_q1 + cost_q5) / 2  # Average across long and short\n    monthly_tc = turnover * avg_cost\n    \n    net_ann = gross_ann - monthly_tc * 12\n    \n    print(f\"{name:<18}: Gross = {gross_ann*100:>6.2f}%, \"\n          f\"TC = {monthly_tc*1200:>6.1f} bps/mo, \"\n          f\"Net = {net_ann*100:>6.2f}%\")\n```\n:::\n\n\n## Liquidity During Market Stress {#sec-stress}\n\n### Flight to Liquidity\n\nDuring market stress, investors sell illiquid assets and buy liquid ones—a \"flight to liquidity\" that widens the return differential between liquid and illiquid stocks. @hameed2010stock show that this pattern is strongest when market returns are most negative.\n\n::: {#flight-to-liquidity .cell execution_count=25}\n``` {.python .cell-code code-summary=\"Test for flight-to-liquidity during market stress\"}\n# Merge Amihud-sorted portfolio returns with market returns\namihud_ports = sort_results.get('Amihud')\nif amihud_ports is not None and 'long_short' in amihud_ports.columns:\n    ftl_data = pd.merge(\n        amihud_ports['long_short'].to_frame('illiq_premium'),\n        factors[['month_end', 'mkt_excess']].set_index('month_end'),\n        left_index=True, right_index=True, how='inner'\n    )\n    \n    # Classify market states\n    ftl_data['mkt_state'] = pd.cut(\n        ftl_data['mkt_excess'],\n        bins=[-np.inf,\n              ftl_data['mkt_excess'].quantile(0.20),\n              ftl_data['mkt_excess'].quantile(0.80),\n              np.inf],\n        labels=['Bear (bottom 20%)', 'Normal', 'Bull (top 20%)']\n    )\n    \n    # Illiquidity premium by market state\n    state_premium = (\n        ftl_data.groupby('mkt_state')['illiq_premium']\n        .agg(['mean', 'std', 'count'])\n    )\n    state_premium['ann_premium'] = state_premium['mean'] * 12\n    state_premium['t_stat'] = (\n        state_premium['mean']\n        / (state_premium['std'] / np.sqrt(state_premium['count']))\n    )\n    \n    print(\"Illiquidity Premium by Market State:\")\n    print(state_premium[['ann_premium', 't_stat', 'count']].round(3))\n```\n:::\n\n\n::: {#fig-flight-to-liquidity .cell execution_count=26}\n``` {.python .cell-code code-summary=\"Bar chart of conditional illiquidity premium\"}\nfig, ax = plt.subplots(figsize=(8, 5))\n\nif 'state_premium' in dir():\n    colors_state = ['#C0392B', '#F1C40F', '#27AE60']\n    bars = ax.bar(range(len(state_premium)),\n                   state_premium['ann_premium'] * 100,\n                   color=colors_state, alpha=0.85, edgecolor='white')\n    ax.set_xticks(range(len(state_premium)))\n    ax.set_xticklabels(state_premium.index)\n    ax.set_ylabel('Annualized Q5-Q1 Return (%)')\n    ax.set_title('Illiquidity Premium by Market State')\n    ax.axhline(y=0, color='gray', linewidth=0.8)\n    \n    for i, (_, row) in enumerate(state_premium.iterrows()):\n        ax.text(i, row['ann_premium'] * 100 + 0.3,\n                f\"t={row['t_stat']:.1f}\",\n                ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Liquidity Co-Movement with Global Risk\n\nVietnamese market liquidity may be driven by global risk factors, particularly for stocks held by foreign investors. We test whether global risk measures (VIX, USD strength) predict Vietnamese aggregate liquidity:\n\n::: {#global-risk .cell execution_count=27}\n``` {.python .cell-code code-summary=\"Regress Vietnamese aggregate liquidity on global risk proxies\"}\n# Merge aggregate liquidity with global variables\nglobal_vars = client.get_macro_data(\n    variables=['vix_close', 'dxy_index'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    frequency='monthly'\n)\n\nagg_liq_global = agg_liquidity.merge(\n    global_vars, on='month_end', how='inner'\n)\n\n# Changes in all variables\nfor col in ['amihud_median', 'vix_close', 'dxy_index']:\n    agg_liq_global[f'd_{col}'] = agg_liq_global[col].diff()\n\n# Regression\ny = agg_liq_global['d_amihud_median'].dropna()\nX = sm.add_constant(\n    agg_liq_global.loc[y.index, ['d_vix_close', 'd_dxy_index']]\n)\n\nmodel = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 3})\nprint(\"Aggregate Illiquidity ~ Global Risk:\")\nprint(model.summary().tables[1])\n```\n:::\n\n\n## Constructing a Tradeable Liquidity Factor {#sec-factor}\n\nFollowing @pastor2003liquidity, we construct an aggregate liquidity factor that can be used in asset pricing tests. The factor captures innovations in aggregate liquidity—unexpected changes in market-wide trading conditions.\n\n::: {#liquidity-factor .cell execution_count=28}\n``` {.python .cell-code code-summary=\"Construct an aggregate liquidity innovation factor\"}\n# Step 1: Compute market-level Amihud as EW average\nmkt_amihud = (\n    panel.groupby('month_end')['amihud']\n    .mean()\n    .to_frame('mkt_amihud')\n)\n\n# Step 2: Estimate AR(2) model for aggregate illiquidity\nmkt_amihud['mkt_amihud_lag1'] = mkt_amihud['mkt_amihud'].shift(1)\nmkt_amihud['mkt_amihud_lag2'] = mkt_amihud['mkt_amihud'].shift(2)\nmkt_amihud = mkt_amihud.dropna()\n\nar_model = sm.OLS(\n    mkt_amihud['mkt_amihud'],\n    sm.add_constant(mkt_amihud[['mkt_amihud_lag1', 'mkt_amihud_lag2']])\n).fit()\n\n# Step 3: Residuals = liquidity innovations\n# Negative innovation = liquidity improved (good)\n# Positive innovation = liquidity deteriorated (bad)\nmkt_amihud['liq_innovation'] = ar_model.resid\n\n# Step 4: Test whether liquidity innovations predict returns\n# Higher sensitivity to negative innovations = higher expected return\nprint(\"AR(2) Model for Aggregate Amihud:\")\nprint(f\"  R-squared: {ar_model.rsquared:.3f}\")\nprint(f\"  AR(1) coef: {ar_model.params['mkt_amihud_lag1']:.3f} \"\n      f\"(t={ar_model.tvalues['mkt_amihud_lag1']:.2f})\")\n\n# Step 5: Estimate liquidity betas for each firm\npanel_liq_beta = panel.merge(\n    mkt_amihud[['liq_innovation']],\n    left_on='month_end', right_index=True, how='inner'\n)\n\ndef estimate_liq_beta(group, min_obs=36):\n    g = group.dropna(subset=['monthly_return', 'liq_innovation'])\n    if len(g) < min_obs:\n        return None\n    y = g['monthly_return']\n    X = sm.add_constant(g['liq_innovation'])\n    try:\n        model = sm.OLS(y, X).fit()\n        return model.params['liq_innovation']\n    except Exception:\n        return None\n\nliq_betas = (\n    panel_liq_beta\n    .groupby('ticker')\n    .apply(estimate_liq_beta)\n    .dropna()\n    .to_frame('liq_beta')\n)\n\nprint(f\"\\nLiquidity Beta Distribution:\")\nprint(liq_betas['liq_beta'].describe().round(4))\n```\n:::\n\n\n## Practical Guidance for Vietnam {#sec-practical}\n\nThe analysis in this chapter yields the following recommendations:\n\n**For researchers:** The Amihud illiquidity ratio is the single best all-purpose liquidity proxy for Vietnamese equities. It has the highest coverage, the strongest cross-sectional return predictability, and the most robust relationship with firm size. When a second measure is needed for robustness, the zero-return proportion is the natural complement—it captures a different dimension (transaction cost threshold) and has near-complete coverage.\n\n**For portfolio construction:** Any backtest of a Vietnamese equity strategy should compute and report estimated round-trip costs by quintile. Strategies that load on the bottom two size quintiles face costs of 2--5% per round trip, making monthly rebalancing uneconomical. Quarterly or annual rebalancing with a turnover constraint is more realistic.\n\n**For risk management:** Monitor aggregate liquidity conditions using the cross-sectional median Amihud or the market-wide zero-return fraction. Liquidity deterioration predicts negative market returns and wider spreads in subsequent months. Tighten risk limits when aggregate illiquidity exceeds its 90th historical percentile.\n\n**For international comparisons:** When comparing Vietnamese factor premia to U.S. or other developed market evidence, always report results on a \"liquid universe\" subset (top 60% by market cap) alongside the full sample. Many anomalies that appear large in the full sample shrink substantially when restricted to stocks that can actually be traded at scale.\n\n## Summary {#sec-summary}\n\n| Measure | Dimension | Coverage | Size Gradient | Return Predictive | Recommended Use |\n|------------|------------|------------|------------|------------|------------|\n| Amihud | Price impact | High | Very steep | Strong | Primary proxy; portfolio sorts; Fama-MacBeth |\n| Zero-Return | Transaction cost | Complete | Steep | Strong | Robustness check; emerging market studies |\n| Turnover | Trading activity | High | Moderate | Moderate | Volume-based filters; flow analysis |\n| Roll Spread | Tightness | Moderate | Moderate | Moderate | Spread estimation without bid-ask data |\n| Corwin-Schultz | Tightness | Moderate | Moderate | Moderate | High-low based spread; calibration |\n| Quoted Spread | Tightness | Variable | Steep | Strong | Direct measure when available |\n| Kyle Lambda | Price impact | Moderate | Steep | Strong | Market microstructure research |\n\n: Summary of liquidity measure properties in the Vietnamese market. {#tbl-summary}Liquidity is not a secondary consideration for Vietnamese equity research—it is a first-order determinant of which strategies are implementable, which anomalies are real, and which results are artifacts of trading in stocks that cannot actually be traded. Every empirical finding in this book should be evaluated through the lens of the liquidity analysis developed in this chapter.\n\n```{=html}\n<!-- ## Exercises {#sec-exercises}\n\n1. **Closing percent quoted spread.** @fong2017best recommend the closing percent quoted spread as the best global liquidity proxy. Compute this as the closing bid-ask spread divided by the midpoint, averaged over the month. Compare its correlation with the Amihud measure and the Corwin-Schultz spread. Does the recommendation hold for Vietnam?\n\n2. **Liquidity-adjusted momentum.** Compute momentum portfolio returns (12-1 month) separately within each Amihud illiquidity tercile. Is the momentum premium concentrated in illiquid stocks? What does this imply about the implementability of momentum strategies in Vietnam? Connect your findings to @novy2016taxonomy.\n\n3. **Foreign investor liquidity demand.** Using foreign buy and sell volume data, compute a net foreign flow measure for each stock. Test whether foreign net buying predicts next-month liquidity improvements (lower Amihud, lower spreads). Does foreign ownership create a liquidity externality?\n\n4. **Intraday liquidity patterns.** If intraday data are available, compute the Amihud ratio separately for the opening call auction, the morning continuous session, and the afternoon continuous session. How does intraday liquidity variation compare to cross-sectional variation?\n\n5. **Liquidity and earnings announcements.** Test whether liquidity deteriorates before and improves after quarterly earnings announcements. Use an event study framework with the Amihud ratio as the dependent variable. Is the pre-announcement illiquidity increase consistent with the @glosten1985bid adverse selection model?\n\n6. **Market-making profitability.** Using the Roll spread as a proxy for the bid-ask bounce, estimate the gross profit a hypothetical market maker would earn from providing liquidity in Vietnamese stocks. How does this compare to the explicit cost of capital for market making?\n\n7. **Liquidity factor mimicking portfolio.** Construct a tradeable liquidity factor by sorting stocks into deciles based on their liquidity beta (sensitivity to aggregate liquidity innovations). The long-short return of this factor should be correlated with aggregate liquidity changes. Test whether this factor earns a significant premium and whether it explains returns beyond the Fama-French factors.\n\n8. **COVID-19 liquidity shock.** Analyze the liquidity dynamics around the March 2020 COVID-19 market crash. How quickly did aggregate liquidity deteriorate? Which firms experienced the largest liquidity drops? Did the SBV's policy responses (rate cuts, reserve requirement reductions) improve liquidity, and with what lag? -->\n```\n\n",
    "supporting": [
      "16_liquidity_and_turnover_measures_files/figure-pdf"
    ],
    "filters": []
  }
}