{
  "hash": "6df450281d53559a4f72be2763d087d1",
  "result": {
    "engine": "jupyter",
    "markdown": "# Structural Models in Finance\n\nMost of empirical finance is reduced-form: regress returns on characteristics, estimate risk premia from factor portfolios, test whether a coefficient is significantly different from zero. Structural estimation takes a fundamentally different approach. It begins with an explicit economic model (i.e., specifying agents' preferences, information sets, constraints, and optimization problems) and estimates the model's primitive parameters directly from observed data. The payoff is the ability to perform counterfactual analysis: what would happen to prices if trading costs fell by half? How would IPO underpricing change if the allocation mechanism switched from book building to a uniform-price auction? What is the welfare cost of a particular market design choice? These questions are unanswerable within a reduced-form framework because they require knowledge of the data-generating process, not merely statistical associations within a single regime.\n\nThis chapter introduces the key structural estimation frameworks used in financial economics and implements them for Vietnamese equity markets. We cover four domains where structural models have proven most productive: investor demand estimation, trading cost and execution models, primary market auctions, and limit order book dynamics. Each domain involves distinct economic primitives such as preferences, beliefs, transaction costs, information asymmetries and each requires domain-specific identification strategies.\n\nVietnamese markets offer both challenges and opportunities for structural estimation. On the challenge side, data quality is uneven, market microstructure is evolving, and institutional features (price limits, foreign ownership caps, state ownership) introduce constraints that standard models do not accommodate. On the opportunity side, several features of Vietnamese markets create natural variation that aids identification: the coexistence of two exchanges (HOSE and HNX) with different trading rules, regulatory changes that shift trading costs and price limits, and the phased liberalization of foreign ownership caps that generates exogenous demand shocks.\n\n::: {#setup .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats, optimize\nfrom scipy.optimize import minimize, minimize_scalar\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom linearmodels.panel import PanelOLS\nfrom linearmodels.iv import IV2SLS\nimport plotnine as p9\nfrom mizani.formatters import percent_format, comma_format\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n\n## What Structural Estimation Means in Finance\n\n### Reduced Form Versus Structural\n\nConsider two researchers studying the effect of transaction costs on trading volume. The reduced-form researcher exploits a fee reduction event and estimates:\n\n$$\n\\ln V_{i,t} = \\alpha + \\beta \\cdot \\text{Post}_t + \\gamma \\mathbf{X}_{i,t} + \\varepsilon_{i,t}\n$$ {#eq-reduced-form}\n\nThe coefficient $\\beta$ identifies the causal effect of the fee change on volume, but it is local to this specific fee change, this specific market, and this specific time period. It says nothing about what a different fee change would do, or what the optimal fee structure might be.\n\nThe structural researcher writes down an explicit model of trader behavior:\n\n$$\n\\max_{q_t} \\; E_t\\left[\\sum_{s=t}^{T} \\delta^{s-t}\\left(v_s q_s - c(q_s; \\boldsymbol{\\theta})\\right)\\right]\n$$ {#eq-structural-trader}\n\nwhere $v_s$ is the (possibly private) valuation, $q_s$ is the trade quantity, $c(\\cdot; \\boldsymbol{\\theta})$ is the cost function parameterized by $\\boldsymbol{\\theta}$, and $\\delta$ is the discount factor. The researcher estimates $\\boldsymbol{\\theta}$ by requiring that the model's predictions match observed trading patterns. With $\\hat{\\boldsymbol{\\theta}}$ in hand, any counterfactual cost function $c'(\\cdot; \\boldsymbol{\\theta}')$ can be fed into the model to predict the equilibrium response.\n\nThe distinction is not about sophistication (reduced-form work can be highly rigorous) but about the type of question being answered:\n\n|   | Reduced Form | Structural |\n|------------------|------------------------------|-------------------------|\n| Answers | \"What happened?\" | \"What would happen if...?\" |\n| Identification | Exogenous variation (events, instruments) | Model restrictions + data moments |\n| Key assumption | Unconfoundedness or exclusion restriction | Correct model specification |\n| Output | Treatment effects, associations | Primitive parameters, counterfactuals |\n| Risk | Omitted variable bias, weak instruments | Model misspecification |\n\n: Reduced Form vs. Structural Estimation {#tbl-rf-vs-structural}\n\n### Identification Through Economic Primitives\n\nStructural identification relies on the economic model to convert observed outcomes into unobserved primitives. The classic example is the demand-supply system. Observing prices and quantities alone cannot identify demand and supply curves (the simultaneity problem). But if we know the functional form of demand and supply, and have instruments that shift one curve but not the other, the system is identified.\n\nIn financial contexts, identification often comes from the model's equilibrium conditions. In @kyle1985continuous, the informed trader's strategy, the market maker's pricing rule, and the noise trader's demand jointly determine the equilibrium price impact coefficient $\\lambda$. The model maps the observable (i.e., the price impact of order flow) to the unobservable (i.e., the precision of private information). The identification is through the model's structure, not through a conventional instrument.\n\nFormally, let $\\boldsymbol{\\theta} \\in \\Theta$ denote the vector of structural parameters and $\\mathbf{m}(\\boldsymbol{\\theta})$ the model-implied moments. The data provide empirical moments $\\hat{\\mathbf{m}}$. Identification requires that the mapping $\\boldsymbol{\\theta} \\mapsto \\mathbf{m}(\\boldsymbol{\\theta})$ is injective: different parameter values produce different observable implications.\n\n$$\n\\hat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta} \\in \\Theta} \\left[\\hat{\\mathbf{m}} - \\mathbf{m}(\\boldsymbol{\\theta})\\right]' \\mathbf{W} \\left[\\hat{\\mathbf{m}} - \\mathbf{m}(\\boldsymbol{\\theta})\\right]\n$$ {#eq-gmm-structural}\n\nThis is the Generalized Method of Moments (GMM) framework applied to structural estimation. The choice of weighting matrix $\\mathbf{W}$ determines efficiency, and over-identifying restrictions (more moments than parameters) provide specification tests.\n\n### Tradeoffs Between Realism and Tractability\n\nEvery structural model involves choices about which features of reality to include and which to abstract from. @tbl-tradeoff illustrates this for several canonical finance models.\n\n| Model | Key Simplification | What It Misses | What It Gains |\n|-----------------|---------------------|-----------------|-----------------|\n| @kyle1985continuous | Single informed trader, normal distributions | Multiple insiders, fat tails | Closed-form $\\lambda$, clean identification |\n| @glosten1985bid | Binary signal, competitive market makers | Complex information, inventory costs | Bid-ask spread decomposition |\n| @koijen2019demand | Characteristics-based demand, no dynamics | Dynamic portfolio rebalancing | Demand elasticity estimation at scale |\n| @roll1984simple | No information, serial independence | Information-driven trades | Spread estimation from return autocovariance |\n\n: Structural Model Tradeoffs {#tbl-tradeoff}\n\nThe researcher's task is to choose the simplest model that captures the economic mechanism of interest while remaining rich enough that its counterfactual predictions are credible. As @rust1987optimal emphasized, there is a tension between models that are \"structurally correct\" but computationally intractable and models that are tractable but potentially misspecified.\n\n## Demand Estimation for Financial Assets\n\n### The Demand System Approach\n\n@koijen2019demand (hereafter KY) develop a demand system for financial assets that estimates the elasticity of institutional investor demand with respect to asset characteristics. The framework adapts the discrete-choice demand estimation methodology of @berry1995automobile from industrial organization to financial markets.\n\nEach investor $i$ holds a portfolio of $N$ assets. The demand for asset $j$ by investor $i$ at time $t$ is:\n\n$$\nw_{ij,t} = \\frac{\\exp(\\delta_{j,t} + \\boldsymbol{\\beta}_i' \\mathbf{x}_{j,t})}{1 + \\sum_{k=1}^{N} \\exp(\\delta_{k,t} + \\boldsymbol{\\beta}_i' \\mathbf{x}_{k,t})}\n$$ {#eq-ky-demand}\n\nwhere $w_{ij,t}$ is the portfolio weight of asset $j$ for investor $i$, $\\delta_{j,t}$ is the mean utility (common valuation), $\\mathbf{x}_{j,t}$ is a vector of observable characteristics (market cap, book-to-market, momentum, etc.), and $\\boldsymbol{\\beta}_i$ captures investor-specific preferences (heterogeneous demand elasticities). The denominator includes 1 to represent the outside option (holding cash or non-equity assets).\n\nThe key insight is that the market-clearing condition links demand to equilibrium prices:\n\n$$\n\\sum_i A_{i,t} \\cdot w_{ij,t}(\\boldsymbol{\\theta}) = \\text{ME}_{j,t} \\qquad \\forall j, t\n$$ {#eq-market-clearing}\n\nwhere $A_{i,t}$ is investor $i$'s total assets under management and $\\text{ME}_{j,t}$ is the market capitalization of asset $j$. This system of equations determines the equilibrium price impacts and demand elasticities.\n\n### Demand Elasticity Estimation\n\nThe demand elasticity of asset $j$ with respect to its own price (or a characteristic that moves its price) is:\n\n$$\n\\varepsilon_{jj} = \\frac{\\partial \\ln w_{ij}}{\\partial \\ln P_j} = \\beta_{\\text{price}} \\cdot (1 - w_{ij})\n$$ {#eq-demand-elasticity}\n\nIn a frictionless market with perfectly elastic demand, a supply shock (e.g., an index addition) would have zero price impact. Empirically, demand curves for financial assets slope downward with finite elasticity, implying that supply shocks move equilibrium prices.\n\n::: {#load-holdings-data .cell execution_count=2}\n``` {.python .cell-code}\n# DataCore.vn API\nfrom datacore import DataCore\ndc = DataCore()\n\n# Load institutional holdings data\nholdings = dc.get_institutional_holdings(\n    start_date=\"2012-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Load firm characteristics\nfirm_chars = dc.get_firm_characteristics(\n    start_date=\"2012-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Load market data\nmarket_data = dc.get_daily_returns(\n    start_date=\"2012-01-01\",\n    end_date=\"2024-12-31\"\n)\n\nprint(f\"Holdings observations: {len(holdings)}\")\nprint(f\"Unique institutions: {holdings['institution_id'].nunique()}\")\nprint(f\"Unique stocks: {holdings['ticker'].nunique()}\")\n```\n:::\n\n\n::: {#demand-system-construction .cell execution_count=3}\n``` {.python .cell-code}\n# Construct demand system variables\ndemand = holdings.merge(\n    firm_chars[[\"ticker\", \"date\", \"market_cap\", \"book_to_market\",\n                \"momentum_12m\", \"log_size\", \"profitability\",\n                \"investment\", \"industry\"]],\n    on=[\"ticker\", \"date\"],\n    how=\"inner\"\n)\n\n# Portfolio weights\ndemand[\"total_aum\"] = demand.groupby(\n    [\"institution_id\", \"date\"]\n)[\"holding_value\"].transform(\"sum\")\n\ndemand[\"port_weight\"] = demand[\"holding_value\"] / demand[\"total_aum\"]\n\n# Log portfolio weight relative to outside option\n# w_ij / w_i0 where w_i0 = 1 - sum(w_ij) for equity holdings\ndemand[\"sum_equity_weight\"] = demand.groupby(\n    [\"institution_id\", \"date\"]\n)[\"port_weight\"].transform(\"sum\")\n\ndemand[\"outside_weight\"] = 1 - demand[\"sum_equity_weight\"].clip(upper=0.99)\n\ndemand[\"log_weight_ratio\"] = np.log(\n    demand[\"port_weight\"] / demand[\"outside_weight\"]\n)\n```\n:::\n\n\n::: {#demand-estimation .cell execution_count=4}\n``` {.python .cell-code}\n# Simplified KY demand estimation\n# log(w_ij / w_i0) = delta_j + beta_i' x_j + epsilon_ij\n# First stage: estimate mean utility delta_j via OLS with asset FE\n\n# Cross-sectional regression at each date\ndef estimate_demand_cross_section(group):\n    \"\"\"\n    Estimate demand parameters for a single cross-section.\n    Uses OLS with stock fixed effects absorbed.\n    \"\"\"\n    g = group.dropna(subset=[\n        \"log_weight_ratio\", \"log_size\", \"book_to_market\",\n        \"momentum_12m\", \"profitability\"\n    ])\n\n    if len(g) < 50:\n        return pd.Series(dtype=float)\n\n    X = g[[\"log_size\", \"book_to_market\", \"momentum_12m\",\n           \"profitability\"]].values\n    X = sm.add_constant(X)\n    y = g[\"log_weight_ratio\"].values\n\n    try:\n        model = sm.OLS(y, X).fit()\n        return pd.Series({\n            \"beta_size\": model.params[1],\n            \"beta_btm\": model.params[2],\n            \"beta_mom\": model.params[3],\n            \"beta_prof\": model.params[4],\n            \"r_squared\": model.rsquared,\n            \"n_obs\": len(g)\n        })\n    except Exception:\n        return pd.Series(dtype=float)\n\n# Estimate by institution type\ndemand[\"institution_type\"] = demand[\"institution_type\"].fillna(\"Other\")\n\ndemand_params = (\n    demand.groupby([\"institution_type\", \"date\"])\n    .apply(estimate_demand_cross_section)\n    .reset_index()\n    .dropna()\n)\n```\n:::\n\n\n::: {#tbl-demand-params .cell tbl-cap='Demand Elasticities by Investor Type' execution_count=5}\n``` {.python .cell-code}\navg_params = (\n    demand_params.groupby(\"institution_type\")\n    .agg(\n        beta_size=(\"beta_size\", \"mean\"),\n        beta_btm=(\"beta_btm\", \"mean\"),\n        beta_mom=(\"beta_mom\", \"mean\"),\n        beta_prof=(\"beta_prof\", \"mean\"),\n        avg_r2=(\"r_squared\", \"mean\"),\n        n_periods=(\"date\", \"nunique\")\n    )\n    .round(4)\n)\n\navg_params\n```\n:::\n\n\n::: {#fig-demand-elasticity-time .cell execution_count=6}\n``` {.python .cell-code}\ntop_types = demand_params.groupby(\"institution_type\").size().nlargest(4).index\nplot_data = demand_params[\n    demand_params[\"institution_type\"].isin(top_types)\n].copy()\n\n(\n    p9.ggplot(plot_data, p9.aes(\n        x=\"date\", y=\"beta_size\", color=\"institution_type\"\n    ))\n    + p9.geom_smooth(method=\"lowess\", se=False, size=1)\n    + p9.labs(\n        x=\"\", y=\"β (Size)\",\n        title=\"Institutional Demand Sensitivity to Firm Size\",\n        color=\"Institution Type\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5), legend_position=\"top\")\n)\n```\n:::\n\n\n### Price Pressure and Market Impact\n\nThe demand system framework directly yields price impact predictions. If investor $i$ receives an exogenous inflow $\\Delta A_i$ (e.g., from a fund flow shock), the price of each stock must adjust to clear the market with the new demand. The equilibrium price impact of a demand shock is:\n\n$$\n\\frac{\\Delta P_j}{P_j} = \\frac{1}{\\varepsilon_{jj}} \\cdot \\frac{\\Delta D_j}{D_j}\n$$ {#eq-price-impact-demand}\n\nwhere $\\varepsilon_{jj}$ is the demand elasticity and $\\Delta D_j / D_j$ is the percentage demand shock. Less elastic demand implies larger price impact for the same demand shock (a prediction with direct implications for the price impact of index rebalancing, fund flows, and forced selling).\n\n::: {#price-pressure .cell execution_count=7}\n``` {.python .cell-code}\n# Estimate aggregate demand elasticity via index rebalancing events\n# Use VN30 index reconstitutions as demand shocks\n\nindex_changes = dc.get_index_changes(\n    index=\"VN30\",\n    start_date=\"2012-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Event study: abnormal returns around index additions/deletions\ndef event_study_car(events, returns, window=(-5, 20)):\n    \"\"\"\n    Compute cumulative abnormal returns around events.\n\n    Parameters\n    ----------\n    events : DataFrame\n        Columns: ticker, event_date, event_type (addition/deletion).\n    returns : DataFrame\n        Columns: ticker, date, ret, mkt_ret.\n\n    Returns\n    -------\n    DataFrame : CARs by event type and event day.\n    \"\"\"\n    results = []\n\n    for _, event in events.iterrows():\n        ticker = event[\"ticker\"]\n        event_date = pd.to_datetime(event[\"event_date\"])\n\n        # Estimation window: -260 to -11\n        stock_rets = returns[returns[\"ticker\"] == ticker].copy()\n        stock_rets = stock_rets.sort_values(\"date\")\n\n        est_mask = (\n            (stock_rets[\"date\"] >= event_date - pd.Timedelta(days=365)) &\n            (stock_rets[\"date\"] < event_date - pd.Timedelta(days=15))\n        )\n        est_data = stock_rets[est_mask].dropna(subset=[\"ret\", \"mkt_ret\"])\n\n        if len(est_data) < 60:\n            continue\n\n        # Market model\n        model = sm.OLS(\n            est_data[\"ret\"],\n            sm.add_constant(est_data[\"mkt_ret\"])\n        ).fit()\n\n        # Event window\n        evt_mask = (\n            (stock_rets[\"date\"] >= event_date + pd.Timedelta(days=window[0] * 1.5)) &\n            (stock_rets[\"date\"] <= event_date + pd.Timedelta(days=window[1] * 1.5))\n        )\n        evt_data = stock_rets[evt_mask].dropna(subset=[\"ret\", \"mkt_ret\"])\n\n        if len(evt_data) < 5:\n            continue\n\n        # Abnormal returns\n        evt_data = evt_data.copy()\n        evt_data[\"expected\"] = model.predict(\n            sm.add_constant(evt_data[\"mkt_ret\"])\n        )\n        evt_data[\"ar\"] = evt_data[\"ret\"] - evt_data[\"expected\"]\n\n        # Assign event-time index\n        trading_dates = evt_data[\"date\"].sort_values().values\n        event_idx = np.searchsorted(trading_dates, event_date)\n        evt_data[\"event_day\"] = range(-event_idx, len(evt_data) - event_idx)\n\n        evt_data[\"event_type\"] = event[\"event_type\"]\n        evt_data[\"ticker\"] = ticker\n        results.append(evt_data[[\"ticker\", \"event_day\", \"ar\", \"event_type\"]])\n\n    if not results:\n        return pd.DataFrame()\n\n    return pd.concat(results, ignore_index=True)\n\n# Compute event study\ndaily_data = market_data.merge(\n    dc.get_market_returns(\n        start_date=\"2012-01-01\",\n        end_date=\"2024-12-31\",\n        frequency=\"daily\"\n    )[[\"date\", \"mkt_ret\"]],\n    on=\"date\", how=\"left\"\n)\n\ncar_results = event_study_car(index_changes, daily_data)\n```\n:::\n\n\n::: {#fig-index-event-study .cell execution_count=8}\n``` {.python .cell-code}\nif len(car_results) > 0:\n    # Average AR by event day and type\n    avg_ar = (\n        car_results.groupby([\"event_type\", \"event_day\"])\n        .agg(\n            mean_ar=(\"ar\", \"mean\"),\n            se_ar=(\"ar\", lambda x: x.std() / np.sqrt(len(x))),\n            n=(\"ar\", \"count\")\n        )\n        .reset_index()\n    )\n\n    # Cumulative AR\n    for etype in avg_ar[\"event_type\"].unique():\n        mask = avg_ar[\"event_type\"] == etype\n        avg_ar.loc[mask, \"car\"] = avg_ar.loc[mask, \"mean_ar\"].cumsum()\n\n    avg_ar = avg_ar[avg_ar[\"event_day\"].between(-5, 20)]\n\n    (\n        p9.ggplot(avg_ar, p9.aes(\n            x=\"event_day\", y=\"car\", color=\"event_type\"\n        ))\n        + p9.geom_line(size=1)\n        + p9.geom_vline(xintercept=0, linetype=\"dashed\", color=\"gray\")\n        + p9.geom_hline(yintercept=0, linetype=\"dotted\", color=\"gray\")\n        + p9.scale_color_manual(values=[\"#27AE60\", \"#C0392B\"])\n        + p9.labs(\n            x=\"Event Day\",\n            y=\"Cumulative Abnormal Return\",\n            title=\"Price Pressure from VN30 Index Additions and Deletions\",\n            color=\"Event\"\n        )\n        + p9.theme_minimal()\n        + p9.theme(figure_size=(10, 6))\n    )\n```\n:::\n\n\nThe permanent component of the CAR measures the information content of index inclusion, while the temporary component (reversal after the event) measures pure price pressure from demand. In a world with perfectly elastic demand, there would be no temporary price impact. The magnitude of the temporary component is inversely related to the demand elasticity, providing a second identification strategy (complementary to the holdings-based approach) for demand curves.\n\n### Demand Inelasticity and Its Implications\n\n@gabaix2021search argue that the aggregate demand for equities is remarkably inelastic, with elasticity estimates on the order of 0.2 (meaning a 1% supply increase requires a 5% price decline to clear). The implications are profound: if demand is this inelastic, then flows (from index funds, foreign investors, retail traders) have outsized effects on prices. In Vietnamese markets, where foreign ownership caps create binding constraints on a key investor class, demand inelasticity may be even more severe.\n\nThe inelasticity also generates a role for \"the market portfolio\" as an equilibrium concept: if most investors hold portfolios close to the market (as in CAPM), then deviations from market weights require someone to absorb the excess supply, and the compensation required is the inverse of demand elasticity.\n\n## Structural Models of Trading Strategies\n\n### Optimal Execution: The Almgren-Chriss Framework\n\nThe optimal execution problem, formalized by @almgren2001optimal, asks: given a large order to execute over a fixed horizon, what is the optimal trading schedule that minimizes the total execution cost? The problem is fundamental to institutional investing because large orders cannot be executed instantaneously without severe price impact.\n\nLet $X_t$ denote the remaining shares to sell at time $t$, with $X_0 = X$ (total order) and $X_T = 0$ (completion). The trading rate is $n_t = X_{t-1} - X_t$. The execution price for shares traded at $t$ is affected by both temporary and permanent price impact:\n\n$$\nS_t = S_0 + \\sigma W_t - g(n_t) - h\\left(\\frac{n_t}{\\tau}\\right)\n$$ {#eq-almgren-chriss-price}\n\nwhere $g(\\cdot)$ is the permanent impact function, $h(\\cdot)$ is the temporary impact function, $\\sigma$ is the volatility, $W_t$ is a Wiener process, and $\\tau$ is the time interval. The total implementation shortfall (cost of execution relative to the arrival price $S_0$) is:\n\n$$\n\\text{IS} = \\sum_{t=1}^{T} n_t (S_0 - S_t) = \\sum_{t=1}^{T} n_t \\left[\\sigma W_t + g(n_t) + h\\left(\\frac{n_t}{\\tau}\\right)\\right]\n$$ {#eq-shortfall}\n\nThe trader minimizes a mean-variance objective:\n\n$$\n\\min_{\\{n_t\\}} \\; E[\\text{IS}] + \\lambda \\cdot \\text{Var}(\\text{IS})\n$$ {#eq-ac-objective}\n\nwhere $\\lambda$ is the risk aversion parameter. With linear impact functions $g(n) = \\gamma n$ and $h(v) = \\eta v + \\epsilon \\text{sgn}(v)$, the optimal strategy has a closed-form solution.\n\n**TWAP (Time-Weighted Average Price):** Trade uniformly: $n_t = X / T$ for all $t$. Optimal when permanent impact dominates temporary impact.\n\n**VWAP (Volume-Weighted Average Price):** Trade proportionally to expected volume: $n_t \\propto \\hat{V}_t$. Approximates TWAP adjusted for intraday volume patterns.\n\n**Almgren-Chriss Optimal:** The risk-averse optimal trajectory for linear impact is:\n\n$$\nx_t^* = X \\cdot \\frac{\\sinh(\\kappa (T - t))}{\\sinh(\\kappa T)}\n$$ {#eq-ac-trajectory}\n\nwhere $\\kappa = \\sqrt{\\lambda \\sigma^2 / \\eta}$ governs the trade-off between urgency (trading quickly to reduce risk) and patience (trading slowly to reduce impact). High risk aversion ($\\lambda$ large) implies front-loaded execution.\n\n::: {#almgren-chriss .cell execution_count=9}\n``` {.python .cell-code}\ndef almgren_chriss_trajectory(X, T, sigma, eta, gamma, lam, n_steps=100):\n    \"\"\"\n    Compute Almgren-Chriss optimal execution trajectory.\n\n    Parameters\n    ----------\n    X : float\n        Total shares to execute.\n    T : float\n        Execution horizon (in trading days).\n    sigma : float\n        Daily return volatility.\n    eta : float\n        Temporary impact parameter.\n    gamma : float\n        Permanent impact parameter.\n    lam : float\n        Risk aversion parameter.\n    n_steps : int\n        Number of time steps.\n\n    Returns\n    -------\n    DataFrame : Time, remaining shares, trading rate, expected cost.\n    \"\"\"\n    tau = T / n_steps\n    kappa_sq = lam * sigma**2 / (eta / tau)\n\n    if kappa_sq <= 0:\n        # Risk-neutral: trade uniformly\n        kappa = 0\n        x_t = np.linspace(X, 0, n_steps + 1)\n    else:\n        kappa = np.sqrt(kappa_sq)\n        t_grid = np.linspace(0, T, n_steps + 1)\n        x_t = X * np.sinh(kappa * (T - t_grid)) / np.sinh(kappa * T)\n\n    # Trading rates\n    n_t = -np.diff(x_t)\n\n    # Expected cost components\n    perm_cost = gamma * np.sum(n_t**2)\n    temp_cost = (eta / tau) * np.sum(n_t**2)\n    total_expected_cost = perm_cost + temp_cost\n\n    # Variance of cost\n    var_cost = sigma**2 * tau * np.sum(x_t[:-1]**2)\n\n    results = pd.DataFrame({\n        \"time\": np.linspace(0, T, n_steps + 1)[:-1],\n        \"remaining_shares\": x_t[:-1],\n        \"trade_rate\": n_t\n    })\n\n    results.attrs[\"expected_cost\"] = total_expected_cost\n    results.attrs[\"cost_variance\"] = var_cost\n    results.attrs[\"kappa\"] = kappa if kappa_sq > 0 else 0\n\n    return results\n```\n:::\n\n\n::: {#fig-execution-trajectories .cell execution_count=10}\n``` {.python .cell-code}\n# Estimate market impact parameters from Vietnamese data\n# Typical values for mid-cap Vietnamese stocks\nsigma_daily = 0.025  # 2.5% daily vol\neta = 2.5e-7         # Temporary impact\ngamma = 1.0e-7       # Permanent impact\nX_shares = 500000    # 500k shares to sell\nT_days = 5           # 5-day horizon\n\ntrajectories = []\nfor lam, label in [(0, \"Risk Neutral (TWAP)\"),\n                    (1e-6, \"Low Risk Aversion\"),\n                    (1e-5, \"Medium Risk Aversion\"),\n                    (1e-4, \"High Risk Aversion\")]:\n    traj = almgren_chriss_trajectory(\n        X_shares, T_days, sigma_daily, eta, gamma, lam\n    )\n    traj[\"strategy\"] = label\n    traj[\"pct_remaining\"] = traj[\"remaining_shares\"] / X_shares * 100\n    trajectories.append(traj)\n\ntraj_all = pd.concat(trajectories, ignore_index=True)\n\n(\n    p9.ggplot(traj_all, p9.aes(\n        x=\"time\", y=\"pct_remaining\", color=\"strategy\"\n    ))\n    + p9.geom_line(size=1)\n    + p9.scale_color_manual(\n        values=[\"#95A5A6\", \"#27AE60\", \"#2E5090\", \"#C0392B\"]\n    )\n    + p9.labs(\n        x=\"Time (Trading Days)\",\n        y=\"Remaining Order (%)\",\n        title=\"Almgren-Chriss Optimal Execution Trajectories\",\n        color=\"Strategy\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 6), legend_position=\"top\")\n)\n```\n:::\n\n\n### Estimating Market Impact from Transaction Data\n\nThe structural parameters $(\\gamma, \\eta, \\sigma)$ must be estimated from data. The standard approach uses the square-root law of market impact, documented empirically by @kyle1985continuous and @hasbrouck1991measuring and derived theoretically by @gabaix2003theory:\n\n$$\n\\Delta P / P = c \\cdot \\text{sgn}(Q) \\cdot |Q / V|^{\\delta}\n$$ {#eq-impact-model}\n\nwhere $Q$ is the signed order flow, $V$ is daily volume, $c$ is the impact coefficient, and $\\delta \\approx 0.5$ is the impact exponent. The \"square root\" refers to $\\delta = 0.5$, which is remarkably robust across markets, time periods, and asset classes.\n\n::: {#impact-estimation .cell execution_count=11}\n``` {.python .cell-code}\n# Load intraday transaction data (or daily order flow proxy)\norder_flow = dc.get_order_flow(\n    start_date=\"2018-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\n# Construct signed order flow using Lee-Ready classification\n# (buy-initiated minus sell-initiated volume)\nimpact_data = order_flow.merge(\n    dc.get_daily_returns(\n        start_date=\"2018-01-01\",\n        end_date=\"2024-12-31\"\n    )[[\"ticker\", \"date\", \"ret\", \"volume\", \"market_cap\"]],\n    on=[\"ticker\", \"date\"],\n    how=\"inner\"\n)\n\n# Normalize order flow\nimpact_data[\"oib\"] = impact_data[\"net_buy_volume\"] / impact_data[\"volume\"]\nimpact_data[\"abs_oib\"] = np.abs(impact_data[\"oib\"])\nimpact_data[\"sign_oib\"] = np.sign(impact_data[\"oib\"])\n\n# Log-log regression: ln|ΔP| = α + δ ln|Q/V| + ε\nimpact_data[\"ln_abs_ret\"] = np.log(np.abs(impact_data[\"ret\"]).clip(lower=1e-8))\nimpact_data[\"ln_abs_oib\"] = np.log(impact_data[\"abs_oib\"].clip(lower=1e-8))\n\n# Estimate by size quintile\nimpact_data[\"size_quintile\"] = impact_data.groupby(\"date\")[\n    \"market_cap\"\n].transform(lambda x: pd.qcut(x, 5, labels=[1, 2, 3, 4, 5],\n                                duplicates=\"drop\"))\n\nimpact_results = []\nfor q in range(1, 6):\n    subset = impact_data[impact_data[\"size_quintile\"] == q].dropna(\n        subset=[\"ln_abs_ret\", \"ln_abs_oib\"]\n    )\n    if len(subset) < 500:\n        continue\n\n    model = sm.OLS(\n        subset[\"ln_abs_ret\"],\n        sm.add_constant(subset[\"ln_abs_oib\"])\n    ).fit(cov_type=\"HC1\")\n\n    impact_results.append({\n        \"size_quintile\": q,\n        \"delta_hat\": model.params.iloc[1],\n        \"delta_se\": model.bse.iloc[1],\n        \"intercept\": model.params.iloc[0],\n        \"r_squared\": model.rsquared,\n        \"n_obs\": int(model.nobs)\n    })\n\nimpact_df = pd.DataFrame(impact_results)\n```\n:::\n\n\n::: {#tbl-impact-by-size .cell tbl-cap='Market Impact Exponent by Firm Size Quintile' execution_count=12}\n``` {.python .cell-code}\nimpact_df.round(4)\n```\n:::\n\n\nThe impact exponent $\\hat{\\delta}$ near 0.5 across size quintiles confirms the universality of the square-root law. Smaller firms typically exhibit higher impact coefficients (larger $c$), consistent with lower liquidity.\n\n### Transaction Cost Decomposition\n\nTotal transaction costs comprise multiple components, each with distinct economic content:\n\n$$\n\\text{TC} = \\underbrace{\\frac{s}{2}}_{\\text{Half-spread}} + \\underbrace{\\gamma \\cdot Q}_{\\text{Permanent impact}} + \\underbrace{\\eta \\cdot \\frac{Q}{V}}_{\\text{Temporary impact}} + \\underbrace{\\sigma \\sqrt{T}}_{\\text{Timing risk}}\n$$ {#eq-tc-decomposition}\n\nWe estimate each component separately, following the @hasbrouck2009trading methodology for effective spread decomposition.\n\n::: {#tc-decomposition .cell execution_count=13}\n``` {.python .cell-code}\n# Effective spread estimation\nspreads = dc.get_bid_ask_data(\n    start_date=\"2020-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Quoted spread\nspreads[\"quoted_spread\"] = (\n    (spreads[\"ask\"] - spreads[\"bid\"]) / spreads[\"midpoint\"]\n)\n\n# Effective spread (from transaction prices)\nspreads[\"effective_spread\"] = (\n    2 * np.abs(spreads[\"trade_price\"] - spreads[\"midpoint\"]) /\n    spreads[\"midpoint\"]\n)\n\n# Roll (1984) implied spread from return autocovariance\ndef roll_spread(returns):\n    \"\"\"\n    Estimate the Roll (1984) bid-ask spread.\n    Spread = 2 * sqrt(-Cov(r_t, r_{t-1})) if covariance is negative.\n    \"\"\"\n    cov = np.cov(returns[1:], returns[:-1])[0, 1]\n    if cov < 0:\n        return 2 * np.sqrt(-cov)\n    else:\n        return np.nan\n\n# Compute by stock-month\ndaily_rets = dc.get_daily_returns(\n    start_date=\"2020-01-01\",\n    end_date=\"2024-12-31\"\n)\ndaily_rets[\"month\"] = daily_rets[\"date\"].dt.to_period(\"M\")\n\nroll_spreads = (\n    daily_rets.groupby([\"ticker\", \"month\"])\n    .agg(\n        roll_spread=(\"ret\", lambda x: roll_spread(x.values)\n                     if len(x) > 10 else np.nan),\n        n_days=(\"ret\", \"count\"),\n        avg_volume=(\"volume\", \"mean\"),\n        market_cap=(\"market_cap\", \"last\")\n    )\n    .reset_index()\n    .dropna(subset=[\"roll_spread\"])\n)\n```\n:::\n\n\n::: {#fig-spread-size .cell execution_count=14}\n``` {.python .cell-code}\n# Bin by market cap deciles\nroll_spreads[\"size_decile\"] = roll_spreads.groupby(\"month\")[\n    \"market_cap\"\n].transform(lambda x: pd.qcut(x, 10, labels=range(1, 11),\n                                duplicates=\"drop\"))\n\nspread_by_size = (\n    roll_spreads.groupby(\"size_decile\")\n    .agg(\n        median_spread=(\"roll_spread\", \"median\"),\n        mean_spread=(\"roll_spread\", \"mean\"),\n        q25=(\"roll_spread\", lambda x: x.quantile(0.25)),\n        q75=(\"roll_spread\", lambda x: x.quantile(0.75))\n    )\n    .reset_index()\n)\n\n(\n    p9.ggplot(spread_by_size, p9.aes(x=\"size_decile\", y=\"median_spread\"))\n    + p9.geom_bar(stat=\"identity\", fill=\"#2E5090\", alpha=0.7)\n    + p9.geom_errorbar(\n        p9.aes(ymin=\"q25\", ymax=\"q75\"),\n        width=0.3, color=\"#2E5090\"\n    )\n    + p9.labs(\n        x=\"Size Decile (1 = Smallest)\",\n        y=\"Roll Implied Spread\",\n        title=\"Transaction Costs Decrease Monotonically with Firm Size\"\n    )\n    + p9.scale_y_continuous(labels=percent_format())\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n```\n:::\n\n\n## Auction Models in Primary Markets\n\n### The IPO Allocation Problem\n\nInitial public offerings present a classic information asymmetry problem. Issuers and underwriters do not know the true market-clearing price; informed investors do (approximately). The allocation mechanism (i.e., how shares are distributed and at what price) determines the IPO's pricing efficiency, the degree of underpricing, and the distribution of surplus between issuers and investors.\n\n@rock1986new provides the canonical model. There are two types of investors: informed (who know the true value $v$) and uninformed (who know only the distribution $v \\sim F$). If the IPO is priced at $P$:\n\n-   When $v > P$ (good IPO): both informed and uninformed subscribe, so uninformed receive only a fraction of their order (rationing).\n-   When $v < P$ (bad IPO): only uninformed subscribe, so they receive full allocation (the \"winner's curse\").\n\nThe uninformed investor's expected return, accounting for rationing, is:\n\n$$\nE[R_{\\text{uninformed}}] = \\alpha_g \\cdot E\\left[\\frac{v - P}{P} \\mid v > P\\right] \\cdot \\Pr(v > P) + E\\left[\\frac{v - P}{P} \\mid v \\leq P\\right] \\cdot \\Pr(v \\leq P)\n$$ {#eq-rock-return}\n\nwhere $\\alpha_g < 1$ is the allocation probability in good IPOs (rationed) and allocation is 1 in bad IPOs. For uninformed investors to participate, $E[R_{\\text{uninformed}}] \\geq 0$, which requires:\n\n$$\nE\\left[\\frac{v - P}{P}\\right] > 0\n$$ {#eq-underpricing-condition}\n\nThat is, IPOs must be underpriced on average to compensate uninformed investors for the winner's curse. The degree of required underpricing increases with the proportion of informed investors and the variance of the true value.\n\n### Book Building vs. Auction Mechanisms\n\nVietnamese IPO history provides variation in allocation mechanisms. State-owned enterprise equitizations have used Dutch auctions, while private-sector IPOs have used book building. This institutional variation allows structural comparison of the mechanisms.\n\n**Book Building** [@benveniste1989investment]: The underwriter solicits indications of interest from institutional investors during the roadshow. Investors who reveal positive information (high valuations) receive favorable allocations as compensation. The mechanism aggregates information efficiently but grants discretion to the underwriter.\n\n**Uniform-Price Auction**: All winning bidders pay the same market-clearing price. This eliminates the underwriter's allocation discretion but may lead to free-riding on information revelation.\n\n::: {#ipo-data .cell execution_count=15}\n``` {.python .cell-code}\n# Load IPO data\nipo_data = dc.get_ipo_data(\n    start_date=\"2005-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Compute first-day returns (underpricing)\nipo_data[\"underpricing\"] = (\n    (ipo_data[\"first_day_close\"] - ipo_data[\"offer_price\"]) /\n    ipo_data[\"offer_price\"]\n)\n\n# Classify mechanism\nipo_data[\"mechanism\"] = ipo_data[\"ipo_method\"].map({\n    \"auction\": \"Auction\",\n    \"book_building\": \"Book Building\",\n    \"fixed_price\": \"Fixed Price\"\n})\n\nprint(f\"Total IPOs: {len(ipo_data)}\")\nprint(f\"By mechanism:\\n{ipo_data['mechanism'].value_counts()}\")\n```\n:::\n\n\n::: {#tbl-ipo-underpricing .cell tbl-cap='IPO Underpricing by Allocation Mechanism' execution_count=16}\n``` {.python .cell-code}\nipo_summary = (\n    ipo_data.groupby(\"mechanism\")\n    .agg(\n        n_ipos=(\"underpricing\", \"count\"),\n        mean_underpricing=(\"underpricing\", \"mean\"),\n        median_underpricing=(\"underpricing\", \"median\"),\n        std_underpricing=(\"underpricing\", \"std\"),\n        pct_positive=(\"underpricing\", lambda x: (x > 0).mean()),\n        mean_proceeds=(\"proceeds_bn_vnd\", \"mean\")\n    )\n    .round(4)\n)\nipo_summary\n```\n:::\n\n\n### Structural Estimation of Information Asymmetry\n\nWe estimate the @rock1986new model parameters (i.e., the fraction of informed investors ($\\mu$) and the precision of their information ($\\sigma_v^2$)) using the method of simulated moments (MSM).\n\nThe model predicts two key moments: \n\n1 . Average underpricing: $E[U] = f(\\mu, \\sigma_v^2)$ \n2. Cross-sectional variance of underpricing: $\\text{Var}(U) = g(\\mu, \\sigma_v^2)$\n\nWe match these model-implied moments to the data.\n\n::: {#rock-model-estimation .cell execution_count=17}\n``` {.python .cell-code}\ndef rock_model_moments(params, n_sim=10000):\n    \"\"\"\n    Simulate Rock (1986) IPO model and compute moments.\n\n    Parameters\n    ----------\n    params : tuple\n        (mu, sigma_v) - fraction of informed investors,\n        std dev of true value.\n\n    Returns\n    -------\n    tuple : (mean underpricing, variance of underpricing).\n    \"\"\"\n    mu, sigma_v = params\n    np.random.seed(42)\n\n    # True values\n    v = np.random.lognormal(mean=0, sigma=sigma_v, size=n_sim)\n\n    # Offer price: set to break even for uninformed\n    # Simplified: P = E[v] * discount_factor\n    P = np.exp(sigma_v**2 / 2) * 0.9  # 10% average discount\n\n    # First-day returns\n    returns = (v - P) / P\n\n    # Allocation probability for uninformed in good IPOs\n    # Depends on mu: more informed -> more rationing\n    good_mask = v > P\n    alpha_g = (1 - mu) / 1.0  # Simplified rationing\n\n    # Uninformed realized returns\n    uninformed_returns = np.where(\n        good_mask,\n        alpha_g * returns,\n        returns\n    )\n\n    mean_u = uninformed_returns.mean()\n    var_u = uninformed_returns.var()\n\n    return mean_u, var_u\n\n\ndef rock_model_objective(params, target_moments, weight_matrix=None):\n    \"\"\"\n    GMM objective for Rock model estimation.\n    \"\"\"\n    mu, sigma_v = params\n\n    if mu <= 0 or mu >= 1 or sigma_v <= 0 or sigma_v > 2:\n        return 1e10\n\n    model_moments = rock_model_moments(params)\n    diff = np.array(model_moments) - np.array(target_moments)\n\n    if weight_matrix is None:\n        weight_matrix = np.eye(len(diff))\n\n    return diff @ weight_matrix @ diff\n\n\n# Target moments from data\ntarget_mean = ipo_data[\"underpricing\"].mean()\ntarget_var = ipo_data[\"underpricing\"].var()\n\n# Estimate\nresult = minimize(\n    rock_model_objective,\n    x0=[0.3, 0.5],\n    args=([target_mean, target_var],),\n    method=\"Nelder-Mead\",\n    options={\"maxiter\": 5000}\n)\n\nmu_hat, sigma_v_hat = result.x\nprint(f\"Rock Model Estimates:\")\nprint(f\"  Fraction informed (μ): {mu_hat:.4f}\")\nprint(f\"  Value uncertainty (σ_v): {sigma_v_hat:.4f}\")\nprint(f\"  Objective value: {result.fun:.6f}\")\n```\n:::\n\n\n::: {#fig-ipo-underpricing-time .cell execution_count=18}\n``` {.python .cell-code}\nipo_plot = ipo_data.dropna(subset=[\"underpricing\", \"mechanism\"]).copy()\nipo_plot[\"year\"] = pd.to_datetime(ipo_plot[\"ipo_date\"]).dt.year\n\nannual_underpricing = (\n    ipo_plot.groupby([\"year\", \"mechanism\"])\n    .agg(\n        mean_up=(\"underpricing\", \"mean\"),\n        n=(\"underpricing\", \"count\")\n    )\n    .reset_index()\n    .query(\"n >= 3\")\n)\n\n(\n    p9.ggplot(annual_underpricing, p9.aes(\n        x=\"year\", y=\"mean_up\", color=\"mechanism\"\n    ))\n    + p9.geom_line(size=1)\n    + p9.geom_point(p9.aes(size=\"n\"))\n    + p9.geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\")\n    + p9.scale_color_manual(values=[\"#2E5090\", \"#C0392B\", \"#27AE60\"])\n    + p9.scale_y_continuous(labels=percent_format())\n    + p9.labs(\n        x=\"Year\",\n        y=\"Average First-Day Return\",\n        title=\"IPO Underpricing by Allocation Mechanism\",\n        color=\"Mechanism\", size=\"# IPOs\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 6))\n)\n```\n:::\n\n\n### Counterfactual: Welfare Under Alternative Mechanisms\n\nWith the structural parameters $(\\hat{\\mu}, \\hat{\\sigma}_v)$ in hand, we can simulate counterfactual outcomes. For instance: if all Vietnamese IPOs used uniform-price auctions instead of book building, how would underpricing and welfare change?\n\n::: {#counterfactual-ipo .cell execution_count=19}\n``` {.python .cell-code}\ndef simulate_ipo_mechanism(mu, sigma_v, mechanism=\"book_building\",\n                           n_sim=50000):\n    \"\"\"\n    Simulate IPO outcomes under different mechanisms.\n\n    Returns issuer surplus, informed profit, uninformed profit.\n    \"\"\"\n    np.random.seed(42)\n    v = np.random.lognormal(mean=0, sigma=sigma_v, size=n_sim)\n\n    if mechanism == \"book_building\":\n        # Price partially reveals information\n        # P = E[v] + 0.5 * (v - E[v]) * signal_quality\n        signal = v + np.random.normal(0, sigma_v * 0.5, n_sim)\n        P = np.exp(sigma_v**2 / 2) + 0.3 * (signal - np.exp(sigma_v**2 / 2))\n        P = P.clip(min=0.1)\n\n    elif mechanism == \"auction\":\n        # Competitive bidding: P closer to v\n        noise = np.random.normal(0, sigma_v * 0.3, n_sim)\n        P = v + noise\n        P = P.clip(min=0.1)\n\n    elif mechanism == \"fixed_price\":\n        # Fixed at E[v] * discount\n        P = np.full(n_sim, np.exp(sigma_v**2 / 2) * 0.85)\n\n    underpricing = (v - P) / P\n\n    issuer_surplus = P  # Revenue per share\n    investor_surplus = v - P  # Profit per share\n\n    return {\n        \"mechanism\": mechanism,\n        \"mean_underpricing\": underpricing.mean(),\n        \"median_underpricing\": np.median(underpricing),\n        \"std_underpricing\": underpricing.std(),\n        \"issuer_revenue\": issuer_surplus.mean(),\n        \"investor_profit\": investor_surplus.mean(),\n        \"money_left\": (investor_surplus[investor_surplus > 0]).sum() / n_sim\n    }\n\n# Compare mechanisms\ncounterfactual = pd.DataFrame([\n    simulate_ipo_mechanism(mu_hat, sigma_v_hat, \"book_building\"),\n    simulate_ipo_mechanism(mu_hat, sigma_v_hat, \"auction\"),\n    simulate_ipo_mechanism(mu_hat, sigma_v_hat, \"fixed_price\")\n]).set_index(\"mechanism\").round(4)\n\ncounterfactual\n```\n:::\n\n\n## Limit Order Book Models\n\n### Order Submission as a Strategic Choice\n\nIn a limit order market, every trader faces a fundamental tradeoff: submit a market order (immediate execution, but at an adverse price) or a limit order (better price, but risk of non-execution). @parlour1998price models this as a sequential game where each trader's optimal strategy depends on the current state of the order book.\n\nLet $a_t$ and $b_t$ denote the best ask and bid prices at time $t$, with the spread $s_t = a_t - b_t$. A buyer who arrives at time $t$ chooses between:\n\n-   **Market buy**: Execute immediately at $a_t$. Cost: $a_t - v_t$ where $v_t$ is the true value.\n-   **Limit buy at** $b_t$: Provides liquidity. If executed, profit: $v_t - b_t$. Probability of execution: $\\pi(b_t, \\text{book state})$.\n\nThe buyer submits a market order if:\n\n$$\na_t - v_t < (1 - \\pi_t)(v_t - b_t) + \\pi_t \\cdot 0\n$$ {#eq-order-choice}\n\nRearranging: market orders are optimal when the spread is narrow relative to the non-execution risk of limit orders. This generates the empirically observed pattern that limit orders are more attractive when spreads are wide and the book is thin.\n\n### The Glosten-Milgrom Model\n\n@glosten1985bid provide the foundational structural model of bid-ask spread determination under asymmetric information. A competitive market maker sets bid and ask prices to break even in expectation, recognizing that some trades come from informed traders.\n\nLet $\\mu$ denote the probability that an incoming order is from an informed trader, and let $V^H$ and $V^L$ denote the high and low values of the asset ($\\Pr(V = V^H) = p$). The zero-profit conditions yield:\n\n$$\na = E[V \\mid \\text{buy order}] = \\frac{p(1-\\mu) + p\\mu}{(1-\\mu) + p\\mu} V^H + \\frac{(1-p)(1-\\mu)}{(1-\\mu) + p\\mu} V^L\n$$ {#eq-gm-ask}\n\n$$\nb = E[V \\mid \\text{sell order}] = \\frac{p(1-\\mu)}{(1-\\mu) + (1-p)\\mu} V^H + \\frac{(1-p)(1-\\mu) + (1-p)\\mu}{(1-\\mu) + (1-p)\\mu} V^L\n$$ {#eq-gm-bid}\n\nThe spread $s = a - b$ is positive whenever $\\mu > 0$, and increases in both the proportion of informed traders ($\\mu$) and the information asymmetry ($V^H - V^L$). This decomposition is foundational: the spread compensates liquidity providers for the adverse selection cost of trading with informed counterparties.\n\n### PIN: Probability of Informed Trading\n\n\n@easley1996liquidity extend the Glosten-Milgrom framework to a dynamic setting and develop the Probability of Informed Trading (PIN) measure, which can be estimated from trade data. The model assumes that on each trading day, an information event occurs with probability $\\alpha$. Conditional on an event, it is bad news with probability $\\delta$. Informed traders arrive at rate $\\mu$; uninformed buyers and sellers arrive at rates $\\varepsilon_b$ and $\\varepsilon_s$, respectively.\n\nThe likelihood of observing $B_t$ buys and $S_t$ sells on day $t$ is:\n\n$$\n\\mathcal{L}(B_t, S_t | \\boldsymbol{\\theta}) = (1-\\alpha) f(B_t|\\varepsilon_b) f(S_t|\\varepsilon_s) + \\alpha\\delta \\cdot f(B_t|\\varepsilon_b) f(S_t|\\varepsilon_s + \\mu) + \\alpha(1-\\delta) \\cdot f(B_t|\\varepsilon_b + \\mu) f(S_t|\\varepsilon_s)\n$$ {#eq-pin-likelihood}\n\nwhere $f(\\cdot|\\lambda)$ is the Poisson density with rate $\\lambda$, and $\\boldsymbol{\\theta} = (\\alpha, \\delta, \\mu, \\varepsilon_b, \\varepsilon_s)$. PIN is then:\n\n$$\n\\text{PIN} = \\frac{\\alpha \\mu}{\\alpha \\mu + \\varepsilon_b + \\varepsilon_s}\n$$ {#eq-pin}\n\n::: {#pin-estimation .cell execution_count=20}\n``` {.python .cell-code}\ndef pin_log_likelihood(params, data):\n    \"\"\"\n    Compute negative log-likelihood for the Easley-Kiefer-O'Hara-Paperman\n    (1996) PIN model.\n\n    Parameters\n    ----------\n    params : array\n        [alpha, delta, mu, eps_b, eps_s]\n    data : DataFrame\n        Columns: buys, sells (daily counts).\n\n    Returns\n    -------\n    float : Negative log-likelihood.\n    \"\"\"\n    alpha, delta, mu, eps_b, eps_s = params\n\n    # Parameter bounds\n    if (alpha < 0 or alpha > 1 or delta < 0 or delta > 1 or\n        mu < 0 or eps_b < 0 or eps_s < 0):\n        return 1e15\n\n    B = data[\"buys\"].values\n    S = data[\"sells\"].values\n\n    # Use log-sum-exp for numerical stability\n    # Three components: no event, bad news, good news\n    log_L = np.zeros(len(B))\n\n    for i in range(len(B)):\n        b, s = B[i], S[i]\n\n        # Log-Poisson components\n        def log_poisson(k, lam):\n            if lam <= 0:\n                return -1e10 if k > 0 else 0\n            return k * np.log(lam) - lam - np.sum(np.log(np.arange(1, k + 1)))\n\n        # No event\n        c1 = (np.log(1 - alpha + 1e-15) +\n              log_poisson(b, eps_b) + log_poisson(s, eps_s))\n\n        # Bad news (extra sells)\n        c2 = (np.log(alpha * delta + 1e-15) +\n              log_poisson(b, eps_b) + log_poisson(s, eps_s + mu))\n\n        # Good news (extra buys)\n        c3 = (np.log(alpha * (1 - delta) + 1e-15) +\n              log_poisson(b, eps_b + mu) + log_poisson(s, eps_s))\n\n        # Log-sum-exp\n        max_c = max(c1, c2, c3)\n        log_L[i] = max_c + np.log(\n            np.exp(c1 - max_c) + np.exp(c2 - max_c) + np.exp(c3 - max_c)\n        )\n\n    return -np.sum(log_L)\n\n\ndef estimate_pin(buys, sells, n_starts=10):\n    \"\"\"\n    Estimate PIN via MLE with multiple random starts.\n\n    Returns\n    -------\n    dict : Estimated parameters and PIN value.\n    \"\"\"\n    data = pd.DataFrame({\"buys\": buys, \"sells\": sells})\n    best_result = None\n    best_nll = np.inf\n\n    avg_b = data[\"buys\"].mean()\n    avg_s = data[\"sells\"].mean()\n\n    for _ in range(n_starts):\n        # Random initial values\n        alpha0 = np.random.uniform(0.1, 0.8)\n        delta0 = np.random.uniform(0.2, 0.8)\n        mu0 = np.random.uniform(0, max(avg_b, avg_s))\n        eps_b0 = avg_b * np.random.uniform(0.5, 1.5)\n        eps_s0 = avg_s * np.random.uniform(0.5, 1.5)\n\n        try:\n            result = minimize(\n                pin_log_likelihood,\n                x0=[alpha0, delta0, mu0, eps_b0, eps_s0],\n                args=(data,),\n                method=\"L-BFGS-B\",\n                bounds=[(0.01, 0.99), (0.01, 0.99),\n                        (0.01, None), (0.01, None), (0.01, None)],\n                options={\"maxiter\": 1000}\n            )\n\n            if result.fun < best_nll:\n                best_nll = result.fun\n                best_result = result\n        except Exception:\n            continue\n\n    if best_result is None:\n        return {\"pin\": np.nan}\n\n    alpha, delta, mu, eps_b, eps_s = best_result.x\n    pin = alpha * mu / (alpha * mu + eps_b + eps_s)\n\n    return {\n        \"alpha\": alpha,\n        \"delta\": delta,\n        \"mu\": mu,\n        \"eps_b\": eps_b,\n        \"eps_s\": eps_s,\n        \"pin\": pin,\n        \"log_likelihood\": -best_nll\n    }\n```\n:::\n\n\n::: {#pin-cross-section .cell execution_count=21}\n``` {.python .cell-code}\n# Estimate PIN for a cross-section of stocks\n# Using daily buy/sell counts from Lee-Ready classification\ntrade_counts = dc.get_trade_counts(\n    start_date=\"2023-01-01\",\n    end_date=\"2023-12-31\"\n)\n\n# Sample of stocks for estimation (PIN is computationally intensive)\ntop_stocks = (\n    trade_counts.groupby(\"ticker\")\n    .agg(n_days=(\"date\", \"nunique\"))\n    .query(\"n_days >= 200\")\n    .index[:50]\n)\n\npin_estimates = []\nfor ticker in top_stocks:\n    stock_data = trade_counts[trade_counts[\"ticker\"] == ticker]\n    result = estimate_pin(\n        stock_data[\"buys\"].values,\n        stock_data[\"sells\"].values,\n        n_starts=5\n    )\n    result[\"ticker\"] = ticker\n    pin_estimates.append(result)\n\npin_df = pd.DataFrame(pin_estimates).dropna(subset=[\"pin\"])\nprint(f\"PIN estimates for {len(pin_df)} stocks\")\nprint(f\"  Mean PIN: {pin_df['pin'].mean():.4f}\")\nprint(f\"  Median PIN: {pin_df['pin'].median():.4f}\")\n```\n:::\n\n\n::: {#fig-pin-distribution .cell execution_count=22}\n``` {.python .cell-code}\n(\n    p9.ggplot(pin_df, p9.aes(x=\"pin\"))\n    + p9.geom_histogram(bins=25, fill=\"#2E5090\", alpha=0.7)\n    + p9.geom_vline(\n        xintercept=pin_df[\"pin\"].median(),\n        linetype=\"dashed\", color=\"#C0392B\", size=0.8\n    )\n    + p9.labs(\n        x=\"PIN\",\n        y=\"Count\",\n        title=\"Distribution of Informed Trading Probability\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n```\n:::\n\n\n### PIN and Asset Pricing\n\n@easley2002information argue that PIN should be priced in the cross-section of expected returns: stocks with higher information asymmetry require a risk premium to compensate uninformed investors. We test this prediction using Fama-MacBeth regressions.\n\n::: {#pin-asset-pricing .cell execution_count=23}\n``` {.python .cell-code}\n# Merge PIN with firm characteristics and forward returns\npin_chars = pin_df[[\"ticker\", \"pin\"]].merge(\n    dc.get_firm_characteristics(\n        start_date=\"2023-01-01\",\n        end_date=\"2023-12-31\"\n    ).groupby(\"ticker\").last().reset_index()[\n        [\"ticker\", \"log_size\", \"book_to_market\", \"momentum_12m\"]\n    ],\n    on=\"ticker\",\n    how=\"inner\"\n)\n\n# Forward 12-month returns (2024)\nforward_returns = dc.get_annual_returns(\n    start_date=\"2024-01-01\",\n    end_date=\"2024-12-31\"\n)\n\npin_returns = pin_chars.merge(\n    forward_returns[[\"ticker\", \"annual_ret\"]],\n    on=\"ticker\",\n    how=\"inner\"\n)\n\n# Cross-sectional regression\nif len(pin_returns) > 20:\n    cs_model = sm.OLS(\n        pin_returns[\"annual_ret\"],\n        sm.add_constant(\n            pin_returns[[\"pin\", \"log_size\", \"book_to_market\", \"momentum_12m\"]]\n        )\n    ).fit(cov_type=\"HC1\")\n\n    print(\"Cross-Sectional Return Regression with PIN:\")\n    for var in [\"pin\", \"log_size\", \"book_to_market\", \"momentum_12m\"]:\n        print(f\"  {var}: {cs_model.params[var]:.4f} \"\n              f\"(t = {cs_model.tvalues[var]:.3f})\")\n```\n:::\n\n\n### Estimation Challenges in Limit Order Book Models\n\nStructural estimation of limit order book models faces several challenges specific to Vietnamese markets:\n\n**Tick size effects.** Vietnamese exchanges use discrete tick sizes that vary with price level. At low prices, the minimum tick size represents a large fraction of the price, artificially widening the spread and distorting the structural parameters. The standard Glosten-Milgrom and PIN models assume continuous prices; adapting them to discrete price grids requires the modifications proposed by @bollen2004modeling.\n\n**Price limits.** When a stock hits its price limit, the order book is effectively frozen at one side. Orders accumulate but cannot execute until the limit is lifted. This creates a censoring problem analogous to the price limit censoring in return distributions: the observed order flow is a truncated version of the latent flow.\n\n**Missing data.** Full order book snapshots at high frequency are not universally available for Vietnamese stocks. PIN estimation requires only daily buy/sell counts, making it feasible with lower-frequency data. But richer models (@foucault1999order dynamic limit order book, @rocsu2009dynamic continuous-time model) require tick-by-tick order book data that may be unavailable for smaller stocks.\n\n**Institutional features.** The coexistence of HOSE (continuous auction) and HNX (with periodic call auctions for less liquid stocks) creates heterogeneity in the appropriate structural model. A model estimated on HOSE continuous trading data does not directly apply to HNX call auction stocks.\n\n## When Structural Models Are Worth It\n\n### Decision Framework\n\nStructural estimation is not always the right tool. The additional complexity, data requirements, and model risk must be justified by the research question. @tbl-when-structural provides a decision framework.\n\n| Criterion | Structural Preferred | Reduced Form Preferred |\n|------------------|---------------------------|----------------------------|\n| Research question | Counterfactual or policy evaluation | Causal effect of observed variation |\n| Data availability | Rich micro-data (transactions, holdings) | Standard panel (returns, fundamentals) |\n| Institutional environment | Stable (model assumptions plausible) | Rapidly changing (model may be misspecified) |\n| Number of parameters | Few primitives, many observables | Many parameters, few identifying assumptions |\n| Publication venue | JF, RFS, Econometrica, JPE | JFE, RFS, MS, JFQA |\n| Computational budget | Weeks to months of estimation | Hours to days |\n\n: When to Choose Structural vs. Reduced Form {#tbl-when-structural}\n\n### Data Requirements\n\nStructural models are data-hungry. @tbl-data-requirements summarizes the minimum data needs for each model class covered in this chapter.\n\n| Model | Minimum Data | Ideal Data | Vietnamese Availability |\n|----------------|----------------|----------------|-------------------------|\n| KY demand system | Quarterly institutional holdings + prices | 13F-equivalent filings + fund flows | Partial (semi-annual disclosure) |\n| Almgren-Chriss | Daily volume, spread, volatility | Intraday order-by-order data | Good (HOSE provides) |\n| Rock/IPO | Offer price, first-day close, allocation | Investor-level bids and allocations | Limited (auction data available) |\n| PIN | Daily buy/sell counts (Lee-Ready) | Tick-by-tick trade and quote | Moderate (requires trade classification) |\n| Glosten-Milgrom | Best bid/ask, trade direction | Full order book snapshots | Moderate (top-of-book available) |\n\n: Data Requirements for Structural Estimation {#tbl-data-requirements}\n\n### Computational Cost\n\nStructural estimation is orders of magnitude more expensive than reduced-form regression. The cost arises from three sources:\n\n**Likelihood evaluation.** Each evaluation of the structural likelihood or moment function requires solving the model for given parameters. For dynamic models (e.g., inventory models, dynamic discrete choice), this involves solving a dynamic programming problem at each iteration.\n\n**Optimization.** The GMM or MLE objective is typically non-convex, requiring multiple starting points and global optimization algorithms. The PIN model with 5 parameters requires $\\sim 10$ random starts; a richer model with $\\sim 20$ parameters might require hundreds.\n\n**Inference.** Standard errors for structural parameters often require bootstrapping (because the asymptotic distribution is non-standard or the delta method is unreliable), adding a multiplicative factor of 200--1000 to the computational budget.\n\n::: {#computational-cost-comparison .cell execution_count=24}\n``` {.python .cell-code}\n# Illustration: computational cost comparison\nimport time\n\n# Reduced form: OLS regression\nn_obs = 100000\nX_sim = np.random.randn(n_obs, 5)\ny_sim = X_sim @ np.random.randn(5) + np.random.randn(n_obs)\n\nstart = time.time()\nfor _ in range(100):\n    sm.OLS(y_sim, sm.add_constant(X_sim)).fit()\nols_time = (time.time() - start) / 100\n\n# Structural: PIN estimation (single stock)\nsim_buys = np.random.poisson(50, 250)\nsim_sells = np.random.poisson(45, 250)\n\nstart = time.time()\npin_result = estimate_pin(sim_buys, sim_sells, n_starts=5)\npin_time = time.time() - start\n\nprint(f\"Computational Cost Comparison:\")\nprint(f\"  OLS (100k obs): {ols_time*1000:.1f} ms per estimation\")\nprint(f\"  PIN (250 days, 5 starts): {pin_time:.1f} s per stock\")\nprint(f\"  Ratio: {pin_time / ols_time:.0f}x\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComputational Cost Comparison:\n  OLS (100k obs): 41.6 ms per estimation\n  PIN (250 days, 5 starts): 17.0 s per stock\n  Ratio: 408x\n```\n:::\n:::\n\n\n### Interpretation Risks\n\nThe primary risk of structural estimation is model misspecification. If the model is wrong, the estimated parameters have no economic meaning and the counterfactual predictions are unreliable. Several strategies mitigate this risk:\n\n**Specification tests.** Over-identifying restrictions (more moments than parameters) provide the Hansen $J$-test for model fit. A rejection suggests misspecification, though the test has low power in small samples.\n\n**Model comparison.** Estimate multiple nested or non-nested models and compare fit. If a simpler model fits the data equally well, prefer it (Occam's razor).\n\n**Sensitivity analysis.** Report how structural parameters change under plausible alternative assumptions (e.g., different functional forms for the impact function, different distributional assumptions for valuations).\n\n**Out-of-sample validation.** Estimate the model on one sample period and test its predictions on a holdout sample. Structural models that fit in-sample but fail out-of-sample are likely overfit.\n\n### Journal Expectations\n\nStructural papers in top finance journals are held to specific standards:\n\n**Identification clarity.** The paper must clearly state which parameters are identified, from which moments, and what variation in the data provides identification. The @rust1987optimal critique that without clear identification, structural estimation is curve-fitting, must be addressed head-on.\n\n**Counterfactual credibility.** Counterfactual exercises should be economically motivated and the range of counterfactual scenarios should not extrapolate far beyond the data.\n\n**Robustness to specification.** Reviewers will ask what happens under alternative distributional assumptions, alternative moment conditions, and alternative model features.\n\n**Transparency.** Code and data should be made available. Structural estimation is sufficiently complex that replicability is a first-order concern. \n\n<!-- ## Exercises\n\n1.  **Demand Elasticity Heterogeneity.** Extend the KY demand system by allowing demand elasticities to vary by investor type (mutual funds, insurance companies, pension funds, foreign institutional investors). Estimate separate $\\boldsymbol{\\beta}_i$ vectors for each type. Which investor type has the most elastic demand? Which has the most inelastic? Relate your findings to the price impact predictions of @gabaix2021search.\n\n2.  **Optimal Execution with Price Limits.** Modify the Almgren-Chriss framework to incorporate Vietnamese daily price limits. The constraint is that the execution price on any day cannot exceed the limit: $|S_t - S_{t-1}| \\leq L$. Solve the modified optimization numerically and compare the optimal trajectory with the unconstrained solution. How do price limits affect total execution cost and the optimal time horizon?\n\n3.  **Mechanism Design for SOE Equitization.** Using the estimated Rock model parameters, simulate IPO outcomes under a Vickrey (second-price) auction mechanism. Compare welfare (issuer revenue, investor surplus, total surplus) with the observed auction and book building outcomes. Under what conditions does the Vickrey mechanism dominate?\n\n4.  **PIN and Corporate Events.** Estimate PIN in event windows around earnings announcements, M&A announcements, and regulatory changes for a sample of Vietnamese firms. Does PIN spike before public announcements (consistent with informed trading or leakage)? Compare the magnitude of the pre-announcement PIN increase across event types.\n\n5.  **Kyle's Lambda and Market Depth.** Estimate Kyle's $\\lambda$ (the price impact coefficient from the regression of price changes on signed order flow) for the cross-section of Vietnamese stocks. Test whether $\\lambda$ is inversely related to measures of market depth (average volume, number of active traders, institutional ownership). Construct a tradeable portfolio long low-$\\lambda$ (liquid) stocks and short high-$\\lambda$ (illiquid) stocks. Does this liquidity factor earn a risk premium?\n\n6.  **Structural Break in Market Microstructure.** Vietnam implemented tick size changes and trading hour extensions at various points. Using the PIN model and the Roll spread, estimate structural breaks in information asymmetry and transaction costs around these regulatory changes. Do the structural parameters change in the direction predicted by market microstructure theory? Use the @bai1998estimating sequential breakpoint methodology to formally test for and date the breaks. -->\n\n## Summary\n\nThis chapter introduced structural estimation as a distinct methodology for financial economics, one that trades the simplicity and transparency of reduced-form analysis for the ability to estimate economic primitives and conduct counterfactual policy evaluation. We implemented four classes of structural models: demand systems for financial assets, optimal execution and transaction cost models, IPO auction models, and limit order book models of informed trading.\n\nThe demand system approach of @koijen2019demand reveals that institutional investors in Vietnam exhibit heterogeneous demand elasticities across characteristics, with foreign investors showing different sensitivity patterns than domestic institutions. Market impact estimation confirms the universality of the square-root law, while the Almgren-Chriss framework provides the optimal execution benchmark for large institutional orders. The Rock IPO model quantifies the information asymmetry premium embedded in Vietnamese IPO underpricing, and the PIN model estimates the probability of informed trading across the cross-section.\n\nEach model involves a tradeoff between the economic questions it can answer and the assumptions it requires. The decision to use structural estimation should be driven by the research question (specifically, whether counterfactual analysis is essential) and tempered by honest assessment of whether the model's assumptions are sufficiently credible in the Vietnamese institutional environment. When they are, structural models provide insights that no amount of reduced-form regression can deliver.\n\n",
    "supporting": [
      "63_structural_models_finance_files/figure-pdf"
    ],
    "filters": []
  }
}