{
  "hash": "7bfaea162e9baae0b830c9ad6dd000ea",
  "result": {
    "engine": "jupyter",
    "markdown": "# Compound Returns\n\nIn this chapter, we provide a treatment of compound returns. Whether constructing buy-and-hold portfolios, evaluating fund performance, computing cumulative wealth indices, or estimating long-horizon risk measures, the ability to correctly compound returns over arbitrary horizons is indispensable. We begin with the mathematical foundations: the distinction between simple and log returns, the relationship between arithmetic and geometric means, and the properties of continuously compounded returns. Along the way, we address practical complications that arise in real-world equity data, such as trading halts, price limit mechanisms, partial-period returns, and delisting events, and show how to handle them in the Vietnamese context.\n\nThe chapter proceeds to rolling compound returns over standard horizons (3, 6, 9, and 12 months), compound returns aligned to fiscal period ends, forward-looking cumulative returns for event studies, and rolling volatility estimation.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport matplotlib.pyplot as plt\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format, date_format\nfrom itertools import product\nfrom datetime import datetime, timedelta\n```\n:::\n\n\n## Simple Returns versus Log Returns\n\nBefore discussing compounding, we must distinguish between the two fundamental return conventions used in finance.\n\n### Simple (Arithmetic) Returns\n\nThe simple gross return on an asset from period $t-1$ to $t$ is defined as\n\n$$\n1 + R_t = \\frac{P_t + D_t}{P_{t-1}},\n$$ {#eq-simple-return}\n\nwhere $P_t$ denotes the price at the end of period $t$ and $D_t$ denotes any cash distributions (dividends, coupons) paid during period $t$. The simple net return is $R_t$ itself. When we speak of \"returns\" without qualification, we typically mean simple net returns.\n\nThe key property of simple returns is that **multi-period compounding is multiplicative**:\n\n$$\n1 + R_t(k) = \\prod_{j=0}^{k-1} (1 + R_{t-j}) = (1 + R_t)(1 + R_{t-1}) \\cdots (1 + R_{t-k+1}),\n$$ {#eq-compound-simple}\n\nwhere $R_t(k)$ is the $k$-period compound return ending at time $t$. This multiplicative structure is the foundation of all compounding methods discussed in this chapter.\n\n### Continuously Compounded (Log) Returns\n\nThe continuously compounded return, or log return, is defined as\n\n$$\nr_t = \\ln(1 + R_t) = \\ln\\!\\left(\\frac{P_t + D_t}{P_{t-1}}\\right).\n$$ {#eq-log-return}\n\nThe central advantage of log returns for compounding is that **multi-period compounding becomes additive**:\n\n$$\nr_t(k) = \\ln(1 + R_t(k)) = \\sum_{j=0}^{k-1} r_{t-j} = r_t + r_{t-1} + \\cdots + r_{t-k+1}.\n$$ {#eq-compound-log}\n\nThis additive property follows directly from the logarithmic identity $\\ln(ab) = \\ln(a) + \\ln(b)$. It is computationally convenient because summation is numerically more stable than iterated multiplication, and because many statistical procedures (means, variances, regressions) operate naturally on additive quantities.\n\nTo recover the simple compound return from the sum of log returns, we apply the exponential function:\n\n$$\nR_t(k) = \\exp\\!\\left(\\sum_{j=0}^{k-1} r_{t-j}\\right) - 1.\n$$ {#eq-recover-simple}\n\n### When Do They Diverge?\n\nFor small returns, the approximation $r_t \\approx R_t$ holds to first order (via the Taylor expansion $\\ln(1+x) \\approx x$ for $|x| \\ll 1$). However, for large returns, which is common in emerging markets, small-cap stocks, or crisis periods, the two can diverge substantially. Consider a stock that doubles in price ($R_t = 1.0$): the log return is $r_t = \\ln(2) \\approx 0.693$, a 31% discrepancy. Conversely, for a stock that loses half its value ($R_t = -0.5$): the log return is $r_t = \\ln(0.5) \\approx -0.693$, which is 39% larger in magnitude.\n\nThis divergence is especially relevant in Vietnam, where daily price limits of $\\pm 7\\%$ on HOSE, $\\pm 10\\%$ on HNX, and $\\pm 15\\%$ on UPCoM can produce sequences of limit-up or limit-down days. Over a week of consecutive limit-up days on HOSE, the simple return is $(1.07)^5 - 1 = 40.3\\%$ while the log return is $5 \\times \\ln(1.07) = 33.8\\%$, which is a meaningful gap.\n\n@tbl-return-comparison illustrates this divergence across a range of return magnitudes.\n\n::: {#tbl-return-comparison .cell tbl-cap='Comparison of simple and log returns for various price changes. The divergence grows with the magnitude of the simple return, which is particularly relevant for volatile emerging market stocks.' execution_count=3}\n``` {.python .cell-code}\nsimple_returns = [-0.50, -0.30, -0.15, -0.10, -0.07, -0.05, -0.01,\n                  0.00, 0.01, 0.05, 0.07, 0.10, 0.15, 0.30, 0.50, 1.00]\ncomparison_df = pd.DataFrame({\n    \"Simple Return\": [f\"{r:.2%}\" for r in simple_returns],\n    \"Log Return\": [f\"{np.log(1+r):.4f}\" for r in simple_returns],\n    \"Difference\": [f\"{np.log(1+r) - r:.4f}\" for r in simple_returns],\n    \"Relative Error (%)\": [\n        f\"{((np.log(1+r) - r) / abs(r) * 100):.2f}\" if r != 0 else \"—\"\n        for r in simple_returns\n    ]\n})\ncomparison_df\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Simple Return</th>\n      <th>Log Return</th>\n      <th>Difference</th>\n      <th>Relative Error (%)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-50.00%</td>\n      <td>-0.6931</td>\n      <td>-0.1931</td>\n      <td>-38.63</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-30.00%</td>\n      <td>-0.3567</td>\n      <td>-0.0567</td>\n      <td>-18.89</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-15.00%</td>\n      <td>-0.1625</td>\n      <td>-0.0125</td>\n      <td>-8.35</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-10.00%</td>\n      <td>-0.1054</td>\n      <td>-0.0054</td>\n      <td>-5.36</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-7.00%</td>\n      <td>-0.0726</td>\n      <td>-0.0026</td>\n      <td>-3.67</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-5.00%</td>\n      <td>-0.0513</td>\n      <td>-0.0013</td>\n      <td>-2.59</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-1.00%</td>\n      <td>-0.0101</td>\n      <td>-0.0001</td>\n      <td>-0.50</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.00%</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>—</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.00%</td>\n      <td>0.0100</td>\n      <td>-0.0000</td>\n      <td>-0.50</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5.00%</td>\n      <td>0.0488</td>\n      <td>-0.0012</td>\n      <td>-2.42</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>7.00%</td>\n      <td>0.0677</td>\n      <td>-0.0023</td>\n      <td>-3.34</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10.00%</td>\n      <td>0.0953</td>\n      <td>-0.0047</td>\n      <td>-4.69</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>15.00%</td>\n      <td>0.1398</td>\n      <td>-0.0102</td>\n      <td>-6.83</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>30.00%</td>\n      <td>0.2624</td>\n      <td>-0.0376</td>\n      <td>-12.55</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>50.00%</td>\n      <td>0.4055</td>\n      <td>-0.0945</td>\n      <td>-18.91</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>100.00%</td>\n      <td>0.6931</td>\n      <td>-0.3069</td>\n      <td>-30.69</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n> **Key takeaway**: log returns are convenient for compounding (additive aggregation), but portfolio returns aggregate cross-sectionally in simple return space. In practice, we often transform to log returns for temporal compounding, then convert back to simple returns for reporting.\n\n## Mathematical Foundations of Compounding\n\n### Geometric Mean Return\n\nThe geometric mean return over $T$ periods is\n\n$$\n\\bar{R}_g = \\left(\\prod_{t=1}^{T} (1 + R_t)\\right)^{1/T} - 1,\n$$ {#eq-geometric-mean}\n\nwhich represents the constant per-period return that would yield the same terminal wealth as the actual return sequence. It is always less than or equal to the arithmetic mean $\\bar{R}_a = \\frac{1}{T}\\sum_{t=1}^{T} R_t$, with equality only when all returns are identical. The relationship between the two is approximately:\n\n$$\n\\bar{R}_g \\approx \\bar{R}_a - \\frac{\\sigma^2}{2},\n$$ {#eq-arithmetic-geometric}\n\nwhere $\\sigma^2$ is the variance of returns. This approximation, sometimes called the \"volatility drag,\" has important implications: high-volatility assets have a larger wedge between their arithmetic and geometric means, meaning their actual compound growth understates what a naive average would suggest. In a market like Vietnam's, where individual stock volatility is often two to three times that of developed-market equities, the volatility drag can be substantial.\n\n### Wealth Index and Drawdowns\n\nGiven an initial investment of $W_0$, the wealth at time $T$ is\n\n$$\nW_T = W_0 \\prod_{t=1}^{T} (1 + R_t).\n$$ {#eq-wealth-index}\n\nThe cumulative return (net) is simply $W_T / W_0 - 1$. The maximum drawdown, a widely used risk measure, is defined as\n\n$$\n\\text{MDD} = \\max_{0 \\le s \\le t \\le T} \\left(\\frac{W_s - W_t}{W_s}\\right),\n$$ {#eq-max-drawdown}\n\nwhich measures the largest peak-to-trough decline in the wealth index. We will compute this quantity alongside compound returns below. Drawdowns are particularly informative in emerging markets that experience sharp corrections, as occurred during the global financial crisis of 2008 when the VN-Index fell roughly 66% from its 2007 peak.\n\n<!--# Show the data here -->\n\n### Annualization\n\nFor a $k$-period compound return $R_t(k)$ where each period has length $\\Delta$ (e.g., $\\Delta = 1/12$ for monthly data), the annualized return is\n\n$$\nR_{\\text{ann}} = (1 + R_t(k))^{1/(k\\Delta)} - 1.\n$$ {#eq-annualize}\n\nSimilarly, for volatility estimated from $k$-period returns with period length $\\Delta$:\n\n$$\n\\sigma_{\\text{ann}} = \\sigma / \\sqrt{\\Delta},\n$$ {#eq-annualize-vol}\n\nso monthly volatility is annualized by multiplying by $\\sqrt{12}$ and daily volatility by approximately $\\sqrt{252}$ (assuming 252 trading days per year). For Vietnam specifically, the HOSE typically has around 245–250 trading days per year after accounting for Vietnamese public holidays, which is close enough that the $\\sqrt{252}$ convention is standard.\n\n## Data Preparation\n\nWe start by loading monthly stock return data from our SQLite database. As prepared in previous chapters, this database contains monthly returns sourced from [DataCore.vn](https://datacore.vn/) for all securities listed on the Ho Chi Minh Stock Exchange (HOSE), Hanoi Stock Exchange (HNX), and the Unlisted Public Company Market (UPCoM). Returns are adjusted for stock splits, bonus issues, and rights offerings, and include reinvested cash dividends.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, ret, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n).dropna()\n\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nprices_monthly = prices_monthly.merge(\n    factors_ff3_monthly,\n    on=\"date\",\n    how=\"left\"\n)\n\nprices_monthly[\"ret_total\"] = prices_monthly[\"ret\"]\nprices_monthly[\"mkt_total\"] = (\n    prices_monthly[\"mkt_excess\"] + prices_monthly[\"risk_free\"]\n)\n```\n:::\n\n\nLet us inspect the sample:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(f\"Sample period: {prices_monthly['date'].min()} to \"\n      f\"{prices_monthly['date'].max()}\")\nprint(f\"Number of stocks: {prices_monthly['symbol'].nunique():,}\")\nprint(f\"Total observations: {len(prices_monthly):,}\")\n# print(f\"Exchanges: {prices_monthly['exchange'].unique()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample period: 2010-02-28 00:00:00 to 2023-12-31 00:00:00\nNumber of stocks: 1,457\nTotal observations: 165,499\n```\n:::\n:::\n\n\n@tbl-sample-overview provides summary statistics for the raw monthly returns, broken down by exchange. Differences across exchanges reflect the size and liquidity gradient: HOSE lists the largest and most liquid firms, HNX covers mid-cap companies, and UPCoM hosts smaller and more thinly traded securities.\n\n::: {#tbl-sample-overview .cell tbl-cap='Summary statistics of monthly stock returns by exchange. HOSE firms tend to have lower return dispersion and fewer extreme observations compared to HNX and UPCoM, consistent with their larger market capitalization and greater liquidity.' execution_count=6}\n``` {.python .cell-code}\nsample_stats = (\n    prices_monthly\n    .groupby(\"exchange\")[\"ret_total\"]\n    .describe(percentiles=[0.05, 0.25, 0.50, 0.75, 0.95])\n    .round(4)\n)\nsample_stats\n```\n:::\n\n\n## Method 1: Cumulative Product via GroupBy\n\nThe most direct approach to compound returns uses the multiplicative property in @eq-compound-simple. For each security, we compute the cumulative product of gross returns $(1 + R_t)$ over the desired window.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef compute_cumret_cumprod(df, ret_col=\"ret_total\",\n                           group_col=\"symbol\"):\n    \"\"\"Compute cumulative returns using cumulative product.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain `group_col`, 'date', and `ret_col`.\n    ret_col : str\n        Column name for period returns.\n    group_col : str\n        Column name for grouping (e.g., security identifier).\n\n    Returns\n    -------\n    pd.DataFrame\n        Original DataFrame augmented with 'cumret' and 'wealth_index'.\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[ret_col]\n    df[\"wealth_index\"] = (\n        df.groupby(group_col)[\"gross_ret\"]\n        .cumprod()\n    )\n    df[\"cumret\"] = df[\"wealth_index\"] - 1\n    df.drop(columns=[\"gross_ret\"], inplace=True)\n    return df\n```\n:::\n\n\nLet us apply this to the full sample and examine the resulting wealth indices for a few selected stocks:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nstock_cumret = compute_cumret_cumprod(prices_monthly)\n\n# Select stocks with long histories for illustration\nstock_counts = (\n    stock_cumret.groupby(\"symbol\")[\"date\"]\n    .count()\n    .reset_index(name=\"n_obs\")\n)\nlong_history_stocks = (\n    stock_counts.nlargest(5, \"n_obs\")[\"symbol\"].tolist()\n)\n\nsample_wealth = stock_cumret[\n    stock_cumret[\"symbol\"].isin(long_history_stocks)\n]\n```\n:::\n\n\n@fig-wealth-index plots the wealth indices (value of 1 VND invested) for these five securities over the full sample period.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nplot_wealth = (\n    ggplot(sample_wealth, aes(x=\"date\", y=\"wealth_index\",\n                              color=\"factor(symbol)\")) +\n    geom_line(size=0.6) +\n    labs(\n        x=\"\", y=\"Wealth index (1 VND invested)\",\n        color=\"Stock\"\n    ) +\n    theme_minimal() +\n    theme(legend_position=\"bottom\",\n          figure_size=(10, 5))\n)\nplot_wealth.draw()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![Wealth index (value of 1 VND invested) for selected long-history Vietnamese stocks. Each line represents the cumulative value of a 1 VND investment in a single stock, with all dividends reinvested. The divergence in terminal wealth illustrates the power of compounding over long horizons.](16_compound_return_files/figure-pdf/fig-wealth-index-output-1.pdf){#fig-wealth-index fig-alt='Line chart showing the growth of 1 VND invested in five different Vietnamese stocks over time.' fig-pos='htbp'}\n:::\n:::\n\n\n### Handling Missing Returns\n\nThe cumulative product approach propagates missing values: if any $R_t$ is `NaN`, the entire cumulative product from that point onward becomes `NaN`. This is conservative because it effectively assumes that a missing return renders the subsequent wealth index undefined. In many applications, this is the desired behavior because a missing return may indicate a data error or a period during which the stock was not trading.\n\nHowever, in the Vietnamese market, missing returns can arise from extended trading halts. The State Securities Commission (SSC) and exchanges may suspend trading in a stock for various regulatory reasons, such as financial reporting delays, pending corporate restructuring announcements, or suspected market manipulation. These halts can last days, weeks, or even months. During such halts, the stock's value has not changed (the last traded price remains the reference), so treating the missing return as zero (i.e., no price change) may be more appropriate than propagating `NaN`.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ndef compute_cumret_skipna(df, ret_col=\"ret_total\",\n                          group_col=\"symbol\"):\n    \"\"\"Compute cumulative returns, treating missing returns as zero.\"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[ret_col].fillna(0)\n    df[\"wealth_index\"] = (\n        df.groupby(group_col)[\"gross_ret\"]\n        .cumprod()\n    )\n    df[\"cumret\"] = df[\"wealth_index\"] - 1\n    df.drop(columns=[\"gross_ret\"], inplace=True)\n    return df\n```\n:::\n\n\n::: callout-warning\nTreating missing returns as zero is an assumption that may or may not be appropriate. If returns are missing because the stock was halted, zero may be reasonable. If returns are missing due to data errors or because the stock was genuinely not trading (e.g., awaiting relisting after a corporate event), imputing zero can introduce bias. Always investigate the reason for missing values before deciding on a treatment.\n:::\n\n<!--# The DataCore.vn documentation provides flags for different types of trading suspensions that can guide this decision. -->\n\n## Method 2: Log-Sum-Exp Approach\n\nThe log-sum-exp method exploits the additive property of log returns (@eq-compound-log). This approach is particularly useful when computing compound returns over fixed windows (e.g., annual returns from monthly data) because summation is both computationally efficient and numerically stable.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ndef compute_cumret_logsum(df, ret_col=\"ret_total\",\n                          group_col=\"symbol\",\n                          date_col=\"date\"):\n    \"\"\"Compute cumulative returns using the log-sum-exp approach.\n\n    Steps:\n    1. Transform to log returns: r_t = ln(1 + R_t)\n    2. Cumulative sum of log returns within each group\n    3. Exponentiate to recover simple cumulative return\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    ret_col : str\n    group_col : str\n    date_col : str\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    df = df.sort_values([group_col, date_col]).copy()\n    df[\"log_ret\"] = np.log(1 + df[ret_col])\n    df[\"cum_log_ret\"] = (\n        df.groupby(group_col)[\"log_ret\"].cumsum()\n    )\n    df[\"wealth_index_log\"] = np.exp(df[\"cum_log_ret\"])\n    df[\"cumret_log\"] = df[\"wealth_index_log\"] - 1\n    df.drop(columns=[\"log_ret\", \"cum_log_ret\"], inplace=True)\n    return df\n```\n:::\n\n\nLet us verify that the two methods produce identical results (up to floating-point precision):\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nstock_both = compute_cumret_cumprod(prices_monthly)\nstock_both = compute_cumret_logsum(stock_both)\n\n# Compare on non-missing observations\nmask = (stock_both[\"cumret\"].notna()\n        & stock_both[\"cumret_log\"].notna())\nmax_diff = (stock_both.loc[mask, \"cumret\"] -\n            stock_both.loc[mask, \"cumret_log\"]).abs().max()\nprint(f\"Maximum absolute difference between methods: {max_diff:.2e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMaximum absolute difference between methods: 1.78e-14\n```\n:::\n:::\n\n\nThe difference is at the level of machine epsilon ($\\approx 10^{-15}$), confirming numerical equivalence.\n\n### Period-Specific Compound Returns\n\nA common task is to compute compound returns within calendar periods (months, quarters, years). The log-sum-exp approach lends itself naturally to grouped aggregation:\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef compound_return_by_period(df, ret_col=\"ret_total\",\n                              group_col=\"symbol\",\n                              period=\"year\"):\n    \"\"\"Compute compound returns within calendar periods.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain 'date' and `ret_col`.\n    period : str\n        One of 'year', 'quarter', 'month'.\n\n    Returns\n    -------\n    pd.DataFrame with compound returns per group-period.\n    \"\"\"\n    df = df.copy()\n    df[\"log_ret\"] = np.log(1 + df[ret_col])\n    if period == \"year\":\n        df[\"period\"] = df[\"date\"].dt.year\n    elif period == \"quarter\":\n        df[\"period\"] = df[\"date\"].dt.to_period(\"Q\")\n    elif period == \"month\":\n        df[\"period\"] = df[\"date\"].dt.to_period(\"M\")\n\n    result = (\n        df.groupby([group_col, \"period\"])\n        .agg(\n            cumret=(\n                \"log_ret\",\n                lambda x: np.exp(x.sum()) - 1\n            ),\n            n_obs=(\"log_ret\", \"count\"),\n            n_miss=(ret_col, lambda x: x.isna().sum()),\n            start_date=(\"date\", \"min\"),\n            end_date=(\"date\", \"max\")\n        )\n        .reset_index()\n    )\n    return result\n```\n:::\n\n\n@tbl-annual-returns shows annual compound returns for a subset of securities.\n\n::: {#tbl-annual-returns .cell tbl-cap='Annual compound returns for selected Vietnamese securities. The number of non-missing monthly observations (n_obs) and missing observations (n_miss) are reported to flag potentially incomplete years. A stock-year with n_obs substantially below 12 indicates either partial listing or extended trading halts.' execution_count=14}\n``` {.python .cell-code}\nannual_returns = compound_return_by_period(\n    prices_monthly[\n        prices_monthly[\"symbol\"].isin(long_history_stocks)\n    ],\n    period=\"year\"\n)\n\nrecent_annual = (\n    annual_returns\n    .sort_values([\"symbol\", \"period\"])\n    .groupby(\"symbol\")\n    .tail(5)\n    .round(4)\n)\nrecent_annual.head(20)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_2230066/2619242959.py:13: UserWarning: obj.round has no effect with datetime, timedelta, or period dtypes. Use obj.dt.round(...) instead.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>period</th>\n      <th>cumret</th>\n      <th>n_obs</th>\n      <th>n_miss</th>\n      <th>start_date</th>\n      <th>end_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9</th>\n      <td>AAM</td>\n      <td>2019</td>\n      <td>-0.2810</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2019-01-31</td>\n      <td>2019-12-31</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>AAM</td>\n      <td>2020</td>\n      <td>-0.1622</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2020-01-31</td>\n      <td>2020-12-31</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>AAM</td>\n      <td>2021</td>\n      <td>0.1250</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2021-01-31</td>\n      <td>2021-12-31</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>AAM</td>\n      <td>2022</td>\n      <td>-0.0913</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2022-01-31</td>\n      <td>2022-12-31</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>AAM</td>\n      <td>2023</td>\n      <td>-0.2337</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2023-01-31</td>\n      <td>2023-12-31</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>ABI</td>\n      <td>2019</td>\n      <td>0.1946</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2019-01-31</td>\n      <td>2019-12-31</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>ABI</td>\n      <td>2020</td>\n      <td>0.2418</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2020-01-31</td>\n      <td>2020-12-31</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>ABI</td>\n      <td>2021</td>\n      <td>0.2896</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2021-01-31</td>\n      <td>2021-12-31</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>ABI</td>\n      <td>2022</td>\n      <td>-0.5085</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2022-01-31</td>\n      <td>2022-12-31</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>ABI</td>\n      <td>2023</td>\n      <td>-0.5042</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2023-01-31</td>\n      <td>2023-12-31</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>ABT</td>\n      <td>2019</td>\n      <td>-0.1893</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2019-01-31</td>\n      <td>2019-12-31</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>ABT</td>\n      <td>2020</td>\n      <td>-0.1400</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2020-01-31</td>\n      <td>2020-12-31</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>ABT</td>\n      <td>2021</td>\n      <td>0.0842</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2021-01-31</td>\n      <td>2021-12-31</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>ABT</td>\n      <td>2022</td>\n      <td>-0.0789</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2022-01-31</td>\n      <td>2022-12-31</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>ABT</td>\n      <td>2023</td>\n      <td>-0.0768</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2023-01-31</td>\n      <td>2023-12-31</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>ACC</td>\n      <td>2019</td>\n      <td>-0.1875</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2019-01-31</td>\n      <td>2019-12-31</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>ACC</td>\n      <td>2020</td>\n      <td>-0.4923</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2020-01-31</td>\n      <td>2020-12-31</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>ACC</td>\n      <td>2021</td>\n      <td>1.2339</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2021-01-31</td>\n      <td>2021-12-31</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>ACC</td>\n      <td>2022</td>\n      <td>-0.8538</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2022-01-31</td>\n      <td>2022-12-31</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>ACC</td>\n      <td>2023</td>\n      <td>0.1027</td>\n      <td>12</td>\n      <td>0</td>\n      <td>2023-01-31</td>\n      <td>2023-12-31</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: callout-important\nWhen the number of non-missing observations (`n_obs`) is less than 12 for an annual return, the compound return represents only a partial year. This commonly occurs in the first and last years of a security's listing on HOSE, HNX, or UPCoM, or when a stock transfers between exchanges (e.g., from UPCoM to HOSE upon meeting listing requirements). Users should decide whether to retain or exclude such partial-year observations depending on their research design.\n:::\n\n## Method 3: Iterative Compounding with Retain Logic\n\nIn some applications, we need fine-grained control over how missing values, delisting events, or other special conditions affect the compounding process. The iterative approach processes each observation sequentially, carrying forward the cumulative return and applying conditional logic at each step.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndef compute_cumret_iterative(df, ret_col=\"ret_total\",\n                              group_col=\"symbol\",\n                              handle_missing=\"carry\"):\n    \"\"\"Compute cumulative returns iteratively with flexible\n    missing value handling.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    ret_col : str\n    group_col : str\n    handle_missing : str\n        'carry' : treat missing as zero return (carry forward)\n        'propagate' : propagate NaN (conservative)\n        'reset' : reset wealth index to 1 after missing spell\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    results = []\n\n    for name, group in df.groupby(group_col):\n        cumret = 1.0\n        cumrets = []\n        for _, row in group.iterrows():\n            ret = row[ret_col]\n            if pd.notna(ret):\n                cumret = cumret * (1 + ret)\n            else:\n                if handle_missing == \"propagate\":\n                    cumret = np.nan\n                elif handle_missing == \"reset\":\n                    cumret = 1.0\n                # 'carry' does nothing (cumret unchanged)\n            cumrets.append(cumret)\n        group = group.copy()\n        group[\"wealth_iter\"] = cumrets\n        group[\"cumret_iter\"] = group[\"wealth_iter\"] - 1\n        results.append(group)\n\n    return pd.concat(results, ignore_index=True)\n```\n:::\n\n\n::: callout-note\nThe iterative method is the slowest of the four approaches because it cannot leverage NumPy's vectorized operations. For large datasets, prefer Method 1 or 2 unless the conditional logic in Method 3 is essential. On a dataset with 1 million observations, Method 1 runs in approximately 0.1 seconds versus 10+ seconds for Method 3.\n:::\n\n### Comparison of Missing Value Treatments\n\nTo illustrate how the three missing-value strategies differ, consider a hypothetical stock with one missing return in the middle of its history:\n\n::: {#tbl-missing-comparison .cell tbl-cap='Effect of different missing value treatments on cumulative returns. The \\'carry\\' strategy assumes zero return for missing periods (appropriate for trading halts); \\'propagate\\' makes all subsequent values undefined (conservative); \\'reset\\' restarts the cumulative product after the missing spell.' execution_count=16}\n``` {.python .cell-code}\nexample = pd.DataFrame({\n    \"symbol\": [1]*6,\n    \"date\": pd.date_range(\"2024-01-31\", periods=6, freq=\"ME\"),\n    \"ret_total\": [0.05, 0.03, np.nan, 0.04, -0.02, 0.06]\n})\n\ncarry = compute_cumret_iterative(example, handle_missing=\"carry\")\npropagate = compute_cumret_iterative(\n    example, handle_missing=\"propagate\"\n)\nreset = compute_cumret_iterative(example, handle_missing=\"reset\")\n\ncomparison = pd.DataFrame({\n    \"Date\": example[\"date\"].dt.strftime(\"%Y-%m\"),\n    \"Return\": example[\"ret_total\"],\n    \"Carry\": carry[\"cumret_iter\"].round(6),\n    \"Propagate\": propagate[\"cumret_iter\"].round(6),\n    \"Reset\": reset[\"cumret_iter\"].round(6)\n})\ncomparison\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Return</th>\n      <th>Carry</th>\n      <th>Propagate</th>\n      <th>Reset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2024-01</td>\n      <td>0.05</td>\n      <td>0.050000</td>\n      <td>0.0500</td>\n      <td>0.050000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2024-02</td>\n      <td>0.03</td>\n      <td>0.081500</td>\n      <td>0.0815</td>\n      <td>0.081500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2024-03</td>\n      <td>NaN</td>\n      <td>0.081500</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2024-04</td>\n      <td>0.04</td>\n      <td>0.124760</td>\n      <td>NaN</td>\n      <td>0.040000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2024-05</td>\n      <td>-0.02</td>\n      <td>0.102265</td>\n      <td>NaN</td>\n      <td>0.019200</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2024-06</td>\n      <td>0.06</td>\n      <td>0.168401</td>\n      <td>NaN</td>\n      <td>0.080352</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Method 4: Rolling Compound Returns\n\nFor many empirical applications, including momentum strategies, performance evaluation, and risk estimation, we need compound returns over rolling windows of fixed length. This section implements efficient rolling compounding using pandas.\n\n### Rolling Window via Log Returns\n\nThe most efficient approach combines the log-sum-exp method with rolling sums:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndef rolling_compound_return(df, ret_col=\"ret_total\",\n                             group_col=\"symbol\",\n                             windows=[3, 6, 9, 12]):\n    \"\"\"Compute rolling compound returns over specified windows.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must be sorted by [group_col, 'date'] with no gaps.\n    ret_col : str\n    group_col : str\n    windows : list of int\n        Rolling window lengths (in periods).\n\n    Returns\n    -------\n    pd.DataFrame with new columns ret_{k} for each window k.\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[\"log_ret\"] = np.log(1 + df[ret_col])\n\n    for k in windows:\n        rolling_logsum = (\n            df.groupby(group_col)[\"log_ret\"]\n            .transform(\n                lambda x: x.rolling(\n                    window=k, min_periods=k\n                ).sum()\n            )\n        )\n        df[f\"ret_{k}\"] = np.exp(rolling_logsum) - 1\n\n    df.drop(columns=[\"log_ret\"], inplace=True)\n    return df\n```\n:::\n\n\nWe apply this to our full sample to compute 3-, 6-, 9-, and 12-month trailing compound returns:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nstock_rolling = rolling_compound_return(\n    prices_monthly,\n    windows=[3, 6, 9, 12]\n)\n```\n:::\n\n\nLet us also compute the same rolling returns for the market index, which serves as a benchmark for excess return calculations:\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Compute market rolling returns\nmarket_monthly = (\n    prices_monthly[[\"date\", \"mkt_total\"]]\n    .drop_duplicates()\n    .sort_values(\"date\")\n    .copy()\n)\nmarket_monthly[\"log_mkt\"] = np.log(1 + market_monthly[\"mkt_total\"])\n\nfor k in [3, 6, 9, 12]:\n    market_monthly[f\"mkt_{k}\"] = (\n        np.exp(\n            market_monthly[\"log_mkt\"]\n            .rolling(window=k, min_periods=k)\n            .sum()\n        ) - 1\n    )\n\nmarket_monthly.drop(columns=[\"log_mkt\"], inplace=True)\n\n# Merge market rolling returns back\nstock_rolling = stock_rolling.merge(\n    market_monthly[\n        [\"date\"] + [f\"mkt_{k}\" for k in [3, 6, 9, 12]]\n    ],\n    on=\"date\",\n    how=\"left\"\n)\n```\n:::\n\n\n@fig-rolling-returns displays the distribution of 12-month rolling compound returns over time.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nrolling_stats = (\n    stock_rolling\n    .dropna(subset=[\"ret_12\"])\n    .groupby(\"date\")[\"ret_12\"]\n    .agg([\"median\", lambda x: x.quantile(0.25),\n           lambda x: x.quantile(0.75)])\n    .reset_index()\n)\nrolling_stats.columns = [\"date\", \"median\", \"p25\", \"p75\"]\n\nplot_rolling = (\n    ggplot(rolling_stats, aes(x=\"date\")) +\n    geom_ribbon(aes(ymin=\"p25\", ymax=\"p75\"),\n                alpha=0.3, fill=\"#2166ac\") +\n    geom_line(aes(y=\"median\"), color=\"#2166ac\", size=0.7) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    labs(x=\"\", y=\"12-month compound return\") +\n    scale_y_continuous(labels=percent_format()) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_rolling.draw()\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n![Cross-sectional distribution of 12-month rolling compound returns for Vietnamese stocks over time. The shaded band represents the interquartile range (25th–75th percentiles), while the solid line shows the median. Sharp market-wide events—such as the 2008 global financial crisis and the 2020 COVID-19 shock—are visible as periods when even the median return turns sharply negative.](16_compound_return_files/figure-pdf/fig-rolling-returns-output-1.pdf){#fig-rolling-returns fig-alt='Time series chart showing the distribution of 12-month rolling stock returns in Vietnam.' fig-pos='htbp'}\n:::\n:::\n\n\n### Verifying Rolling Returns\n\nIt is prudent to verify rolling compound returns against a direct calculation. We select one stock and recompute its 12-month return manually:\n\n::: {#tbl-rolling-verify .cell tbl-cap='Verification of rolling compound return calculation. The \\'Direct\\' column computes the product of the preceding 12 monthly gross returns minus one; \\'Rolling\\' uses our log-sum-exp function. Differences are at machine precision.' execution_count=21}\n``` {.python .cell-code}\ntest_stock = long_history_stocks[0]\ntest_data = (\n    stock_rolling[stock_rolling[\"symbol\"] == test_stock]\n    .sort_values(\"date\")\n    .tail(15)\n    .copy()\n)\n\n# Direct computation\ntest_data[\"direct_ret_12\"] = (\n    test_data[\"ret_total\"]\n    .transform(\n        lambda x: x.add(1).rolling(\n            12, min_periods=12\n        ).apply(np.prod, raw=True) - 1\n    )\n)\n\nverify = (\n    test_data[[\"date\", \"ret_12\", \"direct_ret_12\"]]\n    .dropna()\n    .tail(5)\n    .copy()\n)\nverify[\"difference\"] = (\n    verify[\"ret_12\"] - verify[\"direct_ret_12\"]\n).abs()\nverify.round(8)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_2230066/922053246.py:28: UserWarning: obj.round has no effect with datetime, timedelta, or period dtypes. Use obj.dt.round(...) instead.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>ret_12</th>\n      <th>direct_ret_12</th>\n      <th>difference</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>386</th>\n      <td>2023-09-30</td>\n      <td>-0.152407</td>\n      <td>-0.152407</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>387</th>\n      <td>2023-10-31</td>\n      <td>-0.208836</td>\n      <td>-0.208836</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>2023-11-30</td>\n      <td>-0.199018</td>\n      <td>-0.199018</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>389</th>\n      <td>2023-12-31</td>\n      <td>-0.233698</td>\n      <td>-0.233698</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Delisting Returns and Survivorship Bias\n\nA critical practical concern when computing compound returns is the treatment of securities that are removed from an exchange. Delisting occurs for various reasons: mergers and acquisitions, bankruptcy, failure to meet listing requirements, voluntary withdrawal, or transfer to another exchange. If delisting returns are not incorporated, the resulting compound returns suffer from survivorship bias: they overstate performance because the worst outcomes (bankruptcies, forced delistings) are excluded [@shumway1997delisting].\n\n### The Vietnamese Context\n\nIn Vietnam, securities can be removed from their exchange listing for several reasons as specified by the SSC and exchange regulations:\n\n-   **Mandatory delisting**: when a firm has accumulated losses exceeding its charter capital, fails to meet financial reporting obligations for three consecutive years, or has its business license revoked.\n-   **Voluntary delisting**: when a firm's shareholders vote to withdraw from the exchange.\n-   **Transfer**: when a firm moves from UPCoM to HOSE/HNX (upgrade) or from HOSE/HNX to UPCoM (downgrade). These transfers are not true delistings in the economic sense but require careful handling in return calculations.\n\nUnlike more developed markets where detailed delisting return data is systematically compiled, Vietnamese market data may not always provide an explicit delisting return. When a stock is delisted for cause (e.g., bankruptcy), the last traded price may significantly overstate the security's recovery value. Researchers should be aware of this limitation and consider imputing delisting returns based on the delisting reason, following the methodology of @shumway1997delisting.\n\n### Incorporating Delisting Returns\n\nWhen a security is delisted, a final \"delisting return\" captures the value change between the last regular trading day and the realization of value after delisting. This return must be combined with the regular return in the delisting month:\n\n$$\nR_t^{\\text{adj}} = (1 + R_t)(1 + R_t^{\\text{delist}}) - 1,\n$$ {#eq-delisting-adj}\n\nwhere $R_t$ is the regular return and $R_t^{\\text{delist}}$ is the delisting return. If the regular return is missing (the stock ceased trading before month end), we use the delisting return alone.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ndef adjust_for_delisting(df, ret_col=\"ret_total\",\n                          dlret_col=\"dlret\"):\n    \"\"\"Adjust returns for delisting events.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain `ret_col` and `dlret_col`.\n\n    Returns\n    -------\n    pd.DataFrame with adjusted return column 'ret_adj'.\n    \"\"\"\n    df = df.copy()\n    df[\"ret_adj\"] = df[ret_col]\n\n    # Case 1: Both regular and delisting returns available\n    mask_both = df[ret_col].notna() & df[dlret_col].notna()\n    df.loc[mask_both, \"ret_adj\"] = (\n        (1 + df.loc[mask_both, ret_col]) *\n        (1 + df.loc[mask_both, dlret_col]) - 1\n    )\n\n    # Case 2: Only delisting return available\n    mask_dlret_only = (\n        df[ret_col].isna() & df[dlret_col].notna()\n    )\n    df.loc[mask_dlret_only, \"ret_adj\"] = (\n        df.loc[mask_dlret_only, dlret_col]\n    )\n\n    return df\n```\n:::\n\n\n### Impact of Delisting Adjustment\n\nThe magnitude of the delisting bias depends on the frequency and severity of delisting events. @shumway1997delisting showed that, in developed markets, ignoring delisting returns introduces an upward bias of approximately 1% per year in equal-weighted portfolio returns. The bias is larger for small-cap stocks and value stocks, which are more prone to financial distress. In Vietnam, where smaller firms on HNX and UPCoM face tighter liquidity constraints and higher default risk, the bias may be even more pronounced. In emerging market delistings, mandatory delistings often involve firms with severe financial distress where residual equity value is near zero, implying delisting returns close to $-100\\%$ in the worst cases.\n\n## Rolling Volatility Estimation\n\nStock return volatility is a key input for risk management, option pricing, and many empirical asset pricing models. A common approach is to estimate rolling standard deviations of returns over a trailing window.\n\n### 24-Month Rolling Volatility\n\nFollowing @ben2012hedge, we compute the total stock return volatility as the rolling standard deviation of monthly returns over a 24-month window:\n\n$$\n\\hat{\\sigma}_{i,t}^{24} = \\sqrt{\\frac{1}{23}\\sum_{j=0}^{23}(R_{i,t-j} - \\bar{R}_{i,t}^{24})^2},\n$$ {#eq-rolling-vol}\n\nwhere $\\bar{R}_{i,t}^{24} = \\frac{1}{24}\\sum_{j=0}^{23} R_{i,t-j}$ is the trailing 24-month mean return.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\ndef rolling_volatility(df, ret_col=\"ret_total\",\n                        group_col=\"symbol\",\n                        window=24):\n    \"\"\"Compute rolling return volatility.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    ret_col : str\n    group_col : str\n    window : int\n        Rolling window length in periods.\n\n    Returns\n    -------\n    pd.DataFrame with 'vol_{window}' column (annualized).\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[f\"vol_{window}\"] = (\n        df.groupby(group_col)[ret_col]\n        .transform(\n            lambda x: x.rolling(\n                window=window, min_periods=window\n            ).std()\n        )\n    )\n    # Annualize (monthly to annual)\n    df[f\"vol_{window}_ann\"] = df[f\"vol_{window}\"] * np.sqrt(12)\n    return df\n```\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nstock_vol = rolling_volatility(stock_rolling)\n```\n:::\n\n\n@fig-vol-distribution shows the cross-sectional distribution of annualized 24-month volatility over time.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nvol_stats = (\n    stock_vol\n    .dropna(subset=[\"vol_24_ann\"])\n    .groupby(\"date\")[\"vol_24_ann\"]\n    .agg([\"median\", lambda x: x.quantile(0.25),\n           lambda x: x.quantile(0.75)])\n    .reset_index()\n)\nvol_stats.columns = [\"date\", \"median\", \"p25\", \"p75\"]\n\nplot_vol = (\n    ggplot(vol_stats, aes(x=\"date\")) +\n    geom_ribbon(aes(ymin=\"p25\", ymax=\"p75\"),\n                alpha=0.3, fill=\"#b2182b\") +\n    geom_line(aes(y=\"median\"), color=\"#b2182b\", size=0.7) +\n    labs(x=\"\", y=\"Annualized 24-month volatility\") +\n    scale_y_continuous(labels=percent_format()) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_vol.draw()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n![Cross-sectional distribution of annualized 24-month rolling stock return volatility for Vietnamese equities. The median volatility (solid line) and interquartile range (shaded band) capture both secular trends and crisis episodes. Vietnamese stocks exhibit structurally higher volatility than developed-market peers, with the median annualized volatility typically ranging between 30% and 50%.](16_compound_return_files/figure-pdf/fig-vol-distribution-output-1.pdf){#fig-vol-distribution fig-alt='Time series of the cross-sectional distribution of stock return volatility in Vietnam.' fig-pos='htbp'}\n:::\n:::\n\n\n### Volatility and Compound Returns: The Variance Drain\n\nAs noted in @eq-arithmetic-geometric, the geometric mean return falls below the arithmetic mean by approximately $\\sigma^2/2$. This \"variance drain\" or \"volatility drag\" means that two portfolios with the same arithmetic mean return but different volatilities will have different compound returns: the lower-volatility portfolio will compound to greater terminal wealth.\n\nThis effect is quantitatively important in Vietnam. A stock with an arithmetic mean monthly return of 1.5% and a monthly standard deviation of 10% suffers a volatility drag of approximately $0.10^2/2 = 0.5\\%$ per month, or roughly 6% per year. This is consistent with the observation that Vietnamese investors face substantial erosion of compound wealth from the high idiosyncratic volatility of individual stocks. We can verify this empirically by sorting stocks into volatility quintiles and comparing compound returns:\n\n::: {#tbl-vol-drag .cell tbl-cap='Arithmetic mean, geometric mean, and volatility by volatility quintile for Vietnamese stocks. The difference between arithmetic and geometric mean increases with volatility, confirming the variance drain effect. The magnitude of the drag is notably large for the highest-volatility quintile, typical of small and illiquid stocks on HNX and UPCoM.' execution_count=26}\n``` {.python .cell-code}\nannual_data = compound_return_by_period(\n    prices_monthly, period=\"year\"\n)\nannual_data = annual_data[annual_data[\"n_obs\"] >= 10].copy()\n\nvol_annual = (\n    prices_monthly\n    .groupby([\"symbol\", prices_monthly[\"date\"].dt.year])[\n        \"ret_total\"\n    ]\n    .agg([\"std\", \"mean\", \"count\"])\n    .reset_index()\n)\nvol_annual.columns = [\"symbol\", \"period\", \"monthly_std\",\n                       \"monthly_mean\", \"n_months\"]\nvol_annual = vol_annual[vol_annual[\"n_months\"] >= 10].copy()\nvol_annual[\"ann_vol\"] = vol_annual[\"monthly_std\"] * np.sqrt(12)\nvol_annual[\"arith_mean_ann\"] = vol_annual[\"monthly_mean\"] * 12\n\nvol_analysis = annual_data.merge(\n    vol_annual, on=[\"symbol\", \"period\"]\n)\n\nvol_analysis[\"vol_quintile\"] = (\n    vol_analysis.groupby(\"period\")[\"ann_vol\"]\n    .transform(\n        lambda x: pd.qcut(\n            x, 5, labels=[1, 2, 3, 4, 5], duplicates=\"drop\"\n        )\n    )\n)\n\nvol_summary = (\n    vol_analysis\n    .groupby(\"vol_quintile\")\n    .agg(\n        arithmetic_mean=(\"arith_mean_ann\", \"mean\"),\n        geometric_mean=(\"cumret\", \"mean\"),\n        avg_volatility=(\"ann_vol\", \"mean\"),\n        n_stockyears=(\"cumret\", \"count\")\n    )\n    .round(4)\n    .reset_index()\n)\nvol_summary\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vol_quintile</th>\n      <th>arithmetic_mean</th>\n      <th>geometric_mean</th>\n      <th>avg_volatility</th>\n      <th>n_stockyears</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>-0.0908</td>\n      <td>-0.0763</td>\n      <td>0.1887</td>\n      <td>2708</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>-0.0754</td>\n      <td>-0.0610</td>\n      <td>0.3312</td>\n      <td>2700</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>-0.0388</td>\n      <td>-0.0169</td>\n      <td>0.4493</td>\n      <td>2701</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0.0404</td>\n      <td>0.0494</td>\n      <td>0.6005</td>\n      <td>2700</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.4411</td>\n      <td>0.3389</td>\n      <td>1.0288</td>\n      <td>2705</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Compound Returns Around Fiscal Year Ends\n\nA widely used approach in accounting and finance research aligns compound returns to firm-specific fiscal period end dates. This is essential for computing buy-and-hold abnormal returns (BHARs) for event studies, post-earnings-announcement drift, and other studies where the event date varies by firm.\n\nIn Vietnam, the majority of listed firms follow a calendar fiscal year (January–December), as required by the Law on Accounting unless the Ministry of Finance grants an exemption. However, firms in certain industries (e.g., agriculture, tourism) may use non-standard fiscal years ending in March, June, or September.\n\n<!--# The DataCore.vn financial statements database records the firm-specific fiscal year end for each firm-year. -->\n\n### Aligning Returns to Fiscal Periods\n\nThe key challenge is that fiscal year ends differ across firms. We need to compute compound returns over windows anchored at these firm-specific dates.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ndef compound_returns_around_event(\n    returns_df, events_df,\n    id_col=\"symbol\", date_col=\"date\",\n    event_date_col=\"datadate\", ret_col=\"ret_total\",\n    pre_windows=[3, 6, 9, 12],\n    post_windows=[3, 6]\n):\n    \"\"\"Compute compound returns in windows around firm-specific\n    event dates.\n\n    Parameters\n    ----------\n    returns_df : pd.DataFrame\n        Monthly returns with [id_col, date_col, ret_col].\n    events_df : pd.DataFrame\n        Event dates with [id_col, event_date_col].\n    pre_windows : list of int\n        Trailing window lengths (months before event).\n    post_windows : list of int\n        Forward window lengths (months after event).\n\n    Returns\n    -------\n    pd.DataFrame with compound returns for each window.\n    \"\"\"\n    returns_df = returns_df.sort_values(\n        [id_col, date_col]\n    ).copy()\n    events_df = events_df.copy()\n\n    # Align event dates to month ends\n    events_df[\"event_month\"] = (\n        pd.to_datetime(events_df[event_date_col])\n        + pd.offsets.MonthEnd(0)\n    )\n\n    results = []\n\n    for _, event in events_df.iterrows():\n        sid = event[id_col]\n        edate = event[\"event_month\"]\n\n        sec_rets = returns_df[\n            returns_df[id_col] == sid\n        ].copy()\n        sec_rets = sec_rets.set_index(date_col)[ret_col]\n\n        row = {id_col: sid,\n               event_date_col: event[event_date_col]}\n\n        # Pre-event compound returns\n        for k in pre_windows:\n            start = edate - pd.DateOffset(months=k-1)\n            start = (start - pd.offsets.MonthEnd(0)\n                     + pd.offsets.MonthEnd(0))\n            window_rets = sec_rets[\n                (sec_rets.index >= start)\n                & (sec_rets.index <= edate)\n            ]\n            if len(window_rets) >= k * 0.8:\n                cumret = (\n                    np.exp(np.log(1 + window_rets).sum()) - 1\n                )\n            else:\n                cumret = np.nan\n            row[f\"ret_pre_{k}\"] = cumret\n\n        # Post-event compound returns\n        for k in post_windows:\n            start = edate + pd.DateOffset(months=1)\n            end = (edate + pd.DateOffset(months=k)\n                   + pd.offsets.MonthEnd(0))\n            window_rets = sec_rets[\n                (sec_rets.index >= start)\n                & (sec_rets.index <= end)\n            ]\n            if len(window_rets) >= k * 0.8:\n                cumret = (\n                    np.exp(np.log(1 + window_rets).sum()) - 1\n                )\n            else:\n                cumret = np.nan\n            row[f\"ret_post_{k}\"] = cumret\n\n        results.append(row)\n\n    return pd.DataFrame(results)\n```\n:::\n\n\n### Buy-and-Hold Abnormal Returns versus Cumulative Abnormal Returns\n\nFor event studies and performance evaluation, we often want the **excess** compound return, which is the stock's compound return minus a benchmark's compound return over the same window. The buy-and-hold abnormal return (BHAR) is defined as\n\n$$\n\\text{BHAR}_{i,t}(k) = \\prod_{j=1}^{k}(1 + R_{i,t+j}) - \\prod_{j=1}^{k}(1 + R_{b,t+j}),\n$$ {#eq-bhar}\n\nwhere $R_{b,t}$ is the benchmark return (market index, size-matched portfolio, etc.). This differs from the cumulative abnormal return (CAR), which sums simple abnormal returns:\n\n$$\n\\text{CAR}_{i,t}(k) = \\sum_{j=1}^{k}(R_{i,t+j} - R_{b,t+j}).\n$$ {#eq-car}\n\nThe BHAR better captures the actual investor experience because it reflects the compounding of returns, whereas the CAR implicitly assumes daily rebalancing to maintain equal dollar positions in the stock and benchmark [@barber1997detecting]. The distinction is particularly important in Vietnam, where individual stock returns can be highly volatile and the compounding effect is therefore magnified. @lyon1999improved provide further analysis of the statistical properties of BHARs and recommend bootstrapped critical values for inference.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ndef compute_bhar(stock_returns, benchmark_returns):\n    \"\"\"Compute buy-and-hold abnormal return.\n\n    Parameters\n    ----------\n    stock_returns : array-like\n        Sequence of stock returns.\n    benchmark_returns : array-like\n        Sequence of benchmark returns (same length).\n\n    Returns\n    -------\n    float : BHAR\n    \"\"\"\n    stock_cumret = (\n        np.prod(1 + np.array(stock_returns)) - 1\n    )\n    bench_cumret = (\n        np.prod(1 + np.array(benchmark_returns)) - 1\n    )\n    return stock_cumret - bench_cumret\n```\n:::\n\n\n## Book Value of Equity\n\nMany empirical applications that use compound returns also require firm-level accounting variables. A commonly used variable is the book value of equity, computed following @daniel1997evidence:\n\n$$\n\\text{BE} = \\text{SE} + \\text{DT} + \\text{ITC} - \\text{PS},\n$$ {#eq-book-equity}\n\nwhere SE is stockholders' equity, DT is deferred taxes, ITC is investment tax credit, and PS is the preferred stock value. For preferred stock, the hierarchy is: redemption value if available, then liquidating value, then carrying value.\n\nIn Vietnam, the accounting standards (Vietnamese Accounting Standards, VAS, and increasingly IFRS adoption) provide a somewhat different chart of accounts. Stockholders' equity is reported on the balance sheet as *Vốn chủ sở hữu*, which includes contributed capital (*Vốn góp của chủ sở hữu*), share premium (*Thặng dư vốn cổ phần*), treasury stock adjustments, retained earnings (*Lợi nhuận sau thuế chưa phân phối*), and other reserves. Deferred tax assets and liabilities are reported separately. Preferred stock is rare among Vietnamese listed firms (most issue only common shares), but when present, its book value should be subtracted from total equity.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ndef compute_book_equity(df):\n    \"\"\"Compute book value of equity for Vietnamese firms.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain at minimum: equity (stockholders' equity),\n        deferred_tax (deferred tax liabilities, net),\n        pref_stock (preferred stock, if applicable).\n\n    Returns\n    -------\n    pd.DataFrame with 'be' column.\n    \"\"\"\n    df = df.copy()\n    df[\"pref\"] = df.get(\n        \"pref_stock\", pd.Series(0, index=df.index)\n    )\n    df[\"dt\"] = df.get(\n        \"deferred_tax\", pd.Series(0, index=df.index)\n    )\n    df[\"be\"] = (\n        df[\"equity\"].fillna(0)\n        + df[\"dt\"].fillna(0)\n        - df[\"pref\"].fillna(0)\n    )\n    # Set non-positive book equity to NaN\n    df.loc[df[\"be\"] <= 0, \"be\"] = np.nan\n    return df\n```\n:::\n\n\n## Maximum Drawdown\n\nThe maximum drawdown is a key risk metric that complements volatility. While volatility measures the dispersion of returns symmetrically, the maximum drawdown captures the worst cumulative loss an investor could experience: a measure that aligns more closely with how investors psychologically experience risk [@kahneman2013prospect].\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\ndef compute_max_drawdown(df, ret_col=\"ret_total\",\n                          group_col=\"symbol\"):\n    \"\"\"Compute maximum drawdown for each security.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    ret_col : str\n    group_col : str\n\n    Returns\n    -------\n    pd.DataFrame with 'max_drawdown' and running drawdown.\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[ret_col]\n    df[\"wealth\"] = (\n        df.groupby(group_col)[\"gross_ret\"].cumprod()\n    )\n    df[\"peak\"] = df.groupby(group_col)[\"wealth\"].cummax()\n    df[\"drawdown\"] = (\n        (df[\"wealth\"] - df[\"peak\"]) / df[\"peak\"]\n    )\n\n    max_dd = (\n        df.groupby(group_col)[\"drawdown\"]\n        .min()\n        .reset_index(name=\"max_drawdown\")\n    )\n    df = df.merge(max_dd, on=group_col)\n    df.drop(columns=[\"gross_ret\"], inplace=True)\n    return df\n```\n:::\n\n\n@fig-drawdown illustrates the drawdown profile for a selected stock.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\ndd_data = compute_max_drawdown(\n    prices_monthly[\n        prices_monthly[\"symbol\"] == long_history_stocks[0]\n    ]\n)\nmdd = dd_data[\"max_drawdown\"].iloc[0]\n\nplot_dd = (\n    ggplot(dd_data, aes(x=\"date\", y=\"drawdown\")) +\n    geom_area(fill=\"#b2182b\", alpha=0.4) +\n    geom_line(color=\"#b2182b\", size=0.5) +\n    geom_hline(yintercept=mdd, linetype=\"dashed\") +\n    labs(x=\"\", y=\"Drawdown from peak\") +\n    scale_y_continuous(labels=percent_format()) +\n    theme_minimal() +\n    theme(figure_size=(10, 4))\n)\nplot_dd.draw()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n![Drawdown profile for a selected Vietnamese stock showing the percentage decline from each running peak. The maximum drawdown (horizontal dashed line) represents the worst peak-to-trough loss over the full sample. Vietnamese stocks frequently exhibit drawdowns exceeding 50%, reflecting the market's high volatility and susceptibility to sentiment-driven corrections.](16_compound_return_files/figure-pdf/fig-drawdown-output-1.pdf){#fig-drawdown fig-alt='Time series chart of drawdowns for a single Vietnamese stock.' fig-pos='htbp'}\n:::\n:::\n\n\n## Putting It All Together: A Comprehensive Pipeline\n\nWe now combine all the methods into a single pipeline that produces a research-ready dataset with rolling compound returns, market returns, volatility, and drawdown measures.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\ndef build_compound_return_dataset(\n    stock_df, windows=[3, 6, 9, 12], vol_window=24\n):\n    \"\"\"Build comprehensive compound return dataset.\n\n    Parameters\n    ----------\n    stock_df : pd.DataFrame\n        Monthly stock return data with columns:\n        symbol, date, ret_total, mkt_total.\n    windows : list of int\n        Rolling compound return windows.\n    vol_window : int\n        Rolling volatility window.\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    df = stock_df.sort_values([\"symbol\", \"date\"]).copy()\n\n    # Step 1: Log returns\n    df[\"log_ret\"] = np.log(1 + df[\"ret_total\"])\n    df[\"log_mkt\"] = np.log(1 + df[\"mkt_total\"])\n\n    # Step 2: Rolling compound returns (stock and market)\n    for k in windows:\n        df[f\"ret_{k}\"] = np.exp(\n            df.groupby(\"symbol\")[\"log_ret\"]\n            .transform(\n                lambda x: x.rolling(k, min_periods=k).sum()\n            )\n        ) - 1\n\n        df[f\"mkt_{k}\"] = np.exp(\n            df[\"log_mkt\"]\n            .rolling(k, min_periods=k)\n            .sum()\n        ) - 1\n\n        # Excess compound return (BHAR vs market)\n        df[f\"exret_{k}\"] = df[f\"ret_{k}\"] - df[f\"mkt_{k}\"]\n\n    # Step 3: Cumulative return (full history)\n    df[\"wealth\"] = (\n        df.groupby(\"symbol\")[\"log_ret\"]\n        .cumsum()\n        .apply(np.exp)\n    )\n    df[\"cumret\"] = df[\"wealth\"] - 1\n\n    # Step 4: Rolling volatility\n    df[f\"vol_{vol_window}\"] = (\n        df.groupby(\"symbol\")[\"ret_total\"]\n        .transform(\n            lambda x: x.rolling(\n                vol_window, min_periods=vol_window\n            ).std()\n        )\n    ) * np.sqrt(12)  # annualize\n\n    # Step 5: Drawdown\n    df[\"peak\"] = df.groupby(\"symbol\")[\"wealth\"].cummax()\n    df[\"drawdown\"] = (df[\"wealth\"] - df[\"peak\"]) / df[\"peak\"]\n\n    # Clean up\n    df.drop(\n        columns=[\"log_ret\", \"log_mkt\", \"peak\"], inplace=True\n    )\n\n    return df\n```\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n# Build the full dataset\ncompound_dataset = build_compound_return_dataset(prices_monthly)\n```\n:::\n\n\n@tbl-summary-stats provides summary statistics for the key variables in our compound return dataset.\n\n::: {#tbl-summary-stats .cell tbl-cap='Summary statistics for compound return variables across all Vietnamese stock-month observations. Returns are in decimal form (0.10 = 10%). The wide dispersion of 12-month compound returns and the high median volatility reflect the emerging market characteristics of the Vietnamese equity market.' execution_count=34}\n``` {.python .cell-code}\nsummary_cols = [\"ret_total\", \"ret_3\", \"ret_6\", \"ret_12\",\n                \"exret_3\", \"exret_12\", \"vol_24\", \"drawdown\"]\navailable_cols = [c for c in summary_cols\n                  if c in compound_dataset.columns]\n\nsummary = (\n    compound_dataset[available_cols]\n    .describe(percentiles=[0.05, 0.25, 0.50, 0.75, 0.95])\n    .T\n    .round(4)\n)\nsummary\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>5%</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>95%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ret_total</th>\n      <td>165499.0</td>\n      <td>0.0042</td>\n      <td>0.1862</td>\n      <td>-0.9900</td>\n      <td>-0.2381</td>\n      <td>-0.0703</td>\n      <td>0.0000</td>\n      <td>0.0553</td>\n      <td>0.2773</td>\n      <td>12.7500</td>\n    </tr>\n    <tr>\n      <th>ret_3</th>\n      <td>162586.0</td>\n      <td>0.0094</td>\n      <td>0.3393</td>\n      <td>-0.9999</td>\n      <td>-0.3889</td>\n      <td>-0.1436</td>\n      <td>-0.0126</td>\n      <td>0.0987</td>\n      <td>0.5000</td>\n      <td>27.2911</td>\n    </tr>\n    <tr>\n      <th>ret_6</th>\n      <td>158227.0</td>\n      <td>0.0171</td>\n      <td>0.5053</td>\n      <td>-0.9999</td>\n      <td>-0.5095</td>\n      <td>-0.2196</td>\n      <td>-0.0400</td>\n      <td>0.1404</td>\n      <td>0.7320</td>\n      <td>35.7136</td>\n    </tr>\n    <tr>\n      <th>ret_12</th>\n      <td>149520.0</td>\n      <td>0.0375</td>\n      <td>0.8136</td>\n      <td>-0.9999</td>\n      <td>-0.6522</td>\n      <td>-0.3191</td>\n      <td>-0.0877</td>\n      <td>0.1807</td>\n      <td>1.0767</td>\n      <td>47.9515</td>\n    </tr>\n    <tr>\n      <th>exret_3</th>\n      <td>153637.0</td>\n      <td>0.0385</td>\n      <td>0.3343</td>\n      <td>-1.1691</td>\n      <td>-0.3420</td>\n      <td>-0.1163</td>\n      <td>0.0067</td>\n      <td>0.1378</td>\n      <td>0.4992</td>\n      <td>27.3041</td>\n    </tr>\n    <tr>\n      <th>exret_12</th>\n      <td>140571.0</td>\n      <td>0.1401</td>\n      <td>0.8031</td>\n      <td>-1.5858</td>\n      <td>-0.5388</td>\n      <td>-0.2003</td>\n      <td>0.0281</td>\n      <td>0.2880</td>\n      <td>1.1119</td>\n      <td>48.0488</td>\n    </tr>\n    <tr>\n      <th>vol_24</th>\n      <td>132233.0</td>\n      <td>0.5493</td>\n      <td>0.3488</td>\n      <td>0.0000</td>\n      <td>0.2070</td>\n      <td>0.3445</td>\n      <td>0.4827</td>\n      <td>0.6737</td>\n      <td>1.0739</td>\n      <td>9.1792</td>\n    </tr>\n    <tr>\n      <th>drawdown</th>\n      <td>165499.0</td>\n      <td>-0.5927</td>\n      <td>0.2975</td>\n      <td>-1.0000</td>\n      <td>-0.9631</td>\n      <td>-0.8501</td>\n      <td>-0.6616</td>\n      <td>-0.3725</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Cross-Sectional Distribution of Compound Returns\n\nTo understand how compound returns vary across securities, we examine the cross-sectional distribution at different horizons.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nhorizon_data = pd.DataFrame()\nfor k in [3, 6, 12]:\n    col = f\"ret_{k}\"\n    temp = compound_dataset[[col]].dropna().copy()\n    temp.columns = [\"compound_return\"]\n    temp[\"horizon\"] = f\"{k} months\"\n    lo, hi = temp[\"compound_return\"].quantile([0.01, 0.99])\n    temp = temp[\n        (temp[\"compound_return\"] >= lo)\n        & (temp[\"compound_return\"] <= hi)\n    ]\n    horizon_data = pd.concat([horizon_data, temp])\n\nplot_horizons = (\n    ggplot(horizon_data,\n           aes(x=\"compound_return\", fill=\"horizon\")) +\n    geom_density(alpha=0.4) +\n    geom_vline(xintercept=0, linetype=\"dashed\") +\n    labs(x=\"Compound return\", y=\"Density\", fill=\"Horizon\") +\n    scale_x_continuous(labels=percent_format()) +\n    theme_minimal() +\n    theme(legend_position=\"bottom\",\n          figure_size=(10, 5))\n)\nplot_horizons.draw()\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n![Cross-sectional distribution of compound returns at different horizons (3, 6, and 12 months) for Vietnamese stocks. Longer horizons exhibit greater dispersion and more pronounced right skewness, reflecting the compounding of idiosyncratic risk. The fat tails are more extreme than those typically observed in developed markets, consistent with the higher volatility environment.](16_compound_return_files/figure-pdf/fig-horizon-comparison-output-1.pdf){#fig-horizon-comparison fig-alt='Overlaid density plots of compound returns at 3, 6, and 12 month horizons for Vietnamese stocks.' fig-pos='htbp'}\n:::\n:::\n\n\n## Vietnam-Specific Considerations\n\n### Price Limits and Their Effect on Compounding\n\nVietnam's stock exchanges impose daily price limits that cap the maximum price change from the reference price. As of the latest regulations:\n\n-   **HOSE**: $\\pm 7\\%$\n-   **HNX**: $\\pm 10\\%$\n-   **UPCoM**: $\\pm 15\\%$\n\nThese limits truncate the daily return distribution and can create sequences of limit-hit days when large information events occur. For compound return computation, this means that the adjustment to new information may be spread over multiple days rather than occurring instantaneously. When computing monthly compound returns from daily data, this is handled correctly because the compound return accumulates the full adjustment regardless of how many days it takes.\n\nHowever, price limits can introduce bias in short-horizon return computations. If a large positive event occurs and the stock hits the limit-up ceiling for several consecutive days, the 1-day or 1-week compound return will understate the true information content of the event [@kim2013reconsidering]. For event study applications, researchers should verify that the event window is long enough to accommodate the price-limit-induced delay in price adjustment.\n\n### Foreign Ownership Limits\n\nVietnam imposes foreign ownership limits (FOL) on listed companies, typically capped at 49% for most industries and lower (30% or less) for certain restricted sectors such as banking and telecommunications. When a stock reaches its FOL, foreign investors can only purchase shares from other foreign sellers, creating a parallel premium market for foreign-board shares. This does not directly affect the computation of compound returns (which use official traded prices), but researchers studying cross-border portfolio returns should be aware that the effective price paid by foreign investors may differ from the board price [@vo2017foreign].\n\n### The VN-Index and Market Benchmarks\n\nFor benchmark compound returns, Vietnam's primary indices are:\n\n-   **VN-Index**: The capitalization-weighted index of all HOSE-listed stocks.\n-   **VN30**: The 30 largest and most liquid stocks on HOSE, reviewed semi-annually.\n-   **HNX-Index**: The capitalization-weighted index of HNX-listed stocks.\n\nThe VN-Index is the most widely used benchmark and is the default market return in our dataset.\n\n## Performance Considerations\n\nWhen working with large datasets, computational efficiency matters. @tbl-performance-benchmark compares the execution time of our four compounding methods on a standardized dataset.\n\n::: {#tbl-performance-benchmark .cell tbl-cap='Execution time comparison for different compounding methods on a dataset of 10,000 stock-month observations. The cumulative product and log-sum-exp methods are orders of magnitude faster than the iterative approach due to NumPy vectorization.' execution_count=36}\n``` {.python .cell-code}\nimport time\n\nnp.random.seed(42)\nn_stocks = 100\nn_months = 100\ntest_df = pd.DataFrame({\n    \"symbol\": np.repeat(range(n_stocks), n_months),\n    \"date\": np.tile(\n        pd.date_range(\"2015-01-31\", periods=n_months,\n                       freq=\"ME\"),\n        n_stocks\n    ),\n    \"ret_total\": np.random.normal(\n        0.01, 0.08, n_stocks * n_months\n    )\n})\n\nmethods = {}\n\nt0 = time.time()\n_ = compute_cumret_cumprod(test_df)\nmethods[\"Cumulative Product\"] = time.time() - t0\n\nt0 = time.time()\n_ = compute_cumret_logsum(test_df)\nmethods[\"Log-Sum-Exp\"] = time.time() - t0\n\nt0 = time.time()\n_ = compute_cumret_iterative(test_df)\nmethods[\"Iterative (carry)\"] = time.time() - t0\n\nt0 = time.time()\n_ = rolling_compound_return(test_df, windows=[12])\nmethods[\"Rolling (12-month)\"] = time.time() - t0\n\nperf_df = pd.DataFrame({\n    \"Method\": methods.keys(),\n    \"Time (seconds)\": [f\"{v:.4f}\" for v in methods.values()],\n    \"Relative Speed\": [\n        f\"{v/min(methods.values()):.1f}x\"\n        for v in methods.values()\n    ]\n})\nperf_df\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Method</th>\n      <th>Time (seconds)</th>\n      <th>Relative Speed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cumulative Product</td>\n      <td>0.0043</td>\n      <td>1.0x</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Log-Sum-Exp</td>\n      <td>0.0044</td>\n      <td>1.0x</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Iterative (carry)</td>\n      <td>0.5345</td>\n      <td>125.7x</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rolling (12-month)</td>\n      <td>0.0228</td>\n      <td>5.4x</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Common Pitfalls and Best Practices\n\nSeveral subtle issues can lead to incorrect compound return calculations. We summarize the most important ones:\n\n**Gaps in the time series.** If a security has months with no observations (not even a missing return flag), rolling window calculations based on positional indexing will produce incorrect results. The rolling window will span the wrong calendar period. Always ensure that the time series is complete, fill gaps with explicit missing values before computing rolling statistics. This is particularly relevant in Vietnam, where trading suspensions can create gaps.\n\n**Survivorship bias.** As discussed in the delisting returns section, excluding securities that cease trading biases compound returns upward. Always incorporate delisting returns when available. When delisting returns are unavailable (as is sometimes the case in Vietnamese data), consider using imputed values based on the delisting reason.\n\n**Look-ahead bias.** When aligning compound returns to fiscal year ends for cross-sectional analysis, be careful not to use returns from before the fiscal year end to predict post-announcement returns. Vietnamese firms are required to publish audited annual financial statements within 90 days of the fiscal year end, so a buffer of at least 3 months is advisable when constructing forward-looking compound returns.\n\n**Numerical overflow and underflow.** For very long compounding horizons or extreme returns, the cumulative product can overflow (`inf`) or underflow (0). The log-sum-exp approach is more robust to such numerical issues because it operates in log space where the range is compressed.\n\n**Annualization of partial periods.** When computing annualized returns from partial-period data (e.g., 7 months of data annualized to 12), the annualization formula $(1+R)^{12/k} - 1$ assumes that the observed return rate will persist. This assumption is stronger for short partial periods and can produce misleading results. Report the actual compound return and the number of periods alongside any annualized figures.\n\n**Exchange transfers.** In Vietnam, stocks sometimes transfer between UPCoM, HNX, and HOSE. These transfers may involve temporary trading halts and can cause apparent gaps in the return series. When computing compound returns that span an exchange transfer, ensure that the return series is continuous across the transfer date.\n\n<!-- ## Exercises\n\n1.  Compute the annual compound return for the VN-Index for each year in the sample. How does it compare to the simple sum of monthly returns? In which years is the difference largest, and why?\n\n2.  Implement a function that computes the **Calmar ratio** (annualized return divided by the absolute value of maximum drawdown) for each security. Which securities have the highest Calmar ratios over the most recent 5-year period? Do they tend to be on HOSE, HNX, or UPCoM?\n\n3.  Sort stocks into quintiles based on their 12-month trailing compound return (the momentum signal) and compute the equal-weighted average return of each quintile in the subsequent month. Does the momentum effect appear in Vietnamese data? Compare your findings with evidence from @vo2018does.\n\n4.  Extend the `rolling_compound_return` function to handle overlapping fiscal years, where the compounding window is aligned to each firm's fiscal year end rather than calendar months.\n\n5.  Compare the BHAR and CAR measures for a set of randomly chosen event dates. Under what conditions do they diverge most? Relate your findings to the theoretical discussion of @barber1997detecting.\n\n6.  Compute the cross-sectional correlation between 24-month rolling volatility and subsequent 12-month compound returns. Is there a risk-return tradeoff in the Vietnamese equity market cross section? How does this compare to the positive risk-return relationship documented in developed markets by @ghysels2005there?\n\n7.  Examine how daily price limits affect the computation of weekly compound returns. Select stocks that hit the limit-up or limit-down ceiling on at least one day, and compare their weekly compound return to similar-magnitude events in stocks that did not hit the limit. -->\n\n",
    "supporting": [
      "16_compound_return_files/figure-pdf"
    ],
    "filters": []
  }
}