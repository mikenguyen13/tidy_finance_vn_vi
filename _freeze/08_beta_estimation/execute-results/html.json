{
  "hash": "6dcbcaefccee61d45b310f42ab7f66de",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Beta Estimation\nformat:\n  html:\n    toc: true\n    number-sections: true\njupyter: python3\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\nIn this chapter, we introduce an important concept in financial economics: The exposure of an individual stock to changes in the market portfolio. According to the Capital Asset Pricing Model (CAPM) of @Sharpe1964, @Lintner1965, and @Mossin1966, cross-sectional variation in expected asset returns should be a function of the covariance between the excess return of the asset and the excess return on the market portfolio.\\index{CAPM} The regression coefficient of excess market returns on excess stock returns is usually called the market beta. We show an estimation procedure for the market betas.\\index{Beta} We provide details about all the functions that we use to compute the results. In particular, we leverage useful computational concepts: Rolling-window estimation and parallelization.\n\nWe use the following Python packages throughout this chapter:\n\n::: {#513bd053 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom joblib import Parallel, delayed, cpu_count\nfrom itertools import product\nfrom dateutil.relativedelta import relativedelta\n```\n:::\n\n\nCompared to previous chapters, we introduce `statsmodels` [@seabold2010statsmodels] for regression analysis and for sliding-window regressions and `joblib` [@joblib] for parallelization.\\index{Parallelization}\n\n## Estimating Beta Using Monthly Returns\n\nThe estimation procedure is based on a rolling-window estimation, where we may use either monthly or daily returns and different window lengths. First, let us start with loading the monthly Stock data from our SQLite database introduced in [Accessing and Managing Financial Data](accessing-and-managing-financial-data.qmd) and [DataCore](datacore_data.qmd).\n\n::: {#3c3c916c .cell execution_count=2}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\n# Prepare accounting data with a 'year' column for merging\ncomp_vn = (pd.read_sql_query(\n    sql=\"SELECT symbol, datadate, icb_name_vi FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"})\n  .assign(year=lambda x: x[\"datadate\"].dt.year)\n  .dropna()\n)\n\n# Ensure prices_monthly has a 'year' column and is merged correctly\nprices_monthly = (pd.read_sql_query(\n    sql=\"SELECT symbol, date, ret_excess FROM prices_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .assign(year=lambda x: x[\"date\"].dt.year)\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nprices_monthly = (prices_monthly\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n  .merge(comp_vn, how=\"left\", on=[\"symbol\", \"year\"])\n  .dropna()\n)\n```\n:::\n\n\n::: {#b7f9bf22 .cell execution_count=3}\n``` {.python .cell-code}\nfrom scipy.stats.mstats import winsorize\n\n# Apply 1% winsorization to returns\nprices_monthly = (prices_monthly\n  .assign(\n    ret_excess = lambda x: winsorize(x[\"ret_excess\"], limits=[0.01, 0.01]),\n    mkt_excess = lambda x: winsorize(x[\"mkt_excess\"], limits=[0.01, 0.01])\n  )\n)\n```\n:::\n\n\nTo estimate the CAPM regression coefficients\n\n$$\nr_{i, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{i, t},\n$${#eq-capm-equation}\n\nwe regress stock excess returns `ret_excess` on excess returns of the market portfolio `mkt_excess`. \n\nPython provides a simple solution to estimate (linear) models with the function `smf.ols()`. The function requires a formula as input that is specified in a compact symbolic form. An expression of the form `y ~ model` is interpreted as a specification that the response `y` is modeled by a linear predictor specified symbolically by `model`. Such a model consists of a series of terms separated by `+` operators. In addition to standard linear models, `smf.ols()` provides a lot of flexibility. To start, we restrict the data only to the time series of observations in Stock data that correspond to a company's stock  and compute $\\hat\\alpha_i$ as well as $\\hat\\beta_i$.\n\n::: {#d7f3dd87 .cell execution_count=4}\n``` {.python .cell-code}\nmodel_fit = smf.ols(\n    formula=\"ret_excess ~ mkt_excess\", \n    data=prices_monthly.query(\"symbol == 'VIN'\")\n).fit()\ncoefficients = model_fit.summary2().tables[1]\ncoefficients\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coef.</th>\n      <th>Std.Err.</th>\n      <th>t</th>\n      <th>P&gt;|t|</th>\n      <th>[0.025</th>\n      <th>0.975]</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Intercept</th>\n      <td>0.015495</td>\n      <td>0.016506</td>\n      <td>0.938751</td>\n      <td>0.349934</td>\n      <td>-0.017220</td>\n      <td>0.048210</td>\n    </tr>\n    <tr>\n      <th>mkt_excess</th>\n      <td>3.133326</td>\n      <td>1.865697</td>\n      <td>1.679440</td>\n      <td>0.095931</td>\n      <td>-0.564424</td>\n      <td>6.831076</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n`smf.ols()` returns an object of class `RegressionModel`, which contains all the information we usually care about with linear models. `summary2()` returns information about the estimated parameters. The output above indicates that Apple moves excessively with the market as the estimated $\\hat\\beta_i$ is above one ($\\hat\\beta_i \\approx 1.4$). \n\n## Rolling-Window Estimation\n\nAfter we estimated the regression coefficients on an example, we scale the estimation of  $\\beta_i$ to a whole different level and perform rolling-window estimations for the entire CRSP sample.\\index{Rolling-window estimation} The following function implements the CAPM regression for a data frame (or a part thereof) containing at least `min_obs` observations to avoid huge fluctuations if the time series is too short. The function conveniently returns the regression results as a data frame, which ensures that our approach is scalable. If the `min_obs`-condition is violated, that is, the time series is too short, the function returns an empty data frame for consistency. \n\n::: {#b614f466 .cell execution_count=5}\n``` {.python .cell-code}\ndef estimate_capm(data, min_obs=1):\n    if data.shape[0] < min_obs:\n        capm = pd.DataFrame()\n    else:\n        fit = smf.ols(formula=\"ret_excess ~ mkt_excess\", data=data).fit()\n        coefficients = fit.summary2().tables[1]\n\n        capm = pd.DataFrame(\n            {\n                \"coefficient\": coefficients.index,\n                \"estimate\": coefficients[\"Coef.\"],\n                \"t_statistic\": coefficients[\"t\"],\n            }\n        ).assign(\n            coefficient=lambda x: np.where(\n                x[\"coefficient\"] == \"Intercept\", \"alpha\", x[\"coefficient\"]\n            )\n        )\n\n    return capm\n```\n:::\n\n\nNext, we define a function that does the rolling estimation. We use a simple for-loop to implement the sliding window estimation in a straightforward manner. The following function takes input data and slides across the `date` vector, considering only a total of `look_back` months. The function essentially performs three steps: (i) arrange all rows, (ii) compute betas by sliding across months, and (iii) return a tibble with dates and corresponding parameter estimates. As we demonstrate further below, we can also apply the same function to daily return data.\n\n::: {#6aed049a .cell execution_count=6}\n``` {.python .cell-code}\ndef roll_capm_estimation(data, look_back=60, min_obs=48):\n    results = []\n    dates = data[\"date\"].sort_values().drop_duplicates()\n\n    for i in range(look_back - 1, len(dates)):\n        end_date = dates.iloc[i]\n        start_date = end_date - relativedelta(months=look_back - 1)\n\n        window_data = data.query(\"date >= @start_date & date <= @end_date\")\n\n        result = estimate_capm(window_data, min_obs=min_obs)\n        result[\"date\"] = np.max(window_data[\"date\"])\n        results.append(result)\n\n    if results:\n        rolling_capm_estimation = pd.concat(results, ignore_index=True)\n    else:\n        rolling_capm_estimation = pd.DataFrame()\n\n    return rolling_capm_estimation\n```\n:::\n\n\nBefore we approach the whole Stock sample, let us focus on a couple of examples for well-known firms.\n\n::: {#5e515aab .cell execution_count=7}\n``` {.python .cell-code}\nexamples = pd.DataFrame({\n    \"symbol\": [\"FPT\", \"VNM\", \"VIC\", \"HPG\"],\n    \"company\": [\"FPT Corporation\", \"Vinamilk\", \"Vingroup\", \"Hoa Phat Group\"]\n})\n```\n:::\n\n\n::: {#5bebf577 .cell execution_count=8}\n``` {.python .cell-code}\n# Check how many months of data each example firm has\n(prices_monthly\n  .query(\"symbol in @examples['symbol']\")\n  .groupby(\"symbol\")\n  .size()\n  .reset_index(name=\"obs_count\")\n)\n\nprices_monthly[prices_monthly['symbol'] == \"FPT\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>date</th>\n      <th>ret_excess</th>\n      <th>year</th>\n      <th>mkt_excess</th>\n      <th>datadate</th>\n      <th>icb_name_vi</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>41646</th>\n      <td>FPT</td>\n      <td>2011-07-31</td>\n      <td>0.194583</td>\n      <td>2011</td>\n      <td>-0.011287</td>\n      <td>2011-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n    <tr>\n      <th>41647</th>\n      <td>FPT</td>\n      <td>2011-08-31</td>\n      <td>-0.038116</td>\n      <td>2011</td>\n      <td>0.007856</td>\n      <td>2011-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n    <tr>\n      <th>41648</th>\n      <td>FPT</td>\n      <td>2011-09-30</td>\n      <td>-0.093421</td>\n      <td>2011</td>\n      <td>-0.006501</td>\n      <td>2011-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n    <tr>\n      <th>41649</th>\n      <td>FPT</td>\n      <td>2011-10-31</td>\n      <td>-0.042168</td>\n      <td>2011</td>\n      <td>-0.005363</td>\n      <td>2011-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n    <tr>\n      <th>41650</th>\n      <td>FPT</td>\n      <td>2011-11-30</td>\n      <td>-0.011414</td>\n      <td>2011</td>\n      <td>-0.009524</td>\n      <td>2011-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41791</th>\n      <td>FPT</td>\n      <td>2023-08-31</td>\n      <td>0.113277</td>\n      <td>2023</td>\n      <td>0.004178</td>\n      <td>2023-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n    <tr>\n      <th>41792</th>\n      <td>FPT</td>\n      <td>2023-09-30</td>\n      <td>-0.043664</td>\n      <td>2023</td>\n      <td>-0.000150</td>\n      <td>2023-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n    <tr>\n      <th>41793</th>\n      <td>FPT</td>\n      <td>2023-10-31</td>\n      <td>-0.108937</td>\n      <td>2023</td>\n      <td>-0.024628</td>\n      <td>2023-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n    <tr>\n      <th>41794</th>\n      <td>FPT</td>\n      <td>2023-11-30</td>\n      <td>0.103896</td>\n      <td>2023</td>\n      <td>-0.009693</td>\n      <td>2023-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n    <tr>\n      <th>41795</th>\n      <td>FPT</td>\n      <td>2023-12-31</td>\n      <td>0.042369</td>\n      <td>2023</td>\n      <td>-0.000822</td>\n      <td>2023-12-31</td>\n      <td>Công nghệ phần mềm</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows × 7 columns</p>\n</div>\n```\n:::\n:::\n\n\nThe main idea is to apply the function to each stock individually and then combine the results into a single data frame. First, we nest the data by `symbol`. Nested data means we now have a list of `symbol` with corresponding grouped time series data. We get one row of output for each unique combination of non-nested variables which is only `symbol` in this case.\n\n::: {#de854ce5 .cell execution_count=9}\n``` {.python .cell-code}\ncapm_examples_nested = (prices_monthly\n    .query(\"symbol in @examples['symbol']\")\n    .groupby(\"symbol\", group_keys=True)\n)\ncapm_examples_nested\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n<pandas.api.typing.DataFrameGroupBy object at 0x7fe4337e7250>\n```\n:::\n:::\n\n\nNext, we want to apply the `roll_capm_estimation()` function to each stock. This situation is an ideal use case for `apply()`, which takes a list or vector as input and returns an object of the same length as the input. In our case, `apply()` returns a single data frame with a time series of beta estimates for each stock. Therefore, we use `reset_index()` to transform the list of outputs to a tidy data frame. \n\n::: {#0a2244ee .cell execution_count=10}\n``` {.python .cell-code}\n# use this after fixing the data\ncapm_examples = (capm_examples_nested\n    .apply(lambda x: roll_capm_estimation(x), include_groups=False)\n    .reset_index()\n    .get([\"symbol\", \"date\", \"coefficient\", \"estimate\", \"t_statistic\"])\n)\n```\n:::\n\n\n@fig-601 displays the resulting beta estimates, focusing exclusively on the coefficient fo `\"mkt_excess\"`.\n\n::: {#cell-fig-601 .cell execution_count=11}\n``` {.python .cell-code}\nbeta_examples_sub = capm_examples.merge(examples, how=\"left\", on=\"symbol\").query(\n    \"coefficient == 'mkt_excess'\"\n)\n\nbeta_figure = (\n    ggplot(\n        beta_examples_sub,\n        aes(x=\"date\", y=\"estimate\", color=\"company\", linetype=\"company\"),\n    )\n    + geom_line()\n    + labs(\n        x=\"\",\n        y=\"\",\n        color=\"\",\n        linetype=\"\",\n        title=\"Monthly beta estimates for example stocks using 5 years of data\",\n    )\n    + scale_x_datetime(date_breaks=\"5 year\", date_labels=\"%Y\")\n)\nbeta_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The figure shows monthly beta estimates for example stocks using five years of data. The CAPM betas are estimated with monthly data and a rolling window of length five years based on adjusted excess returns from Stock Data.](08_beta_estimation_files/figure-html/fig-601-output-1.png){#fig-601 width=672 height=480 fig-alt='Title: Monthly beta estimates for example stocks using five years of data. The figure shows a time series of beta estimates based on five years of monthly data. The estimated betas vary over time and across varies but are always positive for each stock.' fig-pos='htb'}\n:::\n:::\n\n\n## Parallelized Rolling-Window Estimation\n\nEven though we could now just apply the function using `.groubby()` on the whole Stock sample, we advise against doing it as it is computationally quite expensive. Remember that we have to perform rolling-window estimations across all stocks and time periods. However, this estimation problem is an ideal scenario to employ the power of parallelization. Parallelization means that we split the tasks which perform rolling-window estimations across different workers (or cores on your local machine). \n\nIf you have a Windows or Mac machine, it makes most sense use the default parallelization backend of `joblib`, which means that separate Python processes are running in the background on the same machine to perform the individual jobs. If you check out the documentation of `joblib.parallel_config()`, you can also see other ways to resolve the parallelization in different environments. Note that we use `availableCores()` to determine the number of cores available for parallelization, but keep one core free for other tasks. Some machines might freeze if all cores are busy with Python jobs. \n\n::: {#9594e42a .cell execution_count=12}\n``` {.python .cell-code}\nn_cores = cpu_count() - 1\nn_cores\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n3\n```\n:::\n:::\n\n\nYou can speed up things considerably by having more cores available to share the workload or by having more powerful cores. Instead of using `.apply()` on groups, we use `Parallel()` to execute multiple tasks concurrently and `delayed()` to wrap each function call, allowing the calls to be queued and distributed to worker processes rather than executed immediately.\n\n::: {#c4337dd0 .cell execution_count=13}\n``` {.python .cell-code}\nprices_monthly_nested = (prices_monthly\n    .groupby(\"symbol\", group_keys=False)\n)\n\ncapm_monthly = pd.concat(\n    Parallel(n_jobs=n_cores)(\n        delayed(\n            lambda name, group: roll_capm_estimation(group).assign(symbol=name)\n        )(\n            name, group\n        )\n        for name, group in prices_monthly_nested\n    )\n).get([\"symbol\", \"date\", \"coefficient\", \"estimate\", \"t_statistic\"])\ncapm_monthly\n```\n:::\n\n\n::: {#f39b947e .cell execution_count=14}\n``` {.python .cell-code}\n(capm_monthly\n  .to_sql(name=\"capm_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n:::\n\n\n::: {#ba7f8df1 .cell execution_count=15}\n``` {.python .cell-code}\ncapm_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM capm_monthly\", \n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\ncapm_monthly.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>date</th>\n      <th>coefficient</th>\n      <th>estimate</th>\n      <th>t_statistic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A32</td>\n      <td>2023-10-31</td>\n      <td>alpha</td>\n      <td>-0.003802</td>\n      <td>-0.735962</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A32</td>\n      <td>2023-10-31</td>\n      <td>mkt_excess</td>\n      <td>0.685640</td>\n      <td>1.218415</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A32</td>\n      <td>2023-11-30</td>\n      <td>alpha</td>\n      <td>-0.003740</td>\n      <td>-0.721261</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A32</td>\n      <td>2023-11-30</td>\n      <td>mkt_excess</td>\n      <td>0.674715</td>\n      <td>1.205106</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A32</td>\n      <td>2023-12-31</td>\n      <td>alpha</td>\n      <td>-0.003796</td>\n      <td>-0.734142</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Estimating Beta Using Daily Returns\n\nBefore we provide some descriptive statistics of our beta estimates, we implement the estimation for the daily CRSP sample as well. Depending on the application, you might either use longer horizon beta estimates based on monthly data or shorter horizon estimates based on daily returns. As loading the full daily CRSP data requires relatively large amounts of memory, we split the beta estimation into smaller chunks.\\index{Parallelization} The logic follows the approach that we use to download the daily CRSP data (see [WRDS, CRSP, and Compustat](wrds-crsp-and-compustat.qmd)).\n\nFirst, we load the daily Fama-French market excess returns and extract the vector of dates.\n\n::: {#902299e2 .cell execution_count=16}\n``` {.python .cell-code}\nfactors_ff3_daily = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_daily\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n```\n:::\n\n\nWe use the stocks from the monthly Stock dataset as our reference point and process them in batches of 500. To estimate the CAPM over a consistent lookback window while accommodating different return frequencies, we adjust the minimum required number of observations accordingly. Specifically, we require at least 1,000 daily returns over a five‑year period for a valid estimation. This threshold is consistent with the monthly requirement of 48 observations out of 60 months, given that there are roughly 252 trading days in a year.\n\n::: {#130747da .cell execution_count=17}\n``` {.python .cell-code}\nsymbols = list(prices_monthly[\"symbol\"].unique().astype(str))\n\nbatch_size = 500\nbatches = np.ceil(len(symbols)/batch_size).astype(int)\nmin_obs = 1_000\n```\n:::\n\n\nWe then proceed to perform the same steps as with the monthly Stock data, just in batches: Load in daily returns, nest the data by stock, and parallelize the beta estimation across stocks. Note that we also convert the daily date to the beginning of the month so that we can still look back over 60 months and get one beta estimate per month, even though we are using daily data.\n\n::: {#ac7e4038 .cell execution_count=18}\n``` {.python .cell-code}\ncapm_daily = []\n\nfor j in range(1, batches+1):  \n    symbol_batch = symbols[\n      ((j-1)*batch_size):(min(j*batch_size, len(symbols)))\n    ]\n    \n    symbol_batch_formatted = (\n      \", \".join(f\"'{symbol}'\" for symbol in symbol_batch)\n    )\n    symbol_string = f\"({symbol_batch_formatted})\"\n    \n    prices_daily_sub_query = (\n      \"SELECT symbol, date, ret_excess \"\n        \"FROM prices_daily \"\n       f\"WHERE symbol IN {symbol_string}\" \n    )\n      \n    prices_daily_sub = pd.read_sql_query(\n      sql=prices_daily_sub_query,\n      con=tidy_finance,\n      dtype={\"symbol\": int},\n      parse_dates={\"date\"}\n    )\n    \n    prices_daily_sub_nested = (prices_daily_sub\n      .merge(factors_ff3_daily, how=\"inner\", on=\"date\")\n      .assign(\n          date = lambda x: \n            x[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n        )\n      .groupby(\"symbol\", group_keys=False)\n    )\n    \n    results = Parallel(n_jobs=n_cores)(\n        delayed(\n            lambda name, group: roll_capm_estimation(group, min_obs=min_obs).assign(symbol=name)\n        )(\n            name, group\n        )\n        for name, group in prices_daily_sub_nested\n    )\n    \n    if results:\n        capm_daily_sub = pd.concat(results).get(\n            [\"symbol\", \"date\", \"coefficient\", \"estimate\", \"t_statistic\"]\n        )\n        capm_daily.append(capm_daily_sub)\n    else:\n        print(f\"Warning: Batch {j} produced no results (insufficient data)\")\n              \n    print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")\n  \ncapm_daily = pd.concat(capm_daily)\n```\n:::\n\n\n## Comparing Beta Estimates\n\nWhat is a typical value for stock betas? First, let us extract the relevant estimates from our CAPM results based on monthly returns.\n\n::: {#ecae41bd .cell execution_count=19}\n``` {.python .cell-code}\nbeta_monthly = (capm_monthly\n    .query(\"coefficient == 'mkt_excess'\")\n    .get([\"symbol\", \"date\", \"estimate\"])\n    .rename(columns={\"estimate\": \"beta\"})\n    .assign(return_type=\"monthly\")\n)\n```\n:::\n\n\n::: {#f4a5d53b .cell execution_count=20}\n``` {.python .cell-code}\n(beta_monthly.to_sql(\n  name=\"beta_monthly\", \n  con=tidy_finance, \n  if_exists=\"replace\",\n  index=False\n  )\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n71488\n```\n:::\n:::\n\n\nTo get some feeling, we illustrate the dispersion of the estimated $\\hat\\beta_i$ across different industries and across time below. @fig-602 shows that typical business models across industries imply different exposure to the general market economy. \n\n::: {#cell-fig-602 .cell fig.alt='Title: Firm-specific beta distributions by industry. The figure shows box plots for each industry. Firms with the highest average CAPM beta belong to the public administration industry. Firms from the utility sector have the lowest average CAPM beta. The figure indicates very few outliers with negative CAPM betas. The large majority of all stocks has CAPM betas between 0.5 and 1.5.' execution_count=21}\n``` {.python .cell-code}\nbeta_industries = (beta_monthly\n    .merge(prices_monthly, how=\"inner\", on=[\"symbol\", \"date\"])\n    .dropna(subset=\"beta\")\n    .groupby([\"icb_name_vi\", \"symbol\"])[\"beta\"]\n    .aggregate(\"mean\")\n    .reset_index()\n)\n\nindustry_order = (beta_industries\n    .groupby(\"icb_name_vi\")[\"beta\"]\n    .aggregate(\"median\")\n    .sort_values()\n    .index.tolist()\n)\n\n# To show the 10 industries with the highest median beta\ntop_10_industries = industry_order[-10:]\n\n# To show the 10 industries with the lowest median beta\nbottom_10_industries = industry_order[:10]\n\n# Update the plot call\nbeta_industries_figure = (\n    ggplot(beta_industries, aes(x=\"icb_name_vi\", y=\"beta\"))\n    + geom_boxplot()\n    + coord_flip()\n    + scale_x_discrete(limits=top_10_industries) # Use the sliced list here\n    + labs(title=\"Top 10 Industries by Beta\")\n)\n\n# beta_industries_figure = (\n#     ggplot(\n#         beta_industries, \n#         aes(x=\"icb_name_vi\", y=\"beta\") \n#     )\n#     + geom_boxplot()\n#     + coord_flip()\n#     + labs(\n#         x=\"\",\n#         y=\"Beta\", \n#         title=\"Firm-specific beta distributions by industry\"\n#     )\n#     + scale_x_discrete(limits=industry_order)\n# )\n\nbeta_industries_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The box plots show the average firm-specific beta estimates by industry.](08_beta_estimation_files/figure-html/fig-602-output-1.png){#fig-602 width=672 height=480 fig-pos='htb'}\n:::\n:::\n\n\nNext, we illustrate the time-variation in the cross-section of estimated betas. @fig-603 shows the monthly deciles of estimated betas (based on monthly data) and indicates an interesting pattern: First, betas seem to vary over time in the sense that during some periods, there is a clear trend across all deciles. Second, the sample exhibits periods where the dispersion across stocks increases in the sense that the lower decile decreases and the upper decile increases, which indicates that for some stocks the correlation with the market increases while for others it decreases. Note also here: stocks with negative betas are a rare exception.\n\n::: {#cell-fig-603 .cell execution_count=22}\n``` {.python .cell-code}\nbeta_quantiles = (\n    beta_monthly.groupby(\"date\")[\"beta\"]\n    .quantile(q=np.arange(0.1, 1.0, 0.1))\n    .reset_index()\n    .rename(columns={\"level_1\": \"quantile\"})\n    .assign(quantile=lambda x: (x[\"quantile\"] * 100).astype(int))\n    .dropna()\n)\n\nlinetypes = [\"-\", \"--\", \"-.\", \":\"]\nn_quantiles = beta_quantiles[\"quantile\"].nunique()\n\nbeta_quantiles_figure = (\n    ggplot(\n        beta_quantiles,\n        aes(x=\"date\", y=\"beta\", color=\"factor(quantile)\", linetype=\"factor(quantile)\"),\n    )\n    + geom_line()\n    + labs(\n        x=\"\", y=\"\", color=\"\", linetype=\"\", title=\"Monthly deciles of estimated betas\"\n    )\n    + scale_x_datetime(date_breaks=\"5 year\", date_labels=\"%Y\")\n    + scale_linetype_manual(\n        values=[linetypes[l % len(linetypes)] for l in range(n_quantiles)]\n    )\n)\nbeta_quantiles_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The figure shows monthly deciles of estimated betas. Each line corresponds to the monthly cross-sectional quantile of the estimated CAPM beta.](08_beta_estimation_files/figure-html/fig-603-output-1.png){#fig-603 width=672 height=480 fig-alt='Title: Monthly deciles of estimated betas. The figure shows time series of deciles of estimated betas to illustrate the distribution of betas over time.' fig-pos='htb'}\n:::\n:::\n\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table. \n\n::: {#0c2705d9 .cell execution_count=23}\n``` {.python .cell-code}\nbeta_daily = (capm_daily\n    .query(\"coefficient == 'mkt_excess'\")\n    .get([\"symbol\", \"date\", \"estimate\"])\n    .rename(columns={\"estimate\": \"beta\"})\n    .assign(return_type=\"daily\")\n)\n\nbeta = pd.concat([beta_monthly, beta_daily], ignore_index=True)\n```\n:::\n\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table. Then, we use the table to plot a comparison of beta estimates for our example stocks in @fig-604. \n\n::: {#fig-604 .cell execution_count=24}\n``` {.python .cell-code}\nbeta_comparison = beta.merge(examples, how=\"inner\", on=\"symbol\")\n\nbeta_comparison_figure = (\n    ggplot(\n        beta_comparison,\n        aes(x=\"date\", y=\"beta\", color=\"return_type\", linetype=\"return_type\"),\n    )\n    + geom_line()\n    + facet_wrap(\"~company\", ncol=1)\n    + labs(\n        x=\"\",\n        y=\"\",\n        color=\"\",\n        linetype=\"\",\n        title=\"Comparison of beta estimates using monthly and daily data\",\n    )\n    + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n    + theme(figure_size=(6.4, 6.4))\n)\nbeta_comparison_figure.show()\n```\n:::\n\n\nThe estimates in @fig-604 look as expected. As you can see, it really depends on the data frequency how your beta estimates turn out because the estimates based on daily data are much smoother due to the higher number of observations in each regression. \n\nFinally, we write the estimates to our database so that we can use them in later chapters. \n\n::: {#c17a2913 .cell execution_count=25}\n``` {.python .cell-code}\n(beta.to_sql(\n  name=\"beta\", \n  con=tidy_finance, \n  if_exists=\"replace\",\n  index=False\n  )\n)\n```\n:::\n\n\nWhenever you perform some kind of estimation, it also makes sense to do rough plausibility tests. A possible check is to plot the share of stocks with beta estimates over time. This descriptive helps us discover potential errors in our data preparation or estimation procedure. For instance, suppose there was a gap in our output where we do not have any betas. In this case, we would have to go back and check all previous steps to find out what went wrong. \n\n::: {#fig-605 .cell execution_count=26}\n``` {.python .cell-code}\nreturn_types = pd.DataFrame({\"return_type\": [\"monthly\", \"daily\"]})\n\nbeta_coverage = (\n    prices_monthly.merge(return_types, how=\"cross\")\n    .merge(beta, on=[\"symbol\", \"date\", \"return_type\"], how=\"left\")\n    .groupby([\"date\", \"return_type\"], as_index=False)\n    .apply(lambda x: pd.Series({\"share\": x[\"beta\"].notna().sum() / len(x)}))\n)\n\nbeta_coverage_figure = (\n    ggplot(\n        beta_coverage,\n        aes(x=\"date\", y=\"share\", color=\"return_type\", linetype=\"return_type\"),\n    )\n    + geom_line()\n    + labs(\n        x=\"\",\n        y=\"\",\n        color=\"\",\n        linetype=\"\",\n        title=\"End-of-month share of securities with beta estimates\",\n    )\n    + scale_y_continuous(labels=percent_format())\n    + scale_x_datetime(date_breaks=\"10 year\", date_labels=\"%Y\")\n)\nbeta_coverage_figure.show()\n```\n:::\n\n\n@fig-605 shows no issues, as the two coverage lines track each other closely, so we can proceed to the next check.\n\nWe also encourage everyone to always look at the distributional summary statistics of variables. You can easily spot outliers or weird distributions when looking at such tables.\\index{Summary statistics}\n\n::: {#a6f63b3a .cell execution_count=27}\n``` {.python .cell-code}\n(beta\n    .groupby(\"return_type\")[\"beta\"]\n    .describe()\n    .round(2)\n)\n```\n:::\n\n\nThe summary statistics also look plausible for the two estimation procedures. \n\nFinally, since we have two different estimators for the same theoretical object, we expect the estimators to be at least positively correlated (although not perfectly as the estimators are based on different sample periods and frequencies).\n\n::: {#4841c225 .cell execution_count=28}\n``` {.python .cell-code}\n(beta\n    .pivot_table(index=[\"symbol\", \"date\"], columns=\"return_type\", values=\"beta\")\n    .reset_index()\n    .get([\"monthly\", \"daily\"])\n    .corr()\n    .round(2)\n)\n```\n:::\n\n\nIndeed, we find a positive correlation between our beta estimates. In the subsequent chapters, we mainly use the estimates based on monthly data, as most readers should be able to replicate them due to potential memory limitations that might arise with the daily data. \n\n## Key Takeaways\n\n- CAPM betas can be estimated using rolling-window estimation  and processed in parallel via `joblib`.\n- Both monthly and daily return data can be used to estimate betas with different frequencies and window lengths, depending on the application.\n-  Summary statistics, visualization, and plausibility checks help to validate beta estimates across time and industries.\n\n",
    "supporting": [
      "08_beta_estimation_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}