{
  "hash": "828e10e2b075bf298030add56f40dc74",
  "result": {
    "engine": "jupyter",
    "markdown": "# Factor Construction Principles\n\n::: callout-note\nIn this chapter, we develop a general-purpose factor construction engine for the Vietnamese equity market. We cover every methodological decision in the pipeline, including universe definition, breakpoint computation, portfolio formation, weighting, rebalancing, and factor return calculation, and demonstrate how each choice affects the resulting factor.\n:::\n\nThe previous chapters introduced specific asset pricing models, including the CAPM, the Fama-French three-factor model, and momentum. Each of those chapters presented its factor as given. This chapter steps behind the curtain and addresses the engineering question: *how exactly do you build a factor?* The question matters because seemingly minor methodological decisions, such as where to set breakpoints, whether to value-weight or equal-weight, how to handle missing accounting data, which stocks to exclude, can alter the magnitude, statistical significance, and even the sign of a factor premium.\n\nIn the U.S. context, @fama1993common established a canonical procedure: sort stocks independently on size and a characteristic, form six value-weighted portfolios from 2×3 intersections, and define the factor as the average return of the two high-characteristic portfolios minus the average return of the two low-characteristic portfolios. This procedure has been replicated thousands of times. But it was designed for the U.S. market circa 1990, with its deep liquidity, broad cross-section, and CRSP/Compustat data infrastructure. Applying it mechanically to Vietnam, a market with 700 listed stocks, extreme illiquidity in the bottom tercile, high concentration in the top decile, and accounting data that arrives with variable lags, requires careful adaptation.\n\n@hou2020replicating replicated 452 anomalies from the U.S. literature and found that over half fail to replicate even in U.S. data with minor methodological variations. The replication crisis in empirical asset pricing makes it essential that researchers understand and document every construction choice. This chapter provides the tools to do so transparently.\n\n## The Factor Construction Pipeline {#sec-factor-con-pipeline}\n\nEvery tradeable factor follows the same logical pipeline:\n\n1. Define the universe: Select the eligible securities (e.g., common stocks with liquidity and size filters).\n2. Compute the signal: Calculate the characteristic of interest (e.g., value, momentum, profitability).\n3. Set breakpoints: Determine how stocks will be sorted (e.g., median, quintiles, deciles).\n4. Assign portfolios: Group stocks into high and low (or multiple) portfolios based on the signal.\n5. Compute returns: Calculate portfolio returns (equal- or value-weighted).\n6. Construct the factor: Take Long (high) - Short (low).\n7. Validate: Test performance, significance, and robustness.\n\nEach step involves choices that interact with each other. A breakpoint that works well for a liquid universe may be inappropriate for the full cross-section. A weighting scheme that reduces noise in the U.S. may amplify it in Vietnam. We address each step systematically.\n\n## Data Construction {#sec-factor-con-data}\n\n::: {#setup .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Import libraries and configure environment\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom itertools import product\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n```\n:::\n\n\n::: {#data-load .cell execution_count=3}\n``` {.python .cell-code code-summary=\"Load returns, characteristics, and accounting data\"}\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Monthly returns (survivorship-bias-free)\nmonthly = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'shares_outstanding', 'volume_avg_20d', 'turnover_value_avg_20d',\n        'n_zero_volume_days', 'exchange'\n    ]\n)\n\n# Annual accounting data\naccounting = client.get_fundamentals(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2006-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    frequency='annual',\n    fields=[\n        'ticker', 'fiscal_year', 'filing_date',\n        'total_assets', 'total_equity', 'book_equity',\n        'net_income', 'revenue', 'gross_profit',\n        'operating_profit', 'total_debt', 'retained_earnings',\n        'dividends_paid', 'capex', 'depreciation',\n        'shares_outstanding_fy'\n    ]\n)\n\n# Daily prices for momentum and volatility signals\ndaily = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=['ticker', 'date', 'adjusted_close', 'volume', 'turnover_value']\n)\n\nmonthly['month_end'] = pd.to_datetime(monthly['month_end'])\nmonthly = monthly.sort_values(['ticker', 'month_end'])\n\nprint(f\"Monthly returns: {len(monthly):,} firm-months\")\nprint(f\"Accounting: {len(accounting):,} firm-years\")\nprint(f\"Unique tickers: {monthly['ticker'].nunique()}\")\n```\n:::\n\n\n## Step 1: Universe Definition {#sec-factor-con-universe}\n\nThe first and most consequential choice is which stocks enter the factor construction universe. The universe definition determines what population the factor premium describes and whether it is implementable.\n\n### The Universe Problem in Vietnam\n\nVietnam presents a specific challenge: the cross-section is small (600-800 stocks on HOSE and HNX combined), and the size distribution is extremely skewed. The top 10 stocks by market capitalization account for roughly 50% of the total market cap on HOSE. The bottom tercile consists of micro-cap stocks that often trade fewer than 5 days per month. Including these stocks inflates apparent factor premia because their prices are noisy and stale, but excluding them shrinks the already small cross-section.\n\n::: {#universe-filters .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Define universe filters and document their effects\"}\ndef apply_universe_filters(df, filters='standard'):\n    \"\"\"\n    Apply universe filters to the monthly return panel.\n    \n    Parameters\n    ----------\n    filters : str\n        'none': all stocks\n        'minimal': exclude zero market cap and extreme returns\n        'standard': + minimum listing age + positive volume\n        'strict': + minimum market cap + minimum turnover\n    \n    Returns\n    -------\n    Filtered DataFrame with 'in_universe' column\n    \"\"\"\n    d = df.copy()\n    d['in_universe'] = True\n    \n    # Always: remove missing returns and market cap\n    d.loc[d['monthly_return'].isna(), 'in_universe'] = False\n    d.loc[d['market_cap'].isna() | (d['market_cap'] <= 0), 'in_universe'] = False\n    \n    # Minimal: winsorize extreme returns (likely data errors)\n    if filters in ['minimal', 'standard', 'strict']:\n        d.loc[d['monthly_return'].abs() > 1.0, 'in_universe'] = False\n    \n    # Standard: listing age >= 6 months\n    if filters in ['standard', 'strict']:\n        d['listing_age'] = (\n            d.groupby('ticker').cumcount() + 1\n        )\n        d.loc[d['listing_age'] < 6, 'in_universe'] = False\n        \n        # Require at least 10 positive-volume days in the month\n        d.loc[d['n_zero_volume_days'] > 12, 'in_universe'] = False\n    \n    # Strict: minimum market cap (20th percentile of HOSE)\n    if filters == 'strict':\n        mcap_threshold = (\n            d[d['exchange'] == 'HOSE']\n            .groupby('month_end')['market_cap']\n            .transform(lambda x: x.quantile(0.20))\n        )\n        # Apply HOSE threshold to all stocks\n        d['mcap_threshold'] = (\n            d.groupby('month_end')['market_cap']\n            .transform(lambda x: x.quantile(0.20))\n        )\n        d.loc[d['market_cap'] < d['mcap_threshold'], 'in_universe'] = False\n        \n        # Minimum average daily turnover (VND 200 million)\n        d.loc[d['turnover_value_avg_20d'] < 2e8, 'in_universe'] = False\n    \n    return d\n\n# Apply all filter levels and compare\nfilter_summary = {}\nfor level in ['none', 'minimal', 'standard', 'strict']:\n    filtered = apply_universe_filters(monthly, filters=level)\n    in_univ = filtered[filtered['in_universe']]\n    filter_summary[level] = {\n        'Firm-months': len(in_univ),\n        'Avg stocks/month': in_univ.groupby('month_end')['ticker'].nunique().mean(),\n        'Avg MCap coverage (%)': (\n            in_univ.groupby('month_end')['market_cap'].sum()\n            / filtered.groupby('month_end')['market_cap'].sum()\n        ).mean() * 100\n    }\n\nfilter_df = pd.DataFrame(filter_summary).T\nprint(\"Universe Filter Effects:\")\nprint(filter_df.round(1).to_string())\n```\n:::\n\n\n::: {#fig-universe-size .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Plot universe size under different filters over time\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ncolors_filter = {\n    'none': '#BDC3C7', 'minimal': '#3498DB',\n    'standard': '#2C5F8A', 'strict': '#C0392B'\n}\n\nfor level in ['none', 'minimal', 'standard', 'strict']:\n    filtered = apply_universe_filters(monthly, filters=level)\n    counts = (\n        filtered[filtered['in_universe']]\n        .groupby('month_end')['ticker']\n        .nunique()\n    )\n    axes[0].plot(counts.index, counts.values,\n                 color=colors_filter[level], linewidth=1.5, label=level)\n\naxes[0].set_ylabel('Number of Stocks')\naxes[0].set_title('Panel A: Universe Size')\naxes[0].legend()\n\n# Panel B: Market cap coverage\nfor level in ['minimal', 'standard', 'strict']:\n    filtered = apply_universe_filters(monthly, filters=level)\n    total_mcap = filtered.groupby('month_end')['market_cap'].sum()\n    filtered_mcap = (\n        filtered[filtered['in_universe']]\n        .groupby('month_end')['market_cap']\n        .sum()\n    )\n    coverage = (filtered_mcap / total_mcap * 100).dropna()\n    axes[1].plot(coverage.index, coverage.values,\n                 color=colors_filter[level], linewidth=1.5, label=level)\n\naxes[1].set_ylabel('Market Cap Coverage (%)')\naxes[1].set_title('Panel B: Market Capitalization Coverage')\naxes[1].legend()\naxes[1].set_ylim([60, 102])\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Step 2: Signal Construction {#sec-factor-con-signals}\n\n### Point-in-Time Accounting Data\n\nAs discussed in the missing data chapter, accounting signals must be aligned with their public availability date to avoid look-ahead bias. We implement a general-purpose point-in-time merge:\n\n::: {#pit-merge .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Point-in-time alignment of accounting data with monthly returns\"}\ndef pit_merge_accounting(monthly_df, accounting_df, lag_months=4):\n    \"\"\"\n    Merge accounting data with monthly returns respecting\n    the point-in-time availability constraint.\n    \n    Vietnamese annual reports are due within 90 days of fiscal\n    year-end. We use a conservative 4-month lag.\n    \n    Parameters\n    ----------\n    lag_months : int\n        Number of months after fiscal year-end before data\n        are assumed to be publicly available.\n    \"\"\"\n    acc = accounting_df.copy()\n    \n    # Accounting data becomes available lag_months after FY end\n    # If filing_date is available, use it; otherwise use FY-end + lag\n    if 'filing_date' in acc.columns:\n        acc['filing_date'] = pd.to_datetime(acc['filing_date'])\n        acc['available_date'] = acc['filing_date']\n        # Fallback for missing filing dates\n        acc['fy_end'] = pd.to_datetime(\n            acc['fiscal_year'].astype(str) + '-12-31'\n        )\n        acc['available_date'] = acc['available_date'].fillna(\n            acc['fy_end'] + pd.DateOffset(months=lag_months)\n        )\n    else:\n        acc['available_date'] = pd.to_datetime(\n            acc['fiscal_year'].astype(str) + '-12-31'\n        ) + pd.DateOffset(months=lag_months)\n    \n    # For each firm-month, find the most recent available accounting data\n    merged = monthly_df.copy()\n    \n    # Efficient approach: for June rebalancing, use FY t-1 data\n    # which is available by April of year t (4-month lag)\n    merged['year'] = merged['month_end'].dt.year\n    merged['month'] = merged['month_end'].dt.month\n    \n    # Map: if month >= (lag_months + 1), use current year's FY-1 data\n    # Otherwise, use FY-2 data\n    merged['data_fy'] = np.where(\n        merged['month'] >= lag_months + 1,\n        merged['year'] - 1,\n        merged['year'] - 2\n    )\n    \n    # Merge\n    acc_cols = [c for c in acc.columns if c not in\n                ['filing_date', 'available_date', 'fy_end']]\n    merged = merged.merge(\n        acc[acc_cols].rename(columns={'fiscal_year': 'data_fy'}),\n        on=['ticker', 'data_fy'],\n        how='left'\n    )\n    \n    return merged\n\n# Apply point-in-time merge\npanel = pit_merge_accounting(monthly, accounting, lag_months=4)\n\n# Construct common signals\npanel['log_mcap'] = np.log(panel['market_cap'].clip(lower=1))\n\n# Book-to-market\npanel['bm'] = panel['book_equity'] / panel['market_cap']\npanel.loc[panel['bm'] <= 0, 'bm'] = np.nan  # Negative BE firms\n\n# Gross profitability (Novy-Marx 2013)\npanel['gp_at'] = panel['gross_profit'] / panel['total_assets']\n\n# Operating profitability (Fama-French 2015)\npanel['op'] = panel['operating_profit'] / panel['book_equity']\n\n# Investment (asset growth)\npanel['investment'] = (\n    panel.groupby('ticker')['total_assets']\n    .pct_change(periods=1)\n)\n\n# Leverage\npanel['leverage'] = panel['total_debt'] / panel['total_assets']\n\nprint(\"Signal Coverage:\")\nfor sig in ['bm', 'gp_at', 'op', 'investment', 'leverage']:\n    pct = panel[sig].notna().mean()\n    print(f\"  {sig:<15}: {pct:.1%}\")\n```\n:::\n\n\n### Momentum and Volatility Signals\n\nPrice-based signals require return history, not accounting data, so they have different timing requirements.\n\n::: {#price-signals .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Construct momentum, reversal, and volatility signals\"}\n# Past returns for momentum signals\npanel = panel.sort_values(['ticker', 'month_end'])\n\n# Momentum: cumulative return from month t-12 to t-2 (skip most recent month)\npanel['ret_12_2'] = (\n    panel.groupby('ticker')['monthly_return']\n    .transform(lambda x: x.shift(2).rolling(11).apply(\n        lambda r: (1 + r).prod() - 1, raw=True))\n)\n\n# Short-term reversal: month t-1 return\npanel['ret_1'] = panel.groupby('ticker')['monthly_return'].shift(1)\n\n# Idiosyncratic volatility (from daily data, rolling 60 days)\ndaily['date'] = pd.to_datetime(daily['date'])\ndaily['daily_return'] = daily.groupby('ticker')['adjusted_close'].pct_change()\n\nivol = (\n    daily.groupby('ticker')\n    .apply(lambda g: g.set_index('date')['daily_return']\n           .rolling(60, min_periods=40).std() * np.sqrt(252))\n    .reset_index(name='ivol')\n)\nivol['month_end'] = ivol['date'].dt.to_period('M').dt.to_timestamp('M')\nivol_monthly = (\n    ivol.groupby(['ticker', 'month_end'])['ivol']\n    .last()\n    .reset_index()\n)\n\npanel = panel.merge(ivol_monthly, on=['ticker', 'month_end'], how='left')\n\nprint(\"Price Signal Coverage:\")\nfor sig in ['ret_12_2', 'ret_1', 'ivol']:\n    pct = panel[sig].notna().mean()\n    print(f\"  {sig:<15}: {pct:.1%}\")\n```\n:::\n\n\n## Step 3: Breakpoint Computation {#sec-factor-con-breakpoints}\n\n### The Breakpoint Decision\n\nBreakpoints determine which stocks are \"high\" versus \"low\" on a given characteristic. The two key choices are:\n\n1.  **Breakpoint universe:** Should breakpoints be computed from all stocks or from a subset (e.g., HOSE only)?\n2.  **Number of groups:** 2×3 (Fama-French standard), 5×5 (for finer sorts), or independent terciles/quintiles?\n\n@fama1993common use NYSE breakpoints for U.S. sorts because this prevents the large number of small Nasdaq/AMEX stocks from dominating the breakpoint distribution. The analog in Vietnam is to use HOSE breakpoints, since HOSE lists the larger, more liquid firms and HNX lists smaller firms. Using all-stock breakpoints would place most HOSE stocks in the upper size groups and most HNX stocks in the lower groups, producing mechanically different results.\n\n::: {#breakpoints .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Compare breakpoint choices and their effects on portfolio composition\"}\ndef compute_breakpoints(df, signal_col, n_groups, bp_universe='hose',\n                          exchange_col='exchange'):\n    \"\"\"\n    Compute cross-sectional breakpoints for portfolio sorting.\n    \n    Parameters\n    ----------\n    bp_universe : str\n        'all': use all stocks in universe\n        'hose': use only HOSE stocks (analogous to NYSE breakpoints)\n    n_groups : int\n        Number of groups (2, 3, 5, or 10)\n    \n    Returns\n    -------\n    Series of breakpoints (quantiles)\n    \"\"\"\n    signal = df[signal_col].dropna()\n    \n    if bp_universe == 'hose':\n        mask = df[exchange_col] == 'HOSE'\n        signal = df.loc[mask, signal_col].dropna()\n    \n    quantiles = np.linspace(0, 1, n_groups + 1)[1:-1]\n    breakpoints = signal.quantile(quantiles)\n    \n    return breakpoints\n\n# Example: compare HOSE vs all-stock breakpoints for book-to-market\nexample_month = panel[panel['month_end'] == '2023-06-30'].copy()\nexample_month = example_month[example_month['bm'].notna()]\n\nbp_hose = compute_breakpoints(example_month, 'bm', 3, bp_universe='hose')\nbp_all = compute_breakpoints(example_month, 'bm', 3, bp_universe='all')\n\nprint(\"BM Tercile Breakpoints (June 2023):\")\nprint(f\"  HOSE-only: {bp_hose.values.round(3)}\")\nprint(f\"  All stocks: {bp_all.values.round(3)}\")\nprint(f\"\\n  Difference: HOSE breakpoints are \"\n      f\"{'higher' if bp_hose.values[0] > bp_all.values[0] else 'lower'} \"\n      f\"than all-stock breakpoints\")\n```\n:::\n\n\n::: {#fig-breakpoints .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Visualize breakpoint effects over time\"}\n# Compute breakpoints for every month\nbp_comparison = []\nfor month, group in panel.dropna(subset=['bm']).groupby('month_end'):\n    bp_h = compute_breakpoints(group, 'bm', 3, bp_universe='hose')\n    bp_a = compute_breakpoints(group, 'bm', 3, bp_universe='all')\n    \n    # Count stocks in each tercile under each rule\n    for bp_name, bp_vals in [('HOSE', bp_h), ('All', bp_a)]:\n        low = (group['bm'] <= bp_vals.iloc[0]).sum()\n        mid = ((group['bm'] > bp_vals.iloc[0]) &\n               (group['bm'] <= bp_vals.iloc[1])).sum()\n        high = (group['bm'] > bp_vals.iloc[1]).sum()\n        bp_comparison.append({\n            'month_end': month, 'bp_rule': bp_name,\n            'median_bp': bp_vals.iloc[0],\n            'n_low': low, 'n_mid': mid, 'n_high': high\n        })\n\nbp_df = pd.DataFrame(bp_comparison)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Median breakpoint over time\nfor rule, color in [('HOSE', '#2C5F8A'), ('All', '#C0392B')]:\n    subset = bp_df[bp_df['bp_rule'] == rule]\n    axes[0].plot(subset['month_end'], subset['median_bp'],\n                 color=color, linewidth=1.5, label=f'{rule} breakpoints')\naxes[0].set_ylabel('Lower Tercile Breakpoint (BM)')\naxes[0].set_title('Panel A: Breakpoint Time Series')\naxes[0].legend()\n\n# Panel B: Number of stocks in high BM group\nfor rule, color in [('HOSE', '#2C5F8A'), ('All', '#C0392B')]:\n    subset = bp_df[bp_df['bp_rule'] == rule]\n    axes[1].plot(subset['month_end'], subset['n_high'],\n                 color=color, linewidth=1.5, label=f'{rule} breakpoints')\naxes[1].set_ylabel('Stocks in High BM Group')\naxes[1].set_title('Panel B: High BM Portfolio Size')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Step 4: Portfolio Formation {#sec-factor-con-formation}\n\n### The Generic Factor Engine\n\nWe implement a general-purpose factor construction function that takes any signal column and produces a long-short factor return series. The function encapsulates all methodological choices as parameters, making it easy to test sensitivity.\n\n::: {#factor-engine .cell execution_count=10}\n``` {.python .cell-code code-summary=\"General-purpose factor construction engine\"}\ndef construct_factor(\n    panel_df,\n    signal_col,\n    size_col='market_cap',\n    return_col='monthly_return',\n    date_col='month_end',\n    exchange_col='exchange',\n    formation_month=6,\n    rebalance_freq='annual',\n    n_signal_groups=3,\n    n_size_groups=2,\n    weighting='value',\n    bp_universe='hose',\n    independent_sorts=True,\n    long_group='high',\n    min_stocks_per_portfolio=5,\n    signal_lag=0,\n    universe_filter='standard'\n):\n    \"\"\"\n    Construct a tradeable factor following the Fama-French methodology.\n    \n    Parameters\n    ----------\n    signal_col : str\n        Column with the sorting variable.\n    formation_month : int\n        Month of year for portfolio formation (6 = June for FF).\n    rebalance_freq : str\n        'annual' (FF standard), 'semi', 'quarterly', 'monthly'.\n    n_signal_groups : int\n        Number of signal groups (3 for FF standard, 5 for quintiles).\n    n_size_groups : int\n        Number of size groups (2 for FF standard).\n    weighting : str\n        'value' (VW) or 'equal' (EW).\n    bp_universe : str\n        'hose' or 'all' for breakpoint computation.\n    independent_sorts : bool\n        True for independent double sorts (FF standard).\n    long_group : str\n        'high' or 'low'—which signal group is the long leg.\n    signal_lag : int\n        Additional months to lag the signal beyond the\n        standard point-in-time alignment.\n    \n    Returns\n    -------\n    Dictionary with 'factor_returns', 'portfolio_returns',\n    'diagnostics'.\n    \"\"\"\n    df = panel_df.copy()\n    \n    # Apply universe filter\n    df = apply_universe_filters(df, filters=universe_filter)\n    df = df[df['in_universe']].copy()\n    \n    # Lag the signal if requested\n    if signal_lag > 0:\n        df[signal_col] = (\n            df.groupby('ticker')[signal_col].shift(signal_lag)\n        )\n    \n    # Determine formation dates\n    if rebalance_freq == 'annual':\n        # Form portfolios in formation_month, hold for 12 months\n        df['formation_date'] = df[date_col].apply(\n            lambda d: pd.Timestamp(\n                year=d.year if d.month >= formation_month else d.year - 1,\n                month=formation_month, day=30\n            )\n        )\n    elif rebalance_freq == 'monthly':\n        df['formation_date'] = df[date_col] - pd.DateOffset(months=1)\n    elif rebalance_freq == 'quarterly':\n        df['formation_date'] = df[date_col].apply(\n            lambda d: pd.Timestamp(\n                year=d.year,\n                month=((d.month - 1) // 3) * 3 + 1,\n                day=1\n            ) - pd.DateOffset(days=1)\n        )\n    \n    # Assign signal and size groups at each formation date\n    all_portfolios = []\n    \n    formation_dates = sorted(df['formation_date'].unique())\n    \n    for f_date in formation_dates:\n        # Stocks available at formation\n        formation_data = df[df['formation_date'] == f_date].copy()\n        \n        # Get signal values at formation\n        available = formation_data.dropna(subset=[signal_col, size_col])\n        if len(available) < min_stocks_per_portfolio * n_signal_groups * n_size_groups:\n            continue\n        \n        # Compute breakpoints\n        size_bp = compute_breakpoints(\n            available, size_col, n_size_groups, bp_universe\n        )\n        signal_bp = compute_breakpoints(\n            available, signal_col, n_signal_groups, bp_universe\n        )\n        \n        # Assign groups\n        available['size_group'] = np.searchsorted(\n            size_bp.values, available[size_col].values\n        )\n        available['signal_group'] = np.searchsorted(\n            signal_bp.values, available[signal_col].values\n        )\n        \n        all_portfolios.append(available)\n    \n    if not all_portfolios:\n        return None\n    \n    portfolios = pd.concat(all_portfolios, ignore_index=True)\n    \n    # Compute portfolio returns\n    def weighted_return(group):\n        if weighting == 'value':\n            if group[size_col].sum() > 0:\n                return np.average(group[return_col], weights=group[size_col])\n            else:\n                return group[return_col].mean()\n        else:\n            return group[return_col].mean()\n    \n    port_returns = (\n        portfolios\n        .groupby([date_col, 'size_group', 'signal_group'])\n        .apply(weighted_return)\n        .reset_index(name='port_return')\n    )\n    \n    # Construct factor: average of high-signal portfolios minus\n    # average of low-signal portfolios (across size groups)\n    high_label = n_signal_groups - 1 if long_group == 'high' else 0\n    low_label = 0 if long_group == 'high' else n_signal_groups - 1\n    \n    high_ports = port_returns[port_returns['signal_group'] == high_label]\n    low_ports = port_returns[port_returns['signal_group'] == low_label]\n    \n    high_avg = high_ports.groupby(date_col)['port_return'].mean()\n    low_avg = low_ports.groupby(date_col)['port_return'].mean()\n    \n    factor_returns = (high_avg - low_avg).to_frame('factor_return')\n    \n    # Diagnostics\n    port_counts = (\n        portfolios\n        .groupby([date_col, 'size_group', 'signal_group'])['ticker']\n        .nunique()\n        .reset_index(name='n_stocks')\n    )\n    \n    diagnostics = {\n        'avg_stocks_per_portfolio': port_counts['n_stocks'].mean(),\n        'min_stocks_per_portfolio': port_counts['n_stocks'].min(),\n        'ann_return': factor_returns['factor_return'].mean() * 12,\n        'ann_vol': factor_returns['factor_return'].std() * np.sqrt(12),\n        'sharpe': (factor_returns['factor_return'].mean()\n                   / factor_returns['factor_return'].std() * np.sqrt(12)),\n        't_stat': (factor_returns['factor_return'].mean()\n                   / (factor_returns['factor_return'].std()\n                      / np.sqrt(len(factor_returns)))),\n        'n_months': len(factor_returns)\n    }\n    \n    return {\n        'factor_returns': factor_returns,\n        'portfolio_returns': port_returns,\n        'portfolios': portfolios,\n        'diagnostics': diagnostics\n    }\n```\n:::\n\n\n### Building the Core Factors\n\nWe now use the engine to construct the standard Fama-French factors for Vietnam:\n\n::: {#core-factors .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Construct SMB, HML, RMW, CMA, and WML for Vietnam\"}\n# SMB (Size): small minus big\n# Signal = market cap; long_group = 'low' (small stocks)\nsmb_result = construct_factor(\n    panel, signal_col='log_mcap', long_group='low',\n    formation_month=6, rebalance_freq='annual',\n    n_signal_groups=2, n_size_groups=1,  # No double sort for size itself\n    weighting='value', bp_universe='hose'\n)\n\n# HML (Value): high BM minus low BM\nhml_result = construct_factor(\n    panel, signal_col='bm', long_group='high',\n    formation_month=6, rebalance_freq='annual',\n    n_signal_groups=3, n_size_groups=2,\n    weighting='value', bp_universe='hose'\n)\n\n# RMW (Profitability): robust minus weak\nrmw_result = construct_factor(\n    panel, signal_col='op', long_group='high',\n    formation_month=6, rebalance_freq='annual',\n    n_signal_groups=3, n_size_groups=2,\n    weighting='value', bp_universe='hose'\n)\n\n# CMA (Investment): conservative minus aggressive\ncma_result = construct_factor(\n    panel, signal_col='investment', long_group='low',\n    formation_month=6, rebalance_freq='annual',\n    n_signal_groups=3, n_size_groups=2,\n    weighting='value', bp_universe='hose'\n)\n\n# WML (Momentum): winners minus losers\nwml_result = construct_factor(\n    panel, signal_col='ret_12_2', long_group='high',\n    formation_month=6, rebalance_freq='monthly',\n    n_signal_groups=3, n_size_groups=2,\n    weighting='value', bp_universe='hose'\n)\n\n# Summary table\nprint(\"Vietnamese Factor Summary:\")\nprint(f\"{'Factor':<8} {'Ann. Ret':>10} {'Ann. Vol':>10} {'Sharpe':>8} \"\n      f\"{'t-stat':>8} {'Avg N':>8}\")\nprint(\"-\" * 54)\nfor name, result in [('SMB', smb_result), ('HML', hml_result),\n                       ('RMW', rmw_result), ('CMA', cma_result),\n                       ('WML', wml_result)]:\n    if result is None:\n        continue\n    d = result['diagnostics']\n    print(f\"{name:<8} {d['ann_return']:>10.4f} {d['ann_vol']:>10.4f} \"\n          f\"{d['sharpe']:>8.2f} {d['t_stat']:>8.2f} \"\n          f\"{d['avg_stocks_per_portfolio']:>8.1f}\")\n```\n:::\n\n\n::: {#fig-factor-cumulative .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Plot cumulative factor returns\"}\nfig, ax = plt.subplots(figsize=(14, 6))\n\nfactor_colors = {\n    'SMB': '#2C5F8A', 'HML': '#C0392B', 'RMW': '#27AE60',\n    'CMA': '#E67E22', 'WML': '#8E44AD'\n}\n\nfor name, result in [('SMB', smb_result), ('HML', hml_result),\n                       ('RMW', rmw_result), ('CMA', cma_result),\n                       ('WML', wml_result)]:\n    if result is None:\n        continue\n    fr = result['factor_returns']\n    cum = (1 + fr['factor_return']).cumprod()\n    ax.plot(cum.index, cum.values, color=factor_colors[name],\n            linewidth=2, label=name)\n\nax.axhline(y=1, color='gray', linewidth=0.5)\nax.set_ylabel('Cumulative Return')\nax.set_xlabel('Date')\nax.set_title('Vietnamese Factor Cumulative Returns (2×3 VW, HOSE Breakpoints)')\nax.legend(ncol=5)\nax.set_yscale('log')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Step 5: Sensitivity to Construction Choices {#sec-factor-con-sensitivity}\n\nThe most important lesson in factor construction is that the resulting factor premium is *not* uniquely determined by the economic hypothesis; it depends substantially on implementation choices. We systematically vary each choice and examine how the factor changes.\n\n### Weighting: Value-Weighted vs. Equal-Weighted\n\n::: {#vw-vs-ew .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Compare VW and EW factor construction\"}\nsensitivity_results = {}\n\nfor name, signal, long_grp in [\n    ('HML', 'bm', 'high'), ('RMW', 'op', 'high'), ('WML', 'ret_12_2', 'high')\n]:\n    for wt in ['value', 'equal']:\n        result = construct_factor(\n            panel, signal_col=signal, long_group=long_grp,\n            weighting=wt, bp_universe='hose',\n            rebalance_freq='annual' if name != 'WML' else 'monthly'\n        )\n        if result:\n            d = result['diagnostics']\n            sensitivity_results[f\"{name}_{wt}\"] = {\n                'Factor': name, 'Weighting': wt,\n                'Ann. Return': d['ann_return'],\n                't-stat': d['t_stat']\n            }\n\nsens_df = pd.DataFrame(sensitivity_results).T\nprint(\"VW vs EW Factor Returns:\")\nprint(sens_df.round(3).to_string())\n```\n:::\n\n\n### Breakpoint Universe: HOSE vs. All Stocks\n\n::: {#bp-sensitivity .cell execution_count=14}\n``` {.python .cell-code code-summary=\"Compare HOSE vs all-stock breakpoints\"}\nfor name, signal, long_grp in [\n    ('HML', 'bm', 'high'), ('WML', 'ret_12_2', 'high')\n]:\n    for bp in ['hose', 'all']:\n        result = construct_factor(\n            panel, signal_col=signal, long_group=long_grp,\n            bp_universe=bp, weighting='value',\n            rebalance_freq='annual' if name != 'WML' else 'monthly'\n        )\n        if result:\n            d = result['diagnostics']\n            sensitivity_results[f\"{name}_bp_{bp}\"] = {\n                'Factor': name, 'Breakpoints': bp,\n                'Ann. Return': d['ann_return'],\n                't-stat': d['t_stat'],\n                'Avg N': d['avg_stocks_per_portfolio']\n            }\n\nbp_sens = pd.DataFrame({k: v for k, v in sensitivity_results.items()\n                          if 'bp_' in k}).T\nprint(\"\\nBreakpoint Universe Sensitivity:\")\nprint(bp_sens.round(3).to_string())\n```\n:::\n\n\n### Number of Groups: 2×3 vs. 5×5 vs. Deciles\n\n::: {#group-sensitivity .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Compare different sorting granularities\"}\ngroup_configs = [\n    (2, 3, '2x3 (FF standard)'),\n    (2, 5, '2x5 (quintiles)'),\n    (1, 10, '1x10 (deciles)')\n]\n\ngroup_results = {}\nfor n_size, n_signal, label in group_configs:\n    result = construct_factor(\n        panel, signal_col='bm', long_group='high',\n        n_size_groups=n_size, n_signal_groups=n_signal,\n        weighting='value', bp_universe='hose',\n        rebalance_freq='annual'\n    )\n    if result:\n        d = result['diagnostics']\n        group_results[label] = {\n            'Ann. Return': d['ann_return'],\n            't-stat': d['t_stat'],\n            'Avg N per port': d['avg_stocks_per_portfolio'],\n            'Min N per port': d['min_stocks_per_portfolio']\n        }\n\nprint(\"HML: Sorting Granularity Sensitivity:\")\nprint(pd.DataFrame(group_results).T.round(3).to_string())\n```\n:::\n\n\n### Rebalancing Frequency\n\n::: {#rebalance-sensitivity .cell execution_count=16}\n``` {.python .cell-code code-summary=\"Compare annual vs quarterly vs monthly rebalancing\"}\nrebal_results = {}\nfor freq in ['annual', 'quarterly', 'monthly']:\n    result = construct_factor(\n        panel, signal_col='bm', long_group='high',\n        rebalance_freq=freq, weighting='value', bp_universe='hose'\n    )\n    if result:\n        d = result['diagnostics']\n        rebal_results[freq] = {\n            'Ann. Return': d['ann_return'],\n            'Ann. Vol': d['ann_vol'],\n            't-stat': d['t_stat']\n        }\n\nprint(\"HML: Rebalancing Frequency Sensitivity:\")\nprint(pd.DataFrame(rebal_results).T.round(4).to_string())\n```\n:::\n\n\n### Comprehensive Sensitivity Summary\n\n::: {#fig-sensitivity-heatmap .cell execution_count=17}\n``` {.python .cell-code code-summary=\"Heatmap of factor premium sensitivity\"}\n# Systematic grid search for HML\nconfigs = list(product(\n    ['value', 'equal'],        # Weighting\n    ['hose', 'all'],           # Breakpoint universe\n    [(2, 3), (2, 5), (1, 10)]  # (n_size, n_signal)\n))\n\ngrid_results = []\nfor wt, bp, (ns, nsig) in configs:\n    result = construct_factor(\n        panel, signal_col='bm', long_group='high',\n        weighting=wt, bp_universe=bp,\n        n_size_groups=ns, n_signal_groups=nsig,\n        rebalance_freq='annual'\n    )\n    if result:\n        d = result['diagnostics']\n        grid_results.append({\n            'Weighting': 'VW' if wt == 'value' else 'EW',\n            'Breakpoints': bp.upper(),\n            'Sort': f'{ns}x{nsig}',\n            'Ann. Return': d['ann_return'],\n            't-stat': d['t_stat']\n        })\n\ngrid_df = pd.DataFrame(grid_results)\nprint(\"HML Factor: Full Sensitivity Grid:\")\nprint(grid_df.to_string(index=False))\n\n# Pivot for heatmap\npivot = grid_df.pivot_table(\n    values='Ann. Return', index=['Weighting', 'Breakpoints'],\n    columns='Sort'\n)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.heatmap(pivot * 100, annot=True, fmt='.1f', cmap='RdYlGn',\n            center=0, linewidths=0.5, ax=ax,\n            cbar_kws={'label': 'Ann. Return (%)'})\nax.set_title('HML Premium: Sensitivity to Construction Choices')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Factor Correlation Structure {#sec-factor-con-correlation}\n\n::: {#fig-factor-correlations .cell execution_count=18}\n``` {.python .cell-code code-summary=\"Compute and plot factor return correlation matrix\"}\n# Merge all factor return series\nfactor_panel = pd.DataFrame()\nfor name, result in [('SMB', smb_result), ('HML', hml_result),\n                       ('RMW', rmw_result), ('CMA', cma_result),\n                       ('WML', wml_result)]:\n    if result is None:\n        continue\n    fr = result['factor_returns'].rename(columns={'factor_return': name})\n    if factor_panel.empty:\n        factor_panel = fr\n    else:\n        factor_panel = factor_panel.merge(fr, left_index=True,\n                                           right_index=True, how='outer')\n\ncorr = factor_panel.corr()\n\nfig, ax = plt.subplots(figsize=(7, 6))\nmask = np.triu(np.ones_like(corr, dtype=bool), k=1)\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n            center=0, vmin=-1, vmax=1, square=True,\n            linewidths=0.5, ax=ax)\nax.set_title('Factor Return Correlations')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Factor Validation {#sec-factor-con-validation}\n\nA well-constructed factor should pass several diagnostic tests before being used in asset pricing research.\n\n### Diagnostic Checklist\n\n::: {#validation .cell execution_count=19}\n``` {.python .cell-code code-summary=\"Run diagnostic tests on each factor\"}\ndef validate_factor(factor_result, name):\n    \"\"\"\n    Run standard diagnostic tests on a constructed factor.\n    \"\"\"\n    fr = factor_result['factor_returns']['factor_return']\n    diag = factor_result['diagnostics']\n    ports = factor_result['portfolios']\n    \n    tests = {}\n    \n    # 1. Statistical significance (t > 2)\n    tests['t-stat'] = diag['t_stat']\n    tests['t > 2'] = abs(diag['t_stat']) > 2.0\n    \n    # 2. Economic magnitude\n    tests['Ann. Return'] = diag['ann_return']\n    tests['Ann. Vol'] = diag['ann_vol']\n    tests['Sharpe'] = diag['sharpe']\n    \n    # 3. Adequate portfolio diversification\n    tests['Avg N per portfolio'] = diag['avg_stocks_per_portfolio']\n    tests['Min N per portfolio'] = diag['min_stocks_per_portfolio']\n    tests['Min N >= 5'] = diag['min_stocks_per_portfolio'] >= 5\n    \n    # 4. Not dominated by a single month\n    tests['Max monthly return'] = fr.max()\n    tests['Min monthly return'] = fr.min()\n    tests['Fraction > 0'] = (fr > 0).mean()\n    \n    # 5. Persistence (ACF at lag 1)\n    if len(fr) > 12:\n        tests['ACF(1)'] = fr.autocorr(lag=1)\n    \n    # 6. Consistency across subperiods\n    mid = len(fr) // 2\n    first_half = fr.iloc[:mid]\n    second_half = fr.iloc[mid:]\n    tests['Return (1st half)'] = first_half.mean() * 12\n    tests['Return (2nd half)'] = second_half.mean() * 12\n    tests['Same sign both halves'] = (\n        np.sign(first_half.mean()) == np.sign(second_half.mean())\n    )\n    \n    return tests\n\nprint(\"Factor Validation Summary:\")\nprint(\"=\" * 70)\nfor name, result in [('SMB', smb_result), ('HML', hml_result),\n                       ('RMW', rmw_result), ('CMA', cma_result),\n                       ('WML', wml_result)]:\n    if result is None:\n        continue\n    tests = validate_factor(result, name)\n    print(f\"\\n{name}:\")\n    for test, value in tests.items():\n        if isinstance(value, bool):\n            status = 'PASS' if value else 'FAIL'\n            print(f\"  {test:<30}: {status}\")\n        elif isinstance(value, float):\n            print(f\"  {test:<30}: {value:.4f}\")\n        else:\n            print(f\"  {test:<30}: {value}\")\n```\n:::\n\n\n### Monotonicity Test\n\nA factor built from a characteristic sort should produce monotonically increasing (or decreasing) average returns across quantiles. Violations of monotonicity suggest the signal-return relationship is nonlinear or absent.\n\n::: {#fig-monotonicity .cell execution_count=20}\n``` {.python .cell-code code-summary=\"Test monotonicity of returns across signal deciles\"}\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\naxes = axes.flatten()\n\nsignals = [\n    ('bm', 'Book-to-Market', 'high'),\n    ('op', 'Operating Profit.', 'high'),\n    ('investment', 'Investment', 'low'),\n    ('ret_12_2', 'Momentum (12-2)', 'high'),\n    ('ivol', 'Idio. Volatility', 'low'),\n]\n\nfor i, (sig, label, long_grp) in enumerate(signals):\n    result = construct_factor(\n        panel, signal_col=sig, long_group=long_grp,\n        n_size_groups=1, n_signal_groups=10,\n        weighting='value', bp_universe='hose',\n        rebalance_freq='annual' if sig not in ['ret_12_2'] else 'monthly'\n    )\n    if result is None:\n        continue\n    \n    port_ret = result['portfolio_returns']\n    decile_means = (\n        port_ret.groupby('signal_group')['port_return']\n        .mean() * 12 * 100\n    )\n    \n    colors_mono = plt.cm.RdYlGn_r(np.linspace(0.1, 0.9, len(decile_means)))\n    axes[i].bar(range(len(decile_means)), decile_means.values,\n                color=colors_mono, edgecolor='white')\n    axes[i].set_xticks(range(len(decile_means)))\n    axes[i].set_xticklabels([f'D{d+1}' for d in range(len(decile_means))],\n                             fontsize=8)\n    axes[i].set_ylabel('Ann. Return (%)')\n    axes[i].set_title(label)\n    axes[i].axhline(y=0, color='gray', linewidth=0.5)\n\naxes[5].set_visible(False)\nplt.suptitle('Decile Portfolio Returns by Signal', fontsize=14)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Spanning Tests\n\nDoes a new factor add information beyond existing factors? We test this by regressing each factor on all other factors and examining the intercept (alpha). A significant alpha means the factor captures return variation not spanned by the others:\n\n::: {#spanning .cell execution_count=21}\n``` {.python .cell-code code-summary=\"Run spanning tests: regress each factor on remaining factors\"}\nfactor_names = [c for c in factor_panel.columns if factor_panel[c].notna().sum() > 24]\nspanning_data = factor_panel[factor_names].dropna()\n\nprint(\"Spanning Tests (alpha = intercept when regressed on other factors):\")\nprint(f\"{'Factor':<8} {'Alpha (ann.)':>12} {'t-stat':>8} {'R²':>6}\")\nprint(\"-\" * 36)\n\nfor target in factor_names:\n    others = [f for f in factor_names if f != target]\n    y = spanning_data[target]\n    X = sm.add_constant(spanning_data[others])\n    model = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n    \n    alpha_ann = model.params['const'] * 12\n    alpha_t = model.tvalues['const']\n    r2 = model.rsquared\n    \n    print(f\"{target:<8} {alpha_ann:>12.4f} {alpha_t:>8.2f} {r2:>6.3f}\")\n```\n:::\n\n\n## Vietnamese-Specific Considerations {#sec-factor-con-vietnam}\n\n### State-Owned Enterprise Classification\n\nA unique feature of Vietnamese equities is the high proportion of state-owned enterprises (SOEs). The state retains majority or significant minority stakes in many listed firms, which affects governance, information environment, and trading dynamics. Factors may behave differently within SOE and non-SOE subsamples:\n\n::: {#soe-split .cell execution_count=22}\n``` {.python .cell-code code-summary=\"Compare factor premia within SOE and non-SOE subsamples\"}\n# Get SOE classification\nfirm_info = client.get_firm_info(\n    exchanges=['HOSE', 'HNX'],\n    fields=['ticker', 'state_ownership_pct', 'is_soe']\n)\n\npanel_soe = panel.merge(firm_info[['ticker', 'is_soe']], on='ticker', how='left')\n\nfor label, subset in [('SOE', panel_soe[panel_soe['is_soe'] == True]),\n                        ('Non-SOE', panel_soe[panel_soe['is_soe'] == False])]:\n    hml = construct_factor(\n        subset, signal_col='bm', long_group='high',\n        weighting='value', bp_universe='all',\n        rebalance_freq='annual'\n    )\n    if hml:\n        d = hml['diagnostics']\n        print(f\"HML ({label}): Ann = {d['ann_return']:.4f}, \"\n              f\"t = {d['t_stat']:.2f}, N = {d['avg_stocks_per_portfolio']:.0f}\")\n```\n:::\n\n\n### Foreign Ownership Limits\n\nForeign ownership caps (49% for most sectors, 30% for banking) affect the investable universe for international investors. Factors constructed from the full universe may not be achievable by foreign investors if the long leg concentrates in stocks at the foreign ownership limit:\n\n::: {#foreign-ownership .cell execution_count=23}\n``` {.python .cell-code code-summary=\"Check foreign ownership proximity in factor portfolios\"}\nfol = client.get_foreign_ownership(\n    exchanges=['HOSE', 'HNX'],\n    fields=['ticker', 'month_end', 'foreign_pct', 'foreign_limit_pct']\n)\n\n# Merge with HML portfolios\nif hml_result:\n    hml_ports = hml_result['portfolios'].merge(\n        fol, on=['ticker', 'month_end'], how='left'\n    )\n    hml_ports['near_limit'] = (\n        (hml_ports['foreign_limit_pct'] - hml_ports['foreign_pct']) < 5\n    )\n    \n    fol_by_group = (\n        hml_ports.groupby('signal_group')\n        .agg(\n            avg_foreign_pct=('foreign_pct', 'mean'),\n            pct_near_limit=('near_limit', 'mean')\n        )\n    )\n    print(\"Foreign Ownership in HML Portfolios:\")\n    print(fol_by_group.round(3))\n```\n:::\n\n\n## Recommended Factor Specifications for Vietnam {#sec-factor-con-recommendations}\n\nBased on the sensitivity analysis, we recommend the following baseline specifications (@tbl-factor-con-recommendations).\n\n| Choice | Recommendation | Rationale |\n|------------------------|------------------------|------------------------|\n| Universe | Standard filter (listing age ≥ 6m, volume \\> 0) | Excludes shell firms without losing too much breadth |\n| Breakpoints | HOSE stocks only | Prevents HNX micro-caps from dominating breakpoints |\n| Size groups | 2 (median split) | Sufficient control with limited cross-section |\n| Signal groups | 3 (terciles) for factor construction; 5 or 10 for portfolio analysis | 2×3 = adequate diversification per portfolio |\n| Weighting | Value-weighted | Investable; less noisy than EW |\n| Rebalancing | Annual (June) for accounting signals; monthly for momentum | Standard; consistent with Fama-French |\n| Accounting lag | 4 months (available by April for Dec FY) | Conservative PIT alignment |\n| Minimum stocks | ≥ 5 per portfolio per month | Below this, single-stock idiosyncratic risk dominates |\n\n: Recommended factor construction parameters for Vietnamese equities. {#tbl-factor-con-recommendations}\n\nAlways report results under the baseline and at least one alternative specification (e.g., EW, all-stock breakpoints) to demonstrate robustness.\n\n## Summary {#sec-factor-con-summary}\n\nThis chapter has developed a modular, transparent factor construction framework for Vietnamese equities. The key insights are:\n\n-   The @fama1993common 2×3 methodology translates well to Vietnam with one critical adaptation: breakpoints should be computed from HOSE stocks only. Using all-stock breakpoints allows HNX micro-caps to dominate the small-stock groups, inflating apparent premia with economically untradeable returns.\n\n-   Value weighting produces more conservative (and more implementable) factor premia than equal weighting. The difference is particularly large for signals correlated with size (BM, investment), where EW overweights the smallest stocks that contribute most to the premium but least to investable returns.\n\n-   No single construction choice determines whether a factor \"exists.\" A factor premium that appears only under one specific combination of breakpoints, weighting, and rebalancing frequency is fragile and should be treated with skepticism. Robust factors survive a grid of specifications. The sensitivity analysis framework developed here (e.g., varying weighting, breakpoints, sort granularity, and rebalancing simultaneously) should be standard practice.\n\nThe `construct_factor()` function developed in this chapter is designed for reuse throughout the book. Any anomaly variable can be fed through the same pipeline, producing a tradeable factor with full diagnostics. This ensures methodological consistency across chapters and makes it easy to compare premia on an apples-to-apples basis.\n\n<!-- ## Exercises {#sec-factor-con-exercises}\n\n1.  **Conditional sorts.** Implement conditional (sequential) double sorts as an alternative to the independent sorts used by @fama1993common. First sort on size, then within each size group sort on BM. Compare the resulting HML to the independent-sort HML. When do conditional sorts produce meaningfully different results?\n\n2.  **Winsorization sensitivity.** Construct HML with and without winsorizing the book-to-market ratio at the 1st and 99th percentiles each month. How sensitive is the factor premium to extreme BM values? Investigate whether the extreme BM firms are concentrated in specific industries.\n\n3.  **Lagged accounting data.** Reconstruct all five factors using a 6-month accounting lag instead of 4 months. How much does the additional lag reduce the premium? This tests how much of the apparent premium comes from stale data that was not yet publicly available.\n\n4.  **Industry-neutral factors.** Construct an industry-neutral HML by sorting on BM within each ICB sector (rather than across the entire market). Does the within-industry value premium differ from the cross-industry premium? Which component is larger in Vietnam?\n\n5.  **Liquidity-controlled factors.** Add a third independent sort on Amihud illiquidity (2×3×3 = 18 portfolios). Does the HML premium survive after controlling for liquidity? This tests whether the apparent value premium is actually a liquidity premium in disguise.\n\n6.  **Small-sample bootstrap.** The Vietnamese cross-section is small enough that portfolio sorts may produce noisy estimates. Implement a bootstrap procedure: resample the cross-section with replacement 1,000 times at each formation date, reconstruct the factor each time, and report the 90% confidence interval for the annualized premium. How wide is the interval?\n\n7.  **Factor timing.** Test whether factor premia are predictable using lagged aggregate variables (market volatility, credit spread, aggregate turnover, SBV policy rate). Estimate a predictive regression of next-month factor returns on these variables. Can you build a factor timing strategy that improves on the unconditional buy-and-hold factor?\n\n8.  **Replication of @hou2020replicating for Vietnam.** Select 10 well-known anomalies from the international literature (e.g., gross profitability, asset growth, accruals, share issuance, ROE, beta, idiosyncratic volatility, earnings momentum, investment-to-assets, net share issues). Construct each as a long-short factor using the engine from this chapter. How many survive with t \\> 2.0? How does this replication rate compare to the \\~50% rate reported by @hou2020replicating for U.S. data? -->\n\n",
    "supporting": [
      "23_factor_construction_principles_files/figure-pdf"
    ],
    "filters": []
  }
}