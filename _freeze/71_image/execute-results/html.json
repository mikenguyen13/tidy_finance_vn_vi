{
  "hash": "50b212b277fb19b9cdfe248336373c33",
  "result": {
    "engine": "jupyter",
    "markdown": "# Image and Visual Data in Finance\n\nThe previous chapter demonstrated how unstructured text (e.g., earnings reports, news articles, business descriptions) can be transformed into structured signals for financial analysis. This chapter extends the alternative data toolkit to a second modality: images. Visual data is abundant in financial contexts yet systematically underexploited. Satellite photographs reveal real economic activity (e.g., parking lot occupancy at retail locations, construction progress at industrial sites, nighttime luminosity as a proxy for regional GDP, ship traffic at port terminals, crop health across agricultural zones). Corporate documents arrive as scanned PDFs whose tables and figures resist standard text extraction. Financial charts encode information that analysts interpret visually but that systematic strategies cannot consume without digitization. And the visual content of social media, advertising, and product imagery carries sentiment and brand signals that complement textual analysis.\n\nThe core challenge is representational: an image is a three-dimensional tensor of pixel intensities with no inherent semantic structure. Converting this raw array into a financial signal (i.e., a number that predicts returns, measures risk, or proxies for economic activity) requires either hand-crafted feature engineering or learned representations via deep convolutional neural networks (CNNs) and vision transformers (ViTs). This chapter covers both approaches.\n\nWe organize the material around five application domains, each with distinct data sources, modeling requirements, and economic motivations. First, satellite and geospatial imagery for nowcasting economic activity. Second, document image analysis for extracting structured data from Vietnamese financial filings. Third, chart and figure digitization for systematic backtesting. Fourth, visual sentiment analysis from social and news media. Fifth, multimodal fusion, combining image and text signals into joint predictive models.\n\nVietnamese markets present particular opportunities in this space. Satellite imagery is especially informative in an economy with large agricultural and manufacturing sectors where ground-truth data arrives with significant lags. Vietnamese financial filings are often distributed as scanned images rather than machine-readable formats, making document AI essential rather than optional. And the rapid urbanization visible in construction and infrastructure imagery provides high-frequency proxies for macroeconomic momentum that official statistics cannot match.\n\n::: {#setup .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Core image processing\nfrom PIL import Image\nimport io\n\n# Deep learning\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\n# Visualization\nimport plotnine as p9\nfrom mizani.formatters import percent_format, comma_format\nimport matplotlib.pyplot as plt\n\n# Statistical analysis\nfrom scipy import stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom linearmodels.panel import PanelOLS\n```\n:::\n\n\n## Foundations: From Pixels to Financial Signals\n\n### Image Representation\n\nA digital image is a function $I: \\{1, \\ldots, H\\} \\times \\{1, \\ldots, W\\} \\times \\{1, \\ldots, C\\} \\rightarrow [0, 255]$ mapping spatial coordinates and color channels to intensity values. For an RGB image of height $H$ and width $W$, the representation is a tensor $\\mathbf{I} \\in \\mathbb{R}^{H \\times W \\times 3}$. A single $224 \\times 224$ RGB image (i.e., the standard input for modern CNNs) contains $224 \\times 224 \\times 3 = 150{,}528$ dimensions. This extreme dimensionality, combined with spatial structure (nearby pixels are correlated), makes images fundamentally different from tabular financial data and demands specialized architectures.\n\nThe key insight of convolutional neural networks is parameter sharing via local filters. A convolutional layer applies a kernel $\\mathbf{K} \\in \\mathbb{R}^{k \\times k \\times C_{\\text{in}}}$ to produce a feature map:\n\n$$\n(\\mathbf{I} * \\mathbf{K})(i, j) = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} \\sum_{c=1}^{C_{\\text{in}}} I(i+m, j+n, c) \\cdot K(m, n, c)\n$$ {#eq-convolution}\n\nBy stacking convolutional layers with nonlinearities and pooling operations, the network builds a hierarchy of representations: early layers detect edges and textures; middle layers detect parts and patterns; deep layers detect objects and scenes. The final layer output $\\mathbf{z} \\in \\mathbb{R}^{d}$ (with $d$ typically 512-2048) is a compact representation of the image's semantic content, which can be used directly as a feature vector for financial prediction.\n\n### Transfer Learning for Finance\n\nTraining a CNN from scratch requires millions of labeled images, which is far more than any financial application can provide. Transfer learning solves this by using networks pre-trained on ImageNet (1.2 million images, 1,000 classes) as feature extractors. The pre-trained network has already learned generic visual representations (edges, textures, shapes, objects); we simply replace the final classification layer with a task-specific head.\n\nFormally, let $f_{\\boldsymbol{\\theta}}(\\mathbf{I})$ denote a pre-trained network with parameters $\\boldsymbol{\\theta}$ partitioned into feature extractor $\\boldsymbol{\\theta}_{\\text{feat}}$ and classifier $\\boldsymbol{\\theta}_{\\text{cls}}$. For financial applications, we:\n\n1.  **Feature extraction**: Freeze $\\boldsymbol{\\theta}_{\\text{feat}}$, extract $\\mathbf{z} = f_{\\boldsymbol{\\theta}_{\\text{feat}}}(\\mathbf{I})$, and train a simple model (linear regression, gradient boosting) on $\\mathbf{z}$.\n2.  **Fine-tuning**: Initialize from $\\boldsymbol{\\theta}$ and train all parameters on the financial task with a small learning rate to avoid catastrophic forgetting.\n\n::: {#feature-extractor .cell execution_count=2}\n``` {.python .cell-code}\n# DataCore.vn API\nfrom datacore import DataCore\ndc = DataCore()\n\ndef build_feature_extractor(model_name=\"resnet50\", device=\"cpu\"):\n    \"\"\"\n    Build a pre-trained CNN feature extractor.\n\n    Parameters\n    ----------\n    model_name : str\n        One of 'resnet50', 'efficientnet_b0', 'vit_b_16'.\n    device : str\n        'cpu' or 'cuda'.\n\n    Returns\n    -------\n    model : nn.Module\n        Feature extraction model.\n    transform : transforms.Compose\n        Image preprocessing pipeline.\n    \"\"\"\n    if model_name == \"resnet50\":\n        weights = models.ResNet50_Weights.IMAGENET1K_V2\n        model = models.resnet50(weights=weights)\n        model.fc = nn.Identity()  # Remove classification head\n        dim = 2048\n    elif model_name == \"efficientnet_b0\":\n        weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\n        model = models.efficientnet_b0(weights=weights)\n        model.classifier = nn.Identity()\n        dim = 1280\n    elif model_name == \"vit_b_16\":\n        weights = models.ViT_B_16_Weights.IMAGENET1K_V1\n        model = models.vit_b_16(weights=weights)\n        model.heads = nn.Identity()\n        dim = 768\n\n    model = model.to(device).eval()\n\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n\n    return model, transform, dim\n\n\ndef extract_features(image_paths, model, transform, device=\"cpu\",\n                     batch_size=32):\n    \"\"\"\n    Extract deep features from a list of images.\n\n    Parameters\n    ----------\n    image_paths : list\n        Paths to image files.\n    model : nn.Module\n        Feature extraction model.\n    transform : transforms.Compose\n        Preprocessing pipeline.\n\n    Returns\n    -------\n    np.ndarray : Feature matrix (n_images x feature_dim).\n    \"\"\"\n    features = []\n\n    for i in range(0, len(image_paths), batch_size):\n        batch_paths = image_paths[i:i + batch_size]\n        batch_tensors = []\n\n        for path in batch_paths:\n            try:\n                img = Image.open(path).convert(\"RGB\")\n                tensor = transform(img)\n                batch_tensors.append(tensor)\n            except Exception:\n                batch_tensors.append(torch.zeros(3, 224, 224))\n\n        batch = torch.stack(batch_tensors).to(device)\n\n        with torch.no_grad():\n            batch_features = model(batch).cpu().numpy()\n\n        features.append(batch_features)\n\n    return np.vstack(features)\n```\n:::\n\n\n## Satellite and Geospatial Imagery\n\n### Economic Activity from Space\n\nSatellite imagery provides high-frequency, spatially granular measurements of economic activity that are independent of and often lead official statistics. The foundational work of @henderson2012measuring demonstrated that nighttime luminosity, measured by the Defense Meteorological Satellite Program (DMSP), is a reliable proxy for GDP, particularly in countries where official statistics are noisy or delayed. @donaldson2018railroads use satellite-derived agricultural output measures to study the welfare gains from railroads in colonial India. @jean2016combining combine daytime satellite imagery with CNNs to predict poverty from space with $r^2 > 0.7$.\n\nFor financial applications, the key insight is that satellite data arrives faster than corporate earnings or government statistics. A retailer's quarterly revenue is reported 4-8 weeks after the quarter ends; satellite imagery of its parking lots is available within days. This temporal advantage creates a natural use case for nowcasting (i.e., estimating current economic conditions before official data arrives) and for constructing trading signals based on information that is public but costly to process.\n\n### Application 1: Nighttime Luminosity and Provincial GDP\n\nVietnam's General Statistics Office (GSO) publishes provincial GDP with a lag of several months. Nighttime luminosity from the VIIRS (Visible Infrared Imaging Radiometer Suite) sensor provides a near-real-time alternative. We construct a firm-level exposure measure by linking each listed firm's registered location to the luminosity of its province.\n\n::: {#nightlight-data .cell execution_count=3}\n``` {.python .cell-code}\n# Load nighttime luminosity data (VIIRS monthly composites)\n# Source: Earth Observation Group (EOG) / NOAA\nnightlights = dc.get_nightlight_data(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    resolution=\"province\"\n)\n\n# Load firm location data\nfirm_locations = dc.get_firm_locations()\n\n# Provincial GDP from GSO\nprovincial_gdp = dc.get_provincial_gdp(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\n\nprint(f\"Nightlight observations: {len(nightlights)}\")\nprint(f\"Provinces covered: {nightlights['province'].nunique()}\")\nprint(f\"Firms with location: {firm_locations['ticker'].nunique()}\")\n```\n:::\n\n\n::: {#nightlight-gdp-validation .cell execution_count=4}\n``` {.python .cell-code}\n# Validate: does nightlight predict provincial GDP?\nnl_gdp = nightlights.merge(\n    provincial_gdp,\n    on=[\"province\", \"year\", \"quarter\"],\n    how=\"inner\"\n)\n\n# Log-log specification (standard in the literature)\nnl_gdp[\"ln_luminosity\"] = np.log(nl_gdp[\"mean_radiance\"].clip(lower=0.01))\nnl_gdp[\"ln_gdp\"] = np.log(nl_gdp[\"provincial_gdp\"].clip(lower=1))\n\n# Cross-sectional regression by year\nvalidation_results = []\nfor year in nl_gdp[\"year\"].unique():\n    subset = nl_gdp[nl_gdp[\"year\"] == year]\n    if len(subset) < 20:\n        continue\n\n    model = sm.OLS(\n        subset[\"ln_gdp\"],\n        sm.add_constant(subset[\"ln_luminosity\"])\n    ).fit()\n\n    validation_results.append({\n        \"year\": year,\n        \"beta\": model.params.iloc[1],\n        \"r_squared\": model.rsquared,\n        \"n_provinces\": int(model.nobs)\n    })\n\nvalidation_df = pd.DataFrame(validation_results)\nprint(f\"Avg R² (ln GDP ~ ln Luminosity): {validation_df['r_squared'].mean():.3f}\")\n```\n:::\n\n\n::: {#fig-nightlight-gdp .cell execution_count=5}\n``` {.python .cell-code}\n(\n    p9.ggplot(nl_gdp[nl_gdp[\"year\"] == 2023],\n              p9.aes(x=\"ln_luminosity\", y=\"ln_gdp\"))\n    + p9.geom_point(color=\"#2E5090\", alpha=0.6, size=2)\n    + p9.geom_smooth(method=\"lm\", color=\"#C0392B\", se=True, size=0.8)\n    + p9.labs(\n        x=\"ln(Mean Nighttime Radiance)\",\n        y=\"ln(Provincial GDP)\",\n        title=\"Nighttime Luminosity vs. Provincial GDP (2023)\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 6))\n)\n```\n:::\n\n\n::: {#nightlight-firm-signal .cell execution_count=6}\n``` {.python .cell-code}\n# Construct firm-level nightlight signal\n# Logic: firms in provinces with accelerating luminosity\n# are experiencing positive local economic conditions\n\nnl_growth = nightlights.copy().sort_values([\"province\", \"year\", \"quarter\"])\n\n# Year-over-year luminosity growth by province\nnl_growth[\"ln_radiance\"] = np.log(nl_growth[\"mean_radiance\"].clip(lower=0.01))\nnl_growth[\"ln_radiance_lag4\"] = nl_growth.groupby(\n    \"province\"\n)[\"ln_radiance\"].shift(4)\n\nnl_growth[\"nl_growth\"] = nl_growth[\"ln_radiance\"] - nl_growth[\"ln_radiance_lag4\"]\n\n# Merge with firms via province\nfirm_nl = firm_locations[[\"ticker\", \"province\"]].merge(\n    nl_growth[[\"province\", \"year\", \"quarter\", \"nl_growth\"]],\n    on=\"province\",\n    how=\"inner\"\n)\n\n# Merge with stock returns\nmonthly_returns = dc.get_monthly_returns(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\nmonthly_returns[\"year\"] = monthly_returns[\"date\"].dt.year\nmonthly_returns[\"quarter\"] = monthly_returns[\"date\"].dt.quarter\n\nreturns_with_nl = monthly_returns.merge(\n    firm_nl,\n    on=[\"ticker\", \"year\", \"quarter\"],\n    how=\"inner\"\n)\n```\n:::\n\n\n::: {#nightlight-portfolio .cell execution_count=7}\n``` {.python .cell-code}\n# Portfolio sort: quintiles on provincial nightlight growth\nreturns_with_nl[\"nl_quintile\"] = returns_with_nl.groupby(\"date\")[\n    \"nl_growth\"\n].transform(lambda x: pd.qcut(x, 5, labels=[1, 2, 3, 4, 5],\n                                duplicates=\"drop\"))\n\nnl_port_returns = (\n    returns_with_nl.groupby([\"date\", \"nl_quintile\"])\n    .agg(port_ret=(\"ret\", \"mean\"))\n    .reset_index()\n)\n\nnl_wide = nl_port_returns.pivot(\n    index=\"date\", columns=\"nl_quintile\", values=\"port_ret\"\n)\nnl_wide[\"L-S\"] = nl_wide[5] - nl_wide[1]  # High NL growth - Low\n```\n:::\n\n\n::: {#tbl-nightlight-portfolios .cell tbl-cap='Nighttime Luminosity Growth Quintile Portfolio Returns' execution_count=8}\n``` {.python .cell-code}\nnl_summary = nl_wide.describe().T[[\"mean\", \"std\"]].copy()\nnl_summary[\"mean_ann\"] = nl_summary[\"mean\"] * 12\nnl_summary[\"sharpe\"] = (\n    nl_summary[\"mean_ann\"] / (nl_summary[\"std\"] * np.sqrt(12))\n)\nfor col in nl_wide.columns:\n    t_stat = nl_wide[col].mean() / (\n        nl_wide[col].std() / np.sqrt(len(nl_wide.dropna()))\n    )\n    nl_summary.loc[col, \"t_stat\"] = t_stat\n\nnl_summary = nl_summary[[\"mean_ann\", \"sharpe\", \"t_stat\"]].round(4)\nnl_summary.columns = [\"Ann. Return\", \"Sharpe\", \"t-stat\"]\nnl_summary\n```\n:::\n\n\n### Application 2: Satellite Imagery for Sector Nowcasting\n\nBeyond luminosity, daytime satellite imagery provides sector-specific signals. We implement three channels relevant to the Vietnamese economy.\n\n**Port activity.** Vietnam is a major export-oriented economy. Satellite imagery of container ports (Cát Lái, Hải Phòng) captures trade throughput before customs statistics are released. Ship detection algorithms applied to synthetic aperture radar (SAR) imagery count vessels and estimate cargo volumes.\n\n**Construction progress.** Real estate and construction constitute a significant fraction of Vietnamese GDP and market capitalization. Change detection algorithms applied to high-resolution optical imagery identify construction starts, completion rates, and land-use conversion.\n\n**Agricultural monitoring.** Vietnam is a leading exporter of rice, coffee, rubber, and seafood. The Normalized Difference Vegetation Index (NDVI), computed from multispectral satellite data, provides crop health assessments:\n\n$$\n\\text{NDVI} = \\frac{\\rho_{\\text{NIR}} - \\rho_{\\text{Red}}}{\\rho_{\\text{NIR}} + \\rho_{\\text{Red}}}\n$$ {#eq-ndvi}\n\nwhere $\\rho_{\\text{NIR}}$ and $\\rho_{\\text{Red}}$ are reflectance in the near-infrared and red bands. NDVI ranges from $-1$ to $+1$, with values above 0.3 indicating healthy vegetation. Deviations from seasonal norms proxy for crop yield surprises.\n\n::: {#ndvi-agriculture .cell execution_count=9}\n``` {.python .cell-code}\n# Load NDVI data for Vietnamese agricultural regions\n# Source: MODIS/Terra (MOD13Q1, 250m resolution, 16-day composites)\nndvi_data = dc.get_ndvi_data(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    regions=[\"mekong_delta\", \"central_highlands\",\n             \"red_river_delta\", \"southeast\"]\n)\n\n# Compute NDVI anomaly: deviation from 5-year seasonal average\nndvi_data[\"month\"] = ndvi_data[\"date\"].dt.month\n\nseasonal_mean = (\n    ndvi_data.groupby([\"region\", \"month\"])\n    [\"mean_ndvi\"].transform(\n        lambda x: x.rolling(5 * 12, min_periods=12).mean()\n    )\n)\nndvi_data[\"ndvi_anomaly\"] = ndvi_data[\"mean_ndvi\"] - seasonal_mean\n\n# Agricultural sector firms\nagri_firms = dc.get_firms_by_sector(sector=\"agriculture\")\n\n# Link NDVI anomaly to agricultural firm returns\nagri_returns = monthly_returns[\n    monthly_returns[\"ticker\"].isin(agri_firms[\"ticker\"])\n].copy()\n\nagri_returns[\"month\"] = agri_returns[\"date\"].dt.month\nagri_returns[\"year\"] = agri_returns[\"date\"].dt.year\n\n# Regional NDVI aggregation (Mekong Delta for rice firms, etc.)\nmekong_ndvi = ndvi_data[ndvi_data[\"region\"] == \"mekong_delta\"].copy()\nmekong_ndvi[\"year\"] = mekong_ndvi[\"date\"].dt.year\nmekong_ndvi[\"month\"] = mekong_ndvi[\"date\"].dt.month\n\nmekong_monthly = (\n    mekong_ndvi.groupby([\"year\", \"month\"])\n    .agg(ndvi_anomaly=(\"ndvi_anomaly\", \"mean\"))\n    .reset_index()\n)\n```\n:::\n\n\n::: {#fig-ndvi-timeseries .cell execution_count=10}\n``` {.python .cell-code}\nndvi_plot = ndvi_data[ndvi_data[\"region\"] == \"mekong_delta\"].copy()\n\n(\n    p9.ggplot(ndvi_plot, p9.aes(x=\"date\", y=\"ndvi_anomaly\"))\n    + p9.geom_line(color=\"#27AE60\", alpha=0.5, size=0.4)\n    + p9.geom_smooth(method=\"lowess\", color=\"#2E5090\", size=1, se=False)\n    + p9.geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\")\n    + p9.labs(\n        x=\"\",\n        y=\"NDVI Anomaly\",\n        title=\"Mekong Delta Vegetation Health: Deviation from Seasonal Norm\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5))\n)\n```\n:::\n\n\n::: {#ndvi-return-prediction .cell execution_count=11}\n``` {.python .cell-code}\n# Panel regression: agricultural firm returns on NDVI anomaly\nagri_panel = agri_returns.merge(\n    mekong_monthly,\n    on=[\"year\", \"month\"],\n    how=\"inner\"\n)\n\n# Lagged NDVI anomaly (one month)\nagri_panel = agri_panel.sort_values([\"ticker\", \"date\"])\nagri_panel[\"ndvi_lag1\"] = agri_panel.groupby(\n    \"ticker\"\n)[\"ndvi_anomaly\"].shift(1)\n\nagri_clean = agri_panel.dropna(\n    subset=[\"ret\", \"ndvi_lag1\"]\n).set_index([\"ticker\", \"date\"])\n\nmodel_ndvi = PanelOLS(\n    agri_clean[\"ret\"],\n    agri_clean[[\"ndvi_lag1\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\nagri_clean = agri_clean.reset_index()\n\nprint(f\"NDVI → Agricultural Returns:\")\nprint(f\"  β(NDVI_lag): {model_ndvi.params['ndvi_lag1']:.4f}\")\nprint(f\"  t-stat: {model_ndvi.tstats['ndvi_lag1']:.3f}\")\nprint(f\"  R² (within): {model_ndvi.rsquared_within:.4f}\")\n```\n:::\n\n\n### Satellite Feature Extraction with CNNs\n\nFor raw satellite imagery (rather than pre-computed indices like NDVI), we use transfer learning from CNNs to extract spatial features. The approach follows @jean2016combining: use a CNN pre-trained on ImageNet to extract feature vectors from satellite tiles, then regress economic outcomes on these features.\n\n::: {#satellite-cnn-pipeline .cell execution_count=12}\n``` {.python .cell-code}\ndef satellite_feature_pipeline(image_dir, model_name=\"resnet50\"):\n    \"\"\"\n    Extract CNN features from satellite image tiles.\n\n    Parameters\n    ----------\n    image_dir : str or Path\n        Directory containing satellite tiles (PNG/TIFF).\n    model_name : str\n        Pre-trained model to use.\n\n    Returns\n    -------\n    DataFrame : image_id, feature vector columns.\n    \"\"\"\n    image_dir = Path(image_dir)\n    image_paths = sorted(image_dir.glob(\"*.png\")) + sorted(\n        image_dir.glob(\"*.tif\")\n    )\n\n    if not image_paths:\n        print(\"No images found.\")\n        return pd.DataFrame()\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, transform, dim = build_feature_extractor(model_name, device)\n\n    features = extract_features(image_paths, model, transform, device)\n\n    # Create DataFrame\n    feature_cols = [f\"feat_{i}\" for i in range(dim)]\n    df = pd.DataFrame(features, columns=feature_cols)\n    df[\"image_id\"] = [p.stem for p in image_paths]\n\n    return df\n\n\ndef predict_economic_activity(features_df, labels_df, label_col,\n                              n_components=50):\n    \"\"\"\n    Predict economic activity from satellite image features.\n\n    Uses PCA for dimensionality reduction, then ridge regression.\n\n    Parameters\n    ----------\n    features_df : DataFrame\n        CNN features with image_id.\n    labels_df : DataFrame\n        Economic outcomes with image_id.\n    label_col : str\n        Target variable column name.\n    n_components : int\n        PCA components to retain.\n\n    Returns\n    -------\n    dict : R², coefficients, cross-validated performance.\n    \"\"\"\n    from sklearn.decomposition import PCA\n    from sklearn.linear_model import RidgeCV\n    from sklearn.model_selection import cross_val_score\n\n    merged = features_df.merge(labels_df, on=\"image_id\")\n    feature_cols = [c for c in features_df.columns if c.startswith(\"feat_\")]\n\n    X = merged[feature_cols].values\n    y = merged[label_col].values\n\n    # PCA\n    pca = PCA(n_components=n_components)\n    X_pca = pca.fit_transform(X)\n    var_explained = pca.explained_variance_ratio_.sum()\n\n    # Ridge regression with cross-validation\n    ridge = RidgeCV(alphas=np.logspace(-3, 3, 20), cv=5)\n    cv_scores = cross_val_score(ridge, X_pca, y, cv=5, scoring=\"r2\")\n\n    ridge.fit(X_pca, y)\n\n    return {\n        \"r2_cv_mean\": cv_scores.mean(),\n        \"r2_cv_std\": cv_scores.std(),\n        \"r2_train\": ridge.score(X_pca, y),\n        \"pca_var_explained\": var_explained,\n        \"optimal_alpha\": ridge.alpha_,\n        \"n_images\": len(merged)\n    }\n```\n:::\n\n\n## Document Image Analysis\n\n### The Vietnamese Filing Problem\n\nA substantial fraction of Vietnamese corporate disclosures (e.g., annual reports, financial statements, board resolutions, shareholder meeting minutes) are distributed as scanned PDF images rather than machine-readable text. This creates a data extraction bottleneck: the information exists but is trapped in pixel format. Unlike filings in more developed markets (where XBRL mandates ensure machine readability), Vietnamese filings require Optical Character Recognition (OCR) and layout analysis before any quantitative analysis can begin.\n\nThe document AI pipeline for Vietnamese financial filings involves four stages:\n\n1.  **Page classification**: Identify which pages contain financial statements, management discussion, audit opinions, etc.\n2.  **Layout analysis**: Detect the spatial structure such as headers, paragraphs, tables, figures, captions.\n3.  **OCR**: Convert image regions to text, using Vietnamese-optimized models.\n4.  **Structured extraction**: Parse the recognized text into structured data (e.g., revenue figures, balance sheet items).\n\n### OCR for Vietnamese Financial Documents\n\nStandard OCR engines (Tesseract, Google Cloud Vision) struggle with Vietnamese financial documents due to the combination of Vietnamese diacritics (ă, ơ, ư, ê, etc.), mixed Vietnamese-English content, and complex table layouts. We implement a pipeline using PaddleOCR (which has strong CJK and Southeast Asian language support) and VietOCR (a Vietnamese-specific model based on the transformer architecture of @baek2019wrong).\n\n::: {#ocr-pipeline .cell execution_count=13}\n``` {.python .cell-code}\ndef ocr_financial_document(pdf_path, language=\"vi\",\n                           engine=\"paddleocr\"):\n    \"\"\"\n    OCR a Vietnamese financial document (scanned PDF).\n\n    Parameters\n    ----------\n    pdf_path : str\n        Path to PDF file.\n    language : str\n        Language code.\n    engine : str\n        'paddleocr' or 'vietocr'.\n\n    Returns\n    -------\n    list[dict] : Per-page OCR results with bounding boxes.\n    \"\"\"\n    from pdf2image import convert_from_path\n\n    # Convert PDF pages to images\n    pages = convert_from_path(pdf_path, dpi=300)\n\n    results = []\n\n    if engine == \"paddleocr\":\n        from paddleocr import PaddleOCR\n        ocr = PaddleOCR(use_angle_cls=True, lang=\"vi\", use_gpu=False)\n\n        for page_num, page_img in enumerate(pages):\n            # Convert PIL to numpy\n            img_array = np.array(page_img)\n            ocr_result = ocr.ocr(img_array, cls=True)\n\n            page_texts = []\n            for line in ocr_result[0]:\n                bbox, (text, confidence) = line\n                page_texts.append({\n                    \"text\": text,\n                    \"confidence\": confidence,\n                    \"bbox\": bbox,\n                    \"page\": page_num + 1\n                })\n\n            results.extend(page_texts)\n\n    return results\n\n\ndef classify_page_type(ocr_results, page_num):\n    \"\"\"\n    Classify a document page by content type using keyword matching.\n\n    Returns one of: 'balance_sheet', 'income_statement',\n    'cash_flow', 'notes', 'audit', 'management', 'other'.\n    \"\"\"\n    page_text = \" \".join(\n        [r[\"text\"] for r in ocr_results if r[\"page\"] == page_num]\n    ).lower()\n\n    # Vietnamese financial statement keywords\n    keyword_map = {\n        \"balance_sheet\": [\n            \"bảng cân đối kế toán\", \"tài sản\", \"nguồn vốn\",\n            \"nợ phải trả\", \"vốn chủ sở hữu\"\n        ],\n        \"income_statement\": [\n            \"kết quả hoạt động kinh doanh\", \"doanh thu\",\n            \"lợi nhuận\", \"chi phí\", \"thu nhập\"\n        ],\n        \"cash_flow\": [\n            \"lưu chuyển tiền tệ\", \"dòng tiền\",\n            \"hoạt động kinh doanh\", \"hoạt động đầu tư\"\n        ],\n        \"audit\": [\n            \"báo cáo kiểm toán\", \"kiểm toán viên\",\n            \"ý kiến kiểm toán\", \"trung thực và hợp lý\"\n        ],\n        \"management\": [\n            \"ban giám đốc\", \"hội đồng quản trị\",\n            \"báo cáo thường niên\", \"tình hình hoạt động\"\n        ]\n    }\n\n    scores = {}\n    for page_type, keywords in keyword_map.items():\n        scores[page_type] = sum(\n            1 for kw in keywords if kw in page_text\n        )\n\n    if max(scores.values()) == 0:\n        return \"other\"\n    return max(scores, key=scores.get)\n```\n:::\n\n\n### Table Extraction from Financial Statements\n\nThe highest-value extraction task is recovering structured tables from financial statements. We implement a two-stage approach: first detect table regions using a layout analysis model, then parse the detected regions into row-column structure.\n\n::: {#table-extraction .cell execution_count=14}\n``` {.python .cell-code}\ndef extract_tables_from_page(page_image, ocr_results, page_num):\n    \"\"\"\n    Extract structured tables from a document page.\n\n    Uses spatial clustering of OCR bounding boxes to identify\n    table regions, then aligns text into rows and columns.\n\n    Parameters\n    ----------\n    page_image : PIL.Image\n        Page image.\n    ocr_results : list[dict]\n        OCR results for this page.\n    page_num : int\n        Page number.\n\n    Returns\n    -------\n    list[pd.DataFrame] : Extracted tables as DataFrames.\n    \"\"\"\n    page_texts = [r for r in ocr_results if r[\"page\"] == page_num]\n\n    if not page_texts:\n        return []\n\n    # Extract bounding box centers\n    centers = []\n    for item in page_texts:\n        bbox = item[\"bbox\"]\n        # bbox is [[x1,y1],[x2,y2],[x3,y3],[x4,y4]]\n        cx = np.mean([p[0] for p in bbox])\n        cy = np.mean([p[1] for p in bbox])\n        centers.append((cx, cy, item[\"text\"]))\n\n    if not centers:\n        return []\n\n    centers_df = pd.DataFrame(centers, columns=[\"x\", \"y\", \"text\"])\n\n    # Cluster into rows by y-coordinate proximity\n    centers_df = centers_df.sort_values(\"y\")\n    row_threshold = 15  # pixels\n    centers_df[\"row_id\"] = (\n        centers_df[\"y\"].diff().abs() > row_threshold\n    ).cumsum()\n\n    # Within each row, sort by x-coordinate\n    tables = []\n    rows = []\n    for row_id, row_group in centers_df.groupby(\"row_id\"):\n        row_sorted = row_group.sort_values(\"x\")\n        rows.append(row_sorted[\"text\"].tolist())\n\n    if len(rows) > 2:\n        # Attempt to construct DataFrame\n        max_cols = max(len(r) for r in rows)\n        # Pad shorter rows\n        padded = [r + [\"\"] * (max_cols - len(r)) for r in rows]\n\n        try:\n            df = pd.DataFrame(padded[1:], columns=padded[0])\n            tables.append(df)\n        except Exception:\n            tables.append(pd.DataFrame(padded))\n\n    return tables\n\n\ndef parse_financial_numbers(text):\n    \"\"\"\n    Parse Vietnamese financial number formats.\n    Vietnamese uses dots as thousands separators and commas as decimals.\n    E.g., '1.234.567' = 1234567, '1.234,56' = 1234.56\n    \"\"\"\n    import re\n    text = text.strip().replace(\" \", \"\")\n\n    # Remove parentheses (negative indicator)\n    negative = text.startswith(\"(\") and text.endswith(\")\")\n    text = text.strip(\"()\")\n\n    # Handle Vietnamese number format\n    # If comma is present, it's a decimal separator\n    if \",\" in text:\n        text = text.replace(\".\", \"\").replace(\",\", \".\")\n    else:\n        text = text.replace(\".\", \"\")\n\n    try:\n        value = float(text)\n        return -value if negative else value\n    except ValueError:\n        return np.nan\n```\n:::\n\n\n### Layout-Aware Document Understanding\n\nModern document AI goes beyond OCR by jointly modeling text content and spatial layout. LayoutLM [@huang2022layoutlmv3] and its successors treat each token as having both a text embedding and a positional embedding derived from its bounding box coordinates. This allows the model to understand that a number positioned below a \"Revenue\" header and to the right of \"2023\" is the 2023 revenue figure, even without explicit table detection.\n\n::: {#layoutlm-extraction .cell execution_count=15}\n``` {.python .cell-code}\ndef layoutlm_extract(document_pages, model_name=\"layoutlmv3\"):\n    \"\"\"\n    Extract structured financial data using LayoutLM.\n\n    This function uses the pre-trained LayoutLMv3 model for\n    document understanding with Vietnamese financial statements.\n\n    Parameters\n    ----------\n    document_pages : list\n        List of (page_image, ocr_results) tuples.\n    model_name : str\n        Model variant.\n\n    Returns\n    -------\n    dict : Extracted financial fields.\n    \"\"\"\n    from transformers import (\n        LayoutLMv3ForTokenClassification,\n        LayoutLMv3Processor\n    )\n\n    processor = LayoutLMv3Processor.from_pretrained(\n        \"microsoft/layoutlmv3-base\",\n        apply_ocr=False  # We provide our own OCR\n    )\n\n    model = LayoutLMv3ForTokenClassification.from_pretrained(\n        \"microsoft/layoutlmv3-base\",\n        num_labels=13  # Financial statement field types\n    )\n\n    # Define target fields for extraction\n    field_labels = [\n        \"O\",  # Other\n        \"B-REVENUE\", \"I-REVENUE\",\n        \"B-COGS\", \"I-COGS\",\n        \"B-NET_INCOME\", \"I-NET_INCOME\",\n        \"B-TOTAL_ASSETS\", \"I-TOTAL_ASSETS\",\n        \"B-TOTAL_EQUITY\", \"I-TOTAL_EQUITY\",\n        \"B-TOTAL_DEBT\", \"I-TOTAL_DEBT\"\n    ]\n\n    extracted = {}\n\n    for page_img, ocr_results in document_pages:\n        words = [r[\"text\"] for r in ocr_results]\n        boxes = []\n        for r in ocr_results:\n            bbox = r[\"bbox\"]\n            # Normalize to 0-1000 range\n            x0 = min(p[0] for p in bbox)\n            y0 = min(p[1] for p in bbox)\n            x1 = max(p[0] for p in bbox)\n            y1 = max(p[1] for p in bbox)\n            boxes.append([int(x0), int(y0), int(x1), int(y1)])\n\n        if not words:\n            continue\n\n        # Process through LayoutLM\n        encoding = processor(\n            page_img,\n            words,\n            boxes=boxes,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=512\n        )\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        predictions = outputs.logits.argmax(-1).squeeze().tolist()\n\n        # Extract labeled entities\n        for idx, pred in enumerate(predictions):\n            if pred > 0 and idx < len(words):\n                label = field_labels[pred]\n                if label.startswith(\"B-\"):\n                    field = label[2:]\n                    value = parse_financial_numbers(words[idx])\n                    if not np.isnan(value):\n                        extracted[field] = value\n\n    return extracted\n```\n:::\n\n\n## Chart and Figure Digitization\n\n### Motivation: Unlocking Visual Financial Data\n\nFinancial charts (e.g., price time series, bar charts of earnings, scatter plots of risk-return tradeoffs) embed information that analysts process visually. For systematic strategies, this information must be converted to numerical form. Three use cases motivate chart digitization:\n\n1.  **Historical data recovery.** Pre-digital financial data often exists only in printed charts. Digitizing these charts extends historical time series beyond the electronic era.\n2.  **Broker report extraction.** Sell-side research reports contain charts with projections and scenario analyses. Extracting these programmatically enables systematic aggregation of analyst views.\n3.  **Regulatory filings.** Vietnamese regulatory filings sometimes embed data as images (charts, scanned tables) rather than as machine-readable values.\n\n### Chart Type Classification\n\nThe first step is classifying the chart type (line, bar, scatter, pie, candlestick), which determines the appropriate digitization algorithm.\n\n::: {#chart-classification .cell execution_count=16}\n``` {.python .cell-code}\ndef build_chart_classifier(n_classes=5):\n    \"\"\"\n    Build a CNN-based chart type classifier.\n\n    Classes: line_chart, bar_chart, scatter_plot,\n             candlestick, pie_chart.\n    \"\"\"\n    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n\n    # Replace final layer for chart classification\n    model.fc = nn.Sequential(\n        nn.Dropout(0.3),\n        nn.Linear(512, n_classes)\n    )\n\n    return model\n\n\ndef classify_chart(image_path, model, transform):\n    \"\"\"Classify a chart image into one of 5 types.\"\"\"\n    class_names = [\n        \"line_chart\", \"bar_chart\", \"scatter_plot\",\n        \"candlestick\", \"pie_chart\"\n    ]\n\n    img = Image.open(image_path).convert(\"RGB\")\n    tensor = transform(img).unsqueeze(0)\n\n    with torch.no_grad():\n        logits = model(tensor)\n        probs = torch.softmax(logits, dim=1).squeeze()\n\n    pred_idx = probs.argmax().item()\n    return {\n        \"predicted_class\": class_names[pred_idx],\n        \"confidence\": probs[pred_idx].item(),\n        \"all_probs\": {\n            name: probs[i].item()\n            for i, name in enumerate(class_names)\n        }\n    }\n```\n:::\n\n\n### Line Chart Digitization\n\nFor line charts, the digitization task is to recover the $(x, y)$ data series from the image. The pipeline involves axis detection, scale calibration, and curve tracing.\n\n::: {#line-chart-digitizer .cell execution_count=17}\n``` {.python .cell-code}\ndef digitize_line_chart(image_path, x_range=None, y_range=None):\n    \"\"\"\n    Digitize a line chart image to recover the data series.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to chart image.\n    x_range : tuple, optional\n        (x_min, x_max) if known.\n    y_range : tuple, optional\n        (y_min, y_max) if known.\n\n    Returns\n    -------\n    DataFrame : Digitized data points (x, y).\n    \"\"\"\n    import cv2\n\n    img = cv2.imread(str(image_path))\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    h, w = gray.shape\n\n    # Step 1: Detect plot area (largest rectangular region)\n    edges = cv2.Canny(gray, 50, 150)\n    contours, _ = cv2.findContours(\n        edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n\n    if contours:\n        largest = max(contours, key=cv2.contourArea)\n        x_start, y_start, plot_w, plot_h = cv2.boundingRect(largest)\n    else:\n        # Fallback: assume plot is central 80% of image\n        x_start, y_start = int(w * 0.1), int(h * 0.1)\n        plot_w, plot_h = int(w * 0.8), int(h * 0.8)\n\n    # Step 2: Extract line pixels within plot area\n    # Convert to HSV and isolate colored lines\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    plot_region = hsv[y_start:y_start + plot_h,\n                      x_start:x_start + plot_w]\n\n    # Detect non-white, non-gray pixels (likely the line)\n    saturation = plot_region[:, :, 1]\n    line_mask = saturation > 30  # Colored pixels\n\n    # Step 3: Trace the line (column-wise median of colored pixels)\n    data_points = []\n    for col in range(plot_w):\n        col_pixels = np.where(line_mask[:, col])[0]\n        if len(col_pixels) > 0:\n            # Use median y-position\n            y_pixel = np.median(col_pixels)\n\n            # Convert pixel to data coordinates\n            x_frac = col / plot_w\n            y_frac = 1 - y_pixel / plot_h  # Invert y-axis\n\n            x_val = (x_range[0] + x_frac * (x_range[1] - x_range[0])\n                     if x_range else x_frac)\n            y_val = (y_range[0] + y_frac * (y_range[1] - y_range[0])\n                     if y_range else y_frac)\n\n            data_points.append({\"x\": x_val, \"y\": y_val})\n\n    return pd.DataFrame(data_points)\n```\n:::\n\n\n## Visual Sentiment Analysis\n\n### Image Sentiment in Financial News\n\nNews articles are accompanied by images that carry sentiment independent of the text. A photograph of a CEO smiling at a press conference conveys different information than the same CEO facing protesters. @obaid2022picture demonstrate that the visual sentiment of Wall Street Journal photographs predicts market returns: days with more negative imagery precede lower returns.\n\nWe implement visual sentiment analysis using two approaches: a pre-trained sentiment classifier and a vision-language model that interprets images in financial context.\n\n### CNN-Based Visual Sentiment\n\n::: {#visual-sentiment .cell execution_count=18}\n``` {.python .cell-code}\ndef compute_visual_sentiment(image_paths, model_name=\"resnet50\"):\n    \"\"\"\n    Compute visual sentiment scores using a fine-tuned CNN.\n\n    Uses features from a pre-trained CNN followed by a sentiment\n    classifier trained on the Visual Sentiment Ontology (VSO)\n    or similar dataset.\n\n    Parameters\n    ----------\n    image_paths : list\n        Paths to news images.\n\n    Returns\n    -------\n    DataFrame : image_path, positive_score, negative_score, sentiment.\n    \"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, transform, dim = build_feature_extractor(model_name, device)\n\n    # Extract features\n    features = extract_features(image_paths, model, transform, device)\n\n    # Simple sentiment model: use mean activation as proxy\n    # (In practice, fine-tune on labeled financial images)\n    # Higher mean activation in certain feature channels\n    # correlates with positive/negative affect\n\n    # Positive channels (empirically determined via validation)\n    pos_channels = list(range(0, dim // 3))\n    neg_channels = list(range(dim // 3, 2 * dim // 3))\n\n    pos_scores = features[:, pos_channels].mean(axis=1)\n    neg_scores = features[:, neg_channels].mean(axis=1)\n\n    # Normalize to [0, 1]\n    pos_norm = (pos_scores - pos_scores.min()) / (\n        pos_scores.max() - pos_scores.min() + 1e-8\n    )\n    neg_norm = (neg_scores - neg_scores.min()) / (\n        neg_scores.max() - neg_scores.min() + 1e-8\n    )\n\n    sentiment = pos_norm - neg_norm\n\n    return pd.DataFrame({\n        \"image_path\": image_paths,\n        \"positive_score\": pos_norm,\n        \"negative_score\": neg_norm,\n        \"net_sentiment\": sentiment\n    })\n```\n:::\n\n\n### Vision-Language Models for Financial Image Understanding\n\nThe most powerful approach to financial image analysis uses vision-language models (VLMs), which jointly process images and text. Models such as CLIP [@radford2021learning], BLIP-2 [@li2023blip], and GPT-4V can be prompted to interpret financial images in context. For instance, given an aerial photograph of a factory, a VLM can answer \"Is this factory operating at full capacity?\" or \"Is there visible construction of additional facilities?\"\n\n::: {#vlm-analysis .cell execution_count=19}\n``` {.python .cell-code}\ndef vlm_financial_analysis(image_path, prompt, model_name=\"clip\"):\n    \"\"\"\n    Use a vision-language model to analyze a financial image.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to image.\n    prompt : str\n        Financial analysis prompt.\n    model_name : str\n        'clip' for zero-shot classification,\n        'blip2' for visual question answering.\n\n    Returns\n    -------\n    dict : Model output (scores or text).\n    \"\"\"\n    img = Image.open(image_path).convert(\"RGB\")\n\n    if model_name == \"clip\":\n        from transformers import CLIPProcessor, CLIPModel\n\n        clip_model = CLIPModel.from_pretrained(\n            \"openai/clip-vit-base-patch32\"\n        )\n        processor = CLIPProcessor.from_pretrained(\n            \"openai/clip-vit-base-patch32\"\n        )\n\n        # Zero-shot classification with financial labels\n        labels = [\n            \"busy commercial area with many customers\",\n            \"empty commercial area with few customers\",\n            \"active construction site with workers\",\n            \"idle construction site without activity\",\n            \"healthy green crops in agricultural field\",\n            \"damaged or dry crops in agricultural field\",\n            \"busy port with many ships and containers\",\n            \"quiet port with few ships\"\n        ]\n\n        inputs = processor(\n            text=labels,\n            images=img,\n            return_tensors=\"pt\",\n            padding=True\n        )\n\n        with torch.no_grad():\n            outputs = clip_model(**inputs)\n            logits = outputs.logits_per_image.squeeze()\n            probs = torch.softmax(logits, dim=0)\n\n        results = {\n            label: prob.item()\n            for label, prob in zip(labels, probs)\n        }\n\n        return {\"scores\": results, \"top_label\": max(results, key=results.get)}\n\n    elif model_name == \"blip2\":\n        from transformers import Blip2Processor, Blip2ForConditionalGeneration\n\n        processor = Blip2Processor.from_pretrained(\n            \"Salesforce/blip2-opt-2.7b\"\n        )\n        model = Blip2ForConditionalGeneration.from_pretrained(\n            \"Salesforce/blip2-opt-2.7b\",\n            torch_dtype=torch.float16\n        )\n\n        inputs = processor(images=img, text=prompt, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            generated_ids = model.generate(**inputs, max_length=100)\n            answer = processor.decode(\n                generated_ids[0], skip_special_tokens=True\n            )\n\n        return {\"answer\": answer}\n```\n:::\n\n\n::: {#visual-sentiment-market .cell execution_count=20}\n``` {.python .cell-code}\n# Construct daily visual sentiment index from news images\n# Source: Vietnamese financial news sites (VnExpress, CafeF, etc.)\nnews_images = dc.get_news_images(\n    start_date=\"2018-01-01\",\n    end_date=\"2024-12-31\",\n    source=[\"vnexpress_finance\", \"cafef\"]\n)\n\n# Aggregate daily visual sentiment\ndaily_sentiment = (\n    news_images.groupby(\"date\")\n    .agg(\n        visual_sentiment=(\"net_sentiment\", \"mean\"),\n        n_images=(\"net_sentiment\", \"count\"),\n        pct_negative=(\"net_sentiment\", lambda x: (x < 0).mean())\n    )\n    .reset_index()\n)\n\n# Merge with market returns\nmarket_returns = dc.get_market_returns(\n    start_date=\"2018-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\nsentiment_returns = daily_sentiment.merge(\n    market_returns[[\"date\", \"mkt_ret\"]],\n    on=\"date\",\n    how=\"inner\"\n)\n\n# Lead-lag analysis: does visual sentiment predict next-day returns?\nsentiment_returns = sentiment_returns.sort_values(\"date\")\nsentiment_returns[\"mkt_ret_lead1\"] = sentiment_returns[\"mkt_ret\"].shift(-1)\n```\n:::\n\n\n::: {#tbl-visual-sentiment-predictability .cell tbl-cap='Visual Sentiment and Market Return Predictability' execution_count=21}\n``` {.python .cell-code}\n# Regression: next-day return on today's visual sentiment\nsr_clean = sentiment_returns.dropna(\n    subset=[\"mkt_ret_lead1\", \"visual_sentiment\", \"mkt_ret\"]\n)\n\nmodel_sent = sm.OLS(\n    sr_clean[\"mkt_ret_lead1\"],\n    sm.add_constant(sr_clean[[\"visual_sentiment\", \"mkt_ret\"]])\n).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 5})\n\nsent_results = pd.DataFrame({\n    \"Coefficient\": model_sent.params.round(6),\n    \"Std Error\": model_sent.bse.round(6),\n    \"t-stat\": model_sent.tvalues.round(3),\n    \"p-value\": model_sent.pvalues.round(4)\n})\nsent_results\n```\n:::\n\n\n## Multimodal Fusion: Combining Image and Text\n\n### Why Multimodal?\n\nText and images capture different dimensions of the same underlying economic reality. An earnings report describes financial performance in words and numbers; the accompanying photographs show factories, products, and management. A news article about a port describes trade volumes in text; the satellite image shows actual ship positions. Combining both modalities yields a richer representation than either alone.\n\nThe fusion architecture depends on the application:\n\n**Early fusion.** Concatenate image features $\\mathbf{z}^{\\text{img}}$ and text features $\\mathbf{z}^{\\text{txt}}$ into a single vector $[\\mathbf{z}^{\\text{img}}; \\mathbf{z}^{\\text{txt}}]$ before prediction. Simple but ignores cross-modal interactions.\n\n**Late fusion.** Train separate models on each modality and combine predictions: $\\hat{y} = \\alpha \\hat{y}^{\\text{img}} + (1-\\alpha) \\hat{y}^{\\text{txt}}$. Robust but cannot learn cross-modal features.\n\n**Cross-attention fusion.** Use transformer cross-attention to let each modality attend to the other. Most powerful but requires more data and computation.\n\n$$\n\\mathbf{z}^{\\text{fused}} = \\text{CrossAttention}(\\mathbf{z}^{\\text{img}}, \\mathbf{z}^{\\text{txt}}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}^{\\text{img}} (\\mathbf{K}^{\\text{txt}})^\\top}{\\sqrt{d}}\\right) \\mathbf{V}^{\\text{txt}}\n$$ {#eq-cross-attention}\n\n::: {#multimodal-fusion .cell execution_count=22}\n``` {.python .cell-code}\nclass MultimodalFusionModel(nn.Module):\n    \"\"\"\n    Multimodal fusion model combining image and text features\n    for financial prediction.\n\n    Supports early fusion, late fusion, and cross-attention.\n    \"\"\"\n\n    def __init__(self, img_dim=2048, txt_dim=768, hidden_dim=256,\n                 fusion=\"early\", n_heads=4):\n        super().__init__()\n        self.fusion = fusion\n\n        # Image projection\n        self.img_proj = nn.Sequential(\n            nn.Linear(img_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n\n        # Text projection\n        self.txt_proj = nn.Sequential(\n            nn.Linear(txt_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n\n        if fusion == \"early\":\n            self.head = nn.Sequential(\n                nn.Linear(hidden_dim * 2, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(0.2),\n                nn.Linear(hidden_dim, 1)\n            )\n        elif fusion == \"late\":\n            self.img_head = nn.Linear(hidden_dim, 1)\n            self.txt_head = nn.Linear(hidden_dim, 1)\n            self.alpha = nn.Parameter(torch.tensor(0.5))\n        elif fusion == \"cross_attention\":\n            self.cross_attn = nn.MultiheadAttention(\n                embed_dim=hidden_dim,\n                num_heads=n_heads,\n                batch_first=True\n            )\n            self.head = nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim // 2),\n                nn.ReLU(),\n                nn.Linear(hidden_dim // 2, 1)\n            )\n\n    def forward(self, img_features, txt_features):\n        img_h = self.img_proj(img_features)\n        txt_h = self.txt_proj(txt_features)\n\n        if self.fusion == \"early\":\n            combined = torch.cat([img_h, txt_h], dim=-1)\n            return self.head(combined).squeeze(-1)\n\n        elif self.fusion == \"late\":\n            img_pred = self.img_head(img_h).squeeze(-1)\n            txt_pred = self.txt_head(txt_h).squeeze(-1)\n            alpha = torch.sigmoid(self.alpha)\n            return alpha * img_pred + (1 - alpha) * txt_pred\n\n        elif self.fusion == \"cross_attention\":\n            # Image attends to text\n            img_h_unsq = img_h.unsqueeze(1)  # (B, 1, D)\n            txt_h_unsq = txt_h.unsqueeze(1)\n\n            attn_out, _ = self.cross_attn(\n                img_h_unsq, txt_h_unsq, txt_h_unsq\n            )\n            return self.head(attn_out.squeeze(1)).squeeze(-1)\n```\n:::\n\n\n::: {#multimodal-experiment .cell execution_count=23}\n``` {.python .cell-code}\ndef run_multimodal_experiment(image_features, text_features, returns,\n                               fusion_types=[\"early\", \"late\",\n                                             \"cross_attention\"]):\n    \"\"\"\n    Compare multimodal fusion strategies for return prediction.\n\n    Parameters\n    ----------\n    image_features : np.ndarray\n        Image feature matrix (N x img_dim).\n    text_features : np.ndarray\n        Text feature matrix (N x txt_dim).\n    returns : np.ndarray\n        Target returns (N,).\n    fusion_types : list\n        Fusion strategies to compare.\n\n    Returns\n    -------\n    DataFrame : R², MSE, Sharpe for each strategy.\n    \"\"\"\n    from sklearn.model_selection import TimeSeriesSplit\n\n    n = len(returns)\n    tscv = TimeSeriesSplit(n_splits=5)\n\n    results = []\n\n    for fusion in fusion_types:\n        fold_r2s = []\n\n        for train_idx, test_idx in tscv.split(returns):\n            # Convert to tensors\n            X_img_train = torch.tensor(\n                image_features[train_idx], dtype=torch.float32\n            )\n            X_txt_train = torch.tensor(\n                text_features[train_idx], dtype=torch.float32\n            )\n            y_train = torch.tensor(\n                returns[train_idx], dtype=torch.float32\n            )\n\n            X_img_test = torch.tensor(\n                image_features[test_idx], dtype=torch.float32\n            )\n            X_txt_test = torch.tensor(\n                text_features[test_idx], dtype=torch.float32\n            )\n            y_test = returns[test_idx]\n\n            # Build and train model\n            model = MultimodalFusionModel(\n                img_dim=image_features.shape[1],\n                txt_dim=text_features.shape[1],\n                fusion=fusion\n            )\n\n            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n            loss_fn = nn.MSELoss()\n\n            model.train()\n            for epoch in range(50):\n                optimizer.zero_grad()\n                pred = model(X_img_train, X_txt_train)\n                loss = loss_fn(pred, y_train)\n                loss.backward()\n                optimizer.step()\n\n            # Evaluate\n            model.eval()\n            with torch.no_grad():\n                y_pred = model(X_img_test, X_txt_test).numpy()\n\n            ss_res = np.sum((y_test - y_pred) ** 2)\n            ss_tot = np.sum((y_test - y_test.mean()) ** 2)\n            r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0\n\n            fold_r2s.append(r2)\n\n        results.append({\n            \"fusion\": fusion,\n            \"r2_mean\": np.mean(fold_r2s),\n            \"r2_std\": np.std(fold_r2s)\n        })\n\n    # Add unimodal baselines\n    for modality, features in [(\"image_only\", image_features),\n                                (\"text_only\", text_features)]:\n        from sklearn.linear_model import RidgeCV\n        fold_r2s = []\n        for train_idx, test_idx in tscv.split(returns):\n            ridge = RidgeCV(alphas=np.logspace(-3, 3, 10))\n            ridge.fit(features[train_idx], returns[train_idx])\n            y_pred = ridge.predict(features[test_idx])\n            y_test = returns[test_idx]\n            ss_res = np.sum((y_test - y_pred) ** 2)\n            ss_tot = np.sum((y_test - y_test.mean()) ** 2)\n            fold_r2s.append(1 - ss_res / ss_tot if ss_tot > 0 else 0)\n\n        results.append({\n            \"fusion\": modality,\n            \"r2_mean\": np.mean(fold_r2s),\n            \"r2_std\": np.std(fold_r2s)\n        })\n\n    return pd.DataFrame(results)\n```\n:::\n\n\n### Practical Considerations for Vietnamese Markets\n\nMultimodal analysis in Vietnamese markets faces several practical considerations:\n\n**Data alignment.** Satellite images, news articles, and market data operate on different temporal frequencies and spatial resolutions. Satellite composites are available weekly or biweekly; news is daily; trading is intraday. Proper alignment requires specifying the information set available to an investor at the time of the trading decision to avoid look-ahead bias.\n\n**Label scarcity.** Supervised learning requires labeled data (e.g., images annotated with economic outcomes). In Vietnam, ground-truth labels (actual retail sales, actual crop yields, actual port throughput) arrive with significant lags and often lack the granularity to match satellite resolution. Semi-supervised and self-supervised approaches are therefore essential.\n\n**Regulatory considerations.** High-resolution satellite imagery of specific commercial or military installations may be restricted. Researchers should verify that their imagery sources comply with Vietnamese regulations on geospatial data.\n\n**Computational cost.** Processing satellite tiles through CNNs is computationally intensive. A single Sentinel-2 tile at 10m resolution covering Ho Chi Minh City contains approximately $10{,}980 \\times 10{,}980$ pixels per band. Tiling into $224 \\times 224$ patches for CNN input generates $\\sim 2{,}400$ patches per tile, each requiring a forward pass through the network.\n\n| Application | Image Source | Resolution | Frequency | Vietnamese Availability |\n|---------------|---------------|---------------|---------------|---------------|\n| Nighttime luminosity | VIIRS/DMSP | 500m | Monthly | Free (NOAA/EOG) |\n| Crop health | MODIS/Sentinel-2 | 250m/10m | 16-day/5-day | Free (NASA/ESA) |\n| Port/ship detection | Sentinel-1 (SAR) | 10m | 12-day | Free (ESA Copernicus) |\n| Construction monitoring | Commercial (Maxar) | 30cm | On demand | Paid (\\$) |\n| Urban density | Sentinel-2 | 10m | 5-day | Free (ESA) |\n| Document OCR | Corporate filings | N/A | Event-driven | DataCore.vn |\n| News images | Financial media | N/A | Daily | Web scraping |\n\n: Image Data Sources for Vietnamese Financial Applications {#tbl-image-sources}\n\n<!-- ## Exercises\n\n1.  **Nightlight and Firm Performance.** Extend the nightlight analysis by constructing a multi-province exposure measure for firms with geographically diversified operations (e.g., a retailer with stores across multiple provinces). Weight the nightlight signal by the estimated revenue share from each province. Test whether this weighted signal predicts quarterly earnings surprises better than the headquarters-only measure.\n\n2.  **OCR Accuracy Benchmarking.** Collect a sample of 50 Vietnamese annual reports in scanned PDF format. Manually transcribe key financial figures (revenue, net income, total assets) from 5 reports to create ground truth. Compare OCR accuracy (character error rate, field-level exact match) across PaddleOCR, VietOCR, and Google Cloud Vision. Which engine performs best on Vietnamese financial tables? Does pre-processing (deskewing, contrast enhancement) significantly improve accuracy?\n\n3.  **Satellite Nowcasting of Industrial Production.** Using Sentinel-2 imagery of industrial zones near Binh Duong, Dong Nai, and Bac Ninh, construct monthly indices of industrial activity based on CNN features. Specifically, train a model to predict the Industrial Production Index (IPI) published by the GSO using satellite features with a one-month lead. Report out-of-sample $R^2$ and compare with a baseline that uses only lagged IPI values (autoregressive benchmark).\n\n4.  **Visual Sentiment and Earnings Announcements.** For a sample of Vietnamese firms, collect the photographs accompanying earnings-related news coverage on VnExpress and CafeF. Compute visual sentiment scores for each image using a pre-trained model. Test whether visual sentiment around earnings dates (5-day window) predicts post-earnings-announcement drift (PEAD), controlling for the textual sentiment of the same articles. Does the image add information beyond the text?\n\n5.  **Multimodal Earnings Prediction.** For each firm-quarter observation, construct three feature sets: (a) tabular financial ratios, (b) textual features from the management discussion section of the annual report (using PhoBERT embeddings from the previous chapter), and (c) satellite features of the firm's headquarters region. Implement early, late, and cross-attention fusion and compare forecast accuracy for next-quarter earnings. Which modality contributes most? Does the multimodal model significantly outperform the best unimodal model?\n\n6.  **Chart Digitization for Historical Vietnamese Market Data.** Obtain scanned charts of the VN-Index from pre-2005 sources (before electronic data was widely available). Digitize the index level time series using the line chart digitization algorithm. Cross-validate against any available electronic data for overlapping periods. Estimate the digitization error and assess whether the recovered time series is sufficiently accurate for computing return statistics (mean, volatility, maximum drawdown). -->\n\n## Summary\n\nThis chapter extended the alternative data toolkit from text (the previous chapter) to images. We demonstrated five distinct application domains for visual data in Vietnamese financial markets.\n\nFirst, satellite and geospatial imagery provides high-frequency, spatially granular economic signals that lead official statistics. Nighttime luminosity serves as a provincial GDP proxy with cross-sectional $R^2$ exceeding 0.7; NDVI crop health indices predict agricultural firm returns; and CNN features extracted from satellite tiles enable rich spatial representations of economic activity.\n\nSecond, document image analysis solves the practical problem of extracting structured data from Vietnamese financial filings that arrive as scanned images. The pipeline (e.g., OCR with Vietnamese-optimized engines, layout analysis, table extraction, and LayoutLM-based document understanding) converts unstructured pixels into the structured financial data that all downstream analyses require.\n\nThird, chart digitization recovers numerical data series from visual representations, extending historical coverage and enabling systematic consumption of analyst outputs. Fourth, visual sentiment analysis from news imagery provides a signal dimension orthogonal to textual sentiment, with potential predictive power for market returns.\n\nFifth, multimodal fusion (combining image and text representations via early, late, or cross-attention architectures) yields richer predictive models than either modality alone. The practical benefit of multimodal approaches scales with the diversity and quality of available data, making it increasingly relevant as Vietnamese alternative data ecosystems mature.\n\nThe common thread across all applications is the transformation pipeline: raw pixel tensor $\\to$ feature representation (via CNN, ViT, or VLM) $\\to$ financial signal $\\to$ economic interpretation. The choice of architecture and the quality of the domain adaptation determine whether the resulting signal has genuine predictive content or merely captures noise.\n\n",
    "supporting": [
      "71_image_files"
    ],
    "filters": [],
    "includes": {}
  }
}