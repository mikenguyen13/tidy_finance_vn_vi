{
  "hash": "6b00988f8bc875c8ba705bfde6ef4ac1",
  "result": {
    "engine": "jupyter",
    "markdown": "# Multimodal Models in Finance\n\nThe preceding chapters treated text and images as isolated data modalities, but financial decision-making is inherently multimodal. An analyst evaluating a Vietnamese real estate developer simultaneously reads the annual report (text), inspects satellite imagery of construction sites (image), reviews quarterly financial statements (tabular), monitors the stock's price and volume dynamics (time series), and perhaps listens to the earnings call (audio). No single modality captures the full information set. The question this chapter addresses is: can we build models that fuse multiple modalities in a principled way, and does the fusion yield economically meaningful improvements over the best single-modality model?\n\nThe answer from the recent machine learning literature is increasingly yes, but with important caveats. Multimodal models can exploit complementarities between modalities (e.g., text describes intentions and context; images reveal physical states; tabular data provides precise quantitative snapshots; time series captures dynamics). However, the gains are not automatic. Naive concatenation of heterogeneous features often degrades performance relative to the best unimodal model, a phenomenon known as the \"modality laziness\" problem [@huang2021makes]. Effective fusion requires architectures that align representations across modalities, handle missing modalities gracefully (not every firm-quarter has satellite imagery and an earnings call), and avoid the dominant modality drowning out weaker but complementary signals.\n\nThis chapter develops the multimodal toolkit for Vietnamese financial markets across four progressively complex architectures. We begin with representation alignment (i.e., how to map different modalities into a shared embedding space). We then implement early, late, and cross-attention fusion for return prediction. We build a multimodal document understanding system that jointly processes the text, tables, and images within Vietnamese annual reports. We construct a multimodal earnings surprise model that combines pre-announcement text, satellite imagery, and financial time series. And we address the practical engineering challenges, including missing modalities, computational cost, and evaluation protocols that determine whether multimodal models work in production.\n\n::: {#setup .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Deep learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\n# NLP\nfrom transformers import (\n    AutoTokenizer, AutoModel,\n    CLIPProcessor, CLIPModel\n)\n\n# Tabular and statistical\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom linearmodels.panel import PanelOLS\n\n# Visualization\nimport plotnine as p9\nfrom mizani.formatters import percent_format\n```\n:::\n\n\n## Foundations of Multimodal Learning\n\n### The Information Structure of Financial Data\n\nFinancial data is naturally organized into modalities with distinct statistical properties, temporal frequencies, and information content. @tbl-modality-landscape summarizes the modalities relevant to Vietnamese equity markets.\n\n| Modality | Examples | Dimensionality | Frequency | Encoding |\n|---------------|---------------|---------------|---------------|---------------|\n| Tabular | Financial ratios, ownership, governance | Low ($\\sim$ 50 features) | Quarterly/Annual | Structured numeric |\n| Text | Annual reports, news, filings, social media | High ($\\sim$ 10k tokens) | Event-driven | Sequential tokens |\n| Image | Satellite tiles, document scans, news photos | Very high ($\\sim$ 150k pixels) | Daily to monthly | Spatial grid |\n| Time series | Price, volume, order flow, volatility | Moderate ($\\sim$ 250 days × features) | Daily/Intraday | Temporal sequence |\n| Audio | Earnings calls, conference presentations | Very high (waveform) | Quarterly | Temporal waveform |\n| Graph | Ownership networks, supply chains, co-holdings | Variable | Quarterly | Adjacency + node features |\n\n: Modality Landscape in Financial Data {#tbl-modality-landscape}\n\nEach modality carries both unique and redundant information relative to others. The value of multimodal fusion lies in the unique (complementary) information:\n\n$$\nI(\\text{Returns}; \\text{Text}, \\text{Image}, \\text{Tabular}) \\geq \\max\\left(I(\\text{Returns}; \\text{Text}), I(\\text{Returns}; \\text{Image}), I(\\text{Returns}; \\text{Tabular})\\right)\n$$ {#eq-information-inequality}\n\nwhere $I(\\cdot; \\cdot)$ denotes mutual information. The inequality is strict whenever the modalities carry non-redundant predictive content. The goal of fusion is to design architectures that approach the left-hand side.\n\n### Taxonomies of Fusion\n\nThe multimodal learning literature [@baltruvsaitis2018multimodal; @liang2024foundations] organizes fusion strategies along three dimensions.\n\n**By stage.** Where in the processing pipeline are modalities combined?\n\n-   *Input-level (early) fusion*: Concatenate raw or lightly processed features before any shared model.\n-   *Feature-level (intermediate) fusion*: Align learned representations in a shared latent space, then combine.\n-   *Decision-level (late) fusion*: Train separate models per modality, combine predictions.\n\n**By mechanism.** How are representations combined?\n\n-   *Concatenation*: $\\mathbf{z} = [\\mathbf{z}^{(1)}; \\mathbf{z}^{(2)}; \\ldots; \\mathbf{z}^{(M)}]$. Simple but ignores cross-modal interactions.\n-   *Attention-based*: One modality attends to another. Captures interactions but requires sufficient data.\n-   *Tensor product*: $\\mathbf{z} = \\mathbf{z}^{(1)} \\otimes \\mathbf{z}^{(2)}$. Captures all pairwise interactions but scales quadratically.\n-   *Gating*: $\\mathbf{z} = g(\\mathbf{z}^{(1)}) \\odot \\mathbf{z}^{(2)} + (1 - g(\\mathbf{z}^{(1)})) \\odot \\mathbf{z}^{(3)}$. Modality selection.\n\n**By training.** How are parameters learned?\n\n-   *Joint training*: All modalities processed end-to-end.\n-   *Pre-train then fuse*: Train unimodal encoders separately, then learn the fusion layer.\n-   *Contrastive alignment*: Train modality encoders to produce similar representations for matched pairs (the CLIP approach of @radford2021learning).\n\n::: {#load-multimodal-data .cell execution_count=2}\n``` {.python .cell-code}\n# DataCore.vn API\nfrom datacore import DataCore\ndc = DataCore()\n\n# Load aligned multimodal dataset\n# Each observation: firm × quarter with all available modalities\n\n# Tabular: financial statements\nfinancials = dc.get_firm_financials(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"quarterly\"\n)\n\n# Text: management discussion from annual reports\nreport_text = dc.get_annual_report_text(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    section=\"management_discussion\"\n)\n\n# Image: satellite nightlight features (from Chapter 61)\nsatellite_features = dc.get_satellite_features(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    feature_type=\"cnn_resnet50\"\n)\n\n# Time series: daily returns and volume\ndaily_data = dc.get_daily_returns(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Target: forward quarterly returns\nquarterly_returns = dc.get_quarterly_returns(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\n\nprint(f\"Firms with financials: {financials['ticker'].nunique()}\")\nprint(f\"Firms with report text: {report_text['ticker'].nunique()}\")\nprint(f\"Firms with satellite data: {satellite_features['ticker'].nunique()}\")\n```\n:::\n\n\n## Representation Alignment\n\n### The Alignment Problem\n\nDifferent modalities produce embeddings in different vector spaces with different geometries. A PhoBERT text embedding lives in $\\mathbb{R}^{768}$; a ResNet50 image feature lives in $\\mathbb{R}^{2048}$; a tabular feature vector might have 50 dimensions with heterogeneous scales. Naively concatenating these into a single vector $[\\mathbf{z}^{\\text{text}}; \\mathbf{z}^{\\text{image}}; \\mathbf{z}^{\\text{tab}}] \\in \\mathbb{R}^{2866}$ is problematic because the high-dimensional modalities dominate gradient flow, the scales are mismatched, and there is no mechanism for cross-modal interaction.\n\nAlignment projects each modality into a shared latent space $\\mathbb{R}^d$ where geometric relationships are semantically meaningful (i.e., similar firms should be nearby regardless of which modality is used to represent them).\n\n### Contrastive Alignment: CLIP for Finance\n\nThe Contrastive Language-Image Pre-training (CLIP) framework of @radford2021learning learns aligned representations by training on matched (text, image) pairs. We adapt this to financial data: for each firm-quarter, we have a textual description and a satellite image, and we train the encoders so that matched pairs produce similar embeddings while unmatched pairs produce dissimilar embeddings.\n\nThe contrastive loss is:\n\n$$\n\\mathcal{L}_{\\text{CLIP}} = -\\frac{1}{2N}\\sum_{i=1}^{N}\\left[\\log\\frac{\\exp(\\mathbf{z}_i^{\\text{txt}} \\cdot \\mathbf{z}_i^{\\text{img}} / \\tau)}{\\sum_{j=1}^{N}\\exp(\\mathbf{z}_i^{\\text{txt}} \\cdot \\mathbf{z}_j^{\\text{img}} / \\tau)} + \\log\\frac{\\exp(\\mathbf{z}_i^{\\text{img}} \\cdot \\mathbf{z}_i^{\\text{txt}} / \\tau)}{\\sum_{j=1}^{N}\\exp(\\mathbf{z}_i^{\\text{img}} \\cdot \\mathbf{z}_j^{\\text{txt}} / \\tau)}\\right]\n$$ {#eq-clip-loss}\n\nwhere $\\tau$ is a learnable temperature parameter and the embeddings are $L_2$-normalized. This is a symmetric version of the InfoNCE loss [@oord2018representation] that simultaneously trains the text encoder to predict the correct image and vice versa.\n\n::: {#contrastive-alignment .cell execution_count=3}\n``` {.python .cell-code}\nclass FinancialCLIP(nn.Module):\n    \"\"\"\n    Contrastive alignment of text and image embeddings\n    for Vietnamese financial data.\n    \"\"\"\n\n    def __init__(self, text_dim=768, image_dim=2048, proj_dim=256):\n        super().__init__()\n\n        # Text projection\n        self.text_proj = nn.Sequential(\n            nn.Linear(text_dim, proj_dim),\n            nn.LayerNorm(proj_dim),\n            nn.GELU(),\n            nn.Linear(proj_dim, proj_dim)\n        )\n\n        # Image projection\n        self.image_proj = nn.Sequential(\n            nn.Linear(image_dim, proj_dim),\n            nn.LayerNorm(proj_dim),\n            nn.GELU(),\n            nn.Linear(proj_dim, proj_dim)\n        )\n\n        # Learnable temperature\n        self.log_temp = nn.Parameter(torch.tensor(np.log(1 / 0.07)))\n\n    def forward(self, text_emb, image_emb):\n        \"\"\"Compute aligned embeddings and contrastive loss.\"\"\"\n        # Project and normalize\n        z_text = F.normalize(self.text_proj(text_emb), dim=-1)\n        z_image = F.normalize(self.image_proj(image_emb), dim=-1)\n\n        # Similarity matrix\n        temp = self.log_temp.exp()\n        logits = z_text @ z_image.T * temp\n\n        # Symmetric cross-entropy loss\n        labels = torch.arange(len(text_emb), device=text_emb.device)\n        loss_t2i = F.cross_entropy(logits, labels)\n        loss_i2t = F.cross_entropy(logits.T, labels)\n\n        loss = (loss_t2i + loss_i2t) / 2\n\n        return z_text, z_image, loss\n\n    def encode_text(self, text_emb):\n        return F.normalize(self.text_proj(text_emb), dim=-1)\n\n    def encode_image(self, image_emb):\n        return F.normalize(self.image_proj(image_emb), dim=-1)\n```\n:::\n\n\n### Projection Alignment for Arbitrary Modalities\n\nFor more than two modalities, we generalize to a shared projection space where each modality has its own encoder but all encoders map to the same target space:\n\n$$\n\\mathbf{z}_i^{(m)} = f^{(m)}(\\mathbf{x}_i^{(m)}; \\boldsymbol{\\theta}^{(m)}) \\in \\mathbb{R}^d, \\qquad m = 1, \\ldots, M\n$$ {#eq-projection}\n\nThe alignment loss encourages all modality embeddings for the same observation to be similar:\n\n$$\n\\mathcal{L}_{\\text{align}} = \\sum_{m < m'} \\frac{1}{N}\\sum_{i=1}^{N} \\left\\|\\mathbf{z}_i^{(m)} - \\mathbf{z}_i^{(m')}\\right\\|^2\n$$ {#eq-alignment-loss}\n\nThis MSE alignment is simpler than contrastive alignment but does not enforce the discriminative property (different observations should have dissimilar embeddings). In practice, we combine alignment with a prediction objective:\n\n$$\n\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{predict}}(\\hat{y}, y) + \\lambda \\cdot \\mathcal{L}_{\\text{align}}\n$$ {#eq-total-loss}\n\n::: {#multi-projection .cell execution_count=4}\n``` {.python .cell-code}\nclass MultimodalProjector(nn.Module):\n    \"\"\"\n    Project arbitrary modalities into a shared latent space.\n    Supports variable numbers of modalities per observation.\n    \"\"\"\n\n    def __init__(self, modality_dims, proj_dim=128, dropout=0.2):\n        \"\"\"\n        Parameters\n        ----------\n        modality_dims : dict\n            {modality_name: input_dim}, e.g.,\n            {'text': 768, 'image': 2048, 'tabular': 50, 'ts': 128}\n        proj_dim : int\n            Shared projection dimensionality.\n        \"\"\"\n        super().__init__()\n\n        self.modality_names = list(modality_dims.keys())\n        self.proj_dim = proj_dim\n\n        # Per-modality encoders\n        self.encoders = nn.ModuleDict()\n        for name, dim in modality_dims.items():\n            self.encoders[name] = nn.Sequential(\n                nn.Linear(dim, proj_dim * 2),\n                nn.LayerNorm(proj_dim * 2),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(proj_dim * 2, proj_dim),\n                nn.LayerNorm(proj_dim)\n            )\n\n    def forward(self, modality_inputs):\n        \"\"\"\n        Parameters\n        ----------\n        modality_inputs : dict\n            {modality_name: tensor}, may be missing some modalities.\n\n        Returns\n        -------\n        dict : {modality_name: projected_embedding}\n        \"\"\"\n        embeddings = {}\n        for name, x in modality_inputs.items():\n            if name in self.encoders and x is not None:\n                embeddings[name] = self.encoders[name](x)\n\n        return embeddings\n\n    def compute_alignment_loss(self, embeddings):\n        \"\"\"Pairwise MSE alignment across all available modalities.\"\"\"\n        names = list(embeddings.keys())\n        if len(names) < 2:\n            return torch.tensor(0.0, device=next(self.parameters()).device)\n\n        loss = torch.tensor(0.0, device=next(self.parameters()).device)\n        n_pairs = 0\n        for i in range(len(names)):\n            for j in range(i + 1, len(names)):\n                loss += F.mse_loss(\n                    embeddings[names[i]], embeddings[names[j]]\n                )\n                n_pairs += 1\n\n        return loss / n_pairs if n_pairs > 0 else loss\n```\n:::\n\n\n## Fusion Architectures for Return Prediction\n\n### Unimodal Encoders\n\nBefore fusing modalities, we need encoders that produce fixed-dimensional representations from each raw input. We build four encoders corresponding to the primary modalities in Vietnamese equity markets.\n\n::: {#unimodal-encoders .cell execution_count=5}\n``` {.python .cell-code}\nclass TabularEncoder(nn.Module):\n    \"\"\"Encode financial statement features.\"\"\"\n\n    def __init__(self, input_dim, hidden_dim=128, output_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TextEncoder(nn.Module):\n    \"\"\"\n    Encode Vietnamese text using pre-extracted PhoBERT embeddings.\n    Input: pre-computed [CLS] token embedding (768-d).\n    \"\"\"\n\n    def __init__(self, input_dim=768, output_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass ImageEncoder(nn.Module):\n    \"\"\"\n    Encode satellite / document image features.\n    Input: pre-computed CNN features (e.g., ResNet50 2048-d).\n    \"\"\"\n\n    def __init__(self, input_dim=2048, output_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TimeSeriesEncoder(nn.Module):\n    \"\"\"\n    Encode price/volume time series using a 1D CNN + attention.\n    Input: (batch, seq_len, n_features) tensor of daily data.\n    \"\"\"\n\n    def __init__(self, n_features=5, seq_len=60, output_dim=64):\n        super().__init__()\n\n        # 1D convolutional layers\n        self.conv1 = nn.Conv1d(n_features, 32, kernel_size=5, padding=2)\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n\n        # Temporal attention\n        self.attn = nn.MultiheadAttention(\n            embed_dim=64, num_heads=4, batch_first=True\n        )\n\n        self.fc = nn.Linear(64, output_dim)\n\n    def forward(self, x):\n        # x: (B, T, F) -> (B, F, T) for Conv1d\n        x = x.transpose(1, 2)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n\n        # (B, 64, T) -> (B, T, 64) for attention\n        x = x.transpose(1, 2)\n        attn_out, _ = self.attn(x, x, x)\n\n        # Pool over time\n        x = attn_out.transpose(1, 2)  # (B, 64, T)\n        x = self.pool(x).squeeze(-1)  # (B, 64)\n\n        return self.fc(x)\n```\n:::\n\n\n### Early Fusion\n\nEarly fusion concatenates modality embeddings before a shared prediction head. This is the simplest approach and serves as a natural baseline.\n\n::: {#early-fusion .cell execution_count=6}\n``` {.python .cell-code}\nclass EarlyFusionModel(nn.Module):\n    \"\"\"\n    Concatenate modality embeddings, then predict.\n    \"\"\"\n\n    def __init__(self, encoders, hidden_dim=128, output_dim=1):\n        \"\"\"\n        Parameters\n        ----------\n        encoders : dict\n            {modality_name: encoder_module}\n            Each encoder outputs a vector of the same dimension.\n        \"\"\"\n        super().__init__()\n        self.encoders = nn.ModuleDict(encoders)\n        self.n_modalities = len(encoders)\n\n        # Infer encoder output dim from first encoder\n        sample_encoder = list(encoders.values())[0]\n        enc_dim = list(sample_encoder.parameters())[-1].shape[0]\n\n        self.head = nn.Sequential(\n            nn.Linear(enc_dim * self.n_modalities, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, output_dim)\n        )\n\n    def forward(self, inputs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs : dict\n            {modality_name: tensor}\n        \"\"\"\n        embeddings = []\n        for name, encoder in self.encoders.items():\n            if name in inputs and inputs[name] is not None:\n                embeddings.append(encoder(inputs[name]))\n            else:\n                # Zero-fill missing modalities\n                device = next(self.parameters()).device\n                enc_dim = list(encoder.parameters())[-1].shape[0]\n                embeddings.append(torch.zeros(\n                    inputs[list(inputs.keys())[0]].shape[0],\n                    enc_dim, device=device\n                ))\n\n        combined = torch.cat(embeddings, dim=-1)\n        return self.head(combined).squeeze(-1)\n```\n:::\n\n\n### Late Fusion\n\nLate fusion trains independent models per modality and combines their predictions. The combination weights can be fixed (equal averaging), learned (linear), or adaptive (gating network).\n\n::: {#late-fusion .cell execution_count=7}\n``` {.python .cell-code}\nclass LateFusionModel(nn.Module):\n    \"\"\"\n    Independent prediction per modality, learned combination.\n    \"\"\"\n\n    def __init__(self, encoders, enc_dim=64, combination=\"learned\"):\n        \"\"\"\n        Parameters\n        ----------\n        combination : str\n            'average', 'learned', or 'gating'.\n        \"\"\"\n        super().__init__()\n        self.encoders = nn.ModuleDict(encoders)\n        self.combination = combination\n        self.n_modalities = len(encoders)\n\n        # Per-modality prediction heads\n        self.heads = nn.ModuleDict({\n            name: nn.Linear(enc_dim, 1)\n            for name in encoders\n        })\n\n        if combination == \"learned\":\n            self.weights = nn.Parameter(\n                torch.ones(self.n_modalities) / self.n_modalities\n            )\n        elif combination == \"gating\":\n            # Gating network takes all embeddings as input\n            self.gate = nn.Sequential(\n                nn.Linear(enc_dim * self.n_modalities, self.n_modalities),\n                nn.Softmax(dim=-1)\n            )\n\n    def forward(self, inputs):\n        predictions = {}\n        embeddings = {}\n\n        for name, encoder in self.encoders.items():\n            if name in inputs and inputs[name] is not None:\n                emb = encoder(inputs[name])\n                pred = self.heads[name](emb).squeeze(-1)\n                predictions[name] = pred\n                embeddings[name] = emb\n            else:\n                device = next(self.parameters()).device\n                batch_size = inputs[list(inputs.keys())[0]].shape[0]\n                predictions[name] = torch.zeros(batch_size, device=device)\n                enc_dim = list(encoder.parameters())[-1].shape[0]\n                embeddings[name] = torch.zeros(\n                    batch_size, enc_dim, device=device\n                )\n\n        pred_stack = torch.stack(list(predictions.values()), dim=-1)\n\n        if self.combination == \"average\":\n            return pred_stack.mean(dim=-1)\n        elif self.combination == \"learned\":\n            weights = F.softmax(self.weights, dim=0)\n            return (pred_stack * weights).sum(dim=-1)\n        elif self.combination == \"gating\":\n            all_emb = torch.cat(list(embeddings.values()), dim=-1)\n            gate_weights = self.gate(all_emb)\n            return (pred_stack * gate_weights).sum(dim=-1)\n\n    def get_modality_weights(self):\n        \"\"\"Return the contribution of each modality.\"\"\"\n        if self.combination == \"learned\":\n            return F.softmax(self.weights, dim=0).detach().cpu().numpy()\n        return None\n```\n:::\n\n\n### Cross-Attention Fusion\n\nCross-attention fusion is the most expressive architecture. Each modality attends to every other modality, learning which cross-modal interactions are informative. This is the mechanism underlying modern vision-language models like Flamingo [@alayrac2022flamingo] and GPT-4V.\n\nThe cross-attention operation for modality $m$ attending to modality $m'$ is:\n\n$$\n\\text{CA}^{(m \\to m')} = \\text{softmax}\\left(\\frac{\\mathbf{Q}^{(m)} \\left(\\mathbf{K}^{(m')}\\right)^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V}^{(m')}\n$$ {#eq-cross-attention-detail}\n\nwhere $\\mathbf{Q}^{(m)} = \\mathbf{z}^{(m)} W_Q$, $\\mathbf{K}^{(m')} = \\mathbf{z}^{(m')} W_K$, $\\mathbf{V}^{(m')} = \\mathbf{z}^{(m')} W_V$. The output enriches modality $m$'s representation with information from modality $m'$.\n\n::: {#cross-attention-fusion .cell execution_count=8}\n``` {.python .cell-code}\nclass CrossAttentionBlock(nn.Module):\n    \"\"\"Single cross-attention block: query modality attends to key modality.\"\"\"\n\n    def __init__(self, dim, n_heads=4, dropout=0.1):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(\n            embed_dim=dim, num_heads=n_heads,\n            dropout=dropout, batch_first=True\n        )\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * 4, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, query, key_value):\n        # Cross-attention\n        q = query.unsqueeze(1) if query.dim() == 2 else query\n        kv = key_value.unsqueeze(1) if key_value.dim() == 2 else key_value\n\n        attn_out, attn_weights = self.attn(q, kv, kv)\n        q = self.norm1(q + attn_out)\n\n        # Feed-forward\n        out = self.norm2(q + self.ffn(q))\n        return out.squeeze(1) if query.dim() == 2 else out, attn_weights\n\n\nclass CrossAttentionFusionModel(nn.Module):\n    \"\"\"\n    Full cross-attention fusion across M modalities.\n    Each modality attends to all others via cross-attention blocks.\n    \"\"\"\n\n    def __init__(self, encoders, enc_dim=64, n_layers=2, n_heads=4):\n        super().__init__()\n        self.encoders = nn.ModuleDict(encoders)\n        self.modality_names = list(encoders.keys())\n        self.n_modalities = len(encoders)\n\n        # Cross-attention blocks: each modality attends to each other\n        self.cross_attn_layers = nn.ModuleList()\n        for _ in range(n_layers):\n            layer = nn.ModuleDict()\n            for m in self.modality_names:\n                for m_prime in self.modality_names:\n                    if m != m_prime:\n                        layer[f\"{m}_to_{m_prime}\"] = CrossAttentionBlock(\n                            enc_dim, n_heads\n                        )\n            self.cross_attn_layers.append(layer)\n\n        # Prediction head\n        self.head = nn.Sequential(\n            nn.Linear(enc_dim * self.n_modalities, enc_dim),\n            nn.LayerNorm(enc_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(enc_dim, 1)\n        )\n\n    def forward(self, inputs):\n        # Encode each modality\n        embeddings = {}\n        for name, encoder in self.encoders.items():\n            if name in inputs and inputs[name] is not None:\n                embeddings[name] = encoder(inputs[name])\n            else:\n                device = next(self.parameters()).device\n                batch_size = inputs[list(inputs.keys())[0]].shape[0]\n                enc_dim = list(encoder.parameters())[-1].shape[0]\n                embeddings[name] = torch.zeros(\n                    batch_size, enc_dim, device=device\n                )\n\n        # Cross-attention layers\n        all_attn_weights = {}\n        for layer in self.cross_attn_layers:\n            new_embeddings = {k: v.clone() for k, v in embeddings.items()}\n            for key, block in layer.items():\n                parts = key.split(\"_to_\")\n                query_mod, kv_mod = parts[0], parts[1]\n                if query_mod in embeddings and kv_mod in embeddings:\n                    updated, weights = block(\n                        embeddings[query_mod],\n                        embeddings[kv_mod]\n                    )\n                    new_embeddings[query_mod] = (\n                        new_embeddings[query_mod] + updated\n                    )\n                    all_attn_weights[key] = weights\n\n            embeddings = new_embeddings\n\n        # Concatenate and predict\n        combined = torch.cat(\n            [embeddings[name] for name in self.modality_names],\n            dim=-1\n        )\n        return self.head(combined).squeeze(-1), all_attn_weights\n```\n:::\n\n\n### Comparison Experiment\n\nWe now compare the three fusion architectures against unimodal baselines on forward quarterly return prediction for Vietnamese equities.\n\n::: {#multimodal-dataset .cell execution_count=9}\n``` {.python .cell-code}\nclass MultimodalFinanceDataset(Dataset):\n    \"\"\"\n    Dataset that aligns multiple modalities per firm-quarter.\n    Handles missing modalities with None values.\n    \"\"\"\n\n    def __init__(self, tabular_df, text_embeddings, image_features,\n                 ts_features, returns, tickers, dates):\n        self.tabular = tabular_df\n        self.text = text_embeddings\n        self.image = image_features\n        self.ts = ts_features\n        self.returns = returns\n        self.tickers = tickers\n        self.dates = dates\n\n    def __len__(self):\n        return len(self.returns)\n\n    def __getitem__(self, idx):\n        sample = {\n            \"tabular\": torch.tensor(\n                self.tabular[idx], dtype=torch.float32\n            ) if self.tabular[idx] is not None else None,\n            \"text\": torch.tensor(\n                self.text[idx], dtype=torch.float32\n            ) if self.text[idx] is not None else None,\n            \"image\": torch.tensor(\n                self.image[idx], dtype=torch.float32\n            ) if self.image[idx] is not None else None,\n            \"ts\": torch.tensor(\n                self.ts[idx], dtype=torch.float32\n            ) if self.ts[idx] is not None else None,\n            \"return\": torch.tensor(\n                self.returns[idx], dtype=torch.float32\n            ),\n            \"ticker\": self.tickers[idx],\n            \"date\": self.dates[idx]\n        }\n        return sample\n\n\ndef collate_multimodal(batch):\n    \"\"\"Custom collate that handles None modalities.\"\"\"\n    result = {\"return\": torch.stack([b[\"return\"] for b in batch])}\n\n    for mod in [\"tabular\", \"text\", \"image\", \"ts\"]:\n        values = [b[mod] for b in batch]\n        if all(v is not None for v in values):\n            result[mod] = torch.stack(values)\n        elif any(v is not None for v in values):\n            # Fill None with zeros, matching shape of non-None entries\n            ref = next(v for v in values if v is not None)\n            filled = [v if v is not None else torch.zeros_like(ref)\n                      for v in values]\n            result[mod] = torch.stack(filled)\n        else:\n            result[mod] = None\n\n    return result\n```\n:::\n\n\n::: {#prepare-aligned-data .cell execution_count=10}\n``` {.python .cell-code}\n# Prepare aligned firm-quarter dataset\n# Step 1: Financial ratios (tabular)\ntabular_features = [\n    \"roe\", \"roa\", \"book_to_market\", \"log_size\", \"leverage\",\n    \"asset_growth\", \"gross_profitability\", \"capex_to_assets\",\n    \"cash_to_assets\", \"dividend_yield\", \"sales_growth\",\n    \"accruals\", \"earnings_volatility\", \"beta\"\n]\n\nfinancials[\"quarter_date\"] = pd.to_datetime(\n    financials[\"year\"].astype(str) + \"-\" +\n    (financials[\"quarter\"] * 3).astype(str).str.zfill(2) + \"-01\"\n)\n\n# Step 2: Text embeddings from PhoBERT\n# (Pre-computed in Chapter 60)\ntext_emb = dc.get_text_embeddings(\n    model=\"phobert\",\n    section=\"management_discussion\",\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Step 3: Image features (pre-computed in Chapter 61)\n# Satellite CNN features linked to firm headquarters province\n\n# Step 4: Time series features (60-day window before quarter end)\ndef compute_ts_features(ticker, date, daily_df, lookback=60):\n    \"\"\"Extract time-series feature tensor for a firm-quarter.\"\"\"\n    mask = (\n        (daily_df[\"ticker\"] == ticker) &\n        (daily_df[\"date\"] <= date) &\n        (daily_df[\"date\"] >= date - pd.Timedelta(days=lookback * 1.5))\n    )\n    subset = daily_df[mask].sort_values(\"date\").tail(lookback)\n\n    if len(subset) < lookback // 2:\n        return None\n\n    features = subset[[\"ret\", \"volume_log\", \"volatility_20d\",\n                        \"spread\", \"turnover\"]].values\n\n    # Pad if shorter than lookback\n    if len(features) < lookback:\n        padding = np.zeros((lookback - len(features), features.shape[1]))\n        features = np.vstack([padding, features])\n\n    return features\n\n# Step 5: Forward quarterly returns (target)\n# Align everything to quarter-end dates\nprint(\"Preparing aligned multimodal dataset...\")\n```\n:::\n\n\n::: {#training-loop .cell execution_count=11}\n``` {.python .cell-code}\ndef train_multimodal_model(model, train_loader, val_loader,\n                            n_epochs=50, lr=1e-3, patience=10,\n                            alignment_weight=0.0):\n    \"\"\"\n    Train a multimodal model with early stopping.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Multimodal fusion model.\n    alignment_weight : float\n        Weight for modality alignment loss (0 = no alignment).\n\n    Returns\n    -------\n    dict : Training history and best validation metrics.\n    \"\"\"\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n                                   weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", patience=5, factor=0.5\n    )\n\n    best_val_loss = float(\"inf\")\n    epochs_no_improve = 0\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_r2\": []}\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            optimizer.zero_grad()\n\n            inputs = {k: batch[k] for k in [\"tabular\", \"text\", \"image\", \"ts\"]}\n            targets = batch[\"return\"]\n\n            # Forward pass (handle both output types)\n            output = model(inputs)\n            if isinstance(output, tuple):\n                predictions, attn_weights = output\n            else:\n                predictions = output\n\n            loss = F.mse_loss(predictions, targets)\n\n            # Optional alignment loss\n            if alignment_weight > 0 and hasattr(model, \"projector\"):\n                embeddings = model.projector(inputs)\n                align_loss = model.projector.compute_alignment_loss(embeddings)\n                loss = loss + alignment_weight * align_loss\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            train_losses.append(loss.item())\n\n        # Validation\n        model.eval()\n        val_preds, val_targets = [], []\n        val_losses = []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {k: batch[k]\n                          for k in [\"tabular\", \"text\", \"image\", \"ts\"]}\n                targets = batch[\"return\"]\n\n                output = model(inputs)\n                if isinstance(output, tuple):\n                    predictions, _ = output\n                else:\n                    predictions = output\n\n                val_losses.append(F.mse_loss(predictions, targets).item())\n                val_preds.extend(predictions.cpu().numpy())\n                val_targets.extend(targets.cpu().numpy())\n\n        val_loss = np.mean(val_losses)\n        val_r2 = r2_score(val_targets, val_preds) if len(val_preds) > 10 else 0\n\n        history[\"train_loss\"].append(np.mean(train_losses))\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_r2\"].append(val_r2)\n\n        scheduler.step(val_loss)\n\n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            best_state = {k: v.cpu().clone()\n                         for k, v in model.state_dict().items()}\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                break\n\n    # Restore best model\n    model.load_state_dict(best_state)\n\n    return {\n        \"history\": history,\n        \"best_val_loss\": best_val_loss,\n        \"best_val_r2\": max(history[\"val_r2\"]),\n        \"epochs_trained\": len(history[\"train_loss\"])\n    }\n```\n:::\n\n\n::: {#model-comparison .cell execution_count=12}\n``` {.python .cell-code}\ndef compare_fusion_strategies(dataset, n_splits=5):\n    \"\"\"\n    Compare unimodal baselines and multimodal fusion strategies\n    using expanding-window time-series cross-validation.\n\n    Returns\n    -------\n    DataFrame : Out-of-sample R², MSE, IC for each model.\n    \"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = []\n\n    enc_dim = 64\n\n    for fold, (train_idx, test_idx) in enumerate(\n        tscv.split(range(len(dataset)))\n    ):\n        # Create data loaders\n        train_subset = torch.utils.data.Subset(dataset, train_idx)\n        test_subset = torch.utils.data.Subset(dataset, test_idx)\n\n        train_loader = DataLoader(\n            train_subset, batch_size=128, shuffle=True,\n            collate_fn=collate_multimodal\n        )\n        test_loader = DataLoader(\n            test_subset, batch_size=256, shuffle=False,\n            collate_fn=collate_multimodal\n        )\n\n        # Define encoders\n        def make_encoders():\n            return {\n                \"tabular\": TabularEncoder(len(tabular_features), 128, enc_dim),\n                \"text\": TextEncoder(768, enc_dim),\n                \"image\": ImageEncoder(2048, enc_dim),\n                \"ts\": TimeSeriesEncoder(5, 60, enc_dim)\n            }\n\n        # Unimodal baselines\n        for mod_name in [\"tabular\", \"text\", \"image\", \"ts\"]:\n            single_encoder = {mod_name: make_encoders()[mod_name]}\n            model = EarlyFusionModel(single_encoder, enc_dim, 1)\n\n            result = train_multimodal_model(\n                model, train_loader, test_loader, n_epochs=30\n            )\n            results.append({\n                \"fold\": fold,\n                \"model\": f\"Unimodal ({mod_name})\",\n                \"val_r2\": result[\"best_val_r2\"],\n                \"val_loss\": result[\"best_val_loss\"]\n            })\n\n        # Multimodal: Early Fusion\n        model_early = EarlyFusionModel(make_encoders(), enc_dim * 2, 1)\n        result = train_multimodal_model(\n            model_early, train_loader, test_loader, n_epochs=30\n        )\n        results.append({\n            \"fold\": fold,\n            \"model\": \"Early Fusion\",\n            \"val_r2\": result[\"best_val_r2\"],\n            \"val_loss\": result[\"best_val_loss\"]\n        })\n\n        # Multimodal: Late Fusion (gating)\n        model_late = LateFusionModel(\n            make_encoders(), enc_dim, combination=\"gating\"\n        )\n        result = train_multimodal_model(\n            model_late, train_loader, test_loader, n_epochs=30\n        )\n        results.append({\n            \"fold\": fold,\n            \"model\": \"Late Fusion (Gating)\",\n            \"val_r2\": result[\"best_val_r2\"],\n            \"val_loss\": result[\"best_val_loss\"]\n        })\n\n        # Multimodal: Cross-Attention\n        model_ca = CrossAttentionFusionModel(\n            make_encoders(), enc_dim, n_layers=2, n_heads=4\n        )\n        result = train_multimodal_model(\n            model_ca, train_loader, test_loader, n_epochs=30\n        )\n        results.append({\n            \"fold\": fold,\n            \"model\": \"Cross-Attention Fusion\",\n            \"val_r2\": result[\"best_val_r2\"],\n            \"val_loss\": result[\"best_val_loss\"]\n        })\n\n    return pd.DataFrame(results)\n```\n:::\n\n\n::: {#tbl-fusion-comparison .cell tbl-cap='Out-of-Sample Return Prediction: Unimodal vs. Multimodal' execution_count=13}\n``` {.python .cell-code}\n# results_df = compare_fusion_strategies(dataset)\n\n# Aggregate across folds\n# summary = (\n#     results_df.groupby(\"model\")\n#     .agg(\n#         mean_r2=(\"val_r2\", \"mean\"),\n#         std_r2=(\"val_r2\", \"std\"),\n#         mean_loss=(\"val_loss\", \"mean\")\n#     )\n#     .sort_values(\"mean_r2\", ascending=False)\n#     .round(4)\n# )\n# summary\n```\n:::\n\n\n::: {#fig-fusion-comparison .cell execution_count=14}\n``` {.python .cell-code}\n# (\n#     p9.ggplot(results_df, p9.aes(x=\"model\", y=\"val_r2\", fill=\"model\"))\n#     + p9.geom_boxplot(alpha=0.7)\n#     + p9.coord_flip()\n#     + p9.labs(\n#         x=\"\", y=\"Out-of-Sample R²\",\n#         title=\"Multimodal Fusion Improves Return Prediction\"\n#     )\n#     + p9.theme_minimal()\n#     + p9.theme(figure_size=(10, 6), legend_position=\"none\")\n# )\n```\n:::\n\n\n## Handling Missing Modalities\n\n### The Missing Modality Problem\n\nIn practice, not every firm-quarter has every modality available. A firm may not have an earnings call transcript (no audio), its headquarters may be in a province where satellite coverage is intermittent (no image), or its annual report may not be publicly available in digital form (no text). This creates a missing modality problem that is structurally different from missing values in tabular data: an entire feature vector (hundreds or thousands of dimensions) is absent.\n\nThe fraction of observations with all four modalities available is typically much smaller than the fraction with at least one:\n\n| Available Modalities     | Typical Coverage (Vietnamese Firms) |\n|--------------------------|-------------------------------------|\n| Tabular only             | $\\sim$ 95% of firm-quarters         |\n| Tabular + Text           | $\\sim$ 70%                          |\n| Tabular + Text + Image   | $\\sim$ 50%                          |\n| All four (+ time series) | $\\sim$ 45%                          |\n\n: Modality Availability in Vietnamese Market Data {#tbl-modality-coverage}\n\nRestricting the sample to complete cases discards half the data and introduces selection bias (larger, more transparent firms are overrepresented). We need architectures that degrade gracefully when modalities are missing.\n\n### Strategies for Missing Modalities\n\n-   **Zero imputation.** Replace missing modality embeddings with zeros. Simple but introduces bias: the model cannot distinguish \"this modality is absent\" from \"this modality has zero signal.\"\n\n-   **Learned default embedding.** Replace missing modalities with a learnable \"default\" vector $\\mathbf{d}^{(m)}$ that is trained alongside the model. This allows the model to learn what the absence of a modality implies.\n\n-   **Modality dropout.** During training, randomly drop entire modalities with probability $p$ (analogous to dropout on neurons). This forces the model to perform well even when modalities are missing, and acts as regularization.\n\n-   **Mixture of Experts (MoE).** Route each observation to a fusion subnetwork specialized for its available modality combination. With $M$ modalities, there are $2^M - 1$ possible subsets, requiring efficient parameter sharing.\n\n::: {#missing-modality-handler .cell execution_count=15}\n``` {.python .cell-code}\nclass ModalityDropout(nn.Module):\n    \"\"\"\n    Randomly drop entire modalities during training.\n    Forces robustness to missing inputs at test time.\n    \"\"\"\n\n    def __init__(self, drop_prob=0.2):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, modality_inputs):\n        if not self.training:\n            return modality_inputs\n\n        result = {}\n        for name, tensor in modality_inputs.items():\n            if tensor is not None and torch.rand(1).item() > self.drop_prob:\n                result[name] = tensor\n            else:\n                result[name] = None\n\n        # Ensure at least one modality remains\n        if all(v is None for v in result.values()):\n            # Keep the first available modality\n            for name, tensor in modality_inputs.items():\n                if tensor is not None:\n                    result[name] = tensor\n                    break\n\n        return result\n\n\nclass RobustFusionModel(nn.Module):\n    \"\"\"\n    Multimodal model robust to missing modalities.\n    Uses learned default embeddings and modality dropout.\n    \"\"\"\n\n    def __init__(self, encoders, enc_dim=64, drop_prob=0.2):\n        super().__init__()\n        self.encoders = nn.ModuleDict(encoders)\n        self.modality_names = list(encoders.keys())\n        self.n_modalities = len(encoders)\n        self.enc_dim = enc_dim\n\n        # Learned default embeddings for missing modalities\n        self.defaults = nn.ParameterDict({\n            name: nn.Parameter(torch.randn(enc_dim) * 0.01)\n            for name in encoders\n        })\n\n        # Modality presence indicator embedding\n        self.presence_proj = nn.Linear(self.n_modalities, enc_dim)\n\n        # Modality dropout\n        self.mod_dropout = ModalityDropout(drop_prob)\n\n        # Attention-based aggregation\n        self.attn_pool = nn.Sequential(\n            nn.Linear(enc_dim, 1),\n            nn.Softmax(dim=0)\n        )\n\n        # Prediction head\n        self.head = nn.Sequential(\n            nn.Linear(enc_dim * 2, enc_dim),\n            nn.LayerNorm(enc_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(enc_dim, 1)\n        )\n\n    def forward(self, inputs):\n        # Apply modality dropout during training\n        inputs = self.mod_dropout(inputs)\n\n        embeddings = []\n        presence = []\n\n        for name in self.modality_names:\n            if name in inputs and inputs[name] is not None:\n                emb = self.encoders[name](inputs[name])\n                embeddings.append(emb)\n                presence.append(1.0)\n            else:\n                batch_size = next(\n                    v.shape[0] for v in inputs.values()\n                    if v is not None\n                )\n                emb = self.defaults[name].unsqueeze(0).expand(\n                    batch_size, -1\n                )\n                embeddings.append(emb)\n                presence.append(0.0)\n\n        # Stack: (n_modalities, batch, enc_dim)\n        emb_stack = torch.stack(embeddings, dim=0)\n\n        # Attention-weighted aggregation\n        attn_weights = self.attn_pool(emb_stack)  # (n_mod, batch, 1)\n        aggregated = (emb_stack * attn_weights).sum(dim=0)  # (batch, enc_dim)\n\n        # Presence indicator\n        device = aggregated.device\n        presence_tensor = torch.tensor(\n            presence, device=device\n        ).unsqueeze(0).expand(aggregated.shape[0], -1)\n        presence_emb = self.presence_proj(presence_tensor)\n\n        # Combine\n        combined = torch.cat([aggregated, presence_emb], dim=-1)\n        return self.head(combined).squeeze(-1)\n```\n:::\n\n\n## Multimodal Document Understanding\n\n### Annual Report as a Multimodal Object\n\nA Vietnamese annual report is inherently multimodal: it contains running text (management discussion, risk factors, strategy), tables (financial statements, segment data, shareholder structure), images (photographs of facilities, products, management), and charts (revenue trends, market share). Prior chapters treated these as separate extraction problems. Here we build a model that processes the entire report as a unified multimodal document.\n\nThe architecture follows the Document Understanding Transformer (Donut) approach of @kim2022ocr, adapted for Vietnamese financial filings:\n\n$$\n\\mathbf{h} = \\text{Encoder}(\\mathbf{I}_{\\text{page}}) + \\text{Encoder}(\\mathbf{T}_{\\text{ocr}}) + \\text{Encoder}(\\mathbf{L}_{\\text{layout}})\n$$ {#eq-donut}\n\nwhere $\\mathbf{I}$ is the page image, $\\mathbf{T}$ is the OCR text, and $\\mathbf{L}$ is the spatial layout (bounding boxes). The joint representation $\\mathbf{h}$ captures both what is written and where it appears on the page.\n\n::: {#multimodal-document .cell execution_count=16}\n``` {.python .cell-code}\nclass MultimodalDocumentEncoder(nn.Module):\n    \"\"\"\n    Joint encoder for Vietnamese annual report pages.\n    Processes text, layout, and page image simultaneously.\n    \"\"\"\n\n    def __init__(self, vocab_size=64000, max_boxes=512,\n                 img_dim=2048, hidden_dim=256, n_layers=4,\n                 n_heads=8):\n        super().__init__()\n\n        # Text embedding (Vietnamese tokens)\n        self.text_emb = nn.Embedding(vocab_size, hidden_dim)\n\n        # Layout embedding (bounding box coordinates)\n        # Each box: [x0, y0, x1, y1] normalized to [0, 1000]\n        self.x_emb = nn.Embedding(1001, hidden_dim // 4)\n        self.y_emb = nn.Embedding(1001, hidden_dim // 4)\n\n        # Image patch embedding\n        self.img_proj = nn.Sequential(\n            nn.Linear(img_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim)\n        )\n\n        # Modality type embedding\n        self.modality_emb = nn.Embedding(3, hidden_dim)  # text, layout, image\n\n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=n_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=0.1,\n            activation=\"gelu\",\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(\n            encoder_layer, num_layers=n_layers\n        )\n\n        # [CLS] token\n        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n\n    def embed_layout(self, boxes):\n        \"\"\"Embed bounding box coordinates.\"\"\"\n        x0 = self.x_emb(boxes[:, :, 0])\n        y0 = self.y_emb(boxes[:, :, 1])\n        x1 = self.x_emb(boxes[:, :, 2])\n        y1 = self.y_emb(boxes[:, :, 3])\n        return torch.cat([x0, y0, x1, y1], dim=-1)\n\n    def forward(self, token_ids, boxes, img_features,\n                attention_mask=None):\n        \"\"\"\n        Parameters\n        ----------\n        token_ids : LongTensor (B, T)\n            OCR token IDs.\n        boxes : LongTensor (B, T, 4)\n            Bounding boxes for each token.\n        img_features : Tensor (B, P, img_dim)\n            Image patch features from CNN.\n        \"\"\"\n        batch_size = token_ids.shape[0]\n\n        # Text + layout\n        text_h = self.text_emb(token_ids) + self.embed_layout(boxes)\n        text_h = text_h + self.modality_emb(\n            torch.zeros(batch_size, text_h.shape[1],\n                       dtype=torch.long, device=text_h.device)\n        )\n\n        # Image patches\n        img_h = self.img_proj(img_features)\n        img_h = img_h + self.modality_emb(\n            torch.full((batch_size, img_h.shape[1]), 2,\n                      dtype=torch.long, device=img_h.device)\n        )\n\n        # Prepend [CLS]\n        cls = self.cls_token.expand(batch_size, -1, -1)\n\n        # Concatenate all modalities\n        sequence = torch.cat([cls, text_h, img_h], dim=1)\n\n        # Transformer encoding\n        output = self.transformer(sequence)\n\n        # Return [CLS] representation\n        return output[:, 0, :]\n```\n:::\n\n\n### Extracting Structured Financials from Multimodal Reports\n\nWith the document encoder, we can build extraction heads for specific financial fields. The key advantage over the OCR-only pipeline in previous chapter is that the multimodal encoder can resolve ambiguities using visual context (e.g., a number's meaning depends on where it appears on the page and what headers and labels surround it).\n\n::: {#financial-extraction-head .cell execution_count=17}\n``` {.python .cell-code}\nclass FinancialFieldExtractor(nn.Module):\n    \"\"\"\n    Extract specific financial fields from a document embedding.\n    Uses the multimodal document encoder as backbone.\n    \"\"\"\n\n    def __init__(self, doc_encoder, fields, hidden_dim=256):\n        \"\"\"\n        Parameters\n        ----------\n        doc_encoder : MultimodalDocumentEncoder\n        fields : list\n            Target field names, e.g.,\n            ['revenue', 'net_income', 'total_assets', 'total_equity']\n        \"\"\"\n        super().__init__()\n        self.doc_encoder = doc_encoder\n        self.fields = fields\n\n        # Per-field extraction heads\n        self.extractors = nn.ModuleDict({\n            field: nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim // 2),\n                nn.GELU(),\n                nn.Linear(hidden_dim // 2, 1)\n            )\n            for field in fields\n        })\n\n        # Confidence head\n        self.confidence = nn.ModuleDict({\n            field: nn.Sequential(\n                nn.Linear(hidden_dim, 1),\n                nn.Sigmoid()\n            )\n            for field in fields\n        })\n\n    def forward(self, token_ids, boxes, img_features):\n        doc_emb = self.doc_encoder(token_ids, boxes, img_features)\n\n        results = {}\n        for field in self.fields:\n            value = self.extractors[field](doc_emb).squeeze(-1)\n            conf = self.confidence[field](doc_emb).squeeze(-1)\n            results[field] = {\"value\": value, \"confidence\": conf}\n\n        return results\n```\n:::\n\n\n## Multimodal Earnings Surprise Model\n\n### Architecture\n\nWe now build the chapter's central empirical application: a multimodal model that predicts earnings surprises using all available modalities observed before the earnings announcement date.\n\nThe information set at time $t^-$ (just before the announcement) includes:\n\n-   **Tabular**: Last reported financial ratios, analyst consensus forecasts\n-   **Text**: News articles and filings in the pre-announcement window\n-   **Image**: Satellite features of the firm's operating region\n-   **Time series**: Price and volume dynamics in the 60 trading days before announcement\n\nThe target is the standardized unexpected earnings (SUE):\n\n$$\n\\text{SUE}_{i,q} = \\frac{E_{i,q} - \\hat{E}_{i,q}}{\\sigma_{i,q}}\n$$ {#eq-sue}\n\nwhere $E_{i,q}$ is actual earnings per share, $\\hat{E}_{i,q}$ is the consensus forecast (or seasonal random walk forecast if analyst coverage is absent), and $\\sigma_{i,q}$ is the standard deviation of forecast errors.\n\n::: {#earnings-surprise-data .cell execution_count=18}\n``` {.python .cell-code}\n# Construct earnings surprise dataset\nearnings = dc.get_earnings_announcements(\n    start_date=\"2016-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Standardized Unexpected Earnings\nearnings[\"sue\"] = (\n    (earnings[\"actual_eps\"] - earnings[\"consensus_eps\"]) /\n    earnings[\"forecast_std\"].clip(lower=0.01)\n)\n\n# Pre-announcement features\n# Text: aggregate PhoBERT sentiment of news in [-30, -1] window\npre_ann_text = dc.get_pre_announcement_text_features(\n    start_date=\"2016-01-01\",\n    end_date=\"2024-12-31\",\n    window_days=30,\n    model=\"phobert\"\n)\n\n# Image: satellite features at quarter end\npre_ann_image = satellite_features.copy()\n\n# Time series: 60 trading days before announcement\n# (Pre-computed above)\n\n# Tabular: most recent quarterly financials\npre_ann_tabular = financials[tabular_features + [\"ticker\", \"quarter_date\"]]\n\nprint(f\"Earnings announcements: {len(earnings)}\")\nprint(f\"With text features: {len(pre_ann_text)}\")\n```\n:::\n\n\n::: {#earnings-model .cell execution_count=19}\n``` {.python .cell-code}\nclass MultimodalEarningsSurpriseModel(nn.Module):\n    \"\"\"\n    Predict standardized unexpected earnings (SUE) from\n    multimodal pre-announcement information.\n    \"\"\"\n\n    def __init__(self, tab_dim, text_dim=768, img_dim=2048,\n                 ts_features=5, ts_len=60, hidden_dim=64,\n                 n_heads=4, drop_prob=0.2):\n        super().__init__()\n\n        # Unimodal encoders\n        self.tab_enc = TabularEncoder(tab_dim, 128, hidden_dim)\n        self.text_enc = TextEncoder(text_dim, hidden_dim)\n        self.img_enc = ImageEncoder(img_dim, hidden_dim)\n        self.ts_enc = TimeSeriesEncoder(ts_features, ts_len, hidden_dim)\n\n        # Modality dropout\n        self.mod_dropout = ModalityDropout(drop_prob)\n\n        # Cross-attention: text attends to time series\n        # (news context informs price dynamics interpretation)\n        self.text_ts_attn = CrossAttentionBlock(hidden_dim, n_heads)\n\n        # Cross-attention: tabular attends to image\n        # (financial ratios contextualized by physical activity)\n        self.tab_img_attn = CrossAttentionBlock(hidden_dim, n_heads)\n\n        # Modality importance weights (learned)\n        self.importance = nn.Parameter(torch.ones(4))\n\n        # Prediction head\n        self.head = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.GELU(),\n            nn.Linear(hidden_dim // 2, 1)\n        )\n\n    def forward(self, tabular, text, image, ts):\n        # Encode each modality\n        h_tab = self.tab_enc(tabular) if tabular is not None else None\n        h_txt = self.text_enc(text) if text is not None else None\n        h_img = self.img_enc(image) if image is not None else None\n        h_ts = self.ts_enc(ts) if ts is not None else None\n\n        # Cross-attention pairs (if both available)\n        if h_txt is not None and h_ts is not None:\n            h_txt_enriched, _ = self.text_ts_attn(h_txt, h_ts)\n        else:\n            h_txt_enriched = h_txt\n\n        if h_tab is not None and h_img is not None:\n            h_tab_enriched, _ = self.tab_img_attn(h_tab, h_img)\n        else:\n            h_tab_enriched = h_tab\n\n        # Weighted combination of available modalities\n        embeddings = []\n        weights = F.softmax(self.importance, dim=0)\n\n        for i, h in enumerate([h_tab_enriched, h_txt_enriched,\n                                h_img, h_ts]):\n            if h is not None:\n                embeddings.append(h * weights[i])\n            else:\n                device = next(self.parameters()).device\n                batch_size = next(\n                    x.shape[0] for x in [tabular, text, image, ts]\n                    if x is not None\n                )\n                embeddings.append(\n                    torch.zeros(batch_size, h_tab.shape[-1]\n                               if h_tab is not None else 64,\n                               device=device)\n                )\n\n        # Aggregate\n        stacked = torch.stack(embeddings, dim=0)\n        aggregated = stacked.sum(dim=0)\n\n        # Also compute variance across modalities (disagreement signal)\n        if stacked.shape[0] > 1:\n            disagreement = stacked.var(dim=0)\n        else:\n            disagreement = torch.zeros_like(aggregated)\n\n        combined = torch.cat([aggregated, disagreement], dim=-1)\n        return self.head(combined).squeeze(-1)\n```\n:::\n\n\n### Modality Importance Analysis\n\nA key interpretability question is: which modality contributes most to earnings surprise prediction? We analyze the learned importance weights and conduct ablation experiments.\n\n::: {#modality-importance .cell execution_count=20}\n``` {.python .cell-code}\ndef ablation_study(model, test_loader, modality_names):\n    \"\"\"\n    Measure each modality's contribution via leave-one-out ablation.\n\n    For each modality m, zero out that modality's input and measure\n    the degradation in prediction accuracy.\n\n    Returns\n    -------\n    DataFrame : Modality, R² with all, R² without, Δ R².\n    \"\"\"\n    model.eval()\n\n    # Full model performance\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = {k: batch[k] for k in modality_names}\n            targets = batch[\"return\"]\n\n            output = model(inputs)\n            pred = output[0] if isinstance(output, tuple) else output\n            all_preds.extend(pred.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    r2_full = r2_score(all_targets, all_preds)\n\n    # Ablation: remove one modality at a time\n    results = [{\"modality\": \"All\", \"r2\": r2_full, \"delta_r2\": 0.0}]\n\n    for drop_mod in modality_names:\n        ablated_preds = []\n        with torch.no_grad():\n            for batch in test_loader:\n                inputs = {}\n                for k in modality_names:\n                    if k == drop_mod:\n                        inputs[k] = None  # Remove this modality\n                    else:\n                        inputs[k] = batch[k]\n\n                targets = batch[\"return\"]\n                output = model(inputs)\n                pred = output[0] if isinstance(output, tuple) else output\n                ablated_preds.extend(pred.cpu().numpy())\n\n        r2_ablated = r2_score(all_targets, ablated_preds)\n        results.append({\n            \"modality\": f\"Without {drop_mod}\",\n            \"r2\": r2_ablated,\n            \"delta_r2\": r2_full - r2_ablated\n        })\n\n    return pd.DataFrame(results)\n```\n:::\n\n\n::: {#tbl-ablation .cell tbl-cap='Modality Ablation Study: Contribution to Earnings Surprise Prediction' execution_count=21}\n``` {.python .cell-code}\n# ablation_df = ablation_study(model, test_loader, modality_names)\n# ablation_df.round(4)\n```\n:::\n\n\n::: {#fig-modality-importance .cell execution_count=22}\n``` {.python .cell-code}\n# Track importance weights during training\n# importance_history = pd.DataFrame(...)\n\n# (\n#     p9.ggplot(importance_history, p9.aes(\n#         x=\"epoch\", y=\"weight\", color=\"modality\"\n#     ))\n#     + p9.geom_line(size=1)\n#     + p9.labs(\n#         x=\"Training Epoch\", y=\"Softmax Weight\",\n#         title=\"Modality Importance Convergence\",\n#         color=\"Modality\"\n#     )\n#     + p9.scale_color_manual(\n#         values=[\"#2E5090\", \"#C0392B\", \"#27AE60\", \"#8E44AD\"]\n#     )\n#     + p9.theme_minimal()\n#     + p9.theme(figure_size=(10, 5))\n# )\n```\n:::\n\n\n## Large Multimodal Models for Financial Analysis\n\n### Prompting Vision-Language Models\n\nThe most powerful multimodal systems available today are large vision-language models (VLMs) such as GPT-4V, Gemini, and open-source alternatives (LLaVA, InternVL). These models can jointly process images and text through natural language prompts, enabling zero-shot financial analysis without model training.\n\nFor Vietnamese financial applications, VLMs can:\n\n-   Interpret satellite imagery of industrial zones and estimate activity levels\n-   Read and extract data from scanned financial tables\n-   Analyze news photographs for sentiment\n-   Compare current and historical aerial views for change detection\n\n::: {#vlm-financial-prompting .cell execution_count=23}\n``` {.python .cell-code}\ndef vlm_financial_qa(image_path, question, context=None):\n    \"\"\"\n    Financial question-answering using a vision-language model.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to image (satellite tile, document page, news photo).\n    question : str\n        Financial analysis question.\n    context : str, optional\n        Additional textual context (e.g., firm name, sector).\n\n    Returns\n    -------\n    dict : Answer, confidence, extracted entities.\n    \"\"\"\n    from transformers import (\n        LlavaForConditionalGeneration,\n        LlavaProcessor\n    )\n\n    model_id = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n    processor = LlavaProcessor.from_pretrained(model_id)\n    model = LlavaForConditionalGeneration.from_pretrained(\n        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n    )\n\n    img = Image.open(image_path).convert(\"RGB\")\n\n    # Build financial analysis prompt\n    system_prompt = (\n        \"You are a financial analyst examining visual evidence. \"\n        \"Provide specific, quantitative observations when possible. \"\n        \"State your confidence level (high/medium/low).\"\n    )\n\n    if context:\n        prompt = (\n            f\"{system_prompt}\\n\\nContext: {context}\\n\\n\"\n            f\"Question: {question}\\n\\nAnswer:\"\n        )\n    else:\n        prompt = f\"{system_prompt}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n\n    inputs = processor(\n        text=prompt,\n        images=img,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=300,\n            temperature=0.1,\n            do_sample=False\n        )\n\n    answer = processor.decode(output[0], skip_special_tokens=True)\n    answer = answer.split(\"Answer:\")[-1].strip()\n\n    return {\"answer\": answer, \"question\": question}\n\n\n# Example financial VLM queries\nFINANCIAL_VLM_PROMPTS = {\n    \"satellite_activity\": (\n        \"Examine this satellite image of an industrial zone. \"\n        \"Estimate the occupancy rate of factory buildings, \"\n        \"the density of vehicles in parking areas, \"\n        \"and whether the site appears to be operating at \"\n        \"full, partial, or minimal capacity.\"\n    ),\n    \"document_extraction\": (\n        \"This is a page from a Vietnamese annual report. \"\n        \"Extract the following if present: \"\n        \"total revenue (doanh thu), net income (lợi nhuận ròng), \"\n        \"total assets (tổng tài sản). \"\n        \"Report values in billions VND.\"\n    ),\n    \"construction_progress\": (\n        \"Compare this aerial image to a baseline. \"\n        \"Estimate the percentage completion of visible \"\n        \"construction projects. Note any new structures, \"\n        \"cleared land, or infrastructure changes.\"\n    )\n}\n```\n:::\n\n\n### Retrieval-Augmented Multimodal Analysis\n\nFor complex financial questions, we can combine VLM capabilities with retrieval from structured databases. The pipeline:\n\n1.  **Query**: Analyst asks \"Is Vingroup's construction activity in Vinhomes Grand Park accelerating?\"\n2.  **Retrieve**: Fetch satellite time series, financial statements, news articles\n3.  **Process**: VLM analyzes satellite images; NLP processes text; tabular model processes financials\n4.  **Fuse**: Aggregate evidence across modalities\n5.  **Answer**: Generate a structured response with confidence scores and supporting evidence\n\n::: {#rag-multimodal .cell execution_count=24}\n``` {.python .cell-code}\nclass MultimodalRAG:\n    \"\"\"\n    Retrieval-Augmented Generation with multimodal evidence.\n    \"\"\"\n\n    def __init__(self, datacore_client, vlm_model=None):\n        self.dc = datacore_client\n        self.vlm = vlm_model\n\n    def retrieve_evidence(self, ticker, date, modalities=None):\n        \"\"\"\n        Retrieve all available evidence for a firm at a given date.\n        \"\"\"\n        evidence = {}\n\n        if modalities is None or \"tabular\" in modalities:\n            evidence[\"tabular\"] = self.dc.get_firm_financials(\n                ticker=ticker,\n                end_date=date,\n                n_quarters=4\n            )\n\n        if modalities is None or \"text\" in modalities:\n            evidence[\"text\"] = self.dc.get_news(\n                ticker=ticker,\n                start_date=pd.to_datetime(date) - pd.Timedelta(days=30),\n                end_date=date,\n                limit=20\n            )\n\n        if modalities is None or \"image\" in modalities:\n            evidence[\"image\"] = self.dc.get_satellite_images(\n                ticker=ticker,\n                date=date,\n                lookback_months=6\n            )\n\n        if modalities is None or \"ts\" in modalities:\n            evidence[\"ts\"] = self.dc.get_daily_returns(\n                ticker=ticker,\n                start_date=pd.to_datetime(date) - pd.Timedelta(days=90),\n                end_date=date\n            )\n\n        return evidence\n\n    def analyze(self, ticker, date, question):\n        \"\"\"\n        Full multimodal analysis pipeline.\n        \"\"\"\n        evidence = self.retrieve_evidence(ticker, date)\n\n        analysis = {\n            \"ticker\": ticker,\n            \"date\": date,\n            \"question\": question,\n            \"evidence_available\": list(evidence.keys()),\n            \"modality_signals\": {}\n        }\n\n        # Tabular signal\n        if \"tabular\" in evidence and evidence[\"tabular\"] is not None:\n            latest = evidence[\"tabular\"].iloc[-1]\n            analysis[\"modality_signals\"][\"tabular\"] = {\n                \"revenue_growth\": latest.get(\"revenue_growth\", None),\n                \"roe\": latest.get(\"roe\", None),\n                \"leverage\": latest.get(\"leverage\", None)\n            }\n\n        # Text signal\n        if \"text\" in evidence and evidence[\"text\"] is not None:\n            # Aggregate sentiment from PhoBERT\n            texts = evidence[\"text\"]\n            if len(texts) > 0:\n                avg_sentiment = texts[\"sentiment_score\"].mean()\n                analysis[\"modality_signals\"][\"text\"] = {\n                    \"avg_sentiment\": avg_sentiment,\n                    \"n_articles\": len(texts),\n                    \"sentiment_trend\": (\n                        \"improving\" if texts[\"sentiment_score\"].is_monotonic_increasing\n                        else \"deteriorating\" if texts[\"sentiment_score\"].is_monotonic_decreasing\n                        else \"mixed\"\n                    )\n                }\n\n        # Time series signal\n        if \"ts\" in evidence and evidence[\"ts\"] is not None:\n            ts = evidence[\"ts\"]\n            analysis[\"modality_signals\"][\"ts\"] = {\n                \"return_60d\": (1 + ts[\"ret\"]).prod() - 1,\n                \"volatility\": ts[\"ret\"].std() * np.sqrt(252),\n                \"avg_turnover\": ts[\"turnover\"].mean()\n            }\n\n        return analysis\n```\n:::\n\n\n## Evaluation and Deployment Considerations\n\n### Evaluation Protocol for Multimodal Financial Models\n\nStandard machine learning evaluation (random train/test split) is inappropriate for financial prediction. We require time-series-aware evaluation that respects the temporal ordering of information.\n\n| Evaluation Aspect | Correct Approach | Common Mistake |\n|--------------------------|------------------------|----------------------|\n| Train/test split | Expanding or rolling time window | Random split (look-ahead bias) |\n| Feature timing | Features available before prediction date | Using concurrent or future information |\n| Missing modalities | Test with realistic missingness patterns | Complete-case only |\n| Performance metric | OOS $R^2$, IC, Sharpe of L-S portfolio | In-sample $R^2$ |\n| Statistical inference | @diebold2002comparing test for forecast comparison | Point estimates without SE |\n| Economic significance | Transaction-cost-adjusted portfolio returns | Ignoring implementation costs |\n\n: Evaluation Best Practices for Multimodal Finance Models {#tbl-evaluation}\n\n### Computational Budget\n\nMultimodal models are computationally expensive. @tbl-compute-budget provides order-of-magnitude estimates for Vietnamese equity markets.\n\n| Component | Single Firm-Quarter | Full Panel (1000 firms × 40 quarters) |\n|-------------------|---------------|----------------------------|\n| PhoBERT text encoding | 0.5s | \\~5.5 hours |\n| ResNet50 satellite feature | 0.1s | \\~1.1 hours |\n| Time series encoding (CNN) | 0.01s | \\~7 minutes |\n| Tabular preprocessing | \\<0.01s | \\~1 minute |\n| Cross-attention fusion (forward) | 0.05s | \\~33 minutes |\n| Training (50 epochs) | -- | \\~12 hours (GPU) |\n| Full pipeline | -- | \\~1 day (single GPU) |\n\n: Computational Budget for Multimodal Pipeline {#tbl-compute-budget}\n\nThe practical implication is that pre-computation of unimodal embeddings is essential. Extract and cache PhoBERT embeddings, CNN features, and time-series representations once; reuse them across all fusion experiments. Only the fusion layers need retraining when the architecture changes.\n\n<!-- ## Exercises\n\n1.  **Contrastive Pre-Training for Vietnamese Finance.** Implement the FinancialCLIP contrastive alignment for paired (annual report text, satellite image) data. Train on 2014-2020 data and evaluate alignment quality on 2021-2024 by measuring zero-shot retrieval accuracy: given a text description, can the model retrieve the correct satellite image (and vice versa)? Report Recall\\@1, Recall\\@5, and Recall\\@10.\n\n2.  **Fusion Architecture Search.** Implement the following additional fusion strategies beyond the three in this chapter: (a) bilinear fusion with low-rank approximation, (b) mixture-of-experts with $2^M - 1$ expert networks, and (c) FiLM (Feature-wise Linear Modulation) conditioning. Compare all six strategies on forward quarterly return prediction using the same time-series cross-validation protocol. Which architecture achieves the best out-of-sample Sharpe ratio after transaction costs?\n\n3.  **Missing Modality Robustness.** Using the RobustFusionModel, systematically evaluate degradation as modalities are removed. Start with all four modalities and progressively remove them in all possible orderings. Plot the performance surface: $R^2$ as a function of the number and identity of available modalities. Which single modality is most informative? Which pair? Is there diminishing returns to adding the third and fourth modality?\n\n4.  **Multimodal Earnings Call Analysis.** If earnings call audio or transcripts are available from DataCore.vn, extend the multimodal earnings surprise model to include a fifth modality: audio features (vocal tone, speaking rate, pause patterns). Use the wav2vec2 model to extract audio embeddings. Does the CEO's vocal tone add predictive power beyond the transcript text? Implement the @mayew2012power methodology for comparing vocal and textual signals.\n\n5.  **Document Understanding Benchmark.** Construct a benchmark dataset of 200 Vietnamese annual report pages with manually annotated financial fields (revenue, COGS, net income, total assets, total equity, total debt). Evaluate three approaches: (a) OCR + rule-based extraction (Chapter 61), (b) LayoutLMv3 fine-tuned on 100 pages, and (c) zero-shot VLM extraction via GPT-4V or Gemini. Report field-level exact match accuracy and mean absolute percentage error for numerical values.\n\n6.  **Multimodal Factor Construction.** Using the best-performing fusion model, generate firm-quarter-level multimodal scores. Sort firms into quintiles on this score and compute equal- and value-weighted portfolio returns. Is the multimodal long-short factor priced in the cross-section (Fama-MacBeth)? Does it subsume the textual sentiment factor from Chapter 60 or the satellite-based factor from Chapter 61? Report spanning test results. -->\n\n## Summary\n\nThis chapter developed the multimodal learning framework for Vietnamese financial markets, progressing from foundational representation alignment through production-ready fusion architectures.\n\nThe key contributions are threefold. First, we demonstrated that financial data is inherently multimodal and that effective fusion requires explicit architectural choices (e.g., contrastive alignment, cross-attention mechanisms, and missing-modality handling) rather than naive concatenation. The FinancialCLIP alignment framework learns a shared embedding space where text, image, tabular, and time-series representations are geometrically comparable, enabling cross-modal retrieval and transfer.\n\nSecond, we built and compared five fusion architectures (early, late with gating, cross-attention, robust with modality dropout, and the custom earnings surprise model) on the prediction of forward returns and earnings surprises. The cross-attention architecture with modality dropout consistently outperforms unimodal baselines and simpler fusion strategies, though the margin varies across prediction horizons and firm characteristics.\n\nThird, we showed how large vision-language models can perform zero-shot financial analysis on Vietnamese documents and satellite imagery, offering a path to multimodal analysis without task-specific training. The retrieval-augmented multimodal pipeline combines the strengths of structured retrieval (from DataCore.vn) with the reasoning capabilities of VLMs.\n\nThe practical lesson for researchers working with Vietnamese financial data is that multimodal fusion is most valuable when modalities are complementary: text captures management intent and market narrative, images capture physical economic activity, tabular data provides precise quantitative snapshots, and time series captures market dynamics. When a single modality already captures most of the relevant signal (as tabular features do for many standard prediction tasks), the marginal gain from fusion is modest. When the prediction task requires information that no single modality captures well (as earnings surprises require both quantitative and qualitative assessment), multimodal models provide their largest advantage.\n\n",
    "supporting": [
      "74_multimodel_files/figure-pdf"
    ],
    "filters": []
  }
}