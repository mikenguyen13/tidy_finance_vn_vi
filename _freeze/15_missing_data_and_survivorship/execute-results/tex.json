{
  "hash": "e5fecd18fa0a7a1f80ffe76f1f69c028",
  "result": {
    "engine": "jupyter",
    "markdown": "# Missing Data and Survivorship Bias\n\n::: callout-note\nIn this chapter, we document the patterns of missing data, survivorship bias, and delisting bias in Vietnamese equity markets, develop diagnostic tools to detect these problems, and implement correction methods that yield more reliable empirical results.\n:::\n\nEvery empirical study in finance implicitly assumes that the data it analyzes are representative of the population it claims to study. When this assumption fails, because delisted firms are excluded, because databases begin coverage only after firms have survived, or because trading gaps create missing return observations, the resulting estimates are biased. In the U.S. context, @shumway1997delisting showed that ignoring delisting returns biases average returns upward by approximately 1% per year for NYSE stocks and substantially more for Nasdaq stocks, with severe consequences for anomaly-based strategies that overweight small, distressed firms.\n\nThe Vietnamese market presents a distinct and, in many ways, more acute set of data integrity challenges. The market is young. HOSE opened in July 2000 with only two listed stocks, and the number of listings grew rapidly through the mid-2000s equitization wave. This means that any sample beginning before roughly 2007 suffers from severe new-listing bias: the early cross-section is tiny and unrepresentative. Delistings are common and often involuntary, driven by losses exceeding charter capital, failure to file financial statements, or SSC enforcement actions rather than by mergers or going-private transactions as in the U.S. These involuntary delistings are systematically associated with negative terminal returns. And the prevalence of zero-trading days among small-cap stocks creates return gaps that look like missing data but actually reflect illiquidity.\n\nThis chapter provides the tools to diagnose and, where possible, correct these problems.\n\n## Taxonomy of Data Problems {#sec-missing-taxonomy}\n\nMissing data in financial research is not monolithic. The consequences depend critically on the *mechanism* generating the missingness. @rubin1976inference and @little2019statistical classify missing data into three types:\n\n1.  **Missing Completely at Random (MCAR).** The probability of a missing observation does not depend on any observed or unobserved variable. Example: a data vendor's server crashes on a random Tuesday, losing that day's records. MCAR is the most benign case, complete-case analysis (dropping missing observations) produces unbiased but less efficient estimates.\n2.  **Missing at Random (MAR).** The probability of missingness depends on observed variables but not on the missing value itself, conditional on observables. Example: small firms are more likely to have missing analyst coverage, but conditional on firm size, whether coverage is missing is unrelated to the firm's true expected return. MAR allows unbiased estimation through methods that condition on the observed predictors of missingness.\n3.  **Missing Not at Random (MNAR).** The probability of missingness depends on the missing value itself. Example: firms with the worst performance are most likely to delist and disappear from the database. MNAR is a pathological case and, unfortunately, the most common in financial data. Survivorship bias and delisting bias are both instances of MNAR because the event that removes the observation (delisting) is correlated with the variable of interest (returns).\n\nIn the Vietnamese context, we encounter all three types, often simultaneously (@tbl-missing-taxonomy).\n\n| Data Problem | Missingness Type | Mechanism in Vietnam |\n|------------------------|------------------------|------------------------|\n| Zero-trading days | MAR/MNAR | Small/illiquid stocks; correlated with returns |\n| Price limit hits | MNAR | True return truncated at limit; observed return censored |\n| Delisting | MNAR | Worst-performing firms exit; returns disappear |\n| Late listing coverage | Selection bias | Database begins after firm survives initial period |\n| Exchange transfers | Administrative | HOSE→HNX or UPCoM transfers break ticker continuity |\n| Suspended trading | MNAR | Suspension precedes negative events; returns missing |\n\n: Taxonomy of missing data in Vietnamese equity databases {#tbl-missing-taxonomy}\n\n## Data Construction {#sec-missing-data}\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code code-summary=\"Import libraries and configure environment\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n```\n:::\n\n\n::: {#data-load .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Load comprehensive listing, delisting, and return data\"}\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Complete listing history: includes all firms ever listed, not just current\nlisting_history = client.get_listing_history(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    include_delisted=True,\n    fields=[\n        'ticker', 'company_name', 'exchange', 'listing_date',\n        'delisting_date', 'delisting_reason', 'is_active',\n        'transfer_from', 'transfer_to', 'transfer_date',\n        'ipo_date', 'equitization_date', 'sector'\n    ]\n)\n\n# Daily returns: includes delisted firms' full history\ndaily_returns = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    start_date='2000-07-28',   # HOSE opening date\n    end_date='2024-12-31',\n    include_delisted=True,      # Critical flag\n    fields=[\n        'ticker', 'date', 'close', 'adjusted_close', 'volume',\n        'turnover_value', 'market_cap', 'shares_outstanding',\n        'price_limit_hit'       # +1 = limit up, -1 = limit down, 0 = neither\n    ]\n)\n\n# Monthly returns (pre-computed, survivorship-bias-free)\nmonthly_returns = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    start_date='2000-07-28',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'volume_avg_20d', 'n_trading_days', 'n_zero_volume_days'\n    ]\n)\n\nprint(f\"Listing history: {listing_history.shape[0]:,} firms\")\nprint(f\"  Active: {listing_history['is_active'].sum():,}\")\nprint(f\"  Delisted: {(~listing_history['is_active']).sum():,}\")\nprint(f\"Daily observations: {daily_returns.shape[0]:,}\")\nprint(f\"Monthly observations: {monthly_returns.shape[0]:,}\")\n```\n:::\n\n\n## Listing Dynamics in Vietnam {#sec-missing-listing-dynamics}\n\n### The Growth of the Vietnamese Market\n\nThe Vietnamese stock market's short history creates a distinctive pattern: the investable universe has grown from near-zero to over 1,500 listed firms in approximately two decades. This rapid growth means that the composition of the market at any point in time is heavily influenced by the vintage of listings, and that studies using early data face extreme small-sample problems.\n\n::: {#fig-listing-growth .cell execution_count=3}\n``` {.python .cell-code code-summary=\"Plot the growth of the Vietnamese listed universe\"}\nlisting_history['listing_date'] = pd.to_datetime(listing_history['listing_date'])\nlisting_history['delisting_date'] = pd.to_datetime(listing_history['delisting_date'])\n\n# Count active listings at each month-end\nmonths = pd.date_range('2000-07-01', '2024-12-31', freq='M')\nactive_counts = []\n\nfor month in months:\n    for exchange in ['HOSE', 'HNX', 'UPCoM']:\n        active = listing_history[\n            (listing_history['exchange'] == exchange) &\n            (listing_history['listing_date'] <= month) &\n            ((listing_history['delisting_date'].isna()) |\n             (listing_history['delisting_date'] > month))\n        ]\n        active_counts.append({\n            'month': month,\n            'exchange': exchange,\n            'n_active': len(active)\n        })\n\nactive_df = pd.DataFrame(active_counts)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Active listings over time\nfor exchange, color in [('HOSE', '#2C5F8A'), ('HNX', '#E67E22'),\n                         ('UPCoM', '#27AE60')]:\n    subset = active_df[active_df['exchange'] == exchange]\n    axes[0].plot(subset['month'], subset['n_active'],\n                 color=color, linewidth=2, label=exchange)\n\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Number of Active Listings')\naxes[0].set_title('Panel A: Active Listings by Exchange')\naxes[0].legend()\n\n# Panel B: Annual listings and delistings\nlisting_history['listing_year'] = listing_history['listing_date'].dt.year\nlisting_history['delisting_year'] = listing_history['delisting_date'].dt.year\n\nannual_listings = (\n    listing_history\n    .groupby('listing_year')\n    .size()\n    .reindex(range(2000, 2025), fill_value=0)\n)\nannual_delistings = (\n    listing_history\n    .dropna(subset=['delisting_year'])\n    .groupby('delisting_year')\n    .size()\n    .reindex(range(2000, 2025), fill_value=0)\n)\n\nx = np.arange(2000, 2025)\naxes[1].bar(x - 0.2, annual_listings.values, width=0.4,\n            color='#27AE60', alpha=0.85, label='New Listings')\naxes[1].bar(x + 0.2, annual_delistings.values, width=0.4,\n            color='#C0392B', alpha=0.85, label='Delistings')\naxes[1].set_xlabel('Year')\naxes[1].set_ylabel('Number of Firms')\naxes[1].set_title('Panel B: Annual Listings and Delistings')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Delisting Reasons\n\nVietnamese delistings are not homogeneous. The SSC mandates delisting for specific regulatory violations, but firms may also voluntarily delist, merge, or transfer between exchanges. The reason for delisting matters because it determines the likely terminal return.\n\n::: {#delisting-reasons .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Categorize and tabulate delisting reasons\"}\ndelisted = listing_history[listing_history['delisting_date'].notna()].copy()\n\n# Standardize delisting reasons into categories\nreason_map = {\n    'losses_exceed_charter': 'Involuntary - Financial Distress',\n    'bankruptcy': 'Involuntary - Financial Distress',\n    'failure_to_file': 'Involuntary - Regulatory',\n    'audit_qualification': 'Involuntary - Regulatory',\n    'ssc_enforcement': 'Involuntary - Regulatory',\n    'merger': 'Voluntary - M&A',\n    'going_private': 'Voluntary - Going Private',\n    'transfer_exchange': 'Transfer',\n    'voluntary': 'Voluntary - Other',\n    'other': 'Other/Unknown'\n}\ndelisted['reason_category'] = (\n    delisted['delisting_reason']\n    .map(reason_map)\n    .fillna('Other/Unknown')\n)\n\n# Tabulate\nreason_counts = (\n    delisted['reason_category']\n    .value_counts()\n    .to_frame('Count')\n)\nreason_counts['Percentage'] = (\n    reason_counts['Count'] / reason_counts['Count'].sum() * 100\n)\n\nprint(\"Delisting Reasons:\")\nprint(reason_counts.round(1).to_string())\n```\n:::\n\n\n::: {#fig-delisting-reasons .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Visualize delisting reasons\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Pie chart\ncolors_pie = ['#C0392B', '#E74C3C', '#8E44AD', '#27AE60',\n              '#2C5F8A', '#F1C40F', '#BDC3C7']\naxes[0].pie(reason_counts['Count'], labels=reason_counts.index,\n            colors=colors_pie[:len(reason_counts)],\n            autopct='%1.0f%%', startangle=90, textprops={'fontsize': 8})\naxes[0].set_title('Panel A: Delisting Reasons')\n\n# Panel B: Delisting reasons over time\ndelisted['year'] = delisted['delisting_date'].dt.year\nreason_by_year = pd.crosstab(delisted['year'], delisted['reason_category'])\nreason_by_year = reason_by_year.reindex(range(2000, 2025), fill_value=0)\n\nreason_by_year.plot(kind='bar', stacked=True, ax=axes[1],\n                     colormap='Set2', edgecolor='white', width=0.8)\naxes[1].set_xlabel('Year')\naxes[1].set_ylabel('Number of Delistings')\naxes[1].set_title('Panel B: Delisting Reasons Over Time')\naxes[1].legend(fontsize=7, loc='upper left')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Firm Characteristics at Delisting\n\nDo delisted firms differ systematically from survivors? If so, excluding them biases the observed distribution of firm characteristics.\n\n::: {#fig-delisting-characteristics .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Compare characteristics of delisted vs surviving firms\"}\n# Get fundamentals in the last available year before delisting\nlast_year_delisted = (\n    delisted[['ticker', 'delisting_date']]\n    .assign(last_fy=lambda x: x['delisting_date'].dt.year - 1)\n)\n\nfundamentals = client.get_fundamentals(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    start_date='2005-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'fiscal_year', 'total_assets', 'net_income',\n        'total_equity', 'revenue', 'market_cap'\n    ]\n)\n\n# Characteristics of delisted firms (last year before delisting)\ndelist_chars = (\n    last_year_delisted\n    .merge(fundamentals.rename(columns={'fiscal_year': 'last_fy'}),\n           on=['ticker', 'last_fy'], how='inner')\n)\ndelist_chars['roa'] = delist_chars['net_income'] / delist_chars['total_assets']\ndelist_chars['leverage'] = (\n    (delist_chars['total_assets'] - delist_chars['total_equity'])\n    / delist_chars['total_assets']\n)\ndelist_chars['log_assets'] = np.log(delist_chars['total_assets'])\ndelist_chars['group'] = 'Delisted'\n\n# Characteristics of all active firms (pooled)\nall_chars = fundamentals.copy()\nall_chars['roa'] = all_chars['net_income'] / all_chars['total_assets']\nall_chars['leverage'] = (\n    (all_chars['total_assets'] - all_chars['total_equity'])\n    / all_chars['total_assets']\n)\nall_chars['log_assets'] = np.log(all_chars['total_assets'])\nall_chars['group'] = 'All Active'\n\n# Compare distributions\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nvariables = [\n    ('log_assets', 'Log Total Assets', axes[0, 0]),\n    ('roa', 'Return on Assets', axes[0, 1]),\n    ('leverage', 'Leverage Ratio', axes[1, 0]),\n]\n\nfor col, label, ax in variables:\n    for grp, color in [('All Active', '#2C5F8A'), ('Delisted', '#C0392B')]:\n        if grp == 'Delisted':\n            data = delist_chars[col].dropna()\n        else:\n            data = all_chars[col].dropna()\n        data = data[np.isfinite(data)]\n        ax.hist(data, bins=50, density=True, alpha=0.5,\n                color=color, label=grp, edgecolor='white')\n    ax.set_xlabel(label)\n    ax.set_ylabel('Density')\n    ax.legend()\n\n# Panel D: Market cap distribution\nfor grp, color in [('All Active', '#2C5F8A'), ('Delisted', '#C0392B')]:\n    if grp == 'Delisted':\n        data = np.log(delist_chars['market_cap'].dropna())\n    else:\n        data = np.log(all_chars['market_cap'].dropna())\n    data = data[np.isfinite(data)]\n    axes[1, 1].hist(data, bins=50, density=True, alpha=0.5,\n                     color=color, label=grp, edgecolor='white')\naxes[1, 1].set_xlabel('Log Market Cap')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\n\nplt.suptitle('Characteristics of Delisted vs Active Firms', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Formal comparison\nprint(\"\\nMean Comparison (Delisted vs All Active):\")\nfor col in ['log_assets', 'roa', 'leverage']:\n    d = delist_chars[col].dropna()\n    a = all_chars[col].dropna()\n    d = d[np.isfinite(d)]\n    a = a[np.isfinite(a)]\n    t, p = stats.ttest_ind(d, a, equal_var=False)\n    print(f\"  {col:<15}: Delisted = {d.mean():.3f}, \"\n          f\"Active = {a.mean():.3f}, t = {t:.2f}, p = {p:.4f}\")\n```\n:::\n\n\n## Survivorship Bias {#sec-missing-survivorship}\n\n### Definition and Magnitude\n\nSurvivorship bias arises when a study uses only firms that are currently listed (or listed at the end of the sample), excluding firms that delisted during the sample period. Because delisted firms disproportionately experienced negative returns before delisting, their exclusion inflates average returns, understates risk, and distorts cross-sectional patterns.\n\nWe quantify the magnitude of survivorship bias by comparing portfolio returns computed from the **survivorship-bias-free** sample (all firms, including those that subsequently delisted) against a **survivors-only** sample (firms that remained listed through the end of the sample).\n\n::: {#survivorship-magnitude .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Quantify survivorship bias by comparing survivor-only vs full-sample portfolios\"}\n# Define survivors: firms active as of 2024-12-31\nsurvivors = set(\n    listing_history[listing_history['is_active']]['ticker']\n)\n\n# Full sample: all firms, including delisted\nfull_sample = monthly_returns.copy()\n\n# Survivors only: restrict to firms still listed at end of sample\nsurvivors_only = monthly_returns[\n    monthly_returns['ticker'].isin(survivors)\n].copy()\n\n# Compute EW monthly portfolio returns\ndef compute_ew_portfolio(df):\n    return (\n        df\n        .groupby('month_end')['monthly_return']\n        .mean()\n        .to_frame('portfolio_return')\n    )\n\ndef compute_vw_portfolio(df):\n    return (\n        df\n        .groupby('month_end')\n        .apply(lambda g: np.average(g['monthly_return'],\n                                     weights=g['market_cap'])\n               if g['market_cap'].sum() > 0 else np.nan)\n        .to_frame('portfolio_return')\n    )\n\new_full = compute_ew_portfolio(full_sample)\new_survivors = compute_ew_portfolio(survivors_only)\nvw_full = compute_vw_portfolio(full_sample)\nvw_survivors = compute_vw_portfolio(survivors_only)\n\n# Merge and compute bias\nbias_ew = pd.merge(\n    ew_full.rename(columns={'portfolio_return': 'full'}),\n    ew_survivors.rename(columns={'portfolio_return': 'survivors'}),\n    left_index=True, right_index=True\n)\nbias_ew['bias'] = bias_ew['survivors'] - bias_ew['full']\n\nbias_vw = pd.merge(\n    vw_full.rename(columns={'portfolio_return': 'full'}),\n    vw_survivors.rename(columns={'portfolio_return': 'survivors'}),\n    left_index=True, right_index=True\n)\nbias_vw['bias'] = bias_vw['survivors'] - bias_vw['full']\n\nprint(\"Survivorship Bias (Annualized):\")\nprint(f\"  EW: {bias_ew['bias'].mean() * 12:.4f} \"\n      f\"({bias_ew['bias'].mean() * 1200:.1f} bps/year)\")\nprint(f\"  VW: {bias_vw['bias'].mean() * 12:.4f} \"\n      f\"({bias_vw['bias'].mean() * 1200:.1f} bps/year)\")\n```\n:::\n\n\n::: {#fig-survivorship-bias .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Plot cumulative returns: full sample vs survivors only\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor i, (bias_df, title) in enumerate(\n    [(bias_ew, 'Panel A: Equal-Weighted'),\n     (bias_vw, 'Panel B: Value-Weighted')]\n):\n    cum_full = (1 + bias_df['full']).cumprod()\n    cum_surv = (1 + bias_df['survivors']).cumprod()\n\n    axes[i].plot(cum_full.index, cum_full,\n                 color='#2C5F8A', linewidth=2, label='Full Sample')\n    axes[i].plot(cum_surv.index, cum_surv,\n                 color='#C0392B', linewidth=2, label='Survivors Only')\n    axes[i].set_ylabel('Cumulative Wealth')\n    axes[i].set_xlabel('Date')\n    axes[i].set_title(title)\n    axes[i].legend()\n    axes[i].set_yscale('log')\n\n    ann_bias = bias_df['bias'].mean() * 12\n    axes[i].text(0.05, 0.95,\n                 f'Annual Bias: {ann_bias*100:.1f}%',\n                 transform=axes[i].transAxes, fontsize=11,\n                 verticalalignment='top',\n                 bbox=dict(facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Time-Varying Survivorship Bias\n\nThe magnitude of survivorship bias is not constant. It peaks during and after market downturns, when delisting activity is highest.\n\n::: {#fig-rolling-bias .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Compute rolling survivorship bias\"}\nbias_ew['rolling_bias_12m'] = bias_ew['bias'].rolling(12).mean() * 12\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), height_ratios=[2, 1])\n\n# Panel A: Rolling bias\naxes[0].fill_between(\n    bias_ew.index, 0, bias_ew['rolling_bias_12m'] * 100,\n    where=bias_ew['rolling_bias_12m'] > 0,\n    color='#C0392B', alpha=0.4\n)\naxes[0].plot(bias_ew.index, bias_ew['rolling_bias_12m'] * 100,\n             color='#C0392B', linewidth=1.5)\naxes[0].axhline(y=0, color='gray', linewidth=0.5)\naxes[0].set_ylabel('Annualized Bias (%)')\naxes[0].set_title('Panel A: Rolling 12-Month Survivorship Bias (EW)')\n\n# Panel B: Number of delistings per quarter\ndelistings_quarterly = (\n    delisted\n    .set_index('delisting_date')\n    .resample('Q')\n    .size()\n)\naxes[1].bar(delistings_quarterly.index, delistings_quarterly.values,\n            width=80, color='#2C5F8A', alpha=0.7)\naxes[1].set_ylabel('Delistings per Quarter')\naxes[1].set_xlabel('Date')\naxes[1].set_title('Panel B: Quarterly Delisting Activity')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Survivorship Bias in Cross-Sectional Anomalies\n\nThe bias is not uniform across strategies. Anomalies that overweight small, distressed, or low-quality firms, precisely the firms most likely to delist, are most severely affected. We test this for the size, value, and momentum anomalies.\n\n::: {#anomaly-bias .cell execution_count=10}\n``` {.python .cell-code code-summary=\"Quantify survivorship bias by anomaly portfolio\"}\ndef compute_long_short(df, sort_var, n_quantiles=5):\n    \"\"\"\n    Compute long-short portfolio returns from quintile sorts.\n    Long = top quintile, Short = bottom quintile.\n    \"\"\"\n    results = []\n    for month, group in df.groupby('month_end'):\n        group = group.dropna(subset=[sort_var, 'monthly_return'])\n        if len(group) < 20:\n            continue\n        group['quantile'] = pd.qcut(\n            group[sort_var], n_quantiles, labels=False, duplicates='drop'\n        )\n        long_ret = group[group['quantile'] == n_quantiles - 1]['monthly_return'].mean()\n        short_ret = group[group['quantile'] == 0]['monthly_return'].mean()\n        results.append({\n            'month_end': month,\n            'long': long_ret,\n            'short': short_ret,\n            'long_short': long_ret - short_ret\n        })\n    return pd.DataFrame(results)\n\n# Prepare sort variables\nmonthly_with_chars = monthly_returns.merge(\n    fundamentals[['ticker', 'fiscal_year', 'total_assets',\n                   'net_income', 'total_equity']],\n    left_on=['ticker', monthly_returns['month_end'].dt.year],\n    right_on=['ticker', 'fiscal_year'],\n    how='left'\n)\nmonthly_with_chars['log_mcap'] = np.log(monthly_with_chars['market_cap'])\nmonthly_with_chars['bm'] = (\n    monthly_with_chars['total_equity'] / monthly_with_chars['market_cap']\n)\nmonthly_with_chars['past_12m'] = (\n    monthly_with_chars\n    .groupby('ticker')['monthly_return']\n    .transform(lambda x: x.rolling(12).sum())\n)\n\n# Compute anomalies on full sample and survivors only\nanomaly_bias = {}\nfor anomaly, sort_var, ascending in [\n    ('Size (SMB)', 'log_mcap', True),\n    ('Value (HML)', 'bm', True),\n    ('Momentum (WML)', 'past_12m', True)\n]:\n    full_ls = compute_long_short(\n        monthly_with_chars, sort_var\n    )\n    surv_data = monthly_with_chars[\n        monthly_with_chars['ticker'].isin(survivors)\n    ]\n    surv_ls = compute_long_short(surv_data, sort_var)\n\n    # Merge\n    merged = pd.merge(\n        full_ls[['month_end', 'long_short']].rename(\n            columns={'long_short': 'full'}),\n        surv_ls[['month_end', 'long_short']].rename(\n            columns={'long_short': 'survivors'}),\n        on='month_end'\n    )\n    merged['bias'] = merged['survivors'] - merged['full']\n\n    ann_full = merged['full'].mean() * 12\n    ann_surv = merged['survivors'].mean() * 12\n    ann_bias = merged['bias'].mean() * 12\n\n    anomaly_bias[anomaly] = {\n        'Full Sample (ann.)': ann_full,\n        'Survivors Only (ann.)': ann_surv,\n        'Bias (ann.)': ann_bias,\n        'Bias (% of premium)': ann_bias / ann_full * 100 if ann_full != 0 else np.nan\n    }\n\nanomaly_bias_df = pd.DataFrame(anomaly_bias).T\nprint(\"Survivorship Bias by Anomaly:\")\nprint(anomaly_bias_df.round(4).to_string())\n```\n:::\n\n\n## Delisting Bias and Return Imputation {#sec-missing-delisting}\n\n### The Shumway Correction\n\n@shumway1997delisting showed that CRSP's treatment of delisting returns, often recording them as missing or zero, creates a systematic upward bias in average returns. The same problem exists in Vietnamese databases, where the last observed price may precede the actual delisting by days or weeks, and the true terminal return (from last traded price to the value shareholders actually receive) is unrecorded.\n\nWe implement a delisting return imputation procedure adapted for Vietnam:\n\n**Step 1.** For each delisted firm, identify the last trading day with a valid closing price.\n\n**Step 2.** Classify the delisting reason to determine the appropriate imputation (@tbl-missing-imputation).\n\n| Delisting Reason | Imputed Return | Rationale |\n|------------------------|------------------------|------------------------|\n| M&A / Acquisition | Actual tender offer premium (if available) | Acquisition at premium |\n| Going private | 0% (or actual buyout price) | Negotiated exit |\n| Financial distress | −30% to −100% | Substantial loss of value |\n| Regulatory violation | −50% | Partial loss; some recovery possible |\n| Exchange transfer | 0% (link to new ticker) | No economic event |\n\n: Delisting return imputation rules. {#tbl-missing-imputation}\n\n**Step 3.** Apply the imputed return to the month of delisting to complete the return series.\n\n::: {#delisting-imputation .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Implement delisting return imputation for Vietnamese firms\"}\ndef impute_delisting_returns(listing_df, daily_df, monthly_df):\n    \"\"\"\n    Impute terminal returns for delisted firms.\n\n    Returns a DataFrame of imputed delisting returns to be\n    appended to the monthly return panel.\n    \"\"\"\n    delisted_firms = listing_df[listing_df['delisting_date'].notna()].copy()\n    imputed = []\n\n    for _, firm in delisted_firms.iterrows():\n        ticker = firm['ticker']\n        delist_date = firm['delisting_date']\n        reason = firm.get('reason_category', firm.get('delisting_reason', ''))\n\n        # Find last trading day\n        firm_daily = daily_df[daily_df['ticker'] == ticker].sort_values('date')\n        if len(firm_daily) == 0:\n            continue\n\n        last_trade = firm_daily.iloc[-1]\n        last_price = last_trade['adjusted_close']\n        last_date = last_trade['date']\n\n        # Check if last trade is already close to delisting date\n        gap_days = (pd.Timestamp(delist_date) - pd.Timestamp(last_date)).days\n        if gap_days < 0:\n            continue  # Data issue\n\n        # Determine imputation based on reason\n        if 'M&A' in str(reason) or 'merger' in str(reason).lower():\n            imputed_return = 0.0  # Conservative; ideally use tender price\n        elif 'Going Private' in str(reason) or 'voluntary' in str(reason).lower():\n            imputed_return = 0.0\n        elif 'Transfer' in str(reason):\n            imputed_return = 0.0  # Not a real delisting\n        elif 'Financial Distress' in str(reason) or 'bankruptcy' in str(reason).lower():\n            imputed_return = -0.50  # Conservative estimate\n        elif 'Regulatory' in str(reason):\n            imputed_return = -0.30\n        else:\n            imputed_return = -0.30  # Default for unknown reasons\n\n        # Assign to the delisting month\n        delist_month = pd.Timestamp(delist_date).to_period('M').to_timestamp()\n\n        imputed.append({\n            'ticker': ticker,\n            'month_end': delist_month,\n            'monthly_return': imputed_return,\n            'market_cap': last_trade.get('market_cap', np.nan),\n            'source': 'imputed_delisting',\n            'delisting_reason': reason,\n            'gap_days': gap_days\n        })\n\n    return pd.DataFrame(imputed)\n\n# Apply imputation\nimputed_returns = impute_delisting_returns(\n    delisted.assign(reason_category=delisted['reason_category']),\n    daily_returns, monthly_returns\n)\n\nprint(f\"Imputed delisting returns: {len(imputed_returns)}\")\nprint(f\"\\nImputed return distribution:\")\nprint(imputed_returns['monthly_return'].value_counts().sort_index())\n```\n:::\n\n\n### Impact of Delisting Return Imputation\n\n::: {#delisting-impact .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Quantify the impact of including imputed delisting returns\"}\n# Augmented sample: monthly returns + imputed delisting returns\naugmented = pd.concat([\n    monthly_returns[['ticker', 'month_end', 'monthly_return', 'market_cap']],\n    imputed_returns[['ticker', 'month_end', 'monthly_return', 'market_cap']]\n], ignore_index=True)\n\n# Compare original vs augmented EW portfolios\new_original = compute_ew_portfolio(monthly_returns)\new_augmented = compute_ew_portfolio(augmented)\n\ncomparison = pd.merge(\n    ew_original.rename(columns={'portfolio_return': 'original'}),\n    ew_augmented.rename(columns={'portfolio_return': 'augmented'}),\n    left_index=True, right_index=True\n)\ncomparison['imputation_effect'] = (\n    comparison['augmented'] - comparison['original']\n)\n\nann_original = comparison['original'].mean() * 12\nann_augmented = comparison['augmented'].mean() * 12\nann_effect = comparison['imputation_effect'].mean() * 12\n\nprint(\"Delisting Return Imputation Impact:\")\nprint(f\"  EW without imputation: {ann_original:.4f} ({ann_original*100:.2f}%/yr)\")\nprint(f\"  EW with imputation:    {ann_augmented:.4f} ({ann_augmented*100:.2f}%/yr)\")\nprint(f\"  Difference:            {ann_effect:.4f} ({ann_effect*100:.2f}%/yr)\")\n```\n:::\n\n\n## Zero-Trading Days and Illiquidity Gaps {#sec-missing-zero-trading}\n\n### Prevalence of Zero-Trading Days\n\nA distinctive feature of Vietnamese equity data is the high frequency of zero-volume days (i.e., days on which a listed stock records no trades). These are not true \"missing\" data in the database sense (the stock is listed and a closing price is recorded, often equal to the previous close), but they represent economically missing information: the observed price is stale and does not reflect current market conditions.\n\n::: {#fig-zero-trading .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Analyze the prevalence of zero-trading days\"}\n# Compute zero-volume fraction per firm-year\ndaily_returns['year'] = pd.to_datetime(daily_returns['date']).dt.year\ndaily_returns['zero_volume'] = (daily_returns['volume'] == 0).astype(int)\n\nzero_vol_fy = (\n    daily_returns\n    .groupby(['ticker', 'year'])\n    .agg(\n        n_days=('zero_volume', 'count'),\n        n_zero=('zero_volume', 'sum'),\n        avg_mcap=('market_cap', 'mean')\n    )\n    .reset_index()\n)\nzero_vol_fy['zero_frac'] = zero_vol_fy['n_zero'] / zero_vol_fy['n_days']\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Distribution over time (boxplot by year)\nyears_to_plot = range(2008, 2025)\ndata_by_year = [\n    zero_vol_fy[zero_vol_fy['year'] == y]['zero_frac'].dropna().values\n    for y in years_to_plot\n]\nbp = axes[0].boxplot(data_by_year, positions=range(len(years_to_plot)),\n                      widths=0.6, showfliers=False, patch_artist=True,\n                      medianprops={'color': 'black'})\nfor patch in bp['boxes']:\n    patch.set_facecolor('#2C5F8A')\n    patch.set_alpha(0.6)\naxes[0].set_xticks(range(len(years_to_plot)))\naxes[0].set_xticklabels(years_to_plot, rotation=45, fontsize=8)\naxes[0].set_ylabel('Zero-Volume Fraction')\naxes[0].set_title('Panel A: Zero-Volume Days by Year')\n\n# Panel B: By market cap decile\nzero_vol_fy['mcap_decile'] = pd.qcut(\n    zero_vol_fy['avg_mcap'].rank(method='first'),\n    10, labels=[f'D{i}' for i in range(1, 11)]\n)\ndecile_zero = (\n    zero_vol_fy\n    .groupby('mcap_decile')['zero_frac']\n    .agg(['mean', 'median'])\n)\naxes[1].bar(range(10), decile_zero['mean'],\n            color='#2C5F8A', alpha=0.85, edgecolor='white')\naxes[1].set_xticks(range(10))\naxes[1].set_xticklabels(decile_zero.index)\naxes[1].set_xlabel('Market Cap Decile (D1 = smallest)')\naxes[1].set_ylabel('Mean Zero-Volume Fraction')\naxes[1].set_title('Panel B: Zero-Volume Days by Size')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Return Measurement During Zero-Trading Periods\n\nWhen a stock does not trade, the standard approach, using the last available closing price, produces a stale price that understates true volatility and biases returns toward zero. Several approaches exist to handle this:\n\n**Approach 1: Drop zero-volume observations.** Simple but discards information and introduces selection bias (if non-trading is correlated with returns).\n\n**Approach 2: Multi-day compounding.** Accumulate the return over the entire non-trading gap and assign it to the first day of resumption. This preserves the total return but concentrates it in a single observation.\n\n**Approach 3: Distribute uniformly.** Spread the accumulated return evenly across zero-volume days. This is economically unrealistic, but it reduces the impact of single-day outliers.\n\n**Approach 4: Treat as missing and model.** Treat zero-volume days as genuinely missing returns and use the @lesmond2005liquidity zero-return measure as a liquidity proxy.\n\n::: {#zero-vol-correction .cell execution_count=14}\n``` {.python .cell-code code-summary=\"Implement zero-trading-day return corrections\"}\ndef correct_zero_volume_returns(daily_df, method='compound'):\n    \"\"\"\n    Correct returns during zero-volume periods.\n\n    Parameters\n    ----------\n    method : str\n        'compound': assign accumulated return to first non-zero day\n        'distribute': spread return evenly across gap\n        'drop': remove zero-volume observations\n    \"\"\"\n    df = daily_df.copy()\n    df = df.sort_values(['ticker', 'date'])\n    df['daily_return'] = (\n        df.groupby('ticker')['adjusted_close']\n        .pct_change()\n    )\n\n    if method == 'drop':\n        return df[df['volume'] > 0]\n\n    elif method == 'compound':\n        # For each zero-volume streak, accumulate return and\n        # assign to the next trading day\n        results = []\n        for ticker, group in df.groupby('ticker'):\n            group = group.sort_values('date').reset_index(drop=True)\n            accumulated = 0\n            gap_length = 0\n\n            for idx, row in group.iterrows():\n                if row['volume'] == 0:\n                    accumulated += row['daily_return'] if pd.notna(row['daily_return']) else 0\n                    gap_length += 1\n                else:\n                    if gap_length > 0:\n                        # Add accumulated return to this day's return\n                        total_return = (1 + accumulated) * (1 + (row['daily_return'] or 0)) - 1\n                        group.loc[idx, 'daily_return'] = total_return\n                        accumulated = 0\n                        gap_length = 0\n                    results.append(group.loc[idx])\n\n            # If series ends with zero-volume days, include last non-zero\n            if gap_length > 0 and len(results) > 0:\n                last_valid = results[-1].copy()\n                last_valid['daily_return'] = (\n                    (1 + last_valid['daily_return']) * (1 + accumulated) - 1\n                )\n                results[-1] = last_valid\n\n        return pd.DataFrame(results)\n\n    elif method == 'distribute':\n        results = []\n        for ticker, group in df.groupby('ticker'):\n            group = group.sort_values('date').reset_index(drop=True)\n            i = 0\n            while i < len(group):\n                if group.loc[i, 'volume'] > 0:\n                    results.append(group.loc[i])\n                    i += 1\n                else:\n                    # Find end of zero-volume streak\n                    j = i\n                    while j < len(group) and group.loc[j, 'volume'] == 0:\n                        j += 1\n                    # Total return over gap\n                    if j < len(group):\n                        total_ret = (\n                            group.loc[j, 'adjusted_close']\n                            / group.loc[i - 1, 'adjusted_close'] - 1\n                            if i > 0 else 0\n                        )\n                        n_days = j - i + 1\n                        daily_r = (1 + total_ret) ** (1 / n_days) - 1\n                        for k in range(i, j + 1):\n                            row = group.loc[k].copy()\n                            row['daily_return'] = daily_r\n                            results.append(row)\n                    i = j + 1\n\n        return pd.DataFrame(results)\n\n# Apply corrections and compare\nfor method in ['drop', 'compound', 'distribute']:\n    corrected = correct_zero_volume_returns(\n        daily_returns.head(500000), method=method\n    )\n    mean_ret = corrected['daily_return'].mean() * 252\n    vol = corrected['daily_return'].std() * np.sqrt(252)\n    print(f\"{method:<12}: Ann. Return = {mean_ret:.4f}, \"\n          f\"Ann. Vol = {vol:.4f}, N = {len(corrected):,}\")\n```\n:::\n\n\n## Look-Ahead Bias {#sec-missing-lookahead}\n\n### Definition\n\nLook-ahead bias occurs when a study uses information that was not available at the time the investment decision would have been made. In the Vietnamese context, the most common sources are:\n\n1.  **Conditioning on survival.** Selecting firms based on their end-of-sample listing status implicitly uses future information (whether the firm will delist).\n2.  **Using revised financial data.** Vietnamese firms often restate financial statements after audit. Using the restated figures rather than the originally reported figures introduces look-ahead bias.\n3.  **Backfill bias.** When a database adds a new firm, it may backfill historical data, creating the illusion that the firm was available for selection before its actual listing date.\n4.  **Point-in-time accounting data.** Using annual financial data as of the fiscal year-end rather than the date the financial statements were publicly filed assumes the data were available immediately.\n\n### Point-in-Time Adjustment\n\nWe implement a point-in-time adjustment for accounting data that respects the actual reporting lag:\n\n::: {#point-in-time .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Implement point-in-time alignment of accounting data\"}\ndef point_in_time_merge(monthly_df, fundamentals_df, filings_df,\n                         lag_months=0):\n    \"\"\"\n    Merge accounting data with monthly returns respecting\n    the actual filing date (point-in-time).\n\n    Parameters\n    ----------\n    monthly_df : DataFrame with ticker, month_end\n    fundamentals_df : DataFrame with ticker, fiscal_year, and accounting vars\n    filings_df : DataFrame with ticker, fiscal_year, filing_date\n    lag_months : int, additional safety lag beyond filing date\n    \"\"\"\n    # Merge fundamentals with filing dates\n    fund_with_date = fundamentals_df.merge(\n        filings_df[['ticker', 'fiscal_year', 'filing_date']],\n        on=['ticker', 'fiscal_year'], how='left'\n    )\n\n    # If filing date is missing, assume available 4 months after FY end\n    fund_with_date['filing_date'] = pd.to_datetime(\n        fund_with_date['filing_date']\n    )\n    fund_with_date['fy_end'] = pd.to_datetime(\n        fund_with_date['fiscal_year'].astype(str) + '-12-31'\n    )\n    fund_with_date['available_date'] = fund_with_date['filing_date'].fillna(\n        fund_with_date['fy_end'] + pd.DateOffset(months=4)\n    )\n\n    # Add safety lag\n    if lag_months > 0:\n        fund_with_date['available_date'] += pd.DateOffset(months=lag_months)\n\n    # For each firm-month, find the most recent accounting data\n    # that was available (filing_date <= month_end)\n    results = []\n    for _, row in monthly_df.iterrows():\n        ticker = row['ticker']\n        month = row['month_end']\n\n        available = fund_with_date[\n            (fund_with_date['ticker'] == ticker) &\n            (fund_with_date['available_date'] <= month)\n        ]\n\n        if len(available) > 0:\n            latest = available.sort_values('fiscal_year').iloc[-1]\n            result = row.to_dict()\n            for col in ['total_assets', 'net_income', 'total_equity',\n                        'revenue']:\n                if col in latest:\n                    result[col] = latest[col]\n            result['data_fiscal_year'] = latest['fiscal_year']\n            result['data_lag_months'] = (\n                (pd.Timestamp(month) - pd.Timestamp(latest['available_date']))\n                .days / 30.44\n            )\n            results.append(result)\n\n    return pd.DataFrame(results)\n\n# Example: compare point-in-time vs naive merge\nfilings = client.get_filings(\n    exchanges=['HOSE', 'HNX'],\n    report_types=['annual'],\n    fields=['ticker', 'fiscal_year', 'filing_date']\n)\n\nprint(\"Point-in-time merge vs naive merge:\")\nprint(\"  Naive: use fiscal year directly (introduces look-ahead bias)\")\nprint(\"  PIT: use only data available as of the portfolio formation date\")\n```\n:::\n\n\n### Quantifying Look-Ahead Bias in Value Strategies\n\nValue strategies sort stocks on book-to-market ratios computed from accounting data. Using end-of-fiscal-year data without respecting reporting lags inflates the value premium because it implicitly uses information that was not yet publicly available.\n\n::: {#lookahead-value .cell execution_count=16}\n``` {.python .cell-code code-summary=\"Compare value premium with and without point-in-time adjustment\"}\n# Naive approach: use fiscal year data immediately\nmonthly_naive = monthly_returns.merge(\n    fundamentals[['ticker', 'fiscal_year', 'total_equity']],\n    left_on=['ticker', monthly_returns['month_end'].dt.year],\n    right_on=['ticker', 'fiscal_year'],\n    how='left'\n)\nmonthly_naive['bm_naive'] = (\n    monthly_naive['total_equity'] / monthly_naive['market_cap']\n)\n\n# Point-in-time approach (using 4-month lag as conservative default)\nmonthly_pit = monthly_returns.copy()\nmonthly_pit['bm_pit'] = np.nan  # Would be filled by point_in_time_merge\n\n# For demonstration: approximate PIT by using t-1 fiscal year data\n# (ensures data were available at formation date)\nfund_lagged = fundamentals.copy()\nfund_lagged['merge_year'] = fund_lagged['fiscal_year'] + 1\nmonthly_pit = monthly_pit.merge(\n    fund_lagged[['ticker', 'merge_year', 'total_equity']].rename(\n        columns={'merge_year': 'year'}),\n    left_on=['ticker', monthly_pit['month_end'].dt.year],\n    right_on=['ticker', 'year'],\n    how='left'\n)\nmonthly_pit['bm_pit'] = (\n    monthly_pit['total_equity'] / monthly_pit['market_cap']\n)\n\n# Compute HML for both approaches\nhml_naive = compute_long_short(monthly_naive, 'bm_naive')\nhml_pit = compute_long_short(monthly_pit, 'bm_pit')\n\nann_naive = hml_naive['long_short'].mean() * 12\nann_pit = hml_pit['long_short'].mean() * 12\n\nprint(\"Value Premium (HML):\")\nprint(f\"  Naive (look-ahead):     {ann_naive:.4f} ({ann_naive*100:.2f}%/yr)\")\nprint(f\"  Point-in-time:          {ann_pit:.4f} ({ann_pit*100:.2f}%/yr)\")\nprint(f\"  Look-ahead inflation:   {(ann_naive - ann_pit)*100:.2f}%/yr\")\n```\n:::\n\n\n## Exchange Transfers and Ticker Discontinuities {#sec-missing-transfers}\n\n### The Transfer Problem\n\nVietnamese firms frequently transfer between exchanges (e.g., from HNX to HOSE upon meeting HOSE's listing requirements, or from HOSE to UPCoM/HNX following regulatory issues). These transfers can break the continuity of return series if the database treats each exchange listing as a separate entity.\n\n::: {#transfers .cell execution_count=17}\n``` {.python .cell-code code-summary=\"Identify and link exchange transfers\"}\ntransfers = listing_history[\n    listing_history['transfer_from'].notna()\n].copy()\n\nprint(f\"Total exchange transfers: {len(transfers)}\")\nprint(f\"\\nTransfer patterns:\")\ntransfer_pattern = transfers.groupby(\n    ['transfer_from', 'transfer_to']\n).size().sort_values(ascending=False)\nprint(transfer_pattern.head(10))\n```\n:::\n\n\n::: {#link-transfers .cell execution_count=18}\n``` {.python .cell-code code-summary=\"Link pre-transfer and post-transfer return series\"}\ndef link_transfer_returns(monthly_df, transfers_df):\n    \"\"\"\n    Link return series across exchange transfers to create\n    continuous firm-level return histories.\n    \"\"\"\n    # Build mapping: old_ticker -> new_ticker -> transfer_date\n    transfer_map = {}\n    for _, row in transfers_df.iterrows():\n        old_ticker = row.get('transfer_from_ticker', row['ticker'])\n        new_ticker = row['ticker']\n        transfer_date = row['transfer_date']\n\n        if old_ticker and new_ticker and old_ticker != new_ticker:\n            transfer_map[old_ticker] = {\n                'new_ticker': new_ticker,\n                'date': transfer_date\n            }\n\n    # Create unified ticker mapping\n    df = monthly_df.copy()\n    df['unified_ticker'] = df['ticker']\n\n    for old_t, info in transfer_map.items():\n        mask = (\n            (df['ticker'] == old_t) &\n            (df['month_end'] < info['date'])\n        )\n        df.loc[mask, 'unified_ticker'] = info['new_ticker']\n\n    n_linked = sum(1 for t in transfer_map if t in df['ticker'].values)\n    print(f\"Linked {n_linked} transfer pairs\")\n\n    return df\n\nlinked = link_transfer_returns(monthly_returns, transfers)\n```\n:::\n\n\n## Sensitivity Analysis Framework {#sec-missing-sensitivity}\n\n### How Fragile Are Your Results?\n\nRather than choosing a single approach to handle missing data, a robust study tests how sensitive its conclusions are to different assumptions. We implement a systematic sensitivity framework that re-runs a given analysis under multiple data treatment assumptions.\n\n::: {#sensitivity-framework .cell execution_count=19}\n``` {.python .cell-code code-summary=\"Implement sensitivity analysis for missing data assumptions\"}\ndef sensitivity_analysis(monthly_df, listing_df, sort_variable,\n                          compute_fn):\n    \"\"\"\n    Run an analysis under multiple data treatment assumptions.\n\n    Parameters\n    ----------\n    monthly_df : Full monthly return panel (including delisted)\n    listing_df : Listing history with delisting info\n    sort_variable : Column name for portfolio sorting\n    compute_fn : Function that takes a DataFrame and returns a\n                 scalar (e.g., annualized long-short return)\n\n    Returns\n    -------\n    DataFrame with results under each assumption\n    \"\"\"\n    survivors = set(listing_df[listing_df['is_active']]['ticker'])\n    results = {}\n\n    # 1. Survivors only (maximum bias)\n    surv_only = monthly_df[monthly_df['ticker'].isin(survivors)]\n    results['Survivors Only'] = compute_fn(surv_only, sort_variable)\n\n    # 2. Full sample, no delisting imputation\n    results['Full Sample (no imputation)'] = compute_fn(\n        monthly_df, sort_variable\n    )\n\n    # 3. Full sample + conservative delisting imputation (-50%)\n    augmented_50 = monthly_df.copy()\n    # Append imputed returns at -50%\n    for _, firm in listing_df[listing_df['delisting_date'].notna()].iterrows():\n        if 'Financial Distress' in str(firm.get('reason_category', '')):\n            augmented_50 = pd.concat([augmented_50, pd.DataFrame([{\n                'ticker': firm['ticker'],\n                'month_end': firm['delisting_date'],\n                'monthly_return': -0.50,\n                'market_cap': np.nan,\n                sort_variable: np.nan\n            }])], ignore_index=True)\n    results['Full + Impute -50%'] = compute_fn(\n        augmented_50, sort_variable\n    )\n\n    # 4. Full sample + aggressive delisting imputation (-100%)\n    augmented_100 = monthly_df.copy()\n    for _, firm in listing_df[listing_df['delisting_date'].notna()].iterrows():\n        if 'Financial Distress' in str(firm.get('reason_category', '')):\n            augmented_100 = pd.concat([augmented_100, pd.DataFrame([{\n                'ticker': firm['ticker'],\n                'month_end': firm['delisting_date'],\n                'monthly_return': -1.00,\n                'market_cap': np.nan,\n                sort_variable: np.nan\n            }])], ignore_index=True)\n    results['Full + Impute -100%'] = compute_fn(\n        augmented_100, sort_variable\n    )\n\n    # 5. Exclude bottom market cap quintile (liquidity filter)\n    liquid = monthly_df.copy()\n    liquid['mcap_quintile'] = (\n        liquid.groupby('month_end')['market_cap']\n        .transform(lambda x: pd.qcut(x, 5, labels=False, duplicates='drop'))\n    )\n    liquid = liquid[liquid['mcap_quintile'] > 0]\n    results['Exclude Bottom Quintile'] = compute_fn(\n        liquid, sort_variable\n    )\n\n    return pd.DataFrame.from_dict(results, orient='index',\n                                   columns=['Result'])\n\n# Example: sensitivity of size premium\ndef compute_size_premium(df, sort_var):\n    ls = compute_long_short(df, sort_var, n_quantiles=5)\n    return ls['long_short'].mean() * 12 if len(ls) > 0 else np.nan\n\n# Would need sort variable in the data; illustrative call:\n# sensitivity_results = sensitivity_analysis(\n#     monthly_with_chars, listing_history, 'log_mcap',\n#     compute_size_premium\n# )\n```\n:::\n\n\n::: {#fig-sensitivity .cell execution_count=20}\n``` {.python .cell-code code-summary=\"Visualize sensitivity of anomaly premia to data treatment\"}\n# Illustrative: create synthetic sensitivity results for plotting\nassumptions = [\n    'Survivors Only',\n    'Full Sample\\n(no imputation)',\n    'Full +\\nImpute -30%',\n    'Full +\\nImpute -50%',\n    'Full +\\nImpute -100%',\n    'Exclude Bottom\\nMcap Quintile'\n]\n\n# Hypothetical results (would be computed from actual data)\nsize_premium = [0.08, 0.06, 0.055, 0.05, 0.04, 0.07]\nvalue_premium = [0.07, 0.065, 0.063, 0.06, 0.055, 0.068]\nmomentum_premium = [0.10, 0.08, 0.075, 0.07, 0.06, 0.09]\n\nfig, ax = plt.subplots(figsize=(14, 6))\nx = np.arange(len(assumptions))\nwidth = 0.25\n\nax.bar(x - width, [s * 100 for s in size_premium], width,\n       color='#2C5F8A', alpha=0.85, label='Size (SMB)')\nax.bar(x, [v * 100 for v in value_premium], width,\n       color='#27AE60', alpha=0.85, label='Value (HML)')\nax.bar(x + width, [m * 100 for m in momentum_premium], width,\n       color='#E67E22', alpha=0.85, label='Momentum (WML)')\n\nax.set_xticks(x)\nax.set_xticklabels(assumptions, fontsize=9)\nax.set_ylabel('Annualized Premium (%)')\nax.set_title('Anomaly Premium Sensitivity to Data Treatment')\nax.legend()\nax.axhline(y=0, color='gray', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Practical Recommendations {#sec-missing-recommendations}\n\nBased on the analysis in this chapter, we offer the following recommendations for researchers working with Vietnamese equity data:\n\n**1. Always use survivorship-bias-free databases.** When querying DataCore.vn (or any database), explicitly request `include_delisted=True`. Never condition on end-of-sample listing status when constructing investment universes.\n\n**2. Impute delisting returns.** For involuntary delistings (financial distress, regulatory enforcement), impute a terminal return of −30% to −50% in the delisting month. Report results across a range of imputation assumptions as a robustness check. For voluntary delistings and exchange transfers, impute 0%.\n\n**3. Respect point-in-time data availability.** Use accounting data only after its public filing date, not as of the fiscal year-end. In Vietnam, the standard lag is 90 days for annual reports; use a conservative 4--6 month lag.\n\n**4. Handle zero-volume days explicitly.** Document the prevalence of zero-volume days in your sample. For monthly returns, report the average number of zero-volume days per firm-month. Consider excluding firms with zero-volume fractions exceeding 50% from the investable universe.\n\n**5. Link exchange transfers.** Use unified tickers that link pre-transfer and post-transfer series. Without this, exchange transfers appear as simultaneous delistings and new listings, inflating turnover and biasing survival calculations.\n\n**6. Report sensitivity analysis.** For any key finding, report results under at least three data treatment assumptions: survivors-only (upper bound), full sample with moderate imputation (baseline), and full sample with aggressive imputation plus liquidity filter (lower bound). If the finding survives all three, it is robust.\n\n**7. Be especially cautious with pre-2007 data.** The Vietnamese market had fewer than 100 listings before 2006, and the equitization wave of 2006-2009 produced a cohort of firms with systematically different characteristics than earlier listings. Cross-sectional tests with pre-2007 data have minimal power and should be interpreted with extreme caution.\n\n## Summary {#sec-missing-summary}\n\n| Data Problem | Bias Direction | Magnitude (Vietnam) | Recommended Fix |\n|------------------|------------------|------------------|------------------|\n| Survivorship bias (EW) | Upward on returns | \\~1-3% per year | Include all delisted firms |\n| Survivorship bias (VW) | Upward (smaller) | \\~0.2-0.5% per year | Include all delisted firms |\n| Delisting return bias | Upward on returns | \\~0.5--2% per year (EW) | Impute terminal returns |\n| Look-ahead bias | Inflates predictability | Varies by strategy | Point-in-time data alignment |\n| Zero-trading days | Understates volatility | Severe for small caps | Compound or drop; document |\n| Exchange transfers | Creates false delistings | \\~50-100 firms | Link unified tickers |\n| New-listing bias | Early sample unrepresentative | Extreme pre-2007 | Start sample after 2007 |\n\n: Summary of data problems, their effects, and recommended corrections. {#tbl-missing-summary}\n\nThe central message is that data problems in Vietnamese equity research are not merely a nuisance--they can create economically significant biases that alter the conclusions of empirical studies. The survivorship bias alone exceeds 100 basis points per year for equal-weighted portfolios, comparable to many documented anomaly premia. Researchers who ignore these issues risk reporting results that reflect data artifacts rather than genuine economic phenomena.\n\n```{=html}\n<!-- ## Exercises {#sec-missing-exercises}\n\n1. **Delisting return estimation from final prices.** For a sample of delisted Vietnamese firms, obtain the last 60 trading days of price data before delisting. Compute the cumulative return over this window. What is the average \"run-up to delisting\" return? Does it vary by delisting reason?\n\n2. **Survivorship bias in fund performance.** Vietnamese open-ended mutual funds also experience survivorship bias (poorly performing funds are closed or merged). Using DataCore.vn's fund data, compare average fund returns in a survivorship-bias-free sample versus a survivors-only sample. How does the bias compare to @rohleder2011survivorship's findings for U.S. funds?\n\n3. **Heckman selection correction.** Implement the @heckman1979sample two-step correction for survivorship bias. In the first stage, estimate a probit model for the probability of delisting. In the second stage, include the inverse Mills ratio as a control variable in cross-sectional return regressions. Does the correction materially change the estimated size or value premia?\n\n4. **Multiple imputation.** Instead of a single imputed delisting return, implement a multiple imputation procedure: draw the delisting return from a distribution calibrated to observed terminal returns of similar firms. Repeat the analysis 100 times and report the distribution of results. How does parameter uncertainty from imputation compare to sampling uncertainty?\n\n5. **Backfill bias detection.** Identify firms in the DataCore.vn database where historical data appears before the official listing date. What fraction of firms exhibit this pattern? Does excluding backfilled observations change the estimated size premium?\n\n6. **Zero-volume and momentum.** Test whether the momentum anomaly is affected by the treatment of zero-volume days. Specifically, do zero-volume days create spurious autocorrelation in individual stock returns that inflates the apparent momentum premium?\n\n7. **Real-time delisting monitor.** Design a scoring model that predicts involuntary delisting 6--12 months in advance using financial ratios, trading activity, and governance indicators. What is the model's predictive accuracy? How would a portfolio that screens out high-delisting-risk firms perform relative to an unscreened benchmark?\n\n8. **Cross-country comparison.** Compare the survivorship bias magnitude in Vietnam to estimates for the U.S. [@shumway1997delisting], other ASEAN markets, and China. What institutional features explain cross-country differences in the severity of survivorship bias? -->\n```\n\n",
    "supporting": [
      "15_missing_data_and_survivorship_files/figure-pdf"
    ],
    "filters": []
  }
}