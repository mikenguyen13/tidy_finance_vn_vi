{
  "hash": "b3c970cbdefe753ae82a58e5433571f3",
  "result": {
    "engine": "jupyter",
    "markdown": "# Measuring Divergence of Investor Opinion\n\nA foundational question in financial economics concerns how differences in investor beliefs affect asset prices and trading activity. In markets where investors hold heterogeneous expectations about a firm's future cash flows, the aggregation of these divergent views into a single market price becomes a non-trivial exercise with profound implications for asset valuation, return predictability, and market efficiency. The concept of **divergence of investor opinion** (hereafter DIVOP) has emerged as a central construct in both the accounting and finance literatures, serving as a lens through which researchers examine the information environment of firms, the dynamics of uncertainty resolution, and the nature of market reactions to news.\n\nThe theoretical foundations of the DIVOP literature trace back to @miller1977risk, who proposed that when investors disagree about the value of a security and short-sale constraints prevent pessimistic investors from fully expressing their views, the market price will reflect the valuation of the most optimistic investors. This leads to systematic overpricing that is increasing in the degree of opinion divergence. The overpricing persists until information events, such as earnings announcements, reduce disagreement and prices converge toward fundamental values [@berkman2009sell]. @varian1985divergence offers an alternative perspective in which divergence of opinion represents an additional risk factor, leading to *higher* rather than lower expected returns, creating a theoretical tension that has motivated extensive empirical investigation.\n\nThe empirical literature on DIVOP has expanded considerably since these seminal contributions. Researchers have documented that divergence of opinion helps explain a range of asset pricing anomalies, including post-earnings announcement drift [@garfinkel2006volume; @anderson2007opinion], the cross-sectional return difference between value and growth stocks [@doukas2004divergent], short- and long-run post-IPO returns [@houge2001divergence], pre- and post-acquisition stock returns [@alexandridis2007divergence], takeover premia [@chatterjee2012takeovers], and the broad cross-section of stock returns [@diether2002differences; @doukas2006divergence]. The explanatory power of DIVOP has been demonstrated using a rich set of empirical proxies, ranging from analyst forecast dispersion and abnormal trading volume to bid-ask spreads and idiosyncratic volatility.\n\nDespite the maturity of the DIVOP literature in developed markets, particularly the United States, its application to emerging markets remains remarkably thin. This gap is especially notable given that the theoretical conditions under which divergence of opinion matters most (namely, binding short-sale constraints, information asymmetry, and heterogeneous investor sophistication) are arguably *more* prevalent in emerging markets than in their developed counterparts. The Vietnamese equity market presents a compelling laboratory for studying investor disagreement. The market is characterized by several features that amplify the relevance of the DIVOP framework:\n\n1.  **Binding short-sale constraints.** Short selling was not permitted in Vietnam until January 2025, and even after its introduction, the mechanism remains restricted to a limited set of securities with significant regulatory constraints on execution. This closely mirrors the theoretical setting of @miller1977risk where pessimistic investors are unable to fully express their views through short positions.\n\n2.  **Dominance of retail investors.** Individual investors account for approximately 80-85% of daily trading volume on HOSE and HNX, compared to roughly 25% in the United States. Retail investors are more susceptible to behavioral biases, sentiment-driven trading, and information processing limitations that naturally give rise to heterogeneous beliefs [@phan2023role].\n\n3.  **Information asymmetry and transparency challenges.** Despite improvements in disclosure standards, Vietnam's regulatory framework for corporate reporting remains less stringent than those in developed markets. Selective disclosure, delayed filing of financial statements, and limited enforcement of insider trading regulations create an environment in which investors operate with substantially different information sets [@vo2017further].\n\n4.  **Foreign ownership limits.** Caps on foreign ownership (currently 49% for most sectors, with exceptions) create a segmented market where domestic and foreign investors may hold systematically different views about firm value, amplifying the divergence of opinion.\n\n5.  **Thin analyst coverage.** Whereas a typical S&P 500 firm is followed by 15-25 sell-side analysts, coverage of Vietnamese equities is concentrated among a relatively small number of domestic brokerages and a handful of international research houses. This limits the informativeness of traditional analyst-based DIVOP measures and necessitates greater reliance on market-based proxies.\n\nThis chapter provides a methodology for constructing multiple proxies for divergence of investor opinion adapted to the institutional characteristics of the Vietnamese market. We draw on the methodological frameworks established by @garfinkel2009measuring and @diether2002differences, while introducing modifications that account for the microstructure of Vietnamese exchanges, the $T+2$ settlement cycle, the absence (until recently) of short selling, and the availability of data through domestic financial platforms. Specifically, we construct and analyze the following DIVOP proxies:\n\n-   **Unexplained Volume (DTO):** Market-adjusted turnover detrended by its rolling median, capturing abnormal trading activity attributable to disagreement after controlling for liquidity and market-wide effects.\n-   **Standardized Unexplained Volume (SUV):** A regression-based measure that explicitly controls for the informedness and liquidity components of volume by modeling turnover as a function of signed returns.\n-   **Stock Return Volatility (VOLATILITY):** The standard deviation of daily returns over a rolling estimation window, serving as a proxy for the dispersion of investor valuations.\n-   **Bid-Ask Spread (BASPREAD):** The proportional quoted spread, reflecting the adverse selection component associated with heterogeneous information among market participants.\n-   **Analyst Forecast Dispersion (DISP):** The cross-sectional standard deviation of individual analyst earnings forecasts, directly measuring disagreement among informed market participants.\n-   **Idiosyncratic Volatility (IVOL):** The residual volatility from a factor model regression, isolating the firm-specific component of return variation that reflects divergent investor interpretations of firm-level information.\n-   **Amihud Illiquidity (ILLIQ):** The price impact ratio proposed by @amihud2002illiquidity, which captures the information asymmetry dimension of disagreement through the price response to order flow.\n\nFor each proxy, we describe the theoretical motivation, the data requirements, the construction methodology adapted for Vietnamese data, the empirical properties observed in the Vietnamese cross-section, and the practical considerations that researchers should bear in mind when employing these measures. We pay particular attention to issues that are specific to emerging markets, including thin trading, corporate action adjustments, exchange-specific microstructure effects, and the interplay between foreign ownership constraints and measures of investor disagreement.\n\n------------------------------------------------------------------------\n\n# Theoretical Framework {#sec-theoretical-framework}\n\n## The Miller (1977) Overpricing Hypothesis\n\nThe canonical model of divergence of opinion and asset pricing begins with @miller1977risk. Miller's central insight is simple: in a market where investors hold heterogeneous beliefs about the future payoffs of a risky asset and short-sale constraints prevent some investors from acting on their pessimistic views, the equilibrium price will be set by the subset of investors who are most optimistic about the asset's value. The severity of overpricing is increasing in both the degree of opinion divergence and the stringency of short-sale constraints. Formally, if investor $i$ assigns a valuation $V_i$ to a security, the market price $P$ satisfies:\n\n$$\nP = E[V_i \\mid V_i \\geq V^*]\n$$\n\nwhere $V^*$ is the marginal investor's valuation, which exceeds the unconditional mean valuation $E[V_i]$ whenever short-sale constraints bind for some investors. The degree of overpricing is:\n\n$$\n\\text{Overpricing} = P - E[V_i] = E[V_i \\mid V_i \\geq V^*] - E[V_i]\n$$\n\nwhich is positive and increasing in the dispersion of the distribution of $V_i$ (i.e., divergence of opinion) and in $V^*$ (i.e., the severity of short-sale constraints).\n\nMiller's model generates several testable predictions:\n\n-   **Cross-sectional prediction:** Stocks with **higher divergence of opinion should have *lower* subsequent returns** as prices gradually correct toward fundamental values.\n-   **Time-series prediction:** Information events that reduce disagreement (e.g., earnings announcements) should be associated with negative abnormal returns for high-DIVOP stocks, as the \"optimism premium\" dissipates.\n-   **Interaction prediction:** The overpricing effect should be strongest among stocks that simultaneously exhibit high divergence of opinion *and* binding short-sale constraints.\n\n## Alternative Theoretical Perspectives\n\n@varian1985divergence proposes an alternative framework in which divergence of opinion acts as a risk factor. If investors are risk-averse and disagreement represents genuine uncertainty about future payoffs, then **higher dispersion of beliefs should be associated with *higher* expected returns** as compensation for bearing the additional risk. This creates a sharp empirical dichotomy: the Miller hypothesis predicts a negative DIVOP-return relation, whereas the Varian model predicts a positive relation.\n\nThe distinction between these theories hinges critically on the market microstructure and institutional setting (\\@tbl-divop-theories).\n\n| Theoretical Framework | Short-Sale Constraints | DIVOP-Return Relation | Key Mechanism |\n|:-----------------|:-----------------|:-----------------|:-----------------|\n| @miller1977risk | Binding | Negative | Optimistic bias in price |\n| @varian1985divergence | Non-binding | Positive | Risk premium for uncertainty |\n| @hong2003differences | Binding, gradual info | Negative, time-varying | Slow diffusion of bearish views |\n| @scheinkman2003overconfidence | Binding, overconfidence | Negative | Speculative bubble premium |\n\n: Summary of theoretical predictions for the DIVOP-return relation under different assumptions {#tbl-divop-theories}\n\n@hong2003differences extend Miller's framework by incorporating gradual information diffusion. In their model, bearish information is impounded into prices more slowly than bullish information because short-sale constraints raise the cost of acting on negative views. This generates momentum-like patterns in which high-DIVOP stocks exhibit positive short-run returns (as optimists push prices up) followed by negative long-run returns (as bearish information eventually reaches the market).\n\n@scheinkman2003overconfidence introduce an additional dimension by noting that when investors are overconfident about their private signals *and* short-sale constraints bind, stock prices contain a \"speculative bubble\" component that reflects the option value of reselling the asset to a future investor who may be even more optimistic. This model predicts that both high trading volume and high price volatility should be associated with overpricing, providing a theoretical basis for using volume-based and volatility-based DIVOP proxies.\n\n## Relevance to the Vietnamese Market\n\nThe Vietnamese equity market provides an unusually clean setting for testing the Miller hypothesis. Vietnam's equity market operated without any short-selling mechanism from its inception in 2000 through January 2025, which was a full quarter-century in which the first necessary condition of Miller's model (binding short-sale constraints) was satisfied by regulation rather than by market frictions. Even after the introduction of covered short selling in 2025, the mechanism remains restricted to securities meeting specific liquidity and market capitalization thresholds, and the regulatory environment imposes borrowing requirements that significantly raise the cost of shorting relative to developed markets.\n\nThe dominance of retail investors amplifies the second necessary condition (i.e., heterogeneous beliefs). Research on the Vietnamese market has documented significant herding behavior [@vo2017further; @vo2015foreign], sentiment-driven trading [@phan2023role; @nguyen2018search], and information asymmetry between domestic and foreign investors [@vo2017foreign]. These behavioral characteristics naturally generate wider dispersion of investor valuations compared to markets dominated by institutional investors with access to similar analytical frameworks and information sources.\n\n@tbl-divop-vietnam-us compares key institutional features relevant to the DIVOP framework between Vietnam and the United States.\n\n| Feature | Vietnam (HOSE/HNX) | United States (NYSE/NASDAQ) |\n|:-----------------------|:-----------------------|:-----------------------|\n| Short selling | Introduced Jan 2025 (limited) | Permitted (Reg SHO since 2005) |\n| Retail investor share of volume | \\~80-85% | \\~25% |\n| Settlement cycle | T+2 (T+1 planned for 2026) | T+1 (since May 2024) |\n| Daily price limits | $\\pm$ 7% (HOSE), $\\pm$ 10% (HNX) | None |\n| Foreign ownership cap | 49% (most sectors) | None |\n| Average analyst coverage (VN30) | 5-10 analysts | 15-25 analysts |\n| Mandatory quarterly reporting | Yes (since 2012) | Yes |\n| Options/derivatives market | VN30 Index Futures (since 2017) | Extensive options/futures |\n\n: Institutional comparison of Vietnam and the United States relevant to divergence of opinion {#tbl-divop-vietnam-us}\n\nThe presence of daily price limits ($\\pm$ 7% on HOSE and $\\pm$ 10% on HNX) creates an additional mechanism through which divergence of opinion can be amplified. When a stock hits its price limit, investors who wish to trade in the direction of the limit are unable to do so, leading to accumulated unfilled orders and delayed price discovery. This institutional feature may create short-term spikes in measured DIVOP that reflect limit-induced friction rather than genuine disagreement. We address this issue in our empirical methodology by flagging limit-hit days and conducting robustness checks that exclude these observations.\n\n# Data Sources and Sample Construction {#sec-data}\n\n## Data Sources\n\nThe construction of DIVOP proxies for the Vietnamese market requires daily stock-level trading data and, for the analyst dispersion measures, individual analyst forecast data. We source all data from [DataCore.vn](https://datacore.vn/en), which provides coverage of all securities listed on HOSE, HNX, and the UPCoM (Unlisted Public Company Market) exchange. @tbl-divop-data-sources summarizes the datasets and key variables used in this study.\n\n| Dataset | Key Variables | Frequency |\n|:-----------------------|:-----------------------|:-----------------------|\n| Daily Stock Trading | Close price, high, low, open, volume, shares outstanding, adjusted price, bid, ask | Daily |\n| Corporate Actions | Dividends, stock splits, bonus issues, rights offerings | Event-based |\n| Company Information | Exchange code, industry classification (ICB), listing date, delisting date | Static/Periodic |\n| Analyst Forecasts | Individual analyst EPS forecasts, announcement dates, fiscal period end, analyst ID, broker name | Per estimate |\n| Market Index | VN-Index daily returns, VN30 returns, HNX-Index returns | Daily |\n| Foreign Ownership | Foreign buy/sell volume, foreign ownership percentage, remaining foreign room | Daily |\n\n: Data sources and key variables for DIVOP proxy construction {#tbl-divop-data-sources}\n\n## Sample Construction\n\nWe construct our sample using the following filters, applied sequentially:\n\n::: {#sample-construction .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Sample construction and initial data loading\"}\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats as scipy_stats\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# =============================================================================\n# Configuration Parameters\n# =============================================================================\n# Users can modify these parameters to adjust the methodology\nCONFIG = {\n    # Sample period\n    'beg_date': '2007-01-01',\n    'end_date': '2024-12-31',\n    \n    # Estimation windows (in trading days)\n    'est_window': 60,          # Rolling window for SUV and volatility\n    'detrend_window': 180,     # Window for DTO detrending median\n    'lag': 7,                  # Lag for DTO detrending\n    'gap': 5,                  # Gap between estimation period and event date\n    \n    # Filters\n    'min_price': 1000,         # Minimum price in VND\n    'min_volume_days': 0.8,    # Min fraction of non-zero volume days in window\n    'min_analysts': 3,         # Minimum number of analysts for DISP\n    'max_spread_pct': 0.50,    # Maximum bid-ask spread as fraction of midpoint\n    'forecast_carry_days': 105,# Days to carry forward stale analyst forecasts\n    \n    # Exchange identifiers\n    'exchanges': ['HOSE', 'HNX'],\n    \n    # Price limit thresholds (for flagging)\n    'price_limit_hose': 0.07,\n    'price_limit_hnx': 0.10,\n}\n\nprint(\"Configuration parameters loaded successfully.\")\nprint(f\"Sample period: {CONFIG['beg_date']} to {CONFIG['end_date']}\")\nprint(f\"Estimation window: {CONFIG['est_window']} trading days\")\nprint(f\"Detrending window: {CONFIG['detrend_window']} trading days\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfiguration parameters loaded successfully.\nSample period: 2007-01-01 to 2024-12-31\nEstimation window: 60 trading days\nDetrending window: 180 trading days\n```\n:::\n:::\n\n\nThe sample universe includes all common stocks (ordinary shares) listed on HOSE and HNX during the period January 2007 through December 2024. We begin in 2007 rather than at market inception (2000 for HOSE, 2005 for HNX) for two reasons. First, the early years of the Vietnamese market were characterized by an extremely small number of listed firms (fewer than 30 on HOSE through 2005), making cross-sectional analysis unreliable. Second, data quality and consistency improve substantially after the market expansion of 2006-2007, during which the number of listed firms on HOSE grew from approximately 40 to over 100.\n\nWe apply the following filters to construct the analysis sample:\n\n1.  **Security type filter.** We retain only common stocks (ordinary shares), excluding preferred shares, exchange-traded funds (ETFs), covered warrants, and certificates of deposit. This is analogous to the standard filter in the U.S. literature that restricts to CRSP share codes 10 and 11.\n\n2.  **Exchange filter.** We include stocks listed on HOSE and HNX but exclude UPCoM securities in our baseline analysis. UPCoM is a registration-based trading venue with less stringent listing requirements and substantially lower liquidity, which may introduce noise into volume-based and spread-based measures. We include UPCoM in robustness checks.\n\n3.  **Price filter.** We exclude stock-day observations with closing prices below 1,000 VND. This threshold serves the same purpose as the \"penny stock\" exclusion common in U.S. studies (typically \\$1 or \\$5 thresholds) and helps mitigate the influence of extreme percentage returns and spreads at very low price levels.\n\n4.  **Minimum trading activity.** For volume-based measures, we require that a stock has non-zero trading volume on at least 80% of trading days within each estimation window. This filter eliminates the most thinly traded securities for which turnover-based measures would be unreliable.\n\n::: {#load-and-filter .cell execution_count=3}\n``` {.python .cell-code code-summary=\"Load and filter daily stock data\"}\ndef load_daily_data(config):\n    \"\"\"\n    Load daily stock trading data from DataCore.vn.\n    \n    In practice, this function connects to the DataCore API or reads\n    from a local database/CSV. Here we document the expected schema.\n    \n    Expected columns:\n    - ticker: str, stock ticker symbol (e.g., 'VCB', 'HPG', 'VNM')\n    - date: datetime, trading date\n    - open, high, low, close: float, daily OHLC prices (VND)\n    - volume: int, trading volume (shares)\n    - shares_outstanding: int, total shares outstanding\n    - adjusted_close: float, price adjusted for corporate actions\n    - adj_factor: float, cumulative adjustment factor\n    - bid, ask: float, best bid/ask at close\n    - exchange: str, exchange code ('HOSE', 'HNX', 'UPCOM')\n    - industry_icb: str, ICB industry classification code\n    - foreign_buy_vol, foreign_sell_vol: int, foreign investor volumes\n    - foreign_ownership_pct: float, foreign ownership percentage\n    \"\"\"\n    # =========================================================================\n    # Replace with actual DataCore API call:\n    # from datacore import Client\n    # client = Client(api_key='YOUR_KEY')\n    # df = client.daily_stock(\n    #     start=config['beg_date'], end=config['end_date'],\n    #     exchanges=config['exchanges']\n    # )\n    # =========================================================================\n    print(\"Connect to DataCore.vn and load daily stock data.\")\n    print(\"Expected schema: ticker, date, open, high, low, close, volume,\")\n    print(\"  shares_outstanding, adjusted_close, adj_factor, bid, ask,\")\n    print(\"  exchange, industry_icb, foreign_buy_vol, foreign_sell_vol,\")\n    print(\"  foreign_ownership_pct\")\n    return None  # Replace with actual data\n\n\ndef apply_sample_filters(df, config):\n    \"\"\"Apply sequential sample construction filters.\"\"\"\n    print(\"\\n=== Sample Construction ===\")\n    n0 = len(df)\n    \n    # Date filter\n    df = df[(df['date'] >= config['beg_date']) &\n            (df['date'] <= config['end_date'])].copy()\n    print(f\"[1] Date filter: {len(df):,} obs (from {n0:,})\")\n    \n    # Exchange filter\n    df = df[df['exchange'].isin(config['exchanges'])].copy()\n    print(f\"[2] Exchange filter ({config['exchanges']}): {len(df):,} obs\")\n    \n    # Price filter\n    df = df[df['close'] >= config['min_price']].copy()\n    print(f\"[3] Price >= {config['min_price']:,} VND: {len(df):,} obs\")\n    \n    # Compute daily return from adjusted prices\n    df = df.sort_values(['ticker', 'date'])\n    df['ret'] = df.groupby('ticker')['adjusted_close'].pct_change()\n    \n    # Flag price limit hits\n    df['limit_hit'] = (\n        ((df['exchange'] == 'HOSE') &\n         (df['ret'].abs() >= config['price_limit_hose'] - 0.001)) |\n        ((df['exchange'] == 'HNX') &\n         (df['ret'].abs() >= config['price_limit_hnx'] - 0.001))\n    )\n    \n    n_tickers = df['ticker'].nunique()\n    print(f\"\\nFinal sample: {len(df):,} stock-day obs, \"\n          f\"{n_tickers} unique tickers\")\n    print(f\"Limit-hit days: {df['limit_hit'].sum():,} \"\n          f\"({100*df['limit_hit'].mean():.2f}%)\")\n    return df\n```\n:::\n\n\n## Corporate Action Adjustments {#sec-corp-actions}\n\nProper adjustment for corporate actions is critical for volume-based DIVOP measures, as events such as stock splits, bonus share issues, and rights offerings change the number of shares outstanding and can create artificial spikes in measured turnover. We need to use cumulative adjustment factors that account for stock dividends (bonus shares), stock splits, rights offerings, and cash dividends (price adjustment only). We use these to construct adjusted volume and adjusted shares outstanding:\n\n$$\n\\text{AdjVolume}_{i,t} = \\text{Volume}_{i,t} \\times \\text{CumAdjFactor}_{i,t}\n$$\n\n$$\n\\text{AdjSharesOut}_{i,t} = \\text{SharesOut}_{i,t} \\times \\text{CumAdjFactor}_{i,t}\n$$\n\nThis ensures that the turnover ratio is consistent across corporate action events.\n\n::: {#corp-action .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Corporate action adjustment\"}\ndef adjust_for_corporate_actions(df):\n    \"\"\"Apply cumulative adjustment factors to volume and shares outstanding.\"\"\"\n    df = df.copy()\n    df['adj_volume'] = df['volume'] * df['adj_factor']\n    df['adj_shares_out'] = df['shares_outstanding'] * df['adj_factor']\n    \n    # Daily turnover ratio\n    df['turnover'] = np.where(\n        df['adj_shares_out'] > 0,\n        df['adj_volume'] / df['adj_shares_out'],\n        np.nan\n    )\n    \n    # Flag extreme turnover (> 50% of float)\n    extreme = df['turnover'] > 0.50\n    if extreme.any():\n        print(f\"Warning: {extreme.sum()} obs with turnover > 50%, set to NaN\")\n        df.loc[extreme, 'turnover'] = np.nan\n    \n    return df\n```\n:::\n\n\n## Trading Calendar Construction {#sec-calendar}\n\nThe rolling regression approach for SUV and volatility requires a trading calendar that ensures each estimation window contains exactly the specified number of trading days. We construct this directly from observed trading dates.\n\n::: {#trading-calendar .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Build trading calendar for rolling estimation windows\"}\ndef build_trading_calendar(df, config):\n    \"\"\"\n    Map each trading date to its estimation window [est_start, est_end].\n    \n    For date t, the estimation window runs from\n    t - gap - est_window to t - gap - 1 (in trading-day terms).\n    \"\"\"\n    trading_dates = sorted(df['date'].unique())\n    trading_dates = pd.Series(trading_dates)\n    \n    est_window = config['est_window']\n    gap = config['gap']\n    offset = est_window + gap\n    \n    records = []\n    for i in range(offset, len(trading_dates)):\n        records.append({\n            'date': trading_dates.iloc[i],\n            'est_start': trading_dates.iloc[i - gap - est_window],\n            'est_end': trading_dates.iloc[i - gap - 1]\n        })\n    \n    calendar = pd.DataFrame(records)\n    print(f\"Trading calendar: {len(calendar)} dates, \"\n          f\"{calendar['date'].min()} to {calendar['date'].max()}\")\n    return calendar\n```\n:::\n\n\n# Volume-Based DIVOP Proxies {#sec-volume-based}\n\n## Theoretical Motivation\n\nTrading volume has long been recognized as a natural proxy for divergence of investor opinion. In the rational expectations framework of @milgrom1982information, trade occurs only when investors disagree about the value of a security (i.e., a \"no-trade theorem\" that implies, by contrapositive, that observed trading volume must reflect some form of heterogeneous beliefs). @harris1993differences and @kandel1995differential formalize this intuition, showing that trading volume is positively related to the dispersion of investors' prior beliefs and to the degree to which public information is differentially interpreted.\n\nThe challenge in using raw trading volume as a DIVOP proxy is that volume is also driven by factors unrelated to disagreement, including portfolio rebalancing, liquidity needs, tax-loss selling, and index reconstitution effects. @garfinkel2009measuring proposes two approaches to extract the disagreement component from raw volume. The first, **Unexplained Volume (DTO)**, removes market-wide volume effects and secular trends. The second, **Standardized Unexplained Volume (SUV)**, additionally controls for the information content of returns through a cross-sectional regression, isolating the \"pure disagreement\" component of trading activity.\n\n## Unexplained Volume (DTO) {#sec-dto}\n\n### Construction Methodology\n\nThe construction of the Unexplained Volume measure proceeds in four steps.\n\n**Step 1: Compute firm-level daily turnover.** For each stock $i$ on day $t$:\n\n$$\n\\text{Turn}_{i,t} = \\frac{\\text{AdjVolume}_{i,t}}{\\text{AdjSharesOut}_{i,t}}\n$$\n\n**Step 2: Compute market-wide turnover.** We calculate aggregate turnover across all common stocks as a value-weighted average:\n\n$$\n\\text{MktTurn}_{t} = \\frac{\\sum_{i} \\text{AdjVolume}_{i,t}}{\\sum_{i} \\text{AdjSharesOut}_{i,t}}\n$$\n\nUnlike the U.S. methodology that computes market turnover across NYSE/AMEX stocks only and applies a scaling adjustment for NASDAQ securities [following @anderson2005market], we compute market turnover across all HOSE and HNX common stocks without any exchange-specific volume scaling. Both Vietnamese exchanges operate as order-driven markets (HOSE uses continuous order matching; HNX uses a combination of continuous matching and periodic call auctions) without the dealer-market double-counting issue that necessitates the NASDAQ volume adjustment in U.S. studies.\n\n**Step 3: Compute market-adjusted turnover.**\n\n$$\n\\text{MATO}_{i,t} = \\text{Turn}_{i,t} - \\text{MktTurn}_{t}\n$$\n\n**Step 4: Detrend by rolling median.** To remove secular trends in firm-specific trading activity:\n\n$$\n\\text{DTO}_{i,t} = \\text{MATO}_{i,t} - \\text{Median}_{180}(\\text{MATO}_{i,t-7})\n$$\n\nwhere $\\text{Median}_{180}(\\text{MATO}_{i,t-7})$ is the median of market-adjusted turnover over the 180-trading-day window ending 7 days before date $t$. The 7-day lag prevents the current day's turnover from influencing its own detrending baseline.\n\n::: {#dto-construction .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Construct the Unexplained Volume (DTO) measure\"}\ndef compute_market_turnover(df):\n    \"\"\"Compute daily market-wide turnover across all stocks.\"\"\"\n    mkt_turn = df.groupby('date').apply(\n        lambda x: x['adj_volume'].sum() / x['adj_shares_out'].sum()\n        if x['adj_shares_out'].sum() > 0 else np.nan\n    ).reset_index()\n    mkt_turn.columns = ['date', 'market_turnover']\n    return mkt_turn\n\n\ndef compute_dto(df, config):\n    \"\"\"\n    Construct Unexplained Volume (DTO).\n    \n    Steps:\n    1. Subtract market turnover -> MATO\n    2. Rolling 180-day median of MATO (lagged 7 days) -> trend\n    3. DTO = MATO - trend\n    \"\"\"\n    detrend_window = config['detrend_window']\n    lag = config['lag']\n    \n    # Market turnover\n    mkt_turn = compute_market_turnover(df)\n    df = df.merge(mkt_turn, on='date', how='left')\n    \n    # Market-adjusted turnover\n    df['mato'] = df['turnover'] - df['market_turnover']\n    \n    # Rolling median with lag, computed per stock\n    df = df.sort_values(['ticker', 'date'])\n    \n    def _rolling_median_lagged(group):\n        mato = group['mato']\n        med = mato.rolling(\n            window=detrend_window,\n            min_periods=int(detrend_window * 0.5)\n        ).median()\n        return med.shift(lag)\n    \n    df['mato_trend'] = (\n        df.groupby('ticker', group_keys=False)\n          .apply(lambda g: _rolling_median_lagged(g))\n    )\n    \n    # DTO\n    df['dto'] = df['mato'] - df['mato_trend']\n    \n    print(\"DTO construction complete.\")\n    print(f\"  Non-missing: {df['dto'].notna().sum():,}\")\n    print(f\"  Mean: {df['dto'].mean():.6f}, Std: {df['dto'].std():.6f}\")\n    return df\n```\n:::\n\n\n### Vietnam-Specific Considerations for DTO\n\nSeveral features of the Vietnamese market require attention when constructing DTO:\n\n1.  **No NASDAQ-type volume adjustment needed.** Both HOSE and HNX are order-driven auction markets. The double-counting adjustment applied to NASDAQ securities in the U.S. literature is not necessary.\n\n2.  **Thinly traded stocks.** A substantial fraction of listed Vietnamese stocks, particularly on HNX, may have zero volume on many trading days. For stocks with intermittent trading, the rolling median may be biased toward zero, making DTO less informative. We require at least 80% non-zero volume days in each estimation window.\n\n3.  **Price limit effects on volume.** When a stock hits its daily price limit, unfilled orders accumulate and recorded volume may understate true clearing volume. The following day often shows a \"catch-up\" effect. Researchers should consider flagging limit-hit days.\n\n4.  **Foreign investor trading decomposition.** DataCore provides volume by investor type (foreign versus domestic). Researchers may wish to construct separate DTO measures for foreign and domestic volume, or use the foreign-to-domestic volume ratio as an additional dimension of disagreement.\n\n## Standardized Unexplained Volume (SUV) {#sec-suv}\n\n### Construction Methodology\n\nThe Standardized Unexplained Volume measure, proposed by @garfinkel2009measuring, isolates the disagreement component of volume by explicitly controlling for the information content of returns. The insight is that trading volume has both a **liquidity** component and an **informedness** component correlated with the magnitude and sign of returns. By regressing turnover on signed returns and extracting the standardized residual, SUV captures volume attributable to disagreement after controlling for both liquidity trends and information-driven trading.\n\nFor each stock $i$, on each trading date $t$, we estimate using data from the estimation window $[\\tau_1, \\tau_2]$:\n\n$$\n\\text{Turn}_{i,s} = \\alpha_i + \\beta_i^{+} \\cdot \\text{RetPos}_{i,s} + \\beta_i^{-} \\cdot \\text{RetNeg}_{i,s} + \\epsilon_{i,s}, \\quad s \\in [\\tau_1, \\tau_2]\n$$ {#eq-suv-regression}\n\nwhere $\\text{RetPos}_{i,s} = |r_{i,s}| \\cdot \\mathbf{1}(r_{i,s} > 0)$ and $\\text{RetNeg}_{i,s} = |r_{i,s}| \\cdot \\mathbf{1}(r_{i,s} < 0)$.\n\nThe Standardized Unexplained Volume on date $t$ is:\n\n$$\n\\text{SUV}_{i,t} = \\frac{\\text{Turn}_{i,t} - \\hat{\\text{Turn}}_{i,t}}{\\hat{\\sigma}_{\\epsilon,i}}\n$$ {#eq-suv}\n\nwhere $\\hat{\\text{Turn}}_{i,t}$ is the predicted turnover and $\\hat{\\sigma}_{\\epsilon,i}$ is the RMSE from @eq-suv-regression.\n\nThe asymmetric specification with separate coefficients for positive and negative returns reflects that the volume-return relation differs by return sign. In the U.S., buying pressure tends to generate more volume than selling pressure due to short-sale frictions. In Vietnam, where short selling was unavailable until 2025, this asymmetry should be even more pronounced because all selling activity was constrained to existing shareholders.\n\n::: {#suv-construction .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Construct Standardized Unexplained Volume (SUV)\"}\ndef compute_suv(df, calendar, config):\n    \"\"\"\n    Compute Standardized Unexplained Volume via rolling regressions.\n    \n    For each stock-date, regress Turn on RetPos and RetNeg over the\n    estimation window, then compute SUV = (actual - predicted) / RMSE.\n    \"\"\"\n    est_window = config['est_window']\n    min_obs = int(est_window * config['min_volume_days'])\n    \n    # Prepare signed return components\n    df = df.copy()\n    df['ret_pos'] = np.where(df['ret'] > 0, np.abs(df['ret']), 0.0)\n    df['ret_neg'] = np.where(\n        (df['ret'] < 0) & df['ret'].notna(), np.abs(df['ret']), 0.0\n    )\n    \n    results = []\n    grouped = {t: g for t, g in df.groupby('ticker')}\n    \n    for _, cal_row in calendar.iterrows():\n        dt = cal_row['date']\n        est_s, est_e = cal_row['est_start'], cal_row['est_end']\n        \n        for ticker, tdata in grouped.items():\n            # Estimation window\n            est = tdata[\n                (tdata['date'] >= est_s) & (tdata['date'] <= est_e)\n            ].dropna(subset=['turnover', 'ret_pos', 'ret_neg'])\n            \n            if len(est) < min_obs:\n                continue\n            \n            # Event date\n            evt = tdata[tdata['date'] == dt]\n            if evt.empty or evt['turnover'].isna().all():\n                continue\n            \n            # OLS: Turn = alpha + beta_pos * RetPos + beta_neg * RetNeg\n            X = est[['ret_pos', 'ret_neg']].values\n            y = est['turnover'].values\n            \n            reg = LinearRegression().fit(X, y)\n            y_hat = reg.predict(X)\n            rmse = np.sqrt(np.mean((y - y_hat) ** 2))\n            \n            if rmse <= 0:\n                continue\n            \n            # Predict and standardize for event date\n            X_evt = evt[['ret_pos', 'ret_neg']].values\n            pred = reg.predict(X_evt)[0]\n            actual = evt['turnover'].values[0]\n            suv = (actual - pred) / rmse\n            \n            results.append({\n                'ticker': ticker, 'date': dt,\n                'suv': suv,\n                'predicted_turnover': pred,\n                'rmse_turn': rmse,\n                'n_est': len(est),\n                'alpha_turn': reg.intercept_,\n                'beta_pos': reg.coef_[0],\n                'beta_neg': reg.coef_[1],\n            })\n    \n    suv_df = pd.DataFrame(results)\n    print(f\"SUV: {len(suv_df):,} stock-date obs\")\n    print(f\"  Mean: {suv_df['suv'].mean():.4f}, \"\n          f\"Median: {suv_df['suv'].median():.4f}\")\n    return suv_df\n```\n:::\n\n\n### Interpreting the SUV Regression Coefficients\n\nThe estimated coefficients from @eq-suv-regression are informative about market microstructure. @garfinkel2009measuring reports $\\hat{\\beta}^{+} > \\hat{\\beta}^{-}$ for most U.S. stocks. In Vietnam, we expect this asymmetry to be even stronger because:\n\n-   **No short selling (pre-2025):** All selling is by existing shareholders, limiting volume response to negative returns.\n-   **T+2 settlement:** Investors cannot immediately reinvest sale proceeds, further dampening sell-side volume.\n-   **Price limits:** The $\\pm$ 7% (HOSE) and $\\pm$ 10% (HNX) daily limits truncate the return distribution, compressing the range of both regressors.\n\nResearchers should report summary statistics of $(\\hat{\\alpha}, \\hat{\\beta}^{+}, \\hat{\\beta}^{-}, R^2)$ across the cross-section and over time.\n\n::: {#suv-diagnostics .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Diagnostic statistics for SUV turnover regressions\"}\ndef suv_diagnostics(suv_df):\n    \"\"\"Report cross-sectional summary of SUV regression parameters.\"\"\"\n    print(\"\\n=== SUV Regression Diagnostics ===\")\n    \n    params = ['alpha_turn', 'beta_pos', 'beta_neg']\n    print(suv_df[params].describe(\n        percentiles=[.05, .25, .50, .75, .95]\n    ).T.to_string(float_format='{:.6f}'.format))\n    \n    # Asymmetry test\n    diff = suv_df['beta_pos'] - suv_df['beta_neg']\n    print(f\"\\nbeta_pos - beta_neg: mean = {diff.mean():.6f}, \"\n          f\"frac > 0 = {(diff > 0).mean():.3f}\")\n```\n:::\n\n\n# Volatility-Based DIVOP Proxies {#sec-volatility}\n\n## Total Return Volatility {#sec-total-vol}\n\n### Theoretical Motivation\n\nStock return volatility serves as a proxy for divergence of opinion through several channels. @shalen1993volume develops a model in which both volume and volatility are increasing in the dispersion of investor beliefs. @scheinkman2003overconfidence predict that higher volatility reflects the speculative trading component driven by overconfident investors who disagree about value. Empirically, @boehme2006short and @chatterjee2012takeovers use idiosyncratic volatility as a DIVOP proxy and find it positively correlated with other disagreement measures and negatively associated with subsequent returns when short-sale constraints bind.\n\n### Construction\n\nTotal return volatility is the standard deviation of daily returns over the rolling estimation window:\n\n$$\n\\text{VOLATILITY}_{i,t} = \\sqrt{\\frac{1}{N_i - 1} \\sum_{s \\in [\\tau_1, \\tau_2]} (r_{i,s} - \\bar{r}_i)^2}\n$$ {#eq-volatility}\n\nwhere $N_i$ is the number of non-missing return observations for stock $i$ in the window $[\\tau_1, \\tau_2]$.\n\n## Idiosyncratic Volatility (IVOL) {#sec-ivol}\n\nIdiosyncratic volatility isolates firm-specific return variation by removing the systematic component explained by market movements. We compute IVOL from the residuals of a market model:\n\n$$\nr_{i,s} = \\alpha_i + \\beta_i \\cdot r_{m,s} + \\epsilon_{i,s}, \\quad s \\in [\\tau_1, \\tau_2]\n$$ {#eq-market-model}\n\n$$\n\\text{IVOL}_{i,t} = \\text{Std}(\\hat{\\epsilon}_{i,s})\n$$ {#eq-ivol}\n\nResearchers may extend this to a @fama1993common three-factor or five-factor model using Vietnamese factor portfolios constructed elsewhere in this book. A richer factor model yields IVOL estimates that better isolate truly idiosyncratic disagreement, at the cost of requiring factor portfolio construction.\n\n::: {#volatility-construction .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Construct total and idiosyncratic volatility\"}\ndef compute_volatility(df, calendar, config):\n    \"\"\"\n    Compute total return volatility and idiosyncratic volatility\n    via rolling estimation windows.\n    \n    Total vol = std(returns) in window.\n    IVOL = std(residuals) from market model regression.\n    \"\"\"\n    est_window = config['est_window']\n    min_obs = int(est_window * config['min_volume_days'])\n    \n    # Value-weighted market return\n    def _vw_ret(g):\n        valid = g.dropna(subset=['ret'])\n        if valid.empty:\n            return np.nan\n        w = valid['adj_shares_out'] * valid['close']\n        return np.average(valid['ret'], weights=w)\n    \n    mkt_ret = df.groupby('date').apply(_vw_ret).reset_index()\n    mkt_ret.columns = ['date', 'mkt_ret']\n    df = df.merge(mkt_ret, on='date', how='left')\n    \n    results = []\n    grouped = {t: g for t, g in df.groupby('ticker')}\n    \n    for _, cal_row in calendar.iterrows():\n        dt = cal_row['date']\n        est_s, est_e = cal_row['est_start'], cal_row['est_end']\n        \n        for ticker, tdata in grouped.items():\n            est = tdata[\n                (tdata['date'] >= est_s) & (tdata['date'] <= est_e)\n            ].dropna(subset=['ret', 'mkt_ret'])\n            \n            if len(est) < min_obs:\n                continue\n            \n            # Total volatility\n            total_vol = est['ret'].std()\n            \n            # Market model -> IVOL\n            X = est[['mkt_ret']].values\n            y = est['ret'].values\n            reg = LinearRegression().fit(X, y)\n            resid = y - reg.predict(X)\n            ivol = np.std(resid, ddof=1)\n            \n            results.append({\n                'ticker': ticker, 'date': dt,\n                'total_volatility': total_vol,\n                'idio_volatility': ivol,\n                'market_beta': reg.coef_[0],\n                'market_alpha': reg.intercept_,\n                'r_squared_mm': reg.score(X, y),\n                'n_vol': len(est),\n            })\n    \n    vol_df = pd.DataFrame(results)\n    print(f\"Volatility: {len(vol_df):,} stock-date obs\")\n    print(f\"  Total vol (ann. mean): \"\n          f\"{vol_df['total_volatility'].mean() * np.sqrt(252):.4f}\")\n    print(f\"  IVOL (ann. mean): \"\n          f\"{vol_df['idio_volatility'].mean() * np.sqrt(252):.4f}\")\n    return vol_df\n```\n:::\n\n\n### Vietnam-Specific Considerations for Volatility\n\n1.  **Price limits compress measured volatility.** Daily limits of $\\pm$ 7% (HOSE) and $\\pm$ 10% (HNX) mechanically truncate the return distribution, leading to underestimation of true volatility. On limit-hit days, the true equilibrium return may exceed the observed return. Researchers should be aware that volatility-based DIVOP measures may be downward-biased for stocks that frequently hit limits.\n\n2.  **VN-Index concentration.** The VN-Index is highly concentrated, the top 10 stocks often account for 50-60% of index weight. For small- and mid-cap stocks, an equal-weighted market return or a composite HOSE+HNX index may provide a better market factor in @eq-market-model.\n\n3.  **Thin trading and non-synchronous returns.** For thinly traded stocks, consecutive zero-return days can depress measured volatility. The @dimson1979risk adjustment (including lagged and lead market returns in the market model) may help correct for non-synchronous trading bias in the beta estimate, though its effect on IVOL is typically small.\n\n# Spread-Based and Liquidity DIVOP Proxies {#sec-spread}\n\n## Bid-Ask Spread (BASPREAD) {#sec-baspread}\n\n### Theoretical Motivation\n\nThe bid-ask spread reflects the adverse selection costs faced by limit order providers. When investors hold heterogeneous beliefs, each trade is more likely to convey private information, raising the adverse selection component of the spread. @handa2003quote show that in order-driven markets the spread widens when divergence of opinion increases because limit order providers face greater risk of being picked off by informed traders. @chung2014simple demonstrate that closing bid-ask spreads from daily data provide a reliable approximation to intraday effective spreads.\n\n### Construction\n\nWe compute the proportional bid-ask spread using end-of-day quote data:\n\n$$\n\\text{BASPREAD}_{i,t} = \\frac{\\text{Ask}_{i,t} - \\text{Bid}_{i,t}}{\\text{Midpoint}_{i,t}}\n$$ {#eq-baspread}\n\nwhere $\\text{Midpoint}_{i,t} = (\\text{Ask}_{i,t} + \\text{Bid}_{i,t}) / 2$. When end-of-day bid and ask are unavailable, we use the daily high-low range as a fallback. Following @chung2014simple, we delete observations where both Bid and Ask are zero, and where the spread exceeds 50% of the midpoint.\n\n## Amihud Illiquidity (ILLIQ) {#sec-amihud}\n\nThe @amihud2002illiquidity ratio measures the price impact of order flow:\n\n$$\n\\text{ILLIQ}_{i,t} = \\frac{|r_{i,t}|}{\\text{DolVol}_{i,t}}\n$$ {#eq-amihud}\n\nwhere $\\text{DolVol}_{i,t} = \\text{Volume}_{i,t} \\times \\text{Price}_{i,t}$ (in billions VND for scaling). Higher ILLIQ reflects greater information asymmetry. We average daily ratios over monthly horizons and use the log transformation due to heavy right skew.\n\n::: {#spread-illiq .cell execution_count=10}\n``` {.python .cell-code code-summary=\"Construct bid-ask spread and Amihud illiquidity\"}\ndef compute_spread_and_illiq(df, config):\n    \"\"\"Compute bid-ask spread (BASPREAD) and Amihud illiquidity.\"\"\"\n    df = df.copy()\n    \n    # --- Bid-Ask Spread ---\n    df['midpoint_ba'] = (df['ask'] + df['bid']) / 2\n    df['baspread_ba'] = np.where(\n        (df['ask'] > 0) & (df['bid'] > 0) & (df['midpoint_ba'] > 0),\n        (df['ask'] - df['bid']) / df['midpoint_ba'], np.nan\n    )\n    \n    # Fallback: high/low range\n    df['midpoint_hl'] = (df['high'] + df['low']) / 2\n    df['baspread_hl'] = np.where(\n        (df['high'] > 0) & (df['low'] > 0) & (df['midpoint_hl'] > 0),\n        (df['high'] - df['low']) / df['midpoint_hl'], np.nan\n    )\n    \n    df['baspread'] = df['baspread_ba'].fillna(df['baspread_hl'])\n    df['midpoint'] = df['midpoint_ba'].fillna(df['midpoint_hl'])\n    \n    # Chung & Zhang (2009) filters\n    bad = (df['baspread'].isna()) | \\\n          (df['baspread'] > config['max_spread_pct']) | \\\n          (df['baspread'] < 0)\n    df.loc[bad, 'baspread'] = np.nan\n    \n    # --- Amihud Illiquidity ---\n    df['dollar_vol'] = df['volume'] * df['close'] / 1e9\n    df['amihud_daily'] = np.where(\n        df['dollar_vol'] > 0,\n        np.abs(df['ret']) / df['dollar_vol'], np.nan\n    )\n    \n    print(f\"BASPREAD: {df['baspread'].notna().sum():,} valid obs, \"\n          f\"mean = {df['baspread'].mean():.6f}\")\n    print(f\"AMIHUD: {df['amihud_daily'].notna().sum():,} valid obs, \"\n          f\"mean = {df['amihud_daily'].mean():.6f}\")\n    return df\n\n\ndef compute_amihud_monthly(df):\n    \"\"\"Monthly Amihud = mean daily |ret|/dollar_vol (min 15 days).\"\"\"\n    df = df.copy()\n    df['ym'] = df['date'].dt.to_period('M')\n    agg = df.groupby(['ticker', 'ym']).agg(\n        illiq_mean=('amihud_daily', 'mean'),\n        n_days=('amihud_daily', 'count'),\n    ).reset_index()\n    agg = agg[agg['n_days'] >= 15].copy()\n    agg['log_illiq'] = np.log(agg['illiq_mean'] + 1e-10)\n    return agg\n```\n:::\n\n\n### Vietnam-Specific Considerations for Spread and Liquidity\n\n1.  **Tick size schedule.** Vietnam uses variable tick sizes: 10 VND (prices \\< 10,000), 50 VND (10,000--49,950), and 100 VND (â‰¥ 50,000) on HOSE. These impose a floor on quoted spreads for low-priced stocks. Researchers should be cautious interpreting cross-price-decile spread variation as reflecting opinion divergence rather than tick-size mechanics.\n\n2.  **Order-driven market structure.** Both HOSE and HNX are pure order-driven markets where public limit orders provide liquidity. This makes the @chung2014simple CRSP-based spread approximation appropriate.\n\n3.  **Lot size requirements.** HOSE requires 100-share standard lots for continuous trading. For high-priced stocks, the standard lot represents a large capital commitment, potentially inflating quoted spreads relative to effective trading costs.\n\n4.  **Call auction effects.** Opening and closing sessions on HOSE use periodic call auctions, which can produce bid-ask quotes that differ substantially from continuous-trading spreads.\n\n# Analyst Forecast Dispersion {#sec-analyst}\n\n## Theoretical Motivation\n\nAnalyst forecast dispersion, the cross-sectional standard deviation of individual analysts' earnings forecasts, is the most direct measure of divergence of opinion. Unlike market-based proxies that capture disagreement indirectly, forecast dispersion directly measures disagreement among informed market participants. @abarbanell1995analysts establish the theoretical basis, and @diether2002differences demonstrate that stocks with higher analyst forecast dispersion earn lower subsequent returns, consistent with the Miller overpricing hypothesis.\n\n## Data Challenges in Vietnam\n\nConstructing analyst forecast dispersion in Vietnam presents substantial challenges relative to the U.S.:\n\n-   **Coverage breadth.** While I/B/E/S covers over 4,000 U.S. companies, only 100--150 Vietnamese firms typically have coverage by at least 3 analysts, concentrated among VN30 constituents.\n-   **Data sources.** Analyst forecasts are available from DataCore.vn, FiinPro, Bloomberg, and Refinitiv. The choice of source affects coverage and timeliness.\n-   **Forecast staleness.** With limited coverage, forecasts may go unrevised for months. Following I/B/E/S methodology, we carry each forecast forward for a maximum of 105 days.\n\n## Construction Methodology\n\nThe construction proceeds as follows:\n\n1.  **Clean individual forecasts.** Remove observations where the announcement date precedes the review date. Keep only annual EPS forecasts. For each analyst-ticker-fiscal period, retain only the latest forecast per calendar month.\n2.  **Handle stopped and excluded estimates.** Remove forecasts where the analyst has left the brokerage or the estimate has been excluded from consensus.\n3.  **Carry forward with staleness control.** Each forecast is valid until the earlier of: (a) the next forecast by the same analyst, (b) 105 days after the announcement, or (c) the actual earnings announcement date.\n4.  **Expand to monthly frequency.** For each ticker-month, identify all valid outstanding forecasts and compute dispersion.\n5.  **Compute scaled measures:**\n\n$$\n\\text{DISP1}_{i,m} = \\frac{\\text{Std}(\\hat{\\text{EPS}}_{i,m}^{(a)})}{|\\text{Mean}(\\hat{\\text{EPS}}_{i,m}^{(a)})|}\n\\qquad\n\\text{DISP2}_{i,m} = \\frac{\\text{Std}(\\hat{\\text{EPS}}_{i,m}^{(a)})}{\\bar{P}_{i,m}}\n$$\n\n::: {#analyst-dispersion .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Construct analyst forecast dispersion (DISP1, DISP2)\"}\ndef construct_analyst_dispersion(forecasts_df, price_df, config):\n    \"\"\"\n    Construct analyst forecast dispersion measures.\n    \n    Parameters\n    ----------\n    forecasts_df : pd.DataFrame\n        Individual analyst forecasts with: ticker, analyst_id, broker,\n        fpedats, anndats, revdats, value (EPS), anndats_act.\n    price_df : pd.DataFrame\n        Monthly price: ticker, month, mean_price.\n    config : dict\n        With min_analysts, forecast_carry_days.\n    \"\"\"\n    carry_days = config['forecast_carry_days']\n    min_analysts = config['min_analysts']\n    \n    df = forecasts_df.copy()\n    df = df[df['anndats'] <= df['revdats']].copy()\n    df = df.dropna(subset=['fpedats', 'anndats', 'value'])\n    \n    # Latest forecast per analyst-month\n    df['ym'] = df['anndats'].dt.to_period('M')\n    df = df.sort_values(\n        ['ticker', 'fpedats', 'analyst_id', 'ym', 'anndats', 'revdats']\n    )\n    df = df.groupby(['ticker', 'fpedats', 'analyst_id', 'ym']).tail(1)\n    \n    # Carry-forward end date\n    df = df.sort_values(\n        ['ticker', 'analyst_id', 'fpedats', 'anndats'],\n        ascending=[True, True, True, False]\n    )\n    df['next_ann'] = df.groupby(\n        ['ticker', 'analyst_id', 'fpedats']\n    )['anndats'].shift(-1)\n    \n    def _carry_end(row):\n        candidates = [row['anndats'] + pd.Timedelta(days=carry_days)]\n        if pd.notna(row.get('next_ann')):\n            candidates.append(row['next_ann'])\n        if pd.notna(row.get('anndats_act')):\n            candidates.append(row['anndats_act'])\n        return min(candidates)\n    \n    df['carry_end'] = df.apply(_carry_end, axis=1)\n    \n    # Monthly expansion\n    months = pd.period_range(config['beg_date'], config['end_date'], freq='M')\n    records = []\n    for month in months:\n        me = month.to_timestamp(how='end')\n        valid = df[(df['anndats'] <= me) & (df['carry_end'] > me)].copy()\n        valid = valid[valid['fpedats'] > me]\n        valid = valid.sort_values(['ticker', 'analyst_id', 'anndats'])\n        valid = valid.groupby(['ticker', 'analyst_id']).tail(1)\n        \n        disp = valid.groupby('ticker').agg(\n            n_analysts=('analyst_id', 'nunique'),\n            mean_fcst=('value', 'mean'),\n            std_fcst=('value', 'std'),\n        ).reset_index()\n        disp['month'] = month\n        records.append(disp)\n    \n    if not records:\n        return pd.DataFrame()\n    disp_df = pd.concat(records, ignore_index=True)\n    \n    # Scaled measures\n    disp_df['disp1'] = np.where(\n        disp_df['mean_fcst'].abs() > 0,\n        disp_df['std_fcst'] / disp_df['mean_fcst'].abs(), np.nan\n    )\n    disp_df = disp_df.merge(price_df, on=['ticker', 'month'], how='left')\n    disp_df['disp2'] = np.where(\n        disp_df['mean_price'] > 0,\n        disp_df['std_fcst'] / disp_df['mean_price'], np.nan\n    )\n    disp_df['disp_raw'] = disp_df['std_fcst']\n    \n    out = disp_df[disp_df['n_analysts'] >= min_analysts].copy()\n    print(f\"DISP: {len(out):,} ticker-months (>= {min_analysts} analysts)\")\n    print(f\"  Mean analysts: {out['n_analysts'].mean():.1f}\")\n    return out\n```\n:::\n\n\n## Scaling Considerations\n\nFollowing @cheong2011eps, we note that each scaling choice has pitfalls. DISP1 (scaled by absolute mean forecast) can produce extreme values when the mean forecast approaches zero---common for Vietnamese firms near breakeven. DISP2 (scaled by price) introduces a mechanical negative correlation between price and scaled dispersion. We recommend reporting all three versions (DISP1, DISP2, and unscaled DISP_RAW with $\\ln(\\text{Price})$ as an additional control), and winsorizing DISP1 at the 1st and 99th percentiles.\n\n::: callout-warning\n## Caution on Analyst Dispersion in Thin-Coverage Markets\n\nWith typical coverage of 5--10 analysts per firm in Vietnam (versus 15--25 in the U.S.), forecast dispersion is estimated with substantially greater noise. A dispersion measure from 3 analysts has a very different sampling distribution than one from 20. Always include the number of analysts as a control and test robustness with varying minimum-analyst thresholds (3, 5, 7).\n:::\n\n# Cross-Sectional Correlations Among DIVOP Proxies {#sec-correlation}\n\nAn important empirical question is the degree to which the various DIVOP proxies capture the same underlying construct. If divergence of opinion is a well-defined latent variable, we expect positive correlations among all proxies, though correlations need not be high since each captures a different facet of disagreement.\n\n::: {#correlation-analysis .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Spearman rank correlations among DIVOP proxies\"}\ndef compute_divop_correlations(merged_df, proxies=None):\n    \"\"\"\n    Compute and visualize Spearman correlations among DIVOP proxies.\n    We use rank correlations because many proxies are right-skewed.\n    \"\"\"\n    if proxies is None:\n        proxies = [\n            'dto', 'suv', 'total_volatility', 'idio_volatility',\n            'baspread', 'amihud_daily', 'disp1', 'disp2'\n        ]\n    available = [p for p in proxies if p in merged_df.columns]\n    data = merged_df[available].dropna()\n    \n    n = len(available)\n    rho_mat = np.eye(n)\n    p_mat = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            rho, p = scipy_stats.spearmanr(\n                data[available[i]], data[available[j]]\n            )\n            rho_mat[i, j] = rho_mat[j, i] = rho\n            p_mat[i, j] = p_mat[j, i] = p\n    \n    labels = {'dto': 'DTO', 'suv': 'SUV',\n              'total_volatility': 'VOL', 'idio_volatility': 'IVOL',\n              'baspread': 'SPREAD', 'amihud_daily': 'ILLIQ',\n              'disp1': 'DISP1', 'disp2': 'DISP2'}\n    pretty = [labels.get(c, c) for c in available]\n    corr_df = pd.DataFrame(rho_mat, index=pretty, columns=pretty)\n    \n    # Heatmap\n    fig, ax = plt.subplots(figsize=(9, 7))\n    mask = np.triu(np.ones_like(corr_df, dtype=bool), k=1)\n    sns.heatmap(\n        corr_df, mask=mask, annot=True, fmt='.3f',\n        cmap='RdBu_r', center=0, vmin=-0.4, vmax=0.7,\n        square=True, linewidths=0.5,\n        cbar_kws={'shrink': 0.8, 'label': 'Spearman Ï'}, ax=ax\n    )\n    ax.set_title('Spearman Correlations Among DIVOP Proxies\\n'\n                  'Vietnamese Equity Market', fontsize=13, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig('divop_correlations.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    return corr_df\n```\n:::\n\n\n### Expected Correlation Patterns\n\nBased on U.S. evidence and theory, we expect:\n\n| Pair | Expected | Rationale |\n|:-----------------------|:-----------------------|:-----------------------|\n| DTO Ã— SUV | High positive | Both capture abnormal volume; SUV refines DTO |\n| VOL Ã— IVOL | High positive | IVOL is a subset of total volatility |\n| SPREAD Ã— ILLIQ | Moderate-high positive | Both capture information asymmetry |\n| Volume Ã— Volatility | Moderate positive | @shalen1993volume links both to belief dispersion |\n| Analyst Ã— Market-based | Weak-moderate positive | Different investor populations |\n\n: Expected correlation structure among DIVOP proxies {#tbl-expected-corr}\n\n# Descriptive Statistics and Cross-Sectional Properties {#sec-empirical}\n\n## Summary Statistics\n\n::: {#descriptive-stats .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Descriptive statistics for all DIVOP proxies\"}\ndef descriptive_statistics(merged_df):\n    \"\"\"Comprehensive descriptive statistics for DIVOP proxies.\"\"\"\n    proxies = {\n        'dto': 'Unexplained Volume (DTO)',\n        'suv': 'Std Unexplained Volume (SUV)',\n        'total_volatility': 'Total Return Volatility',\n        'idio_volatility': 'Idiosyncratic Volatility',\n        'baspread': 'Bid-Ask Spread',\n        'amihud_daily': 'Amihud Illiquidity',\n        'disp1': 'Analyst Disp (mean-scaled)',\n        'disp2': 'Analyst Disp (price-scaled)',\n    }\n    avail = {k: v for k, v in proxies.items() if k in merged_df.columns}\n    rows = []\n    for col, label in avail.items():\n        s = merged_df[col].dropna()\n        rows.append({\n            'Proxy': label, 'N': f'{len(s):,}',\n            'Mean': f'{s.mean():.6f}', 'Std': f'{s.std():.6f}',\n            'P5': f'{s.quantile(.05):.6f}',\n            'Median': f'{s.median():.6f}',\n            'P95': f'{s.quantile(.95):.6f}',\n            'Skew': f'{s.skew():.2f}',\n            'Kurt': f'{s.kurtosis():.2f}',\n        })\n    stats = pd.DataFrame(rows).set_index('Proxy')\n    print(\"\\n\" + \"=\" * 90)\n    print(\"Descriptive Statistics of DIVOP Proxies\")\n    print(\"Vietnamese Equity Market, HOSE and HNX\")\n    print(\"=\" * 90)\n    print(stats.to_string())\n    return stats\n```\n:::\n\n\n## DIVOP by Firm Characteristics\n\n::: {#by-characteristics .cell execution_count=14}\n``` {.python .cell-code code-summary=\"DIVOP by size, exchange, and foreign ownership\"}\ndef divop_by_size(merged_df):\n    \"\"\"Mean DIVOP proxies by market-cap quintile.\"\"\"\n    df = merged_df.copy()\n    df['mkt_cap'] = df['close'] * df['shares_outstanding']\n    df['size_q'] = df.groupby('date')['mkt_cap'].transform(\n        lambda x: pd.qcut(x, 5,\n            labels=['Q1 Small','Q2','Q3','Q4','Q5 Large'],\n            duplicates='drop')\n    )\n    proxies = ['dto','suv','total_volatility','idio_volatility',\n               'baspread','amihud_daily']\n    avail = [p for p in proxies if p in df.columns]\n    tab = df.groupby('size_q')[avail].mean()\n    print(\"\\n=== Mean DIVOP by Size Quintile ===\")\n    print(tab.to_string(float_format='{:.6f}'.format))\n    return tab\n\ndef divop_by_exchange(merged_df):\n    \"\"\"Compare mean DIVOP across HOSE and HNX.\"\"\"\n    proxies = ['dto','suv','total_volatility','idio_volatility',\n               'baspread','amihud_daily']\n    avail = [p for p in proxies if p in merged_df.columns]\n    tab = merged_df.groupby('exchange')[avail].mean()\n    print(\"\\n=== Mean DIVOP by Exchange ===\")\n    print(tab.to_string(float_format='{:.6f}'.format))\n    return tab\n```\n:::\n\n\n## Time-Series Evolution\n\n::: {#time-series-plot .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Time-series evolution of DIVOP proxies\"}\ndef plot_divop_timeseries(merged_df):\n    \"\"\"Plot monthly cross-sectional median DIVOP with crisis shading.\"\"\"\n    df = merged_df.copy()\n    df['ym'] = df['date'].dt.to_period('M')\n    proxies = ['dto','suv','total_volatility','baspread']\n    avail = [p for p in proxies if p in df.columns]\n    monthly = df.groupby('ym')[avail].median()\n    monthly.index = monthly.index.to_timestamp()\n    \n    fig, axes = plt.subplots(len(avail), 1,\n        figsize=(13, 3.5*len(avail)), sharex=True)\n    if len(avail) == 1: axes = [axes]\n    \n    labels = {'dto':'DTO','suv':'SUV',\n              'total_volatility':'Volatility','baspread':'Spread'}\n    colors = ['#1976D2','#388E3C','#F57C00','#D32F2F']\n    \n    for i, (proxy, ax) in enumerate(zip(avail, axes)):\n        ax.plot(monthly.index, monthly[proxy],\n                color=colors[i], linewidth=1.3)\n        ax.set_ylabel(labels.get(proxy, proxy), fontsize=10)\n        ax.grid(True, alpha=0.25)\n        for s, e, c in [('2008-01','2009-06','red'),\n                         ('2020-01','2020-12','orange'),\n                         ('2022-09','2023-06','purple')]:\n            ax.axvspan(pd.Timestamp(s), pd.Timestamp(e),\n                        alpha=0.1, color=c)\n    \n    axes[0].set_title(\n        'Time-Series of DIVOP Proxies\\n'\n        'Monthly Cross-Sectional Median, HOSE & HNX',\n        fontsize=13, fontweight='bold')\n    from matplotlib.patches import Patch\n    axes[-1].legend(handles=[\n        Patch(facecolor='red', alpha=.2, label='GFC 2008-09'),\n        Patch(facecolor='orange', alpha=.2, label='COVID-19'),\n        Patch(facecolor='purple', alpha=.2, label='Bond Crisis 2022-23'),\n    ], loc='upper right', fontsize=8)\n    plt.tight_layout()\n    plt.savefig('divop_timeseries.png', dpi=300, bbox_inches='tight')\n    plt.show()\n```\n:::\n\n\n# Putting It All Together {#sec-pipeline}\n\n::: {#merge-all .cell execution_count=16}\n``` {.python .cell-code code-summary=\"Master pipeline: build full DIVOP dataset\"}\ndef build_divop_dataset(config):\n    \"\"\"\n    Master pipeline: load data, construct all DIVOP proxies,\n    merge into a single stock-date panel.\n    \"\"\"\n    df = load_daily_data(config)\n    df = apply_sample_filters(df, config)\n    df = adjust_for_corporate_actions(df)\n    calendar = build_trading_calendar(df, config)\n    \n    df = compute_dto(df, config)\n    suv_df = compute_suv(df, calendar, config)\n    vol_df = compute_volatility(df, calendar, config)\n    df = compute_spread_and_illiq(df, config)\n    \n    # Merge\n    base = df[['ticker','date','ret','close','volume',\n                'shares_outstanding','exchange','industry_icb',\n                'foreign_ownership_pct','turnover',\n                'mato','dto','baspread','amihud_daily','limit_hit']].copy()\n    \n    if not suv_df.empty:\n        base = base.merge(\n            suv_df[['ticker','date','suv','predicted_turnover']],\n            on=['ticker','date'], how='left')\n    if not vol_df.empty:\n        base = base.merge(\n            vol_df[['ticker','date','total_volatility',\n                     'idio_volatility','market_beta']],\n            on=['ticker','date'], how='left')\n    \n    print(f\"\\n=== Final DIVOP Dataset ===\")\n    print(f\"Shape: {base.shape}\")\n    print(f\"Tickers: {base['ticker'].nunique()}\")\n    return base\n```\n:::\n\n\n# Empirical Applications {#sec-applications}\n\n## Application 1: DIVOP and the Cross-Section of Returns\n\nThe fundamental test of the Miller hypothesis is whether stocks with higher divergence of opinion earn lower subsequent returns. We implement Fama-MacBeth cross-sectional regressions:\n\n$$\nr_{i,t+1:t+h} = \\gamma_{0,t} + \\gamma_{1,t} \\cdot \\text{DIVOP}_{i,t} + \\gamma_{2,t}' \\mathbf{X}_{i,t} + \\varepsilon_{i,t}\n$$\n\nwhere $\\mathbf{X}_{i,t}$ includes controls for market beta, log market capitalization, and log book-to-market ratio. The Miller hypothesis predicts $\\bar{\\gamma}_1 < 0$.\n\n::: {#fama-macbeth .cell execution_count=17}\n``` {.python .cell-code code-summary=\"Fama-MacBeth regressions of returns on DIVOP\"}\ndef fama_macbeth_divop(merged_df, divop_proxy='suv',\n                        controls=None, horizon=21):\n    \"\"\"\n    Fama-MacBeth cross-sectional regressions.\n    Miller predicts gamma_1 < 0; Varian predicts gamma_1 > 0.\n    \"\"\"\n    if controls is None:\n        controls = ['market_beta', 'log_mktcap']\n    \n    df = merged_df.copy()\n    df = df.sort_values(['ticker', 'date'])\n    df['fwd_ret'] = df.groupby('ticker')['ret'].transform(\n        lambda x: x.shift(-1).rolling(horizon).sum().shift(-(horizon-1))\n    )\n    df['log_mktcap'] = np.log(\n        df['close'] * df['shares_outstanding'] + 1\n    )\n    \n    reg_vars = ['fwd_ret', divop_proxy] + \\\n               [c for c in controls if c in df.columns]\n    df_reg = df[['ticker','date'] + reg_vars].dropna()\n    \n    from numpy.linalg import lstsq\n    results = []\n    for date, cross in df_reg.groupby('date'):\n        if len(cross) < 30: continue\n        y = cross['fwd_ret'].values\n        X_cols = [divop_proxy] + [c for c in controls if c in cross.columns]\n        X = np.column_stack([np.ones(len(cross)), cross[X_cols].values])\n        try:\n            coefs, _, _, _ = lstsq(X, y, rcond=None)\n            results.append({\n                'date': date, 'intercept': coefs[0],\n                f'gamma_{divop_proxy}': coefs[1], 'n': len(cross),\n            })\n        except Exception: continue\n    \n    fm = pd.DataFrame(results)\n    gc = f'gamma_{divop_proxy}'\n    mu = fm[gc].mean()\n    se = fm[gc].std() / np.sqrt(len(fm))\n    t = mu / se\n    \n    print(f\"\\n=== Fama-MacBeth: {divop_proxy} -> \"\n          f\"{horizon}-day fwd returns ===\")\n    print(f\"  Mean gamma: {mu:.6f}, t-stat: {t:.3f}\")\n    if t < -1.96:   print(\"  -> Supports Miller (1977)\")\n    elif t > 1.96:   print(\"  -> Supports Varian (1985)\")\n    else:            print(\"  -> Inconclusive at 5%\")\n    return fm\n```\n:::\n\n\n## Application 2: DIVOP and Earnings Announcements\n\nFollowing @berkman2009sell, we test whether high-DIVOP stocks experience negative abnormal returns around earnings announcements, as uncertainty resolution reduces the optimism premium.\n\n::: {#ea-event .cell execution_count=18}\n``` {.python .cell-code code-summary=\"Event study: DIVOP and earnings announcement returns\"}\ndef divop_earnings_event(merged_df, ea_dates_df,\n                          divop_proxy='suv', window=(-1, 3)):\n    \"\"\"\n    Sort stocks into DIVOP quintiles pre-EA, compute CAR in window.\n    Miller predicts: Q5 (high DIVOP) has lower CAR than Q1 (low DIVOP).\n    \"\"\"\n    df = merged_df.copy()\n    ea = ea_dates_df.copy()\n    \n    # Pre-EA DIVOP value (5 days before)\n    ea['pre_date'] = ea['ea_date'] - pd.Timedelta(days=5)\n    ea = ea.merge(\n        df[['ticker','date',divop_proxy]].rename(\n            columns={'date':'pre_date'}),\n        on=['ticker','pre_date'], how='inner'\n    )\n    ea['divop_q'] = pd.qcut(\n        ea[divop_proxy], 5,\n        labels=['Q1 Low','Q2','Q3','Q4','Q5 High'],\n        duplicates='drop'\n    )\n    \n    print(f\"\\n=== EA Event Study by {divop_proxy} quintile ===\")\n    print(f\"  Window: ({window[0]}, {window[1]}) days\")\n    print(f\"  Miller predicts: Q5 has lower CAR than Q1\")\n    return ea\n```\n:::\n\n\n## Application 3: Composite DIVOP Index via PCA\n\nWhen a single summary measure of disagreement is needed, PCA on the battery of standardized proxies extracts the common \"disagreement factor.\"\n\n::: {#pca-composite .cell execution_count=19}\n``` {.python .cell-code code-summary=\"Composite DIVOP index via PCA\"}\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndef composite_divop_pca(merged_df, proxies=None):\n    \"\"\"Extract first principal component from standardized DIVOP proxies.\"\"\"\n    if proxies is None:\n        proxies = ['dto','suv','total_volatility','idio_volatility',\n                   'baspread','amihud_daily']\n    avail = [p for p in proxies if p in merged_df.columns]\n    data = merged_df[['ticker','date'] + avail].dropna()\n    \n    scaler = StandardScaler()\n    X = scaler.fit_transform(data[avail])\n    \n    pca = PCA(n_components=3)\n    factors = pca.fit_transform(X)\n    data['divop_composite'] = factors[:, 0]\n    \n    # Ensure positive correlation with inputs\n    for col in avail:\n        if data['divop_composite'].corr(data[col]) < 0:\n            data['divop_composite'] *= -1\n            break\n    \n    loadings = pd.DataFrame(\n        pca.components_.T, index=avail,\n        columns=['PC1','PC2','PC3']\n    )\n    \n    print(f\"\\n=== PCA Composite DIVOP ===\")\n    print(f\"Variance explained: \"\n          f\"{pca.explained_variance_ratio_[:3].round(3)}\")\n    print(f\"\\nLoadings:\\n{loadings.to_string(float_format='{:.4f}'.format)}\")\n    return data[['ticker','date','divop_composite']], loadings\n```\n:::\n\n\n# Conclusion and Practical Recommendations\n\nThis chapter has provided a comprehensive methodology for constructing seven distinct proxies for divergence of investor opinion adapted to the Vietnamese equity market. We conclude with practical recommendations:\n\n**1. Prefer multiple proxies.** No single DIVOP measure is without limitations. We recommend constructing and reporting results for at least three proxies spanning different economic channels (volume, volatility, spreads or analyst-based).\n\n**2. Account for Vietnam-specific microstructure.** Daily price limits, T+2 settlement, foreign ownership constraints, and the order-driven market structure all affect DIVOP properties. Flag limit-hit days, include exchange fixed effects, and control for foreign ownership.\n\n**3. Vietnam as a natural laboratory for Miller (1977).** The absence of short selling through 2024 and the dominance of retail investors create conditions that closely match Miller's theoretical setting. The introduction of short selling in 2025 creates a natural experiment for examining how relaxation of short-sale constraints affects the DIVOP-return relation.\n\n**4. Control for analyst coverage when using DISP measures.** With typical coverage of 5--10 analysts per firm, forecast dispersion is estimated with greater noise than in developed markets. Always include the number of analysts as a control variable and conduct robustness checks with varying minimum-analyst thresholds.\n\n**5. Consider constructing a composite index.** When researchers need a single summary measure of disagreement, the PCA-based composite index described in @sec-applications provides a principled approach to aggregating information across the individual proxies. The first principal component typically explains 30-50% of the common variation in the battery of DIVOP measures.\n\n**6. Winsorize aggressively.** Several DIVOP proxies (particularly DISP1, Amihud ILLIQ, and SUV) exhibit extreme outliers in the Vietnamese data. Winsorization at the 1st and 99th percentiles (or even 2nd and 98th for DISP1) is essential for obtaining reliable regression results.\n\n**7. Be cautious about causal inference.** DIVOP proxies are endogenous, they respond to the same firm characteristics (size, leverage, growth) that also affect returns. Researchers should use appropriate controls, consider instrumental variables where feasible, and be explicit about the limitations of their identification strategy.\n\nThe DIVOP framework is particularly relevant for the Vietnamese market at this point in its development. As the market matures toward potential FTSE Emerging Market reclassification, as short selling becomes more widely available, and as institutional investor participation grows, the dynamics of opinion divergence and its pricing implications are likely to evolve significantly. The methodology presented in this chapter provides researchers with the tools to document and analyze these changes as they unfold.\n\n",
    "supporting": [
      "19_divop_files/figure-pdf"
    ],
    "filters": []
  }
}