{
  "hash": "aec0f420565175f088738b7ae170644a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: DataCore Data\nformat:\n  html:\n    toc: true\n    number-sections: true\njupyter: python3\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\nThis chapter shows how to connect to [DataCore](https://datacore.vn/) a provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics (i.e., Stock Data and Public Companies). Unfortunately, this data is not freely available, but most students and researchers typically have access to DataCore through their university libraries. Assuming that you have access to DataCore, we show you how to prepare and merge the databases and store them in the SQLite database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the DataCore database.\n\nIf you don't have access to DataCore but still want to run the code in this book, we refer to [DataCore Demo Data](https://datacore.vn/demo/dataset-groups), where we show how to create a dummy database that contains the DataCore tables and corresponding columns. With this database at hand, all code chunks in this book can be executed with this dummy database.\n\nFirst, we load the Python packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them. The last two packages are used for plotting.\n\n::: {#6b442ed0 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\nimport sqlite3\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format, percent_format\nfrom datetime import datetime\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n```\n:::\n\n\nWe use the same date range as in the previous chapter to ensure consistency. However, we have to use the date format that the DataCore database expects.\n\n::: {#7161bcaa .cell execution_count=3}\n``` {.python .cell-code}\nstart_date = \"01/01/1960\"\nend_date = \"12/31/2024\"\n```\n:::\n\n\n## Accessing DataCore\n\nDataCore is the most widely used source for asset and firm-specific financial data used in academic settings. DataCore is a data platform that provides data validation, flexible delivery options, and access to many different data sources.\n\n## Preparing Company Fundamentals Data\n\nFirm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters.\n\nTo access Company Fundamentals data, we can again tap DataCore, which hosts the `funda` data table that contains annual firm-level information on Vietnam companies. We follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, which includes companies that are primarily involved in manufacturing, services, and other non-financial business activities,[^1], (ii) in the standard format (i.e., consolidated information in standard presentation).\n\n[^1]: Companies that operate in the banking, insurance, or utilities sector typically report in different industry formats that reflect their specific regulatory requirements.\n\n::: {#405d4833 .cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\nfrom io import BytesIO\nimport datetime as dt\n\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\npaths = [\n    \"fundamental_annual_1767674486317/fundamental_annual_1.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_2.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_3.xlsx\",\n]\n\ndfs = []\nfor key in paths:\n    obj = s3.get_object(Bucket=bucket_name, Key=key)\n    df_tmp = pd.read_excel(BytesIO(obj[\"Body\"].read()))\n    dfs.append(df_tmp)\n\ndf_company_fundamental = pd.concat(dfs, ignore_index=True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n```\n:::\n:::\n\n\n::: {#88dc429a .cell execution_count=5}\n``` {.python .cell-code}\ndf = df_company_fundamental.copy()\n\n# core keys\ndf[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\ndf[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n\n# drop rows with missing keys\ndf = df.dropna(subset=[\"symbol\", \"year\"])\n\n# if some numeric columns are objects, force numeric for the ones we will use\nneed = [\n    \"total_asset\",\n    \"total_equity\",\n    \"total_liabilities\",\n    \"total_current_liabilities\",\n    \"is_net_revenue\",\n    \"is_cogs\",\n    \"is_manage_expense\",\n    \"is_interest_expense\",\n    \"na_tax_deferred\",\n    \"nl_tax_deferred\",\n    \"e_preferred_stock\",\n    \"capex\",\n    \"total_cfo\",\n    \"is_eat\",\n    \"total_current_asset\", \n    \"ca_cce\", \n    \"total_equity\", \n    \"cfo_interest_expense\",\n    \"ca_total_inventory\", \n    \"ca_acc_receiv\",\n    # The is_net_business_profit field captures the core profitability of a company's business activities before accounting for \"other\" non-core incomes and expenses, and before corporate income tax.\n    \"is_net_business_profit\"\n]\nfor c in need:\n    if c in df.columns:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n# Keep the row with the most non missing fields.\ndf[\"_non_missing\"] = df.notna().sum(axis=1)\n\ndf = (\n    df.sort_values([\"symbol\", \"year\", \"_non_missing\"])\n      .drop_duplicates(subset=[\"symbol\", \"year\"], keep=\"last\")\n      .drop(columns=\"_non_missing\")\n      .reset_index(drop=True)\n)\n\nprint(\"Remaining duplicates:\",\n      df.duplicated([\"symbol\", \"year\"]).sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemaining duplicates: 0\n```\n:::\n:::\n\n\n::: {#9151b6ac .cell execution_count=6}\n``` {.python .cell-code}\n# preferably use Tax ID as Identifer\n# ---- Firm identifier and fiscal date ----\ndf[\"datadate\"] = pd.to_datetime(df[\"year\"].astype(str) + \"-12-31\")  # Fiscal year-end date\n\n# ---- Core balance sheet ----\ndf[\"at\"]  = df[\"total_asset\"]                                      # Total assets\ndf[\"act\"] = df[\"total_current_asset\"] # Total Current Assets\n\ndf[\"lt\"]  = df[\"total_liabilities\"]                                 # Total liabilities\ndf[\"lct\"] = df[\"total_current_liabilities\"] # Total Current Liabilities\ndf[\"seq\"] = df[\"total_equity\"]                                      # Stockholders' equity\ndf[\"ceq\"] = df[\"e_equity\"] if \"e_equity\" in df.columns else np.nan  # Common equity (fallback)\n\n# ---- Deferred taxes ----\ndf[\"txditc\"] = df[\"na_tax_deferred\"] if \"na_tax_deferred\" in df.columns else 0  # Deferred tax assets\ndf[\"txdb\"]   = df[\"nl_tax_deferred\"] if \"nl_tax_deferred\" in df.columns else 0  # Deferred tax liabilities\ndf[\"itcb\"]   = 0                                                                # Investment tax credit (rare, set 0)\n\n# ---- Preferred stock (Compustat has multiple versions, we map one) ----\npref = df[\"e_preferred_stock\"] if \"e_preferred_stock\" in df.columns else 0\ndf[\"pstk\"]   = pref\ndf[\"pstkrv\"] = pref\ndf[\"pstkl\"]  = pref\n\n# ---- Income statement ----\ndf[\"sale\"] = df[\"is_net_revenue\"]                                                # Sales\ndf[\"cogs\"] = df[\"is_cogs\"] if \"is_cogs\" in df.columns else 0                    # Cost of goods sold\ndf[\"xsga\"] = df[\"is_manage_expense\"] if \"is_manage_expense\" in df.columns else 0# SG&A proxy\ndf[\"xint\"] = df[\"is_interest_expense\"] if \"is_interest_expense\" in df.columns else 0  # Interest expense\n\n# ---- Cash flow and investment ----\ndf[\"oancf\"] = df[\"total_cfo\"] if \"total_cfo\" in df.columns else np.nan  # Operating cash flow\ndf[\"capx\"]  = df[\"capex\"] if \"capex\" in df.columns else np.nan          # Capital expenditures\n\ncomp_vn = df\n\ncomp_vn.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>year</th>\n      <th>total_current_asset</th>\n      <th>ca_fin</th>\n      <th>ca_cce</th>\n      <th>ca_cash</th>\n      <th>ca_cash_inbank</th>\n      <th>ca_cash_attransit</th>\n      <th>ca_cash_equivalent</th>\n      <th>ca_fin_invest</th>\n      <th>...</th>\n      <th>itcb</th>\n      <th>pstk</th>\n      <th>pstkrv</th>\n      <th>pstkl</th>\n      <th>sale</th>\n      <th>cogs</th>\n      <th>xsga</th>\n      <th>xint</th>\n      <th>oancf</th>\n      <th>capx</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A32</td>\n      <td>2016</td>\n      <td>3.432787e+11</td>\n      <td>NaN</td>\n      <td>1.397735e+11</td>\n      <td>1.006804e+10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.297055e+11</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.292595e+11</td>\n      <td>5.636144e+11</td>\n      <td>3.108361e+10</td>\n      <td>0.0</td>\n      <td>-3.127942e+10</td>\n      <td>2.369143e+10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A32</td>\n      <td>2017</td>\n      <td>3.741267e+11</td>\n      <td>NaN</td>\n      <td>1.456583e+11</td>\n      <td>5.095282e+10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>9.470550e+10</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.118207e+11</td>\n      <td>5.462587e+11</td>\n      <td>2.972820e+10</td>\n      <td>0.0</td>\n      <td>-6.679452e+09</td>\n      <td>1.598215e+10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A32</td>\n      <td>2018</td>\n      <td>3.358630e+11</td>\n      <td>NaN</td>\n      <td>5.829081e+10</td>\n      <td>1.229081e+10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4.600000e+10</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.461948e+11</td>\n      <td>5.837470e+11</td>\n      <td>1.852045e+10</td>\n      <td>0.0</td>\n      <td>5.224003e+10</td>\n      <td>1.405200e+10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A32</td>\n      <td>2019</td>\n      <td>2.987680e+11</td>\n      <td>NaN</td>\n      <td>6.051375e+10</td>\n      <td>4.451375e+10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.600000e+10</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>6.914857e+11</td>\n      <td>6.092057e+11</td>\n      <td>3.032093e+10</td>\n      <td>0.0</td>\n      <td>1.120983e+10</td>\n      <td>1.296578e+10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A32</td>\n      <td>2020</td>\n      <td>3.566913e+11</td>\n      <td>NaN</td>\n      <td>4.435908e+10</td>\n      <td>2.235908e+10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.200000e+10</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.285810e+11</td>\n      <td>6.489849e+11</td>\n      <td>3.222692e+10</td>\n      <td>0.0</td>\n      <td>4.279751e+09</td>\n      <td>5.433554e+09</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 327 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#e3eb0030 .cell execution_count=7}\n``` {.python .cell-code}\n# Keep only firm-years with core fundamentals present\n\n# Required for most accounting ratios\nreq = [\"at\", \"lt\", \"seq\", \"sale\"]\n\ncomp_vn = comp_vn.dropna(subset=req)\ncomp_vn = comp_vn[comp_vn[\"at\"] > 0]          # assets must be positive\ncomp_vn = comp_vn[comp_vn[\"sale\"] >= 0]       # sales cannot be negative\n\n# Quick diagnostics\nprint(\"Rows:\", len(comp_vn))\nprint(\"Firms:\", comp_vn[\"symbol\"].nunique())\nprint(\"Years:\", comp_vn[\"datadate\"].dt.year.min(), \"-\", comp_vn[\"datadate\"].dt.year.max())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 20091\nFirms: 1502\nYears: 1998 - 2023\n```\n:::\n:::\n\n\nNext, we calculate the book value of preferred stock and equity `be` and the operating profitability `op` inspired by the [variable definitions in Kenneth French's data library.](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html) Note that we set negative or zero equity to missing, which is a common practice when working with book-to-market ratios [see @Fama1992 for details].\n\n::: {#743ef75a .cell execution_count=8}\n``` {.python .cell-code}\ncomp_vn = (comp_vn\n  .assign(\n    be=lambda x: \n      (x[\"seq\"].combine_first(x[\"ceq\"]+x[\"pstk\"])\n       .combine_first(x[\"at\"]-x[\"lt\"])+\n       x[\"txditc\"].combine_first(x[\"txdb\"]+x[\"itcb\"]).fillna(0)-\n       x[\"pstkrv\"].combine_first(x[\"pstkl\"])\n       .combine_first(x[\"pstk\"]).fillna(0))\n  )\n  .assign(\n    be=lambda x: x[\"be\"].apply(lambda y: np.nan if y <= 0 else y)\n  )\n  .assign(\n    op=lambda x: \n      ((x[\"sale\"]-x[\"cogs\"].fillna(0)- \n        x[\"xsga\"].fillna(0)-x[\"xint\"].fillna(0))/x[\"be\"])\n  )\n)\n```\n:::\n\n\nWe keep only the last available information for each firm-year group (by using the `tail(1)` pandas function for each group). Note that `datadate` defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2022). Therefore, `datadate` is not the date when data was made available to the public. Check out the Exercises for more insights into the peculiarities of `datadate`.\n\n::: {#db651e70 .cell execution_count=9}\n``` {.python .cell-code}\ncomp_vn = (comp_vn\n  .assign(year=lambda x: pd.DatetimeIndex(x[\"datadate\"]).year)\n  .sort_values(\"datadate\")\n  .groupby([\"symbol\", \"year\"])\n  .tail(1)\n  .reset_index(drop=True)\n)\n```\n:::\n\n\n::: {#734e7307 .cell execution_count=10}\n``` {.python .cell-code}\ncomp_vn_lag = (comp_vn\n  .get([\"symbol\", \"year\", \"at\"])\n  .assign(year=lambda x: x[\"year\"]+1)\n  .rename(columns={\"at\": \"at_lag\"})\n)\n\ncomp_vn = (comp_vn\n  .merge(comp_vn_lag, how=\"left\", on=[\"symbol\", \"year\"])\n  .assign(inv=lambda x: x[\"at\"]/x[\"at_lag\"]-1)\n  .assign(inv=lambda x: np.where(x[\"at_lag\"] <= 0, np.nan, x[\"inv\"]))\n)\n```\n:::\n\n\nIn a standard Vietnamese financial report, the **LIABILITIES** section (`total_liabilities`) includes all forms of debt, including non-interest-bearing items like **Accounts Payable** (`cl_acc_payable`) and **Taxes payable** (`cl_tax_state_payable`).\n\nTo get what financial analysts call \"Total Debt\" (interest-bearing debt), you must manually aggregate the specific loan and lease variables from your dataset:\n\n-   **Short-term interest-bearing debt**: Sum of `cl_loan` and `cl_finlease`.\n\n-   **Current portion of long-term debt**: `cl_due_long_debt`.\n\n-   **Long-term interest-bearing debt**: Sum of `nl_loan` and `nl_finlease`.\n\n::: {#fef732f9 .cell execution_count=11}\n``` {.python .cell-code}\ncomp_vn = comp_vn.assign(\n    total_debt = lambda x: (\n        x[\"cl_loan\"].fillna(0) + \n        x[\"cl_finlease\"].fillna(0) + \n        x[\"cl_due_long_debt\"].fillna(0) + \n        x[\"nl_loan\"].fillna(0) + \n        x[\"nl_finlease\"].fillna(0)\n    ), \n    selling_general_and_administrative_expenses = lambda x: (\n        x[\"is_cos_of_sales\"].fillna(0) + x[\"is_manage_expense\"].fillna(0)\n    )\n)\n```\n:::\n\n\nWith the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database.\n\n::: {#019f67c1 .cell execution_count=12}\n``` {.python .cell-code}\n(comp_vn\n  .to_sql(name=\"comp_vn\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n:::\n\n\n## Downloading and Preparing Stock Data\n\n::: {#04b5aaa9 .cell execution_count=13}\n``` {.python .cell-code}\nimport pandas as pd\nfrom io import BytesIO\nimport datetime as dt\n\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\n\nprices = prices.sort_values([\"symbol\", \"date\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n```\n:::\n:::\n\n\n::: {#b4cb171c .cell execution_count=14}\n``` {.python .cell-code}\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nprices[\"ret\"] = (\n    prices.groupby(\"symbol\")[\"adjusted_close\"]\n    .pct_change()\n)\n\n# Remove impossible crashes beyond -100 percent\nprices[\"ret\"] = prices[\"ret\"].clip(lower=-0.99)\n```\n:::\n\n\nNow, we have all the relevant daily price data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters.\n\nThe first additional variable we create is market capitalization (`mktcap`), which is the product of the number of outstanding shares (`shrout`) and the last traded price in a month (`prc`). Note that in contrast to returns (`ret`), these two variables are not adjusted ex-post for any corporate actions like stock splits. Therefore, if you want to use a stock's price, you need to adjust it with a cumulative adjustment factor. We also keep the market cap in millions of VND just for convenience, as we do not want to print huge numbers in our figures and tables. In addition, we set zero market capitalization to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).\n\n::: {#fa056d2f .cell execution_count=15}\n``` {.python .cell-code}\n# 1. Calculate the Share Count Proxy from Annual Fundamentals\n# df represents your fundamental_annual dataset\ndf[\"shrout\"] = (\n    df[\"is_shareholders_eat\"] / df[\"basic_eps\"]\n)\n\n# 2. Merge with Daily Prices\n# We join on 'symbol' and 'year' so every daily row in 'prices' \n# receives the share count corresponding to that fiscal year.\nprices = prices.merge(\n    df[[\"symbol\", \"year\", \"shrout\"]],\n    on=[\"symbol\", \"year\"],\n    how=\"left\"\n)\n\n# 3. Calculate Daily Market Cap\n# Market Cap = Daily Close Price * Annual Shares Outstanding\n# We use 'close' (unadjusted) to reflect the actual market value.\nprices[\"mktcap\"] = prices[\"close\"] * prices[\"shrout\"]\n\n# 4. Filter and Scale\n# scales values to millions for readability\nprices[\"mktcap\"] = (prices[\"mktcap\"] / 1000000).replace(0, np.nan)\n```\n:::\n\n\n::: {#4d4cd4c6 .cell execution_count=16}\n``` {.python .cell-code}\n# Resample to Monthly Frequency\n# We group by symbol and resample the date to Month End (ME).\n# .last() picks the final available data point for each month.\nprices_monthly = (\n    prices.sort_values([\"symbol\", \"date\"])\n    .groupby(\"symbol\")\n    .resample(\"ME\", on=\"date\")\n    .last()\n)\n\n# After .last(), 'symbol' is index level 0 and 'date' is index level 1.\n# We remove 'symbol' and 'date' from the index to make them regular columns.\nprices_monthly = prices_monthly.drop(columns=[\"symbol\", \"date\"], errors=\"ignore\").reset_index()\n```\n:::\n\n\nThe next variable we frequently use is the one-month *lagged* market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly data.\n\n::: {#c0104016 .cell execution_count=17}\n``` {.python .cell-code}\n# 3. Lagged Market Capitalization\n# We calculate the market cap at t-1 to use for weighting returns at t.\nprices_monthly[\"mktcap_lag\"] = (\n    prices_monthly.groupby(\"symbol\")[\"mktcap\"]\n    .shift(1)\n)\n```\n:::\n\n\n::: {#1be57741 .cell execution_count=18}\n``` {.python .cell-code}\n# remove rows missing returns or capitalization data.\nprices_monthly = (\n    prices_monthly\n    .dropna(subset=[\"ret\", \"mktcap\", \"mktcap_lag\"])\n)\n```\n:::\n\n\nNext, we transform primary listing exchange codes to explicit exchange names.\n\n::: {#542dc696 .cell execution_count=19}\n``` {.python .cell-code}\ndef map_vn_exchange(row):\n    # This assumes you have an exchange column or can derive it from the symbol\n    # Adjust the logic based on your specific metadata availability.\n    if hasattr(row, 'exchange_code'):\n        if row.exchange_code == \"HOSE\": return \"NYSE_Equiv\"\n        if row.exchange_code == \"HNX\": return \"AMEX_Equiv\"\n    return \"Other\"\n```\n:::\n\n\nNext, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop the risk-free rate from our data frame. Note that we ensure excess returns are bounded by -1 from below as a return less than -100% makes no sense conceptually.\n\nTo implement a placeholder for the Vietnam risk-free rate, we use an annualized value of 4.0% (0.04), which closely approximates the 10-year Vietnam Government Bond yield observed in early 2026. In professional asset pricing research for emerging markets, the 1-year or 10-year government bond rate is a standard proxy when a dedicated monthly Fama-French risk-free rate is unavailable.\n\n::: {#4942b583 .cell execution_count=20}\n``` {.python .cell-code}\n# Create a date range covering your prices_monthly sample\nall_dates = pd.date_range(\n    start=prices_monthly['date'].min(), \n    end=prices_monthly['date'].max(), \n    freq='ME'\n)\n\n# Professional Proxy: 4% Annualized (0.04)\n# Monthly rate = 0.04 / 12\nannual_rf = 0.04\nmonthly_rf = annual_rf / 12\n\nrf_monthly = pd.DataFrame({\n    'date': all_dates,\n    'risk_free': monthly_rf\n})\n  \nprices_monthly = (prices_monthly\n  # we don't have Fama-French data for Vietnam\n  .merge(rf_monthly, how=\"left\", on=\"date\")\n  .assign(ret_excess=lambda x: x[\"ret\"]-x[\"risk_free\"])\n  .assign(ret_excess=lambda x: x[\"ret_excess\"].clip(lower=-1))\n  # .drop(columns=[\"risk_free\"])\n)\n\n# Clean the infinite values before replication\nprices_monthly = prices_monthly.replace([np.inf, -np.inf], np.nan).dropna(subset=['ret_excess'])\n```\n:::\n\n\n::: {#a249ad2d .cell execution_count=21}\n``` {.python .cell-code}\n# Check for non-finite values (NaN or Inf)\nprint(\"Missing or Infinite Values:\")\nprint(prices_monthly[['ret_excess', 'mktcap_lag']].isna().sum())\nprint(np.isinf(prices_monthly['ret_excess']).sum())\n\n# Basic summary statistics to catch outliers\nprint(\"\\nRet_excess Summary Statistics:\")\nprint(prices_monthly['ret_excess'].describe())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMissing or Infinite Values:\nret_excess    0\nmktcap_lag    0\ndtype: int64\n0\n\nRet_excess Summary Statistics:\ncount    165468.000000\nmean         -0.001458\nstd           0.041601\nmin          -0.993333\n25%          -0.006044\n50%          -0.003333\n75%           0.002764\nmax           4.139524\nName: ret_excess, dtype: float64\n```\n:::\n:::\n\n\nInterpretation: Excess returns in a monthly dataset should typically fall between -1.0 and 1.0. If your max value is extremely high, it indicates an error in your return calculation or the risk-free rate merge. Any inf values will cause the np.average in your replication code to fail.\n\n::: {#62d89b2e .cell execution_count=22}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Clean data for plotting\nplot_data = prices_monthly['ret_excess'].dropna()\nplot_data = plot_data[np.isfinite(plot_data)]\n\n# Histogram with fixed range\nplt.figure(figsize=(10, 6))\nplt.hist(plot_data, bins=100, range=(-0.5, 0.5), color='skyblue', edgecolor='black')\nplt.title('Distribution of Monthly Excess Returns (Vietnam)')\nplt.xlabel('Excess Return')\nplt.ylabel('Frequency')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07_datacore_data_files/figure-html/cell-22-output-1.png){width=833 height=523}\n:::\n:::\n\n\nInterpretation: A healthy distribution should be centered slightly above 0 and look roughly \"bell-shaped\" but with fatter tails (kurtosis). If the distribution is heavily skewed toward one side, check if your monthly_rf_placeholder was applied correctly.\n\nSince excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization.\n\n::: {#f0ea05d8 .cell execution_count=23}\n``` {.python .cell-code}\nprices_monthly = (prices_monthly\n  .dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n)\n```\n:::\n\n\nFinally, we store the monthly Stock prices data file in our database.\n\n::: {#9da4f8fc .cell execution_count=24}\n``` {.python .cell-code}\n(prices_monthly\n  .to_sql(name=\"prices_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n```\n:::\n\n\n## First Glimpse of the Stock Sample\n\nBefore we move on to other data sources, let us look at some descriptive statistics of the Stock sample, which is our main source for stock returns.\n\n@fig-211 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks.\n\n::: {#fig-211 .cell execution_count=25}\n``` {.python .cell-code}\nsecurities_per_exchange = (prices_monthly\n  .groupby([\"exchange\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nsecurities_per_exchange_figure = (\n  ggplot(\n    securities_per_exchange, \n    aes(x=\"date\", y=\"n\", color=\"exchange\", linetype=\"exchange\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly number of securities by listing exchange\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\n\nsecurities_per_exchange_figure.show()\n```\n:::\n\n\nNext, we look at the aggregate market capitalization grouped by the respective listing exchanges in @fig-212. To ensure that we look at meaningful data that is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange. All values in @fig-212 are in terms of the end of 2024 USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.\\index{Data!CPI}\n\n::: {#fig-212 .cell execution_count=26}\n``` {.python .cell-code}\ncpi_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM cpi_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nmarket_cap_per_exchange = (prices_monthly\n  .merge(cpi_monthly, how=\"left\", on=\"date\")\n  .groupby([\"date\", \"exchange\"])\n  .apply(\n    lambda group: pd.Series({\n      \"mktcap\": group[\"mktcap\"].sum()/group[\"cpi\"].mean()\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_exchange_figure = (\n  ggplot(\n    market_cap_per_exchange, \n    aes(x=\"date\", y=\"mktcap/1000\", color=\"exchange\", linetype=\"exchange\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly market cap by listing exchange\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\nmarket_cap_per_exchange_figure.show()\n```\n:::\n\n\nNext, we look at the same descriptive statistics by industry. @fig-213 plots the number of stocks in the sample for each of the VSIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.\n\n::: {#fig-213 .cell execution_count=27}\n``` {.python .cell-code}\nsecurities_per_industry = (prices_monthly\n  .groupby([\"industry\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nlinetypes = [\"-\", \"--\", \"-.\", \":\"]\nn_industries = securities_per_industry[\"industry\"].nunique()\n\nsecurities_per_industry_figure = (\n  ggplot(\n    securities_per_industry, \n    aes(x=\"date\", y=\"n\", color=\"industry\", linetype=\"industry\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly number of securities by industry\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n  + scale_linetype_manual(\n      values=[linetypes[l % len(linetypes)] for l in range(n_industries)]\n    ) \n)\nsecurities_per_industry_figure.show()\n```\n:::\n\n\nWe also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in @fig-214. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.\n\n::: {#fig-214 .cell execution_count=28}\n``` {.python .cell-code}\nmarket_cap_per_industry = (prices_monthly\n  .merge(cpi_monthly, how=\"left\", on=\"date\")\n  .groupby([\"date\", \"industry\"])\n  .apply(\n    lambda group: pd.Series({\n      \"mktcap\": (group[\"mktcap\"].sum()/group[\"cpi\"].mean())\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_industry_figure = (\n  ggplot(\n    market_cap_per_industry, \n    aes(x=\"date\", y=\"mktcap/1000\", color=\"industry\", linetype=\"industry\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly market cap by industry in billions of Dec 2024 USD\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n  + scale_linetype_manual(\n      values=[linetypes[l % len(linetypes)] for l in range(n_industries)]\n    ) \n)\nmarket_cap_per_industry_figure.show()\n```\n:::\n\n\n## Daily Stock Data\n\nBefore we turn to accounting data, we provide a proposal for downloading daily Stock data with the same filters used for the monthly data. While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily Stock data file is substantially larger than monthly data. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire dataset to your local database), and in our experience, the download usually crashes (or never stops).\n\nThere is a solution to this challenge. As with many *big data* problems, you can split up the big task into several smaller tasks that are easier to handle. That is, instead of downloading data about all stocks at once, download the data in small batches of stocks consecutively. Such operations can be implemented in `for`-loops, where we download, prepare, and store the data for a small number of stocks in each iteration. This operation might nonetheless take around 5 minutes, depending on your internet connection. To keep track of the progress, we create ad-hoc progress updates using `print()`. Notice that we also use the method `to_sql()` here with the option to append the new data to an existing table, when we process the second and all following batches. \n\n::: {#d77dff5c .cell execution_count=29}\n``` {.python .cell-code}\nfactors_ff3_daily = pd.read_sql(\n  sql=\"SELECT * FROM factors_ff3_daily\", \n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\npermnos = pd.read_sql(\n  sql=\"SELECT DISTINCT permno FROM crsp.stksecurityinfohist\", \n  con=DataCore,\n  dtype={\"permno\": int}\n)\n\npermnos = list(permnos[\"permno\"].astype(str))\n  \nbatch_size = 500\nbatches = np.ceil(len(permnos)/batch_size).astype(int)\n\nfor j in range(1, batches+1):  \n    \n  permno_batch = permnos[\n    ((j-1)*batch_size):(min(j*batch_size, len(permnos)))\n  ]\n  \n  permno_batch_formatted = (\n    \", \".join(f\"'{permno}'\" for permno in permno_batch)\n  )\n  permno_string = f\"({permno_batch_formatted})\"\n  \n  crsp_daily_sub_query = (\n    \"SELECT dsf.permno, dlycaldt AS date, dlyret AS ret \"\n      \"FROM crsp.dsf_v2 AS dsf \"\n      \"INNER JOIN crsp.stksecurityinfohist AS ssih \"\n      \"ON dsf.permno = ssih.permno AND \"\n         \"ssih.secinfostartdt <= dsf.dlycaldt AND \"\n         \"dsf.dlycaldt <= ssih.secinfoenddt \"\n      f\"WHERE dsf.permno IN {permno_string} \"\n           f\"AND dlycaldt BETWEEN '{start_date}' AND '{end_date}' \"\n            \"AND ssih.sharetype = 'NS' \"\n            \"AND ssih.securitytype = 'EQTY' \"  \n            \"AND ssih.securitysubtype = 'COM' \" \n            \"AND ssih.usincflg = 'Y' \" \n            \"AND ssih.issuertype in ('ACOR', 'CORP') \" \n            \"AND ssih.primaryexch in ('N', 'A', 'Q') \"\n            \"AND ssih.conditionaltype in ('RW', 'NW') \"\n            \"AND ssih.tradingstatusflg = 'A'\"\n  )\n    \n  crsp_daily_sub = (pd.read_sql_query(\n      sql=crsp_daily_sub_query,\n      con=DataCore,\n      dtype={\"permno\": int},\n      parse_dates={\"date\"}\n    )\n    .dropna()\n   )\n\n  if not crsp_daily_sub.empty:\n    \n      crsp_daily_sub = (crsp_daily_sub\n        .merge(factors_ff3_daily[[\"date\", \"risk_free\"]], \n               on=\"date\", how=\"left\")\n        .assign(\n          ret_excess = lambda x: \n            ((x[\"ret\"] - x[\"risk_free\"]).clip(lower=-1))\n        )\n        .get([\"permno\", \"date\", \"ret_excess\"])\n      )\n        \n      if j == 1:\n        if_exists_string = \"replace\"\n      else:\n        if_exists_string = \"append\"\n\n      crsp_daily_sub.to_sql(\n        name=\"crsp_daily\", \n        con=tidy_finance, \n        if_exists=if_exists_string, \n        index=False\n      )\n            \n  print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")\n```\n:::\n\n\n## Merging Stock with Company Fundamentals\n\nTo link the two datasets, we need to use the stock symbol.\n\nBefore we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, @fig-215 plots the share of securities with book equity values for each exchange. \n\n::: {#cell-fig-215 .cell execution_count=30}\n``` {.python .cell-code}\nshare_with_be = (prices_monthly\n  .assign(year=lambda x: pd.DatetimeIndex(x[\"date\"]).year)\n  .sort_values(\"date\")\n  .groupby([\"symbol\", \"year\"])\n  .tail(1)\n  .reset_index()\n  .merge(comp_vn, how=\"left\", on=[\"symbol\", \"year\"])\n  \n  # Group by year only for now; easy to add \"exchange\" later\n  .groupby([\"year\"]) \n  # # .groupby([\"exchange\", \"year\"]) # Uncomment this later\n  .apply(\n    lambda x: pd.Series({\n      \"share\": x[\"symbol\"][x[\"be\"].notnull()].nunique() / x[\"symbol\"].nunique()\n    }),\n    include_groups=False # Recommended for newer pandas versions\n  )\n  .reset_index()\n)\n\n# Professional visualization check\nshare_with_be_figure = (\n  ggplot(\n    share_with_be, \n \n    aes(x=\"year\", y=\"share\") \n    # aes(x=\"year\", y=\"share\", color=\"exchange\", linetype=\"exchange\")\n  )\n  + geom_line(size=1)\n  + labs(\n      x=\"Year\", y=\"Share with BE\",\n      title=\"Share of securities with book equity values\"\n      # title=\"Share of securities with book equity values by exchange\"\n    )\n  + scale_y_continuous(labels=percent_format())\n  + coord_cartesian(ylim=(0, 1))\n  + theme_minimal()\n)\nshare_with_be_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The figure shows the end-of-year share of securities with book equity values by listing exchange.](07_datacore_data_files/figure-html/fig-215-output-1.png){#fig-215 width=672 height=480 fig-alt='Title: Share of securities with book equity values by exchange. The figure shows a line chart of end-of-year shares of securities with book equity values by exchange with years on the horizontal axis and the corresponding share on the vertical axis.'}\n:::\n:::\n\n\n## Key Takeaways\n\n-   DataCore provides secure access to essential financial databases like Stock and Company Fundamentals, which are critical for empirical finance research.\n-   Stock data provides return, market capitalization and industry data for US common stocks listed.\n-   Company Fundamentals provides firm-level accounting data such as book equity, profitability, and investment.\n\n",
    "supporting": [
      "07_datacore_data_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}