{
  "hash": "63029b3ecf0d1587b431587f33828ee9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Datacore Data\nformat:\n  html:\n    toc: true\n    number-sections: true\njupyter: python3\nexecute:\n  echo: true\n  warning: false\n  message: false\n---\n\nThis chapter demonstrates how to connect to [Datacore](https://datacore.vn/), Vietnam's premier provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock prices and firm characteristics, including historical trading data and company fundamentals. While Datacore requires a subscription, most students and researchers typically have access through their university libraries or research institutions. For those without access, Datacore provides [demo datasets](https://datacore.vn/demo/dataset-groups) that allow you to run the code examples in this book with sample data.\n\nThe chapter is organized as follows. We first establish the connection to Datacore's cloud storage infrastructure. Then, we download and prepare company fundamentals data, including balance sheet items, income statement variables, and derived metrics essential for asset pricing research. Next, we retrieve and process stock price data, computing returns, market capitalizations, and excess returns. We conclude by merging these datasets and providing descriptive statistics that characterize the Vietnamese equity market.\n\n## Setting Up the Environment\n\nWe begin by loading the Python packages used throughout this chapter. The core packages include `pandas` for data manipulation, `numpy` for numerical operations, and `sqlite3` for local database management. We also import visualization libraries for creating publication-quality figures.\n\n::: {#7476ce39 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom datetime import datetime\nfrom io import BytesIO\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format, percent_format\n```\n:::\n\n\nWe establish a connection to our local SQLite database, which serves as the central repository for all processed data. This database was introduced in the previous chapter and will store the cleaned datasets for use in subsequent analyses.\n\n::: {#ff359635 .cell execution_count=2}\n``` {.python .cell-code}\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n```\n:::\n\n\nWe define the date range for our data collection. The Vietnamese stock market began operations in July 2000 with the establishment of the Ho Chi Minh City Stock Exchange (HOSE), so our sample period starts from 2000 and extends through the end of 2024.\n\n::: {#f4b0c468 .cell execution_count=3}\n``` {.python .cell-code}\nstart_date = \"2000-01-01\"\nend_date = \"2024-12-31\"\n```\n:::\n\n\n## Connecting to Datacore\n\nDatacore delivers data through a cloud-based object storage system built on MinIO, an S3-compatible storage infrastructure. This architecture enables efficient, programmatic access to large datasets without the limitations of traditional database connections. To access the data, you need credentials provided by Datacore upon subscription: an endpoint URL, access key, and secret key.\n\nThe following class establishes the connection to Datacore's storage system. The credentials are stored as environment variables for security, following best practices for credential management in research computing environments.\n\n::: {#6e9c7bfe .cell execution_count=4}\n``` {.python .cell-code}\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass DatacoreConnection:\n    \"\"\"\n    Connection handler for Datacore's MinIO-based storage system.\n    \n    This class manages authentication and provides methods for\n    accessing financial datasets stored in Datacore's cloud infrastructure.\n    \n    Attributes\n    ----------\n    s3 : boto3.client\n        S3-compatible client for interacting with Datacore storage\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize connection using environment variables.\"\"\"\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n        \n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n    \n    def test_connection(self):\n        \"\"\"Verify connection by listing available buckets.\"\"\"\n        response = self.s3.list_buckets()\n        print(\"Connected successfully. Available buckets:\")\n        for bucket in response.get(\"Buckets\", []):\n            print(f\"  - {bucket['Name']}\")\n    \n    def list_objects(self, bucket_name, prefix=\"\"):\n        \"\"\"List objects in a bucket with optional prefix filter.\"\"\"\n        response = self.s3.list_objects_v2(\n            Bucket=bucket_name, \n            Prefix=prefix\n        )\n        return [obj[\"Key\"] for obj in response.get(\"Contents\", [])]\n    \n    def read_excel(self, bucket_name, key):\n        \"\"\"Read an Excel file from Datacore storage.\"\"\"\n        obj = self.s3.get_object(Bucket=bucket_name, Key=key)\n        return pd.read_excel(BytesIO(obj[\"Body\"].read()))\n    \n    def read_csv(self, bucket_name, key, **kwargs):\n        \"\"\"Read a CSV file from Datacore storage.\"\"\"\n        obj = self.s3.get_object(Bucket=bucket_name, Key=key)\n        return pd.read_csv(BytesIO(obj[\"Body\"].read()), **kwargs)\n```\n:::\n\n\nWith the connection class defined, we can establish a connection and verify access to Datacore's data repositories.\n\n::: {#c0c8c5e1 .cell execution_count=5}\n``` {.python .cell-code}\n# Initialize connection\nconn = DatacoreConnection()\nconn.test_connection()\n\n# Get bucket name from environment\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConnected successfully. Available buckets:\n  - dsteam-data\n  - rawbctc\n```\n:::\n:::\n\n\n## Company Fundamentals Data\n\nFirm accounting data are essential for portfolio analyses, factor construction, and valuation studies. Datacore hosts comprehensive fundamentals data for Vietnamese listed companies, including annual and quarterly financial statements prepared according to Vietnamese Accounting Standards (VAS).\n\n### Understanding Vietnamese Financial Statements\n\nBefore processing the data, it is important to understand the structure of Vietnamese financial reports. Vietnamese companies follow VAS, which shares similarities with International Financial Reporting Standards (IFRS) but has notable differences:\n\n1. **Fiscal Year**: Most Vietnamese companies use a calendar fiscal year ending December 31, though some companies (particularly in retail and agriculture) use different fiscal year-ends.\n\n2. **Reporting Frequency**: Listed companies must publish quarterly financial statements within 20 days of quarter-end and annual audited statements within 90 days of fiscal year-end.\n\n3. **Industry-Specific Formats**: Companies in banking, insurance, and securities sectors follow specialized reporting formats that differ from the standard industrial format.\n\n4. **Currency**: All figures are reported in Vietnamese Dong (VND). Given the large nominal values (millions to trillions of VND), we often scale figures to millions or billions for readability.\n\n### Downloading Fundamentals Data\n\nDatacore organizes fundamentals data in Excel files partitioned by time period for efficient access. We download and concatenate these files to create a comprehensive dataset spanning our sample period.\n\n::: {#036e8d4c .cell execution_count=6}\n``` {.python .cell-code}\n# Define paths to fundamentals data files\nfundamentals_paths = [\n    \"fundamental_annual_1767674486317/fundamental_annual_1.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_2.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_3.xlsx\",\n]\n\n# Download and combine all files\nfundamentals_list = []\nfor path in fundamentals_paths:\n    df_temp = conn.read_excel(bucket_name, path)\n    fundamentals_list.append(df_temp)\n    print(f\"Downloaded: {path} ({len(df_temp):,} rows)\")\n\ndf_fundamentals_raw = pd.concat(fundamentals_list, ignore_index=True)\nprint(f\"\\nTotal observations: {len(df_fundamentals_raw):,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_1.xlsx (10,000 rows)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_2.xlsx (10,000 rows)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_3.xlsx (2,821 rows)\n\nTotal observations: 22,821\n```\n:::\n:::\n\n\n### Cleaning and Standardizing Fundamentals\n\nThe raw fundamentals data requires several cleaning steps to ensure consistency and usability. We standardize variable names, handle missing values, and create derived variables commonly used in asset pricing research.\n\n::: {#eebaf473 .cell execution_count=7}\n``` {.python .cell-code}\ndef clean_fundamentals(df):\n    \"\"\"\n    Clean and standardize company fundamentals data.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Raw fundamentals data from Datacore\n    \n    Returns\n    -------\n    pd.DataFrame\n        Cleaned fundamentals with standardized column names\n    \"\"\"\n    df = df.copy()\n    \n    # Standardize identifiers\n    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\n    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n    \n    # Drop rows with missing identifiers\n    df = df.dropna(subset=[\"symbol\", \"year\"])\n    \n    # Define columns that should be numeric\n    numeric_columns = [\n        \"total_asset\", \"total_equity\", \"total_liabilities\",\n        \"total_current_asset\", \"total_current_liabilities\",\n        \"is_net_revenue\", \"is_cogs\", \"is_manage_expense\",\n        \"is_interest_expense\", \"is_eat\", \"is_net_business_profit\",\n        \"na_tax_deferred\", \"nl_tax_deferred\", \"e_preferred_stock\",\n        \"capex\", \"total_cfo\", \"ca_cce\", \"ca_total_inventory\",\n        \"ca_acc_receiv\", \"cfo_interest_expense\", \"basic_eps\",\n        \"is_shareholders_eat\", \"cl_loan\", \"cl_finlease\",\n        \"cl_due_long_debt\", \"nl_loan\", \"nl_finlease\",\n        \"is_cos_of_sales\", \"e_equity\"\n    ]\n    \n    for col in numeric_columns:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n    \n    # Handle duplicates: keep row with most non-missing values\n    df[\"_completeness\"] = df.notna().sum(axis=1)\n    df = (df\n        .sort_values([\"symbol\", \"year\", \"_completeness\"])\n        .drop_duplicates(subset=[\"symbol\", \"year\"], keep=\"last\")\n        .drop(columns=\"_completeness\")\n        .reset_index(drop=True)\n    )\n    \n    return df\n\ndf_fundamentals = clean_fundamentals(df_fundamentals_raw)\nprint(f\"After cleaning: {len(df_fundamentals):,} firm-year observations\")\nprint(f\"Unique firms: {df_fundamentals['symbol'].nunique():,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAfter cleaning: 21,232 firm-year observations\nUnique firms: 1,554\n```\n:::\n:::\n\n\n### Creating Standardized Variables\n\nTo facilitate comparison with international studies and ensure compatibility with standard asset pricing methodologies, we create variables following conventions established in the academic literature. We map Vietnamese financial statement items to their Compustat equivalents where possible.\n\n::: {#88a7dd1f .cell execution_count=8}\n``` {.python .cell-code}\ndef create_standard_variables(df):\n    \"\"\"\n    Create standardized financial variables for asset pricing research.\n    \n    This function maps Vietnamese financial statement items to standard\n    variable names used in the academic finance literature, following\n    conventions from Fama and French (1992, 1993, 2015).\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Cleaned fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with standardized variables added\n    \"\"\"\n    df = df.copy()\n    \n    # Fiscal date (assume December year-end)\n    df[\"datadate\"] = pd.to_datetime(df[\"year\"].astype(str) + \"-12-31\")\n    \n    # === Balance Sheet Items ===\n    df[\"at\"] = df[\"total_asset\"]                    # Total assets\n    df[\"lt\"] = df[\"total_liabilities\"]              # Total liabilities\n    df[\"seq\"] = df[\"total_equity\"]                  # Stockholders' equity\n    df[\"act\"] = df[\"total_current_asset\"]           # Current assets\n    df[\"lct\"] = df[\"total_current_liabilities\"]     # Current liabilities\n    \n    # Common equity (fallback to total equity if not available)\n    df[\"ceq\"] = df.get(\"e_equity\", df[\"seq\"])\n    \n    # === Deferred Taxes ===\n    df[\"txditc\"] = df.get(\"na_tax_deferred\", 0).fillna(0)  # Deferred tax assets\n    df[\"txdb\"] = df.get(\"nl_tax_deferred\", 0).fillna(0)    # Deferred tax liab.\n    df[\"itcb\"] = 0  # Investment tax credit (rare in Vietnam)\n    \n    # === Preferred Stock ===\n    pref = df.get(\"e_preferred_stock\", 0)\n    if isinstance(pref, pd.Series):\n        pref = pref.fillna(0)\n    df[\"pstk\"] = pref\n    df[\"pstkrv\"] = pref  # Redemption value\n    df[\"pstkl\"] = pref   # Liquidating value\n    \n    # === Income Statement Items ===\n    df[\"sale\"] = df[\"is_net_revenue\"]                        # Net sales/revenue\n    df[\"cogs\"] = df.get(\"is_cogs\", 0).fillna(0)              # Cost of goods sold\n    df[\"xsga\"] = df.get(\"is_manage_expense\", 0).fillna(0)    # SG&A expenses\n    df[\"xint\"] = df.get(\"is_interest_expense\", 0).fillna(0)  # Interest expense\n    df[\"ni\"] = df.get(\"is_eat\", np.nan)                      # Net income\n    df[\"oibdp\"] = df.get(\"is_net_business_profit\", np.nan)   # Operating income\n    \n    # === Cash Flow Items ===\n    df[\"oancf\"] = df.get(\"total_cfo\", np.nan)  # Operating cash flow\n    df[\"capx\"] = df.get(\"capex\", np.nan)       # Capital expenditures\n    \n    return df\n\ndf_fundamentals = create_standard_variables(df_fundamentals)\n```\n:::\n\n\n### Computing Book Equity and Profitability\n\nBook equity is a crucial variable for value investing strategies and the construction of HML (High Minus Low) factor portfolios. We follow the definition from Kenneth French's data library, which accounts for deferred taxes and preferred stock.\n\n::: {#1499506b .cell execution_count=9}\n``` {.python .cell-code}\ndef compute_book_equity(df):\n    \"\"\"\n    Compute book equity following Fama-French conventions.\n    \n    Book equity = Stockholders' equity \n                  + Deferred taxes and investment tax credit\n                  - Preferred stock\n    \n    Negative or zero book equity is set to missing, as book-to-market\n    ratios are undefined for such firms.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals with standardized variables\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with book equity (be) added\n    \"\"\"\n    df = df.copy()\n    \n    # Primary measure: stockholders' equity\n    # Fallback 1: common equity + preferred stock\n    # Fallback 2: total assets - total liabilities\n    seq_measure = (df[\"seq\"]\n        .combine_first(df[\"ceq\"] + df[\"pstk\"])\n        .combine_first(df[\"at\"] - df[\"lt\"])\n    )\n    \n    # Add deferred taxes\n    deferred_taxes = (df[\"txditc\"]\n        .combine_first(df[\"txdb\"] + df[\"itcb\"])\n        .fillna(0)\n    )\n    \n    # Subtract preferred stock (use redemption value as primary)\n    preferred = (df[\"pstkrv\"]\n        .combine_first(df[\"pstkl\"])\n        .combine_first(df[\"pstk\"])\n        .fillna(0)\n    )\n    \n    # Book equity calculation\n    df[\"be\"] = seq_measure + deferred_taxes - preferred\n    \n    # Set non-positive book equity to missing\n    df[\"be\"] = df[\"be\"].where(df[\"be\"] > 0, np.nan)\n    \n    return df\n\ndf_fundamentals = compute_book_equity(df_fundamentals)\n\n# Summary statistics for book equity\nprint(\"Book Equity Summary Statistics (in million VND):\")\nprint(df_fundamentals[\"be\"].describe().round(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBook Equity Summary Statistics (in million VND):\ncount    2.023500e+04\nmean     1.031884e+12\nstd      4.705269e+12\nmin      4.404402e+07\n25%      7.267610e+10\n50%      1.803885e+11\n75%      5.304653e+11\nmax      1.836314e+14\nName: be, dtype: float64\n```\n:::\n:::\n\n\nOperating profitability, introduced by @Fama2015, measures a firm's profits relative to its book equity. Firms with higher operating profitability tend to have higher expected returns.\n\n::: {#70e53967 .cell execution_count=10}\n``` {.python .cell-code}\ndef compute_profitability(df):\n    \"\"\"\n    Compute operating profitability following Fama-French (2015).\n    \n    Operating profitability = (Revenue - COGS - SG&A - Interest) / Book Equity\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals with book equity computed\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with operating profitability (op) added\n    \"\"\"\n    df = df.copy()\n    \n    # Operating profit before taxes\n    operating_profit = (\n        df[\"sale\"] \n        - df[\"cogs\"].fillna(0) \n        - df[\"xsga\"].fillna(0) \n        - df[\"xint\"].fillna(0)\n    )\n    \n    # Scale by book equity\n    df[\"op\"] = operating_profit / df[\"be\"]\n    \n    # Winsorize extreme values (outside 1st and 99th percentiles)\n    lower = df[\"op\"].quantile(0.01)\n    upper = df[\"op\"].quantile(0.99)\n    df[\"op\"] = df[\"op\"].clip(lower=lower, upper=upper)\n    \n    return df\n\ndf_fundamentals = compute_profitability(df_fundamentals)\n```\n:::\n\n\n### Computing Investment\n\nInvestment, measured as asset growth, captures firms' investment behavior. @Fama2015 document that firms with high asset growth (aggressive investment) tend to have lower future returns.\n\n::: {#cad6136e .cell execution_count=11}\n``` {.python .cell-code}\ndef compute_investment(df):\n    \"\"\"\n    Compute investment (asset growth) following Fama-French (2015).\n    \n    Investment = (Total Assets_t / Total Assets_{t-1}) - 1\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with investment (inv) added\n    \"\"\"\n    df = df.copy()\n    \n    # Create lagged assets\n    df_lag = (df[[\"symbol\", \"year\", \"at\"]]\n        .assign(year=lambda x: x[\"year\"] + 1)\n        .rename(columns={\"at\": \"at_lag\"})\n    )\n    \n    # Merge lagged values\n    df = df.merge(df_lag, on=[\"symbol\", \"year\"], how=\"left\")\n    \n    # Compute investment (asset growth)\n    df[\"inv\"] = df[\"at\"] / df[\"at_lag\"] - 1\n    \n    # Set to missing if lagged assets non-positive\n    df[\"inv\"] = df[\"inv\"].where(df[\"at_lag\"] > 0, np.nan)\n    \n    return df\n\ndf_fundamentals = compute_investment(df_fundamentals)\n```\n:::\n\n\n### Computing Total Debt\n\nIn Vietnamese financial statements, total liabilities include non-interest-bearing items such as accounts payable and tax payables. For leverage analysis, we compute total interest-bearing debt by aggregating loan and lease obligations.\n\n::: {#81312f12 .cell execution_count=12}\n``` {.python .cell-code}\ndef compute_total_debt(df):\n    \"\"\"\n    Compute total interest-bearing debt.\n    \n    Total Debt = Short-term loans + Finance leases (current)\n                 + Current portion of long-term debt\n                 + Long-term loans + Finance leases (non-current)\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with total_debt added\n    \"\"\"\n    df = df.copy()\n    \n    df[\"total_debt\"] = (\n        df.get(\"cl_loan\", 0).fillna(0) +           # Short-term bank loans\n        df.get(\"cl_finlease\", 0).fillna(0) +       # Current finance leases\n        df.get(\"cl_due_long_debt\", 0).fillna(0) +  # Current portion LT debt\n        df.get(\"nl_loan\", 0).fillna(0) +           # Long-term bank loans\n        df.get(\"nl_finlease\", 0).fillna(0)         # Non-current finance leases\n    )\n    \n    return df\n\ndf_fundamentals = compute_total_debt(df_fundamentals)\n```\n:::\n\n\n### Applying Filters and Final Preparation\n\nWe apply standard filters to ensure data quality: requiring positive assets, non-negative sales, and presence of core variables needed for portfolio construction.\n\n::: {#5d6365b0 .cell execution_count=13}\n``` {.python .cell-code}\n# Keep only observations with required variables\nrequired_vars = [\"at\", \"lt\", \"seq\", \"sale\"]\ncomp_vn = df_fundamentals.dropna(subset=required_vars)\n\n# Apply quality filters\ncomp_vn = comp_vn.query(\"at > 0\")      # Positive assets\ncomp_vn = comp_vn.query(\"sale >= 0\")   # Non-negative sales\n\n# Keep last observation per firm-year (in case of restatements)\ncomp_vn = (comp_vn\n    .sort_values(\"datadate\")\n    .groupby([\"symbol\", \"year\"])\n    .tail(1)\n    .reset_index(drop=True)\n)\n\n# Diagnostic summary\nprint(f\"Final sample: {len(comp_vn):,} firm-year observations\")\nprint(f\"Unique firms: {comp_vn['symbol'].nunique():,}\")\nprint(f\"Sample period: {comp_vn['year'].min()} - {comp_vn['year'].max()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal sample: 20,091 firm-year observations\nUnique firms: 1,502\nSample period: 1998 - 2023\n```\n:::\n:::\n\n\n### Storing Fundamentals Data\n\nWe store the prepared fundamentals data in our local SQLite database for use in subsequent chapters.\n\n::: {#220f9558 .cell execution_count=14}\n``` {.python .cell-code}\ncomp_vn.to_sql(\n    name=\"comp_vn\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Company fundamentals saved to database.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCompany fundamentals saved to database.\n```\n:::\n:::\n\n\n## Stock Price Data\n\nStock price data forms the foundation of return-based analyses in empirical finance. Datacore provides comprehensive historical price data for all securities traded on HOSE, HNX, and UPCoM, including adjusted prices that account for corporate actions.\n\n### Downloading Price Data\n\nWe download the historical price data from Datacore's storage system. The data includes daily observations with open, high, low, close prices, trading volume, and adjustment factors.\n\n::: {#311ff47a .cell execution_count=15}\n``` {.python .cell-code}\n# Download historical price data\nprices_raw = conn.read_csv(\n    bucket_name,\n    \"historycal_price/dataset_historical_price.csv\",\n    low_memory=False\n)\n\nprint(f\"Downloaded {len(prices_raw):,} daily price observations\")\nprint(f\"Date range: {prices_raw['date'].min()} to {prices_raw['date'].max()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloaded 4,307,791 daily price observations\nDate range: 2010-01-04 to 2025-05-12\n```\n:::\n:::\n\n\n### Processing Price Data\n\nWe clean the price data and compute adjusted prices that account for stock splits, stock dividends, and other corporate actions.\n\n::: {#d25d655c .cell execution_count=16}\n``` {.python .cell-code}\ndef process_price_data(df):\n    \"\"\"\n    Process raw price data from Datacore.\n    \"\"\"\n    df = df.copy()\n    \n    # Parse dates\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    \n    # Standardize column names\n    df = df.rename(columns={\n        \"open_price\": \"open\",\n        \"high_price\": \"high\",\n        \"low_price\": \"low\",\n        \"close_price\": \"close\",\n        \"vol_total\": \"volume\"\n    })\n    \n    # Compute adjusted close price\n    df[\"adjusted_close\"] = df[\"close\"] * df[\"adj_ratio\"]\n    \n    # Standardize symbol\n    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\n    \n    # Sort for return calculation\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Add year and month\n    df[\"year\"] = df[\"date\"].dt.year\n    df[\"month\"] = df[\"date\"].dt.month\n    \n    return df\n\nprices = process_price_data(prices_raw)\n```\n:::\n\n\n### Computing Shares Outstanding and Market Capitalization\n\nMarket capitalization is computed as the product of price and shares outstanding. Since Datacore provides earnings per share and net income, we can infer shares outstanding from these variables.\n\n::: {#0ad25dea .cell execution_count=17}\n``` {.python .cell-code}\ndef compute_shares_outstanding(fundamentals_df):\n    \"\"\"\n    Compute shares outstanding from fundamentals.\n    \"\"\"\n    shares = fundamentals_df.copy()\n    shares[\"shrout\"] = shares[\"is_shareholders_eat\"] / shares[\"basic_eps\"]\n    shares = shares[[\"symbol\", \"year\", \"shrout\"]].dropna()\n    \n    return shares\n\nshares_outstanding = compute_shares_outstanding(df_fundamentals)\n```\n:::\n\n\n::: {#9acd0025 .cell execution_count=18}\n``` {.python .cell-code}\ndef add_market_cap(df, shares_df):\n    \"\"\"\n    Add market capitalization to price data.\n    \"\"\"\n    df = df.merge(shares_df, on=[\"symbol\", \"year\"], how=\"left\")\n    \n    # Compute market cap (in million VND)\n    df[\"mktcap\"] = (df[\"close\"] * df[\"shrout\"]) / 1_000_000\n    \n    # Set zero or negative market cap to missing\n    df[\"mktcap\"] = df[\"mktcap\"].where(df[\"mktcap\"] > 0, np.nan)\n    \n    return df\n\nprices = add_market_cap(prices, shares_outstanding)\n```\n:::\n\n\n### Computing Returns and Excess Returns\nWe compute returns using adjusted closing prices to ensure returns correctly reflect total shareholder returns including dividends and corporate actions.\n\n#### Creating Daily Dataset\n\n1. Sequential version\n\n::: {#c0b8d136 .cell execution_count=19}\n``` {.python .cell-code}\ndef create_daily_dataset(df, annual_rf=0.04):\n    \"\"\"\n    Create daily price dataset with returns and excess returns.\n    \"\"\"\n    df = df.copy()\n    \n    # Sort by symbol and date (critical for correct return calculation)\n    df = df.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Remove duplicate dates within each symbol (keep last observation)\n    df = df.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Compute daily returns\n    df[\"ret\"] = df.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    \n    # Cap extreme negative returns\n    df[\"ret\"] = df[\"ret\"].clip(lower=-0.99)\n    \n    # Daily risk-free rate (assuming 252 trading days)\n    df[\"risk_free\"] = annual_rf / 252\n    \n    # Excess returns\n    df[\"ret_excess\"] = df[\"ret\"] - df[\"risk_free\"]\n    df[\"ret_excess\"] = df[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    df[\"mktcap_lag\"] = df.groupby(\"symbol\")[\"mktcap\"].shift(1)\n    \n    return df\n\nprices_daily = create_daily_dataset(prices)\n```\n:::\n\n\n2. Parallel version\n\n::: {#1bfa90d1 .cell execution_count=20}\n``` {.python .cell-code}\nfrom joblib import Parallel, delayed\nimport os\n\ndef process_daily_symbol(symbol_df, annual_rf=0.04):\n    \"\"\"\n    Process a single symbol's daily data.\n    \"\"\"\n    df = symbol_df.copy()\n    \n    # Sort by date (critical for correct return calculation)\n    df = df.sort_values(\"date\").reset_index(drop=True)\n    \n    # Remove duplicate dates (keep last observation if duplicates exist)\n    df = df.drop_duplicates(subset=[\"date\"], keep=\"last\")\n    \n    # Compute daily returns\n    df[\"ret\"] = df[\"adjusted_close\"].pct_change()\n\n    # Replace infinite values with NaN\n    df[\"ret\"] = df[\"ret\"].replace([np.inf, -np.inf], np.nan)\n    \n    # Cap extreme negative returns\n    df[\"ret\"] = df[\"ret\"].clip(lower=-0.99)\n    \n    # Daily risk-free rate\n    df[\"risk_free\"] = annual_rf / 252\n    \n    # Excess returns\n    df[\"ret_excess\"] = df[\"ret\"] - df[\"risk_free\"]\n    df[\"ret_excess\"] = df[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    df[\"mktcap_lag\"] = df[\"mktcap\"].shift(1)\n    \n    return df\n\ndef create_daily_dataset_parallel(df, annual_rf=0.04):\n    \"\"\"\n    Create daily price dataset using parallel processing.\n    \"\"\"\n    # Ensure data is sorted before splitting\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Split by symbol\n    symbol_groups = [group for _, group in df.groupby(\"symbol\")]\n    \n    n_jobs = max(1, os.cpu_count() - 1)\n    print(f\"Processing {len(symbol_groups):,} symbols using {n_jobs} cores...\")\n    \n    results = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(process_daily_symbol)(group, annual_rf) \n        for group in symbol_groups\n    )\n    \n    return pd.concat(results, ignore_index=True)\n\nprices_daily = create_daily_dataset_parallel(prices)\n\n# Quick validation\nprint(\"\\nValidation checks:\")\nprint(f\"Any duplicate (symbol, date): {prices_daily.duplicated(subset=['symbol', 'date']).sum()}\")\nprint(f\"Sample of non-zero returns:\")\nprint(prices_daily[prices_daily[\"ret\"] != 0][[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(10))\n\n\nprices_daily.query(\"symbol == 'FPT'\")[[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing 1,837 symbols using 23 cores...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nValidation checks:\nAny duplicate (symbol, date): 0\nSample of non-zero returns:\n   symbol       date  adjusted_close       ret\n0     A32 2018-10-23       44.574418       NaN\n27    A32 2018-11-29       55.072640  0.235521\n30    A32 2018-12-04       48.188560 -0.125000\n43    A32 2018-12-21       51.974804  0.078571\n49    A32 2019-01-02       55.072640  0.059603\n53    A32 2019-01-08       50.030370 -0.091557\n74    A32 2019-02-13       44.289180 -0.114754\n75    A32 2019-02-14       41.008500 -0.074074\n78    A32 2019-02-19       36.087480 -0.120000\n91    A32 2019-03-08       41.336568  0.145455\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>date</th>\n      <th>adjusted_close</th>\n      <th>ret</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1146076</th>\n      <td>FPT</td>\n      <td>2010-01-04</td>\n      <td>1170.9885</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1146077</th>\n      <td>FPT</td>\n      <td>2010-01-05</td>\n      <td>1170.9885</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1146078</th>\n      <td>FPT</td>\n      <td>2010-01-06</td>\n      <td>1149.6978</td>\n      <td>-0.018182</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#a6a13a4a .cell execution_count=21}\n``` {.python .cell-code}\n# Select columns\ndaily_columns = [\n    \"symbol\", \"date\", \"year\", \"month\",\n    \"open\", \"high\", \"low\", \"close\", \"volume\",\n    \"adjusted_close\", \"shrout\", \"mktcap\", \"mktcap_lag\",\n    \"ret\", \"risk_free\", \"ret_excess\"\n]\nprices_daily = prices_daily[daily_columns]\n\n# Remove observations with missing essential variables\nprices_daily = prices_daily.dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n\nprint(\"Daily Return Summary Statistics:\")\nprint(prices_daily[\"ret\"].describe().round(4))\nprint(f\"\\nFinal daily sample: {len(prices_daily):,} observations\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaily Return Summary Statistics:\ncount    3.462157e+06\nmean     3.000000e-04\nstd      4.480000e-02\nmin     -9.900000e-01\n25%     -4.900000e-03\n50%      0.000000e+00\n75%      4.000000e-03\nmax      3.250000e+01\nName: ret, dtype: float64\n\nFinal daily sample: 3,462,157 observations\n```\n:::\n:::\n\n\n#### Creating Monthly Dataset\n\nFor monthly returns, we compute returns directly from month-end adjusted prices rather than compounding daily returns. This avoids compounding errors from missing days and is the standard approach in empirical finance.\n\n1. Sequential version\n\n::: {#49265798 .cell execution_count=22}\n``` {.python .cell-code}\ndef create_monthly_dataset(df, annual_rf=0.04):\n    \"\"\"\n    Create monthly price dataset with returns computed from \n    month-end to month-end adjusted prices.\n    \"\"\"\n    df = df.copy()\n    \n    # Sort by symbol and date (critical for correct return calculation)\n    df = df.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Remove duplicate dates within each symbol (keep last observation)\n    df = df.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Get month-end observations\n    monthly = (df\n        .groupby(\"symbol\")\n        .resample(\"ME\", on=\"date\")\n        .agg({\n            \"open\": \"first\",           # First day open\n            \"high\": \"max\",             # Monthly high\n            \"low\": \"min\",              # Monthly low\n            \"close\": \"last\",           # Last day close\n            \"volume\": \"sum\",           # Total monthly volume\n            \"adjusted_close\": \"last\",  # Month-end adjusted price\n            \"shrout\": \"last\",          # Month-end shares outstanding\n            \"mktcap\": \"last\",          # Month-end market cap\n            \"year\": \"last\",\n            \"month\": \"last\"\n        })\n        .reset_index()\n    )\n    \n    # Remove duplicate (symbol, date) after resampling (safety check)\n    monthly = monthly.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Sort again after resampling\n    monthly = monthly.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Compute monthly returns from month-end to month-end adjusted prices\n    monthly[\"ret\"] = monthly.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    \n    # Cap extreme returns\n    monthly[\"ret\"] = monthly[\"ret\"].clip(lower=-0.99)\n    \n    # Monthly risk-free rate\n    monthly[\"risk_free\"] = annual_rf / 12\n    \n    # Excess returns\n    monthly[\"ret_excess\"] = monthly[\"ret\"] - monthly[\"risk_free\"]\n    monthly[\"ret_excess\"] = monthly[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap for portfolio weighting\n    monthly[\"mktcap_lag\"] = monthly.groupby(\"symbol\")[\"mktcap\"].shift(1)\n    \n    return monthly\n\nprices_monthly = create_monthly_dataset(prices)\n```\n:::\n\n\n2. Parallel version\n\n::: {#620832f4 .cell execution_count=23}\n``` {.python .cell-code}\nfrom joblib import Parallel, delayed\nimport os\n\ndef process_monthly_symbol(symbol_df, annual_rf=0.04):\n    \"\"\"\n    Process a single symbol's data to monthly frequency.\n    \"\"\"\n    df = symbol_df.copy()\n    \n    # Sort by date (critical for correct return calculation)\n    df = df.sort_values(\"date\").reset_index(drop=True)\n    \n    # Remove duplicate dates (keep last observation if duplicates exist)\n    df = df.drop_duplicates(subset=[\"date\"], keep=\"last\")\n    \n    # Set date as index for resampling\n    df = df.set_index(\"date\")\n    \n    # Resample to monthly\n    monthly = df.resample(\"ME\").agg({\n        \"symbol\": \"last\",\n        \"open\": \"first\",\n        \"high\": \"max\",\n        \"low\": \"min\",\n        \"close\": \"last\",\n        \"volume\": \"sum\",\n        \"adjusted_close\": \"last\",\n        \"shrout\": \"last\",\n        \"mktcap\": \"last\",\n        \"year\": \"last\",\n        \"month\": \"last\"\n    }).reset_index()\n    \n    # Remove rows where symbol is NaN (months with no trading)\n    monthly = monthly.dropna(subset=[\"symbol\"])\n    \n    # Sort by date\n    monthly = monthly.sort_values(\"date\").reset_index(drop=True)\n    \n    # Compute monthly returns\n    monthly[\"ret\"] = monthly[\"adjusted_close\"].pct_change()\n    \n    # Replace infinite values with NaN\n    monthly[\"ret\"] = monthly[\"ret\"].replace([np.inf, -np.inf], np.nan)\n    \n    # Cap extreme returns\n    monthly[\"ret\"] = monthly[\"ret\"].clip(lower=-0.99)\n    \n    # Monthly risk-free rate\n    monthly[\"risk_free\"] = annual_rf / 12\n    \n    # Excess returns\n    monthly[\"ret_excess\"] = monthly[\"ret\"] - monthly[\"risk_free\"]\n    monthly[\"ret_excess\"] = monthly[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    monthly[\"mktcap_lag\"] = monthly[\"mktcap\"].shift(1)\n    \n    return monthly\n\ndef create_monthly_dataset_parallel(df, annual_rf=0.04):\n    \"\"\"\n    Create monthly price dataset using parallel processing.\n    \"\"\"\n    # Ensure data is sorted before splitting\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Split by symbol\n    symbol_groups = [group for _, group in df.groupby(\"symbol\")]\n    \n    n_jobs = max(1, os.cpu_count() - 1)\n    print(f\"Processing {len(symbol_groups):,} symbols using {n_jobs} cores...\")\n    \n    results = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(process_monthly_symbol)(group, annual_rf) \n        for group in symbol_groups\n    )\n    \n    return pd.concat(results, ignore_index=True)\n\nprices_monthly = create_monthly_dataset_parallel(prices)\n\n# Validation checks\nprint(\"\\nValidation checks:\")\nprint(f\"Any duplicate (symbol, date): {prices_monthly.duplicated(subset=['symbol', 'date']).sum()}\")\nprint(f\"\\nSample of non-zero returns:\")\nprint(prices_monthly[prices_monthly[\"ret\"] != 0][[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(10))\n\nprices_monthly.query(\"symbol == 'FPT'\")[[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing 1,837 symbols using 23 cores...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nValidation checks:\nAny duplicate (symbol, date): 0\n\nSample of non-zero returns:\n   symbol       date  adjusted_close       ret\n0     A32 2018-10-31       44.574418       NaN\n1     A32 2018-11-30       55.072640  0.235521\n2     A32 2018-12-31       51.974804 -0.056250\n3     A32 2019-01-31       50.030370 -0.037411\n4     A32 2019-02-28       36.087480 -0.278689\n5     A32 2019-03-31       41.828670  0.159091\n7     A32 2019-05-31       43.304976  0.035294\n8     A32 2019-06-30       35.929125 -0.170323\n9     A32 2019-07-31       37.525975  0.044444\n10    A32 2019-08-31       38.324400  0.021277\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=21}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>date</th>\n      <th>adjusted_close</th>\n      <th>ret</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>55963</th>\n      <td>FPT</td>\n      <td>2010-01-31</td>\n      <td>1092.9226</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>55964</th>\n      <td>FPT</td>\n      <td>2010-02-28</td>\n      <td>1107.1164</td>\n      <td>0.012987</td>\n    </tr>\n    <tr>\n      <th>55965</th>\n      <td>FPT</td>\n      <td>2010-03-31</td>\n      <td>1185.1823</td>\n      <td>0.070513</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#b2c661b4 .cell execution_count=24}\n``` {.python .cell-code}\n# Select columns (same structure as daily)\nmonthly_columns = [\n    \"symbol\", \"date\", \"year\", \"month\",\n    \"open\", \"high\", \"low\", \"close\", \"volume\",\n    \"adjusted_close\", \"shrout\", \"mktcap\", \"mktcap_lag\",\n    \"ret\", \"risk_free\", \"ret_excess\"\n]\nprices_monthly = prices_monthly[monthly_columns]\n\n# Remove observations with missing essential variables\nprices_monthly = prices_monthly.dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n\nprint(\"Monthly Return Summary Statistics:\")\nprint(prices_monthly[\"ret\"].describe().round(4))\nprint(f\"\\nFinal monthly sample: {len(prices_monthly):,} observations\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMonthly Return Summary Statistics:\ncount    165499.0000\nmean          0.0042\nstd           0.1862\nmin          -0.9900\n25%          -0.0703\n50%           0.0000\n75%           0.0553\nmax          12.7500\nName: ret, dtype: float64\n\nFinal monthly sample: 165,499 observations\n```\n:::\n:::\n\n\n### Storing Price Data\n\n::: {#36eeca3f .cell execution_count=25}\n``` {.python .cell-code}\nprices_daily.to_sql(\n    name=\"prices_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(\"Daily price data saved to database.\")\n\nprices_monthly.to_sql(\n    name=\"prices_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(\"Monthly price data saved to database.\")\n```\n:::\n\n\n## Descriptive Statistics\n\nBefore proceeding to asset pricing analyses, we examine the characteristics of our sample to understand the Vietnamese equity market's evolution and composition.\n\n### Market Evolution Over Time\n\nWe first examine how the number of listed securities has grown over time.\n\n::: {#5a959008 .cell execution_count=26}\n``` {.python .cell-code}\nsecurities_over_time = (prices_monthly\n    .groupby(\"date\")\n    .agg(\n        n_securities=(\"symbol\", \"nunique\"),\n        total_mktcap=(\"mktcap\", \"sum\")\n    )\n    .reset_index()\n)\n```\n:::\n\n\n::: {#cell-fig-securities-over-time .cell execution_count=27}\n``` {.python .cell-code}\nsecurities_figure = (\n    ggplot(securities_over_time, aes(x=\"date\", y=\"n_securities\"))\n    + geom_line(color=\"steelblue\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Number of Securities\",\n        title=\"Growth of Vietnamese Stock Market\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + scale_y_continuous(labels=comma_format())\n    + theme_minimal()\n)\nsecurities_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The figure shows the monthly number of securities in the Vietnamese stock market sample.](07_datacore_data_files/figure-html/fig-securities-over-time-output-1.png){#fig-securities-over-time width=672 height=480 fig-alt='Line chart showing the growth in number of listed securities over time.'}\n:::\n:::\n\n\n### Market Capitalization Evolution\n\nThe aggregate market capitalization reflects the overall size and development of the Vietnamese equity market.\n\n::: {#cell-fig-market-cap-over-time .cell execution_count=28}\n``` {.python .cell-code}\nmktcap_figure = (\n    ggplot(securities_over_time, aes(x=\"date\", y=\"total_mktcap / 1000\"))\n    + geom_line(color=\"darkgreen\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Market Cap (Trillion VND)\",\n        title=\"Total Market Capitalization of Vietnamese Equities\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + scale_y_continuous(labels=comma_format())\n    + theme_minimal()\n)\nmktcap_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The figure shows the total market capitalization of Vietnamese listed companies over time.](07_datacore_data_files/figure-html/fig-market-cap-over-time-output-1.png){#fig-market-cap-over-time width=672 height=480 fig-alt='Line chart showing total market capitalization growth.'}\n:::\n:::\n\n\n### Return Distribution\n\nUnderstanding the distribution of monthly returns helps identify potential data quality issues and characterize market risk.\n\n::: {#cell-fig-return-distribution .cell execution_count=29}\n``` {.python .cell-code}\nreturn_distribution = (\n    ggplot(prices_monthly, aes(x=\"ret_excess\"))\n    + geom_histogram(\n        binwidth=0.02, \n        fill=\"steelblue\", \n        color=\"white\",\n        alpha=0.7\n    )\n    + labs(\n        x=\"Monthly Excess Return\",\n        y=\"Frequency\",\n        title=\"Distribution of Monthly Excess Returns\"\n    )\n    + scale_x_continuous(limits=(-0.5, 0.5))\n    + theme_minimal()\n)\nreturn_distribution.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of monthly excess returns for Vietnamese stocks.](07_datacore_data_files/figure-html/fig-return-distribution-output-1.png){#fig-return-distribution width=672 height=480 fig-alt='Histogram showing the distribution of monthly excess returns.'}\n:::\n:::\n\n\n### Coverage of Book Equity\n\nBook equity is essential for constructing value portfolios. We examine what fraction of our sample has book equity data available over time.\n\n::: {#cell-fig-book-equity-coverage .cell execution_count=30}\n``` {.python .cell-code}\n# Merge prices with fundamentals\ncoverage_data = (prices_monthly\n    .assign(year=lambda x: x[\"date\"].dt.year)\n    .groupby([\"symbol\", \"year\"])\n    .tail(1)\n    .merge(comp_vn[[\"symbol\", \"year\", \"be\"]], \n           on=[\"symbol\", \"year\"], \n           how=\"left\")\n)\n\n# Compute coverage by year\nbe_coverage = (coverage_data\n    .groupby(\"year\")\n    .apply(lambda x: pd.Series({\n        \"share_with_be\": x[\"be\"].notna().mean()\n    }))\n    .reset_index()\n)\n\ncoverage_figure = (\n    ggplot(be_coverage, aes(x=\"year\", y=\"share_with_be\"))\n    + geom_line(color=\"darkorange\", size=1)\n    + geom_point(color=\"darkorange\", size=2)\n    + labs(\n        x=\"Year\",\n        y=\"Share with Book Equity\",\n        title=\"Coverage of Book Equity Data\"\n    )\n    + scale_y_continuous(labels=percent_format(), limits=(0, 1))\n    + theme_minimal()\n)\ncoverage_figure.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Share of securities with available book equity data by year.](07_datacore_data_files/figure-html/fig-book-equity-coverage-output-1.png){#fig-book-equity-coverage width=672 height=480 fig-alt='Line chart showing the coverage of book equity data over time.'}\n:::\n:::\n\n\n## Merging Stock and Fundamental Data\n\nThe final step links price data with fundamental data using the stock symbol as the common identifier. This merged dataset forms the basis for constructing portfolios sorted on firm characteristics.\n\n::: {#580f42a1 .cell execution_count=31}\n``` {.python .cell-code}\n# Example: Create merged dataset for end-of-June each year\nmerged_data = (prices_monthly\n    .query(\"month == 6\")\n    .merge(\n        comp_vn[[\"symbol\", \"year\", \"be\", \"op\", \"inv\", \"at\"]],\n        on=[\"symbol\", \"year\"],\n        how=\"left\",\n        suffixes=(\"\", \"_fundamental\")\n    )\n)\n\n# Convert BE from VND to BILLION VND\nmerged_data[\"be\"] = merged_data[\"be\"] / 1e9\n\n# Compute book-to-market ratio\nmerged_data[\"bm\"] = merged_data[\"be\"] / merged_data[\"mktcap\"]\n\nmerged_data.loc[\n    (merged_data[\"bm\"] <= 0) |\n    (merged_data[\"bm\"] > 20),\n    \"bm\"\n] = pd.NA\n\n\nmerged_data[\"bm\"].describe(percentiles=[.01, .1, .5, .9, .99])\n\nprint(f\"Merged observations: {len(merged_data):,}\")\nprint(f\"With book-to-market: {merged_data['bm'].notna().sum():,}\")\nmerged_data.head(3)\nmerged_data.describe()\nmerged_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMerged observations: 13,756\nWith book-to-market: 12,859\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>symbol</th>\n      <th>date</th>\n      <th>year</th>\n      <th>month</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>adjusted_close</th>\n      <th>...</th>\n      <th>mktcap</th>\n      <th>mktcap_lag</th>\n      <th>ret</th>\n      <th>risk_free</th>\n      <th>ret_excess</th>\n      <th>be</th>\n      <th>op</th>\n      <th>inv</th>\n      <th>at</th>\n      <th>bm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A32</td>\n      <td>2019-06-30</td>\n      <td>2019.0</td>\n      <td>6.0</td>\n      <td>26.4</td>\n      <td>26.4</td>\n      <td>21.0</td>\n      <td>22.5</td>\n      <td>3700</td>\n      <td>35.929125</td>\n      <td>...</td>\n      <td>153.000</td>\n      <td>179.52</td>\n      <td>-0.170323</td>\n      <td>0.003333</td>\n      <td>-0.173657</td>\n      <td>223.612748</td>\n      <td>0.232362</td>\n      <td>-0.072329</td>\n      <td>4.349303e+11</td>\n      <td>1.461521</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A32</td>\n      <td>2020-06-30</td>\n      <td>2020.0</td>\n      <td>6.0</td>\n      <td>25.0</td>\n      <td>26.3</td>\n      <td>24.5</td>\n      <td>26.3</td>\n      <td>7500</td>\n      <td>38.811173</td>\n      <td>...</td>\n      <td>178.840</td>\n      <td>187.00</td>\n      <td>-0.067977</td>\n      <td>0.003333</td>\n      <td>-0.071311</td>\n      <td>242.216943</td>\n      <td>0.195565</td>\n      <td>0.122698</td>\n      <td>4.882955e+11</td>\n      <td>1.354378</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A32</td>\n      <td>2021-06-30</td>\n      <td>2021.0</td>\n      <td>6.0</td>\n      <td>30.2</td>\n      <td>37.0</td>\n      <td>29.5</td>\n      <td>32.0</td>\n      <td>78400</td>\n      <td>45.363520</td>\n      <td>...</td>\n      <td>217.600</td>\n      <td>214.20</td>\n      <td>0.015873</td>\n      <td>0.003333</td>\n      <td>0.012540</td>\n      <td>238.385190</td>\n      <td>0.157723</td>\n      <td>0.081581</td>\n      <td>5.281309e+11</td>\n      <td>1.095520</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A32</td>\n      <td>2022-06-30</td>\n      <td>2022.0</td>\n      <td>6.0</td>\n      <td>30.9</td>\n      <td>35.5</td>\n      <td>25.0</td>\n      <td>35.3</td>\n      <td>15200</td>\n      <td>47.503210</td>\n      <td>...</td>\n      <td>240.040</td>\n      <td>210.12</td>\n      <td>0.142395</td>\n      <td>0.003333</td>\n      <td>0.139061</td>\n      <td>215.399735</td>\n      <td>0.172085</td>\n      <td>0.036584</td>\n      <td>5.474523e+11</td>\n      <td>0.897349</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A32</td>\n      <td>2023-06-30</td>\n      <td>2023.0</td>\n      <td>6.0</td>\n      <td>30.1</td>\n      <td>33.5</td>\n      <td>29.2</td>\n      <td>29.4</td>\n      <td>2400</td>\n      <td>35.064204</td>\n      <td>...</td>\n      <td>199.920</td>\n      <td>204.68</td>\n      <td>-0.023256</td>\n      <td>0.003333</td>\n      <td>-0.026589</td>\n      <td>222.024135</td>\n      <td>0.174658</td>\n      <td>-0.076752</td>\n      <td>5.054342e+11</td>\n      <td>1.110565</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13751</th>\n      <td>YTC</td>\n      <td>2019-06-30</td>\n      <td>2019.0</td>\n      <td>6.0</td>\n      <td>70.0</td>\n      <td>79.9</td>\n      <td>70.0</td>\n      <td>79.9</td>\n      <td>38900</td>\n      <td>171.451817</td>\n      <td>...</td>\n      <td>246.092</td>\n      <td>215.60</td>\n      <td>0.141429</td>\n      <td>0.003333</td>\n      <td>0.138095</td>\n      <td>59.901389</td>\n      <td>0.738190</td>\n      <td>-0.021758</td>\n      <td>7.521980e+11</td>\n      <td>0.243411</td>\n    </tr>\n    <tr>\n      <th>13752</th>\n      <td>YTC</td>\n      <td>2020-06-30</td>\n      <td>2020.0</td>\n      <td>6.0</td>\n      <td>88.5</td>\n      <td>88.5</td>\n      <td>77.0</td>\n      <td>87.0</td>\n      <td>150640</td>\n      <td>180.966960</td>\n      <td>...</td>\n      <td>267.960</td>\n      <td>272.58</td>\n      <td>-0.016949</td>\n      <td>0.003333</td>\n      <td>-0.020282</td>\n      <td>13.459082</td>\n      <td>-0.458548</td>\n      <td>0.323501</td>\n      <td>9.955348e+11</td>\n      <td>0.050228</td>\n    </tr>\n    <tr>\n      <th>13753</th>\n      <td>YTC</td>\n      <td>2021-06-30</td>\n      <td>2021.0</td>\n      <td>6.0</td>\n      <td>76.0</td>\n      <td>115.5</td>\n      <td>61.0</td>\n      <td>61.0</td>\n      <td>34100</td>\n      <td>126.884880</td>\n      <td>...</td>\n      <td>187.880</td>\n      <td>234.08</td>\n      <td>-0.197368</td>\n      <td>0.003333</td>\n      <td>-0.200702</td>\n      <td>21.746595</td>\n      <td>0.539521</td>\n      <td>-0.215694</td>\n      <td>7.808035e+11</td>\n      <td>0.115747</td>\n    </tr>\n    <tr>\n      <th>13754</th>\n      <td>YTC</td>\n      <td>2022-06-30</td>\n      <td>2022.0</td>\n      <td>6.0</td>\n      <td>68.0</td>\n      <td>68.0</td>\n      <td>65.0</td>\n      <td>65.5</td>\n      <td>200</td>\n      <td>136.245240</td>\n      <td>...</td>\n      <td>201.740</td>\n      <td>209.44</td>\n      <td>-0.036765</td>\n      <td>0.003333</td>\n      <td>-0.040098</td>\n      <td>32.403055</td>\n      <td>0.483088</td>\n      <td>0.182911</td>\n      <td>9.236206e+11</td>\n      <td>0.160618</td>\n    </tr>\n    <tr>\n      <th>13755</th>\n      <td>YTC</td>\n      <td>2023-06-30</td>\n      <td>2023.0</td>\n      <td>6.0</td>\n      <td>59.0</td>\n      <td>59.0</td>\n      <td>59.0</td>\n      <td>59.0</td>\n      <td>49545</td>\n      <td>122.724720</td>\n      <td>...</td>\n      <td>181.720</td>\n      <td>181.72</td>\n      <td>0.000000</td>\n      <td>0.003333</td>\n      <td>-0.003333</td>\n      <td>38.976624</td>\n      <td>0.450157</td>\n      <td>0.017930</td>\n      <td>9.401815e+11</td>\n      <td>0.214487</td>\n    </tr>\n  </tbody>\n</table>\n<p>13756 rows  21 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#4b562011 .cell execution_count=32}\n``` {.python .cell-code}\nfrom plotnine import *\nimport numpy as np\n\nbm_plot_data = (\n    merged_data[[\"bm\"]]\n      .dropna()\n      .assign(bm_plot=lambda x: x[\"bm\"].clip(upper=10))\n)\n\n(\n    ggplot(bm_plot_data, aes(x=\"bm_plot\")) +\n    geom_histogram(bins=80) +\n    labs(\n        title=\"Distribution of Book to Market Ratios\",\n        x=\"Book to Market (capped at 10 for plotting)\",\n        y=\"Number of firms\"\n    ) +\n    theme_minimal()\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n![](07_datacore_data_files/figure-html/cell-33-output-1.png){width=672 height=480}\n:::\n:::\n\n\n::: {#1f0d7766 .cell execution_count=33}\n``` {.python .cell-code}\nsize_plot_data = (\n    merged_data[[\"mktcap_lag\"]]\n      .dropna()\n      .assign(log_size=lambda x: np.log(x[\"mktcap_lag\"]))\n)\n\n(\n    ggplot(size_plot_data, aes(x=\"log_size\")) +\n    geom_histogram(bins=80) +\n    labs(\n        title=\"Distribution of Log Market Capitalization\",\n        x=\"Log Market Cap\",\n        y=\"Number of firms\"\n    ) +\n    theme_minimal()\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n![](07_datacore_data_files/figure-html/cell-34-output-1.png){width=672 height=480}\n:::\n:::\n\n\n::: {#02917d55 .cell execution_count=34}\n``` {.python .cell-code}\nscatter_data = (\n    merged_data[[\"be\", \"mktcap_lag\"]]\n      .dropna()\n      .assign(\n          log_be=lambda x: np.log(x[\"be\"]),\n          log_me=lambda x: np.log(x[\"mktcap_lag\"])\n      )\n)\n\n(\n    ggplot(scatter_data, aes(x=\"log_me\", y=\"log_be\")) +\n    geom_point(alpha=0.2) +\n    labs(\n        title=\"Log Book Equity vs Log Market Equity\",\n        x=\"Log Market Cap\",\n        y=\"Log Book Equity\"\n    ) +\n    theme_minimal()\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n![](07_datacore_data_files/figure-html/cell-35-output-1.png){width=672 height=480}\n:::\n:::\n\n\n## Key Takeaways\n\n1. **Datacore provides unified access** to Vietnamese financial data through a modern cloud-based infrastructure, eliminating the need to aggregate data from multiple fragmented sources.\n\n2. **Company fundamentals** from Datacore include comprehensive balance sheet, income statement, and cash flow data prepared according to Vietnamese Accounting Standards, which we map to standard variables used in international research.\n\n3. **Book equity computation** follows the Fama-French methodology, accounting for deferred taxes and preferred stock to ensure comparability with US-based studies.\n\n4. **Stock price data** includes adjustment factors for corporate actions, enabling accurate return calculations over long horizons.\n\n5. **Monthly frequency** is standard for asset pricing research, reducing noise while maintaining sufficient observations for statistical inference.\n\n6. **Risk-free rate approximation** uses Vietnamese government bond yields as a proxy, given the absence of a standardized short-term rate series comparable to US Treasury bills.\n\n7. **Data quality validation** through descriptive statistics and visualization helps identify potential issues before conducting formal analyses.\n\n8. **Batch processing** enables efficient handling of large daily datasets that would otherwise exceed memory constraints.\n\n",
    "supporting": [
      "07_datacore_data_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}