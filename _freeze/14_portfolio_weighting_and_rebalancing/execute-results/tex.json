{
  "hash": "dc2f54855eab3fe77f38ab9b0d48586d",
  "result": {
    "engine": "jupyter",
    "markdown": "# Portfolio Weighting and Rebalancing\n\n::: callout-note\nIn this chapter, we systematically compare portfolio weighting schemes (e.g., value-weighted, equal-weighted, and several risk-based alternativesin the Vietnamese equity market. We quantify the impact of rebalancing frequency and transaction costs on realized performance, and develop practical tools for constructing implementable portfolios under the frictions characteristic of an emerging market.\n:::\n\nEvery portfolio construction decision ultimately reduces to two choices: which assets to hold, and how much to allocate to each. While earlier chapters have focused on the first question, using factor models, anomalies, and fundamental analysis to select stocks, this chapter addresses the second. The weighting scheme a researcher or investor applies can fundamentally alter the conclusions drawn from portfolio-level tests and the returns earned from an investment strategy.\n\nThe distinction matters more in Vietnam than in large, liquid markets. The Vietnamese equity market features extreme skewness in the market capitalization distribution: the top 10 firms on HOSE account for roughly 50% of total market capitalization, while hundreds of small firms contribute negligible weight. Under value-weighting, a portfolio's performance is dominated by a handful of large-cap names (Vinhomes, Vingroup, Vietcombank, FPT). Under equal-weighting, every firm contributes equally, tilting the portfolio toward small, illiquid stocks that may be expensive or impossible to trade at scale. Neither scheme is inherently correct; the choice depends on the question being asked.\n\nThis chapter develops the analytical framework for making that choice. We begin with the theoretical properties of weighting schemes, implement each scheme in practice with Vietnamese data, quantify the transaction costs of rebalancing, and extend the analysis to risk-based alternatives that explicitly incorporate the covariance structure of returns.\n\n## Theoretical Framework {#sec-weight-theory}\n\n### Value-Weighted Portfolios\n\nA value-weighted (VW) portfolio allocates to each stock in proportion to its market capitalization:\n\n$$\nw_{i,t}^{VW} = \\frac{\\text{MCap}_{i,t}}{\\sum_{j=1}^{N_t} \\text{MCap}_{j,t}}\n$$ {#eq-vw}\n\nwhere $\\text{MCap}_{i,t} = P_{i,t} \\times \\text{Shares}_{i,t}$ is the market capitalization of stock $i$ at time $t$.\n\nThe VW portfolio has a unique theoretical status: it is the portfolio that all investors collectively hold (the \"market portfolio\" in CAPM). Its key properties are:\n\n1.  **Self-rebalancing.** As prices move, weights adjust automatically. A VW portfolio requires trading only when constituents enter or leave the index, or when corporate actions (splits, issuances) change shares outstanding.\n2.  **Low turnover.** Because weights drift with prices rather than being reset to targets, VW portfolios have minimal rebalancing costs.\n3.  **Large-cap bias.** Returns are dominated by the largest firms. In Vietnam, this means the portfolio's risk-return profile is heavily influenced by banking, real estate, and technology conglomerates.\n\n@hsu2006cap argues that VW portfolios are sub-optimal because they mechanically overweight overpriced stocks and underweight underpriced stocks (i.e., any deviation of price from fundamental value creates a systematic drag on VW performance relative to a fundamentally weighted alternative).\n\n### Equal-Weighted Portfolios\n\nAn equal-weighted (EW) portfolio assigns the same weight to each constituent:\n\n$$\nw_{i,t}^{EW} = \\frac{1}{N_t}\n$$ {#eq-ew}\n\n@demiguel2009optimal show that the 1/N portfolio is surprisingly competitive with mean-variance optimized portfolios, particularly when estimation windows are short and the number of assets is large (conditions that closely describe the Vietnamese market). The intuition is that estimation error in expected returns and covariances can overwhelm the gains from optimization, making the \"naive\" equal-weight scheme a robust default.\n\n@plyakha2021equal decompose the EW outperformance over VW into two components:\n\n1.  **Size tilt.** EW allocates more to small firms, which historically earn a size premium.\n2.  **Rebalancing bonus.** Monthly rebalancing back to equal weights is a contrarian strategy: it sells recent winners and buys recent losers, profiting from mean reversion in individual stock returns.\n<!-- 3.  **Diversification.** EW provides lower concentration and higher effective diversification. -->\n\nHowever, the EW portfolio has practical disadvantages that are particularly severe in Vietnam:\n\n-   **High turnover.** Every rebalancing date requires trading every stock back to equal weight.\n-   **Illiquidity exposure.** Equal weighting of micro-cap stocks that trade VND 100 million/day alongside large-caps trading VND 500 billion/day creates severe implementation challenges.\n-   **Price impact.** In a market with daily price limits ($\\pm$ 7% on HOSE, $\\pm$ 10% on HNX), rebalancing trades for illiquid names may hit limit-up or limit-down, preventing full execution.\n\n### The Weighting Spectrum\n\nBetween VW and EW lies a continuum of weighting schemes. @tbl-weight-schemes summarizes the major alternatives.\n\n| Scheme | Weight Formula | Key Property | Key Risk |\n|------------------|------------------|------------------|------------------|\n| Value-weighted | $w_i \\propto \\text{MCap}_i$ | Self-rebalancing, low turnover | Large-cap concentration |\n| Equal-weighted | $w_i = 1/N$ | Maximum naive diversification | High turnover, illiquidity |\n| Fundamental | $w_i \\propto F_i$ (revenue, book equity, etc.) | Breaks price-value link | Requires accounting data |\n| Minimum variance | $\\mathbf{w} = \\arg\\min \\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}$ | Lowest portfolio volatility | Estimation error in $\\boldsymbol{\\Sigma}$ |\n| Risk parity | $w_i \\sigma_i = w_j \\sigma_j \\; \\forall \\, i,j$ | Equal risk contribution | Leverages low-vol assets |\n| Maximum diversification | $\\max \\frac{\\mathbf{w}'\\boldsymbol{\\sigma}}{\\sqrt{\\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}}}$ | Maximizes diversification ratio | Sensitive to correlation estimates |\n| Capped VW | $w_i \\propto \\text{MCap}_i$, $w_i \\leq \\bar{w}$ | Reduces concentration | Arbitrary cap threshold |\n\n: Summary of portfolio weighting schemes. {#tbl-weight-schemes}\n\n## Data Construction {#sec-weight-data}\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code code-summary=\"Import libraries and configure environment\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.optimize import minimize\nfrom sklearn.covariance import LedoitWolf\nfrom linearmodels.panel import PanelOLS\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (10, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n```\n:::\n\n\n::: {#data-load .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Load market and fundamental data\"}\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Daily prices and volume\ndaily = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    fields=[\n        'ticker', 'date', 'close', 'adjusted_close', 'volume',\n        'turnover_value', 'market_cap', 'shares_outstanding',\n        'bid_ask_spread', 'free_float_pct'\n    ]\n)\n\n# Monthly returns (pre-computed for convenience)\nmonthly = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'volume_avg_20d', 'turnover_value_avg_20d'\n    ]\n)\n\n# Fundamentals for fundamental weighting\nfundamentals = client.get_fundamentals(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    frequency='annual',\n    fields=[\n        'ticker', 'fiscal_year', 'revenue', 'book_equity',\n        'total_assets', 'dividends_paid', 'operating_cash_flow'\n    ]\n)\n\n# Market-level returns for benchmarking\nmarket_index = client.get_index(\n    index='VNINDEX',\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    frequency='monthly'\n)\n\nprint(f\"Daily observations: {daily.shape[0]:,}\")\nprint(f\"Monthly observations: {monthly.shape[0]:,}\")\nprint(f\"Unique tickers: {monthly['ticker'].nunique()}\")\n```\n:::\n\n\n### Universe Construction and Liquidity Filters\n\nA critical pre-processing step is defining the investable universe. Including all listed stocks, regardless of liquidity, inflates the apparent benefits of equal-weighting and other small-cap-tilted schemes because it implicitly assumes the ability to trade illiquid stocks without friction. We apply graduated liquidity filters and track how results change.\n\n::: {#universe-construction .cell execution_count=3}\n``` {.python .cell-code code-summary=\"Apply liquidity filters to define investable universes\"}\ndef construct_universe(monthly_df, min_mcap_pct=0, min_turnover=0, min_months=12):\n    \"\"\"\n    Construct investable universe with liquidity filters.\n\n    Parameters\n    ----------\n    min_mcap_pct : float\n        Exclude stocks below this market cap percentile (0-100).\n    min_turnover : float\n        Minimum average daily turnover in VND billion.\n    min_months : int\n        Minimum months of return history required.\n    \"\"\"\n    df = monthly_df.copy()\n\n    # Market cap percentile filter (within each month)\n    if min_mcap_pct > 0:\n        df[\"mcap_pctile\"] = df.groupby(\"month_end\")[\"market_cap\"].transform(\n            lambda x: x.rank(pct=True) * 100\n        )\n        df = df[df[\"mcap_pctile\"] >= min_mcap_pct]\n\n    # Turnover filter\n    if min_turnover > 0:\n        df = df[df[\"turnover_value_avg_20d\"] >= min_turnover * 1e9]\n\n    # History filter\n    ticker_months = df.groupby(\"ticker\")[\"month_end\"].transform(\"count\")\n    df = df[ticker_months >= min_months]\n\n    return df\n\n\n# Define three universes of increasing restrictiveness\nuniverse_all = construct_universe(monthly)\nuniverse_mid = construct_universe(monthly, min_mcap_pct=20, min_turnover=0.5)\nuniverse_liquid = construct_universe(monthly, min_mcap_pct=40, min_turnover=2.0)\n\nfor name, univ in [\n    (\"All stocks\", universe_all),\n    (\"Mid filter\", universe_mid),\n    (\"Liquid only\", universe_liquid),\n]:\n    n_stocks = univ.groupby(\"month_end\")[\"ticker\"].nunique().median()\n    print(f\"{name}: median {n_stocks:.0f} stocks/month\")\n```\n:::\n\n\n### Market Capitalization Concentration\n\nBefore comparing weighting schemes, it is instructive to document how concentrated the Vietnamese market actually is.\n\n::: {#fig-concentration .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Visualize market cap concentration\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Cumulative market cap share (latest month)\nlatest = monthly[monthly['month_end'] == monthly['month_end'].max()].copy()\nlatest = latest.sort_values('market_cap', ascending=False)\nlatest['cum_mcap_share'] = (\n    latest['market_cap'].cumsum() / latest['market_cap'].sum()\n)\nlatest['rank'] = range(1, len(latest) + 1)\nlatest['rank_pct'] = latest['rank'] / len(latest) * 100\n\naxes[0].plot(latest['rank_pct'], latest['cum_mcap_share'] * 100,\n             color='#2C5F8A', linewidth=2)\naxes[0].axhline(y=50, color='gray', linestyle='--', linewidth=0.8)\naxes[0].axhline(y=80, color='gray', linestyle='--', linewidth=0.8)\n\n# Mark top 10 and top 30\nn_at_50 = (latest['cum_mcap_share'] <= 0.50).sum()\naxes[0].annotate(f'Top {n_at_50} stocks = 50%',\n                 xy=(n_at_50 / len(latest) * 100, 50),\n                 fontsize=9, color='#C0392B')\naxes[0].set_xlabel('Cumulative Stock Rank (%)')\naxes[0].set_ylabel('Cumulative Market Cap Share (%)')\naxes[0].set_title('Panel A: Market Cap Concentration Curve')\n\n# Panel B: HHI over time\nhhi_ts = (\n    monthly\n    .groupby('month_end')\n    .apply(lambda g: (g['market_cap'] / g['market_cap'].sum()).pow(2).sum())\n    .reset_index(name='hhi')\n)\nhhi_ts['month_end'] = pd.to_datetime(hhi_ts['month_end'])\naxes[1].plot(hhi_ts['month_end'], hhi_ts['hhi'] * 10000,\n             color='#2C5F8A', linewidth=1.5)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('HHI (basis points)')\naxes[1].set_title('Panel B: Herfindahl Index of VW Weights')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Implementing Weighting Schemes {#sec-weight-implementation}\n\nWe now implement each weighting scheme and compute monthly portfolio returns. All implementations follow a common structure: at each rebalancing date, compute target weights from available information, then compute the weighted return over the subsequent holding period.\n\n### Core Portfolio Engine\n\n::: {#portfolio-engine .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Generic portfolio construction and backtesting engine\"}\ndef compute_portfolio_returns(\n    monthly_df,\n    weight_fn,\n    rebal_freq=\"M\",\n    max_weight=1.0,\n    min_weight=0.0,\n):\n    \"\"\"\n    Compute time series of portfolio returns for a given weighting function.\n\n    Parameters\n    ----------\n    monthly_df : DataFrame\n        Must contain 'ticker', 'month_end', 'monthly_return', and any\n        columns needed by weight_fn.\n    weight_fn : callable\n        Function that takes a cross-section DataFrame and returns a\n        Series of weights indexed by ticker. Weights need not sum to 1\n        (they will be normalized).\n    rebal_freq : str\n        'M' for monthly, 'Q' for quarterly, 'A' for annual.\n    max_weight : float\n        Maximum weight per stock (for capped schemes).\n    min_weight : float\n        Minimum weight per stock.\n\n    Returns\n    -------\n    DataFrame with columns: month_end, port_return, n_stocks,\n    turnover, hhi, effective_n\n    \"\"\"\n    months = sorted(monthly_df[\"month_end\"].unique())\n\n    # Determine rebalancing dates\n    if rebal_freq == \"M\":\n        rebal_dates = set(months)\n    elif rebal_freq == \"Q\":\n        rebal_dates = set(pd.to_datetime(months).to_period(\"Q\").to_timestamp(\"M\"))\n        # Map to nearest month-end\n        rebal_dates = {m for m in months if pd.Timestamp(m).month % 3 == 0}\n        if not rebal_dates:\n            rebal_dates = set(months[::3])\n    elif rebal_freq == \"A\":\n        rebal_dates = {m for m in months if pd.Timestamp(m).month == 6}\n        if not rebal_dates:\n            rebal_dates = set(months[::12])\n    else:\n        rebal_dates = set(months)\n\n    results = []\n    prev_weights = None\n\n    for month in months:\n        cross_section = monthly_df[monthly_df[\"month_end\"] == month].copy()\n        cross_section = cross_section.dropna(subset=[\"monthly_return\"])\n\n        if len(cross_section) < 5:\n            continue\n\n        if month in rebal_dates or prev_weights is None:\n            # Compute fresh weights\n            raw_weights = weight_fn(cross_section)\n            raw_weights = raw_weights.clip(lower=min_weight, upper=max_weight)\n            total = raw_weights.sum()\n            if total <= 0:\n                continue\n            target_weights = raw_weights / total\n        else:\n            # Drift weights forward from previous month\n            if prev_weights is not None:\n                available = cross_section.set_index(\"ticker\")\n                drifted = prev_weights.reindex(available.index, fill_value=0)\n                # Adjust for returns\n                drifted = drifted * (1 + available[\"monthly_return\"])\n                total = drifted.sum()\n                if total <= 0:\n                    continue\n                target_weights = drifted / total\n            else:\n                continue\n\n        # Align weights with available stocks\n        cross_section = cross_section.set_index(\"ticker\")\n        aligned_w = target_weights.reindex(cross_section.index, fill_value=0)\n        aligned_w = aligned_w / aligned_w.sum()\n\n        # Portfolio return\n        port_ret = (aligned_w * cross_section[\"monthly_return\"]).sum()\n\n        # Turnover (two-way)\n        if prev_weights is not None:\n            prev_aligned = prev_weights.reindex(aligned_w.index, fill_value=0)\n            # Drift previous weights\n            prev_drifted = prev_aligned * (\n                1\n                + cross_section[\"monthly_return\"].reindex(\n                    prev_aligned.index, fill_value=0\n                )\n            )\n            prev_drifted = (\n                prev_drifted / prev_drifted.sum()\n                if prev_drifted.sum() > 0\n                else prev_drifted\n            )\n            turnover = (aligned_w - prev_drifted).abs().sum() / 2\n        else:\n            turnover = 1.0\n\n        # Concentration metrics\n        hhi = (aligned_w**2).sum()\n        effective_n = 1.0 / hhi if hhi > 0 else 0\n\n        results.append(\n            {\n                \"month_end\": month,\n                \"port_return\": port_ret,\n                \"n_stocks\": (aligned_w > 1e-6).sum(),\n                \"turnover\": turnover,\n                \"hhi\": hhi,\n                \"effective_n\": effective_n,\n            }\n        )\n\n        prev_weights = aligned_w.copy()\n\n    return pd.DataFrame(results)\n```\n:::\n\n\n### Value-Weighted Portfolio\n\n::: {#vw-portfolio .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Implement value-weighted portfolio\"}\ndef vw_weights(cross_section):\n    \"\"\"Value-weighted: proportional to market cap.\"\"\"\n    return cross_section.set_index('ticker')['market_cap']\n\nvw_returns = compute_portfolio_returns(universe_mid, vw_weights, rebal_freq='M')\nprint(f\"VW portfolio: {len(vw_returns)} months\")\nprint(f\"Mean monthly return: {vw_returns['port_return'].mean():.4f}\")\nprint(f\"Mean turnover: {vw_returns['turnover'].mean():.4f}\")\nprint(f\"Mean effective N: {vw_returns['effective_n'].mean():.1f}\")\n```\n:::\n\n\n### Equal-Weighted Portfolio\n\n::: {#ew-portfolio .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Implement equal-weighted portfolio at different rebalancing frequencies\"}\ndef ew_weights(cross_section):\n    \"\"\"Equal-weighted: 1/N.\"\"\"\n    tickers = cross_section.set_index('ticker').index\n    return pd.Series(1.0, index=tickers)\n\n# Monthly rebalancing\new_monthly = compute_portfolio_returns(\n    universe_mid, ew_weights, rebal_freq='M'\n)\n\n# Quarterly rebalancing\new_quarterly = compute_portfolio_returns(\n    universe_mid, ew_weights, rebal_freq='Q'\n)\n\n# Annual rebalancing (June)\new_annual = compute_portfolio_returns(\n    universe_mid, ew_weights, rebal_freq='A'\n)\n\nfor name, df in [('EW Monthly', ew_monthly),\n                  ('EW Quarterly', ew_quarterly),\n                  ('EW Annual', ew_annual)]:\n    print(f\"{name}: mean ret = {df['port_return'].mean():.4f}, \"\n          f\"turnover = {df['turnover'].mean():.4f}\")\n```\n:::\n\n\n### Capped Value-Weighted Portfolio\n\nTo mitigate the concentration of pure VW while retaining its low-turnover properties, we impose a cap on individual stock weights. A common choice is 5% or 10%, mimicking the construction rules of capped indices such as the MSCI Capped indices.\n\n::: {#capped-vw .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Implement capped value-weighted portfolios\"}\ndef capped_vw_weights(cross_section, cap=0.05):\n    \"\"\"Capped VW: market cap weights with an upper bound.\"\"\"\n    w = cross_section.set_index('ticker')['market_cap']\n    w = w / w.sum()\n    # Iterative capping (redistribute excess weight)\n    for _ in range(20):\n        excess = w[w > cap] - cap\n        if excess.sum() <= 1e-8:\n            break\n        w[w > cap] = cap\n        w[w <= cap] *= (1 + excess.sum() / w[w <= cap].sum())\n    return w\n\ncapped5 = compute_portfolio_returns(\n    universe_mid, lambda cs: capped_vw_weights(cs, 0.05), rebal_freq='M'\n)\ncapped10 = compute_portfolio_returns(\n    universe_mid, lambda cs: capped_vw_weights(cs, 0.10), rebal_freq='M'\n)\n\nprint(f\"Capped 5%: eff_N = {capped5['effective_n'].mean():.1f}, \"\n      f\"turnover = {capped5['turnover'].mean():.4f}\")\nprint(f\"Capped 10%: eff_N = {capped10['effective_n'].mean():.1f}, \"\n      f\"turnover = {capped10['turnover'].mean():.4f}\")\n```\n:::\n\n\n### Fundamental-Weighted Portfolio\n\n@arnott2005fundamental propose weighting stocks by fundamental measures (revenue, book equity, dividends, cash flow) rather than market cap. The logic is that fundamental weights are not contaminated by pricing errors, breaking the mechanical overweighting of overvalued stocks inherent in VW. We construct a composite fundamental weight using the average rank across four measures:\n\n$$\nw_{i,t}^{FW} \\propto \\frac{1}{4}\\left(\\text{Rank}_{i,t}^{\\text{Rev}} + \\text{Rank}_{i,t}^{\\text{BE}} + \\text{Rank}_{i,t}^{\\text{Div}} + \\text{Rank}_{i,t}^{\\text{CFO}}\\right)\n$$ {#eq-fw}\n\n::: {#fundamental-weighted .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Implement fundamental-weighted portfolio\"}\n# Merge fundamentals with monthly data (use most recent fiscal year)\nfundamentals[\"merge_year\"] = fundamentals[\"fiscal_year\"] + 1  # Lag by 1 year\nmonthly_fund = monthly.copy()\nmonthly_fund[\"year\"] = pd.to_datetime(monthly_fund[\"month_end\"]).dt.year\n\nmonthly_fund = monthly_fund.merge(\n    fundamentals.rename(columns={\"merge_year\": \"year\"}),\n    on=[\"ticker\", \"year\"],\n    how=\"left\",\n)\n\n\ndef fw_weights(cross_section):\n    \"\"\"Fundamental-weighted: composite of revenue, book equity, dividends, CFO.\"\"\"\n    cs = cross_section.set_index(\"ticker\")\n    ranks = pd.DataFrame(index=cs.index)\n\n    for col in [\"revenue\", \"book_equity\", \"dividends_paid\", \"operating_cash_flow\"]:\n        if col in cs.columns:\n            vals = cs[col].clip(lower=0)  # Only positive values\n            ranks[col] = vals.rank(pct=True)\n\n    composite = ranks.mean(axis=1)\n    composite = composite.fillna(0)\n    return composite\n\n\nfw_returns = compute_portfolio_returns(monthly_fund, fw_weights, rebal_freq=\"A\")\nprint(\n    f\"Fundamental-weighted: mean ret = {fw_returns['port_return'].mean():.4f}, \"\n    f\"eff_N = {fw_returns['effective_n'].mean():.1f}\"\n)\n```\n:::\n\n\n## Risk-Based Weighting Schemes {#sec-weight-risk-based}\n\nThe weighting schemes above use only market cap or accounting data. Risk-based schemes incorporate the covariance structure of returns, aiming to produce portfolios with better risk-adjusted performance. The trade-off is that they require estimating the covariance matrix (i.e., a high-dimensional object that is notoriously difficult to estimate precisely with short time series).\n\n### Covariance Estimation\n\nWith $N \\approx 300$ stocks and $T \\approx 60$ months, the sample covariance matrix is severely ill-conditioned. We use the @ledoit2004well shrinkage estimator, which pulls the sample covariance toward a structured target (the identity matrix scaled by the average variance):\n\n$$\n\\hat{\\boldsymbol{\\Sigma}}^{\\text{shrink}} = \\delta \\mathbf{F} + (1 - \\delta) \\mathbf{S}\n$$ {#eq-shrinkage}\n\nwhere $\\mathbf{S}$ is the sample covariance, $\\mathbf{F}$ is the shrinkage target, and $\\delta \\in [0,1]$ is the optimal shrinkage intensity derived analytically.\n\n::: {#covariance-estimation .cell execution_count=10}\n``` {.python .cell-code code-summary=\"Estimate covariance matrix with Ledoit-Wolf shrinkage\"}\ndef estimate_covariance(monthly_df, month, lookback=60, min_obs=36):\n    \"\"\"\n    Estimate covariance matrix using Ledoit-Wolf shrinkage.\n    \n    Parameters\n    ----------\n    monthly_df : DataFrame with ticker, month_end, monthly_return\n    month : target month (use returns before this date)\n    lookback : number of months to use\n    min_obs : minimum observations per stock\n    \n    Returns\n    -------\n    cov_matrix : DataFrame (N x N)\n    tickers : list of tickers with sufficient data\n    \"\"\"\n    end_date = pd.Timestamp(month)\n    start_date = end_date - pd.DateOffset(months=lookback)\n    \n    window = monthly_df[\n        (monthly_df['month_end'] > start_date) &\n        (monthly_df['month_end'] <= end_date)\n    ]\n    \n    # Pivot to wide format\n    returns_wide = window.pivot_table(\n        index='month_end', columns='ticker', values='monthly_return'\n    )\n    \n    # Keep stocks with sufficient observations\n    valid_cols = returns_wide.columns[returns_wide.notna().sum() >= min_obs]\n    returns_wide = returns_wide[valid_cols].dropna(axis=0, how='all')\n    \n    # Fill remaining NAs with 0 (conservative)\n    returns_clean = returns_wide.fillna(0)\n    \n    if returns_clean.shape[1] < 10:\n        return None, None\n    \n    # Ledoit-Wolf shrinkage\n    lw = LedoitWolf()\n    lw.fit(returns_clean.values)\n    \n    cov_matrix = pd.DataFrame(\n        lw.covariance_,\n        index=valid_cols, columns=valid_cols\n    )\n    \n    return cov_matrix, list(valid_cols)\n```\n:::\n\n\n### Minimum Variance Portfolio\n\nThe global minimum variance (GMV) portfolio minimizes portfolio variance without targeting a specific return level [@clarke2011minimum]:\n\n$$\n\\mathbf{w}^{MV} = \\arg\\min_{\\mathbf{w}} \\; \\mathbf{w}'\\hat{\\boldsymbol{\\Sigma}}\\mathbf{w} \\quad \\text{s.t.} \\quad \\mathbf{1}'\\mathbf{w} = 1, \\; w_i \\geq 0\n$$ {#eq-minvar}\n\nThe long-only constraint ($w_i \\geq 0$) is essential in practice and also acts as an implicit shrinkage that improves out-of-sample performance [@jagannathan2003risk].\n\n::: {#minimum-variance .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Implement minimum variance portfolio\"}\ndef minimum_variance_weights(cov_matrix, max_weight=0.05):\n    \"\"\"\n    Solve for the minimum variance portfolio with long-only\n    and position-size constraints.\n    \"\"\"\n    n = cov_matrix.shape[0]\n    Sigma = cov_matrix.values\n    \n    def portfolio_variance(w):\n        return w @ Sigma @ w\n    \n    constraints = [\n        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n    ]\n    bounds = [(0, max_weight) for _ in range(n)]\n    x0 = np.ones(n) / n\n    \n    result = minimize(\n        portfolio_variance, x0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints=constraints,\n        options={'maxiter': 1000, 'ftol': 1e-12}\n    )\n    \n    if result.success:\n        return pd.Series(result.x, index=cov_matrix.index)\n    else:\n        return pd.Series(1.0 / n, index=cov_matrix.index)\n\ndef mv_weight_fn(cross_section, cov_cache={}):\n    \"\"\"Wrapper for portfolio engine: minimum variance.\"\"\"\n    month = cross_section['month_end'].iloc[0]\n    \n    if month not in cov_cache:\n        cov_matrix, tickers = estimate_covariance(monthly, month)\n        cov_cache[month] = (cov_matrix, tickers)\n    \n    cov_matrix, tickers = cov_cache[month]\n    if cov_matrix is None:\n        # Fallback to equal weight\n        return pd.Series(1.0, index=cross_section.set_index('ticker').index)\n    \n    # Restrict to stocks in both cross-section and covariance matrix\n    available = set(cross_section['ticker']) & set(tickers)\n    if len(available) < 10:\n        return pd.Series(1.0, index=cross_section.set_index('ticker').index)\n    \n    sub_cov = cov_matrix.loc[list(available), list(available)]\n    weights = minimum_variance_weights(sub_cov)\n    \n    return weights\n\nmv_returns = compute_portfolio_returns(\n    universe_mid, mv_weight_fn, rebal_freq='Q'\n)\nprint(f\"Min Variance: mean ret = {mv_returns['port_return'].mean():.4f}, \"\n      f\"std = {mv_returns['port_return'].std():.4f}\")\n```\n:::\n\n\n### Risk Parity (Equal Risk Contribution)\n\nRisk parity allocates so that each asset contributes equally to total portfolio risk [@maillard2010properties]. The risk contribution of asset $i$ is:\n\n$$\nRC_i = w_i \\cdot \\frac{(\\boldsymbol{\\Sigma} \\mathbf{w})_i}{\\sqrt{\\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}}}\n$$ {#eq-risk-contribution}\n\nThe risk parity portfolio solves $RC_i = RC_j$ for all $i, j$:\n\n::: {#risk-parity .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Implement risk parity (equal risk contribution) portfolio\"}\ndef risk_parity_weights(cov_matrix, max_weight=0.05):\n    \"\"\"\n    Solve for the risk parity portfolio where each asset\n    contributes equally to total portfolio variance.\n    \"\"\"\n    n = cov_matrix.shape[0]\n    Sigma = cov_matrix.values\n    \n    def risk_parity_objective(w):\n        port_var = w @ Sigma @ w\n        marginal = Sigma @ w\n        risk_contrib = w * marginal\n        target_rc = port_var / n\n        return np.sum((risk_contrib - target_rc) ** 2)\n    \n    constraints = [\n        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n    ]\n    bounds = [(1e-6, max_weight) for _ in range(n)]\n    x0 = np.ones(n) / n\n    \n    result = minimize(\n        risk_parity_objective, x0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints=constraints,\n        options={'maxiter': 1000, 'ftol': 1e-12}\n    )\n    \n    if result.success:\n        return pd.Series(result.x, index=cov_matrix.index)\n    else:\n        return pd.Series(1.0 / n, index=cov_matrix.index)\n\ndef rp_weight_fn(cross_section, cov_cache={}):\n    \"\"\"Wrapper for portfolio engine: risk parity.\"\"\"\n    month = cross_section['month_end'].iloc[0]\n    \n    if month not in cov_cache:\n        cov_matrix, tickers = estimate_covariance(monthly, month)\n        cov_cache[month] = (cov_matrix, tickers)\n    \n    cov_matrix, tickers = cov_cache[month]\n    if cov_matrix is None:\n        return pd.Series(1.0, index=cross_section.set_index('ticker').index)\n    \n    available = set(cross_section['ticker']) & set(tickers)\n    if len(available) < 10:\n        return pd.Series(1.0, index=cross_section.set_index('ticker').index)\n    \n    sub_cov = cov_matrix.loc[list(available), list(available)]\n    return risk_parity_weights(sub_cov)\n\nrp_returns = compute_portfolio_returns(\n    universe_mid, rp_weight_fn, rebal_freq='Q'\n)\nprint(f\"Risk Parity: mean ret = {rp_returns['port_return'].mean():.4f}, \"\n      f\"std = {rp_returns['port_return'].std():.4f}\")\n```\n:::\n\n\n### Maximum Diversification Portfolio\n\n@choueifaty2008towards define the diversification ratio as the ratio of weighted average volatility to portfolio volatility:\n\n$$\nDR(\\mathbf{w}) = \\frac{\\mathbf{w}'\\boldsymbol{\\sigma}}{\\sqrt{\\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}}}\n$$ {#eq-div-ratio}\n\nwhere $\\boldsymbol{\\sigma}$ is the vector of individual asset volatilities. The maximum diversification portfolio maximizes this ratio:\n\n::: {#max-diversification .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Implement maximum diversification portfolio\"}\ndef max_diversification_weights(cov_matrix, max_weight=0.05):\n    \"\"\"Maximize the diversification ratio.\"\"\"\n    n = cov_matrix.shape[0]\n    Sigma = cov_matrix.values\n    sigma = np.sqrt(np.diag(Sigma))\n    \n    def neg_div_ratio(w):\n        port_vol = np.sqrt(w @ Sigma @ w)\n        if port_vol < 1e-10:\n            return 0\n        return -(w @ sigma) / port_vol\n    \n    constraints = [\n        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n    ]\n    bounds = [(0, max_weight) for _ in range(n)]\n    x0 = np.ones(n) / n\n    \n    result = minimize(\n        neg_div_ratio, x0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints=constraints,\n        options={'maxiter': 1000, 'ftol': 1e-12}\n    )\n    \n    if result.success:\n        return pd.Series(result.x, index=cov_matrix.index)\n    else:\n        return pd.Series(1.0 / n, index=cov_matrix.index)\n```\n:::\n\n\n## Comprehensive Performance Comparison {#sec-weight-comparison}\n\nWe now compare all schemes on a common universe with consistent methodology.\n\n::: {#all-portfolios .cell execution_count=14}\n``` {.python .cell-code code-summary=\"Compute returns for all weighting schemes\"}\n# Collect all portfolio return series\nportfolios = {\n    'VW': vw_returns,\n    'EW (Monthly)': ew_monthly,\n    'EW (Quarterly)': ew_quarterly,\n    'EW (Annual)': ew_annual,\n    'Capped VW (5%)': capped5,\n    'Capped VW (10%)': capped10,\n    'Fundamental': fw_returns,\n    'Min Variance': mv_returns,\n    'Risk Parity': rp_returns,\n}\n\n# Align to common date range\ncommon_start = max(df['month_end'].min() for df in portfolios.values())\ncommon_end = min(df['month_end'].max() for df in portfolios.values())\n\nfor name in portfolios:\n    portfolios[name] = portfolios[name][\n        (portfolios[name]['month_end'] >= common_start) &\n        (portfolios[name]['month_end'] <= common_end)\n    ].copy()\n\nprint(f\"Common period: {common_start} to {common_end}\")\nprint(f\"Number of months: {len(portfolios['VW'])}\")\n```\n:::\n\n\n### Performance Metrics\n\n::: {#performance-metrics .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Compute comprehensive performance statistics for all schemes\"}\ndef compute_metrics(returns_df, risk_free_annual=0.04):\n    \"\"\"Compute performance metrics from monthly portfolio returns.\"\"\"\n    r = returns_df['port_return']\n    rf_monthly = (1 + risk_free_annual) ** (1/12) - 1\n    \n    excess = r - rf_monthly\n    n_months = len(r)\n    \n    # Annualized return\n    cum_ret = (1 + r).prod()\n    ann_ret = cum_ret ** (12 / n_months) - 1\n    \n    # Annualized volatility\n    ann_vol = r.std() * np.sqrt(12)\n    \n    # Sharpe ratio\n    sharpe = excess.mean() / excess.std() * np.sqrt(12) if excess.std() > 0 else 0\n    \n    # Maximum drawdown\n    cum = (1 + r).cumprod()\n    running_max = cum.cummax()\n    drawdown = (cum - running_max) / running_max\n    max_dd = drawdown.min()\n    \n    # Sortino ratio\n    downside = excess[excess < 0]\n    downside_vol = np.sqrt((downside ** 2).mean()) * np.sqrt(12)\n    sortino = excess.mean() * 12 / downside_vol if downside_vol > 0 else 0\n    \n    # Calmar ratio\n    calmar = ann_ret / abs(max_dd) if max_dd != 0 else 0\n    \n    # Average turnover and effective N\n    avg_turnover = returns_df['turnover'].mean()\n    avg_eff_n = returns_df['effective_n'].mean()\n    \n    # Skewness and kurtosis\n    skew = r.skew()\n    kurt = r.kurtosis()\n    \n    return {\n        'Ann. Return': ann_ret,\n        'Ann. Volatility': ann_vol,\n        'Sharpe Ratio': sharpe,\n        'Sortino Ratio': sortino,\n        'Max Drawdown': max_dd,\n        'Calmar Ratio': calmar,\n        'Skewness': skew,\n        'Kurtosis': kurt,\n        'Avg. Turnover': avg_turnover,\n        'Effective N': avg_eff_n\n    }\n\n# Compute metrics for all portfolios\nmetrics_list = []\nfor name, df in portfolios.items():\n    m = compute_metrics(df)\n    m['Portfolio'] = name\n    metrics_list.append(m)\n\nmetrics_df = pd.DataFrame(metrics_list).set_index('Portfolio')\n\n# Format for display\ndisplay_cols = [\n    'Ann. Return', 'Ann. Volatility', 'Sharpe Ratio', 'Sortino Ratio',\n    'Max Drawdown', 'Avg. Turnover', 'Effective N'\n]\nprint(metrics_df[display_cols].round(3).to_string())\n```\n:::\n\n\n::: {#fig-cumulative-returns .cell execution_count=16}\n``` {.python .cell-code code-summary=\"Plot cumulative returns for all weighting schemes\"}\nfig, ax = plt.subplots(figsize=(12, 7))\n\ncolors = {\n    'VW': '#2C5F8A', 'EW (Monthly)': '#E67E22',\n    'EW (Quarterly)': '#F39C12', 'EW (Annual)': '#D4AC0D',\n    'Capped VW (5%)': '#8E44AD', 'Capped VW (10%)': '#9B59B6',\n    'Fundamental': '#27AE60', 'Min Variance': '#C0392B',\n    'Risk Parity': '#1ABC9C'\n}\nlinestyles = {\n    'VW': '-', 'EW (Monthly)': '-', 'EW (Quarterly)': '--',\n    'EW (Annual)': ':', 'Capped VW (5%)': '-',\n    'Capped VW (10%)': '--', 'Fundamental': '-',\n    'Min Variance': '-', 'Risk Parity': '-'\n}\n\nfor name, df in portfolios.items():\n    cum = (1 + df.set_index('month_end')['port_return']).cumprod()\n    ax.plot(cum.index, cum.values, label=name,\n            color=colors.get(name, 'gray'),\n            linestyle=linestyles.get(name, '-'),\n            linewidth=1.8 if name in ['VW', 'EW (Monthly)', 'Min Variance'] else 1.2)\n\nax.set_xlabel('Date')\nax.set_ylabel('Cumulative Wealth (VND 1 invested)')\nax.set_title('Cumulative Performance by Weighting Scheme')\nax.legend(loc='upper left', fontsize=9, ncol=2)\nax.set_yscale('log')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Risk-Return Trade-Off\n\n::: {#fig-risk-return .cell execution_count=17}\n``` {.python .cell-code code-summary=\"Risk-return scatter plot\"}\nfig, ax = plt.subplots(figsize=(9, 7))\n\nfor name in metrics_df.index:\n    ax.scatter(\n        metrics_df.loc[name, 'Ann. Volatility'],\n        metrics_df.loc[name, 'Ann. Return'],\n        s=metrics_df.loc[name, 'Effective N'] * 3,\n        color=colors.get(name, 'gray'),\n        alpha=0.85, edgecolors='white', linewidth=1.5, zorder=5\n    )\n    ax.annotate(\n        name,\n        (metrics_df.loc[name, 'Ann. Volatility'] + 0.002,\n         metrics_df.loc[name, 'Ann. Return']),\n        fontsize=8\n    )\n\nax.set_xlabel('Annualized Volatility')\nax.set_ylabel('Annualized Return')\nax.set_title('Risk-Return Profile (bubble size = Effective N)')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Transaction Costs and Net-of-Cost Performance {#sec-weight-transaction-costs}\n\n### Estimating Trading Costs in Vietnam\n\nTransaction costs in Vietnam include explicit components (brokerage commissions, exchange fees, taxes) and implicit components (bid-ask spread, price impact). The explicit cost structure as of 2024 is approximately:\n\n| Component               | Rate       | Notes                            |\n|-------------------------|------------|----------------------------------|\n| Brokerage commission    | 0.15–0.35% | Varies by broker and volume tier |\n| Exchange & clearing fee | 0.003%     | Fixed by exchange                |\n| Selling tax             | 0.10%      | Levied on gross sale proceeds    |\n\n: Explicit transaction cost components for Vietnamese equities. {#tbl-weight-costs}\n\nThe total explicit round-trip cost (buy + sell) ranges from approximately 0.30% to 0.80%. Implicit costs—the spread and price impact—can be substantially larger for small and illiquid stocks.\n\nWe model total transaction costs as a function of trade size and stock liquidity:\n\n$$\nTC_{i,t} = c_{\\text{fixed}} + \\frac{1}{2} \\text{Spread}_{i,t} + \\lambda \\sqrt{\\frac{|\\Delta w_{i,t}| \\cdot \\text{AUM}}{ADV_{i,t}}}\n$$ {#eq-tc-model}\n\nwhere $c_{\\text{fixed}} \\approx 0.25\\%$ is the explicit cost per trade, $\\text{Spread}_{i,t}$ is the quoted bid-ask spread, $\\Delta w_{i,t}$ is the weight change, $\\text{AUM}$ is portfolio size, $ADV_{i,t}$ is average daily volume in VND, and $\\lambda$ is the price impact coefficient estimated from the @amihud2002illiquidity model.\n\n::: {#transaction-cost-model .cell execution_count=18}\n``` {.python .cell-code code-summary=\"Implement transaction cost model calibrated to Vietnamese market\"}\ndef estimate_transaction_costs(weight_changes, stock_data,\n                                aum_vnd=100e9, fixed_cost=0.0025,\n                                impact_coef=0.10):\n    \"\"\"\n    Estimate total transaction costs for a rebalancing event.\n    \n    Parameters\n    ----------\n    weight_changes : Series indexed by ticker, absolute weight changes\n    stock_data : DataFrame with ticker, bid_ask_spread, turnover_value_avg_20d\n    aum_vnd : float, portfolio AUM in VND\n    fixed_cost : float, explicit cost per unit traded (one-way)\n    impact_coef : float, price impact coefficient (lambda)\n    \n    Returns\n    -------\n    total_cost : float, total TC as fraction of AUM\n    cost_detail : DataFrame with per-stock costs\n    \"\"\"\n    costs = []\n    \n    for ticker, dw in weight_changes.items():\n        if abs(dw) < 1e-6:\n            continue\n        \n        trade_vnd = abs(dw) * aum_vnd\n        \n        # Explicit cost (one-way)\n        explicit = fixed_cost * trade_vnd\n        \n        # Spread cost\n        stock_info = stock_data[stock_data['ticker'] == ticker]\n        if len(stock_info) > 0:\n            spread = stock_info['bid_ask_spread'].iloc[0]\n            adv = stock_info['turnover_value_avg_20d'].iloc[0]\n        else:\n            spread = 0.005  # Default 50 bps\n            adv = 1e9  # Default VND 1bn\n        \n        spread_cost = 0.5 * spread * trade_vnd\n        \n        # Price impact (square root model)\n        participation_rate = trade_vnd / max(adv, 1e6)\n        impact_cost = impact_coef * np.sqrt(participation_rate) * trade_vnd\n        \n        total = explicit + spread_cost + impact_cost\n        costs.append({\n            'ticker': ticker,\n            'weight_change': dw,\n            'trade_vnd': trade_vnd,\n            'explicit': explicit,\n            'spread': spread_cost,\n            'impact': impact_cost,\n            'total': total\n        })\n    \n    cost_df = pd.DataFrame(costs)\n    total_cost = cost_df['total'].sum() / aum_vnd if len(cost_df) > 0 else 0\n    \n    return total_cost, cost_df\n```\n:::\n\n\n### Net-of-Cost Performance\n\nWe apply the transaction cost model to compute net-of-cost returns for each weighting scheme at different assumed AUM levels. This is critical because strategies that appear attractive in gross terms may be unimplementable at scale due to the illiquidity of small-cap Vietnamese stocks.\n\n::: {#net-returns .cell execution_count=19}\n``` {.python .cell-code code-summary=\"Compute net-of-cost returns using simplified proportional cost model\"}\ndef compute_net_returns(portfolio_df, cost_per_turnover=0.005):\n    \"\"\"\n    Approximate net returns using a proportional cost model.\n    Net return = gross return - (turnover * cost_per_unit_turnover)\n    \"\"\"\n    df = portfolio_df.copy()\n    df['tc'] = df['turnover'] * cost_per_turnover\n    df['net_return'] = df['port_return'] - df['tc']\n    return df\n\n# Compute at different cost assumptions\ncost_scenarios = {\n    'Low (25 bps)': 0.0025,\n    'Medium (50 bps)': 0.005,\n    'High (100 bps)': 0.01\n}\n\nprint(\"Annualized Net Sharpe Ratios by Cost Scenario:\")\nprint(\"-\" * 70)\n\nfor cost_name, cost_rate in cost_scenarios.items():\n    row = {'Scenario': cost_name}\n    for port_name, port_df in portfolios.items():\n        net_df = compute_net_returns(port_df, cost_rate)\n        rf_monthly = (1.04) ** (1/12) - 1\n        excess = net_df['net_return'] - rf_monthly\n        sharpe = excess.mean() / excess.std() * np.sqrt(12) if excess.std() > 0 else 0\n        row[port_name] = round(sharpe, 3)\n    print(f\"{cost_name}:\")\n    for k, v in row.items():\n        if k != 'Scenario':\n            print(f\"  {k}: {v}\")\n    print()\n```\n:::\n\n\n::: {#fig-turnover-comparison .cell execution_count=20}\n``` {.python .cell-code code-summary=\"Compare turnover distributions across schemes\"}\nfig, ax = plt.subplots(figsize=(12, 5))\n\nturnover_data = {}\nfor name, df in portfolios.items():\n    turnover_data[name] = df['turnover'].values\n\npositions = range(len(turnover_data))\nbp = ax.boxplot(\n    turnover_data.values(),\n    positions=positions,\n    widths=0.6,\n    patch_artist=True,\n    showfliers=False,\n    medianprops={'color': 'black', 'linewidth': 1.5}\n)\n\nfor i, (patch, name) in enumerate(zip(bp['boxes'], turnover_data.keys())):\n    patch.set_facecolor(colors.get(name, 'gray'))\n    patch.set_alpha(0.7)\n\nax.set_xticks(positions)\nax.set_xticklabels(turnover_data.keys(), rotation=45, ha='right', fontsize=9)\nax.set_ylabel('Monthly Turnover (one-way)')\nax.set_title('Turnover Distribution by Weighting Scheme')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Cost Erosion at Scale\n\nThe relationship between portfolio AUM and implementable performance is non-linear because price impact costs grow with trade size. We simulate performance at different AUM levels:\n\n::: {#fig-aum-scaling .cell execution_count=21}\n``` {.python .cell-code code-summary=\"Simulate performance degradation with AUM for different schemes\"}\naum_levels = [10, 50, 100, 500, 1000, 5000]  # VND billions\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nselected_ports = ['VW', 'EW (Monthly)', 'Capped VW (5%)',\n                   'Min Variance', 'Risk Parity']\n\nfor name in selected_ports:\n    sharpes = []\n    df = portfolios[name]\n    \n    for aum in aum_levels:\n        # Cost scales with sqrt(AUM / ADV)\n        base_cost = 0.003  # Base cost at small AUM\n        scale_factor = np.sqrt(aum / 100)  # Normalized to VND 100bn\n        cost_rate = base_cost * scale_factor\n        \n        # VW is less affected (large-cap tilt)\n        if name == 'VW':\n            cost_rate *= 0.3\n        elif 'Capped' in name:\n            cost_rate *= 0.5\n        elif 'Min Variance' in name or 'Risk Parity' in name:\n            cost_rate *= 0.7\n        \n        net_df = compute_net_returns(df, cost_rate)\n        rf_m = (1.04) ** (1/12) - 1\n        excess = net_df['net_return'] - rf_m\n        s = excess.mean() / excess.std() * np.sqrt(12) if excess.std() > 0 else 0\n        sharpes.append(s)\n    \n    ax.plot(aum_levels, sharpes, marker='o', label=name,\n            color=colors.get(name, 'gray'), linewidth=2)\n\nax.set_xlabel('Portfolio AUM (VND Billion)')\nax.set_ylabel('Net Sharpe Ratio')\nax.set_title('Sharpe Ratio Degradation with AUM')\nax.set_xscale('log')\nax.legend(fontsize=9)\nax.axhline(y=0, color='gray', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Rebalancing Frequency Analysis {#sec-weight-rebalancing}\n\n### The Rebalancing Trade-Off\n\nRebalancing serves two purposes: (i) restoring target weights to maintain the desired risk profile, and (ii) harvesting the \"rebalancing bonus\"—the systematic profit from buying low and selling high that arises when weights are reset to targets in the presence of mean-reverting cross-sectional returns.\n\nThe trade-off is clear: more frequent rebalancing maintains tighter adherence to target weights and captures more of the rebalancing bonus, but incurs higher transaction costs. The optimal frequency depends on the magnitude of mean reversion (which determines the gross rebalancing bonus), the level of transaction costs, and the rate at which weights drift from targets.\n\n::: {#fig-rebal-frequency .cell execution_count=22}\n``` {.python .cell-code code-summary=\"Analyze rebalancing frequency impact on EW portfolio\"}\nfrequencies = {\n    'Monthly': 'M',\n    'Quarterly': 'Q',\n    'Semi-annual': 'Q',  # Approximate with every 6 months\n    'Annual': 'A'\n}\n\n# Recompute EW at each frequency\nfreq_results = {}\nfor freq_name, freq_code in frequencies.items():\n    freq_df = compute_portfolio_returns(\n        universe_mid, ew_weights, rebal_freq=freq_code\n    )\n    freq_results[freq_name] = freq_df\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Gross vs Net Sharpe\nfreq_names = list(freq_results.keys())\ngross_sharpes = []\nnet_sharpes = []\n\nfor fn in freq_names:\n    df = freq_results[fn]\n    rf_m = (1.04) ** (1/12) - 1\n    exc = df['port_return'] - rf_m\n    gross_sharpes.append(exc.mean() / exc.std() * np.sqrt(12))\n    \n    net_df = compute_net_returns(df, 0.005)\n    exc_net = net_df['net_return'] - rf_m\n    net_sharpes.append(exc_net.mean() / exc_net.std() * np.sqrt(12))\n\nx = range(len(freq_names))\naxes[0].bar([i - 0.15 for i in x], gross_sharpes, width=0.3,\n            color='#2C5F8A', alpha=0.85, label='Gross')\naxes[0].bar([i + 0.15 for i in x], net_sharpes, width=0.3,\n            color='#C0392B', alpha=0.85, label='Net (50 bps)')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(freq_names)\naxes[0].set_ylabel('Annualized Sharpe Ratio')\naxes[0].set_title('Panel A: Sharpe Ratio by Rebalancing Frequency')\naxes[0].legend()\n\n# Panel B: Average turnover\nturnovers = [freq_results[fn]['turnover'].mean() for fn in freq_names]\naxes[1].bar(x, turnovers, color='#E67E22', alpha=0.85)\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(freq_names)\naxes[1].set_ylabel('Average Monthly Turnover')\naxes[1].set_title('Panel B: Turnover by Rebalancing Frequency')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Threshold-Based Rebalancing\n\nAn alternative to calendar-based rebalancing is to rebalance only when portfolio weights drift beyond a tolerance band. This \"no-trade zone\" approach is motivated by @garleanu2013dynamic, who derive the optimal dynamic trading strategy under quadratic transaction costs and show that the optimal portfolio is a weighted average of the current holdings and the frictionless target.\n\n::: {#threshold-rebalancing .cell execution_count=23}\n``` {.python .cell-code code-summary=\"Implement threshold-based rebalancing for EW portfolio\"}\ndef compute_threshold_rebalanced(monthly_df, weight_fn,\n                                  threshold=0.01):\n    \"\"\"\n    Rebalance only when maximum weight deviation exceeds threshold.\n    \n    Parameters\n    ----------\n    threshold : float\n        Rebalance when max(|w_actual - w_target|) > threshold.\n    \"\"\"\n    months = sorted(monthly_df['month_end'].unique())\n    results = []\n    current_weights = None\n    rebalance_count = 0\n    \n    for month in months:\n        cs = monthly_df[monthly_df['month_end'] == month].copy()\n        cs = cs.dropna(subset=['monthly_return']).set_index('ticker')\n        \n        if len(cs) < 5:\n            continue\n        \n        target = weight_fn(cs.reset_index())\n        target = target / target.sum()\n        \n        if current_weights is None:\n            current_weights = target.copy()\n            rebalance_count += 1\n        else:\n            # Drift weights\n            current_weights = current_weights.reindex(cs.index, fill_value=0)\n            current_weights = current_weights * (1 + cs['monthly_return'])\n            total = current_weights.sum()\n            if total > 0:\n                current_weights = current_weights / total\n            \n            # Check if rebalancing needed\n            max_dev = (current_weights - target.reindex(\n                current_weights.index, fill_value=0\n            )).abs().max()\n            \n            if max_dev > threshold:\n                turnover = (current_weights - target.reindex(\n                    current_weights.index, fill_value=0\n                )).abs().sum() / 2\n                current_weights = target.reindex(cs.index, fill_value=0)\n                current_weights = current_weights / current_weights.sum()\n                rebalance_count += 1\n            else:\n                turnover = 0\n        \n        port_ret = (current_weights.reindex(cs.index, fill_value=0) *\n                    cs['monthly_return']).sum()\n        hhi = (current_weights ** 2).sum()\n        \n        results.append({\n            'month_end': month,\n            'port_return': port_ret,\n            'turnover': turnover,\n            'hhi': hhi,\n            'effective_n': 1/hhi if hhi > 0 else 0,\n            'n_stocks': (current_weights > 1e-6).sum()\n        })\n    \n    print(f\"Threshold {threshold:.1%}: rebalanced {rebalance_count} / \"\n          f\"{len(results)} months ({rebalance_count/len(results):.0%})\")\n    return pd.DataFrame(results)\n\n# Test different thresholds\nthresholds = [0.005, 0.01, 0.02, 0.05]\nthreshold_results = {}\nfor t in thresholds:\n    threshold_results[f'{t:.1%}'] = compute_threshold_rebalanced(\n        universe_mid, ew_weights, threshold=t\n    )\n```\n:::\n\n\n## The Rebalancing Bonus: Decomposition {#sec-weight-rebalancing-bonus}\n\nThe excess return of the rebalanced EW portfolio over a buy-and-hold EW portfolio (which starts equal-weighted but drifts) can be decomposed following @plyakha2021equal. Define $r_i$ as the return of stock $i$ over one period:\n\n$$\nR^{EW}_{\\text{rebal}} - R^{EW}_{\\text{drift}} \\approx \\frac{1}{2N} \\sum_{i=1}^N \\text{Var}(r_i) - \\frac{1}{2N^2}\\sum_{i}\\sum_{j}\\text{Cov}(r_i, r_j)\n$$ {#eq-rebal-bonus}\n\nThe first term captures the \"buy low, sell high\" effect from resetting weights after return dispersion. The second term is the cost of undoing covariance-induced drift. The rebalancing bonus is larger when cross-sectional return dispersion is high (which it is in Vietnam) and when pairwise correlations are low.\n\n::: {#rebalancing-bonus .cell execution_count=24}\n``` {.python .cell-code code-summary=\"Estimate the rebalancing bonus for Vietnamese equities\"}\n# Compute buy-and-hold EW portfolio (no rebalancing after initial equal weight)\nbh_returns = compute_portfolio_returns(\n    universe_mid, ew_weights, rebal_freq='A'  # Rebalance once per year only\n)\n\n# The rebalancing bonus is the difference\nbonus_df = pd.merge(\n    ew_monthly[['month_end', 'port_return']].rename(\n        columns={'port_return': 'rebal_return'}),\n    bh_returns[['month_end', 'port_return']].rename(\n        columns={'port_return': 'bh_return'}),\n    on='month_end'\n)\nbonus_df['bonus'] = bonus_df['rebal_return'] - bonus_df['bh_return']\n\n# Cross-sectional return dispersion\ndispersion = (\n    universe_mid\n    .groupby('month_end')['monthly_return']\n    .std()\n    .reset_index(name='cs_dispersion')\n)\nbonus_df = bonus_df.merge(dispersion, on='month_end')\n\nann_bonus = bonus_df['bonus'].mean() * 12\nprint(f\"Annualized rebalancing bonus (EW monthly vs annual): {ann_bonus:.4f}\")\nprint(f\"Mean cross-sectional dispersion: {bonus_df['cs_dispersion'].mean():.4f}\")\n```\n:::\n\n\n::: {#fig-rebal-bonus-time .cell execution_count=25}\n``` {.python .cell-code code-summary=\"Plot rebalancing bonus over time\"}\nfig, ax1 = plt.subplots(figsize=(12, 5))\n\nax1.bar(pd.to_datetime(bonus_df['month_end']),\n        bonus_df['bonus'] * 100,\n        color='#2C5F8A', alpha=0.6, width=25, label='Rebal. Bonus')\nax1.set_ylabel('Rebalancing Bonus (%)', color='#2C5F8A')\nax1.set_xlabel('Date')\n\nax2 = ax1.twinx()\nax2.plot(pd.to_datetime(bonus_df['month_end']),\n         bonus_df['cs_dispersion'],\n         color='#C0392B', linewidth=1.5, alpha=0.7,\n         label='CS Dispersion')\nax2.set_ylabel('Cross-Sectional Return Dispersion', color='#C0392B')\n\nax1.set_title('Rebalancing Bonus and Return Dispersion')\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Factor Exposure Analysis {#sec-weight-factor-exposure}\n\nDifferent weighting schemes induce different factor exposures, which may explain their return differences. We regress each portfolio's excess returns on the Vietnamese Fama-French factors:\n\n$$\nR_{p,t} - R_{f,t} = \\alpha_p + \\beta_p^{MKT}(R_{m,t} - R_{f,t}) + \\beta_p^{SMB} \\text{SMB}_t + \\beta_p^{HML} \\text{HML}_t + \\varepsilon_{p,t}\n$$ {#eq-factor-regression}\n\n::: {#factor-regressions .cell execution_count=26}\n``` {.python .cell-code code-summary=\"Regress portfolio returns on Fama-French-Carhart factors\"}\n# Retrieve Vietnamese factor returns from DataCore\nfactors = client.get_factor_returns(\n    market='vietnam',\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    factors=['mkt_excess', 'smb', 'hml', 'wml']\n)\n\n# Run factor regressions for each portfolio\nfactor_results = {}\nfor name, df in portfolios.items():\n    merged = pd.merge(\n        df[['month_end', 'port_return']],\n        factors,\n        on='month_end'\n    )\n    rf_m = (1.04) ** (1/12) - 1\n    merged['excess'] = merged['port_return'] - rf_m\n    \n    model = sm.OLS(\n        merged['excess'],\n        sm.add_constant(merged[['mkt_excess', 'smb', 'hml', 'wml']])\n    ).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n    \n    factor_results[name] = {\n        'Alpha (ann.)': model.params['const'] * 12,\n        'Alpha t-stat': model.tvalues['const'],\n        'MKT': model.params['mkt_excess'],\n        'SMB': model.params['smb'],\n        'HML': model.params['hml'],\n        'WML': model.params['wml'],\n        'R²': model.rsquared\n    }\n\nfactor_df = pd.DataFrame(factor_results).T\nprint(factor_df.round(3).to_string())\n```\n:::\n\n\n::: {#fig-factor-exposures .cell execution_count=27}\n``` {.python .cell-code code-summary=\"Heatmap of factor exposures across weighting schemes\"}\nfig, ax = plt.subplots(figsize=(10, 6))\n\nplot_data = factor_df[['MKT', 'SMB', 'HML', 'WML']].copy()\nim = ax.imshow(plot_data.values, cmap='RdBu_r', aspect='auto',\n               vmin=-1.5, vmax=1.5)\n\nax.set_xticks(range(len(plot_data.columns)))\nax.set_xticklabels(plot_data.columns, fontsize=10)\nax.set_yticks(range(len(plot_data.index)))\nax.set_yticklabels(plot_data.index, fontsize=9)\n\n# Add text annotations\nfor i in range(len(plot_data.index)):\n    for j in range(len(plot_data.columns)):\n        val = plot_data.values[i, j]\n        color = 'white' if abs(val) > 0.8 else 'black'\n        ax.text(j, i, f'{val:.2f}', ha='center', va='center',\n                color=color, fontsize=9)\n\nplt.colorbar(im, ax=ax, label='Factor Loading')\nax.set_title('Factor Exposures by Weighting Scheme')\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Practical Guidance for Vietnam {#sec-weight-practical}\n\nThe preceding analysis yields several practical recommendations for researchers and investors working with Vietnamese equities:\n\n**For academic factor research:** VW portfolios remain the default for asset pricing tests because they represent the investable opportunity set and avoid inflating alpha estimates with small-cap illiquidity premia. When EW portfolios are used (e.g., to give equal influence to each stock in cross-sectional sorts), researchers should report both VW and EW results and discuss the sensitivity. @fama2008dissecting follow this practice systematically.\n\n**For fund management:** The choice depends on AUM and mandate. At AUM below VND 500 billion, capped VW or fundamental weighting offers a practical compromise between diversification and implementability. At larger AUM, pure VW or sector-capped VW is more realistic. Risk parity and minimum variance are suitable for low-volatility mandates but require robust covariance estimation and quarterly rebalancing.\n\n**For index construction:** Vietnamese index providers (VN30, VNINDEX) use variants of capped VW. The analysis suggests that the cap level significantly affects the index's diversification properties and tracking error relative to the uncapped VW market. A 10% cap balances concentration reduction against turnover.\n\n**For transaction cost management:** In all schemes, the marginal benefit of rebalancing declines faster than the marginal cost as frequency increases beyond quarterly. Calendar-based quarterly rebalancing or threshold-based rebalancing (with a 1–2% tolerance band) provides the best cost-benefit trade-off in the Vietnamese market.\n\n## Summary {#sec-weight-summary}\n\n| Dimension | VW | EW | Capped VW | Fundamental | Min Var | Risk Parity |\n|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Turnover | Very low | High | Low | Low | Moderate | Moderate |\n| Concentration | High | None | Moderate | Moderate | Variable | Low |\n| Size tilt | Large | Small | Moderate | Large-mid | Low-vol | Mixed |\n| Data required | Prices | None | Prices | Accounting | Returns cov. | Returns cov. |\n| Scale sensitivity | Low | High | Low | Low | Moderate | Moderate |\n| Rebal. frequency | Passive | Monthly | Monthly/Quarterly | Annual | Quarterly | Quarterly |\n| Best use case | Benchmarks, large AUM | Cross-sectional tests | Index tracking | Long-term investing | Low-vol mandates | Balanced risk |\n\n: Summary comparison of weighting schemes for Vietnamese equities. {#tbl-weight-final-comparison}\n\nThe choice of weighting scheme is not merely a technical detail—it reflects a substantive economic decision about the relative importance of diversification, investability, and cost control. In the Vietnamese market, where the capitalization distribution is highly skewed and small-cap liquidity is thin, this choice has larger consequences than in developed markets. Researchers who report results under only one weighting scheme risk conclusions that are specific to that scheme rather than reflective of a genuine economic relationship.\n\n```{=html}\n<!-- ## Exercises {#sec-weight-exercises}\n\n1. **Inverse volatility weighting.** Implement a weighting scheme where $w_i \\propto 1/\\sigma_i$ (inverse of trailing 60-day volatility). Compare its performance to equal-weight and risk parity. How does it differ from minimum variance?\n\n2. **Turnover-constrained optimization.** Modify the minimum variance portfolio to include a turnover constraint: $\\sum_i |w_i - w_{i,\\text{prev}}| \\leq \\tau$ for a given turnover budget $\\tau$. Solve this as a quadratic program with linear constraints. How does the frontier of risk vs. turnover look?\n\n3. **Liquidity-weighted portfolios.** Construct a portfolio where weights are proportional to average daily turnover rather than market cap. What economic interpretation does this portfolio have? How does it compare to VW in terms of factor exposures?\n\n4. **VN30 replication.** The VN30 index comprises the 30 largest and most liquid stocks on HOSE with a 10% cap. Replicate its construction rules using DataCore.vn data and compare the tracking error of your replication against the official VN30 index.\n\n5. **Conditional rebalancing bonus.** Estimate the rebalancing bonus separately in bull markets (VN-Index return > 0) and bear markets (VN-Index return < 0). Is the bonus pro-cyclical or counter-cyclical? Relate your findings to the cross-sectional dispersion pattern in @fig-rebal-bonus-time.\n\n6. **Covariance estimation comparison.** Compare the out-of-sample performance of minimum variance portfolios using: (a) sample covariance, (b) Ledoit-Wolf linear shrinkage, (c) nonlinear shrinkage [@ledoit2017nonlinear], and (d) a single-factor covariance model. Which estimator produces the lowest realized portfolio variance?\n\n7. **Price limit effects on rebalancing.** Vietnamese stocks face daily price limits ($\\pm$ 7% HOSE, $\\pm$ 10% HNX). Simulate the impact of price limits on rebalancing execution: when a stock hits limit-up or limit-down, the rebalancing trade cannot execute. How does this partial execution affect realized portfolio weights and performance?\n\n8. **Foreign ownership constraints.** Many Vietnamese sectors have foreign ownership limits (e.g., 49% for most industries, 30% for banking). Incorporate these constraints into the optimization-based weighting schemes. How do binding foreign ownership limits affect the achievable Sharpe ratio for foreign investors? -->\n```\n\n",
    "supporting": [
      "14_portfolio_weighting_and_rebalancing_files/figure-pdf"
    ],
    "filters": []
  }
}