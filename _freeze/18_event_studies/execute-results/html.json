{
  "hash": "980ddf3b56bc4f2e9e6d7aafebc71452",
  "result": {
    "engine": "jupyter",
    "markdown": "# Event Studies in Finance\n\nEvent studies constitute one of the most enduring and widely deployed empirical methodologies in financial economics. At their core, event studies measure the impact of a specific event on the value of a firm by examining **abnormal security returns** around the time the event occurs. The methodology rests on a simple premise: if capital markets are informationally efficient, the effect of an event will be reflected immediately in security prices, and any deviation from \"normal\" expected returns can be attributed to the event itself.\n\nSince the pioneering work of @fama1969adjustment, who studied how stock prices adjust to new information around stock splits, event studies have become a cornerstone of empirical research across finance, accounting, economics, and law. @ball2013empirical demonstrated that accounting earnings announcements convey information to the market, a finding that launched decades of research in accounting and disclosure. The methodology has since been refined through contributions by @brown1980measuring and @brown1985using, who established the statistical properties of event study methods, and @mackinlay1997event codified best practices that remain standard today.\n\nThe breadth of applications is remarkable. Event studies have been used to examine the wealth effects of mergers and acquisitions [@jensen1983market; @andrade2001new], earnings announcements [@bernard1989post], dividend changes [@aharony1980quarterly], regulatory changes [@schwert1981using], executive turnover [@warner1988stock], and macroeconomic announcements [@flannery2002macroeconomic]. In law and economics, event studies serve as the primary tool for measuring damages in securities fraud litigation [@mitchell1993role] and assessing the impact of regulatory interventions [@binder1998event]. @kothari2007econometrics documented over 500 published event studies in the top five finance journals alone between 1974 and 2000.\n\n### Why Event Studies Matter\n\nThe enduring popularity of event studies stems from several compelling properties:\n\n-   **Direct measurement of economic significance.** Unlike regression-based approaches that estimate associations, event studies directly quantify the dollar impact of events on firm value. A cumulative abnormal return (CAR) of 3% for a firm with \\$10 billion market capitalization translates to \\$300 million in wealth creation, which is a tangible, economically meaningful magnitude.\n\n-   **Minimal maintained assumptions.** The methodology requires only semi-strong market efficiency (i.e., prices reflect publicly available information), a weaker assumption than many alternatives.\n\n-   **Statistical power.** Daily event studies have remarkable power to detect abnormal performance, even with modest sample sizes. @brown1985using demonstrated that the market model detects abnormal returns of 1% or more with high reliability using samples as small as 20 securities.\n\n-   **Versatility.** The basic framework accommodates events that are firm-specific or market-wide, anticipated or surprising, and can be adapted to various asset classes and market structures.\n\n------------------------------------------------------------------------\n\n## Literature Review and Methodological Evolution\n\n### The Classical Framework (1969-1985)\n\nThe modern event study traces its origins to @fama1969adjustment, hereafter FFJR, who examined monthly stock returns around 940 stock splits between 1927 and 1959. Their key innovation was the use of the **market model** to decompose returns into expected (normal) and unexpected (abnormal) components.\n\n@ball2013empirical independently developed a similar approach to study earnings announcements, establishing the information content of accounting data. It was a finding with profound implications for both the efficient markets hypothesis and the relevance of financial reporting.\n\n@brown1980measuring provided the first systematic analysis of event study methodology using simulation. Their study of monthly data established several important results: (i) the simple market model performs at least as well as more complex models, (ii) value-weighted market indices can lead to misspecification when the sample is tilted toward smaller firms, and (iii) the standard cross-sectional test has well-specified size under the null hypothesis. Their follow-up study [@brown1985using] extended the analysis to daily data, documenting the importance of non-normality in daily returns and the increased power of daily versus monthly studies.\n\n### Risk Model Refinements (1992-2015)\n\nThe advent of the **Fama-French three-factor model** [@Fama1993] represented a major advance in modeling expected returns. Adding size (SMB) and value (HML) factors to the market model improved the cross-sectional fit of expected returns considerably. @Carhart1997 augmented this with a momentum factor (UMD), yielding the four-factor model that became standard in event studies through the 2000s. @FamaFrench2015 subsequently introduced profitability (RMW) and investment (CMA) factors in their five-factor model.\n\nThe choice of risk model matters for event studies primarily in **long-horizon settings**. @kothari2007econometrics showed that for short-window studies (3-5 days), the market model and multi-factor models produce virtually identical results because the incremental factors explain very little daily return variation for individual firms. However, for event windows exceeding 20 trading days, model choice can materially affect inferences.\n\n### Testing for Abnormal Returns (1976-2010)\n\nThe statistical testing of abnormal returns has evolved considerably:\n\n| Test | Year | Key Property | Reference |\n|------------------|------------------|-------------------|------------------|\n| **Patell Z** | 1976 | Standardizes by estimation-period $\\sigma$; weights firms inversely by volatility | @patell1976corporate |\n| **Cross-Sectional** $t$ | 1980 | Allows event-induced variance change | @brown1980measuring |\n| **BMP** | 1991 | Robust to event-induced variance | @boehmer1991event |\n| **Corrado Rank** | 1989 | Non-parametric; robust to non-normality | @corrado1989nonparametric |\n| **Generalized Sign** | 1992 | Non-parametric; uses estimation-window baseline | @cowan1992nonparametric |\n| **Kolari-Pynnönen** | 2010 | Accounts for cross-sectional dependence | @kolari2010event |\n| **Skewness-Adjusted** | 1992 | Corrects for BHAR skewness | @hall1992removal |\n\n: Summary of major event study test statistics {#tbl-tests}\n\n### CARs versus BHARs\n\nCumulative abnormal returns (CARs) sum daily abnormal returns, while buy-and-hold abnormal returns (BHARs) compound returns and subtract the compounded benchmark. @barber1997detecting demonstrated that BHARs better capture the actual investor experience, since investors earn compound, not cumulative, returns. However, @fama1998market and @mitchell2000managerial showed that BHARs exhibit severe cross-sectional dependence and positive skewness. For **short event windows** (under 10 days), the difference between CARs and BHARs is negligible. For longer windows, both should be reported.\n\n### Emerging Market Considerations\n\nEvent studies in emerging markets face distinct challenges:\n\n-   **Thin trading.** Many emerging market securities trade infrequently, inducing bias in market model beta estimates. @scholes1977estimating and @dimson1979risk proposed corrections using leading and lagging market returns.\n\n-   **Factor availability.** While Fama-French factors are readily available for developed markets, emerging market factors must often be constructed locally.\n\n-   **Market microstructure.** Price limits ($\\pm$ 7% on HOSE, $\\pm$ 10% on HNX, $\\pm$ 15% on UPCOM in Vietnam), T+2 settlement, and the absence of short-selling affect the speed of price adjustment. Researchers should consider wider event windows to accommodate slower information incorporation [@bhattacharya2000event; @griffin2010market].\n\n## Mathematical Framework\n\nThis section presents the complete mathematical specification of the event study methodology. We follow the notation conventions of @campbell1998econometrics and @kothari2007econometrics.\n\n### Timeline and Windows\n\nThe event study timeline is defined relative to the event date, denoted $\\tau = 0$. All dates are measured in **trading days**:\n\n$$\n\\underbrace{T_0 + 1, \\ldots, T_1}_{\\text{Estimation Window (L₁ days)}} \\quad \\underbrace{\\quad}_{\\text{Gap (G days)}} \\quad \\underbrace{\\tau_1, \\ldots, 0, \\ldots, \\tau_2}_{\\text{Event Window (L₂ days)}}\n$$\n\nwhere:\n\n-   **Estimation window**: $L_1$ trading days over which the risk model parameters are estimated\n-   **Gap**: $G$ trading days separating estimation and event windows, preventing contamination by pre-event information leakage\n-   **Event window**: $L_2 = \\tau_2 - \\tau_1 + 1$ trading days centered around the event date\n\nFor example, with $L_1 = 150$, $G = 15$, $\\tau_1 = -10$, $\\tau_2 = +10$: the estimation window covers trading days $[-175, -25]$ relative to the event, and the event window covers $[-10, +10]$.\n\n### Normal Return Models\n\nLet $R_{it}$ denote the return on security $i$ on trading day $t$, $R_{ft}$ the risk-free rate, and $R_{mt}$ the market return. We implement six models:\n\n**Model 0: Market-Adjusted Returns.** Assumes $\\beta_i = 1$ and $\\alpha_i = 0$ for all firms: \n\n$$\nAR_{it}^{MA} = R_{it} - R_{mt}\n$$\n\n**Model 1: Market Model** [@Sharpe1964]: \n\n$$\nR_{it} = \\alpha_i + \\beta_i R_{mt} + \\varepsilon_{it}, \\quad E[\\varepsilon_{it}] = 0, \\quad \\text{Var}[\\varepsilon_{it}] = \\sigma^2_{\\varepsilon_i}\n$$\n\n$$\nAR_{it}^{MM} = R_{it} - \\hat{\\alpha}_i - \\hat{\\beta}_i R_{mt}\n$$\n\n**Model 2: Fama-French Three-Factor** [@Fama1993]: \n\n$$\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\varepsilon_{it}\n$$\n\n**Model 3: Carhart Four-Factor** [@Carhart1997]: \n\n$$\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\beta_{i,4} \\cdot UMD_t + \\varepsilon_{it}\n$$\n\n**Model 4: Fama-French Five-Factor** [@FamaFrench2015]: \n\n$$\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\beta_{i,4} \\cdot RMW_t + \\beta_{i,5} \\cdot CMA_t + \\varepsilon_{it}\n$$\n\n**Model 5: User-Specified Factor Model:** \n\n$$\nR_{it} - R_{ft} = \\alpha_i + \\sum_{k=1}^{K} \\beta_{i,k} F_{k,t} + \\varepsilon_{it}\n$$\n\n### Aggregation: CARs and BHARs\n\n**Cumulative Abnormal Returns** sum daily abnormal returns: \n\n$$\nCAR_i(\\tau_1, \\tau_2) = \\sum_{t=\\tau_1}^{\\tau_2} AR_{it}, \\qquad \\overline{CAR}(\\tau_1, \\tau_2) = \\frac{1}{N} \\sum_{i=1}^{N} CAR_i(\\tau_1, \\tau_2)\n$$\n\n**Buy-and-Hold Abnormal Returns** compound returns: \n\n$$\nBHAR_i(\\tau_1, \\tau_2) = \\prod_{t=\\tau_1}^{\\tau_2}(1 + R_{it}) - \\prod_{t=\\tau_1}^{\\tau_2}(1 + \\hat{E}[R_{it}])\n$$\n\n### Standardized Returns\n\nThe **standardized abnormal return** for firm $i$ on day $t$ is: \n\n$$\nSAR_{it} = \\frac{AR_{it}}{\\hat{\\sigma}_{\\varepsilon_i}}\n$$\n\nThe **standardized cumulative abnormal return** is: \n\n$$\nSCAR_i(\\tau_1, \\tau_2) = \\frac{CAR_i(\\tau_1, \\tau_2)}{\\hat{\\sigma}_{\\varepsilon_i} \\sqrt{L_2}}\n$$\n\n### Test Statistics\n\nLet $N$ denote the number of firm-event observations.\n\n**Test 1: Cross-Sectional** $t$-Test. Allows event-induced variance; assumes cross-sectional independence: \n\n$$\nt_{CS} = \\frac{\\overline{CAR}}{s_{CAR}/\\sqrt{N}}, \\quad s_{CAR} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(CAR_i - \\overline{CAR})^2}\n$$\n\n**Test 2: Patell Z-Test** [@patell1976corporate]. Weights firms inversely by volatility: \n\n$$\nZ_{Patell} = \\frac{\\sum_{i=1}^{N} SCAR_i}{\\sqrt{\\sum_{i=1}^{N} \\frac{K_i - 2}{K_i - 4}}}\n$$\n\n**Test 3: BMP Test** [@boehmer1991event]. Robust to event-induced variance: \n\n$$\nt_{BMP} = \\frac{\\overline{SCAR}}{s_{SCAR}/\\sqrt{N}}\n$$\n\n**Test 4: Kolari-Pynnönen Adjusted BMP** [@kolari2010event]. Accounts for cross-sectional dependence: \n\n$$\nt_{KP} = t_{BMP} \\times \\sqrt{\\frac{1}{1 + (N-1)\\bar{r}}}\n$$ \n\nwhere $\\bar{r}$ is the mean pairwise cross-correlation of estimation-period residuals.\n\n**Test 5: Generalized Sign Test** [@cowan1992nonparametric]: \n\n$$\nZ_{GSign} = \\frac{\\hat{p} - \\hat{p}_0}{\\sqrt{\\hat{p}_0(1-\\hat{p}_0)/N}}\n$$\n\n**Test 6: Sign Test:** \n\n$$\nZ_{Sign} = \\frac{N^{+} - 0.5N}{\\sqrt{0.25N}}\n$$\n\n**Test 7: Skewness-Adjusted** $t$-Test [@hall1992removal]: \n\n$$\nt_{SA} = \\sqrt{N}\\left(\\bar{z} + \\frac{1}{3}\\hat{\\gamma}\\bar{z}^2 + \\frac{1}{27}\\hat{\\gamma}^2\\bar{z}^3 + \\frac{1}{6N}\\hat{\\gamma}\\right)\n$$\n\n**Test 8: Wilcoxon Signed-Rank Test:** A non-parametric test of whether the median CAR differs from zero.\n\nThe table below summarizes the assumptions of each test:\n\n| Test | Event-Induced Variance | Cross-Sectional Independence | Normality |\n|------------------|:----------------:|:----------------:|:----------------:|\n| Cross-Sectional $t$ | Robust | Assumes | Assumes |\n| Patell Z | Assumes no change | Assumes | Assumes |\n| BMP | Robust | Assumes | Assumes |\n| Kolari-Pynnönen | Robust | Robust | Assumes |\n| Generalized Sign | Robust | Assumes | Robust |\n| Corrado Rank | Robust | Assumes | Robust |\n| Skewness-Adjusted | Robust | Assumes | Partially |\n| Wilcoxon | Robust | Assumes | Robust |\n\n: Assumption requirements for event study test statistics {#tbl-test-assumptions}\n\n## Python Implementation\n\n### Design Philosophy\n\nOur implementation follows these principles:\n\n1.  **Modularity**: Each component (calendar, estimation, AR computation, testing) is a separate function.\n2.  **Vectorization**: All operations use pandas/numpy for performance on large datasets.\n3.  **Configurability**: All parameters are user-configurable via a dataclass.\n4.  **Transparency**: Intermediate outputs are preserved for inspection.\n5.  **Production-ready**: Comprehensive input validation, missing data handling, and edge cases.\n\n### Setup and Imports\n\n::: {#setup .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Import required libraries\"}\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Tuple\nfrom enum import Enum\nimport warnings\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', '{:.6f}'.format)\nprint(\"All libraries loaded.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAll libraries loaded.\n```\n:::\n:::\n\n\n::: {#f7affd66 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_ff3_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff3_daily\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff5_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_daily\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff5_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n```\n:::\n\n\n::: {#c7a193e4 .cell execution_count=4}\n``` {.python .cell-code}\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n```\n:::\n\n\n### Configuration\n\n::: {#config .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Event study configuration\"}\nclass RiskModel(Enum):\n    \"\"\"Supported risk models for expected return computation.\"\"\"\n    MARKET_ADJ = \"market_adjusted\"\n    MARKET_MODEL = \"market_model\"\n    FF3 = \"ff3\"\n    CARHART = \"carhart\"\n    FF5 = \"ff5\"\n    CUSTOM = \"custom\"\n\n@dataclass\nclass EventStudyConfig:\n    \"\"\"Complete configuration for an event study.\n    \n    Attributes\n    ----------\n    estimation_window : int\n        Length of estimation period in trading days. Brown and Warner (1985)\n        suggest ≥100 days; MacKinlay (1997) recommends 120 as standard.\n    event_window_start : int\n        Start of event window relative to event date (e.g., -10).\n    event_window_end : int\n        End of event window relative to event date (e.g., +10).\n    gap : int\n        Trading days between estimation and event windows. Prevents\n        contamination from pre-event information leakage.\n    min_estimation_obs : int\n        Minimum non-missing returns required in estimation period.\n    risk_model : RiskModel\n        Risk model for computing expected returns.\n    custom_factors : list\n        Column names for user-specified factors (CUSTOM model only).\n    thin_trading_adj : str or None\n        None, 'scholes_williams', or 'dimson'.\n    dimson_lags : int\n        Number of leads/lags for Dimson (1979) correction.\n    \"\"\"\n    estimation_window: int = 150\n    event_window_start: int = -10\n    event_window_end: int = 10\n    gap: int = 15\n    min_estimation_obs: int = 120\n    risk_model: RiskModel = RiskModel.MARKET_MODEL\n    custom_factors: List[str] = field(default_factory=list)\n    thin_trading_adj: Optional[str] = None\n    dimson_lags: int = 1\n    \n    @property\n    def event_window_length(self) -> int:\n        return self.event_window_end - self.event_window_start + 1\n    \n    def validate(self):\n        assert self.estimation_window > 0\n        assert self.event_window_start <= self.event_window_end\n        assert self.gap >= 0\n        assert self.min_estimation_obs <= self.estimation_window\n        if self.risk_model == RiskModel.CUSTOM:\n            assert len(self.custom_factors) > 0\n        return True\n\n# Demonstrate\nconfig_demo = EventStudyConfig(\n    estimation_window=150, event_window_start=-10, event_window_end=10,\n    gap=15, min_estimation_obs=120, risk_model=RiskModel.FF3\n)\nconfig_demo.validate()\nprint(f\"Event window length: {config_demo.event_window_length} days\")\nprint(f\"Model: {config_demo.risk_model.value}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEvent window length: 21 days\nModel: ff3\n```\n:::\n:::\n\n\n### Step 1: Trading Calendar Construction\n\nA correct trading calendar is fundamental. It maps any event date to the exact calendar dates for the start/end of estimation and event windows, accounting for weekends, holidays, and non-trading days.\n\n::: {#calendar .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Build trading calendar\"}\ndef build_trading_calendar(trading_dates, config):\n    \"\"\"Build a trading calendar mapping event dates to window boundaries.\n    \n    For each potential event date, identifies the calendar dates for the\n    start/end of the estimation period and event window using only actual\n    trading days.\n    \n    Parameters\n    ----------\n    trading_dates : array-like\n        Sorted unique trading dates in the market.\n    config : EventStudyConfig\n    \n    Returns\n    -------\n    pd.DataFrame with columns: estper_beg, estper_end, evtwin_beg,\n        evtdate, evtwin_end, cal_index\n    \"\"\"\n    dates = pd.Series(sorted(pd.to_datetime(trading_dates).unique()))\n    n = len(dates)\n    \n    L1 = config.estimation_window\n    G = config.gap\n    s = config.event_window_start\n    L2 = config.event_window_length\n    \n    # Offsets (FIRSTOBS logic)\n    o0 = 0                      # estper_beg\n    o1 = L1 - 1                 # estper_end\n    o2 = L1 + G                 # evtwin_beg\n    o3 = L1 + G - s             # evtdate\n    o4 = L1 + G + L2 - 1        # evtwin_end\n    \n    max_offset = o4\n    valid = n - max_offset\n    if valid <= 0:\n        raise ValueError(f\"Need ≥{max_offset+1} trading dates, have {n}\")\n    \n    cal = pd.DataFrame({\n        'estper_beg': dates.iloc[o0:o0+valid].values,\n        'estper_end': dates.iloc[o1:o1+valid].values,\n        'evtwin_beg': dates.iloc[o2:o2+valid].values,\n        'evtdate':    dates.iloc[o3:o3+valid].values,\n        'evtwin_end': dates.iloc[o4:o4+valid].values,\n    })\n    cal['cal_index'] = range(1, len(cal)+1)\n    \n    # Validate window lengths using a sample row\n    idx = min(10, len(cal)-1)\n    row = cal.iloc[idx]\n    est_n = dates[(dates >= row['estper_beg']) & (dates <= row['estper_end'])].shape[0]\n    evt_n = dates[(dates >= row['evtwin_beg']) & (dates <= row['evtwin_end'])].shape[0]\n    assert est_n == L1, f\"Estimation window: {est_n} ≠ {L1}\"\n    assert evt_n == L2, f\"Event window: {evt_n} ≠ {L2}\"\n    \n    return cal\n\n# Demo\ndemo_dates = pd.bdate_range('2018-01-01', '2023-12-31', freq='B')\ndemo_cal = build_trading_calendar(demo_dates, config_demo)\nprint(f\"Calendar: {len(demo_cal)} potential event dates\")\nprint(demo_cal.head(3).to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCalendar: 1380 potential event dates\nestper_beg estper_end evtwin_beg    evtdate evtwin_end  cal_index\n2018-01-01 2018-07-27 2018-08-20 2018-09-03 2018-09-17          1\n2018-01-02 2018-07-30 2018-08-21 2018-09-04 2018-09-18          2\n2018-01-03 2018-07-31 2018-08-22 2018-09-05 2018-09-19          3\n```\n:::\n:::\n\n\n### Step 2: Event Date Alignment\n\nWhen an event occurs on a non-trading day, align to the **next** available trading day.\n\n::: {#alignment .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Align events to trading calendar\"}\ndef align_events(events, calendar, id_col='symbol', date_col='event_date'):\n    \"\"\"Align event dates to trading calendar.\n    \n    Non-trading-day events are shifted forward to the next trading day.\n    \n    Parameters\n    ----------\n    events : pd.DataFrame with [id_col, date_col] and optional 'group'\n    calendar : pd.DataFrame from build_trading_calendar()\n    \n    Returns\n    -------\n    pd.DataFrame with window boundaries for each firm-event\n    \"\"\"\n    events = events.copy()\n    events[date_col] = pd.to_datetime(events[date_col])\n    \n    cal_dates = calendar[['evtdate']].drop_duplicates().sort_values('evtdate')\n    \n    merged = pd.merge_asof(\n        events.sort_values(date_col),\n        cal_dates.rename(columns={'evtdate': 'aligned_date'}),\n        left_on=date_col, right_on='aligned_date',\n        direction='forward'\n    )\n    \n    result = merged.merge(calendar, left_on='aligned_date', right_on='evtdate', how='inner')\n    \n    shifted = (result[date_col] != result['evtdate']).sum()\n    if shifted > 0:\n        print(f\"  {shifted} event(s) shifted to next trading day\")\n    \n    result = result.rename(columns={date_col: 'original_date'})\n    result = result.drop_duplicates(subset=[id_col, 'evtdate'])\n    \n    return result\n```\n:::\n\n\n### Step 3: Data Extraction and Factor Merging\n\nExtract returns for each security-event across the full estimation + event window and merge risk factors. \n\n::: {#extraction .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Extract returns and merge factors\"}\ndef extract_returns(aligned_events, prices, factors, config,\n                    id_col='symbol', date_col='date', ret_col='ret',\n                    mkt_col='mkt_excess', rf_col='risk_free'):\n    \"\"\"Extract stock returns and merge risk factors for each event.\n    \n    For each security-event, retrieves daily returns from estper_beg\n    through evtwin_end and merges appropriate risk factors.\n    \"\"\"\n    prices = prices.copy()\n    factors = factors.copy()\n    prices[date_col] = pd.to_datetime(prices[date_col])\n    factors[date_col] = pd.to_datetime(factors[date_col])\n    \n    # Recover raw return from excess return if needed\n    if ret_col not in prices.columns and 'ret_excess' in prices.columns:\n        if rf_col in factors.columns:\n            prices = prices.merge(factors[[date_col, rf_col]].drop_duplicates(),\n                                  on=date_col, how='left')\n        prices[ret_col] = prices['ret_excess'] + prices[rf_col]\n    \n    # Factor columns based on model\n    model = config.risk_model\n    fac_cols = [mkt_col] if mkt_col in factors.columns else []\n    if rf_col in factors.columns:\n        fac_cols.append(rf_col)\n    \n    model_factors = {\n        RiskModel.FF3: ['smb', 'hml'],\n        RiskModel.CARHART: ['smb', 'hml', 'umd'],\n        RiskModel.FF5: ['smb', 'hml', 'rmw', 'cma'],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n    for f in model_factors.get(model, []):\n        if f in factors.columns:\n            fac_cols.append(f)\n    \n    fac_cols = list(set([date_col] + fac_cols))\n    \n    # Vectorized merge approach: join events with prices on id + date range\n    frames = []\n    for _, evt in aligned_events.iterrows():\n        mask = ((prices[id_col] == evt[id_col]) &\n                (prices[date_col] >= evt['estper_beg']) &\n                (prices[date_col] <= evt['evtwin_end']))\n        fd = prices.loc[mask, [id_col, date_col, ret_col]].copy()\n        if len(fd) == 0:\n            continue\n        fd['evtdate'] = evt['evtdate']\n        fd['estper_beg'] = evt['estper_beg']\n        fd['estper_end'] = evt['estper_end']\n        fd['evtwin_beg'] = evt['evtwin_beg']\n        fd['evtwin_end'] = evt['evtwin_end']\n        if 'group' in evt.index:\n            fd['group'] = evt['group']\n        frames.append(fd)\n    \n    if not frames:\n        raise ValueError(\"No return data found for any events\")\n    \n    result = pd.concat(frames, ignore_index=True)\n    result = result.merge(factors[fac_cols].drop_duplicates(), on=date_col, how='left')\n    \n    # Excess and market-adjusted returns\n    if rf_col in result.columns:\n        result['ret_excess'] = result[ret_col] - result[rf_col]\n    else:\n        result['ret_excess'] = result[ret_col]\n    if mkt_col in result.columns:\n        result['ret_mktadj'] = result['ret_excess'] - result[mkt_col]\n    \n    result = result.sort_values([id_col, 'evtdate', date_col]).reset_index(drop=True)\n    n_evts = result.groupby([id_col, 'evtdate']).ngroups\n    print(f\"  Extracted {len(result):,} obs for {n_evts} firm-events\")\n    return result\n```\n:::\n\n\n### Step 4: Risk Model Estimation\n\nEstimate risk model parameters over the estimation window. \n\n::: {#estimation .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Risk model estimation\"}\ndef estimate_model(\n    event_returns, config, id_col=\"symbol\", date_col=\"date\", ret_col=\"ret\"\n):\n    \"\"\"Estimate risk model parameters for each firm-event.\n\n    Runs OLS over the estimation window. Returns alpha, betas, sigma,\n    R^2, nobs, and residuals for cross-correlation computation.\n    \"\"\"\n    model = config.risk_model\n\n    # Define regression specification\n    dep_var_map = {\n        RiskModel.MARKET_ADJ: \"ret_mktadj\",\n        RiskModel.MARKET_MODEL: ret_col,\n        RiskModel.FF3: \"ret_excess\",\n        RiskModel.CARHART: \"ret_excess\",\n        RiskModel.FF5: \"ret_excess\",\n        RiskModel.CUSTOM: \"ret_excess\",\n    }\n    indep_var_map = {\n        RiskModel.MARKET_ADJ: [],\n        RiskModel.MARKET_MODEL: [\"mkt_excess\"],\n        RiskModel.FF3: [\"mkt_excess\", \"smb\", \"hml\"],\n        RiskModel.CARHART: [\"mkt_excess\", \"smb\", \"hml\", \"umd\"],\n        RiskModel.FF5: [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n\n    dep_var = dep_var_map[model]\n    indep_vars = indep_var_map[model]\n\n    est = event_returns[\n        (event_returns[date_col] >= event_returns[\"estper_beg\"])\n        & (event_returns[date_col] <= event_returns[\"estper_end\"])\n    ].copy()\n\n    params_list = []\n\n    for (firm, evtdate), grp in est.groupby([id_col, \"evtdate\"]):\n        valid = grp.dropna(subset=[dep_var] + indep_vars)\n        nobs = len(valid)\n        if nobs < config.min_estimation_obs:\n            continue\n\n        y = valid[dep_var].values\n\n        if len(indep_vars) == 0:\n            # Market-adjusted: intercept-only for variance\n            p = {\n                id_col: firm,\n                \"evtdate\": evtdate,\n                \"alpha\": y.mean(),\n                \"sigma\": y.std(ddof=1),\n                \"variance\": y.var(ddof=1),\n                \"nobs\": nobs,\n                \"r_squared\": 0.0,\n                \"_residuals\": y - y.mean(),\n            }\n        else:\n            X = sm.add_constant(valid[indep_vars].values)\n            res = sm.OLS(y, X).fit()\n            p = {\n                id_col: firm,\n                \"evtdate\": evtdate,\n                \"alpha\": res.params[0],\n                \"sigma\": np.sqrt(res.mse_resid),\n                \"variance\": res.mse_resid,\n                \"nobs\": nobs,\n                \"r_squared\": res.rsquared if np.isfinite(res.rsquared) else np.nan,\n                \"_residuals\": res.resid,\n            }\n            for j, var in enumerate(indep_vars):\n                p[f\"beta_{var}\"] = res.params[j + 1]\n\n        # Skip degenerate firms (zero or near-zero variance)\n        if p[\"sigma\"] < 1e-6:\n            continue\n\n        params_list.append(p)\n\n    if not params_list:\n        raise ValueError(\"No firm-events passed minimum observation filter\")\n\n    params_df = pd.DataFrame(params_list)\n    n_total = event_returns.groupby([id_col, \"evtdate\"]).ngroups\n    print(\n        f\"  Estimated {len(params_df)}/{n_total} firm-events \"\n        f\"(mean R^2 = {params_df['r_squared'].dropna().mean():.4f})\"\n    )\n    return params_df\n```\n:::\n\n\n### Step 5: Abnormal Return Computation\n\nCompute AR, CAR, BHAR, SAR, SCAR for each firm-event-date. \n\n::: {#abnormal-returns .cell execution_count=10}\n``` {.python .cell-code code-summary=\"Compute abnormal returns, CARs, BHARs\"}\ndef compute_abnormal_returns(\n    event_returns, params, config, id_col=\"symbol\", date_col=\"date\", ret_col=\"ret\"\n):\n    \"\"\"Compute abnormal returns and aggregate to CARs/BHARs.\n\n    Returns\n    -------\n    daily_ar : pd.DataFrame - daily AR/SAR/CAR/BHAR per firm-event-date\n    event_ar : pd.DataFrame - event-level CAR/BHAR/SCAR per firm-event\n    \"\"\"\n    model = config.risk_model\n\n    factor_map = {\n        RiskModel.MARKET_ADJ: [],\n        RiskModel.MARKET_MODEL: [\"mkt_excess\"],\n        RiskModel.FF3: [\"mkt_excess\", \"smb\", \"hml\"],\n        RiskModel.CARHART: [\"mkt_excess\", \"smb\", \"hml\", \"umd\"],\n        RiskModel.FF5: [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n    factor_cols = factor_map[model]\n\n    # Filter to event window\n    evt = event_returns[\n        (event_returns[date_col] >= event_returns[\"evtwin_beg\"])\n        & (event_returns[date_col] <= event_returns[\"evtwin_end\"])\n    ].copy()\n\n    # Merge params (drop residuals column for merge)\n    merge_cols = [c for c in params.columns if c != \"_residuals\"]\n    evt = evt.merge(params[merge_cols], on=[id_col, \"evtdate\"], how=\"inner\")\n\n    # Expected returns\n    if model == RiskModel.MARKET_ADJ:\n        evt[\"expected_ret\"] = evt.get(\"mkt_excess\", 0) + evt.get(\"risk_free\", 0)\n        evt[\"AR\"] = evt[ret_col] - evt[\"expected_ret\"]\n    else:\n        evt[\"expected_ret\"] = evt[\"alpha\"]\n        for fc in factor_cols:\n            bcol = f\"beta_{fc}\"\n            if bcol in evt.columns:\n                evt[\"expected_ret\"] += evt[bcol] * evt[fc]\n\n        if model == RiskModel.MARKET_MODEL:\n            evt[\"AR\"] = evt[ret_col] - evt[\"expected_ret\"]\n        else:\n            evt[\"AR\"] = evt[\"ret_excess\"] - evt[\"expected_ret\"]\n\n    evt[\"SAR\"] = evt[\"AR\"] / evt[\"sigma\"]\n    evt = evt.sort_values([id_col, \"evtdate\", date_col])\n\n    # Compute event time\n    all_dates = sorted(event_returns[date_col].unique())\n    d2i = {d: i for i, d in enumerate(all_dates)}\n    evt[\"evttime\"] = evt[date_col].map(d2i) - evt[\"evtdate\"].map(d2i)\n\n    # Cumulative measures per firm-event\n    daily_recs = []\n    event_recs = []\n\n    for (firm, evtdate), g in evt.groupby([id_col, \"evtdate\"]):\n        g = g.sort_values(date_col).copy()\n        nd = len(g)\n\n        g[\"CAR\"] = g[\"AR\"].cumsum()\n        g[\"cum_ret\"] = (1 + g[ret_col]).cumprod() - 1\n        g[\"cum_expected\"] = (1 + g[\"expected_ret\"]).cumprod() - 1\n        g[\"BHAR\"] = g[\"cum_ret\"] - g[\"cum_expected\"]\n        g[\"SCAR\"] = g[\"CAR\"] / (g[\"sigma\"].iloc[0] * np.sqrt(np.arange(1, nd + 1)))\n\n        daily_recs.append(g)\n\n        last = g.iloc[-1]\n        sigma = g[\"sigma\"].iloc[0]\n        nobs = g[\"nobs\"].iloc[0]\n\n        rec = {\n            id_col: firm,\n            \"evtdate\": evtdate,\n            \"CAR\": last[\"CAR\"],\n            \"BHAR\": last[\"BHAR\"],\n            \"cum_ret\": last[\"cum_ret\"],\n            \"SCAR\": last[\"CAR\"] / (sigma * np.sqrt(nd)),\n            \"sigma\": sigma,\n            \"variance\": g[\"variance\"].iloc[0],\n            \"nobs\": nobs,\n            \"n_event_days\": nd,\n            \"alpha\": g[\"alpha\"].iloc[0],\n            \"pat_scale\": (nobs - 2) / (nobs - 4) if nobs > 4 else np.nan,\n            \"pos_car\": int(last[\"CAR\"] > 0),\n        }\n\n        for fc in factor_cols:\n            bcol = f\"beta_{fc}\"\n            if bcol in g.columns:\n                rec[bcol] = g[bcol].iloc[0]\n        if \"group\" in g.columns:\n            rec[\"group\"] = g[\"group\"].iloc[0]\n\n        event_recs.append(rec)\n\n    daily_ar = pd.concat(daily_recs, ignore_index=True)\n    event_ar = pd.DataFrame(event_recs)\n\n    print(\n        f\"  {len(event_ar)} firm-events | Mean CAR: {event_ar['CAR'].mean():.6f} | \"\n        f\"Mean BHAR: {event_ar['BHAR'].mean():.6f} | \"\n        f\"% positive: {event_ar['pos_car'].mean():.1%}\"\n    )\n    return daily_ar, event_ar\n```\n:::\n\n\n### Step 6: Comprehensive Test Statistics\n\nEight tests covering parametric, non-parametric, and cross-correlation-robust approaches. \n\n::: {#tests .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Eight test statistics\"}\ndef compute_test_statistics(event_ar, params=None, group_col=None):\n    \"\"\"Compute comprehensive test statistics for abnormal returns.\n    \n    Implements 8 tests with varying assumptions about variance,\n    cross-dependence, and distributional form.\n    \"\"\"\n    def _stats(data, label=None):\n        N = len(data)\n        if N < 3:\n            return None\n        \n        cars = data['CAR'].values\n        bhars = data['BHAR'].values\n        scars = data['SCAR'].values\n        pos = data['pos_car'].values\n        \n        m_car, s_car = np.mean(cars), np.std(cars, ddof=1)\n        m_scar, s_scar = np.mean(scars), np.std(scars, ddof=1)\n        \n        r = {'group': label or 'All', 'N': N,\n             'mean_CAR': m_car, 'median_CAR': np.median(cars),\n             'std_CAR': s_car, 'mean_BHAR': np.mean(bhars),\n             'pct_positive': np.mean(pos)}\n        \n        # 1. Cross-sectional t\n        t1 = m_car / (s_car / np.sqrt(N)) if s_car > 0 else np.nan\n        r['t_CS'] = t1\n        r['p_CS'] = 2 * (1 - stats.t.cdf(abs(t1), N-1)) if np.isfinite(t1) else np.nan\n        \n        # 2. Patell Z\n        if 'pat_scale' in data.columns:\n            ps = data['pat_scale'].dropna().values\n            z2 = np.sum(scars[:len(ps)]) / np.sqrt(np.sum(ps)) if len(ps) > 0 else np.nan\n        else:\n            z2 = m_scar * np.sqrt(N)\n        r['Z_Patell'] = z2\n        r['p_Patell'] = 2*(1-stats.norm.cdf(abs(z2))) if np.isfinite(z2) else np.nan\n        \n        # 3. BMP\n        t3 = m_scar / (s_scar / np.sqrt(N)) if s_scar > 0 else np.nan\n        r['t_BMP'] = t3\n        r['p_BMP'] = 2*(1-stats.t.cdf(abs(t3), N-1)) if np.isfinite(t3) else np.nan\n        \n        # 4. Kolari-Pynnönen\n        rbar = 0.0\n        if params is not None and '_residuals' in params.columns:\n            resids = [row['_residuals'] for _, row in params.iterrows()\n                      if isinstance(row.get('_residuals'), np.ndarray)]\n            if len(resids) > 1:\n                ml = min(len(x) for x in resids)\n                aligned = np.column_stack([x[:ml] for x in resids])\n                cm = np.corrcoef(aligned.T)\n                np.fill_diagonal(cm, 0)\n                rbar = cm.sum() / (len(resids) * (len(resids)-1))\n        \n        adj = np.sqrt(1/(1+(N-1)*rbar)) if (1+(N-1)*rbar) > 0 else 1\n        t4 = t3 * adj if np.isfinite(t3) else np.nan\n        r['t_KP'] = t4\n        r['p_KP'] = 2*(1-stats.t.cdf(abs(t4), N-1)) if np.isfinite(t4) else np.nan\n        r['r_bar'] = rbar\n        \n        # 5. Generalized sign test\n        p_hat = np.mean(pos)\n        z5 = (p_hat - 0.5) / np.sqrt(0.25 / N)\n        r['Z_GSign'] = z5\n        r['p_GSign'] = 2*(1-stats.norm.cdf(abs(z5)))\n        \n        # 6. Sign test\n        r['Z_Sign'] = z5  # Same formula with p0=0.5\n        r['p_Sign'] = r['p_GSign']\n        \n        # 7. Skewness-adjusted t\n        if s_scar > 0:\n            zb = m_scar / s_scar\n            gam = stats.skew(scars)\n            t7 = np.sqrt(N) * (zb + gam*zb**2/3 + gam**2*zb**3/27 + gam/(6*N))\n            r['t_SkAdj'] = t7\n            r['p_SkAdj'] = 2*(1-stats.t.cdf(abs(t7), N-1)) if np.isfinite(t7) else np.nan\n        \n        # 8. Wilcoxon signed-rank\n        try:\n            w, pw = stats.wilcoxon(cars, alternative='two-sided')\n            r['W_Wilcoxon'] = w\n            r['p_Wilcoxon'] = pw\n        except:\n            r['W_Wilcoxon'] = r['p_Wilcoxon'] = np.nan\n        \n        return r\n    \n    results = [_stats(event_ar)]\n    if group_col and group_col in event_ar.columns:\n        for gv, gd in event_ar.groupby(group_col):\n            s = _stats(gd, label=gv)\n            if s:\n                results.append(s)\n    \n    return pd.DataFrame([r for r in results if r is not None])\n\n\ndef compute_daily_stats(daily_ar, id_col='symbol'):\n    \"\"\"Compute test statistics at each event time t.\"\"\"\n    rows = []\n    for t, g in daily_ar.groupby('evttime'):\n        n = g[id_col].nunique()\n        if n < 2:\n            continue\n        m_ar = g['AR'].mean()\n        s_ar = g['AR'].std(ddof=1)\n        t_ar = m_ar / (s_ar/np.sqrt(n)) if s_ar > 0 else np.nan\n        rows.append({'evttime': t, 'N': n, 'mean_AR': m_ar,\n                     'mean_CAR': g['CAR'].mean(), 'mean_BHAR': g['BHAR'].mean(),\n                     'mean_cum_ret': g.get('cum_ret', pd.Series()).mean(),\n                     't_AR': t_ar})\n    return pd.DataFrame(rows).sort_values('evttime')\n```\n:::\n\n\n### Step 7: Publication-Ready Visualization\n\n::: {#visualization .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Event study plots\"}\ndef plot_event_study(daily_stats, title=\"Cumulative Abnormal Returns Around Event Date\",\n                     figsize=(12, 7), save_path=None):\n    \"\"\"Publication-ready event study plot with CAR, BHAR, and daily AR panels.\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=figsize, height_ratios=[3, 1],\n                              gridspec_kw={'hspace': 0.05})\n    ds = daily_stats.sort_values('evttime')\n    t = ds['evttime'].values\n    \n    # Top: cumulative returns\n    ax = axes[0]\n    ax.plot(t, ds['mean_CAR']*100, color='#2166AC', lw=2.5, label='Mean CAR')\n    ax.plot(t, ds['mean_BHAR']*100, color='#B2182B', lw=2, ls='--', label='Mean BHAR')\n    if 'mean_cum_ret' in ds.columns:\n        ax.plot(t, ds['mean_cum_ret']*100, color='#666', lw=1.5, ls=':', \n                label='Mean Cum. Return', alpha=0.7)\n    ax.axvline(0, color='k', lw=0.8, alpha=0.5)\n    ax.axhline(0, color='k', lw=0.5, alpha=0.3)\n    ax.set_ylabel('Cumulative Return (%)', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.legend(loc='upper left', fontsize=10)\n    ax.grid(True, alpha=0.2)\n    ax.set_xticklabels([])\n    \n    # Bottom: daily AR bars\n    ax2 = axes[1]\n    colors = ['#2166AC' if v >= 0 else '#B2182B' for v in ds['mean_AR']]\n    ax2.bar(t, ds['mean_AR']*100, color=colors, alpha=0.7, width=0.8)\n    if 't_AR' in ds.columns:\n        sig = np.abs(ds['t_AR'].values) > 1.96\n        if sig.any():\n            ax2.scatter(t[sig], ds['mean_AR'].values[sig]*100, \n                       color='gold', s=40, marker='*', zorder=4, label='p<0.05')\n            ax2.legend(fontsize=9)\n    ax2.axvline(0, color='k', lw=0.8, alpha=0.5)\n    ax2.axhline(0, color='k', lw=0.5, alpha=0.3)\n    ax2.set_xlabel('Event Time (Trading Periods)', fontsize=12)\n    ax2.set_ylabel('Mean AR (%)', fontsize=10)\n    ax2.grid(True, alpha=0.2)\n    \n    for a in axes:\n        a.spines['top'].set_visible(False)\n        a.spines['right'].set_visible(False)\n    plt.tight_layout()\n    if save_path:\n        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n    return fig\n\n\ndef plot_car_distribution(event_ar, var='CAR', figsize=(12, 5)):\n    \"\"\"Cross-sectional distribution of CARs with histogram and QQ plot.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n    data = event_ar[var].dropna() * 100\n    \n    ax1.hist(data, bins=50, density=True, alpha=0.6, color='#2166AC', edgecolor='white')\n    ax1.axvline(data.mean(), color='k', ls='--', lw=1.5, \n                label=f'Mean={data.mean():.2f}%')\n    ax1.axvline(data.median(), color='gray', ls=':', lw=1.5,\n                label=f'Median={data.median():.2f}%')\n    ax1.set_xlabel(f'{var} (%)')\n    ax1.set_ylabel('Density')\n    ax1.set_title(f'Distribution of {var}', fontweight='bold')\n    ax1.legend()\n    ax1.spines['top'].set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    \n    # QQ plot\n    (osm, osr), (slope, intercept, r) = stats.probplot(data, dist='norm')\n    ax2.scatter(osm, osr, alpha=0.4, s=10, color='#2166AC')\n    ax2.plot(osm, slope*np.array(osm)+intercept, 'r--', lw=1)\n    ax2.set_xlabel('Theoretical Quantiles')\n    ax2.set_ylabel('Sample Quantiles')\n    ax2.set_title('Q-Q Plot (Normal)', fontweight='bold')\n    ax2.spines['top'].set_visible(False)\n    ax2.spines['right'].set_visible(False)\n    \n    plt.tight_layout()\n    return fig\n```\n:::\n\n\n### The Master Pipeline\n\nCombine all components into one function:\n\n::: {#master .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Complete event study pipeline\"}\ndef run_event_study(events, prices, factors, config,\n                    id_col='symbol', date_col='date', ret_col='ret',\n                    event_date_col='event_date', mkt_col='mkt_excess',\n                    rf_col='risk_free', group_col=None, verbose=True):\n    \"\"\"Run a complete event study from raw inputs to test statistics.\n    \n    This is the main entry point. Provide your events, price data,\n    factor data, and configuration—get back everything you need.\n    \n    Parameters\n    ----------\n    events : pd.DataFrame\n        Columns: [id_col, event_date_col], optional 'group'.\n    prices : pd.DataFrame\n        Daily returns: [id_col, date_col, ret_col or 'ret_excess', rf_col].\n    factors : pd.DataFrame\n        Factor returns: [date_col, mkt_col, 'smb', 'hml', ...].\n    config : EventStudyConfig\n    \n    Returns\n    -------\n    dict with keys: 'config', 'daily_ar', 'event_ar', 'daily_stats',\n        'test_stats', 'params'\n    \"\"\"\n    config.validate()\n    \n    if verbose:\n        print(f\"═══ Event Study: {config.risk_model.value} model ═══\")\n        print(f\"  Windows: estimation={config.estimation_window}, \"\n              f\"gap={config.gap}, event=({config.event_window_start},{config.event_window_end})\")\n        print(f\"  Min obs: {config.min_estimation_obs}\\n\")\n    \n    # 1. Trading calendar\n    if verbose: print(\"Step 1: Building trading calendar...\")\n    trading_dates = pd.Series(sorted(prices[date_col].unique()))\n    calendar = build_trading_calendar(trading_dates, config)\n    if verbose: print(f\"  {len(calendar)} potential event dates\\n\")\n    \n    # 2. Align events\n    if verbose: print(\"Step 2: Aligning events to trading calendar...\")\n    aligned = align_events(events, calendar, id_col, event_date_col)\n    if verbose: print(f\"  {len(aligned)} aligned events\\n\")\n    \n    # 3. Extract returns\n    if verbose: print(\"Step 3: Extracting returns and merging factors...\")\n    evt_rets = extract_returns(aligned, prices, factors, config,\n                               id_col, date_col, ret_col, mkt_col, rf_col)\n    if verbose: print()\n    \n    # 4. Estimate model\n    if verbose: print(\"Step 4: Estimating risk model parameters...\")\n    params = estimate_model(evt_rets, config, id_col, date_col, ret_col)\n    if verbose: print()\n    \n    # 5. Compute abnormal returns\n    if verbose: print(\"Step 5: Computing abnormal returns...\")\n    daily_ar, event_ar = compute_abnormal_returns(\n        evt_rets, params, config, id_col, date_col, ret_col)\n    if verbose: print()\n    \n    # 6. Test statistics\n    if verbose: print(\"Step 6: Computing test statistics...\")\n    test_stats = compute_test_statistics(event_ar, params, group_col)\n    daily_stats = compute_daily_stats(daily_ar, id_col)\n    if verbose:\n        print(f\"  Done.\\n\")\n        print(\"═══ Results Summary ═══\")\n        cols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 't_BMP', 'p_BMP', 't_KP', 'p_KP']\n        avail = [c for c in cols if c in test_stats.columns]\n        print(test_stats[avail].to_string(index=False))\n    \n    return {\n        'config': config,\n        'params': params,\n        'daily_ar': daily_ar,\n        'event_ar': event_ar,\n        'daily_stats': daily_stats,\n        'test_stats': test_stats,\n        'calendar': calendar,\n    }\n\nprint(\"Master pipeline ready.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMaster pipeline ready.\n```\n:::\n:::\n\n\n## Demonstration with Simulated Data\n\nSince we are building a general-purpose framework (the actual event data will be supplied later), we demonstrate the full pipeline with **realistic simulated data**.\n\n::: {#simulation .cell execution_count=14}\n``` {.python .cell-code code-summary=\"Generate realistic simulated market data\"}\nnp.random.seed(2024)\n\n# --- Simulated trading calendar (Vietnamese market: ~245 days/year) ---\ndates = pd.bdate_range('2019-01-01', '2023-12-31', freq='B')\n# Remove Tet + national holidays (simplified)\ntet_holidays = pd.to_datetime([\n    '2019-02-04','2019-02-05','2019-02-06','2019-02-07','2019-02-08',\n    '2020-01-23','2020-01-24','2020-01-27','2020-01-28','2020-01-29',\n    '2021-02-10','2021-02-11','2021-02-12','2021-02-15','2021-02-16',\n    '2022-01-31','2022-02-01','2022-02-02','2022-02-03','2022-02-04',\n    '2023-01-20','2023-01-23','2023-01-24','2023-01-25','2023-01-26',\n])\ndates = dates.difference(tet_holidays)\nT = len(dates)\n\n# --- Simulated factors (realistic Vietnamese market parameters) ---\nrf_daily = 0.04 / 252  # ~4% annual risk-free\nmkt_excess = np.random.normal(0.0003, 0.012, T)  # ~7.5% annual, ~19% vol\nsmb = np.random.normal(0.0001, 0.006, T)\nhml = np.random.normal(0.0001, 0.005, T)\nrmw = np.random.normal(0.00005, 0.004, T)\ncma = np.random.normal(0.00005, 0.004, T)\n\nfactors_sim = pd.DataFrame({\n    'date': dates, 'mkt_excess': mkt_excess, 'smb': smb, 'hml': hml,\n    'rmw': rmw, 'cma': cma, 'risk_free': rf_daily\n})\n\n# --- 100 simulated stocks ---\nn_stocks = 100\nsymbols = [f'SIM{i:03d}' for i in range(n_stocks)]\nbetas = np.random.uniform(0.5, 1.5, n_stocks)\nalphas = np.random.normal(0, 0.0002, n_stocks)\nidio_vols = np.random.uniform(0.015, 0.035, n_stocks)\n\nprice_rows = []\nfor i, sym in enumerate(symbols):\n    eps = np.random.normal(0, idio_vols[i], T)\n    rets = alphas[i] + betas[i] * mkt_excess + 0.3*smb + 0.2*hml + eps\n    for j in range(T):\n        price_rows.append({\n            'symbol': sym, 'date': dates[j], 'ret': rets[j],\n            'ret_excess': rets[j] - rf_daily,\n            'risk_free': rf_daily,\n            'mktcap': np.random.uniform(100, 5000),\n        })\n\nprices_sim = pd.DataFrame(price_rows)\n\n# --- Simulated events: 50 random firm-dates with KNOWN positive AR ---\nevent_indices = np.random.choice(range(250, T-50), 50, replace=False)\nevent_firms = np.random.choice(symbols, 50, replace=True)\nevent_dates_sim = [dates[i] for i in event_indices]\n\n# Inject abnormal returns on event date (2% positive shock)\nfor firm, edate in zip(event_firms, event_dates_sim):\n    mask = (prices_sim['symbol'] == firm) & (prices_sim['date'] == edate)\n    prices_sim.loc[mask, 'ret'] += 0.02\n    prices_sim.loc[mask, 'ret_excess'] += 0.02\n\nevents_sim = pd.DataFrame({\n    'symbol': event_firms,\n    'event_date': event_dates_sim,\n    'group': np.random.choice([1, 2], 50)\n})\n\nprint(f\"Simulated data: {n_stocks} stocks × {T} days = {len(prices_sim):,} obs\")\nprint(f\"Events: {len(events_sim)} firm-event pairs\")\nprint(f\"Injected abnormal return: +2% on event date\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimulated data: 100 stocks × 1279 days = 127,900 obs\nEvents: 50 firm-event pairs\nInjected abnormal return: +2% on event date\n```\n:::\n:::\n\n\n### Running the Full Pipeline\n\n::: {#run-pipeline .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Execute the event study\"}\nconfig = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\nresults = run_event_study(\n    events=events_sim,\n    prices=prices_sim,\n    factors=factors_sim,\n    config=config,\n    group_col='group'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n═══ Event Study: ff3 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  1094 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 9,300 obs for 50 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 50/50 firm-events (mean R^2 = 0.2368)\n\nStep 5: Computing abnormal returns...\n  50 firm-events | Mean CAR: 0.033009 | Mean BHAR: 0.032498 | % positive: 60.0%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\ngroup  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n  All 50  0.033009   0.032498      0.600000 2.288362 0.026468 2.157106 0.035929 2.161291 0.035587\n    1 20  0.030704   0.028326      0.650000 1.568676 0.133227 1.248382 0.227056 1.249320 0.226720\n    2 30  0.034545   0.035280      0.566667 1.688848 0.101975 1.734699 0.093413 1.736689 0.093055\n```\n:::\n:::\n\n\n### Visualizing Results\n\n::: {#cell-fig-car-dynamics .cell execution_count=16}\n``` {.python .cell-code}\nfig1 = plot_event_study(\n    results['daily_stats'],\n    title=\"Event Study: FF3 Model — Simulated Vietnamese Market\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Dynamics of cumulative abnormal returns (CARs) and buy-and-hold abnormal returns (BHARs) around the event date. The positive jump at t=0 reflects the injected 2% abnormal return.](18_event_studies_files/figure-html/fig-car-dynamics-output-1.png){#fig-car-dynamics width=963 height=602}\n:::\n:::\n\n\n::: {#cell-fig-car-distribution .cell execution_count=17}\n``` {.python .cell-code}\nfig2 = plot_car_distribution(results['event_ar'], 'CAR')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Cross-sectional distribution of cumulative abnormal returns. The rightward shift from zero and positive skewness are consistent with the injected positive event effect.](18_event_studies_files/figure-html/fig-car-distribution-output-1.png){#fig-car-distribution width=1142 height=470}\n:::\n:::\n\n\n### Complete Test Statistics\n\n::: {#tbl-test-results .cell tbl-cap='Event study test statistics for the full sample and by subgroup' execution_count=18}\n``` {.python .cell-code}\n# Format for display\nts = results['test_stats'].copy()\n\n# Select key columns\ndisplay_cols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 'Z_Patell', 'p_Patell',\n                't_BMP', 'p_BMP', 't_KP', 'p_KP',\n                'Z_GSign', 'p_GSign', 't_SkAdj', 'p_SkAdj']\navail = [c for c in display_cols if c in ts.columns]\ndisplay_df = ts[avail].copy()\n\n# Format\nfor c in display_df.columns:\n    if c in ['N']:\n        display_df[c] = display_df[c].astype(int)\n    elif c.startswith('p_'):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4f}' if pd.notna(x) else '')\n    elif c in ['mean_CAR', 'mean_BHAR']:\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4%}' if pd.notna(x) else '')\n    elif c == 'pct_positive':\n        display_df[c] = display_df[c].map(lambda x: f'{x:.1%}' if pd.notna(x) else '')\n    elif isinstance(display_df[c].iloc[0], (int, float, np.floating)):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(display_df.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ngroup  N mean_CAR mean_BHAR pct_positive  t_CS   p_CS Z_Patell p_Patell t_BMP  p_BMP  t_KP   p_KP Z_GSign p_GSign t_SkAdj p_SkAdj\n  All 50  3.3009%   3.2498%        60.0% 2.288 0.0265    1.832   0.0669 2.157 0.0359 2.161 0.0356   1.414  0.1573   2.074  0.0434\n    1 20  3.0704%   2.8326%        65.0% 1.569 0.1332    1.020   0.3075 1.248 0.2271 1.249 0.2267   1.342  0.1797   1.103  0.2836\n    2 30  3.4545%   3.5280%        56.7% 1.689 0.1020    1.532   0.1255 1.735 0.0934 1.737 0.0931   0.730  0.4652   1.727  0.0949\n```\n:::\n:::\n\n\n### Running Multiple Models for Robustness\n\nA key best practice is to report results across multiple risk models. If conclusions are robust across models, this strengthens the findings:\n\n::: {#multi-model .cell execution_count=19}\n``` {.python .cell-code code-summary=\"Run event study across all available models\"}\nmodels_to_run = [\n    (\"Market-Adjusted\", RiskModel.MARKET_ADJ),\n    (\"Market Model\", RiskModel.MARKET_MODEL),\n    (\"Fama-French 3\", RiskModel.FF3),\n    (\"Fama-French 5\", RiskModel.FF5),\n]\n\nrobustness = []\nfor name, mdl in models_to_run:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=-10, event_window_end=10,\n        gap=15, min_estimation_obs=120, risk_model=mdl\n    )\n    res = run_event_study(events_sim, prices_sim, factors_sim, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    robustness.append({\n        'Model': name,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.2f}\",\n        't (BMP)': f\"{full['t_BMP']:.2f}\",\n        't (KP)': f\"{full.get('t_KP', np.nan):.2f}\",\n    })\n\nrob_df = pd.DataFrame(robustness)\nprint(\"Robustness Across Risk Models:\")\nprint(rob_df.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.0000)\n  50 firm-events | Mean CAR: 0.029672 | Mean BHAR: 0.026468 | % positive: 62.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2198)\n  50 firm-events | Mean CAR: 0.033785 | Mean BHAR: 0.029974 | % positive: 62.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2368)\n  50 firm-events | Mean CAR: 0.033009 | Mean BHAR: 0.032498 | % positive: 60.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2516)\n  50 firm-events | Mean CAR: 0.036479 | Mean BHAR: 0.036000 | % positive: 64.0%\nRobustness Across Risk Models:\n          Model  N Mean CAR Mean BHAR % Positive t (CS) t (BMP) t (KP)\nMarket-Adjusted 50  2.9672%   2.6468%      62.0%   2.12    2.03   2.01\n   Market Model 50  3.3785%   2.9974%      62.0%   2.37    2.26   2.28\n  Fama-French 3 50  3.3009%   3.2498%      60.0%   2.29    2.16   2.16\n  Fama-French 5 50  3.6479%   3.6000%      64.0%   2.51    2.36   2.41\n```\n:::\n:::\n\n\n## How to Use This Framework with Your Data {#sec-usage}\n\n### Required Data Format\n\nTo run the event study on real Vietnamese market data, prepare three inputs:\n\n**1. Stock Returns** (`prices` DataFrame):\n\n| Column                | Description            | Example      |\n|-----------------------|------------------------|--------------|\n| `symbol`              | Stock ticker           | `'VNM'`      |\n| `date`                | Trading date           | `2023-06-15` |\n| `ret` or `ret_excess` | Daily return (decimal) | `0.0123`     |\n| `risk_free`           | Daily risk-free rate   | `0.000159`   |\n\n**2. Factor Returns** (`factors` DataFrame):\n\n| Column       | Description                |\n|--------------|----------------------------|\n| `date`       | Trading date               |\n| `mkt_excess` | Market excess return       |\n| `smb`        | Size factor (FF3/FF5)      |\n| `hml`        | Value factor (FF3/FF5)     |\n| `rmw`        | Profitability factor (FF5) |\n| `cma`        | Investment factor (FF5)    |\n| `risk_free`  | Risk-free rate             |\n\n**3. Event File** (`events` DataFrame):\n\n| Column       | Description         | Example      |\n|--------------|---------------------|--------------|\n| `symbol`     | Stock ticker        | `'VNM'`      |\n| `event_date` | Event date          | `2023-03-15` |\n| `group`      | (Optional) subgroup | `1`          |\n\n### Minimal Usage Example\n\n``` python\n# Load your data\nprices = pd.read_csv('prices_daily.csv', parse_dates=['date'])\nfactors = pd.read_csv('factors_ff3_daily.csv', parse_dates=['date'])\nevents = pd.read_csv('my_events.csv', parse_dates=['event_date'])\n\n# Configure\nconfig = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-5,\n    event_window_end=5,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\n# Run\nresults = run_event_study(events, prices, factors, config)\n\n# Access outputs\nresults['test_stats']    # Test statistics\nresults['event_ar']      # Firm-level CARs/BHARs\nresults['daily_ar']      # Daily abnormal returns\nresults['daily_stats']   # Event-time aggregates\n\n# Plot\nplot_event_study(results['daily_stats'], title=\"My Event Study\")\n```\n\n## Demonstration with Vietnamese Market Data\n\nWe now demonstrate the full event study pipeline using actual Vietnamese stock market data. The datasets available are:\n\n- `prices_daily`: `symbol`, `date`, `ret_excess`, `mktcap`, `mktcap_lag`, `risk_free`\n- `prices_monthly`: same structure\n- `factors_ff3_daily`: `date`, `smb`, `hml`, `mkt_excess`, `risk_free`\n- `factors_ff3_monthly` — monthly frequency version\n- `factors_ff5_daily`: `date`, `smb`, `hml`, `mkt_excess`, `risk_free`, `rmw`, `cma`\n- `factors_ff5_monthly`\n\nSince our data provides `ret_excess` rather than raw returns, we recover raw returns as $R_{it} = R^e_{it} + R_{f,t}$, and the market return as $R_{m,t} = R^e_{m,t} + R_{f,t}$. The `extract_event_returns()` function handles this automatically.\n\n### Loading the Data\n\n::: {#load-data .cell execution_count=20}\n``` {.python .cell-code code-summary=\"Load Vietnamese market data\"}\n# --- Recover raw returns ---\n# ret = ret_excess + risk_free\nprices_daily['ret'] = prices_daily['ret_excess'] + prices_daily['risk_free']\nprices_monthly['ret'] = prices_monthly['ret_excess'] + prices_monthly['risk_free']\n\n# --- Inspect the data ---\nprint(\"=\" * 70)\nprint(\"VIETNAMESE MARKET DATA SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"\\nprices_daily: {prices_daily.shape[0]:,} rows, \"\n      f\"{prices_daily['symbol'].nunique()} stocks, \"\n      f\"{prices_daily['date'].min().date()} to {prices_daily['date'].max().date()}\")\nprint(f\"prices_monthly: {prices_monthly.shape[0]:,} rows, \"\n      f\"{prices_monthly['symbol'].nunique()} stocks\")\nprint(f\"\\nfactors_ff3_daily: {factors_ff3_daily.shape[0]:,} trading days\")\nprint(f\"  Columns: {list(factors_ff3_daily.columns)}\")\nprint(f\"factors_ff5_daily: {factors_ff5_daily.shape[0]:,} trading days\")\nprint(f\"  Columns: {list(factors_ff5_daily.columns)}\")\nprint(f\"\\nSample daily returns:\")\nprint(prices_daily[['symbol', 'date', 'ret_excess', 'ret', 'risk_free', 'mktcap']]\n      .head(5).to_string(index=False))\nprint(f\"\\nSample daily factors:\")\nprint(factors_ff3_daily.head(5).to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n======================================================================\nVIETNAMESE MARKET DATA SUMMARY\n======================================================================\n\nprices_daily: 3,462,157 rows, 1459 stocks, 2010-01-05 to 2023-12-29\nprices_monthly: 165,499 rows, 1457 stocks\n\nfactors_ff3_daily: 3,126 trading days\n  Columns: ['date', 'smb', 'hml', 'mkt_excess', 'risk_free']\nfactors_ff5_daily: 3,126 trading days\n  Columns: ['date', 'smb', 'hml', 'rmw', 'cma', 'mkt_excess', 'risk_free']\n\nSample daily returns:\nsymbol       date  ret_excess      ret  risk_free     mktcap\n   A32 2018-10-24   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-25   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-26   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-29   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-30   -0.000159 0.000000   0.000159 176.120000\n\nSample daily factors:\n      date       smb       hml  mkt_excess  risk_free\n2011-07-01  0.008587  0.000967   -0.019862   0.000159\n2011-07-04  0.005099 -0.001099   -0.000633   0.000159\n2011-07-05 -0.009088  0.010152    0.013314   0.000159\n2011-07-06  0.004875 -0.003918   -0.008045   0.000159\n2011-07-07 -0.011239 -0.000584    0.003391   0.000159\n```\n:::\n:::\n\n\n### Creating Sample Events\n\nFor this demonstration, we create a sample event file. In practice, events would come from corporate announcements (earnings, M&A, dividends), regulatory changes, or other information shocks. Here we select 50 large-cap Vietnamese stocks and assign random event dates from the most recent two years of data to illustrate the pipeline mechanics.\n\n::: {#create-events .cell execution_count=21}\n``` {.python .cell-code code-summary=\"Create sample event file for demonstration\"}\nnp.random.seed(2024)\n\n# Select the 50 largest stocks by median market cap\nlargest = (prices_daily.groupby('symbol')['mktcap']\n           .median()\n           .nlargest(50)\n           .index.tolist())\n\n# Date range for events: last 2 years of data, with buffer for windows\ndate_range = prices_daily['date'].sort_values().unique()\nn_dates = len(date_range)\n# Events from the middle portion (need room for estimation + event windows)\nevent_eligible = date_range[int(n_dates * 0.3):int(n_dates * 0.85)]\n\n# Generate 50 random firm-event pairs\nevent_firms = np.random.choice(largest, 50, replace=True)\nevent_dates = np.random.choice(event_eligible, 50, replace=False)\n\nevents_demo = pd.DataFrame({\n    'symbol': event_firms,\n    'event_date': pd.to_datetime(event_dates),\n    'group': np.random.choice(['Group_A', 'Group_B'], 50)\n})\n\n# Remove any duplicate firm-date pairs\nevents_demo = events_demo.drop_duplicates(subset=['symbol', 'event_date'])\n\nprint(f\"Sample event file: {len(events_demo)} firm-event observations\")\nprint(f\"Unique firms: {events_demo['symbol'].nunique()}\")\nprint(f\"Date range: {events_demo['event_date'].min().date()} to \"\n      f\"{events_demo['event_date'].max().date()}\")\nprint(f\"\\nGroup distribution:\")\nprint(events_demo['group'].value_counts().to_string())\nprint(f\"\\nFirst 10 events:\")\nprint(events_demo.sort_values('event_date').head(10).to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSample event file: 50 firm-event observations\nUnique firms: 35\nDate range: 2014-06-25 to 2021-10-29\n\nGroup distribution:\ngroup\nGroup_B    26\nGroup_A    24\n\nFirst 10 events:\nsymbol event_date   group\n   MCH 2014-06-25 Group_B\n   SIP 2014-10-23 Group_B\n   VRE 2014-11-14 Group_B\n   QNS 2014-12-25 Group_A\n   FOX 2015-01-16 Group_B\n   THD 2015-01-26 Group_A\n   QNS 2015-02-12 Group_A\n   HNG 2015-05-07 Group_B\n   MML 2015-08-17 Group_B\n   ACV 2015-10-15 Group_A\n```\n:::\n:::\n\n\n### Daily Event Study: Fama-French 3-Factor Model\n\n::: {#run-daily-ff3 .cell execution_count=22}\n``` {.python .cell-code code-summary=\"Run daily event study with FF3 model\"}\nconfig_ff3 = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\nresults_ff3 = run_event_study(\n    events=events_demo,\n    prices=prices_daily,\n    factors=factors_ff3_daily,\n    config=config_ff3,\n    group_col='group'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n═══ Event Study: ff3 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  3308 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 5,421 obs for 30 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n\nStep 5: Computing abnormal returns...\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n    All 26  0.021513   0.024885      0.615385 0.774999 0.445609 0.726797 0.474101 0.699973 0.490407\nGroup_A 13  0.008601   0.006783      0.538462 0.211606 0.835966 0.577574 0.574229 0.567041 0.581138\nGroup_B 13  0.034425   0.042987      0.692308 0.879892 0.396197 0.437902 0.669236 0.429917 0.674875\n```\n:::\n:::\n\n\n### Visualizing Daily Results\n\n::: {#cell-fig-daily-car .cell execution_count=23}\n``` {.python .cell-code}\nfig1 = plot_event_study(\n    results_ff3['daily_stats'],\n    title=\"Event Study: Fama-French 3-Factor Model — Vietnamese Market (Daily)\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Cumulative abnormal returns around event dates for Vietnamese stocks using the Fama-French 3-factor model. The event window spans [-10, +10] trading days.](18_event_studies_files/figure-html/fig-daily-car-output-1.png){#fig-daily-car width=951 height=602}\n:::\n:::\n\n\n::: {#cell-fig-daily-dist .cell execution_count=24}\n``` {.python .cell-code}\nfig2 = plot_car_distribution(results_ff3['event_ar'], 'CAR')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Cross-sectional distribution of cumulative abnormal returns (CARs) across firm-events. The histogram and Q-Q plot assess normality assumptions underlying parametric tests.](18_event_studies_files/figure-html/fig-daily-dist-output-1.png){#fig-daily-dist width=1142 height=470}\n:::\n:::\n\n\n### Complete Test Statistics (Daily)\n\n::: {#tbl-daily-tests .cell tbl-cap='Event study test statistics for the full sample and by subgroup — Daily frequency, FF3 model' execution_count=25}\n``` {.python .cell-code}\nts = results_ff3['test_stats'].copy()\n\ndisplay_cols = ['group', 'N', 'mean_CAR', 'median_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 'Z_Patell', 'p_Patell',\n                't_BMP', 'p_BMP', 't_KP', 'p_KP',\n                'Z_GSign', 'p_GSign', 't_SkAdj', 'p_SkAdj',\n                'W_Wilcoxon', 'p_Wilcoxon']\navail = [c for c in display_cols if c in ts.columns]\ndisplay_df = ts[avail].copy()\n\nfor c in display_df.columns:\n    if c in ['N']:\n        display_df[c] = display_df[c].astype(int)\n    elif c == 'group':\n        continue\n    elif c.startswith('p_'):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4f}' if pd.notna(x) else '')\n    elif c in ['mean_CAR', 'median_CAR', 'mean_BHAR']:\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4%}' if pd.notna(x) else '')\n    elif c == 'pct_positive':\n        display_df[c] = display_df[c].map(lambda x: f'{x:.1%}' if pd.notna(x) else '')\n    elif isinstance(display_df[c].iloc[0], (int, float, np.floating)):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(display_df.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  group  N mean_CAR median_CAR mean_BHAR pct_positive  t_CS   p_CS Z_Patell p_Patell t_BMP  p_BMP  t_KP   p_KP Z_GSign p_GSign t_SkAdj p_SkAdj W_Wilcoxon p_Wilcoxon\n    All 26  2.1513%    1.5701%   2.4885%        61.5% 0.775 0.4456    0.961   0.3364 0.727 0.4741 0.700 0.4904   1.177  0.2393   0.738  0.4676    158.000     0.6710\nGroup_A 13  0.8601%    2.5330%   0.6783%        53.8% 0.212 0.8360    0.739   0.4596 0.578 0.5742 0.567 0.5811   0.277  0.7815   0.597  0.5618     45.000     1.0000\nGroup_B 13  3.4425%    1.4169%   4.2987%        69.2% 0.880 0.3962    0.620   0.5352 0.438 0.6692 0.430 0.6749   1.387  0.1655   0.444  0.6647     35.000     0.4973\n```\n:::\n:::\n\n\n### Robustness: Multiple Risk Models (Daily)\n\n::: {#tbl-robustness-daily .cell tbl-cap='Robustness of event study results across risk models — Daily frequency' execution_count=26}\n``` {.python .cell-code code-summary=\"Run event study across all available daily models\"}\nmodels_daily = [\n    (\"Market-Adjusted\",  RiskModel.MARKET_ADJ,    factors_ff3_daily),\n    (\"Market Model\",     RiskModel.MARKET_MODEL,   factors_ff3_daily),\n    (\"Fama-French 3\",    RiskModel.FF3,            factors_ff3_daily),\n    (\"Fama-French 5\",    RiskModel.FF5,            factors_ff5_daily),\n]\n\nrobustness_daily = []\nfor name, mdl, facs in models_daily:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=-10, event_window_end=10,\n        gap=15, min_estimation_obs=120, risk_model=mdl\n    )\n    res = run_event_study(events_demo, prices_daily, facs, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    robustness_daily.append({\n        'Model': name,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Median CAR': f\"{full['median_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.3f}\",\n        'p (CS)': f\"{full['p_CS']:.4f}\",\n        't (BMP)': f\"{full['t_BMP']:.3f}\",\n        'p (BMP)': f\"{full['p_BMP']:.4f}\",\n        't (KP)': f\"{full.get('t_KP', np.nan):.3f}\",\n        'p (KP)': f\"{full.get('p_KP', np.nan):.4f}\",\n    })\n\nrob_daily_df = pd.DataFrame(robustness_daily)\nprint(\"Robustness Across Risk Models (Daily Frequency)\")\nprint(\"=\" * 100)\nprint(rob_daily_df.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 28/30 firm-events (mean R^2 = 0.0000)\n  28 firm-events | Mean CAR: 0.035338 | Mean BHAR: 0.036936 | % positive: 50.0%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.1960)\n  26 firm-events | Mean CAR: 0.032107 | Mean BHAR: 0.033221 | % positive: 61.5%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2675)\n  26 firm-events | Mean CAR: 0.025684 | Mean BHAR: 0.028399 | % positive: 57.7%\nRobustness Across Risk Models (Daily Frequency)\n====================================================================================================\n          Model  N Mean CAR Median CAR Mean BHAR % Positive t (CS) p (CS) t (BMP) p (BMP) t (KP) p (KP)\nMarket-Adjusted 28  3.5338%    0.2610%   3.6936%      50.0%  1.390 0.1758   1.103  0.2798  1.062 0.2975\n   Market Model 26  3.2107%    0.6925%   3.3221%      61.5%  1.198 0.2422   1.146  0.2625  1.107 0.2789\n  Fama-French 3 26  2.1513%    1.5701%   2.4885%      61.5%  0.775 0.4456   0.727  0.4741  0.700 0.4904\n  Fama-French 5 26  2.5684%    2.3619%   2.8399%      57.7%  0.968 0.3422   0.987  0.3332  0.962 0.3451\n```\n:::\n:::\n\n\n### Robustness: Multiple Event Windows\n\nA key practice is to examine sensitivity to the event window specification:\n\n::: {#tbl-robustness-windows .cell tbl-cap='Sensitivity of results to event window specification' execution_count=27}\n``` {.python .cell-code code-summary=\"Test across different event window widths\"}\nwindows = [\n    (\"(-1, +1)\",  -1, 1),\n    (\"(-3, +3)\",  -3, 3),\n    (\"(-5, +5)\",  -5, 5),\n    (\"(-10, +10)\", -10, 10),\n    (\"(-1, +5)\",  -1, 5),\n    (\"(-5, +1)\",  -5, 1),\n    (\"(0, 0)\",     0, 0),\n]\n\nwindow_results = []\nfor label, ws, we in windows:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=ws, event_window_end=we,\n        gap=15, min_estimation_obs=120, risk_model=RiskModel.FF3\n    )\n    res = run_event_study(events_demo, prices_daily, factors_ff3_daily, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    window_results.append({\n        'Window': label,\n        'Days': we - ws + 1,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.3f}\",\n        't (BMP)': f\"{full['t_BMP']:.3f}\",\n        'p (BMP)': f\"{full['p_BMP']:.4f}\",\n    })\n\nwin_df = pd.DataFrame(window_results)\nprint(\"Sensitivity to Event Window Specification (FF3 Model)\")\nprint(\"=\" * 90)\nprint(win_df.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Extracted 4,899 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2147)\n  26 firm-events | Mean CAR: 0.004074 | Mean BHAR: 0.004648 | % positive: 50.0%\n  Extracted 5,015 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2155)\n  26 firm-events | Mean CAR: 0.003761 | Mean BHAR: 0.004327 | % positive: 42.3%\n  Extracted 5,131 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2217)\n  26 firm-events | Mean CAR: -0.001133 | Mean BHAR: 0.001027 | % positive: 42.3%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n  Extracted 5,019 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2147)\n  26 firm-events | Mean CAR: -0.005096 | Mean BHAR: -0.005148 | % positive: 42.3%\n  Extracted 5,011 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2217)\n  26 firm-events | Mean CAR: 0.008600 | Mean BHAR: 0.010441 | % positive: 46.2%\n  Extracted 4,841 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2150)\n  26 firm-events | Mean CAR: 0.000344 | Mean BHAR: 0.000502 | % positive: 46.2%\nSensitivity to Event Window Specification (FF3 Model)\n==========================================================================================\n    Window  Days  N Mean CAR Mean BHAR % Positive t (CS) t (BMP) p (BMP)\n  (-1, +1)     3 26  0.4074%   0.4648%      50.0%  0.456   0.601  0.5535\n  (-3, +3)     7 26  0.3761%   0.4327%      42.3%  0.198   0.361  0.7211\n  (-5, +5)    11 26 -0.1133%   0.1027%      42.3% -0.049   0.030  0.9761\n(-10, +10)    21 26  2.1513%   2.4885%      61.5%  0.775   0.727  0.4741\n  (-1, +5)     7 26 -0.5096%  -0.5148%      42.3% -0.385  -0.068  0.9460\n  (-5, +1)     7 26  0.8600%   1.0441%      46.2%  0.439   0.429  0.6715\n    (0, 0)     1 26  0.0344%   0.0502%      46.2%  0.064  -0.020  0.9840\n```\n:::\n:::\n\n\n### Monthly Event Study: Fama-French 3-Factor Model\n\nFor longer-horizon studies, monthly frequency is appropriate. Note that the estimation window is specified in months rather than days:\n\n::: {#run-monthly-ff3 .cell execution_count=28}\n``` {.python .cell-code code-summary=\"Run monthly event study with FF3 model\"}\n# Create monthly events aligned to the monthly data\n# Map daily event dates to the corresponding month-end\nevents_monthly = events_demo.copy()\nevents_monthly['event_date'] = events_monthly['event_date'].dt.to_period('M').dt.to_timestamp('M')\n\n# Use month-end dates from monthly prices\nmonthly_dates = prices_monthly['date'].sort_values().unique()\n\n# Filter events to dates present in monthly data\nevents_monthly = events_monthly[events_monthly['event_date'].isin(monthly_dates)]\nevents_monthly = events_monthly.drop_duplicates(subset=['symbol', 'event_date'])\n\nconfig_monthly = EventStudyConfig(\n    estimation_window=36,     # 36 months\n    event_window_start=-3,    # 3 months before\n    event_window_end=3,       # 3 months after\n    gap=3,                    # 3-month gap\n    min_estimation_obs=24,    # At least 24 months\n    risk_model=RiskModel.FF3\n)\n\nif len(events_monthly) > 0:\n    results_monthly = run_event_study(\n        events=events_monthly,\n        prices=prices_monthly,\n        factors=factors_ff3_monthly,\n        config=config_monthly,\n        group_col='group'\n    )\n    \n    print(\"\\n--- Monthly Test Statistics ---\")\n    ts_m = results_monthly['test_stats']\n    mcols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n             't_CS', 'p_CS', 't_BMP', 'p_BMP']\n    mavail = [c for c in mcols if c in ts_m.columns]\n    print(ts_m[mavail].to_string(index=False))\nelse:\n    print(\"No monthly events could be aligned. Skipping monthly study.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n═══ Event Study: ff3 model ═══\n  Windows: estimation=36, gap=3, event=(-3,3)\n  Min obs: 24\n\nStep 1: Building trading calendar...\n  122 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 1,036 obs for 33 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 18/33 firm-events (mean R^2 = 0.3218)\n\nStep 5: Computing abnormal returns...\n  18 firm-events | Mean CAR: -0.005257 | Mean BHAR: -0.014576 | % positive: 55.6%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive      t_CS     p_CS     t_BMP    p_BMP      t_KP     p_KP\n    All 18 -0.005257  -0.014576      0.555556 -0.081058 0.936342 -0.249547 0.805928 -0.320212 0.752709\nGroup_A  7 -0.141756  -0.135084      0.428571 -1.102110 0.312648 -1.357452 0.223472 -1.462577 0.193905\nGroup_B 11  0.081606   0.062111      0.636364  1.390317 0.194599  1.177372 0.266309  1.342593 0.209085\n\n--- Monthly Test Statistics ---\n  group  N  mean_CAR  mean_BHAR  pct_positive      t_CS     p_CS     t_BMP    p_BMP\n    All 18 -0.005257  -0.014576      0.555556 -0.081058 0.936342 -0.249547 0.805928\nGroup_A  7 -0.141756  -0.135084      0.428571 -1.102110 0.312648 -1.357452 0.223472\nGroup_B 11  0.081606   0.062111      0.636364  1.390317 0.194599  1.177372 0.266309\n```\n:::\n:::\n\n\n::: {#cell-fig-monthly-car .cell execution_count=29}\n``` {.python .cell-code}\nif len(events_monthly) > 0 and 'daily_stats' in results_monthly:\n    fig3 = plot_event_study(\n        results_monthly['daily_stats'],\n        title=\"Event Study: FF3 Model — Vietnamese Market (Monthly)\"\n    )\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Monthly cumulative abnormal returns around event dates. Wider windows capture slower information incorporation typical of emerging markets.](18_event_studies_files/figure-html/fig-monthly-car-output-1.png){#fig-monthly-car width=962 height=602}\n:::\n:::\n\n\n### Daily Event Study: Fama-French 5-Factor Model\n\n::: {#run-daily-ff5 .cell execution_count=30}\n``` {.python .cell-code code-summary=\"Run daily event study with FF5 model\"}\nconfig_ff5 = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF5\n)\n\nresults_ff5 = run_event_study(\n    events=events_demo,\n    prices=prices_daily,\n    factors=factors_ff5_daily,\n    config=config_ff5,\n    group_col='group'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n═══ Event Study: ff5 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  3308 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 5,421 obs for 30 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 26/30 firm-events (mean R^2 = 0.2675)\n\nStep 5: Computing abnormal returns...\n  26 firm-events | Mean CAR: 0.025684 | Mean BHAR: 0.028399 | % positive: 57.7%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n    All 26  0.025684   0.028399      0.576923 0.968332 0.342154 0.986788 0.333201 0.962252 0.345139\nGroup_A 13  0.015352   0.013489      0.461538 0.393655 0.700741 0.786232 0.446980 0.776663 0.452395\nGroup_B 13  0.036016   0.043309      0.692308 0.965100 0.353542 0.585915 0.568789 0.578785 0.573438\n```\n:::\n:::\n\n\n### Comparing FF3 vs FF5 Estimation Quality\n\n::: {#tbl-model-comparison .cell tbl-cap='Comparison of estimation quality between FF3 and FF5 models' execution_count=31}\n``` {.python .cell-code}\nparams_ff3 = results_ff3['params']\nparams_ff5 = results_ff5['params']\n\nprint(\"Model Estimation Diagnostics\")\nprint(\"=\" * 60)\nprint(f\"\\n{'Metric':<30} {'FF3':>12} {'FF5':>12}\")\nprint(\"-\" * 54)\nprint(f\"{'Firm-events estimated':<30} {len(params_ff3):>12} {len(params_ff5):>12}\")\nprint(f\"{'Mean R^2':<30} {params_ff3['r_squared'].mean():>12.4f} {params_ff5['r_squared'].mean():>12.4f}\")\nprint(f\"{'Median R^2':<30} {params_ff3['r_squared'].median():>12.4f} {params_ff5['r_squared'].median():>12.4f}\")\nprint(f\"{'Mean σ(ε)':<30} {params_ff3['sigma'].mean():>12.6f} {params_ff5['sigma'].mean():>12.6f}\")\nprint(f\"{'Mean |α|':<30} {params_ff3['alpha'].abs().mean():>12.6f} {params_ff5['alpha'].abs().mean():>12.6f}\")\nprint(f\"{'Mean β(MKT)':<30} {params_ff3['beta_mkt_excess'].mean():>12.4f} {params_ff5['beta_mkt_excess'].mean():>12.4f}\")\nif 'beta_smb' in params_ff3.columns:\n    print(f\"{'Mean β(SMB)':<30} {params_ff3['beta_smb'].mean():>12.4f} {params_ff5['beta_smb'].mean():>12.4f}\")\nif 'beta_hml' in params_ff3.columns:\n    print(f\"{'Mean β(HML)':<30} {params_ff3['beta_hml'].mean():>12.4f} {params_ff5['beta_hml'].mean():>12.4f}\")\nif 'beta_rmw' in params_ff5.columns:\n    print(f\"{'Mean β(RMW)':<30} {'—':>12} {params_ff5['beta_rmw'].mean():>12.4f}\")\nif 'beta_cma' in params_ff5.columns:\n    print(f\"{'Mean β(CMA)':<30} {'—':>12} {params_ff5['beta_cma'].mean():>12.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel Estimation Diagnostics\n============================================================\n\nMetric                                  FF3          FF5\n------------------------------------------------------\nFirm-events estimated                    26           26\nMean R^2                             0.2245       0.2675\nMedian R^2                           0.1943       0.2692\nMean σ(ε)                          0.021753     0.021351\nMean |α|                           0.001022     0.001130\nMean β(MKT)                          0.8867       0.9721\nMean β(SMB)                         -0.0434       0.0265\nMean β(HML)                          0.2489       0.1493\nMean β(RMW)                               —      -0.0934\nMean β(CMA)                               —       0.1070\n```\n:::\n:::\n\n\n### Event-Level Detail\n\n::: {#tbl-event-detail .cell tbl-cap='Event-level detail: CARs and BHARs for each firm-event (FF3 model)' execution_count=32}\n``` {.python .cell-code}\ndetail = results_ff3['event_ar'].copy()\ndetail_cols = ['symbol', 'evtdate', 'CAR', 'BHAR', 'SCAR', 'sigma',\n               'nobs', 'alpha', 'beta_mkt_excess']\ndetail_avail = [c for c in detail_cols if c in detail.columns]\ndetail_show = detail[detail_avail].copy()\ndetail_show['CAR'] = detail_show['CAR'].map(lambda x: f'{x:.4%}')\ndetail_show['BHAR'] = detail_show['BHAR'].map(lambda x: f'{x:.4%}')\ndetail_show['SCAR'] = detail_show['SCAR'].map(lambda x: f'{x:.3f}')\n\nprint(\"Event-Level Results (first 20 firm-events)\")\nprint(\"=\" * 100)\nprint(detail_show.head(20).to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEvent-Level Results (first 20 firm-events)\n====================================================================================================\nsymbol    evtdate       CAR      BHAR   SCAR    sigma  nobs     alpha  beta_mkt_excess\n   BVH 2016-10-20 -12.6456% -11.6522% -1.603 0.017219   150  0.001193         1.449182\n   DHG 2016-01-25  16.1151%  17.1149%  2.273 0.015472   150 -0.000224         0.754245\n   DNH 2019-10-14   1.7233%   1.8068%  0.104 0.036035   150 -0.000781         1.861996\n   DPM 2020-07-30  -5.9576%  -6.2025% -0.545 0.023873   150  0.001279         0.313932\n   FOX 2021-01-13   2.7478%   2.9194%  0.329 0.018243   150 -0.000696         0.148058\n   GAS 2020-02-11   0.5371%   0.7801%  0.100 0.011694   150  0.000501         1.851155\n   GEX 2020-08-20  11.9985%  13.5843%  1.197 0.021883   150  0.000944         1.583418\n   IDC 2018-10-01   1.3889%   0.5651%  0.094 0.032120   150 -0.000674         0.809305\n   MML 2021-10-29 -12.0139% -12.3759% -1.141 0.022985   150  0.002976         0.260588\n   MSN 2015-10-23  -5.7205%  -5.5645% -0.718 0.017375   150  0.001177         0.453951\n   PGV 2019-06-26  -7.5892%  -9.1366% -0.359 0.046165   150  0.000502         0.151377\n   PLX 2020-01-07   2.5330%   2.7847%  0.423 0.013067   150 -0.000176         0.917578\n   PLX 2020-06-01  -6.1517%  -6.0085% -0.739 0.018168   150 -0.000465         1.815178\n   POW 2020-12-23   7.7751%   9.1225%  1.130 0.015014   150 -0.000575         0.774423\n   PVD 2020-05-25   4.8827%   5.6674%  0.510 0.020875   150 -0.001522         1.316102\n   PVS 2017-08-07   2.1360%   2.1918%  0.297 0.015715   150 -0.000341         1.136273\n   QNS 2018-01-18   4.3001%   3.4011%  0.530 0.017703   150 -0.003716         0.128328\n   SAB 2017-08-24   6.0347%   6.7732%  0.748 0.017614   150  0.003284         2.123751\n   SNZ 2019-03-12  34.5230%  33.1105%  2.145 0.035121   150  0.000235        -1.015554\n   VCI 2020-01-20  -3.3191%  -2.9072% -0.458 0.015797   150  0.000335        -0.086268\n```\n:::\n:::\n\n\n### Daily Abnormal Return Dynamics\n\n::: {#tbl-daily-dynamics .cell tbl-cap='Daily dynamics of mean abnormal returns and test statistics within the event window' execution_count=33}\n``` {.python .cell-code}\nds = results_ff3['daily_stats'].copy()\nds_cols = ['evttime', 'N', 'mean_AR', 'mean_CAR', 'mean_BHAR', 't_AR_CS', 't_AR_BMP']\nds_avail = [c for c in ds_cols if c in ds.columns]\nds_show = ds[ds_avail].copy()\n\nfor c in ['mean_AR', 'mean_CAR', 'mean_BHAR']:\n    if c in ds_show.columns:\n        ds_show[c] = ds_show[c].map(lambda x: f'{x:.4%}')\nfor c in ['t_AR_CS', 't_AR_BMP']:\n    if c in ds_show.columns:\n        ds_show[c] = ds_show[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(\"Daily Event-Window Dynamics (FF3 Model)\")\nprint(\"=\" * 80)\nprint(ds_show.to_string(index=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaily Event-Window Dynamics (FF3 Model)\n================================================================================\n evttime  N  mean_AR mean_CAR mean_BHAR\n     -10 23  0.3571%  0.3571%   0.3729%\n      -9 23  0.0337%  0.3907%   0.4209%\n      -8 23 -0.1581%  0.2326%   0.2766%\n      -7 23  0.3691%  0.6018%   0.6581%\n      -6 23  1.3416%  1.9433%   2.0278%\n      -5 23  0.1509%  2.0942%   2.2313%\n      -4 23  0.5512%  2.6454%   2.9187%\n      -3 23 -0.4641%  2.1814%   2.4297%\n      -2 23  0.2412%  2.4225%   2.8028%\n      -1 23  0.5660%  2.9885%   3.3240%\n       0 23 -0.0281%  2.9604%   3.4926%\n       1 23 -0.2564%  2.7040%   3.1039%\n       2 23 -0.4421%  2.2619%   2.5764%\n       3 23  0.6337%  2.8956%   3.4659%\n       4 23 -0.8432%  2.0524%   2.4738%\n       5 23 -0.4174%  1.6350%   2.1339%\n       6 23 -0.2272%  1.4078%   1.8085%\n       7 23 -0.2085%  1.1993%   1.6819%\n       8 23  0.0893%  1.2886%   1.8609%\n       9 23  0.3307%  1.6193%   2.1658%\n      10 23  0.5320%  2.1513%   2.4885%\n```\n:::\n:::\n\n\n### Summary of Key Findings\n\n::: {#summary .cell execution_count=34}\n``` {.python .cell-code code-summary=\"Summarize key findings\"}\nprint(\"=\" * 70)\nprint(\"EVENT STUDY RESULTS SUMMARY\")\nprint(\"=\" * 70)\n\nff3_all = results_ff3['test_stats'][results_ff3['test_stats']['group'] == 'All'].iloc[0]\n\nprint(f\"\\nSample: {int(ff3_all['N'])} firm-event observations\")\nprint(f\"Frequency: Daily\")\nprint(f\"Primary Model: Fama-French 3-Factor\")\nprint(f\"Estimation Window: {config_ff3.estimation_window} trading days\")\nprint(f\"Event Window: ({config_ff3.event_window_start}, {config_ff3.event_window_end})\")\nprint(f\"Gap: {config_ff3.gap} trading days\")\nprint(f\"\\n--- Abnormal Return Measures ---\")\nprint(f\"Mean CAR({config_ff3.event_window_start},{config_ff3.event_window_end}): \"\n      f\"{ff3_all['mean_CAR']:.4%}\")\nprint(f\"Median CAR: {ff3_all['median_CAR']:.4%}\")\nprint(f\"Mean BHAR: {ff3_all['mean_BHAR']:.4%}\")\nprint(f\"Fraction positive CARs: {ff3_all['pct_positive']:.1%}\")\nprint(f\"\\n--- Statistical Significance ---\")\nprint(f\"Cross-Sectional t: {ff3_all['t_CS']:.3f} (p = {ff3_all['p_CS']:.4f})\")\nprint(f\"Patell Z: {ff3_all['Z_Patell']:.3f} (p = {ff3_all['p_Patell']:.4f})\")\nprint(f\"BMP t: {ff3_all['t_BMP']:.3f} (p = {ff3_all['p_BMP']:.4f})\")\nprint(f\"Kolari-Pynnönen t: {ff3_all['t_KP']:.3f} (p = {ff3_all['p_KP']:.4f})\")\nprint(f\"Generalized Sign Z: {ff3_all['Z_GSign']:.3f} (p = {ff3_all['p_GSign']:.4f})\")\n\nsig_005 = sum(1 for k in ['p_CS','p_Patell','p_BMP','p_KP','p_GSign','p_SkAdj','p_Wilcoxon']\n              if k in ff3_all and pd.notna(ff3_all[k]) and ff3_all[k] < 0.05)\ntotal_tests = sum(1 for k in ['p_CS','p_Patell','p_BMP','p_KP','p_GSign','p_SkAdj','p_Wilcoxon']\n                  if k in ff3_all and pd.notna(ff3_all[k]))\nprint(f\"\\n{sig_005}/{total_tests} tests significant at 5% level\")\n\n# Robustness note\nprint(f\"\\nRobustness: Results checked across {len(models_daily)} risk models \"\n      f\"and {len(windows)} event windows\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n======================================================================\nEVENT STUDY RESULTS SUMMARY\n======================================================================\n\nSample: 26 firm-event observations\nFrequency: Daily\nPrimary Model: Fama-French 3-Factor\nEstimation Window: 150 trading days\nEvent Window: (-10, 10)\nGap: 15 trading days\n\n--- Abnormal Return Measures ---\nMean CAR(-10,10): 2.1513%\nMedian CAR: 1.5701%\nMean BHAR: 2.4885%\nFraction positive CARs: 61.5%\n\n--- Statistical Significance ---\nCross-Sectional t: 0.775 (p = 0.4456)\nPatell Z: 0.961 (p = 0.3364)\nBMP t: 0.727 (p = 0.4741)\nKolari-Pynnönen t: 0.700 (p = 0.4904)\nGeneralized Sign Z: 1.177 (p = 0.2393)\n\n0/7 tests significant at 5% level\n\nRobustness: Results checked across 4 risk models and 7 event windows\n```\n:::\n:::\n\n\n## Practical Recommendations\n\nBased on the literature and our implementation experience:\n\n1.  **Estimation window**: Use 150 trading days (\\~7 months) for daily studies. This balances parameter precision against structural breaks. For monthly studies, 60 months is standard [@kothari2007econometrics].\n\n2.  **Gap**: 15 trading days is standard. Increase to 30 if information leakage is a concern.\n\n3.  **Event window**: Start with (-1, +1) for short-window tests, then expand to (-5, +5) and (-10, +10) for robustness. Report all windows.\n\n4.  **Model choice**: Always report market model as the baseline. Add FF3 or FF5 for robustness. For Vietnam, local factors are preferable to global factors.\n\n5.  **Test statistics**: Report at minimum: cross-sectional t (for ease of interpretation), BMP (robust to event-induced variance), and one non-parametric test (sign or Wilcoxon). Report Kolari-Pynnönen if events cluster in calendar time.\n\n6.  **Thin trading**: For Vietnamese small-caps, consider @dimson1979risk with 1 lead/lag or increase `min_estimation_obs` to filter out illiquid stocks.\n\n7.  **Multiple testing**: If testing multiple event windows or subgroups, apply Bonferroni or Holm corrections to control family-wise error rate.\n\n",
    "supporting": [
      "18_event_studies_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}