{
  "hash": "071c8b971c6a9caba1ae0aee7fbd119b",
  "result": {
    "engine": "jupyter",
    "markdown": "# Time-Series vs. Cross-Sectional Factor Tests\n\n::: callout-note\nIn this chapter, we implement and compare the two dominant approaches to testing asset pricing models: time-series regressions (do factors explain individual stock or portfolio returns?) and cross-sectional regressions (do factor exposures explain expected returns?), and develop the diagnostic tools to understand when and why they disagree.\n:::\n\nEvery factor model makes two distinct claims. The **time-series claim** is that a set of factors explains the common variation in individual stock returns (i.e., when the factors move, stocks move with them in proportion to their exposures). The **cross-sectional claim** is that differences in average returns across stocks are explained by differences in their factor exposures (i.e., stocks that load more heavily on a factor earn higher (or lower) average returns in proportion to the factor's risk premium).\n\nThese claims are logically related but empirically distinct. A factor can explain time-series variation without explaining the cross-section (if the factor's risk premium is zero, exposure to it creates no return differential). Conversely, a characteristic can predict the cross-section of returns without operating through a traded factor (if the characteristic captures mispricing rather than risk).\n\n@fama2020comparing formalize this distinction and show that the two approaches can give materially different answers about which factors matter. @goyal2018cross go further and demonstrate that time-series and cross-sectional tests can disagree about the same model (e.g., a factor can have a significant time-series alpha in one test and an insignificant cross-sectional premium in the other, or vice versa). Understanding *why* they disagree is essential for interpreting asset pricing evidence.\n\nThis chapter equips the reader with both toolkits, applied to Vietnamese data, and develops the intuition for when each is appropriate.\n\n## The Two Testing Frameworks {#sec-ts-cs-framework}\n\n### Time-Series Regressions\n\nThe time-series approach tests whether a factor model explains the returns of a set of test assets (typically portfolios). For each test asset $i$, estimate:\n\n$$\nR_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,1} f_{1,t} + \\beta_{i,2} f_{2,t} + \\ldots + \\beta_{i,K} f_{K,t} + \\varepsilon_{i,t}\n$$ {#eq-ts-regression}\n\nwhere $f_{k,t}$ are the $K$ factor returns. The intercept $\\alpha_i$ measures the average return of asset $i$ that is *not* explained by the factors. Under the null that the model correctly prices asset $i$, $\\alpha_i = 0$.\n\nThe joint test of $H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_N = 0$ across all $N$ test assets is performed using the @gibbons1989test (GRS) test statistic:\n\n$$\n\\text{GRS} = \\frac{T - N - K}{N} \\cdot \\frac{1}{1 + \\bar{f}' \\hat{\\Sigma}_f^{-1} \\bar{f}} \\cdot \\hat{\\alpha}' \\hat{\\Sigma}_\\varepsilon^{-1} \\hat{\\alpha} \\sim F(N, T - N - K)\n$$ {#eq-grs}\n\nwhere $\\bar{f}$ is the vector of factor means, $\\hat{\\Sigma}_f$ is the factor covariance matrix, and $\\hat{\\Sigma}_\\varepsilon$ is the residual covariance matrix. A large GRS statistic rejects the model.\n\n### Cross-Sectional Regressions (Fama-MacBeth)\n\nThe @fama1973risk cross-sectional approach tests whether factor exposures are priced (i.e., whether stocks with higher betas on a factor earn higher average returns). The procedure has two stages:\n\n**Stage 1 (Time-Series).** For each asset, estimate factor betas from a time-series regression over a rolling or full-sample window:\n\n$$\nR_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,1} f_{1,t} + \\ldots + \\beta_{i,K} f_{K,t} + \\varepsilon_{i,t}\n$$ {#eq-fm-stage1}\n\n**Stage 2 (Cross-Section).** Each month $t$, regress the cross-section of realized excess returns on the estimated betas:\n\n$$\nR_{i,t} - R_{f,t} = \\gamma_{0,t} + \\gamma_{1,t} \\hat{\\beta}_{i,1} + \\ldots + \\gamma_{K,t} \\hat{\\beta}_{i,K} + \\eta_{i,t}\n$$ {#eq-fm-stage2}\n\nThe time-series averages $\\bar{\\gamma}_k = \\frac{1}{T} \\sum_t \\gamma_{k,t}$ estimate the risk premia, and the standard errors use the time-series standard deviation of $\\{\\gamma_{k,t}\\}$.\n\n### Key Differences\n\n| Dimension | Time-Series | Cross-Section |\n|------------------------|------------------------|------------------------|\n| What it tests | Do factors explain return variation? | Do factor exposures explain average returns? |\n| Key statistic | Alpha (intercept) | Risk premium ($\\gamma$) |\n| Joint test | GRS F-test | Chi-squared on $\\gamma$'s |\n| Test assets | Portfolios (usually) | Individual stocks or portfolios |\n| Factors | Must be traded returns | Can be non-traded (macro, characteristics) |\n| Null hypothesis | $\\alpha_i = 0$ for all $i$ | $\\gamma_k$ equals factor mean return |\n| Errors-in-variables | Not an issue (factors observed) | Estimated betas create EIV bias |\n\n: Comparison of time-series and cross-sectional testing frameworks. {#tbl-ts-cs-comparison}\n\n## Data Construction {#sec-ts-cs-data}\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code code-summary=\"Import libraries and configure environment\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom scipy.linalg import inv\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n```\n:::\n\n\n::: {#data-load .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Load factor returns, test portfolios, and individual stock data\"}\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Factor returns\nfactors = client.get_factor_returns(\n    market='vietnam',\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    factors=['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml']\n)\nfactors['month_end'] = pd.to_datetime(factors['month_end'])\nfactors = factors.set_index('month_end')\n\n# Test portfolios: 25 size-BM portfolios (5x5 independent sorts)\ntest_portfolios_25 = client.get_sorted_portfolios(\n    market='vietnam',\n    sort_vars=['size', 'bm'],\n    n_groups=[5, 5],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    weighting='value'\n)\n\n# Test portfolios: 25 size-momentum portfolios\ntest_portfolios_mom = client.get_sorted_portfolios(\n    market='vietnam',\n    sort_vars=['size', 'momentum_12_2'],\n    n_groups=[5, 5],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    weighting='value'\n)\n\n# Test portfolios: 10 industry portfolios\ntest_portfolios_ind = client.get_sorted_portfolios(\n    market='vietnam',\n    sort_vars=['icb_sector'],\n    n_groups=[10],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    weighting='value'\n)\n\n# Monthly individual stock returns for Fama-MacBeth\nstock_returns = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=['ticker', 'month_end', 'monthly_return', 'market_cap']\n)\n\n# Risk-free rate\nrf = client.get_risk_free_rate(\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    frequency='monthly'\n)\n\nprint(f\"Factor months: {len(factors)}\")\nprint(f\"25 Size-BM portfolios: {test_portfolios_25.shape}\")\nprint(f\"25 Size-Mom portfolios: {test_portfolios_mom.shape}\")\nprint(f\"10 Industry portfolios: {test_portfolios_ind.shape}\")\nprint(f\"Stock-months: {len(stock_returns):,}\")\n```\n:::\n\n\n::: {#prepare-test-assets .cell execution_count=3}\n``` {.python .cell-code code-summary=\"Prepare excess return matrices for test portfolios\"}\n# Convert test portfolios to excess returns\n# Each portfolio set is a DataFrame: rows = months, columns = portfolios\ndef prepare_test_assets(port_df, rf_df):\n    \"\"\"Convert portfolio returns to excess returns matrix.\"\"\"\n    port = port_df.set_index('month_end')\n    rf_aligned = rf_df.set_index('month_end')['rf'].reindex(port.index)\n    excess = port.subtract(rf_aligned, axis=0)\n    return excess.dropna()\n\nexcess_25_bm = prepare_test_assets(test_portfolios_25, rf)\nexcess_25_mom = prepare_test_assets(test_portfolios_mom, rf)\nexcess_10_ind = prepare_test_assets(test_portfolios_ind, rf)\n\nprint(f\"Size-BM test assets: {excess_25_bm.shape[1]} portfolios, \"\n      f\"{excess_25_bm.shape[0]} months\")\nprint(f\"Size-Mom test assets: {excess_25_mom.shape[1]} portfolios, \"\n      f\"{excess_25_mom.shape[0]} months\")\nprint(f\"Industry test assets: {excess_10_ind.shape[1]} portfolios, \"\n      f\"{excess_10_ind.shape[0]} months\")\n```\n:::\n\n\n## Time-Series Tests {#sec-ts-cs-ts-tests}\n\n### Full-Sample Time-Series Regressions\n\nWe begin by running full-sample time-series regressions of each test portfolio on the Fama-French five-factor model plus momentum:\n\n::: {#ts-regressions .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Run time-series regressions for all test portfolios\"}\ndef time_series_regressions(excess_returns, factor_df, factor_cols,\n                              cov_type='HAC', maxlags=6):\n    \"\"\"\n    Run time-series regressions of test assets on factors.\n    \n    Returns DataFrame of alphas, betas, t-stats, and R-squared.\n    \"\"\"\n    # Align dates\n    common = excess_returns.index.intersection(factor_df.index)\n    Y = excess_returns.loc[common]\n    X = factor_df.loc[common, factor_cols]\n    X_const = sm.add_constant(X)\n    \n    results = {}\n    for col in Y.columns:\n        y = Y[col].dropna()\n        x = X_const.loc[y.index]\n        \n        model = sm.OLS(y, x).fit(\n            cov_type=cov_type,\n            cov_kwds={'maxlags': maxlags} if cov_type == 'HAC' else {}\n        )\n        \n        result = {\n            'alpha': model.params['const'],\n            'alpha_t': model.tvalues['const'],\n            'alpha_p': model.pvalues['const'],\n            'r_squared': model.rsquared,\n            'r_squared_adj': model.rsquared_adj\n        }\n        for f in factor_cols:\n            result[f'beta_{f}'] = model.params[f]\n            result[f't_{f}'] = model.tvalues[f]\n        \n        results[col] = result\n    \n    return pd.DataFrame(results).T\n\n# Define models to test\nmodels = {\n    'CAPM': ['mkt_excess'],\n    'FF3': ['mkt_excess', 'smb', 'hml'],\n    'FF5': ['mkt_excess', 'smb', 'hml', 'rmw', 'cma'],\n    'FF5+WML': ['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml'],\n}\n\n# Run for 25 Size-BM portfolios\nprint(\"Time-Series Alphas: 25 Size-BM Portfolios\")\nprint(\"=\" * 60)\n\nts_results = {}\nfor model_name, factor_cols in models.items():\n    result = time_series_regressions(\n        excess_25_bm, factors, factor_cols\n    )\n    ts_results[model_name] = result\n    \n    mean_alpha = result['alpha'].mean() * 12\n    mean_abs_alpha = result['alpha'].abs().mean() * 12\n    pct_sig = (result['alpha_p'] < 0.05).mean()\n    mean_r2 = result['r_squared'].mean()\n    \n    print(f\"\\n{model_name}:\")\n    print(f\"  Mean alpha (ann.): {mean_alpha:.4f}\")\n    print(f\"  Mean |alpha| (ann.): {mean_abs_alpha:.4f}\")\n    print(f\"  % sig. at 5%: {pct_sig:.1%}\")\n    print(f\"  Mean R²: {mean_r2:.3f}\")\n```\n:::\n\n\n### The GRS Test\n\n::: {#grs-test .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Implement and compute the GRS test statistic\"}\ndef grs_test(excess_returns, factor_df, factor_cols):\n    \"\"\"\n    Gibbons, Ross, Shanken (1989) test of H0: all alphas = 0.\n    \n    Returns GRS statistic, p-value, and related diagnostics.\n    \"\"\"\n    common = excess_returns.index.intersection(factor_df.index)\n    Y = excess_returns.loc[common].values  # T x N\n    F = factor_df.loc[common, factor_cols].values  # T x K\n    \n    T, N = Y.shape\n    K = F.shape[1]\n    \n    # Add constant and run OLS for each asset\n    F_const = np.column_stack([np.ones(T), F])\n    \n    # OLS: beta = (X'X)^-1 X'Y\n    XtX_inv = inv(F_const.T @ F_const)\n    B = XtX_inv @ F_const.T @ Y  # (K+1) x N\n    \n    alpha = B[0, :]  # N-vector of intercepts\n    residuals = Y - F_const @ B  # T x N\n    \n    # Residual covariance matrix\n    Sigma_eps = residuals.T @ residuals / (T - K - 1)\n    \n    # Factor mean and covariance\n    f_bar = F.mean(axis=0)\n    Sigma_f = np.cov(F, rowvar=False, ddof=1)\n    \n    # GRS statistic\n    Sigma_eps_inv = inv(Sigma_eps)\n    Sigma_f_inv = inv(Sigma_f) if K > 1 else np.array([[1 / Sigma_f]])\n    \n    sharpe_sq_alpha = alpha @ Sigma_eps_inv @ alpha\n    sharpe_sq_f = f_bar @ Sigma_f_inv @ f_bar if K > 1 else (f_bar[0] ** 2 / Sigma_f)\n    \n    grs_stat = ((T - N - K) / N) * (1 / (1 + sharpe_sq_f)) * sharpe_sq_alpha\n    \n    # p-value from F distribution\n    p_value = 1 - stats.f.cdf(grs_stat, N, T - N - K)\n    \n    # Additional diagnostics\n    # Sharpe ratio of tangency portfolio of factors\n    sr_factors = np.sqrt(sharpe_sq_f)\n    # Sharpe ratio of tangency portfolio including alphas\n    sr_alpha = np.sqrt(sharpe_sq_alpha + sharpe_sq_f)\n    \n    return {\n        'GRS': grs_stat,\n        'p_value': p_value,\n        'df1': N,\n        'df2': T - N - K,\n        'mean_abs_alpha': np.abs(alpha).mean(),\n        'sr_factors': sr_factors,\n        'sr_alpha_plus_factors': sr_alpha,\n        'sr_improvement': sr_alpha - sr_factors,\n        'T': T, 'N': N, 'K': K\n    }\n\n# Run GRS for each model on each set of test assets\nprint(\"GRS Test Results\")\nprint(\"=\" * 80)\nprint(f\"{'Model':<12} {'Test Assets':<18} {'GRS':>8} {'p-value':>10} \"\n      f\"{'|α| ann':>10} {'SR(f)':>8} {'SR(α+f)':>8}\")\nprint(\"-\" * 80)\n\ntest_asset_sets = {\n    '25 Size-BM': excess_25_bm,\n    '25 Size-Mom': excess_25_mom,\n    '10 Industry': excess_10_ind\n}\n\ngrs_all = {}\nfor model_name, factor_cols in models.items():\n    for ta_name, ta_data in test_asset_sets.items():\n        key = f\"{model_name}_{ta_name}\"\n        result = grs_test(ta_data, factors, factor_cols)\n        grs_all[key] = result\n        \n        print(f\"{model_name:<12} {ta_name:<18} \"\n              f\"{result['GRS']:>8.2f} {result['p_value']:>10.4f} \"\n              f\"{result['mean_abs_alpha']*12:>10.4f} \"\n              f\"{result['sr_factors']:>8.3f} \"\n              f\"{result['sr_alpha_plus_factors']:>8.3f}\")\n```\n:::\n\n\n::: {#fig-alpha-patterns .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Plot the alpha matrix for Size-BM portfolios\"}\nff5_result = ts_results['FF5']\n\n# Reshape alphas into 5x5 matrix\nalpha_matrix = ff5_result['alpha'].values.reshape(5, 5) * 12 * 100\nt_matrix = ff5_result['alpha_t'].values.reshape(5, 5)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Alpha magnitudes\nim = axes[0].imshow(alpha_matrix, cmap='RdBu_r', aspect='auto',\n                     vmin=-np.max(np.abs(alpha_matrix)),\n                     vmax=np.max(np.abs(alpha_matrix)))\nfor i in range(5):\n    for j in range(5):\n        sig = '*' if abs(t_matrix[i, j]) > 2 else ''\n        axes[0].text(j, i, f'{alpha_matrix[i, j]:.1f}{sig}',\n                     ha='center', va='center', fontsize=9,\n                     color='white' if abs(alpha_matrix[i, j]) > 3 else 'black')\n\naxes[0].set_xticks(range(5))\naxes[0].set_xticklabels(['Growth', '2', '3', '4', 'Value'])\naxes[0].set_yticks(range(5))\naxes[0].set_yticklabels(['Small', '2', '3', '4', 'Big'])\naxes[0].set_xlabel('Book-to-Market')\naxes[0].set_ylabel('Size')\naxes[0].set_title('Panel A: FF5 Alphas (% ann.)')\nplt.colorbar(im, ax=axes[0], label='Alpha (% ann.)')\n\n# Panel B: R-squared\nr2_matrix = ff5_result['r_squared'].values.reshape(5, 5) * 100\nim2 = axes[1].imshow(r2_matrix, cmap='YlGn', aspect='auto',\n                       vmin=50, vmax=100)\nfor i in range(5):\n    for j in range(5):\n        axes[1].text(j, i, f'{r2_matrix[i, j]:.0f}%',\n                     ha='center', va='center', fontsize=9)\n\naxes[1].set_xticks(range(5))\naxes[1].set_xticklabels(['Growth', '2', '3', '4', 'Value'])\naxes[1].set_yticks(range(5))\naxes[1].set_yticklabels(['Small', '2', '3', '4', 'Big'])\naxes[1].set_xlabel('Book-to-Market')\naxes[1].set_ylabel('Size')\naxes[1].set_title('Panel B: R-squared (%)')\nplt.colorbar(im2, ax=axes[1], label='R² (%)')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Model Comparison via GRS\n\n::: {#fig-grs-comparison .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Visualize GRS statistics across models and test assets\"}\nfig, ax = plt.subplots(figsize=(12, 5))\n\nmodel_names = list(models.keys())\nta_names = list(test_asset_sets.keys())\nx = np.arange(len(model_names))\nwidth = 0.25\n\ncolors_ta = ['#2C5F8A', '#C0392B', '#27AE60']\n\nfor i, ta_name in enumerate(ta_names):\n    grs_vals = [grs_all[f\"{m}_{ta_name}\"]['GRS'] for m in model_names]\n    bars = ax.bar(x + i * width, grs_vals, width, alpha=0.85,\n                   color=colors_ta[i], label=ta_name, edgecolor='white')\n\nax.set_xticks(x + width)\nax.set_xticklabels(model_names)\nax.set_ylabel('GRS Statistic')\nax.set_title('GRS Test: Model Comparison Across Test Asset Sets')\nax.legend()\n\n# Add critical value line (5% significance)\n# Approximate: depends on N, T, K\nax.axhline(y=1.8, color='gray', linestyle='--', linewidth=1,\n           label='~5% critical value')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Cross-Sectional Tests {#sec-ts-cs-cs-tests}\n\n### Fama-MacBeth with Portfolio Test Assets\n\nWe implement the Fama-MacBeth two-pass procedure using the 25 size-BM portfolios as test assets:\n\n::: {#fama-macbeth-portfolios .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Full Fama-MacBeth two-pass procedure on portfolio test assets\"}\ndef fama_macbeth_two_pass(excess_returns, factor_df, factor_cols,\n                            beta_window='full', rolling_window=60,\n                            shanken_correction=True):\n    \"\"\"\n    Fama-MacBeth (1973) two-pass cross-sectional regression.\n    \n    Parameters\n    ----------\n    beta_window : str\n        'full' for full-sample betas, 'rolling' for rolling betas.\n    shanken_correction : bool\n        Apply Shanken (1992) errors-in-variables correction.\n    \n    Returns\n    -------\n    Dictionary with estimated risk premia, t-statistics, and diagnostics.\n    \"\"\"\n    common = excess_returns.index.intersection(factor_df.index)\n    Y = excess_returns.loc[common]\n    F = factor_df.loc[common, factor_cols]\n    \n    T = len(common)\n    N = Y.shape[1]\n    K = len(factor_cols)\n    months = sorted(common)\n    \n    # ===== STAGE 1: Estimate betas =====\n    if beta_window == 'full':\n        # Full-sample betas (simpler, used in GRS context)\n        F_const = sm.add_constant(F)\n        betas = {}\n        for col in Y.columns:\n            model = sm.OLS(Y[col], F_const).fit()\n            betas[col] = {f: model.params[f] for f in factor_cols}\n        beta_df = pd.DataFrame(betas).T  # N x K\n        \n        # Same betas used every month\n        beta_by_month = {m: beta_df for m in months}\n    \n    elif beta_window == 'rolling':\n        # Rolling betas (more realistic, avoids look-ahead)\n        beta_by_month = {}\n        for t_idx in range(rolling_window, T):\n            month = months[t_idx]\n            window_start = months[t_idx - rolling_window]\n            \n            Y_win = Y.loc[window_start:months[t_idx - 1]]\n            F_win = sm.add_constant(F.loc[window_start:months[t_idx - 1]])\n            \n            betas_t = {}\n            for col in Y.columns:\n                y = Y_win[col].dropna()\n                x = F_win.loc[y.index]\n                if len(y) < 24:\n                    betas_t[col] = {f: np.nan for f in factor_cols}\n                    continue\n                model = sm.OLS(y, x).fit()\n                betas_t[col] = {f: model.params[f] for f in factor_cols}\n            \n            beta_by_month[month] = pd.DataFrame(betas_t).T\n    \n    # ===== STAGE 2: Cross-sectional regressions =====\n    gamma_list = []\n    \n    for month in months:\n        if month not in beta_by_month:\n            continue\n        \n        betas_t = beta_by_month[month]\n        returns_t = Y.loc[month]\n        \n        # Align\n        valid = betas_t.dropna().index.intersection(returns_t.dropna().index)\n        if len(valid) < K + 5:\n            continue\n        \n        y = returns_t[valid].values\n        X = sm.add_constant(betas_t.loc[valid, factor_cols].values)\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            gammas = {'month': month, 'gamma_0': model.params[0]}\n            for j, f in enumerate(factor_cols):\n                gammas[f'gamma_{f}'] = model.params[j + 1]\n            gammas['r_squared'] = model.rsquared\n            gammas['n_assets'] = len(valid)\n            gamma_list.append(gammas)\n        except Exception:\n            pass\n    \n    gamma_df = pd.DataFrame(gamma_list)\n    \n    if len(gamma_df) == 0:\n        return None\n    \n    # ===== INFERENCE =====\n    # Time-series averages of gammas\n    gamma_cols = ['gamma_0'] + [f'gamma_{f}' for f in factor_cols]\n    \n    results = {}\n    for col in gamma_cols:\n        g = gamma_df[col]\n        mean = g.mean()\n        se_fm = g.std() / np.sqrt(len(g))\n        t_fm = mean / se_fm if se_fm > 0 else np.nan\n        \n        results[col] = {\n            'estimate': mean,\n            'se_fm': se_fm,\n            't_fm': t_fm,\n        }\n    \n    # Shanken (1992) correction for errors-in-variables\n    if shanken_correction and beta_window == 'full':\n        Sigma_f = F[factor_cols].cov().values\n        \n        # Shanken correction factor: (1 + lambda' Sigma_f^-1 lambda)\n        gamma_factor = np.array([results[f'gamma_{f}']['estimate']\n                                  for f in factor_cols])\n        try:\n            Sigma_f_inv = inv(Sigma_f)\n            c_shanken = 1 + gamma_factor @ Sigma_f_inv @ gamma_factor\n        except Exception:\n            c_shanken = 1.0\n        \n        for col in gamma_cols:\n            results[col]['se_shanken'] = results[col]['se_fm'] * np.sqrt(c_shanken)\n            results[col]['t_shanken'] = (results[col]['estimate']\n                                          / results[col]['se_shanken']\n                                          if results[col]['se_shanken'] > 0 else np.nan)\n    \n    # Compare estimated premia to factor mean returns\n    factor_means = F[factor_cols].mean()\n    for f in factor_cols:\n        results[f'gamma_{f}']['factor_mean'] = factor_means[f]\n    \n    # Cross-sectional R-squared\n    results['avg_cs_r2'] = gamma_df['r_squared'].mean()\n    \n    return {\n        'results': pd.DataFrame(results).T,\n        'gamma_ts': gamma_df,\n        'beta_df': beta_by_month.get(months[-1], None)\n    }\n\n# Run Fama-MacBeth for each model\nprint(\"Fama-MacBeth Cross-Sectional Results: 25 Size-BM Portfolios\")\nprint(\"=\" * 80)\n\nfm_results = {}\nfor model_name, factor_cols in models.items():\n    for beta_type in ['full', 'rolling']:\n        key = f\"{model_name}_{beta_type}\"\n        fm = fama_macbeth_two_pass(\n            excess_25_bm, factors, factor_cols,\n            beta_window=beta_type,\n            rolling_window=60,\n            shanken_correction=(beta_type == 'full')\n        )\n        if fm is None:\n            continue\n        fm_results[key] = fm\n        \n        print(f\"\\n{model_name} (betas: {beta_type}):\")\n        res = fm['results']\n        for idx, row in res.iterrows():\n            if 'gamma' in idx:\n                t_col = 't_shanken' if 't_shanken' in row and pd.notna(row.get('t_shanken')) else 't_fm'\n                f_mean = row.get('factor_mean', '')\n                f_str = f\"  f_mean={f_mean:.4f}\" if isinstance(f_mean, float) else \"\"\n                print(f\"  {idx:<16}: γ = {row['estimate']:.5f}, \"\n                      f\"t = {row[t_col]:.2f}{f_str}\")\n        if 'avg_cs_r2' in res.index:\n            print(f\"  Avg CS R²: {res.loc['avg_cs_r2', 'estimate']:.3f}\")\n```\n:::\n\n\n### The Shanken Correction\n\nThe @shanken1992estimation correction addresses the errors-in-variables (EIV) problem inherent in the two-pass procedure. Because betas are estimated with error in Stage 1, the Stage 2 standard errors are understated. The correction inflates the standard errors by a factor that depends on the estimated risk premia and the factor covariance matrix:\n\n$$\n\\text{Var}(\\hat{\\gamma})_{\\text{Shanken}} = \\text{Var}(\\hat{\\gamma})_{\\text{FM}} \\times \\left(1 + \\hat{\\gamma}' \\hat{\\Sigma}_f^{-1} \\hat{\\gamma}\\right)\n$$ {#eq-shanken}\n\n::: {#shanken-impact .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Compare FM t-statistics with and without Shanken correction\"}\nprint(\"Impact of Shanken Correction on t-statistics:\")\nprint(f\"{'Model':<12} {'Factor':<12} {'t (FM)':>10} {'t (Shanken)':>12} {'Ratio':>8}\")\nprint(\"-\" * 54)\n\nfor model_name in models:\n    key = f\"{model_name}_full\"\n    if key not in fm_results:\n        continue\n    res = fm_results[key]['results']\n    for idx, row in res.iterrows():\n        if 'gamma_' in str(idx) and idx != 'gamma_0':\n            t_fm = row.get('t_fm', np.nan)\n            t_sh = row.get('t_shanken', np.nan)\n            ratio = t_fm / t_sh if pd.notna(t_sh) and t_sh != 0 else np.nan\n            factor_name = idx.replace('gamma_', '')\n            print(f\"{model_name:<12} {factor_name:<12} {t_fm:>10.2f} \"\n                  f\"{t_sh:>12.2f} {ratio:>8.2f}\")\n```\n:::\n\n\n### Fama-MacBeth with Individual Stocks\n\nUsing individual stocks instead of portfolios as test assets avoids the @lewellen2010skeptical critique that sorted portfolios create artificially strong factor structure:\n\n::: {#fm-individual .cell execution_count=10}\n``` {.python .cell-code code-summary=\"Fama-MacBeth regressions using individual stocks\"}\ndef fama_macbeth_individual(stock_df, factor_df, factor_cols,\n                              beta_window=60, min_obs=36,\n                              min_stocks=100):\n    \"\"\"\n    Fama-MacBeth with individual stocks and rolling betas.\n    \"\"\"\n    stock_df = stock_df.copy()\n    stock_df['month_end'] = pd.to_datetime(stock_df['month_end'])\n    factor_df = factor_df.copy()\n    \n    months = sorted(stock_df['month_end'].unique())\n    \n    # Pre-compute rolling betas for all stocks\n    gamma_list = []\n    \n    for t_idx, month in enumerate(months):\n        if t_idx < beta_window:\n            continue\n        \n        window_months = months[t_idx - beta_window:t_idx]\n        \n        # Get betas for each stock from rolling window\n        betas_t = {}\n        window_data = stock_df[stock_df['month_end'].isin(window_months)]\n        \n        for ticker, group in window_data.groupby('ticker'):\n            if len(group) < min_obs:\n                continue\n            \n            y = group.set_index('month_end')['monthly_return']\n            x = factor_df.loc[y.index, factor_cols]\n            x = sm.add_constant(x)\n            \n            valid = y.index.intersection(x.index)\n            if len(valid) < min_obs:\n                continue\n            \n            try:\n                model = sm.OLS(y[valid], x.loc[valid]).fit()\n                betas_t[ticker] = {f: model.params[f] for f in factor_cols}\n            except Exception:\n                pass\n        \n        if len(betas_t) < min_stocks:\n            continue\n        \n        beta_df = pd.DataFrame(betas_t).T\n        \n        # Current month returns\n        current = stock_df[stock_df['month_end'] == month]\n        current = current.set_index('ticker')\n        \n        # Align\n        valid_tickers = beta_df.index.intersection(current.index)\n        if len(valid_tickers) < min_stocks:\n            continue\n        \n        y = current.loc[valid_tickers, 'monthly_return'].values\n        X = sm.add_constant(beta_df.loc[valid_tickers].values)\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            gammas = {'month': month, 'gamma_0': model.params[0]}\n            for j, f in enumerate(factor_cols):\n                gammas[f'gamma_{f}'] = model.params[j + 1]\n            gammas['n_stocks'] = len(valid_tickers)\n            gammas['r_squared'] = model.rsquared\n            gamma_list.append(gammas)\n        except Exception:\n            pass\n    \n    gamma_df = pd.DataFrame(gamma_list)\n    \n    # Inference\n    gamma_cols = ['gamma_0'] + [f'gamma_{f}' for f in factor_cols]\n    summary = {}\n    for col in gamma_cols:\n        g = gamma_df[col]\n        mean = g.mean()\n        se = g.std() / np.sqrt(len(g))\n        t = mean / se if se > 0 else np.nan\n        summary[col] = {'estimate': mean, 'se': se, 't': t}\n    \n    summary['avg_n_stocks'] = gamma_df['n_stocks'].mean()\n    summary['avg_cs_r2'] = gamma_df['r_squared'].mean()\n    \n    return pd.DataFrame(summary).T, gamma_df\n\n# Run for FF5\nprint(\"\\nFama-MacBeth with Individual Stocks (FF5):\")\nfm_ind_results, fm_ind_gamma = fama_macbeth_individual(\n    stock_returns, factors,\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma'],\n    beta_window=60, min_obs=36\n)\nprint(fm_ind_results.round(4).to_string())\n```\n:::\n\n\n## When Do the Tests Disagree? {#sec-ts-cs-disagreement}\n\n### Theoretical Sources of Disagreement\n\n@goyal2018cross identify three sources of discrepancy between time-series and cross-sectional tests:\n\n**1. Factor structure of test assets.** If test assets have a strong factor structure (as sorted portfolios do by construction), the cross-sectional $R^2$ will be high regardless of whether the factors truly price the assets. @lewellen2010skeptical show that even random factors can achieve high cross-sectional $R^2$ when test assets are size-BM sorted portfolios, because such portfolios have a strong linear structure in the size-value space.\n\n**2. Time-variation in betas.** The time-series regression assumes constant betas. If betas vary over time and co-move with the factor risk premium, the unconditional time-series alpha can be nonzero even if the conditional model holds [@jagannathan1996conditional].\n\n**3. Misspecified risk premia.** The Fama-MacBeth cross-sectional regression estimates the risk premium from the data. The time-series regression implicitly sets the risk premium equal to the factor's sample mean return. When these differ (e.g., because the factor is not a perfect proxy for the underlying risk), the two approaches disagree.\n\n### Empirical Comparison for Vietnam\n\n::: {#ts-vs-cs .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Compare time-series alphas with cross-sectional pricing errors\"}\n# For FF5 on 25 Size-BM portfolios:\n# Time-series: alpha from OLS regression\n# Cross-section: pricing error from Fama-MacBeth\n\nts_alphas = ts_results['FF5']['alpha'].values * 12  # Annualized\nts_names = ts_results['FF5'].index.tolist()\n\n# Cross-sectional pricing errors:\n# Predicted return = beta' * estimated_gamma\n# Pricing error = actual mean return - predicted\nfm_full = fm_results.get('FF5_full')\nif fm_full:\n    betas = fm_full['beta_df']  # N x K\n    factor_cols = ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']\n    gammas = np.array([fm_full['results'].loc[f'gamma_{f}', 'estimate']\n                        for f in factor_cols])\n    gamma_0 = fm_full['results'].loc['gamma_0', 'estimate']\n    \n    actual_means = excess_25_bm.mean() * 12  # Annualized\n    predicted = (gamma_0 + betas[factor_cols].values @ gammas) * 12\n    cs_errors = actual_means.values - predicted\n```\n:::\n\n\n::: {#fig-ts-vs-cs .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Scatter plot comparing TS alphas and CS pricing errors\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: TS alpha vs CS pricing error\nif fm_full:\n    axes[0].scatter(ts_alphas * 100, cs_errors * 100,\n                     color='#2C5F8A', s=60, alpha=0.7, edgecolors='white')\n    lim = max(np.max(np.abs(ts_alphas)), np.max(np.abs(cs_errors))) * 100 + 1\n    axes[0].plot([-lim, lim], [-lim, lim], 'k--', linewidth=1)\n    axes[0].plot([-lim, lim], [0, 0], color='gray', linewidth=0.5)\n    axes[0].plot([0, 0], [-lim, lim], color='gray', linewidth=0.5)\n    axes[0].set_xlabel('Time-Series Alpha (% ann.)')\n    axes[0].set_ylabel('Cross-Sectional Pricing Error (% ann.)')\n    axes[0].set_title('Panel A: TS Alpha vs CS Pricing Error')\n\n    # Correlation\n    rho = np.corrcoef(ts_alphas, cs_errors)[0, 1]\n    axes[0].text(0.05, 0.95, f'ρ = {rho:.2f}',\n                  transform=axes[0].transAxes, fontsize=11)\n\n# Panel B: Actual vs Predicted (CS)\nif fm_full:\n    axes[1].scatter(predicted * 100, actual_means.values * 100,\n                     color='#C0392B', s=60, alpha=0.7, edgecolors='white')\n    lim2 = max(np.max(np.abs(predicted)), np.max(np.abs(actual_means))) * 100 + 2\n    axes[1].plot([-5, lim2], [-5, lim2], 'k--', linewidth=1)\n    axes[1].set_xlabel('Predicted Average Return (% ann.)')\n    axes[1].set_ylabel('Actual Average Return (% ann.)')\n    axes[1].set_title('Panel B: Actual vs Predicted (FM Cross-Section)')\n    \n    # CS R-squared\n    ss_res = np.sum((actual_means.values - predicted) ** 2)\n    ss_tot = np.sum((actual_means.values - actual_means.values.mean()) ** 2)\n    r2_cs = 1 - ss_res / ss_tot\n    axes[1].text(0.05, 0.95, f'CS R² = {r2_cs:.2f}',\n                  transform=axes[1].transAxes, fontsize=11)\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### The Lewellen-Nagel-Shanken Critique\n\n@lewellen2010skeptical demonstrate that the high cross-sectional $R^2$ from Fama-MacBeth regressions on sorted portfolios can be misleading. Sorted portfolios have a strong factor structure by construction, so even irrelevant factors can produce high $R^2$. They recommend:\n\n1.  Including industry portfolios alongside sorted portfolios to break the mechanical factor structure.\n2.  Constraining the estimated risk premia to equal the factor mean returns (this is what the time-series test implicitly does).\n3.  Reporting the cross-sectional $R^2$ from the *constrained* model alongside the unconstrained estimate.\n\n::: {#lns-critique .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Implement the Lewellen-Nagel-Shanken diagnostic\"}\ndef lns_diagnostic(excess_returns, factor_df, factor_cols):\n    \"\"\"\n    Lewellen-Nagel-Shanken (2010) diagnostic:\n    Compare unconstrained CS R² to constrained (factor mean = premium).\n    \"\"\"\n    common = excess_returns.index.intersection(factor_df.index)\n    Y = excess_returns.loc[common]\n    F = factor_df.loc[common, factor_cols]\n    \n    # Full-sample betas\n    betas = {}\n    F_const = sm.add_constant(F)\n    for col in Y.columns:\n        model = sm.OLS(Y[col], F_const).fit()\n        betas[col] = {f: model.params[f] for f in factor_cols}\n    beta_mat = pd.DataFrame(betas).T  # N x K\n    \n    actual = Y.mean() * 12  # Annualized\n    \n    # Unconstrained: FM regression\n    X = sm.add_constant(beta_mat.values)\n    fm_model = sm.OLS(actual.values, X).fit()\n    predicted_unc = fm_model.fittedvalues\n    r2_unconstrained = fm_model.rsquared\n    \n    # Constrained: premium = factor mean return\n    factor_means = F.mean().values * 12  # Annualized\n    predicted_con = beta_mat.values @ factor_means\n    # Add best-fitting intercept\n    intercept_con = np.mean(actual.values - predicted_con)\n    predicted_con += intercept_con\n    \n    ss_res_con = np.sum((actual.values - predicted_con) ** 2)\n    ss_tot = np.sum((actual.values - actual.values.mean()) ** 2)\n    r2_constrained = 1 - ss_res_con / ss_tot\n    \n    return {\n        'r2_unconstrained': r2_unconstrained,\n        'r2_constrained': r2_constrained,\n        'r2_drop': r2_unconstrained - r2_constrained,\n        'gamma_unconstrained': fm_model.params[1:],\n        'gamma_constrained': factor_means\n    }\n\nprint(\"Lewellen-Nagel-Shanken Diagnostic:\")\nprint(f\"{'Model':<12} {'Test Assets':<18} {'R² (unc)':>10} {'R² (con)':>10} \"\n      f\"{'Drop':>8}\")\nprint(\"-\" * 58)\n\nfor model_name, factor_cols in models.items():\n    for ta_name, ta_data in test_asset_sets.items():\n        diag = lns_diagnostic(ta_data, factors, factor_cols)\n        print(f\"{model_name:<12} {ta_name:<18} \"\n              f\"{diag['r2_unconstrained']:>10.3f} \"\n              f\"{diag['r2_constrained']:>10.3f} \"\n              f\"{diag['r2_drop']:>8.3f}\")\n```\n:::\n\n\n::: {#fig-lns .cell execution_count=14}\n``` {.python .cell-code code-summary=\"Visualize constrained vs unconstrained cross-sectional fit\"}\ndiag_ff5 = lns_diagnostic(excess_25_bm, factors,\n                            ['mkt_excess', 'smb', 'hml', 'rmw', 'cma'])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nactual = excess_25_bm.mean().values * 12 * 100\n\n# Panel A: Unconstrained\nF_const = sm.add_constant(factors[['mkt_excess', 'smb', 'hml', 'rmw', 'cma']])\nbetas_full = {}\nfor col in excess_25_bm.columns:\n    common = excess_25_bm.index.intersection(factors.index)\n    model = sm.OLS(excess_25_bm.loc[common, col],\n                    F_const.loc[common]).fit()\n    betas_full[col] = {f: model.params[f] for f in\n                        ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']}\nbeta_mat = pd.DataFrame(betas_full).T\nX_cs = sm.add_constant(beta_mat.values)\nfm_mod = sm.OLS(actual, X_cs).fit()\npred_unc = fm_mod.fittedvalues\n\naxes[0].scatter(pred_unc, actual, color='#2C5F8A', s=60, alpha=0.7,\n                 edgecolors='white')\nrng_plot = [min(pred_unc.min(), actual.min()) - 1,\n            max(pred_unc.max(), actual.max()) + 1]\naxes[0].plot(rng_plot, rng_plot, 'k--', linewidth=1)\naxes[0].set_xlabel('Predicted (% ann.)')\naxes[0].set_ylabel('Actual (% ann.)')\naxes[0].set_title(f\"Panel A: Unconstrained (R² = \"\n                    f\"{diag_ff5['r2_unconstrained']:.2f})\")\n\n# Panel B: Constrained\nfactor_means = factors[['mkt_excess', 'smb', 'hml', 'rmw', 'cma']].mean().values * 12 * 100\npred_con = beta_mat.values @ factor_means\npred_con += np.mean(actual - pred_con)\n\naxes[1].scatter(pred_con, actual, color='#C0392B', s=60, alpha=0.7,\n                 edgecolors='white')\nrng2 = [min(pred_con.min(), actual.min()) - 1,\n        max(pred_con.max(), actual.max()) + 1]\naxes[1].plot(rng2, rng2, 'k--', linewidth=1)\naxes[1].set_xlabel('Predicted (% ann.)')\naxes[1].set_ylabel('Actual (% ann.)')\naxes[1].set_title(f\"Panel B: Constrained (R² = \"\n                    f\"{diag_ff5['r2_constrained']:.2f})\")\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## GMM-Based Tests {#sec-ts-cs-gmm}\n\n### The Stochastic Discount Factor Framework\n\nBoth time-series and cross-sectional tests are special cases of the @hansen1982large Generalized Method of Moments (GMM) framework. The fundamental pricing equation is:\n\n$$\nE[M_t R_{i,t}^e] = 0 \\quad \\text{for all } i\n$$ {#eq-sdf}\n\nwhere $M_t$ is the stochastic discount factor (SDF) and $R_{i,t}^e$ is the excess return of asset $i$. A linear factor model parameterizes the SDF as:\n\n$$\nM_t = 1 - b' (f_t - \\mu_f)\n$$ {#eq-linear-sdf}\n\nwhere $b$ is the vector of SDF loadings and $f_t$ are the factors. The GMM approach estimates $b$ by minimizing the quadratic form of the pricing errors:\n\n$$\n\\hat{b} = \\arg\\min_b \\ g(b)' W \\ g(b)\n$$ {#eq-gmm-objective}\n\nwhere $g(b) = \\frac{1}{T} \\sum_t M_t(b) R_t^e$ is the sample analog of the moment conditions.\n\n::: {#gmm-sdf .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Implement GMM estimation of the linear SDF model\"}\ndef gmm_sdf(excess_returns, factor_df, factor_cols,\n              weighting='identity', n_iter=2):\n    \"\"\"\n    GMM estimation of the linear SDF model.\n    \n    Parameters\n    ----------\n    weighting : str\n        'identity': first-stage identity weighting matrix\n        'optimal': Hansen optimal weighting (iterated)\n    n_iter : int\n        Number of GMM iterations (2 = efficient two-step)\n    \"\"\"\n    common = excess_returns.index.intersection(factor_df.index)\n    Re = excess_returns.loc[common].values  # T x N\n    F = factor_df.loc[common, factor_cols].values  # T x K\n    \n    T, N = Re.shape\n    K = F.shape[1]\n    \n    # Demean factors\n    F_dm = F - F.mean(axis=0)\n    \n    # Moment conditions: E[M * Re] = 0\n    # M = 1 - b' (f - mu_f) = 1 - b' F_dm\n    def pricing_errors(b):\n        M = 1 - F_dm @ b  # T-vector\n        g = (M[:, np.newaxis] * Re).mean(axis=0)  # N-vector\n        return g\n    \n    # GMM objective\n    W = np.eye(N)  # Initial weighting matrix\n    \n    from scipy.optimize import minimize\n    \n    for iteration in range(n_iter):\n        def objective(b):\n            g = pricing_errors(b)\n            return T * g @ W @ g\n        \n        result = minimize(objective, np.zeros(K), method='L-BFGS-B')\n        b_hat = result.x\n        \n        if iteration < n_iter - 1:\n            # Update weighting matrix (Hansen optimal)\n            g_t = np.array([(1 - F_dm[t] @ b_hat) * Re[t]\n                             for t in range(T)])  # T x N\n            S = np.cov(g_t, rowvar=False, ddof=0)\n            \n            # Newey-West adjustment for serial correlation\n            max_lag = int(np.floor(4 * (T / 100) ** (2 / 9)))\n            for lag in range(1, max_lag + 1):\n                w = 1 - lag / (max_lag + 1)\n                Gamma = np.cov(g_t[lag:].T, g_t[:-lag].T, ddof=0)[:N, N:]\n                S += w * (Gamma + Gamma.T)\n            \n            try:\n                W = inv(S)\n            except Exception:\n                W = np.eye(N)\n    \n    # Pricing errors at optimum\n    g_hat = pricing_errors(b_hat)\n    \n    # Implied risk premia: lambda = Sigma_f @ b\n    Sigma_f = np.cov(F, rowvar=False, ddof=1)\n    lambda_hat = Sigma_f @ b_hat\n    \n    # J-test (overidentification)\n    J = T * g_hat @ W @ g_hat\n    df_J = N - K\n    p_J = 1 - stats.chi2.cdf(J, df_J) if df_J > 0 else np.nan\n    \n    # HJ distance (model misspecification)\n    hj_dist = np.sqrt(g_hat @ inv(np.cov(Re, rowvar=False)) @ g_hat)\n    \n    return {\n        'b_hat': b_hat,\n        'lambda_hat': lambda_hat,\n        'pricing_errors': g_hat,\n        'J_stat': J,\n        'J_pvalue': p_J,\n        'J_df': df_J,\n        'hj_distance': hj_dist,\n        'mean_abs_error': np.abs(g_hat).mean() * 12\n    }\n\nprint(\"GMM SDF Estimation:\")\nprint(f\"{'Model':<12} {'Test Assets':<18} {'J-stat':>8} {'J p-val':>10} \"\n      f\"{'HJ dist':>8} {'|e| ann':>10}\")\nprint(\"-\" * 66)\n\nfor model_name, factor_cols in models.items():\n    for ta_name, ta_data in test_asset_sets.items():\n        gmm = gmm_sdf(ta_data, factors, factor_cols, n_iter=2)\n        print(f\"{model_name:<12} {ta_name:<18} \"\n              f\"{gmm['J_stat']:>8.2f} {gmm['J_pvalue']:>10.4f} \"\n              f\"{gmm['hj_distance']:>8.4f} {gmm['mean_abs_error']:>10.4f}\")\n        \n        # Print implied risk premia\n        for j, f in enumerate(factor_cols):\n            print(f\"  λ_{f}: {gmm['lambda_hat'][j]*12:.4f} \"\n                  f\"(factor mean: {factors[f].mean()*12:.4f})\")\n```\n:::\n\n\n## Model Comparison {#sec-ts-cs-model-comparison}\n\n### The Barillas-Shanken Framework\n\n@barillas2018comparing propose a Bayesian framework for comparing non-nested factor models. The key insight is that model comparison should be based on the factors' ability to explain each other's returns, not on their ability to price a fixed set of test assets. Two models differ only in their excluded factors, so the relevant comparison is whether the factors unique to each model have alpha with respect to the other model.\n\n::: {#barillas-shanken .cell execution_count=16}\n``` {.python .cell-code code-summary=\"Implement Barillas-Shanken model comparison\"}\ndef barillas_shanken_compare(factor_df, model_a_cols, model_b_cols):\n    \"\"\"\n    Barillas-Shanken (2018) pairwise model comparison.\n    \n    Compare Model A vs Model B by testing whether the factors\n    unique to each model have alpha with respect to the other.\n    \"\"\"\n    # Factors unique to each model\n    unique_a = [f for f in model_a_cols if f not in model_b_cols]\n    unique_b = [f for f in model_b_cols if f not in model_a_cols]\n    \n    results = {}\n    \n    # Test: do factors unique to A have alpha w.r.t. B?\n    # If yes, A adds value beyond B\n    if unique_a:\n        for f in unique_a:\n            y = factor_df[f]\n            X = sm.add_constant(factor_df[model_b_cols])\n            common = y.dropna().index.intersection(X.dropna().index)\n            model = sm.OLS(y[common], X.loc[common]).fit(\n                cov_type='HAC', cov_kwds={'maxlags': 6}\n            )\n            results[f'alpha_{f}_vs_B'] = {\n                'alpha': model.params['const'] * 12,\n                't': model.tvalues['const'],\n                'r2': model.rsquared\n            }\n    \n    if unique_b:\n        for f in unique_b:\n            y = factor_df[f]\n            X = sm.add_constant(factor_df[model_a_cols])\n            common = y.dropna().index.intersection(X.dropna().index)\n            model = sm.OLS(y[common], X.loc[common]).fit(\n                cov_type='HAC', cov_kwds={'maxlags': 6}\n            )\n            results[f'alpha_{f}_vs_A'] = {\n                'alpha': model.params['const'] * 12,\n                't': model.tvalues['const'],\n                'r2': model.rsquared\n            }\n    \n    return pd.DataFrame(results).T\n\n# Compare FF3 vs FF5\nprint(\"Barillas-Shanken: FF3 vs FF5\")\nbs_3v5 = barillas_shanken_compare(\n    factors,\n    ['mkt_excess', 'smb', 'hml'],\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']\n)\nprint(bs_3v5.round(4).to_string())\n\n# Compare FF5 vs FF5+WML\nprint(\"\\nBarillas-Shanken: FF5 vs FF5+WML\")\nbs_5vw = barillas_shanken_compare(\n    factors,\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma'],\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml']\n)\nprint(bs_5vw.round(4).to_string())\n\n# Compare FF3+WML vs FF5+WML\nprint(\"\\nBarillas-Shanken: FF3+WML vs FF5+WML\")\nbs_3wv5w = barillas_shanken_compare(\n    factors,\n    ['mkt_excess', 'smb', 'hml', 'wml'],\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml']\n)\nprint(bs_3wv5w.round(4).to_string())\n```\n:::\n\n\n### Comprehensive Model Scorecard\n\n::: {#fig-scorecard .cell execution_count=17}\n``` {.python .cell-code code-summary=\"Build comprehensive model comparison table\"}\nscorecard = {}\n\nfor model_name, factor_cols in models.items():\n    metrics = {'Model': model_name}\n    \n    # Time-series (25 Size-BM)\n    grs_result = grs_all.get(f\"{model_name}_25 Size-BM\")\n    if grs_result:\n        metrics['GRS (BM)'] = grs_result['GRS']\n        metrics['GRS p'] = grs_result['p_value']\n        metrics['|α| (BM)'] = grs_result['mean_abs_alpha'] * 12\n    \n    # Cross-section\n    fm_key = f\"{model_name}_full\"\n    if fm_key in fm_results:\n        metrics['CS R² (unc)'] = fm_results[fm_key]['results'].get(\n            'avg_cs_r2', {}).get('estimate', np.nan)\n    \n    # LNS constrained\n    lns = lns_diagnostic(excess_25_bm, factors, factor_cols)\n    metrics['CS R² (con)'] = lns['r2_constrained']\n    \n    # GMM\n    gmm = gmm_sdf(excess_25_bm, factors, factor_cols, n_iter=2)\n    metrics['HJ dist'] = gmm['hj_distance']\n    metrics['J p-val'] = gmm['J_pvalue']\n    \n    # Time-series R²\n    ts_res = time_series_regressions(excess_25_bm, factors, factor_cols)\n    metrics['Avg R²'] = ts_res['r_squared'].mean()\n    \n    scorecard[model_name] = metrics\n\nscorecard_df = pd.DataFrame(scorecard).T\nprint(\"Model Scorecard:\")\nprint(scorecard_df.round(3).to_string())\n```\n:::\n\n\n## Conditional vs. Unconditional Tests {#sec-ts-cs-conditional}\n\n### Time-Varying Betas\n\nUnconditional tests assume constant betas. In practice, Vietnamese stock betas vary substantially over time due to changing market conditions, foreign ownership shifts, and sectoral rotations. We estimate rolling betas to assess the degree of time-variation:\n\n::: {#rolling-betas .cell execution_count=18}\n``` {.python .cell-code code-summary=\"Estimate and visualize time-varying betas\"}\n# Rolling 36-month betas for the 25 Size-BM portfolios on FF5\nrolling_window = 36\nfactor_cols = ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']\n\n# Select a few representative portfolios\nrepresentative = [excess_25_bm.columns[0],   # Small-Growth\n                   excess_25_bm.columns[4],   # Small-Value\n                   excess_25_bm.columns[20],  # Big-Growth\n                   excess_25_bm.columns[24]]  # Big-Value\n\nrolling_betas = {}\ncommon = excess_25_bm.index.intersection(factors.index)\n\nfor port in representative:\n    betas_t = []\n    for t in range(rolling_window, len(common)):\n        window = common[t - rolling_window:t]\n        y = excess_25_bm.loc[window, port]\n        X = sm.add_constant(factors.loc[window, factor_cols])\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            entry = {'month': common[t]}\n            for f in factor_cols:\n                entry[f'beta_{f}'] = model.params[f]\n            betas_t.append(entry)\n        except Exception:\n            pass\n    \n    rolling_betas[port] = pd.DataFrame(betas_t)\n```\n:::\n\n\n::: {#fig-rolling-betas .cell execution_count=19}\n``` {.python .cell-code code-summary=\"Plot rolling betas over time\"}\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nlabels = ['Small-Growth', 'Small-Value', 'Big-Growth', 'Big-Value']\ncolors_port = ['#C0392B', '#2C5F8A', '#E67E22', '#27AE60']\n\nfor i, (port, label, color) in enumerate(zip(representative, labels, colors_port)):\n    rb = rolling_betas[port]\n    \n    axes[i].plot(pd.to_datetime(rb['month']), rb['beta_mkt_excess'],\n                 color=color, linewidth=1.5, label='Market β')\n    axes[i].axhline(y=1, color='gray', linewidth=0.5, linestyle='--')\n    \n    # Add HML beta\n    axes[i].plot(pd.to_datetime(rb['month']), rb['beta_hml'],\n                 color='gray', linewidth=1, linestyle=':', label='HML β')\n    \n    axes[i].set_ylabel('Beta')\n    axes[i].set_title(label)\n    axes[i].legend(fontsize=8)\n\nplt.suptitle('Rolling 36-Month Factor Betas', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Quantify time-variation\nprint(\"\\nBeta Time-Variation (Std Dev of Rolling Betas):\")\nfor port, label in zip(representative, labels):\n    rb = rolling_betas[port]\n    mkt_std = rb['beta_mkt_excess'].std()\n    hml_std = rb['beta_hml'].std()\n    print(f\"  {label:<15}: σ(β_MKT) = {mkt_std:.3f}, σ(β_HML) = {hml_std:.3f}\")\n```\n:::\n\n\n## Practical Recommendations {#sec-ts-cs-recommendations}\n\nThe analysis in this chapter yields the following guidelines for Vietnamese asset pricing research:\n\n**Use both approaches.** Time-series tests (GRS) and cross-sectional tests (Fama-MacBeth) answer different questions. Always report both. If they disagree, investigate *why* rather than cherry-picking the more favorable result.\n\n**Be cautious with cross-sectional** $R^2$. High $R^2$ from Fama-MacBeth on sorted portfolios is nearly guaranteed by the test asset structure. Always report the @lewellen2010skeptical constrained $R^2$ alongside the unconstrained value. Include industry portfolios as additional test assets.\n\n**Apply the Shanken correction.** The EIV bias from estimated betas inflates Fama-MacBeth t-statistics. Always report Shanken-corrected standard errors for full-sample betas. For rolling betas, the correction is not straightforward, but the bias is smaller because beta estimation error averages out over time.\n\n**Use individual stocks with caution.** Fama-MacBeth with individual stocks avoids the Lewellen-Nagel-Shanken critique but introduces severe noise: individual Vietnamese stocks are thin, volatile, and the monthly cross-section is small (500-700). The resulting risk premia will have wide confidence intervals. Report the average number of stocks per cross-section and the cross-sectional $R^2$.\n\n**Examine conditional models.** Rolling betas reveal substantial time-variation in Vietnamese stock exposures. If the research question is about risk compensation (does beta predict returns?), use rolling betas and control for time-variation. If the question is about model adequacy (do the factors span the mean-variance frontier?), full-sample betas and the GRS test are appropriate.\n\n**For model selection, use Barillas-Shanken.** When comparing competing factor models (e.g., FF3 vs. FF5), the @barillas2018comparing approach (i.e., testing whether each model's unique factors have alpha with respect to the other) is more informative than comparing GRS statistics on the same test assets.\n\n## Summary {#sec-ts-cs-summary}\n\n| Test | Question | Statistic | Strength | Weakness |\n|---------------|---------------|---------------|---------------|---------------|\n| TS regression (GRS) | Do all alphas = 0? | F-statistic | Clean null; joint test | Depends on test assets |\n| Fama-MacBeth (portfolios) | Are betas priced? | $\\bar{\\gamma}$, t-stat | Estimates risk premia | EIV bias; LNS critique |\n| FM (individual stocks) | Are betas priced? | $\\bar{\\gamma}$, t-stat | Avoids sorted portfolios | Noisy; small cross-section |\n| LNS diagnostic | Is CS R² spurious? | Constrained R² | Reveals false positives | Only applicable to portfolios |\n| GMM / SDF | Pricing errors jointly | J-stat, HJ distance | Flexible; model-free | Requires large N relative to K |\n| Barillas-Shanken | Which model is better? | Alpha of unique factors | Model comparison | Requires traded factors |\n\n: Summary of testing approaches for factor models. {#tbl-ts-cs-summary-tests}\n\nThe gap between time-series and cross-sectional evidence is not a technicality—it reflects a fundamental ambiguity in what \"a factor model works\" means. In a market like Vietnam, where the cross-section is small, betas are time-varying, and test assets are inherently noisy, the two approaches will often disagree. The honest researcher reports both, investigates the source of disagreement, and treats any single test result with appropriate humility.\n\n<!-- ## Exercises {#sec-ts-cs-exercises}\n\n1.  **Characteristic vs. covariance.** Run Fama-MacBeth regressions that include both estimated factor betas *and* the raw characteristics (log market cap, book-to-market, past returns) as independent variables. Which has more explanatory power—the betas or the characteristics? This tests the @goyal2018cross hypothesis that characteristics dominate covariances in explaining the cross-section.\n\n2.  **Spanning with anomaly portfolios.** Use the 10 highest-t-statistic anomalies from the factor zoo chapter as test assets (instead of size-BM sorted portfolios). Run GRS tests for the CAPM, FF3, and FF5 models. Which anomalies remain unspanned?\n\n3.  **Conditional Fama-MacBeth.** Interact factor betas with a conditioning variable (VIX, VN-Index volatility, or the SBV policy rate) to create time-varying risk premia. Does the conditional model outperform the unconditional model in cross-sectional $R^2$?\n\n4.  **Bootstrap GRS.** The GRS test assumes multivariate normality of returns. Implement a bootstrap version: resample the time series 10,000 times and compute the empirical distribution of the GRS statistic under the null. Compare the bootstrap p-value to the asymptotic p-value. How much do they differ for Vietnamese data?\n\n5.  **Instrumented PCA.** Implement the @kelly2019characteristics instrumented PCA (IPCA) model, which allows betas to vary with observable characteristics. Estimate the latent factors and their time-varying loadings. How many factors does IPCA select for the Vietnamese market?\n\n6.  **Test asset sensitivity.** Run the full battery of tests (GRS, Fama-MacBeth, GMM) separately for three test asset sets: (a) 25 size-BM, (b) 25 size-momentum, (c) 25 size-profitability. Do the same models win across all test asset sets, or is model ranking test-asset-dependent?\n\n7.  **Small-sample adjustment.** With only \\~200 months of Vietnamese data, small-sample bias in the GRS test may be severe. Implement the @kan2013pricing finite-sample correction. How does the corrected GRS compare to the asymptotic version?\n\n8.  **Global vs. local factors.** Augment the Vietnamese FF5 model with global factors (MSCI World excess return, global SMB, global HML from Kenneth French's data library). Run GRS tests with and without the global factors. Does adding global factors improve pricing of Vietnamese portfolios, as @griffin2002fama would predict? -->\n\n",
    "supporting": [
      "26_time_series_vs_cross_section_files/figure-pdf"
    ],
    "filters": []
  }
}