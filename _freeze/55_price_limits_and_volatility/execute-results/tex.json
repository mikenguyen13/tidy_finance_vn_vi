{
  "hash": "e48b21151265dd0a0f40c6d300fc4167",
  "result": {
    "engine": "jupyter",
    "markdown": "# Price Limits and Volatility\n\n::: callout-note\nIn this chapter, we examine how Vietnam's daily price limit regime distorts observed return distributions, biases volatility estimates, and affects the validity of standard asset pricing tests. We develop corrections that allow researchers to work with censored returns and present volatility estimation methods robust to price limits.\n:::\n\nVietnam is one of a handful of active equity markets that still enforce daily price limits on individual stocks. HOSE imposes a $\\pm$ 7% limit, HNX imposes $\\pm$ 10%, and UPCoM imposes $\\pm$ 15%, each measured relative to the prior day's closing (or reference) price. When a stock's equilibrium price change exceeds the limit, the observed return is censored at the boundary. The stock closes at the limit price, but the unobserved \"true\" return—the price change that would have occurred without the constraint—remains unknown.\n\nThis censoring has pervasive consequences for empirical finance. Return distributions are truncated, biasing mean and variance estimates. Volatility models that ignore censoring understate true risk. Factor betas are attenuated. Event study abnormal returns are compressed. Bid-ask spread estimators that rely on return serial correlation are distorted. Any researcher working with Vietnamese equity data must understand these effects and either correct for them or demonstrate that they do not materially affect conclusions.\n\nPrice limits exist for a stated policy purpose: to prevent panic selling and speculative excess, thereby \"cooling\" the market during periods of stress [@brennan1986theory]. Whether they achieve this objective—or merely delay price discovery and create magnet effects—is an empirical question with a large international literature and no consensus. We examine the Vietnamese evidence.\n\n## The Vietnamese Price Limit Regime {#sec-price-limit-regime}\n\n### Institutional Details\n\n::: {#setup .cell execution_count=2}\n``` {.python .cell-code code-summary=\"Import libraries and configure environment\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats, optimize\nfrom arch import arch_model\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n```\n:::\n\n\nThe price limit structure has evolved over time. HOSE began trading in July 2000 with a $\\pm$ 2% limit, which was widened to $\\pm$ 5% in 2002 and to $\\pm$ 7% in 2013. HNX has operated at $\\pm$ 10% since its current form, and UPCoM at $\\pm$ 15% @tbl-price-limit-limits. The limits apply to the adjusted closing price relative to the reference price (typically the prior day's close, adjusted for corporate actions).\n\n| Exchange | Current Limit | Effective Date | Prior Limits                       |\n|------------------|------------------|------------------|-------------------|\n| HOSE     | $\\pm$ 7%      | June 2013      | $\\pm$ 2% (2000), $\\pm$ 5% (2002)   |\n| HNX      | $\\pm$ 10%     | —              | Various, stabilized at $\\pm$ 10%   |\n| UPCoM    | $\\pm$ 15%     | —              | Wider limits reflecting OTC nature |\n\n: Vietnamese daily price limit regime by exchange. {#tbl-price-limit-limits}\n\nImportantly, the limits are *asymmetric in practice*: they apply equally to up and down moves, but the economic consequences differ. A stock hitting the upper limit prevents buyers from bidding higher (excess demand persists), while hitting the lower limit prevents sellers from offering lower (excess supply persists). Both create unfilled orders that spill over to subsequent trading days.\n\n::: {#data-load .cell execution_count=3}\n``` {.python .cell-code code-summary=\"Load daily price data with limit hit indicators\"}\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Daily data with high, low, open, close, volume, and limit indicators\ndaily = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'date', 'exchange',\n        'open', 'high', 'low', 'close', 'adjusted_close',\n        'reference_price', 'ceiling_price', 'floor_price',\n        'volume', 'turnover_value',\n        'limit_up_hit', 'limit_down_hit'\n    ]\n)\n\ndaily['date'] = pd.to_datetime(daily['date'])\ndaily = daily.sort_values(['ticker', 'date'])\n\n# Compute daily returns\ndaily['daily_return'] = daily.groupby('ticker')['adjusted_close'].pct_change()\n\n# Flag limit hits from price data if not provided\nif 'limit_up_hit' not in daily.columns or daily['limit_up_hit'].isna().all():\n    daily['limit_up_hit'] = (daily['close'] >= daily['ceiling_price'])\n    daily['limit_down_hit'] = (daily['close'] <= daily['floor_price'])\n\n# Exchange-specific limits\nexchange_limits = {'HOSE': 0.07, 'HNX': 0.10, 'UPCoM': 0.15}\ndaily['limit_pct'] = daily['exchange'].map(exchange_limits)\n\nprint(f\"Daily observations: {len(daily):,}\")\nprint(f\"Date range: {daily['date'].min()} to {daily['date'].max()}\")\nprint(f\"Unique tickers: {daily['ticker'].nunique()}\")\n```\n:::\n\n\n## Prevalence of Limit Hits {#sec-price-limit-prevalence}\n\n### Aggregate Frequency\n\nHow often do Vietnamese stocks hit their price limits? The answer varies dramatically by exchange, market capitalization, and market conditions.\n\n::: {#limit-frequency .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Compute limit hit frequencies by exchange and over time\"}\n# Overall frequencies\nlimit_stats = daily.groupby('exchange').agg(\n    n_obs=('daily_return', 'count'),\n    n_up=('limit_up_hit', 'sum'),\n    n_down=('limit_down_hit', 'sum'),\n).assign(\n    pct_up=lambda x: x['n_up'] / x['n_obs'] * 100,\n    pct_down=lambda x: x['n_down'] / x['n_obs'] * 100,\n    pct_either=lambda x: (x['n_up'] + x['n_down']) / x['n_obs'] * 100\n)\n\nprint(\"Limit Hit Frequencies by Exchange:\")\nprint(limit_stats[['pct_up', 'pct_down', 'pct_either']].round(2).to_string())\n\n# Monthly aggregate: fraction of stock-days hitting limits\ndaily['year_month'] = daily['date'].dt.to_period('M')\nmonthly_limit = (\n    daily.groupby(['year_month', 'exchange'])\n    .agg(\n        n_obs=('daily_return', 'count'),\n        n_up=('limit_up_hit', 'sum'),\n        n_down=('limit_down_hit', 'sum')\n    )\n    .assign(\n        pct_up=lambda x: x['n_up'] / x['n_obs'] * 100,\n        pct_down=lambda x: x['n_down'] / x['n_obs'] * 100,\n        pct_any=lambda x: (x['n_up'] + x['n_down']) / x['n_obs'] * 100\n    )\n    .reset_index()\n)\nmonthly_limit['date'] = monthly_limit['year_month'].dt.to_timestamp()\n```\n:::\n\n\n::: {#fig-limit-timeseries .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Plot limit hit frequency over time\"}\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True,\n                          gridspec_kw={'height_ratios': [3, 1]})\n\nhose_monthly = monthly_limit[monthly_limit['exchange'] == 'HOSE']\n\naxes[0].fill_between(hose_monthly['date'], 0, hose_monthly['pct_up'],\n                      color='#27AE60', alpha=0.6, label='Upper limit hits')\naxes[0].fill_between(hose_monthly['date'], 0, -hose_monthly['pct_down'],\n                      color='#C0392B', alpha=0.6, label='Lower limit hits')\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].set_ylabel('% of Stock-Days')\naxes[0].set_title('Panel A: HOSE Daily Price Limit Hits')\naxes[0].legend(loc='upper left')\n\n# VN-Index for context\nvnindex = client.get_index_returns(\n    index='VNINDEX', start_date='2008-01-01', end_date='2024-12-31',\n    frequency='monthly'\n)\nvnindex['date'] = pd.to_datetime(vnindex['date'])\naxes[1].bar(vnindex['date'], vnindex['return'] * 100,\n            color=np.where(vnindex['return'] > 0, '#27AE60', '#C0392B'),\n            width=25, alpha=0.7)\naxes[1].set_ylabel('VN-Index (%)')\naxes[1].set_title('Panel B: VN-Index Monthly Returns')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### By Market Capitalization\n\n::: {#fig-price-limit-by-size .cell execution_count=6}\n``` {.python .cell-code code-summary=\"Compute limit hit rates by size quintile\"}\n# Merge with lagged market cap\nmonthly_mcap = client.get_monthly_returns(\n    exchanges=['HOSE'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    fields=['ticker', 'month_end', 'market_cap']\n)\nmonthly_mcap['month_end'] = pd.to_datetime(monthly_mcap['month_end'])\n\n# Assign size quintiles each month\nmonthly_mcap['size_quintile'] = (\n    monthly_mcap.groupby('month_end')['market_cap']\n    .transform(lambda x: pd.qcut(x.rank(method='first'), 5,\n                                   labels=['Q1\\n(Small)', 'Q2', 'Q3', 'Q4',\n                                           'Q5\\n(Big)']))\n)\n\n# Map to daily\ndaily_hose = daily[daily['exchange'] == 'HOSE'].copy()\ndaily_hose['month_end'] = daily_hose['date'].dt.to_period('M').dt.to_timestamp('M')\ndaily_hose = daily_hose.merge(\n    monthly_mcap[['ticker', 'month_end', 'size_quintile']],\n    on=['ticker', 'month_end'], how='left'\n)\n\nsize_limit = (\n    daily_hose.dropna(subset=['size_quintile'])\n    .groupby('size_quintile')\n    .agg(\n        pct_up=('limit_up_hit', 'mean'),\n        pct_down=('limit_down_hit', 'mean'),\n        n=('daily_return', 'count')\n    )\n)\nsize_limit[['pct_up', 'pct_down']] *= 100\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nx = np.arange(len(size_limit))\nwidth = 0.35\nax.bar(x - width / 2, size_limit['pct_up'], width,\n       color='#27AE60', alpha=0.85, label='Upper limit', edgecolor='white')\nax.bar(x + width / 2, size_limit['pct_down'], width,\n       color='#C0392B', alpha=0.85, label='Lower limit', edgecolor='white')\n\nax.set_xticks(x)\nax.set_xticklabels(size_limit.index)\nax.set_ylabel('% of Stock-Days')\nax.set_title('Price Limit Hit Frequency by Size Quintile (HOSE)')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Consecutive Limit Days\n\nA single limit hit might simply reflect a large information event that is absorbed within one day. Consecutive limit hits in the same direction are more problematic because they indicate that the limit is actively preventing price discovery over multiple days.\n\n::: {#consecutive-limits .cell execution_count=7}\n``` {.python .cell-code code-summary=\"Analyze consecutive limit hit sequences\"}\ndef count_consecutive_limits(group):\n    \"\"\"Count consecutive limit-up and limit-down sequences.\"\"\"\n    up_runs = []\n    down_runs = []\n    \n    up_count = 0\n    down_count = 0\n    \n    for _, row in group.iterrows():\n        if row['limit_up_hit']:\n            up_count += 1\n            if down_count > 0:\n                down_runs.append(down_count)\n                down_count = 0\n        elif row['limit_down_hit']:\n            down_count += 1\n            if up_count > 0:\n                up_runs.append(up_count)\n                up_count = 0\n        else:\n            if up_count > 0:\n                up_runs.append(up_count)\n            if down_count > 0:\n                down_runs.append(down_count)\n            up_count = 0\n            down_count = 0\n    \n    if up_count > 0:\n        up_runs.append(up_count)\n    if down_count > 0:\n        down_runs.append(down_count)\n    \n    return up_runs, down_runs\n\n# Sample: compute for HOSE stocks\nhose_tickers = daily_hose['ticker'].unique()\nall_up_runs = []\nall_down_runs = []\n\nfor ticker in hose_tickers:\n    group = daily_hose[daily_hose['ticker'] == ticker].sort_values('date')\n    up_runs, down_runs = count_consecutive_limits(group)\n    all_up_runs.extend(up_runs)\n    all_down_runs.extend(down_runs)\n\nprint(\"Consecutive Limit Hit Distribution (HOSE):\")\nfor direction, runs in [('Upper', all_up_runs), ('Lower', all_down_runs)]:\n    if not runs:\n        continue\n    runs_series = pd.Series(runs)\n    print(f\"\\n  {direction} limit sequences:\")\n    print(f\"    Total sequences: {len(runs_series):,}\")\n    print(f\"    1 day:  {(runs_series == 1).sum():,} ({(runs_series == 1).mean():.1%})\")\n    print(f\"    2 days: {(runs_series == 2).sum():,} ({(runs_series == 2).mean():.1%})\")\n    print(f\"    3 days: {(runs_series == 3).sum():,} ({(runs_series == 3).mean():.1%})\")\n    print(f\"    4+ days: {(runs_series >= 4).sum():,} ({(runs_series >= 4).mean():.1%})\")\n    print(f\"    Max consecutive: {runs_series.max()}\")\n```\n:::\n\n\n## Return Distribution Distortion {#sec-price-limit-distortion}\n\n### Censoring Mechanics\n\nPrice limits create *Type I censoring* (also called \"truncation at a known point\"): the latent (unobserved) return $r^*$ is generated from some continuous distribution, but the observed return is:\n\n$$\nr^{\\text{obs}} = \\begin{cases}\n\\bar{L} & \\text{if } r^* \\geq \\bar{L} \\quad \\text{(upper limit hit)} \\\\\nr^* & \\text{if } \\underline{L} < r^* < \\bar{L} \\quad \\text{(interior)} \\\\\n\\underline{L} & \\text{if } r^* \\leq \\underline{L} \\quad \\text{(lower limit hit)}\n\\end{cases}\n$$ {#eq-censoring}\n\nwhere $\\bar{L}$ and $\\underline{L}$ are the upper and lower limits. For HOSE, $\\bar{L} = +0.07$ and $\\underline{L} = -0.07$.\n\nThe censoring has predictable effects on the observed distribution:\n\n1.  **Mean bias.** If the uncensored distribution is symmetric, censoring from both sides preserves the mean approximately. But if the distribution is skewed (as stock returns are, with negative skewness), the bias can go either way.\n2.  **Variance underestimation.** Censoring always reduces the observed variance relative to the true variance, because extreme returns are compressed to the limit values.\n3.  **Kurtosis distortion.** Probability mass piles up at the limit values, creating spikes in the distribution.\n\n::: {#fig-return-distribution .cell execution_count=8}\n``` {.python .cell-code code-summary=\"Visualize the effect of price limits on the return distribution\"}\nhose_returns = daily_hose['daily_return'].dropna()\nhose_returns = hose_returns[hose_returns.abs() < 0.15]  # Remove data errors\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Full distribution\naxes[0].hist(hose_returns, bins=200, density=True,\n             color='#2C5F8A', alpha=0.7, edgecolor='none')\naxes[0].axvline(x=0.07, color='#C0392B', linewidth=2, linestyle='--',\n                label='$\\pm$ 7% limit')\naxes[0].axvline(x=-0.07, color='#C0392B', linewidth=2, linestyle='--')\naxes[0].set_xlabel('Daily Return')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Panel A: HOSE Daily Return Distribution')\naxes[0].legend()\n\n# Panel B: Zoom on tails\nbins_tail = np.linspace(-0.09, -0.05, 40)\nbins_tail_up = np.linspace(0.05, 0.09, 40)\n\naxes[1].hist(hose_returns[hose_returns < -0.04], bins=80, density=True,\n             color='#C0392B', alpha=0.6, label='Left tail')\naxes[1].hist(hose_returns[hose_returns > 0.04], bins=80, density=True,\n             color='#27AE60', alpha=0.6, label='Right tail')\naxes[1].axvline(x=0.07, color='black', linewidth=2)\naxes[1].axvline(x=-0.07, color='black', linewidth=2)\naxes[1].set_xlabel('Daily Return')\naxes[1].set_ylabel('Density')\naxes[1].set_title('Panel B: Tail Behavior at Limits')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Quantify the spike\nn_at_upper = ((hose_returns >= 0.069) & (hose_returns <= 0.071)).sum()\nn_at_lower = ((hose_returns >= -0.071) & (hose_returns <= -0.069)).sum()\nn_total = len(hose_returns)\nprint(f\"Observations at upper limit ($\\pm$ 0.1% of 7%): {n_at_upper:,} \"\n      f\"({n_at_upper/n_total:.2%})\")\nprint(f\"Observations at lower limit: {n_at_lower:,} \"\n      f\"({n_at_lower/n_total:.2%})\")\n```\n:::\n\n\n### Comparing HOSE vs. HNX vs. UPCoM\n\nThe three Vietnamese exchanges have different limit widths, creating a natural experiment: if limits distort the distribution, wider limits should produce distributions closer to the uncensored benchmark.\n\n::: {#fig-cross-exchange .cell execution_count=9}\n``` {.python .cell-code code-summary=\"Compare return distributions across exchanges\"}\nfig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n\nfor i, (exchange, limit, color) in enumerate([\n    ('HOSE', 0.07, '#2C5F8A'),\n    ('HNX', 0.10, '#C0392B'),\n    ('UPCoM', 0.15, '#27AE60')\n]):\n    rets = daily[daily['exchange'] == exchange]['daily_return'].dropna()\n    rets = rets[rets.abs() < limit + 0.05]\n    \n    axes[i].hist(rets, bins=150, density=True,\n                  color=color, alpha=0.7, edgecolor='none')\n    axes[i].axvline(x=limit, color='black', linewidth=1.5, linestyle='--')\n    axes[i].axvline(x=-limit, color='black', linewidth=1.5, linestyle='--')\n    axes[i].set_title(f'{exchange} ($\\pm$ {limit*100:.0f}%)')\n    axes[i].set_xlabel('Daily Return')\n    if i == 0:\n        axes[i].set_ylabel('Density')\n    \n    # Stats\n    pct_at_limit = ((rets.abs() >= limit - 0.001).sum() / len(rets) * 100)\n    axes[i].text(0.95, 0.95, f'At limit: {pct_at_limit:.2f}%',\n                  transform=axes[i].transAxes, ha='right', va='top',\n                  fontsize=9, bbox=dict(boxstyle='round', facecolor='white',\n                                         alpha=0.8))\n\nplt.suptitle('Return Distributions by Exchange', fontsize=13)\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Variance Bias from Censoring {#sec-price-limit-variance-bias}\n\n### Analytical Bias\n\nIf the true return follows $r^* \\sim N(\\mu, \\sigma^2)$, the variance of the censored return can be derived analytically. Let $a = (\\underline{L} - \\mu) / \\sigma$ and $b = (\\bar{L} - \\mu) / \\sigma$:\n\n$$\n\\text{Var}(r^{\\text{obs}}) = \\sigma^2 \\left[1 - \\frac{b \\phi(b) - a \\phi(a)}{\\Phi(b) - \\Phi(a)} - \\left(\\frac{\\phi(a) - \\phi(b)}{\\Phi(b) - \\Phi(a)}\\right)^2 \\right] + \\text{boundary terms}\n$$ {#eq-censored-variance}\n\nwhere $\\phi$ and $\\Phi$ are the standard normal PDF and CDF. The key result is that $\\text{Var}(r^{\\text{obs}}) < \\sigma^2$ always—censoring systematically underestimates variance.\n\n::: {#variance-bias-simulation .cell execution_count=10}\n``` {.python .cell-code code-summary=\"Simulate the variance bias from censoring under different volatility levels\"}\ndef simulate_censored_variance(true_sigma, limit, n_sim=100000, mu=0):\n    \"\"\"Simulate observed vs true variance under censoring.\"\"\"\n    rng = np.random.default_rng(42)\n    r_star = rng.normal(mu, true_sigma, n_sim)\n    r_obs = np.clip(r_star, -limit, limit)\n    \n    var_true = np.var(r_star)\n    var_obs = np.var(r_obs)\n    \n    pct_censored = ((r_star >= limit) | (r_star <= -limit)).mean()\n    \n    return {\n        'true_sigma': true_sigma,\n        'true_var': var_true,\n        'obs_var': var_obs,\n        'var_ratio': var_obs / var_true,\n        'bias_pct': (1 - var_obs / var_true) * 100,\n        'pct_censored': pct_censored * 100\n    }\n\n# Sweep across volatility levels for each exchange limit\nresults_bias = []\nsigmas = np.linspace(0.005, 0.08, 50)\n\nfor limit_name, limit in [('HOSE $\\pm$ 7%', 0.07), ('HNX $\\pm$ 10%', 0.10),\n                            ('UPCoM $\\pm$ 15%', 0.15)]:\n    for sigma in sigmas:\n        res = simulate_censored_variance(sigma, limit)\n        res['exchange'] = limit_name\n        results_bias.append(res)\n\nbias_df = pd.DataFrame(results_bias)\n```\n:::\n\n\n::: {#fig-variance-bias .cell execution_count=11}\n``` {.python .cell-code code-summary=\"Plot variance bias across volatility levels\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ncolors_exch = {'HOSE $\\pm$ 7%': '#2C5F8A', 'HNX $\\pm$ 10%': '#C0392B',\n                'UPCoM $\\pm$ 15%': '#27AE60'}\n\nfor exch in colors_exch:\n    subset = bias_df[bias_df['exchange'] == exch]\n    axes[0].plot(subset['true_sigma'] * 100, subset['var_ratio'],\n                  color=colors_exch[exch], linewidth=2, label=exch)\n\naxes[0].axhline(y=1, color='gray', linewidth=0.5, linestyle='--')\naxes[0].set_xlabel('True Daily Volatility (%)')\naxes[0].set_ylabel('Observed / True Variance')\naxes[0].set_title('Panel A: Variance Ratio')\naxes[0].legend()\naxes[0].set_ylim([0.5, 1.05])\n\nfor exch in colors_exch:\n    subset = bias_df[bias_df['exchange'] == exch]\n    axes[1].plot(subset['true_sigma'] * 100, subset['pct_censored'],\n                  color=colors_exch[exch], linewidth=2, label=exch)\n\naxes[1].set_xlabel('True Daily Volatility (%)')\naxes[1].set_ylabel('% of Returns Censored')\naxes[1].set_title('Panel B: Censoring Rate')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Empirical Variance Bias by Size\n\n::: {#empirical-bias .cell execution_count=12}\n``` {.python .cell-code code-summary=\"Estimate the empirical variance bias by comparing HOSE stocks that transferred from HNX\"}\n# Cross-listed stocks or transfer events provide a natural experiment:\n# Same stock, different limit regime\n# Alternative: compare variance of HOSE returns to variance of the same\n# stock's returns implied from intraday data (not censored by closing limit)\n\n# Approach: Tobit-based variance estimation\n# Model observed returns as censored normal\ndef tobit_variance(returns, limit):\n    \"\"\"\n    Estimate true variance via Tobit MLE under censored normal.\n    \"\"\"\n    r = returns.dropna().values\n    upper = limit\n    lower = -limit\n    \n    # Classify observations\n    at_upper = r >= (upper - 1e-6)\n    at_lower = r <= (lower + 1e-6)\n    interior = ~at_upper & ~at_lower\n    \n    if interior.sum() < 20:\n        return np.nan, np.nan\n    \n    def neg_loglik(params):\n        mu, log_sigma = params\n        sigma = np.exp(log_sigma)\n        \n        ll = 0\n        # Interior observations\n        if interior.sum() > 0:\n            ll += np.sum(stats.norm.logpdf(r[interior], mu, sigma))\n        # Upper censored\n        if at_upper.sum() > 0:\n            ll += np.sum(np.log(1 - stats.norm.cdf(upper, mu, sigma) + 1e-15))\n        # Lower censored\n        if at_lower.sum() > 0:\n            ll += np.sum(np.log(stats.norm.cdf(lower, mu, sigma) + 1e-15))\n        \n        return -ll\n    \n    # Initial values\n    mu0 = r[interior].mean() if interior.sum() > 0 else 0\n    sigma0 = r[interior].std() if interior.sum() > 0 else r.std()\n    \n    try:\n        result = optimize.minimize(\n            neg_loglik, [mu0, np.log(max(sigma0, 1e-6))],\n            method='Nelder-Mead', options={'maxiter': 5000}\n        )\n        mu_hat = result.x[0]\n        sigma_hat = np.exp(result.x[1])\n        return mu_hat, sigma_hat\n    except Exception:\n        return np.nan, np.nan\n\n# Estimate for each HOSE stock\nhose_stocks = daily_hose.groupby('ticker').filter(\n    lambda x: len(x) >= 250\n)['ticker'].unique()\n\ntobit_results = []\nfor ticker in hose_stocks[:500]:  # Sample for speed\n    rets = daily_hose[daily_hose['ticker'] == ticker]['daily_return'].dropna()\n    if len(rets) < 250:\n        continue\n    \n    naive_sigma = rets.std()\n    mu_hat, sigma_hat = tobit_variance(rets, 0.07)\n    \n    if np.isfinite(sigma_hat) and sigma_hat > 0:\n        tobit_results.append({\n            'ticker': ticker,\n            'naive_sigma': naive_sigma,\n            'tobit_sigma': sigma_hat,\n            'bias_pct': (sigma_hat - naive_sigma) / naive_sigma * 100\n        })\n\ntobit_df = pd.DataFrame(tobit_results)\n\nprint(\"Tobit vs Naive Volatility Estimation (HOSE):\")\nprint(f\"  Mean naive σ:  {tobit_df['naive_sigma'].mean():.4f}\")\nprint(f\"  Mean Tobit σ:  {tobit_df['tobit_sigma'].mean():.4f}\")\nprint(f\"  Mean bias:     {tobit_df['bias_pct'].mean():.1f}%\")\nprint(f\"  Median bias:   {tobit_df['bias_pct'].median():.1f}%\")\nprint(f\"  Max bias:      {tobit_df['bias_pct'].max():.1f}%\")\n```\n:::\n\n\n::: {#fig-tobit-vs-naive .cell execution_count=13}\n``` {.python .cell-code code-summary=\"Plot Tobit vs naive volatility comparison\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].scatter(tobit_df['naive_sigma'] * 100,\n                 tobit_df['tobit_sigma'] * 100,\n                 s=15, alpha=0.5, color='#2C5F8A', edgecolors='none')\nlim = max(tobit_df['tobit_sigma'].max(), tobit_df['naive_sigma'].max()) * 100 + 0.5\naxes[0].plot([0, lim], [0, lim], 'k--', linewidth=1)\naxes[0].set_xlabel('Naive σ (% daily)')\naxes[0].set_ylabel('Tobit σ (% daily)')\naxes[0].set_title('Panel A: Tobit vs Naive Volatility')\n\naxes[1].hist(tobit_df['bias_pct'], bins=50, color='#C0392B',\n             alpha=0.7, edgecolor='white', density=True)\naxes[1].axvline(x=0, color='black', linewidth=1)\naxes[1].axvline(x=tobit_df['bias_pct'].median(), color='#2C5F8A',\n                linewidth=2, linestyle='--',\n                label=f\"Median: {tobit_df['bias_pct'].median():.1f}%\")\naxes[1].set_xlabel('Bias (%): (Tobit - Naive) / Naive')\naxes[1].set_ylabel('Density')\naxes[1].set_title('Panel B: Distribution of Correction')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Volatility Estimation Under Price Limits {#sec-price-limit-vol-estimation}\n\n### Range-Based Estimators\n\nRange-based volatility estimators use the daily high and low prices rather than close-to-close returns, making them partially robust to closing-price censoring (since intraday prices may approach but not be censored at the same points). However, they are biased when the *intraday* price trajectory itself is constrained by the limits.\n\n::: {#range-estimators .cell execution_count=14}\n``` {.python .cell-code code-summary=\"Implement range-based volatility estimators\"}\ndef parkinson_vol(high, low, n_periods=20):\n    \"\"\"\n    Parkinson (1980) range-based volatility estimator.\n    σ² = (1/4ln2) * E[(ln(H/L))²]\n    \"\"\"\n    log_hl = np.log(high / low)\n    var = (1 / (4 * np.log(2))) * (log_hl ** 2)\n    return np.sqrt(var.rolling(n_periods).mean())\n\ndef garman_klass_vol(open_p, high, low, close, n_periods=20):\n    \"\"\"\n    Garman-Klass (1980) OHLC volatility estimator.\n    More efficient than Parkinson by using open and close.\n    \"\"\"\n    log_hl = np.log(high / low)\n    log_co = np.log(close / open_p)\n    var = 0.5 * log_hl ** 2 - (2 * np.log(2) - 1) * log_co ** 2\n    return np.sqrt(var.rolling(n_periods).mean())\n\ndef yang_zhang_vol(open_p, high, low, close, n_periods=20):\n    \"\"\"\n    Yang-Zhang (2000) drift-independent estimator.\n    Combines overnight, Rogers-Satchell, and open-to-close components.\n    \"\"\"\n    log_oc = np.log(open_p / close.shift(1))  # Overnight\n    log_co = np.log(close / open_p)\n    log_ho = np.log(high / open_p)\n    log_lo = np.log(low / open_p)\n    \n    # Rogers-Satchell component\n    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n    \n    k = 0.34 / (1.34 + (n_periods + 1) / (n_periods - 1))\n    \n    var_overnight = log_oc.rolling(n_periods).var()\n    var_open_close = log_co.rolling(n_periods).var()\n    var_rs = rs.rolling(n_periods).mean()\n    \n    var = var_overnight + k * var_open_close + (1 - k) * var_rs\n    return np.sqrt(var.clip(lower=0))\n\n# Compute for HOSE sample\nsample_ticker = 'VNM'  # Large, liquid stock\nsample = daily_hose[daily_hose['ticker'] == sample_ticker].copy()\nsample = sample.sort_values('date').set_index('date')\n\n# Close-to-close realized vol\nsample['cc_vol'] = sample['daily_return'].rolling(20).std() * np.sqrt(252)\n\n# Range-based\nsample['parkinson'] = parkinson_vol(\n    sample['high'], sample['low'], 20\n) * np.sqrt(252)\n\nsample['garman_klass'] = garman_klass_vol(\n    sample['open'], sample['high'], sample['low'], sample['close'], 20\n) * np.sqrt(252)\n\nsample['yang_zhang'] = yang_zhang_vol(\n    sample['open'], sample['high'], sample['low'], sample['close'], 20\n) * np.sqrt(252)\n```\n:::\n\n\n::: {#fig-vol-estimators .cell execution_count=15}\n``` {.python .cell-code code-summary=\"Plot and compare volatility estimators over time\"}\nfig, ax = plt.subplots(figsize=(14, 5))\n\nax.plot(sample.index, sample['cc_vol'], color='#BDC3C7',\n        linewidth=1, label='Close-to-Close', alpha=0.8)\nax.plot(sample.index, sample['parkinson'], color='#2C5F8A',\n        linewidth=1.5, label='Parkinson')\nax.plot(sample.index, sample['yang_zhang'], color='#C0392B',\n        linewidth=1.5, label='Yang-Zhang')\n\nax.set_ylabel('Annualized Volatility')\nax.set_title(f'Volatility Estimators: {sample_ticker}')\nax.legend(ncol=3)\nax.set_ylim([0, ax.get_ylim()[1]])\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### GARCH Models with Censored Returns\n\nStandard GARCH models assume returns are fully observed. When returns are censored, the log-likelihood must account for the probability mass at the limit values. We implement a censored GARCH(1,1):\n\n$$\nr_t = \\mu + \\varepsilon_t, \\quad \\varepsilon_t = \\sigma_t z_t, \\quad z_t \\sim N(0, 1)\n$$ {#eq-garch-mean}\n\n$$\n\\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n$$ {#eq-garch-variance}\n\nThe censored log-likelihood replaces the standard normal density for limit-hit observations:\n\n$$\n\\ell_t = \\begin{cases}\n\\log \\phi\\left(\\frac{r_t - \\mu}{\\sigma_t}\\right) - \\log \\sigma_t & \\text{if interior} \\\\\n\\log \\Phi\\left(\\frac{\\underline{L} - \\mu}{\\sigma_t}\\right) & \\text{if lower limit} \\\\\n\\log\\left[1 - \\Phi\\left(\\frac{\\bar{L} - \\mu}{\\sigma_t}\\right)\\right] & \\text{if upper limit}\n\\end{cases}\n$$ {#eq-censored-likelihood}\n\n::: {#censored-garch .cell execution_count=16}\n``` {.python .cell-code code-summary=\"Implement censored GARCH(1,1) estimation\"}\ndef censored_garch11(returns, limit, max_iter=500):\n    \"\"\"\n    GARCH(1,1) with censored normal likelihood.\n    \n    Parameters\n    ----------\n    returns : array-like\n        Observed daily returns (censored at $\\pm$ limit).\n    limit : float\n        Price limit (e.g., 0.07 for HOSE).\n    \n    Returns\n    -------\n    Dictionary with estimated parameters and conditional variances.\n    \"\"\"\n    r = np.array(returns, dtype=float)\n    T = len(r)\n    upper = limit\n    lower = -limit\n    \n    at_upper = r >= (upper - 1e-6)\n    at_lower = r <= (lower + 1e-6)\n    interior = ~at_upper & ~at_lower\n    \n    def neg_loglik(params):\n        mu, omega, alpha, beta = params\n        \n        if omega <= 0 or alpha < 0 or beta < 0 or (alpha + beta) >= 1:\n            return 1e10\n        \n        sigma2 = np.zeros(T)\n        sigma2[0] = omega / (1 - alpha - beta) if (alpha + beta) < 1 else r.var()\n        \n        ll = 0\n        for t in range(T):\n            if t > 0:\n                eps = r[t - 1] - mu\n                sigma2[t] = omega + alpha * eps ** 2 + beta * sigma2[t - 1]\n            \n            sigma2[t] = max(sigma2[t], 1e-10)\n            sigma = np.sqrt(sigma2[t])\n            \n            if interior[t]:\n                ll += stats.norm.logpdf(r[t], mu, sigma)\n            elif at_upper[t]:\n                prob = 1 - stats.norm.cdf(upper, mu, sigma)\n                ll += np.log(max(prob, 1e-15))\n            elif at_lower[t]:\n                prob = stats.norm.cdf(lower, mu, sigma)\n                ll += np.log(max(prob, 1e-15))\n        \n        return -ll\n    \n    # Initial values from standard GARCH\n    mu0 = r[interior].mean() if interior.any() else 0\n    var0 = r[interior].var() if interior.any() else r.var()\n    \n    try:\n        result = optimize.minimize(\n            neg_loglik,\n            [mu0, var0 * 0.05, 0.10, 0.85],\n            method='Nelder-Mead',\n            options={'maxiter': max_iter, 'xatol': 1e-8}\n        )\n        mu, omega, alpha, beta = result.x\n        \n        # Reconstruct conditional variance\n        sigma2 = np.zeros(T)\n        sigma2[0] = omega / max(1 - alpha - beta, 0.01)\n        for t in range(1, T):\n            eps = r[t - 1] - mu\n            sigma2[t] = omega + alpha * eps ** 2 + beta * sigma2[t - 1]\n        \n        return {\n            'mu': mu, 'omega': omega, 'alpha': alpha, 'beta': beta,\n            'persistence': alpha + beta,\n            'uncond_var': omega / max(1 - alpha - beta, 0.01),\n            'sigma2': sigma2,\n            'loglik': -result.fun,\n            'converged': result.success,\n            'n_censored': at_upper.sum() + at_lower.sum(),\n            'pct_censored': (at_upper.sum() + at_lower.sum()) / T * 100\n        }\n    except Exception as e:\n        return None\n\n# Compare standard vs censored GARCH for a volatile stock\nvolatile_stock = daily_hose.groupby('ticker')['limit_up_hit'].mean()\nvolatile_stock = volatile_stock.sort_values(ascending=False).head(20)\ntest_ticker = volatile_stock.index[0]\n\ntest_returns = (\n    daily_hose[daily_hose['ticker'] == test_ticker]\n    .sort_values('date')['daily_return']\n    .dropna()\n    .values\n)\n\n# Standard GARCH (arch library)\nstd_garch = arch_model(test_returns * 100, vol='GARCH', p=1, q=1,\n                         mean='Constant', dist='normal')\nstd_result = std_garch.fit(disp='off')\n\n# Censored GARCH\ncens_result = censored_garch11(test_returns, limit=0.07)\n\nprint(f\"Stock: {test_ticker}\")\nprint(f\"Observations: {len(test_returns)}, \"\n      f\"Censored: {cens_result['pct_censored']:.1f}%\\n\")\n\nprint(f\"{'Parameter':<12} {'Standard':>12} {'Censored':>12}\")\nprint(\"-\" * 36)\nprint(f\"{'μ':<12} {std_result.params['mu']/100:>12.6f} \"\n      f\"{cens_result['mu']:>12.6f}\")\nprint(f\"{'ω':<12} {std_result.params['omega']/10000:>12.8f} \"\n      f\"{cens_result['omega']:>12.8f}\")\nprint(f\"{'α':<12} {std_result.params['alpha[1]']:>12.4f} \"\n      f\"{cens_result['alpha']:>12.4f}\")\nprint(f\"{'β':<12} {std_result.params['beta[1]']:>12.4f} \"\n      f\"{cens_result['beta']:>12.4f}\")\nprint(f\"{'α+β':<12} \"\n      f\"{std_result.params['alpha[1]']+std_result.params['beta[1]']:>12.4f} \"\n      f\"{cens_result['persistence']:>12.4f}\")\nprint(f\"{'Uncond σ':<12} \"\n      f\"{np.sqrt(std_result.params['omega']/(1-std_result.params['alpha[1]']-std_result.params['beta[1]'])/10000):>12.4f} \"\n      f\"{np.sqrt(cens_result['uncond_var']):>12.4f}\")\n```\n:::\n\n\n::: {#fig-garch-comparison .cell execution_count=17}\n``` {.python .cell-code code-summary=\"Plot standard vs censored GARCH conditional volatility\"}\ntest_data = daily_hose[daily_hose['ticker'] == test_ticker].sort_values('date')\ndates = test_data['date'].values[-len(test_returns):]\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True,\n                          gridspec_kw={'height_ratios': [1, 2]})\n\n# Panel A: Returns with limit hits highlighted\naxes[0].plot(dates, test_returns, color='#2C5F8A', linewidth=0.5, alpha=0.7)\nlimit_up_mask = test_returns >= 0.069\nlimit_down_mask = test_returns <= -0.069\naxes[0].scatter(dates[limit_up_mask], test_returns[limit_up_mask],\n                 color='#27AE60', s=10, zorder=3, label='Upper limit')\naxes[0].scatter(dates[limit_down_mask], test_returns[limit_down_mask],\n                 color='#C0392B', s=10, zorder=3, label='Lower limit')\naxes[0].axhline(y=0.07, color='gray', linewidth=0.5, linestyle='--')\naxes[0].axhline(y=-0.07, color='gray', linewidth=0.5, linestyle='--')\naxes[0].set_ylabel('Return')\naxes[0].set_title(f'Panel A: Daily Returns ({test_ticker})')\naxes[0].legend(fontsize=8)\n\n# Panel B: Conditional volatility\nstd_sigma = std_result.conditional_volatility / 100  # Convert from % to decimal\ncens_sigma = np.sqrt(cens_result['sigma2'])\n\naxes[1].plot(dates, std_sigma * np.sqrt(252), color='#BDC3C7',\n             linewidth=1, label='Standard GARCH')\naxes[1].plot(dates, cens_sigma * np.sqrt(252), color='#C0392B',\n             linewidth=1.5, label='Censored GARCH')\naxes[1].set_ylabel('Annualized Conditional σ')\naxes[1].set_title('Panel B: Conditional Volatility')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## Effects on Asset Pricing Tests {#sec-price-limit-asset-pricing}\n\n### Beta Attenuation\n\nPrice limits attenuate the covariance between stock returns and factor returns, biasing beta estimates toward zero. The intuition is simple: on days when the market moves 3% but a stock's true return would have been 6%, the observed return is capped at 7%, understating the stock's sensitivity.\n\n::: {#beta-attenuation .cell execution_count=18}\n``` {.python .cell-code code-summary=\"Quantify beta attenuation from price limit censoring\"}\n# Compare betas estimated with all days vs excluding limit-hit days\n# Also compare betas from HOSE stocks vs same stocks if they were on HNX\n\ndaily_hose_merged = daily_hose.merge(\n    client.get_index_returns('VNINDEX', frequency='daily',\n                              start_date='2008-01-01',\n                              end_date='2024-12-31')[['date', 'return']],\n    on='date', how='left'\n).rename(columns={'return': 'mkt_return'})\n\n# For each stock, estimate beta:\n# (a) Using all days\n# (b) Excluding limit-hit days\n# (c) Tobit-corrected (censored regression)\nbeta_comparison = []\n\nfor ticker in hose_stocks[:300]:\n    stock = daily_hose_merged[daily_hose_merged['ticker'] == ticker].dropna(\n        subset=['daily_return', 'mkt_return']\n    )\n    if len(stock) < 250:\n        continue\n    \n    # (a) All days\n    X_all = sm.add_constant(stock['mkt_return'])\n    model_all = sm.OLS(stock['daily_return'], X_all).fit()\n    beta_all = model_all.params['mkt_return']\n    \n    # (b) Exclude limit-hit days\n    interior = stock[~stock['limit_up_hit'] & ~stock['limit_down_hit']]\n    if len(interior) < 200:\n        continue\n    X_int = sm.add_constant(interior['mkt_return'])\n    model_int = sm.OLS(interior['daily_return'], X_int).fit()\n    beta_interior = model_int.params['mkt_return']\n    \n    # Limit hit frequency for this stock\n    pct_limit = (stock['limit_up_hit'].sum() + stock['limit_down_hit'].sum()) / len(stock) * 100\n    \n    beta_comparison.append({\n        'ticker': ticker,\n        'beta_all': beta_all,\n        'beta_interior': beta_interior,\n        'beta_diff_pct': (beta_interior - beta_all) / abs(beta_all) * 100,\n        'pct_limit_hits': pct_limit\n    })\n\nbeta_df = pd.DataFrame(beta_comparison)\n\nprint(\"Beta Attenuation from Price Limits:\")\nprint(f\"  Mean β (all days):      {beta_df['beta_all'].mean():.3f}\")\nprint(f\"  Mean β (interior only): {beta_df['beta_interior'].mean():.3f}\")\nprint(f\"  Mean difference:        {beta_df['beta_diff_pct'].mean():.1f}%\")\nprint(f\"  Correlation(diff, limit_freq): \"\n      f\"{beta_df['beta_diff_pct'].corr(beta_df['pct_limit_hits']):.3f}\")\n```\n:::\n\n\n::: {#fig-beta-attenuation .cell execution_count=19}\n``` {.python .cell-code code-summary=\"Visualize beta attenuation\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].scatter(beta_df['beta_all'], beta_df['beta_interior'],\n                 s=15, alpha=0.5, color='#2C5F8A', edgecolors='none')\nlim = max(beta_df['beta_all'].abs().max(),\n           beta_df['beta_interior'].abs().max()) + 0.2\naxes[0].plot([-0.5, lim], [-0.5, lim], 'k--', linewidth=1)\naxes[0].set_xlabel('β (all days)')\naxes[0].set_ylabel('β (interior only)')\naxes[0].set_title('Panel A: Beta with vs without Limit Days')\n\naxes[1].scatter(beta_df['pct_limit_hits'], beta_df['beta_diff_pct'],\n                 s=15, alpha=0.5, color='#C0392B', edgecolors='none')\n# Add regression line\nz = np.polyfit(beta_df['pct_limit_hits'], beta_df['beta_diff_pct'], 1)\nx_line = np.linspace(0, beta_df['pct_limit_hits'].max(), 100)\naxes[1].plot(x_line, np.polyval(z, x_line), 'k-', linewidth=1.5)\naxes[1].axhline(y=0, color='gray', linewidth=0.5)\naxes[1].set_xlabel('Limit Hit Frequency (%)')\naxes[1].set_ylabel('Beta Increase When Excluding Limit Days (%)')\naxes[1].set_title('Panel B: Attenuation vs Limit Frequency')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n### Effect on Factor Premia\n\nIf betas are attenuated by censoring, then cross-sectional Fama-MacBeth risk premia estimates are biased upward (because the denominator of the slope coefficient is too small). We quantify this effect:\n\n::: {#factor-premium-bias .cell execution_count=20}\n``` {.python .cell-code code-summary=\"Compare factor premia with and without limit-hit days\"}\n# Monthly returns: compute with all days vs excluding limit-hit days\n# Then construct factors under each definition\n\nmonthly_all = (\n    daily_hose.groupby(['ticker', daily_hose['date'].dt.to_period('M')])\n    .agg(\n        ret_all=('daily_return', lambda x: (1 + x).prod() - 1),\n        ret_interior=('daily_return',\n                       lambda x: (1 + x[~x.name.map(\n                           lambda idx: daily_hose.loc[idx, 'limit_up_hit'] |\n                                        daily_hose.loc[idx, 'limit_down_hit']\n                       ).values]).prod() - 1 if len(x) > 0 else np.nan),\n        n_limit_days=('limit_up_hit',\n                       lambda x: x.sum() + daily_hose.loc[x.index, 'limit_down_hit'].sum()),\n        n_trading_days=('daily_return', 'count')\n    )\n    .reset_index()\n)\n\nprint(\"Impact on Monthly Returns:\")\nprint(f\"  Mean monthly return (all days):     \"\n      f\"{monthly_all['ret_all'].mean():.4f}\")\nprint(f\"  Mean monthly return (interior):     \"\n      f\"{monthly_all['ret_interior'].mean():.4f}\")\nprint(f\"  Avg limit days per stock-month:     \"\n      f\"{monthly_all['n_limit_days'].mean():.2f}\")\n```\n:::\n\n\n## The Magnet Effect {#sec-price-limit-magnet}\n\n### Do Limits Attract Prices?\n\nThe *magnet effect* hypothesis posits that price limits, rather than cooling the market, actually attract prices to the limit as traders rush to execute before the stock becomes locked [@cho2003price]. If a stock is approaching the upper limit, buyers accelerate their orders to avoid being shut out, creating a self-fulfilling rush to the boundary.\n\nWe test for the magnet effect by examining the speed of price movement toward the limit conditional on approaching it:\n\n::: {#magnet-effect .cell execution_count=21}\n``` {.python .cell-code code-summary=\"Test for the magnet effect using intraday price trajectories\"}\n# Approach: for days that eventually hit the limit,\n# compare the return in the last hour vs the first hour\n# relative to non-limit days with similar initial trajectories\n\n# Without intraday data, we use a cross-day approach:\n# On day t, if the stock is within X% of the limit at some point,\n# what is the probability of hitting the limit on day t vs day t+1?\n\n# Simpler test: return continuation after near-limit days\ndef magnet_test(daily_df, limit, proximity_threshold=0.8):\n    \"\"\"\n    Test for the magnet effect.\n    \n    For each stock-day, classify:\n    - 'near_limit_up': return in [proximity_threshold * limit, limit)\n    - 'near_limit_down': return in (-limit, -proximity_threshold * limit]\n    - 'hit_limit_up': return = limit\n    - 'hit_limit_down': return = -limit\n    - 'normal': all others\n    \n    Then examine next-day behavior.\n    \"\"\"\n    df = daily_df.copy()\n    df['abs_ret'] = df['daily_return'].abs()\n    \n    df['near_up'] = (df['daily_return'] >= proximity_threshold * limit) & \\\n                     (df['daily_return'] < limit - 0.001)\n    df['near_down'] = (df['daily_return'] <= -proximity_threshold * limit) & \\\n                       (df['daily_return'] > -limit + 0.001)\n    \n    df['next_return'] = df.groupby('ticker')['daily_return'].shift(-1)\n    df['next_limit_up'] = df.groupby('ticker')['limit_up_hit'].shift(-1)\n    df['next_limit_down'] = df.groupby('ticker')['limit_down_hit'].shift(-1)\n    \n    results = {}\n    \n    # Near upper limit\n    near_up = df[df['near_up']]\n    if len(near_up) > 100:\n        results['near_up'] = {\n            'n': len(near_up),\n            'next_day_return': near_up['next_return'].mean(),\n            'prob_next_limit_up': near_up['next_limit_up'].mean(),\n            'prob_next_limit_down': near_up['next_limit_down'].mean()\n        }\n    \n    # At upper limit\n    at_up = df[df['limit_up_hit']]\n    if len(at_up) > 100:\n        results['at_up'] = {\n            'n': len(at_up),\n            'next_day_return': at_up['next_return'].mean(),\n            'prob_next_limit_up': at_up['next_limit_up'].mean(),\n            'prob_next_limit_down': at_up['next_limit_down'].mean()\n        }\n    \n    # Near lower limit\n    near_down = df[df['near_down']]\n    if len(near_down) > 100:\n        results['near_down'] = {\n            'n': len(near_down),\n            'next_day_return': near_down['next_return'].mean(),\n            'prob_next_limit_up': near_down['next_limit_up'].mean(),\n            'prob_next_limit_down': near_down['next_limit_down'].mean()\n        }\n    \n    # At lower limit\n    at_down = df[df['limit_down_hit']]\n    if len(at_down) > 100:\n        results['at_down'] = {\n            'n': len(at_down),\n            'next_day_return': at_down['next_return'].mean(),\n            'prob_next_limit_up': at_down['next_limit_up'].mean(),\n            'prob_next_limit_down': at_down['next_limit_down'].mean()\n        }\n    \n    # Normal days (benchmark)\n    normal = df[~df['near_up'] & ~df['near_down'] &\n                 ~df['limit_up_hit'] & ~df['limit_down_hit']]\n    results['normal'] = {\n        'n': len(normal),\n        'next_day_return': normal['next_return'].mean(),\n        'prob_next_limit_up': normal['next_limit_up'].mean(),\n        'prob_next_limit_down': normal['next_limit_down'].mean()\n    }\n    \n    return pd.DataFrame(results).T\n\nmagnet = magnet_test(daily_hose, 0.07, proximity_threshold=0.8)\nprint(\"Magnet Effect Test (HOSE):\")\nprint(magnet.round(4).to_string())\n```\n:::\n\n\n::: {#fig-magnet .cell execution_count=22}\n``` {.python .cell-code code-summary=\"Visualize magnet effect evidence\"}\n# More granular: bin today's return and compute next-day statistics\nbins = np.arange(-0.075, 0.08, 0.005)\ndaily_hose_next = daily_hose.copy()\ndaily_hose_next['next_return'] = (\n    daily_hose_next.groupby('ticker')['daily_return'].shift(-1)\n)\ndaily_hose_next['ret_bin'] = pd.cut(daily_hose_next['daily_return'],\n                                      bins=bins, labels=False)\n\nbin_stats = (\n    daily_hose_next.dropna(subset=['ret_bin', 'next_return'])\n    .groupby('ret_bin')\n    .agg(\n        mean_ret=('daily_return', 'mean'),\n        next_ret=('next_return', 'mean'),\n        n=('next_return', 'count')\n    )\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Next-day return by today's return\naxes[0].bar(bin_stats['mean_ret'] * 100, bin_stats['next_ret'] * 100,\n            width=0.4,\n            color=np.where(bin_stats['next_ret'] > 0, '#27AE60', '#C0392B'),\n            alpha=0.7, edgecolor='white')\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].axvline(x=7, color='gray', linewidth=1, linestyle='--')\naxes[0].axvline(x=-7, color='gray', linewidth=1, linestyle='--')\naxes[0].set_xlabel(\"Today's Return (%)\")\naxes[0].set_ylabel('Next-Day Return (%)')\naxes[0].set_title('Panel A: Next-Day Return by Current Return')\n\n# Panel B: Continuation probability\n# Probability of same-direction move next day\ndaily_hose_next['continuation'] = (\n    np.sign(daily_hose_next['daily_return']) ==\n    np.sign(daily_hose_next['next_return'])\n)\n\ncont_by_bin = (\n    daily_hose_next.dropna(subset=['ret_bin', 'continuation'])\n    .groupby('ret_bin')\n    .agg(\n        mean_ret=('daily_return', 'mean'),\n        cont_prob=('continuation', 'mean'),\n        n=('continuation', 'count')\n    )\n)\n\naxes[1].scatter(cont_by_bin['mean_ret'] * 100, cont_by_bin['cont_prob'] * 100,\n                 color='#2C5F8A', s=40, alpha=0.7)\naxes[1].axhline(y=50, color='gray', linewidth=0.5, linestyle='--')\naxes[1].axvline(x=7, color='gray', linewidth=1, linestyle='--')\naxes[1].axvline(x=-7, color='gray', linewidth=1, linestyle='--')\naxes[1].set_xlabel(\"Today's Return (%)\")\naxes[1].set_ylabel('Continuation Probability (%)')\naxes[1].set_title('Panel B: Same-Direction Move Next Day')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n\n## The Delayed Price Discovery Hypothesis {#sec-price-limit-delayed}\n\n### Volatility Spillover\n\nIf limits prevent full price adjustment on day $t$, the residual adjustment spills over to day $t+1$ (and possibly further). This predicts higher volatility on the day *after* a limit hit, and positive return autocorrelation (continuation in the direction of the limit hit). @kim2013price find strong evidence of this in the Tokyo Stock Exchange.\n\n::: {#spillover .cell execution_count=23}\n``` {.python .cell-code code-summary=\"Test for volatility spillover after limit hit days\"}\n# Compare volatility and return on limit-hit days vs day after\nspillover_data = daily_hose.copy()\nspillover_data['prev_limit_up'] = (\n    spillover_data.groupby('ticker')['limit_up_hit'].shift(1)\n)\nspillover_data['prev_limit_down'] = (\n    spillover_data.groupby('ticker')['limit_down_hit'].shift(1)\n)\nspillover_data['abs_return'] = spillover_data['daily_return'].abs()\n\n# Classify days\nconditions = {\n    'After upper limit': spillover_data['prev_limit_up'] == True,\n    'After lower limit': spillover_data['prev_limit_down'] == True,\n    'Normal day': (spillover_data['prev_limit_up'] == False) &\n                   (spillover_data['prev_limit_down'] == False)\n}\n\nprint(\"Volatility Spillover After Limit Hits:\")\nprint(f\"{'Condition':<25} {'Mean |r|':>10} {'Mean r':>10} \"\n      f\"{'σ(r)':>10} {'N':>12}\")\nprint(\"-\" * 67)\n\nfor label, mask in conditions.items():\n    subset = spillover_data[mask].dropna(subset=['daily_return'])\n    print(f\"{label:<25} \"\n          f\"{subset['abs_return'].mean()*100:>10.3f}% \"\n          f\"{subset['daily_return'].mean()*100:>10.3f}% \"\n          f\"{subset['daily_return'].std()*100:>10.3f}% \"\n          f\"{len(subset):>12,}\")\n\n# Statistical test: is variance higher after limit days?\nnormal = spillover_data[conditions['Normal day']]['daily_return'].dropna()\nafter_up = spillover_data[conditions['After upper limit']]['daily_return'].dropna()\nafter_down = spillover_data[conditions['After lower limit']]['daily_return'].dropna()\n\nf_up = after_up.var() / normal.var()\nf_down = after_down.var() / normal.var()\nprint(f\"\\nVariance ratios (vs normal days):\")\nprint(f\"  After upper limit: {f_up:.3f} \"\n      f\"(p = {1 - stats.f.cdf(f_up, len(after_up)-1, len(normal)-1):.4f})\")\nprint(f\"  After lower limit: {f_down:.3f} \"\n      f\"(p = {1 - stats.f.cdf(f_down, len(after_down)-1, len(normal)-1):.4f})\")\n```\n:::\n\n\n### Multi-Day Return Reconstruction\n\nTo recover the \"true\" return that would have occurred without price limits, we can compound returns over consecutive limit-hit days until the stock resumes normal trading:\n\n::: {#return-reconstruction .cell execution_count=24}\n``` {.python .cell-code code-summary=\"Reconstruct multi-day returns spanning limit-hit sequences\"}\ndef reconstruct_returns(group, limit):\n    \"\"\"\n    For each limit-hit sequence, compound returns until\n    the stock resumes normal trading (first non-limit day).\n    Returns the compound return and the number of days.\n    \"\"\"\n    sequences = []\n    in_sequence = False\n    seq_start = None\n    seq_returns = []\n    seq_direction = None\n    \n    for _, row in group.iterrows():\n        if row['limit_up_hit'] or row['limit_down_hit']:\n            if not in_sequence:\n                in_sequence = True\n                seq_start = row['date']\n                seq_returns = [row['daily_return']]\n                seq_direction = 'up' if row['limit_up_hit'] else 'down'\n            else:\n                seq_returns.append(row['daily_return'])\n        else:\n            if in_sequence:\n                # Include the first non-limit day (the \"resolution\" day)\n                seq_returns.append(row['daily_return'])\n                compound_ret = np.prod([1 + r for r in seq_returns]) - 1\n                sequences.append({\n                    'ticker': group.name if hasattr(group, 'name') else group['ticker'].iloc[0],\n                    'start_date': seq_start,\n                    'n_limit_days': len(seq_returns) - 1,\n                    'compound_return': compound_ret,\n                    'direction': seq_direction,\n                    'limit_return': sum(seq_returns[:-1]),\n                    'resolution_return': seq_returns[-1]\n                })\n                in_sequence = False\n    \n    return sequences\n\n# Run for all HOSE stocks\nall_sequences = []\nfor ticker, group in daily_hose.sort_values('date').groupby('ticker'):\n    seqs = reconstruct_returns(group, 0.07)\n    all_sequences.extend(seqs)\n\nseq_df = pd.DataFrame(all_sequences)\n\nif len(seq_df) > 0:\n    print(\"Limit-Hit Sequence Analysis:\")\n    print(f\"  Total sequences: {len(seq_df):,}\")\n    print(f\"  Mean limit days: {seq_df['n_limit_days'].mean():.1f}\")\n    print(f\"\\nCompound Returns by Direction:\")\n    for direction in ['up', 'down']:\n        subset = seq_df[seq_df['direction'] == direction]\n        print(f\"  {direction.upper()} sequences: {len(subset):,}\")\n        print(f\"    Mean compound return: {subset['compound_return'].mean()*100:.2f}%\")\n        print(f\"    Mean resolution-day return: \"\n              f\"{subset['resolution_return'].mean()*100:.2f}%\")\n        print(f\"    Max compound return: {subset['compound_return'].max()*100:.1f}%\")\n```\n:::\n\n\n## The Idiosyncratic Volatility Puzzle Under Price Limits {#sec-price-limit-ivol}\n\n@ang2006cross document that stocks with high idiosyncratic volatility earn low subsequent returns—the IVOL puzzle. In Vietnam, price limits contaminate IVOL estimation: stocks that frequently hit limits have *understated* IVOL (because their returns are censored), which could create a mechanical relation between measured IVOL and returns.\n\n::: {#ivol-puzzle .cell execution_count=25}\n``` {.python .cell-code code-summary=\"Test whether price limits affect the IVOL-return relation\"}\n# Compute monthly IVOL two ways:\n# (a) Naive: std of daily residuals from FF3\n# (b) Corrected: excluding limit-hit days\n\n# Merge daily data with market returns for IVOL estimation\ndaily_ff = daily_hose_merged.copy()\n\nmonthly_ivol = []\nfor (ticker, month), group in daily_ff.groupby(\n    ['ticker', daily_ff['date'].dt.to_period('M')]\n):\n    if len(group) < 15:\n        continue\n    \n    y = group['daily_return'].dropna()\n    x = group['mkt_return'].reindex(y.index).dropna()\n    common = y.index.intersection(x.index)\n    if len(common) < 15:\n        continue\n    \n    # Naive IVOL\n    model = sm.OLS(y[common], sm.add_constant(x[common])).fit()\n    ivol_naive = model.resid.std() * np.sqrt(252)\n    \n    # Interior-only IVOL\n    interior = group[~group['limit_up_hit'] & ~group['limit_down_hit']]\n    y_int = interior['daily_return'].dropna()\n    x_int = interior['mkt_return'].reindex(y_int.index).dropna()\n    common_int = y_int.index.intersection(x_int.index)\n    \n    if len(common_int) >= 10:\n        model_int = sm.OLS(y_int[common_int],\n                            sm.add_constant(x_int[common_int])).fit()\n        ivol_corrected = model_int.resid.std() * np.sqrt(252)\n    else:\n        ivol_corrected = np.nan\n    \n    n_limit = group['limit_up_hit'].sum() + group['limit_down_hit'].sum()\n    \n    monthly_ivol.append({\n        'ticker': ticker,\n        'month': month.to_timestamp(),\n        'ivol_naive': ivol_naive,\n        'ivol_corrected': ivol_corrected,\n        'n_limit_days': n_limit,\n        'pct_limit': n_limit / len(group) * 100,\n        'next_return': group['daily_return'].iloc[-1]  # Placeholder\n    })\n\nivol_df = pd.DataFrame(monthly_ivol)\n\nprint(\"IVOL Estimation: Naive vs Corrected:\")\nprint(f\"  Mean naive IVOL:     {ivol_df['ivol_naive'].mean():.4f}\")\nprint(f\"  Mean corrected IVOL: {ivol_df['ivol_corrected'].mean():.4f}\")\nprint(f\"  Mean difference:     \"\n      f\"{(ivol_df['ivol_corrected'] - ivol_df['ivol_naive']).mean():.4f}\")\nprint(f\"  Correlation:         \"\n      f\"{ivol_df['ivol_naive'].corr(ivol_df['ivol_corrected']):.3f}\")\n```\n:::\n\n\n## Practical Recommendations {#sec-price-limit-recommendations}\n\nFor researchers working with Vietnamese equity data:\n\n**Always report limit-hit frequency.** Any study using Vietnamese daily returns should document the fraction of observations at the price limits, broken down by exchange and market cap quintile. This tells the reader the severity of the censoring problem in the specific sample.\n\n**Use Tobit-corrected variance estimates.** For volatility-related analyses (IVOL sorts, GARCH, risk modeling), the naive sample variance underestimates true variance by 5–20% depending on the stock's limit-hit frequency. The Tobit MLE provides a consistent estimator under the censored normal assumption.\n\n**Consider range-based estimators.** The @yang2000drift estimator using OHLC prices is partially robust to closing-price censoring and does not require distributional assumptions. It is a good default for individual-stock volatility estimation.\n\n**Exclude limit-hit days for beta estimation.** Interior-only betas are less biased than all-day betas, though noisier. Report both and discuss the difference. For stocks with \\>5% limit-hit frequency, the attenuation is economically meaningful.\n\n**Compound multi-day returns for event studies.** When studying events that coincide with limit hits (earnings announcements, M&A, regulatory changes), use the compound return from the limit-hit sequence start to the first non-limit day. Single-day returns are censored and understate the market's reaction.\n\n**Be cautious interpreting short-term return predictability.** The delayed price discovery effect creates positive return autocorrelation at the daily frequency. This is a mechanical consequence of censoring, not a market inefficiency. Monthly returns are largely free of this artifact because the censoring within a month averages out.\n\n**Test robustness to HNX and UPCoM.** If a result is driven by limit-related distortions, it should appear differently (or not at all) on HNX ($\\pm$ 10%) and UPCoM ($\\pm$ 15%). Cross-exchange comparison is a natural placebo test.\n\n## Summary {#sec-price-limit-summary}\n\n| Issue | Bias Direction | Magnitude (HOSE) | Recommended Fix |\n|------------------|------------------|------------------|------------------|\n| Return variance | Understated | 5–20% for volatile stocks | Tobit MLE or range-based |\n| GARCH vol | Understated | 10–30% during crises | Censored GARCH |\n| Market beta | Attenuated (toward 0) | 3–10% for small-caps | Interior-only estimation |\n| IVOL | Understated | Varies; correlated with size | Corrected IVOL |\n| Return autocorrelation | Positive (spurious) | Significant at daily freq | Use weekly/monthly |\n| Event study CARs | Understated | Up to 50% of true effect | Compound multi-day |\n| Distribution shape | Pile-up at limits | 2–5% of obs at limits | Acknowledge or correct |\n\n: Summary of price limit effects on empirical estimates. {#tbl-price-limit-summary}\n\nPrice limits are not a minor institutional detail—they are a pervasive data-generating process that affects nearly every empirical quantity computed from Vietnamese daily returns. The corrections developed in this chapter—Tobit variance estimation, censored GARCH, interior-only betas, range-based volatility, and multi-day return compounding—form a toolkit that should be applied routinely. Ignoring censoring does not make it go away; it merely makes the resulting estimates quietly wrong.\n\n```{=html}\n<!-- ## Exercises {#sec-price-limit-exercises}\n\n1.  **Natural experiment from limit widening.** HOSE widened its limit from $\\pm$ 5% to $\\pm$ 7% in June 2013. Using a difference-in-differences design with HNX as a control group (whose limits did not change), test whether the widening reduced: (a) the frequency of limit hits, (b) next-day volatility spillover, (c) return autocorrelation. This is one of the cleanest identification strategies available for testing the cooling hypothesis.\n\n2.  **Censored EGARCH.** Extend the censored GARCH to the @nelson1991conditional EGARCH specification, which allows asymmetric volatility responses (leverage effect). Does the asymmetry parameter differ between the standard and censored specifications? The leverage effect in Vietnam may be understated by standard models because limit-down hits are disproportionately censored.\n\n3.  **Lottery demand and limit hits.** @bali2011maxing show that stocks with high maximum daily returns (MAX) earn low subsequent returns, interpreted as lottery demand. In Vietnam, MAX is mechanically bounded by the price limit. Construct the MAX anomaly using (a) the raw observed MAX and (b) the Tobit-corrected MAX. How does the correction affect the premium?\n\n4.  **Limit-hit-adjusted Fama-French factors.** Reconstruct the Vietnamese SMB, HML, and WML factors using only interior (non-limit) daily returns to compute monthly portfolio returns. Compare the resulting factor premia and Sharpe ratios to the standard construction. Quantify the \"censoring premium\"—the component of the factor return that comes from limit-day returns.\n\n5.  **Foreign investor response to limits.** Using foreign ownership data, test whether foreign investors systematically avoid stocks that frequently hit price limits. If foreign investors are more sophisticated, their avoidance of limit-prone stocks could explain part of the foreign ownership premium.\n\n6.  **Intraday volatility estimation.** If intraday data are available, compute realized variance from 5-minute returns. Compare this to close-to-close variance on limit-hit days versus non-limit days. The ratio reveals how much of the true volatility is captured by the closing price.\n\n7.  **Optimal limit design.** Using the Vietnamese data, estimate the \"optimal\" price limit width that minimizes next-day volatility spillover while minimizing the censoring rate. Plot the trade-off curve between censoring (too narrow) and insufficient cooling (too wide). Where does the current $\\pm$ 7% HOSE limit sit on this curve?\n\n8.  **Sentiment and limit hits.** Construct a market-level sentiment indicator based on the ratio of upper-limit hits to lower-limit hits. Test whether this indicator predicts subsequent market returns, analogous to @baker2006investor. A high up/down ratio may signal overheating, while a low ratio may signal panic. -->\n```\n\n",
    "supporting": [
      "55_price_limits_and_volatility_files/figure-pdf"
    ],
    "filters": []
  }
}