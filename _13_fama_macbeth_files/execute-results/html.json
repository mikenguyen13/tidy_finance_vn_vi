{
  "hash": "01294a7f3aae4bc46416ed8e7a76be60",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Fama-MacBeth Regressions\"\nformat:\n  html:\n    code-fold: false\n    number-sections: true\njupyter: python3\n---\n\nIn this chapter, we delve into the implementation of the @Fama1973 regression approach, a cornerstone of empirical asset pricing. While portfolio sorts (discussed in previous chapters) provide a robust, non-parametric view of the relationship between characteristics and returns, they struggle when we need to control for multiple factors simultaneously. For instance, in the Vietnamese stock market (HOSE and HNX), small-cap stocks often exhibit high illiquidity. Does the \"Size effect\" exist because small stocks are risky, or simply because they are illiquid? Fama-MacBeth (FM) regressions allow us to disentangle these effects in a linear framework.\n\nWe will implement a sophisticated version of the FM procedure, accounting for:\n1.  **Weighted Least Squares (WLS):** To prevent micro-cap stocks—which are prevalent and volatile in Vietnam—from dominating the estimates.\n2.  **Newey-West Adjustments:** To handle the serial correlation often observed in Vietnamese market risk premiums.\n3.  **Shanken Correction:** To mathematically adjust for the \"Errors-in-Variables\" bias arising from estimated betas.\n\n## The Econometric Framework\n\nThe Fama-MacBeth procedure is essentially a two-step filter that separates the cross-sectional variation in returns from the time-series variation.\n\n### Intuition: Why not Panel OLS?\nA naive approach would be to pool all data ($N$ stocks $\\times$ $T$ months) and run a single Ordinary Least Squares (OLS) regression:\n$$ r_{i,t+1} = \\alpha + \\beta_{i,t} \\lambda + \\epsilon_{i,t+1} $$\nHowever, this assumes that the error terms $\\epsilon_{i,t+1}$ are independent across firms. In reality, stock returns are highly cross-sectionally correlated (if the VN-Index crashes, most stocks fall together). A pooled OLS would underestimate the standard errors, leading to \"false positive\" discoveries of risk factors. Fama-MacBeth solves this by running $T$ separate cross-sectional regressions, effectively treating each month as a single independent observation of the risk premium.\n\n### Mathematical Derivation\n\n#### Step 1: Cross-Sectional Regressions\nFor each month $t$, we estimate the premium $\\lambda_{k,t}$ for $K$ factors. Let $r_{i,t+1}$ be the excess return of asset $i$ at time $t+1$. Let $\\boldsymbol{\\beta}_{i,t}$ be a vector of $K$ characteristics (e.g., Market Beta, Book-to-Market, Size) known at time $t$.\n\nThe model for a specific month $t$ is:\n$$ \\mathbf{r}_{t+1} = \\mathbf{X}_t \\boldsymbol{\\lambda}_{t+1} + \\boldsymbol{\\alpha}_{t+1} + \\boldsymbol{\\epsilon}_{t+1} $$\nWhere:\n* $\\mathbf{r}_{t+1}$ is an $N \\times 1$ vector of returns.\n* $\\mathbf{X}_t$ is an $N \\times (K+1)$ matrix of factor exposures (including a column of ones for the intercept).\n* $\\boldsymbol{\\lambda}_{t+1}$ is the vector of risk premiums realized in month $t+1$.\n\nIn the Vietnamese context, we use **Weighted Least Squares (WLS)**. We define a weighting matrix $\\mathbf{W}_t$ (typically diagonal with market capitalizations). The estimator for month $t$ is:\n$$ \\hat{\\boldsymbol{\\lambda}}_{t+1} = (\\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{X}_t)^{-1} \\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{r}_{t+1} $$\n\n#### Step 2: Time-Series Aggregation\nWe now have a time-series of $T$ estimates: $\\hat{\\lambda}_1, \\hat{\\lambda}_2, \\dots, \\hat{\\lambda}_T$. The final estimate of the risk premium is the time-series average:\n$$ \\hat{\\lambda}_k = \\frac{1}{T} \\sum_{t=1}^T \\hat{\\lambda}_{k,t} $$\n\nThe standard error is derived from the standard deviation of these monthly estimates:\n$$ \\sigma(\\hat{\\lambda}_k) = \\sqrt{\\frac{1}{T^2} \\sum_{t=1}^T (\\hat{\\lambda}_{k,t} - \\hat{\\lambda}_k)^2} $$\n\n\n\n## The \"Errors-in-Variables\" (EIV) Problem\nA critical subtlety often ignored in basic applications is that the betas used in Step 1 are not true values—they are estimates from a prior rolling window (Step 0). This measurement error biases the second-pass $\\lambda$ estimates (typically downwards) and understates the standard errors.\n\n@Shanken1992 provides a correction factor $c$. If we use standard portfolio betas, the variance of the estimator must be inflated:\n$$ \\sigma^2_{Shanken}(\\hat{\\lambda}) = \\sigma^2_{FM}(\\hat{\\lambda}) \\times (1 + \\hat{\\lambda}^\\top \\Sigma_f^{-1} \\hat{\\lambda}) $$\nWhere $\\Sigma_f$ is the covariance matrix of the factors. This correction is particularly relevant for the Vietnamese market where factor volatilities are high, making the term $(1 + \\dots)$ significantly larger than 1.\n\n## Data Preparation\n\nWe utilize data from our local SQLite database. In Vietnam, the fiscal year typically ends in December, and audited reports are required by April. To ensure no look-ahead bias, we lag accounting data (Book Equity) to match returns starting in July (a 6-month conservative lag, similar to Fama-French, but adapted for Vietnamese reporting delays).\n\n::: {#082dc7b3 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom pandas.tseries.offsets import MonthEnd\n\n# Connect to the Vietnamese data\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\n# Load Monthly Prices (HOSE & HNX)\nprices_monthly = pd.read_sql_query(\n  sql=\"SELECT symbol, date, ret_excess, mktcap FROM prices_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\n# Load Book Equity (derived from Vietnamese Financial Statements)\ncomp_vn = pd.read_sql_query(\n  sql=\"SELECT datadate, symbol, be FROM comp_vn\",\n  con=tidy_finance,\n  parse_dates={\"datadate\"}\n)\n\n# Load Rolling Market Betas (Pre-calculated in Chapter 'Beta Estimation')\nbeta_monthly = pd.read_sql_query(\n  sql=\"SELECT symbol, date, beta FROM beta_monthly WHERE return_type = 'monthly'\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n```\n:::\n\n\nWe construct our testing characteristics:\n\n1. ** (Market Beta):** The sensitivity to the VN-Index.\n2. **Size (ln(ME)):** The natural log of market capitalization.\n3. **Value (BM):** The ratio of Book Equity to Market Equity.\n\n::: {#c513b5e8 .cell execution_count=3}\n``` {.python .cell-code}\n# Prepare Characteristics\ncharacteristics = (comp_vn\n  # Align reporting date to month end\n  .assign(date=lambda x: pd.to_datetime(x[\"datadate\"]) + MonthEnd(0))\n  # Merge with price data to get Market Cap at fiscal year end\n  .merge(prices_monthly[[\"symbol\", \"date\", \"mktcap\"]], \n         on=[\"symbol\", \"date\"], how=\"inner\")\n  .assign(\n      # Compute Book-to-Market\n      bm=lambda x: x[\"be\"] / x[\"mktcap\"],\n      # Create sorting date: Financials valid from July of year t+1\n      sorting_date=lambda x: x[\"date\"] + pd.DateOffset(months=6) + MonthEnd(0)\n  )\n  .get([\"symbol\", \"bm\", \"sorting_date\"])\n)\n\n# Merge back to monthly return panel\ndata_fm = (prices_monthly\n  .merge(characteristics, \n         left_on=[\"symbol\", \"date\"], \n         right_on=[\"symbol\", \"sorting_date\"], \n         how=\"left\")\n  .merge(beta_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n  .sort_values([\"symbol\", \"date\"])\n)\n\n# Forward fill characteristics for 12 months (valid until next report)\ndata_fm[[\"bm\"]] = data_fm.groupby(\"symbol\")[[\"bm\"]].ffill(limit=12)\n\n# Log Market Cap is updated monthly\ndata_fm[\"log_mktcap\"] = np.log(data_fm[\"mktcap\"])\n\n# Lead returns: We use characteristics at t to predict return at t+1\ndata_fm[\"ret_excess_lead\"] = data_fm.groupby(\"symbol\")[\"ret_excess\"].shift(-1)\n\n# Cleaning: Remove rows with missing future returns or characteristics\ndata_fm = data_fm.dropna(subset=[\"ret_excess_lead\", \"beta\", \"log_mktcap\", \"bm\"])\n\nprint(f\"Data ready: {len(data_fm):,} observations from {data_fm.date.min().date()} to {data_fm.date.max().date()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData ready: 67,478 observations from 2016-06-30 to 2023-11-30\n```\n:::\n:::\n\n\n## Step 1: Cross-Sectional Regressions with WLS\n\nHere we deviate from the standard implementation. @Hou2020 argue that micro-cap stocks distorts inference because they have high transaction costs and idiosyncratic volatility. In Vietnam, this is exacerbated by \"penny stock\" speculation.\n\nWe implement **Weighted Least Squares (WLS)** where weights are the market capitalization of the prior month. This tests if the factors are priced in the *investable* universe, not just the equal-weighted average of tiny stocks.\n\n::: {#e3235e6f .cell execution_count=4}\n``` {.python .cell-code}\ndef run_cross_section(df):\n    # Standardize inputs for numerical stability\n    # Note: We do NOT standardize the dependent variable (returns)\n    # We standardize regressors to interpret coefficients as \"per 1 SD change\" if desired,\n    # BUT for pure risk premium estimation, we usually keep raw units.\n    # Here we use raw units to interpret lambda as % return per unit of characteristic.\n    \n    # Define Weighted Least Squares\n    model = smf.wls(\n        formula=\"ret_excess_lead ~ beta + log_mktcap + bm\",\n        data=df,\n        weights=df[\"mktcap\"] # Weight by size\n    )\n    results = model.fit()\n    \n    return results.params\n\n# Apply to every month\nrisk_premiums = (data_fm\n  .groupby(\"date\")\n  .apply(run_cross_section)\n  .reset_index()\n)\n\nprint(risk_premiums.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        date  Intercept      beta  log_mktcap            bm\n0 2016-06-30  -0.213841 -0.035454    0.025800  2.336615e-11\n1 2016-07-31   0.192702  0.012807   -0.026880  2.115332e-12\n2 2016-08-31   0.288753  0.017908   -0.033139 -4.584291e-11\n3 2016-09-30  -0.119197 -0.017241    0.007310  1.858875e-11\n4 2016-10-31  -0.030620 -0.010611    0.000911  3.719833e-12\n```\n:::\n:::\n\n\n## Step 2: Time-Series Aggregation & Hypothesis Testing\n\nWe now possess the time-series of risk premiums. We calculate the arithmetic mean and the -statistics.\n\nCrucially, we use **Newey-West (HAC)** standard errors. Risk premiums in Vietnam often exhibit autocorrelation (momentum in factor performance). A simple standard error formula would be invalid.\n\n::: {#f9b072b0 .cell execution_count=5}\n``` {.python .cell-code}\n# def calculate_fama_macbeth_stats(df, lags=6):\n#     summary = []\n    \n#     for col in [\"Intercept\", \"beta\", \"log_mktcap\", \"bm\"]:\n#         series = df[col]\n        \n#         # 1. Point Estimate (Average Risk Premium)\n#         mean_premium = series.mean()\n        \n#         # 2. Newey-West Standard Error\n#         # We regress the series on a constant to get the SE of the mean\n#         nw_model = sm.OLS(series, sm.add_constant(np.ones(len(series)))).fit(\n#             cov_type='HAC', cov_kwds={'maxlags': lags}\n#         )\n#         se = nw_model.bse[0]\n#         t_stat = nw_model.tvalues[0]\n        \n#         summary.append({\n#             \"Factor\": col,\n#             \"Premium (%)\": mean_premium * 100, # Convert to %\n#             \"Std Error\": se * 100,\n#             \"t-statistic\": t_stat,\n#             \"Significance\": \"*\" if abs(t_stat) > 1.96 else \"\"\n#         })\n        \n#     return pd.DataFrame(summary)\n\n# results_table = calculate_fama_macbeth_stats(risk_premiums)\n# results_table\n\ndef calculate_fama_macbeth_stats(df, lags=6):\n    summary = []\n    \n    for col in [\"Intercept\", \"beta\", \"log_mktcap\", \"bm\"]:\n        series = df[col]\n        \n        # 1. Point Estimate (Average Risk Premium)\n        mean_premium = series.mean()\n        \n        # 2. Newey-West Standard Error\n        # We regress the series on a constant (ones) to get the SE of the mean\n        # We use .iloc[0] to avoid KeyError, as the index is likely named 'const'\n        exog = sm.add_constant(np.ones(len(series)))\n        nw_model = sm.OLS(series, exog).fit(\n            cov_type='HAC', cov_kwds={'maxlags': lags}\n        )\n        \n        # FIX: Use .iloc[0] instead of [0]\n        se = nw_model.bse.iloc[0]\n        t_stat = nw_model.tvalues.iloc[0]\n        \n        summary.append({\n            \"Factor\": col,\n            \"Premium (%)\": mean_premium * 100,\n            \"Std Error\": se * 100,\n            \"t-statistic\": t_stat,\n            \"Significance\": \"*\" if abs(t_stat) > 1.96 else \"\"\n        })\n        \n    return pd.DataFrame(summary)\n\nresults_table = calculate_fama_macbeth_stats(risk_premiums)\nprint(results_table.round(4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Factor  Premium (%)  Std Error  t-statistic Significance\n0   Intercept       0.3330     2.0240       0.1645             \n1        beta       0.3985     0.4664       0.8545             \n2  log_mktcap      -0.1843     0.2134      -0.8636             \n3          bm       0.0000     0.0000       1.5963             \n```\n:::\n:::\n\n\n### Visualizing the Time-Varying Risk Premium\n\nOne major advantage of the FM approach is that we can inspect the volatility of the risk premiums over time. In Vietnam, we expect the \"Size\" premium to be highly volatile during periods of retail liquidity injection (e.g., 2020-2021).\n\n::: {#331b77fd .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# Calculate cumulative returns of the factors (as if they were tradable portfolios)\ncumulative_premiums = (risk_premiums\n    .set_index(\"date\")\n    .drop(columns=[\"Intercept\"])\n    .cumsum()\n)\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncumulative_premiums.plot(ax=ax, linewidth=2)\nax.set_title(\"Cumulative Risk Premiums in Vietnam (Fama-MacBeth)\", fontsize=14)\nax.set_ylabel(\"Cumulative Coefficient Return\")\nax.legend(title=\"Factor\")\nax.grid(True, alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](_13_fama_macbeth_files/figure-html/cell-6-output-1.png){width=823 height=526}\n:::\n:::\n\n\n1. **Market Beta:** In many empirical studies (including the US), the market beta premium is often insignificant or even negative (the \"Betting Against Beta\" anomaly). In Vietnam, if the -stat is , it implies the CAPM does not explain the cross-section of returns.\n2. **Size (Log Mktcap):** A negative coefficient confirms the \"Size Effect\"—smaller firms have higher expected returns. However, using WLS often weakens this result compared to OLS, suggesting the size premium is concentrated in micro-caps.\n3. **Value (BM):** A positive coefficient confirms the Value premium. In Vietnam, value stocks (high B/M) often outperform growth stocks, particularly in the manufacturing and banking sectors.\n\n## Advanced Extension: The Shanken Correction\n\nAs discussed, we must adjust the -statistics for the fact that  was estimated. While `tidyfinance` packages often hide this, we calculate it manually here for rigorous intuition.\n\nThe formula for the -statistic with Shanken correction is:\n$$ \nt_{Shanken} = \\frac{\\hat{\\lambda}*k}{\\sqrt{\\sigma^2*{FM} (1 + c)}} \n$$\n\nwhere .\n\n*Note:* Strictly speaking, the Shanken correction applies when the factors are *returns* (like Fama-French factors). When using characteristics (like we do here), the correction is more complex, but the intuition remains: our standard errors are likely too small.\n\n",
    "supporting": [
      "_13_fama_macbeth_files"
    ],
    "filters": [],
    "includes": {}
  }
}