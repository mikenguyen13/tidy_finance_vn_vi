---
title: Beta Estimation
format:
  html:
    toc: true
    number-sections: true
jupyter: python3
execute:
  echo: true
  warning: false
  message: false
---

## Introduction: Measuring Systematic Risk

The [Capital Asset Pricing Model](capital-asset-pricing-model.qmd) established that expected returns depend on systematic risk—an asset's sensitivity to market-wide movements. This sensitivity, measured by beta ($\beta$), is the single most important parameter in the CAPM framework. Understanding how to estimate beta reliably is essential for portfolio construction, performance evaluation, and risk management.

Recall the CAPM equation:

$$
E(r_i) - r_f = \beta_i [E(r_m) - r_f]
$$

where $\beta_i = \frac{\text{Cov}(r_i, r_m)}{\text{Var}(r_m)}$ measures how much asset $i$'s returns move with the market. A beta of 1.0 means the stock moves one-for-one with the market; beta above 1.0 indicates amplified market sensitivity (aggressive stocks); beta below 1.0 indicates dampened sensitivity (defensive stocks).

In practice, beta is not directly observable—it must be estimated from historical return data. This chapter addresses the practical challenges of beta estimation:

1. **Which data frequency?** Monthly returns provide stability but fewer observations; daily returns offer more data but introduce noise.
2. **What estimation window?** Longer windows provide precision but may miss structural changes; shorter windows are responsive but volatile.
3. **How to scale?** Estimating betas for thousands of stocks requires computational efficiency through parallelization.

We implement rolling-window beta estimation using both monthly and daily returns for the Vietnamese stock market. By the end of this chapter, you will have a database of time-varying beta estimates ready for use in portfolio construction and asset pricing tests.

```{python}
import pandas as pd
import numpy as np
import sqlite3
import statsmodels.formula.api as smf

from plotnine import *
from mizani.formatters import percent_format, comma_format
from joblib import Parallel, delayed, cpu_count
from datetime import datetime
from dateutil.relativedelta import relativedelta
```

We use `statsmodels` for regression analysis and `joblib` for parallel processing across multiple CPU cores.

## Loading the Data

We load monthly stock returns, daily stock returns, and factor data from our SQLite database created in [Accessing and Managing Financial Data](accessing-and-managing-financial-data.qmd).

```{python}
tidy_finance = sqlite3.connect(database="data/tidy_finance_python.sqlite")

# Load company fundamentals for industry classification
comp_vn = pd.read_sql_query(
    sql="SELECT symbol, datadate, icb_name_vi FROM comp_vn",
    con=tidy_finance,
    parse_dates={"datadate"}
)
comp_vn["year"] = comp_vn["datadate"].dt.year
comp_vn = comp_vn.dropna()

# Load monthly stock returns
prices_monthly = pd.read_sql_query(
    sql="SELECT symbol, date, ret_excess FROM prices_monthly",
    con=tidy_finance,
    parse_dates={"date"}
)
prices_monthly["year"] = prices_monthly["date"].dt.year

# Load monthly market factor
factors_monthly = pd.read_sql_query(
    sql="SELECT date, mkt_excess FROM factors_ff5_monthly",
    con=tidy_finance,
    parse_dates={"date"}
)

print(f"Monthly observations: {len(prices_monthly):,}")
print(f"Unique stocks: {prices_monthly['symbol'].nunique():,}")
print(f"Date range: {prices_monthly['date'].min().date()} to {prices_monthly['date'].max().date()}")
```

### Merging Stock Returns with Market Returns

To estimate CAPM betas, we need both stock excess returns and market excess returns aligned by date:

```{python}
# Merge stock returns with market factor and industry classification
prices_monthly = (prices_monthly
    .merge(factors_monthly, on="date", how="left")
    .merge(comp_vn[["symbol", "year", "icb_name_vi"]], on=["symbol", "year"], how="left")
    .dropna(subset=["ret_excess", "mkt_excess"])
)

print(f"Observations after merging: {len(prices_monthly):,}")
```

### Handling Outliers

Extreme returns can unduly influence regression estimates. We apply winsorization to limit the impact of outliers while preserving the data structure:

```{python}
from scipy.stats.mstats import winsorize

# Winsorize at 1st and 99th percentiles
prices_monthly = prices_monthly.assign(
    ret_excess=lambda x: winsorize(x["ret_excess"], limits=[0.01, 0.01]),
    mkt_excess=lambda x: winsorize(x["mkt_excess"], limits=[0.01, 0.01])
)

print("Return statistics after winsorization:")
print(prices_monthly[["ret_excess", "mkt_excess"]].describe().round(4))
```

## The CAPM Regression

The empirical CAPM specification regresses stock excess returns on market excess returns:

$$
r_{i,t} - r_f = \alpha_i + \beta_i (r_{m,t} - r_f) + \varepsilon_{i,t}
$$ {#eq-capm-regression}

where:

- $r_{i,t} - r_f$: Excess return on stock $i$ in period $t$
- $r_{m,t} - r_f$: Excess return on the market portfolio
- $\alpha_i$: Intercept (Jensen's alpha)—should be zero if CAPM holds
- $\beta_i$: Slope coefficient—the stock's systematic risk
- $\varepsilon_{i,t}$: Idiosyncratic shock

### Single-Stock Example

Let's estimate beta for a single well-known Vietnamese company to understand the mechanics:

```{python}
# Estimate CAPM for FPT Corporation
fpt_data = prices_monthly.query("symbol == 'FPT'")

model = smf.ols(
    formula="ret_excess ~ mkt_excess",
    data=fpt_data
).fit()

print(f"FPT Corporation CAPM Regression")
print(f"=" * 50)
print(f"Observations: {model.nobs:.0f}")
print(f"Alpha: {model.params['Intercept']:.4f} (t = {model.tvalues['Intercept']:.2f})")
print(f"Beta:  {model.params['mkt_excess']:.4f} (t = {model.tvalues['mkt_excess']:.2f})")
print(f"R-squared: {model.rsquared:.4f}")
```

The output shows FPT's estimated beta and alpha. A beta above 1.0 indicates the stock is more volatile than the market; an insignificant alpha (t-statistic below 2) is consistent with CAPM predictions.

## Rolling-Window Estimation

A single beta estimate over the entire sample assumes systematic risk is constant—an assumption that rarely holds. Companies change their business mix, leverage, and market exposure over time. **Rolling-window estimation** captures this time-variation by re-estimating beta using only the most recent data.

### The Estimation Functions

We create two functions: one to run a single CAPM regression and return results in a clean format, and another to apply this regression across rolling windows.

```{python}
def estimate_capm(data, min_obs=1):
    """
    Estimate CAPM regression and return coefficients.
    
    Parameters:
    -----------
    data : DataFrame
        Must contain 'ret_excess' and 'mkt_excess' columns
    min_obs : int
        Minimum observations required for estimation
        
    Returns:
    --------
    DataFrame with coefficient estimates and t-statistics
    """
    if len(data) < min_obs:
        return pd.DataFrame()
    
    try:
        model = smf.ols(
            formula="ret_excess ~ mkt_excess",
            data=data
        ).fit()
        
        results = pd.DataFrame({
            "coefficient": ["alpha", "mkt_excess"],
            "estimate": [model.params["Intercept"], model.params["mkt_excess"]],
            "t_statistic": [model.tvalues["Intercept"], model.tvalues["mkt_excess"]]
        })
        
        return results
        
    except Exception:
        return pd.DataFrame()
```

```{python}
def roll_capm_estimation(data, look_back, min_obs):
    """
    Perform rolling-window CAPM estimation.
    
    Parameters:
    -----------
    data : DataFrame
        Stock return data with 'date', 'ret_excess', 'mkt_excess'
    look_back : int
        Number of periods to include in each window
    min_obs : int
        Minimum observations required for valid estimate
        
    Returns:
    --------
    DataFrame with time series of beta estimates
    """
    data = data.sort_values("date").reset_index(drop=True)
    dates = data["date"].drop_duplicates().sort_values()
    
    results = []
    
    for i in range(look_back - 1, len(dates)):
        end_date = dates.iloc[i]
        start_date = end_date - relativedelta(months=look_back - 1)
        
        # Extract window data
        window = data.query("date >= @start_date and date <= @end_date")
        
        # Estimate CAPM
        estimate = estimate_capm(window, min_obs=min_obs)
        
        if not estimate.empty:
            estimate["date"] = end_date
            results.append(estimate)
    
    if results:
        return pd.concat(results, ignore_index=True)
    else:
        return pd.DataFrame()
```

### Example: Rolling Betas for Selected Stocks

Let's apply rolling-window estimation to a few well-known Vietnamese companies:

```{python}
# Define example companies
examples = pd.DataFrame({
    "symbol": ["FPT", "VNM", "VIC", "HPG"],
    "company": ["FPT Corporation", "Vinamilk", "Vingroup", "Hoa Phat Group"]
})

# Check data availability
data_availability = (prices_monthly
    .query("symbol in @examples['symbol']")
    .groupby("symbol")
    .agg(
        n_obs=("ret_excess", "count"),
        first_date=("date", "min"),
        last_date=("date", "max")
    )
    .reset_index()
)

print("Data availability for example stocks:")
print(data_availability)
```

```{python}
# Estimate rolling betas for example stocks
# Using 60-month (5-year) window with minimum 48 observations

example_data = prices_monthly.query("symbol in @examples['symbol']")

# Use apply with a lambda that assigns the symbol back to the result
capm_examples_list = []
for symbol in examples["symbol"]:
    stock_data = example_data.query("symbol == @symbol")
    if len(stock_data) >= 48:  # Only process if enough data
        result = roll_capm_estimation(stock_data, look_back=60, min_obs=48)
        if not result.empty:
            result["symbol"] = symbol
            capm_examples_list.append(result)

if capm_examples_list:
    capm_examples = pd.concat(capm_examples_list, ignore_index=True)
else:
    capm_examples = pd.DataFrame()

print(f"Estimated {len(capm_examples):,} coefficient-date observations for example stocks")
```

### Visualizing Rolling Betas

@fig-601 shows how beta estimates evolve over time for our example stocks:

```{python}
#| label: fig-601
#| fig-cap: "Rolling beta estimates using 60 months of data. Betas vary substantially over time, reflecting changes in business models, leverage, and market conditions."
#| fig-alt: "Title: Monthly beta estimates for example stocks. The figure shows time series of beta estimates that vary over time for each company."

# Extract beta estimates (not alpha)
beta_examples = (capm_examples
    .query("coefficient == 'mkt_excess'")
    .merge(examples, on="symbol", how="left")
)

beta_examples_figure = (
    ggplot(beta_examples, aes(x="date", y="estimate", color="company"))
    + geom_line(size=1)
    + labs(
        x="", y="Beta Estimate",
        color="",
        title="Rolling Beta Estimates (60-Month Window)"
    )
    + theme(legend_position="bottom")
)

beta_examples_figure.show()
```

The figure reveals that betas are not constant. Some stocks show increasing systematic risk over time, while others become more defensive. These dynamics reflect real changes in company operations and market positioning.

## Scaling Up: Parallelized Estimation

Estimating rolling betas for thousands of stocks is computationally intensive. Each stock requires dozens of regressions (one per month in the sample). We use parallel processing to distribute the workload across multiple CPU cores.

### Setting Up Parallelization

```{python}
# Determine available cores (keep one free for system tasks)
n_cores = max(1, cpu_count() - 1)
print(f"Using {n_cores} cores for parallel processing")
```

### Estimating Betas for All Stocks (Monthly Data)

```{python}
#| eval: false

# Define estimation parameters
LOOK_BACK_MONTHLY = 60  # 5 years
MIN_OBS_MONTHLY = 48    # Require 80% of window

def estimate_stock_beta(symbol, data):
    """Estimate rolling beta for a single stock."""
    stock_data = data.query("symbol == @symbol")
    result = roll_capm_estimation(stock_data, look_back=LOOK_BACK_MONTHLY, min_obs=MIN_OBS_MONTHLY)
    if not result.empty:
        result["symbol"] = symbol
    return result

# Get unique symbols
symbols = prices_monthly["symbol"].unique()

# Parallel estimation
capm_monthly_results = Parallel(n_jobs=n_cores)(
    delayed(estimate_stock_beta)(symbol, prices_monthly)
    for symbol in symbols
)

# Combine results (filter out empty DataFrames)
capm_monthly = pd.concat(
    [df for df in capm_monthly_results if not df.empty],
    ignore_index=True
)

# Select relevant columns
capm_monthly = capm_monthly[["symbol", "date", "coefficient", "estimate", "t_statistic"]]

print(f"Estimated {len(capm_monthly):,} coefficient-date observations")
print(f"Covering {capm_monthly['symbol'].nunique():,} stocks")
```

```{python}
#| eval: false
#| output: false

# Store monthly CAPM results
capm_monthly.to_sql(
    name="capm_monthly",
    con=tidy_finance,
    if_exists="replace",
    index=False
)
```

```{python}
# Load stored results
capm_monthly = pd.read_sql_query(
    sql="SELECT * FROM capm_monthly",
    con=tidy_finance,
    parse_dates={"date"}
)

print(f"Loaded {len(capm_monthly):,} CAPM estimates")
capm_monthly.head()
```

## Estimating Beta Using Daily Returns

Daily returns offer more observations per window, potentially improving estimation precision. However, daily data introduces challenges: higher noise, non-synchronous trading, and larger computational requirements.

### Loading Daily Data

Daily data is substantially larger than monthly data. We process it in batches to manage memory:

```{python}
# Check if daily prices table exists
try:
    daily_count = pd.read_sql_query(
        "SELECT COUNT(*) as n FROM prices_daily",
        con=tidy_finance
    )["n"].iloc[0]
    has_daily_data = daily_count > 0
    print(f"Daily price observations available: {daily_count:,}")
except:
    has_daily_data = False
    print("Daily price data not available in database")
```

### Daily Beta Estimation (If Data Available)

For daily estimation, we use a 3-month window with approximately 60 trading days, requiring at least 50 observations:

```{python}
#| eval: false

if has_daily_data:
    # Load daily market factor
    factors_daily = pd.read_sql_query(
        sql="SELECT date, mkt_excess FROM factors_ff3_daily",
        con=tidy_finance,
        parse_dates={"date"}
    )
    
    # Get list of stocks to process
    symbols = list(prices_monthly["symbol"].unique())
    
    # Process in batches
    BATCH_SIZE = 500
    n_batches = int(np.ceil(len(symbols) / BATCH_SIZE))
    
    # Daily estimation parameters
    LOOK_BACK_DAILY = 3    # 3 months
    MIN_OBS_DAILY = 50     # ~50 trading days
    
    def estimate_stock_beta_daily(symbol, data):
        """Estimate rolling beta for a single stock using daily data."""
        stock_data = data.query("symbol == @symbol").copy()
        # Convert date to month-end for consistent output
        stock_data["date"] = (
            stock_data["date"]
            .dt.to_period("M")
            .dt.to_timestamp("M")
        )
        result = roll_capm_estimation(stock_data, look_back=LOOK_BACK_DAILY, min_obs=MIN_OBS_DAILY)
        if not result.empty:
            result["symbol"] = symbol
        return result
    
    capm_daily_results = []
    
    for batch_num in range(n_batches):
        # Get symbols for this batch
        start_idx = batch_num * BATCH_SIZE
        end_idx = min((batch_num + 1) * BATCH_SIZE, len(symbols))
        batch_symbols = symbols[start_idx:end_idx]
        
        # Format for SQL query
        symbols_str = ", ".join(f"'{s}'" for s in batch_symbols)
        
        # Load daily returns for batch
        prices_daily_batch = pd.read_sql_query(
            f"""
            SELECT symbol, date, ret_excess 
            FROM prices_daily 
            WHERE symbol IN ({symbols_str})
            """,
            con=tidy_finance,
            parse_dates={"date"}
        )
        
        if prices_daily_batch.empty:
            continue
        
        # Merge with market factor
        prices_daily_batch = prices_daily_batch.merge(
            factors_daily, on="date", how="inner"
        )
        
        # Get unique symbols in this batch that have data
        batch_symbols_with_data = prices_daily_batch["symbol"].unique()
        
        # Parallel estimation for batch
        batch_results = Parallel(n_jobs=n_cores)(
            delayed(estimate_stock_beta_daily)(symbol, prices_daily_batch)
            for symbol in batch_symbols_with_data
        )
        
        # Collect non-empty results
        for result in batch_results:
            if result is not None and not result.empty:
                capm_daily_results.append(result)
        
        print(f"Batch {batch_num + 1}/{n_batches} complete")
    
    # Combine all daily results
    if capm_daily_results:
        capm_daily = pd.concat(capm_daily_results, ignore_index=True)
        capm_daily = capm_daily[["symbol", "date", "coefficient", "estimate", "t_statistic"]]
        
        print(f"Daily estimation complete: {len(capm_daily):,} observations")
    else:
        capm_daily = pd.DataFrame()
        print("No daily beta estimates produced")
```
```

### Creating a Combined Beta Dataset

We combine monthly and daily estimates into a single dataset for comparison:

```{python}
# Extract beta coefficients from monthly estimates
beta_monthly = (capm_monthly
    .query("coefficient == 'mkt_excess'")
    .loc[:, ["symbol", "date", "estimate"]]
    .rename(columns={"estimate": "beta"})
    .assign(frequency="monthly")
)

print(f"Monthly beta estimates: {len(beta_monthly):,}")
print(f"Stocks covered: {beta_monthly['symbol'].nunique():,}")
```

```{python}
#| eval: false

# If daily data exists, combine with monthly
if has_daily_data and not capm_daily.empty:
    beta_daily = (capm_daily
        .query("coefficient == 'mkt_excess'")
        .loc[:, ["symbol", "date", "estimate"]]
        .rename(columns={"estimate": "beta"})
        .assign(frequency="daily")
    )
    
    beta_combined = pd.concat([beta_monthly, beta_daily], ignore_index=True)
    print(f"Combined beta estimates: {len(beta_combined):,}")
else:
    beta_combined = beta_monthly.copy()
    print("Using monthly estimates only (no daily data)")
```

```{python}
# For this chapter, we'll work with monthly data
beta = beta_monthly.copy()
```

## Cross-Sectional Analysis of Betas

With beta estimates for all stocks, we can examine how systematic risk varies across industries and over time.

### Beta Distribution by Industry

Different industries have inherently different exposures to market-wide economic forces. @fig-602 shows the distribution of firm-level average betas across industries:

```{python}
#| label: fig-602
#| fig-cap: "Distribution of average firm betas by industry. Industries with higher market sensitivity (cyclical businesses) show higher median betas."
#| fig-alt: "Title: Beta distributions by industry. Box plots showing the distribution of average firm betas across Vietnamese industries."

# Compute average beta per firm, then summarize by industry
beta_by_firm = (beta
    .merge(
        prices_monthly[["symbol", "date", "icb_name_vi"]].drop_duplicates(),
        on=["symbol", "date"],
        how="left"
    )
    .dropna(subset=["beta", "icb_name_vi"])
    .groupby(["symbol", "icb_name_vi"])["beta"]
    .mean()
    .reset_index()
)

# Order industries by median beta
industry_order = (beta_by_firm
    .groupby("icb_name_vi")["beta"]
    .median()
    .sort_values()
    .index.tolist()
)

# Show top 10 industries by median beta
top_industries = industry_order[-10:] if len(industry_order) > 10 else industry_order

beta_industry_figure = (
    ggplot(
        beta_by_firm[beta_by_firm["icb_name_vi"].isin(top_industries)],
        aes(x="icb_name_vi", y="beta")
    )
    + geom_boxplot(fill="steelblue", alpha=0.7)
    + coord_flip()
    + scale_x_discrete(limits=top_industries)
    + labs(
        x="", y="Average Firm Beta",
        title="Beta Distribution by Industry (Top 10 by Median)"
    )
)

beta_industry_figure.show()
```

The industry patterns are intuitive. Cyclical industries like manufacturing and real estate tend to have higher betas—their fortunes rise and fall with the overall economy. Defensive industries like utilities and consumer staples typically show lower betas.

### Time Variation in the Cross-Section

@fig-603 examines how the distribution of betas evolves over time by plotting monthly quantiles:

```{python}
#| label: fig-603
#| fig-cap: "Monthly quantiles of beta estimates over time. The spread between high and low quantiles indicates cross-sectional dispersion in systematic risk."
#| fig-alt: "Title: Time series of beta quantiles. Lines showing the 10th through 90th percentiles of beta estimates each month."

# Compute monthly quantiles
beta_quantiles = (beta
    .groupby("date")["beta"]
    .quantile(q=[0.1, 0.25, 0.5, 0.75, 0.9])
    .reset_index()
    .rename(columns={"level_1": "quantile"})
    .assign(quantile=lambda x: (x["quantile"] * 100).astype(int).astype(str) + "th")
)

beta_quantiles_figure = (
    ggplot(beta_quantiles, aes(x="date", y="beta", color="quantile"))
    + geom_line(size=0.8)
    + labs(
        x="", y="Beta",
        color="Percentile",
        title="Monthly Distribution of Beta Estimates"
    )
    + theme(legend_position="right")
)

beta_quantiles_figure.show()
```

Several patterns emerge. First, the median beta hovers around 1.0, as expected since the market-cap-weighted average beta must equal 1.0 by construction. Second, the spread between high and low quantiles varies over time—during market stress, dispersion often increases as some stocks become more correlated with the market while others decouple. Third, there are relatively few stocks with negative betas (below the 10th percentile rarely dips below zero).

## Validation and Quality Checks

Before using beta estimates in downstream analysis, we should verify their reasonableness.

### Summary Statistics

```{python}
print("Beta Summary Statistics (Monthly Estimation)")
print("=" * 50)
print(beta["beta"].describe().round(3))
```

The summary statistics should show:
- Mean close to 1.0 (market-cap-weighted average must be 1.0)
- Reasonable range (most betas between 0 and 2)
- Few extreme outliers

### Coverage Over Time

@fig-604 shows the share of stocks with valid beta estimates over time:

```{python}
#| label: fig-604
#| fig-cap: "Share of stocks with valid beta estimates over time. Coverage improves as more historical data becomes available."
#| fig-alt: "Title: Beta estimation coverage. Line chart showing the percentage of stocks with beta estimates each month."

# Count stocks with and without beta estimates
coverage = (prices_monthly
    .loc[:, ["symbol", "date"]]
    .drop_duplicates()
    .merge(beta, on=["symbol", "date"], how="left")
    .groupby("date")
    .apply(lambda x: pd.Series({
        "coverage": x["beta"].notna().mean()
    }), include_groups=False)
    .reset_index()
)

coverage_figure = (
    ggplot(coverage, aes(x="date", y="coverage"))
    + geom_line(color="darkgreen", size=1)
    + scale_y_continuous(labels=percent_format(), limits=(0, 1))
    + labs(
        x="", y="Share with Beta Estimate",
        title="Beta Estimation Coverage Over Time"
    )
)

coverage_figure.show()
```

Coverage is low in early periods because we require 48 months of historical data. As the sample matures, coverage approaches 100%.

### Persistence of Beta Estimates

If beta estimates are meaningful, they should show some persistence—a stock's beta this month should be correlated with its beta next month:

```{python}
# Compute beta persistence (autocorrelation)
beta_persistence = (beta
    .sort_values(["symbol", "date"])
    .assign(beta_lag=lambda x: x.groupby("symbol")["beta"].shift(1))
    .dropna()
)

correlation = beta_persistence["beta"].corr(beta_persistence["beta_lag"])
print(f"Beta autocorrelation (1-month lag): {correlation:.3f}")
```

High autocorrelation (above 0.9) indicates that beta estimates are stable and not dominated by noise.

## Storing Beta Estimates

We save our beta estimates to the database for use in subsequent chapters:

```{python}
#| eval: false

beta.to_sql(
    name="beta",
    con=tidy_finance,
    if_exists="replace",
    index=False
)

print(f"Stored {len(beta):,} beta estimates in database")
```

## Comparing Monthly and Daily Estimates

When both monthly and daily data are available, comparing the two estimation approaches provides useful validation:

```{python}
#| eval: false

if has_daily_data and "beta_daily" in dir():
    # Merge monthly and daily estimates
    comparison = (beta_monthly
        .merge(
            beta_daily,
            on=["symbol", "date"],
            suffixes=("_monthly", "_daily"),
            how="inner"
        )
    )
    
    # Correlation between estimators
    corr = comparison["beta_monthly"].corr(comparison["beta_daily"])
    print(f"Correlation between monthly and daily beta estimates: {corr:.3f}")
    
    # Summary comparison
    print("\nMonthly vs Daily Beta Comparison:")
    print(comparison[["beta_monthly", "beta_daily"]].describe().round(3))
```

We expect positive correlation between the two estimators, though not perfect agreement since they use different data frequencies and window lengths. Daily estimates tend to be noisier but more responsive to recent changes.

## Practical Considerations

### Choosing the Estimation Window

The choice of look-back period involves a bias-variance tradeoff:

- **Longer windows** (e.g., 60 months) provide more observations and more precise estimates, but may miss structural changes in systematic risk.
- **Shorter windows** (e.g., 24 months) respond quickly to changes but produce noisier estimates.

For most applications, 36-60 months is standard for monthly data. For daily data, 1-3 months provides reasonable estimates.

### Handling Thinly Traded Stocks

Stocks that don't trade frequently pose challenges:
- Missing returns create gaps in the time series
- Non-synchronous trading biases beta toward zero

Our minimum observation requirement (48 of 60 months) partially addresses this by excluding stocks with too many missing values.

### When Beta Fails

Remember that beta measures historical covariance with the market. It may fail to capture:
- Future changes in business model or leverage
- Tail risks not reflected in normal-period correlations
- Sector-specific risks during industry downturns

Beta is a useful summary statistic, not a complete description of risk.

## Key Takeaways

This chapter implemented rolling-window beta estimation for Vietnamese stocks. The main insights are:

1. **Beta measures systematic risk**: The CAPM regression coefficient on market excess returns captures a stock's sensitivity to market-wide movements.

2. **Rolling windows capture time variation**: Betas change as companies evolve. A 60-month rolling window balances stability and responsiveness for monthly data.

3. **Parallelization enables scale**: Estimating betas for thousands of stocks requires distributing computation across CPU cores using libraries like `joblib`.

4. **Data frequency matters**: Monthly and daily estimates capture different dynamics. Monthly provides stability; daily provides responsiveness.

5. **Industry patterns are intuitive**: Cyclical industries show higher betas; defensive industries show lower betas.

6. **Validation is essential**: Summary statistics, coverage analysis, and persistence checks help identify estimation problems.

7. **Beta has limitations**: Historical covariance may not predict future risk exposure, especially during structural changes or tail events.

With time-varying beta estimates stored in our database, subsequent chapters can use them for portfolio construction, risk management, and tests of the CAPM's empirical predictions.
