[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Finance in Vietnam",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Tidy Finance in Vietnam",
    "section": "Motivation",
    "text": "Motivation\nEmpirical finance has undergone a fundamental transformation over the past two decades. Advances in computational capacity, open-source statistical software, and data availability have reshaped how financial research is conducted, evaluated, and disseminated. Increasingly, credible empirical work is expected to be transparent, replicable, and extensible, with results generated through scripted workflows rather than manual intervention. Reproducibility, defined as the ability for independent researchers to regenerate empirical results using the same data and methods, has thus become a core norm in modern financial economics.\nDespite this progress, the adoption of reproducible research practices has been uneven across markets. In developed financial systems, particularly those with long-established databases and standardized reporting regimes, reproducible empirical workflows are now commonplace. In contrast, research on emerging and frontier markets frequently relies on fragmented datasets, undocumented data cleaning procedures, and implicit institutional assumptions that are difficult to verify or extend. As a result, empirical findings in these markets are often fragile, non-comparable across studies, and costly to update as new data become available.\nThis book addresses that gap.\nIt develops a reproducible empirical finance framework designed explicitly for emerging and frontier markets, using Vietnam as a primary empirical case. Rather than adapting developed-market research pipelines post hoc, the book begins from the institutional and data realities of a fast-growing, retail-dominated, regulation-intensive market and builds methodological solutions accordingly. The objective is not merely to analyze Vietnam’s financial markets, but to demonstrate how reproducible finance principles can be extended, stress-tested, and refined in environments characterized by data scarcity, institutional heterogeneity, and rapid structural change.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-emerging-markets-require-different-empirical-infrastructure",
    "href": "index.html#why-emerging-markets-require-different-empirical-infrastructure",
    "title": "Tidy Finance in Vietnam",
    "section": "Why Emerging Markets Require Different Empirical Infrastructure",
    "text": "Why Emerging Markets Require Different Empirical Infrastructure\nMuch of modern empirical finance implicitly assumes the existence of stable, high-frequency, institutionally harmonized datasets. These assumptions are rarely stated, yet they are deeply embedded in standard research designs: survivorship-free security histories, consistent accounting standards, unrestricted trading mechanisms, and deep institutional liquidity.\nEmerging and frontier markets challenge each of these assumptions.\nIn Vietnam, as in many comparable economies, equity markets exhibit binding daily price limits, episodic trading halts, concentrated state ownership, and a predominance of retail investors. Financial disclosures reflect local accounting standards and evolving regulatory frameworks. Corporate actions are frequent, inconsistently documented, and occasionally revised ex post.\nThese characteristics are not inconveniences to be eliminated through aggressive data cleaning. They shape return dynamics, risk premia, factor construction, and statistical inference itself. An empirical framework that ignores these institutional features risks producing results that are internally inconsistent or externally misleading. A reproducible approach for emerging markets must therefore encode institutional context directly into data schemas, transformation logic, and modeling choices.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#reproducibility-as-a-research-design-principle",
    "href": "index.html#reproducibility-as-a-research-design-principle",
    "title": "Tidy Finance in Vietnam",
    "section": "Reproducibility as a Research Design Principle",
    "text": "Reproducibility as a Research Design Principle\nIn this book, reproducibility extends beyond the narrow notion of code availability. It is treated as an organizing principle governing the entire empirical research lifecycle.\nFirst, all datasets are constructed from raw inputs through documented, deterministic transformations, ensuring clear data provenance. Second, empirical methods are implemented in a manner that makes modeling assumptions explicit and modifiable. Third, results are generated through scripted pipelines rather than interactive analysis, guaranteeing that updates to data or parameters propagate consistently throughout the analysis. Finally, empirical designs are modular, allowing researchers to substitute markets, sample periods, or variable definitions without rewriting entire workflows.\nThis approach draws methodological inspiration from the broader reproducible research movement in economics and finance (e.g., Gentzkow and Shapiro (2014); Vilhuber (2020)), while deliberately extending it beyond its original institutional and data environment. The goal is not to reproduce existing studies, but to enable new ones, particularly those that would otherwise be impractical due to fragmented data and institutional complexity.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#vietnam-as-a-case-not-an-exception",
    "href": "index.html#vietnam-as-a-case-not-an-exception",
    "title": "Tidy Finance in Vietnam",
    "section": "Vietnam as a Case, Not an Exception",
    "text": "Vietnam as a Case, Not an Exception\nVietnam serves as the central empirical case throughout the book, but it is not treated as an idiosyncratic exception. Instead, it is presented as a representative example of a class of markets that occupy an intermediate position between frontier and emerging status: large enough to sustain active equity trading, yet still evolving in terms of regulation, disclosure quality, and investor composition.\nBy grounding methodological development in Vietnam’s market structure, the book aims to produce insights that generalize to other contexts, including Southeast Asia, South Asia, Sub-Saharan Africa, and parts of Latin America. Each empirical chapter emphasizes which components are market-specific and which are portable, encouraging readers to adapt the framework rather than adopt it wholesale.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contribution-and-audience",
    "href": "index.html#contribution-and-audience",
    "title": "Tidy Finance in Vietnam",
    "section": "Contribution and Audience",
    "text": "Contribution and Audience\nThis book makes three primary contributions.\nFirst, it proposes a reproducible empirical finance framework explicitly designed for emerging and frontier markets, integrating institutional detail into data construction and model design. Second, it provides original empirical evidence on asset pricing, liquidity, and market microstructure in Vietnam using consistently constructed datasets. Third, it delivers publication-ready, end-to-end research workflows suitable for academic research, policy analysis, and applied financial work.\nThe intended audience includes graduate students in finance and economics, academic researchers working on non-developed markets, and practitioners interested in systematic analysis of emerging market equities. Familiarity with basic asset pricing theory and statistical programming is assumed, but no prior experience with Vietnam or similar markets is required.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-book",
    "href": "index.html#structure-of-the-book",
    "title": "Tidy Finance in Vietnam",
    "section": "Structure of the Book",
    "text": "Structure of the Book\nThe chapters that follow progress from data infrastructure to empirical application. Early chapters focus on institutional context, data construction, and reproducible workflow design. Subsequent chapters develop asset pricing tests, liquidity measures, and market microstructure analyses tailored to Vietnam’s equity market. Each chapter is designed to be self-contained, yet all are linked through a common data and code architecture to ensure internal consistency.\nThe book concludes by reflecting on the broader implications of reproducible empirical finance for emerging markets research and by outlining directions for future methodological and empirical work.\n\n\n\n\nGentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for the Social Sciences: A Practitioner’s Guide.” Working Paper, University of Chicago.\n\n\nVilhuber, Lars. 2020. “Reproducibility and Replicability in Economics.” Harvard Data Science Review 2 (4): 1–39.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00_institutional_background.html",
    "href": "00_institutional_background.html",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "",
    "text": "1.1 Evolution of Vietnam’s Equity Market\nEmpirical analysis of financial markets is inseparable from institutional context. Market design, regulatory constraints, ownership structure, and investor composition shape observed prices, volumes, and returns. In developed markets, many of these features are sufficiently stable and standardized that they fade into the background of empirical research. In emerging markets, by contrast, institutional features are often first-order determinants of empirical outcomes.\nThis chapter provides the institutional foundation for the empirical analyses developed later in the book. It describes the structure of Vietnam’s equity market, the regulatory environment governing trading and disclosure, and the characteristics of listed firms and investors. Rather than offering a purely descriptive account, the discussion emphasizes how institutional features map directly into data construction choices, modeling assumptions, and interpretation of empirical results.\nVietnam’s modern equity market is relatively young. Formal stock exchanges were established only in the early 2000s, as part of broader economic reforms aimed at transitioning from a centrally planned system toward a market-oriented economy. Since then, market capitalization, trading volume, and the number of listed firms have grown rapidly, albeit unevenly across sectors and time.\nThe pace of market development has been shaped by a combination of gradual privatization of state-owned enterprises, episodic regulatory reform, and sustained participation by retail investors. Unlike markets that evolved alongside large institutional investor bases, Vietnam’s equity market matured in an environment where individual investors dominate trading activity and informational asymmetries remain substantial.\nThese features have important empirical implications. Return dynamics may reflect behavioral trading patterns, liquidity shocks can be amplified by coordinated retail activity, and firm-level information is incorporated into prices at varying speeds. A reproducible empirical framework must therefore be capable of capturing these dynamics without imposing assumptions derived from institutionally different markets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#exchange-structure-and-trading-mechanisms",
    "href": "00_institutional_background.html#exchange-structure-and-trading-mechanisms",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.2 Exchange Structure and Trading Mechanisms",
    "text": "1.2 Exchange Structure and Trading Mechanisms\nVietnam operates multiple equity exchanges, each with distinct listing requirements and trading rules. Trading is conducted through a centralized limit order book, with price-time priority determining execution. Importantly, daily price limits constrain the maximum allowable price movement for individual securities. These limits vary by exchange and security type and are binding during periods of heightened volatility.\nPrice limits introduce mechanical truncation in observed returns, clustering at upper and lower bounds, and persistence in price movements across days. From an empirical perspective, this challenges standard assumptions about continuous price adjustment and complicates volatility estimation, momentum measurement, and event-study design.\nIn this book, price limits are treated as structural features rather than anomalies. Data pipelines explicitly preserve limit-hit indicators, and empirical models are adapted to account for constrained price dynamics. This design choice reflects a broader principle: reproducibility in emerging markets requires preserving institutional signals rather than smoothing them away.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#listing-requirements-and-firm-characteristics",
    "href": "00_institutional_background.html#listing-requirements-and-firm-characteristics",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.3 Listing Requirements and Firm Characteristics",
    "text": "1.3 Listing Requirements and Firm Characteristics\nListed firms in Vietnam exhibit substantial heterogeneity in size, ownership structure, and disclosure quality. A defining characteristic of the market is the prevalence of firms with significant state ownership, either directly or through affiliated entities. State ownership affects governance, dividend policy, risk-taking behavior, and responsiveness to market signals.\nAccounting disclosures follow Vietnamese Accounting Standards, which differ in important respects from international standards. While convergence efforts are ongoing, historical financial statements often reflect transitional rules, incomplete adoption of fair value accounting, and limited segment reporting. These features complicate cross-firm comparability and longitudinal analysis.\nFrom a reproducible research standpoint, accounting variables cannot be treated as uniform primitives. Variable definitions, reporting lags, and restatement practices must be explicitly documented and encoded into data construction logic. Later chapters demonstrate how accounting data are harmonized in a transparent, version-controlled manner without obscuring underlying institutional differences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#investor-composition-and-trading-behavior",
    "href": "00_institutional_background.html#investor-composition-and-trading-behavior",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.4 Investor Composition and Trading Behavior",
    "text": "1.4 Investor Composition and Trading Behavior\nRetail investors dominate trading volume in Vietnam’s equity market. Institutional investors, including domestic funds and foreign participants, play a growing but still secondary role. This investor composition has implications for liquidity provision, price discovery, and market stability.\nRetail-dominated markets tend to exhibit higher turnover, episodic herding behavior, and sensitivity to non-fundamental information. These patterns affect the interpretation of empirical results, particularly in studies of short-term return predictability, volume-return relations, and volatility clustering.\nRather than assuming institutional trading as the default, this book explicitly models liquidity and trading activity in a retail-centric environment. Measures of liquidity, for example, are chosen and constructed to remain meaningful in the presence of small trade sizes, intermittent trading, and order imbalances driven by individual investors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#regulatory-environment-and-market-frictions",
    "href": "00_institutional_background.html#regulatory-environment-and-market-frictions",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.5 Regulatory Environment and Market Frictions",
    "text": "1.5 Regulatory Environment and Market Frictions\nRegulatory oversight of Vietnam’s equity market has evolved alongside market development. Trading rules, disclosure requirements, and foreign ownership limits have been periodically revised, sometimes with limited backward compatibility. Regulatory changes can induce structural breaks in data that are not immediately apparent in raw time series.\nShort-selling constraints, limited securities lending, and restrictions on derivative usage further distinguish Vietnam’s market from developed counterparts. These frictions affect arbitrage activity and the feasibility of certain trading strategies, influencing observed return patterns and factor realizations.\nA key principle of the empirical framework developed in this book is regulatory awareness. Data pipelines incorporate regulatory timelines, and empirical tests are designed to be robust to rule changes. This ensures that results are interpretable within the institutional regime in which they arise.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#implications-for-empirical-design",
    "href": "00_institutional_background.html#implications-for-empirical-design",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.6 Implications for Empirical Design",
    "text": "1.6 Implications for Empirical Design\nThe institutional features described in this chapter motivate several design choices that recur throughout the book:\n\nData preservation over simplification: Institutional constraints such as price limits and trading halts are retained and explicitly modeled.\nModular variable construction: Accounting and market variables are constructed through transparent functions that can be adjusted as standards evolve.\nRegime sensitivity: Empirical analyses are structured to detect and accommodate regulatory and structural breaks.\nContext-aware interpretation: Results are interpreted in light of market structure rather than benchmarked mechanically against developed-market findings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#summary",
    "href": "00_institutional_background.html#summary",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nVietnam’s equity market combines rapid growth with distinctive institutional features that challenge conventional empirical finance methods. Price limits, retail investor dominance, state ownership, and evolving regulation shape market outcomes in ways that cannot be ignored or abstracted away. For researchers working in such environments, reproducibility requires more than clean code and documented data—it requires embedding institutional context directly into empirical design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html",
    "href": "01_working_with_stock_returns.html",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "",
    "text": "2.1 Data Access and Preparation\nThis chapter develops a practical framework for transforming raw equity price records into return series suitable for empirical financial analysis. The focus is on methodological clarity and reproducibility, with particular attention to data issues that are prevalent in emerging equity markets such as Vietnam.\nThe discussion proceeds from individual stocks to a broad market cross-section, using constituents of the VN30 index as the primary empirical setting.\nWe begin by loading the core numerical and data manipulation libraries. These provide all functionality required for return construction without relying on specialized financial wrappers.\nimport pandas as pd\nimport numpy as np\nHistorical price data are stored in an S3-compatible object storage system. Access credentials are supplied via environment variables, which keeps sensitive information separate from the analysis and supports collaborative reproducibility.\nimport os\nimport boto3\nfrom botocore.client import Config\n\n\nclass ObjectStorage:\n    def __init__(self):\n        self.client = boto3.client(\n            \"s3\",\n            endpoint_url=os.environ[\"MINIO_ENDPOINT\"],\n            aws_access_key_id=os.environ[\"MINIO_ACCESS_KEY\"],\n            aws_secret_access_key=os.environ[\"MINIO_SECRET_KEY\"],\n            region_name=os.getenv(\"MINIO_REGION\", \"us-east-1\"),\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n\nstorage = ObjectStorage()\nbucket = os.environ[\"MINIO_BUCKET\"]\nThe daily price file is read directly into memory. We explicitly parse dates and harmonize variable names to avoid ambiguity in later steps.\nfrom io import BytesIO\n\nprices = pd.read_csv(\n    BytesIO(\n        storage.client.get_object(\n            Bucket=bucket,\n            Key=\"historycal_price/dataset_historical_price.csv\",\n        )[\"Body\"].read()\n    ),\n    low_memory=False,\n)\n\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\n\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\n\nprices = prices.rename(\n    columns={\n        \"vol_total\": \"volume\",\n        \"open_price\": \"open\",\n        \"low_price\": \"low\",\n        \"high_price\": \"high\",\n        \"close_price\": \"close\",\n    }\n)\n\nprices = prices.sort_values([\"symbol\", \"date\"])\nAdjusted closing prices incorporate mechanical changes due to corporate actions such as cash dividends and stock splits. Using adjusted prices ensures that subsequent return calculations reflect investor-relevant performance rather than accounting artifacts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#examining-a-single-equity",
    "href": "01_working_with_stock_returns.html#examining-a-single-equity",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.2 Examining a Single Equity",
    "text": "2.2 Examining a Single Equity\nTo ground the discussion, we isolate the trading history of a single large-cap stock, FPT, over a long sample period.\n\nimport datetime as dt\n\nstart = pd.Timestamp(\"2000-01-01\")\nend = pd.Timestamp(dt.date.today().year - 1, 12, 31)\n\n\nfpt = prices.loc[\n    (prices[\"symbol\"] == \"FPT\")\n    & (prices[\"date\"] &gt;= start)\n    & (prices[\"date\"] &lt;= end),\n    [\"date\", \"symbol\", \"volume\", \"open\", \"low\", \"high\", \"close\", \"adjusted_close\"],\n].copy()\n\nThis subset contains the standard daily market variables required for most empirical studies. Before computing returns, it is good practice to visually inspect the price series.\n\nfrom plotnine import ggplot, aes, geom_line, labs\n\n\n(\n    ggplot(fpt, aes(x=\"date\", y=\"adjusted_close\"))\n    + geom_line()\n    + labs(title=\"Adjusted price path of FPT\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 2.1: Prices are in VND, adjusted for dividend payments and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#from-prices-to-returns",
    "href": "01_working_with_stock_returns.html#from-prices-to-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.3 From Prices to Returns",
    "text": "2.3 From Prices to Returns\nMost empirical asset pricing models are formulated in terms of returns rather than price levels. The simple daily return is defined as\n\\[\nr_t = \\frac{p_t}{p_t - 1} - 1,\n\\]\nwhere \\(p_t\\) denotes the adjusted closing price at the end of trading day \\(t\\).\nBefore computing returns, we must address invalid price observations. In Vietnamese equity data, adjusted prices occasionally take the value zero. These entries typically arise from IPO placeholders, trading suspensions, or historical backfilling conventions and cannot be used to compute meaningful returns.\n\nprices.loc[prices[\"adjusted_close\"] &lt;= 0, [\"symbol\", \"date\", \"adjusted_close\"]].head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\n\n\n\n\n33886\nADP\n2010-02-09\n0.0\n\n\n33887\nADP\n2010-02-24\n0.0\n\n\n33888\nADP\n2010-03-01\n0.0\n\n\n33889\nADP\n2010-03-03\n0.0\n\n\n33890\nADP\n2010-03-12\n0.0\n\n\n\n\n\n\n\nWe therefore exclude non-positive adjusted prices and compute returns stock by stock. Correct chronological ordering is essential.\n\nreturns = (\n    prices\n    .loc[prices[\"adjusted_close\"] &gt; 0]\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n)\nreturns = returns.dropna(subset=[\"ret\"])\n\nThe initial return for each stock is missing by construction, since no lagged price is available. These observations are mechanical and can safely be removed in most applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "href": "01_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.4 Limiting the Influence of Extreme Returns",
    "text": "2.4 Limiting the Influence of Extreme Returns\nDaily return series often contain extreme observations driven by data errors, thin trading, or abrupt price adjustments. A common approach is to winsorize returns using cross-sectional percentile cutoffs.\n\ndef winsorize_cs(df, column=\"ret\", lower_q=0.01, upper_q=0.99):\n    lo = df[column].quantile(lower_q)\n    hi = df[column].quantile(upper_q)\n    out = df.copy()\n    out[column] = out[column].clip(lo, hi)\n    return out\n\nreturns = winsorize_cs(returns)\n\nApplying winsorization across the full cross-section limits the impact of extreme market-wide observations while preserving relative differences between firms. Winsorizing within each stock is rarely appropriate in panel settings and can severely distort illiquid securities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#distributional-features-of-returns",
    "href": "01_working_with_stock_returns.html#distributional-features-of-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.5 Distributional Features of Returns",
    "text": "2.5 Distributional Features of Returns\nWe next examine the empirical distribution of daily returns for FPT. The figure below also marks the historical 5 percent quantile, which provides a simple, non-parametric measure of downside risk.\n\nfrom mizani.formatters import percent_format\nfrom plotnine import geom_histogram, geom_vline, scale_x_continuous\n\n\nfpt_ret = returns.loc[returns[\"symbol\"] == \"FPT\"].copy()\nq05 = fpt_ret[\"ret\"].quantile(0.05)\n\n\n(\n    ggplot(fpt_ret, aes(x=\"ret\"))\n    + geom_histogram(bins=100)\n    + geom_vline(xintercept=q05, linetype=\"dashed\")\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"Distribution of daily FPT returns\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 2.2: The dotted vertical line indicates the historical five percent quantile.\n\n\n\n\n\nSummary statistics offer a compact description of return behavior and should always be inspected before formal modeling.\n\nreturns[\"ret\"].describe().round(3)\n\ncount    4305063.000\nmean           0.000\nstd            0.035\nmin           -0.125\n25%           -0.004\n50%            0.000\n75%            0.003\nmax            0.130\nName: ret, dtype: float64\n\n\nComputing these statistics by calendar year can reveal periods of elevated volatility or structural change.\n\n(\n    returns\n    .assign(year=lambda x: x[\"date\"].dt.year)\n    .groupby(\"year\")[\"ret\"]\n    .describe()\n    .round(3)\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n2010\n131548.0\n-0.001\n0.036\n-0.125\n-0.021\n0.0\n0.018\n0.13\n\n\n2011\n166826.0\n-0.003\n0.033\n-0.125\n-0.020\n0.0\n0.011\n0.13\n\n\n2012\n177938.0\n0.000\n0.033\n-0.125\n-0.012\n0.0\n0.015\n0.13\n\n\n2013\n180417.0\n0.001\n0.033\n-0.125\n-0.004\n0.0\n0.008\n0.13\n\n\n2014\n181907.0\n0.001\n0.034\n-0.125\n-0.008\n0.0\n0.011\n0.13\n\n\n2015\n197881.0\n0.000\n0.033\n-0.125\n-0.006\n0.0\n0.005\n0.13\n\n\n2016\n227896.0\n0.000\n0.035\n-0.125\n-0.005\n0.0\n0.003\n0.13\n\n\n2017\n283642.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.001\n0.13\n\n\n2018\n329887.0\n0.000\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2019\n352754.0\n0.000\n0.033\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2020\n369367.0\n0.001\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2021\n379415.0\n0.002\n0.038\n-0.125\n-0.005\n0.0\n0.007\n0.13\n\n\n2022\n387050.0\n-0.001\n0.038\n-0.125\n-0.008\n0.0\n0.004\n0.13\n\n\n2023\n391605.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.002\n0.13\n\n\n2024\n400379.0\n0.000\n0.031\n-0.125\n-0.002\n0.0\n0.000\n0.13\n\n\n2025\n146551.0\n0.000\n0.037\n-0.125\n-0.004\n0.0\n0.002\n0.13",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "href": "01_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.6 Expanding to a Market Cross-Section",
    "text": "2.6 Expanding to a Market Cross-Section\nThe same procedures apply naturally to a larger universe of stocks. We now restrict attention to the constituents of the VN30 index.\n\nvn30 = [\n    \"ACB\",\"BCM\",\"BID\",\"BVH\",\"CTG\",\"FPT\",\"GAS\",\"GVR\",\"HDB\",\"HPG\",\n    \"MBB\",\"MSN\",\"MWG\",\"PLX\",\"POW\",\"SAB\",\"SHB\",\"SSB\",\"STB\",\"TCB\",\n    \"TPB\",\"VCB\",\"VHM\",\"VIB\",\"VIC\",\"VJC\",\"VNM\",\"VPB\",\"VRE\",\"EIB\",\n]\n\n\nprices_vn30 = prices.loc[prices[\"symbol\"].isin(vn30)]\nfrom plotnine import theme\n\n\n(\n    ggplot(prices_vn30, aes(x=\"date\", y=\"adjusted_close\", color=\"symbol\"))\n    + geom_line()\n    + labs(title=\"Adjusted prices of VN30 constituents\", x=\"\", y=\"\")\n    + theme(legend_position=\"none\")\n)\n\n\n\n\n\n\n\nFigure 2.3: Prices in VND, adjusted for dividend payments and stock splits.\n\n\n\n\n\nReturns for the VN30 universe are computed analogously.\n\nreturns_vn30 = (\n    prices_vn30\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n    .dropna()\n)\n\n\nreturns_vn30.groupby(\"symbol\")[\"ret\"].describe().round(3)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\n\n\nACB\n3822.0\n-0.000\n0.023\n-0.407\n-0.006\n0.0\n0.007\n0.097\n\n\nBCM\n1795.0\n0.001\n0.027\n-0.136\n-0.010\n0.0\n0.010\n0.159\n\n\nBID\n2811.0\n0.000\n0.024\n-0.369\n-0.010\n0.0\n0.011\n0.070\n\n\nBVH\n3825.0\n0.000\n0.024\n-0.097\n-0.012\n0.0\n0.012\n0.070\n\n\nCTG\n3825.0\n0.000\n0.024\n-0.376\n-0.010\n0.0\n0.010\n0.070\n\n\nEIB\n3825.0\n-0.000\n0.022\n-0.302\n-0.008\n0.0\n0.008\n0.070\n\n\nFPT\n3825.0\n-0.000\n0.024\n-0.439\n-0.008\n0.0\n0.009\n0.070\n\n\nGAS\n3236.0\n0.000\n0.022\n-0.289\n-0.009\n0.0\n0.010\n0.070\n\n\nGVR\n1775.0\n0.001\n0.030\n-0.137\n-0.014\n0.0\n0.016\n0.169\n\n\nHDB\n1828.0\n-0.001\n0.028\n-0.391\n-0.009\n0.0\n0.010\n0.070\n\n\nHPG\n3825.0\n-0.001\n0.032\n-0.581\n-0.010\n0.0\n0.011\n0.070\n\n\nMBB\n3371.0\n-0.000\n0.023\n-0.473\n-0.008\n0.0\n0.008\n0.069\n\n\nMSN\n3825.0\n0.000\n0.024\n-0.553\n-0.010\n0.0\n0.010\n0.070\n\n\nMWG\n2701.0\n-0.000\n0.035\n-0.751\n-0.009\n0.0\n0.011\n0.070\n\n\nPLX\n2009.0\n-0.000\n0.021\n-0.140\n-0.010\n0.0\n0.010\n0.070\n\n\nPOW\n1784.0\n0.000\n0.023\n-0.071\n-0.012\n0.0\n0.011\n0.102\n\n\nSAB\n2100.0\n-0.000\n0.024\n-0.745\n-0.008\n0.0\n0.007\n0.070\n\n\nSHB\n3824.0\n-0.000\n0.028\n-0.338\n-0.013\n0.0\n0.013\n0.100\n\n\nSSB\n1029.0\n-0.000\n0.023\n-0.292\n-0.005\n0.0\n0.004\n0.070\n\n\nSTB\n3825.0\n0.000\n0.024\n-0.321\n-0.010\n0.0\n0.010\n0.070\n\n\nTCB\n1732.0\n-0.000\n0.035\n-0.884\n-0.009\n0.0\n0.010\n0.070\n\n\nTPB\n1761.0\n-0.001\n0.029\n-0.477\n-0.009\n0.0\n0.009\n0.070\n\n\nVCB\n3825.0\n-0.000\n0.024\n-0.539\n-0.009\n0.0\n0.009\n0.070\n\n\nVHM\n1744.0\n-0.000\n0.024\n-0.419\n-0.009\n0.0\n0.008\n0.070\n\n\nVIB\n2072.0\n-0.000\n0.031\n-0.489\n-0.009\n0.0\n0.010\n0.109\n\n\nVIC\n3825.0\n-0.000\n0.027\n-0.673\n-0.008\n0.0\n0.008\n0.070\n\n\nVJC\n2046.0\n-0.000\n0.020\n-0.455\n-0.007\n0.0\n0.006\n0.070\n\n\nVNM\n3825.0\n-0.000\n0.023\n-0.547\n-0.007\n0.0\n0.007\n0.070\n\n\nVPB\n1927.0\n-0.000\n0.033\n-0.678\n-0.010\n0.0\n0.010\n0.070\n\n\nVRE\n1871.0\n-0.000\n0.024\n-0.295\n-0.012\n0.0\n0.011\n0.070",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#aggregating-returns-across-time",
    "href": "01_working_with_stock_returns.html#aggregating-returns-across-time",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.7 Aggregating Returns Across Time",
    "text": "2.7 Aggregating Returns Across Time\nFinancial variables are observed at different frequencies. While equity prices are recorded daily, many empirical questions require monthly or annual returns. Lower-frequency returns are constructed by compounding higher-frequency observations.\n\nreturns_monthly = (\n    returns_vn30\n    .assign(month=lambda x: x[\"date\"].dt.to_period(\"M\").dt.to_timestamp())\n    .groupby([\"symbol\", \"month\"], as_index=False)\n    .agg(ret=(\"ret\", lambda x: np.prod(1 + x) - 1))\n)\n\nComparing daily and monthly return distributions illustrates how aggregation dampens volatility and alters tail behavior.\n\nfrom plotnine import facet_wrap\n\nfpt_d = returns_vn30.loc[returns_vn30[\"symbol\"] == \"FPT\"].assign(freq=\"Daily\")\nfpt_m = returns_monthly.loc[returns_monthly[\"symbol\"] == \"FPT\"].assign(freq=\"Monthly\")\n\n\nfpt_both = pd.concat([\n    fpt_d[[\"ret\", \"freq\"]],\n    fpt_m[[\"ret\", \"freq\"]],\n])\n\n\n(\n    ggplot(fpt_both, aes(x=\"ret\"))\n    + geom_histogram(bins=50)\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"FPT returns at different frequencies\", x=\"\", y=\"\")\n    + facet_wrap(\"freq\", scales=\"free\")\n)\n\n\n\n\n\n\n\nFigure 2.4: Returns are based on prices adjusted for dividend payments and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "href": "01_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.8 Aggregation Across Firms: Trading Activity",
    "text": "2.8 Aggregation Across Firms: Trading Activity\nAggregation is not limited to time. In some settings, it is informative to aggregate variables across firms. As an illustration, we compute total daily trading value for VN30 stocks by multiplying share volume by adjusted prices and summing across firms.\n\ntrading_value = (\n    prices_vn30\n    .assign(value=lambda x: x[\"volume\"] * x[\"adjusted_close\"] / 1e9)\n    .groupby(\"date\")[\"value\"]\n    .sum()\n    .reset_index()\n    .assign(value_lag=lambda x: x[\"value\"].shift(1))\n)\n(\n    ggplot(trading_value, aes(x=\"date\", y=\"value\"))\n    + geom_line()\n    + labs(title=\"Aggregate VN30 trading value (billion VND)\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\n\nFinally, we assess persistence in trading activity by comparing trading value on consecutive days.\n\nfrom plotnine import geom_point, geom_abline\n\n\n(\n    ggplot(trading_value, aes(x=\"value_lag\", y=\"value\"))\n    + geom_point()\n    + geom_abline(intercept=0, slope=1, linetype=\"dashed\")\n    + labs(\n        title=\"Persistence in VN30 trading value\",\n        x=\"Previous day\",\n        y=\"Current day\",\n    )\n)\n\n\n\n\n\n\n\nFigure 2.5: Total daily trading volume.\n\n\n\n\n\nA strong alignment along the 45-degree line indicates that high-activity trading days tend to be followed by similarly active days, a common empirical regularity in equity markets.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#summary",
    "href": "01_working_with_stock_returns.html#summary",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nThis chapter established a reproducible workflow for transforming raw price data into return series, diagnosing common data issues, and aggregating information across time and firms. These steps provide the empirical foundation for subsequent analyses of risk, return predictability, and market dynamics in Vietnam’s equity market.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html",
    "href": "02_modern_portfolio_theory.html",
    "title": "3  Modern Portfolio Theory",
    "section": "",
    "text": "3.0.1 The Core Insight: Diversification as a Free Lunch\nIn the previous chapter, we showed how to download and analyze stock market data with figures and summary statistics. Now, we turn to one of the most fundamental questions in finance: How should an investor allocate their wealth across assets that differ in expected returns, variance, and correlations to optimize their portfolio’s performance?\nThis question might seem straightforward at first glance. Why not simply invest everything in the asset with the highest expected return? The answer lies in a profound insight that transformed financial economics: risk matters, and it can be managed through diversification.\nModern Portfolio Theory (MPT), introduced by Markowitz (1952), revolutionized investment decision-making by formalizing the trade-off between risk and expected return. Before Markowitz, investors largely thought about risk on a security-by-security basis. Markowitz’s genius was recognizing that what matters is not the risk of individual securities in isolation, but how they contribute to the risk of the entire portfolio. This insight was so influential that it earned him the Sveriges Riksbank Prize in Economic Sciences in 1990 and laid the foundation for much of modern finance.\nMPT relies on a crucial mathematical fact: portfolio risk depends not only on individual asset volatilities but also on the correlations between asset returns. This insight reveals the power of diversification—combining assets whose returns don’t move in perfect lockstep can reduce overall portfolio risk without necessarily sacrificing expected return.\nConsider a simple analogy: Imagine you run a business selling both sunscreen and umbrellas. On sunny days, sunscreen sales boom but umbrella sales suffer; on rainy days, the reverse happens. By selling both products, your total revenue becomes more stable than if you sold only one. The “correlation” between sunscreen and umbrella sales is negative, and combining them reduces the variance of your overall income. This is precisely the logic behind portfolio diversification.\nThe fruit basket analogy offers another perspective: If all you have are apples and they spoil, you lose everything. With a variety of fruits, some may spoil, but others will stay fresh. Diversification provides insurance against the idiosyncratic risks of individual assets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-asset-universe-setting-up-the-problem",
    "href": "02_modern_portfolio_theory.html#the-asset-universe-setting-up-the-problem",
    "title": "3  Modern Portfolio Theory",
    "section": "3.1 The Asset Universe: Setting Up the Problem",
    "text": "3.1 The Asset Universe: Setting Up the Problem\nSuppose \\(N\\) different risky assets are available to the investor. Each asset \\(i\\) is characterized by:\n\nExpected return \\(\\mu_i\\): The anticipated profit from holding the asset for one period\nVariance \\(\\sigma_i^2\\): The dispersion of returns around the mean\nCovariances \\(\\sigma_{ij}\\): The degree to which asset \\(i\\)’s returns move together with asset \\(j\\)’s returns\n\nThe investor chooses portfolio weights \\(\\omega_i\\) for each asset \\(i\\). These weights represent the fraction of total wealth invested in each asset. We impose the constraint that weights sum to one:\n\\[\n\\sum_{i=1}^N \\omega_i = 1\n\\]\nThis “budget constraint” ensures that the investor is fully invested—there is no outside option such as keeping money under a mattress. Note that we allow weights to be negative (short selling) or greater than one (leverage), though in practice these positions may face constraints.\n\n3.1.1 The Two Stages of Portfolio Selection\nAccording to Markowitz (1952), portfolio selection involves two distinct stages:\n\nEstimation: Forming expectations about future security performance based on observations, experience, and economic reasoning\nOptimization: Using these expectations to choose an optimal portfolio\n\nIn practice, these stages cannot be fully separated. The estimation stage determines the inputs (\\(\\mu\\), \\(\\Sigma\\)) that feed into the optimization stage. Poor estimation leads to poor portfolio choices, regardless of how sophisticated the optimization procedure.\nTo keep things conceptually clear, we focus primarily on the optimization stage in this chapter. We treat the expected returns and variance-covariance matrix as known, using historical data to compute reasonable proxies. In later chapters, we address the substantial challenges that arise from estimation uncertainty.\n\n\n3.1.2 Loading and Preparing the Data\nWe work with the VN30 index constituents—the 30 largest and most liquid stocks on Vietnam’s Ho Chi Minh Stock Exchange. This provides a realistic asset universe for a domestic Vietnamese investor.\n\nvn30_symbols = [\n    \"ACB\",\"BCM\",\"BID\",\"BVH\",\"CTG\",\"FPT\",\"GAS\",\"GVR\",\"HDB\",\"HPG\",\n    \"MBB\",\"MSN\",\"MWG\",\"PLX\",\"POW\",\"SAB\",\"SHB\",\"SSB\",\"STB\",\"TCB\",\n    \"TPB\",\"VCB\",\"VHM\",\"VIB\",\"VIC\",\"VJC\",\"VNM\",\"VPB\",\"VRE\",\"EIB\"\n]\n\nWe load the historical price data:\n\nimport pandas as pd\nfrom io import BytesIO\nimport datetime as dt\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\nWe filter to keep only the VN30 constituents:\n\nprices_daily = prices[prices[\"symbol\"].isin(vn30_symbols)]\nprices_daily[[\"date\", \"symbol\", \"adjusted_close\"]].head(3)\n\n\n\n\n\n\n\n\ndate\nsymbol\nadjusted_close\n\n\n\n\n18176\n2010-01-04\nACB\n329.408244\n\n\n18177\n2010-01-05\nACB\n329.408244\n\n\n18178\n2010-01-06\nACB\n320.258015\n\n\n\n\n\n\n\n\n\n3.1.3 Computing Expected Returns\nThe sample mean return serves as our proxy for expected returns. For each asset \\(i\\), we compute:\n\\[\n\\hat{\\mu}_i = \\frac{1}{T} \\sum_{t=1}^{T} r_{i,t}\n\\]\nwhere \\(r_{i,t}\\) is the return of asset \\(i\\) in period \\(t\\), and \\(T\\) is the total number of periods.\nWhy monthly returns? While daily data provides more observations, monthly returns offer several advantages for portfolio optimization. First, monthly returns are less noisy and exhibit weaker serial correlation. Second, monthly rebalancing is more realistic for most investors, avoiding excessive transaction costs. Third, the estimation error in mean returns is already substantial—using daily data doesn’t materially improve the precision of mean estimates because the mean return scales with the horizon while estimation error scales with the square root of observations.\n\nreturns_monthly = (prices_daily\n  .assign(\n    date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n  )\n  .groupby([\"symbol\", \"date\"], as_index=False)\n  .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n  .assign(\n    ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n  )\n)\n\n\n\n3.1.4 Computing Volatilities\nIndividual asset risk in MPT is quantified using variance (\\(\\sigma^2_i\\)) or its square root, the standard deviation or volatility (\\(\\sigma_i\\)). We use the sample standard deviation as our proxy:\n\\[\n\\hat{\\sigma}_i = \\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} (r_{i,t} - \\hat{\\mu}_i)^2}\n\\]\nAlternative risk measures exist, including Value-at-Risk, Expected Shortfall, and higher-order moments such as skewness and kurtosis. However, variance remains the workhorse measure in portfolio theory because of its mathematical tractability and the central role of the normal distribution in finance.\n\nassets = (returns_monthly\n  .groupby(\"symbol\", as_index=False)\n  .agg(\n    mu=(\"ret\", \"mean\"),\n    sigma=(\"ret\", \"std\")\n  )\n)\n\n\n\n3.1.5 Visualizing the Risk-Return Trade-off\nFigure 3.1 displays each asset’s expected return (vertical axis) against its volatility (horizontal axis). This “mean-standard deviation” space is fundamental to portfolio theory.\n\nassets_figure = (\n  ggplot(\n    assets, \n    aes(x=\"sigma\", y=\"mu\", label=\"symbol\")\n  )\n  + geom_point()\n  + geom_text(adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"Expected returns and volatilities of VN30 index constituents\"\n  )\n)\nassets_figure.show()\n\n\n\n\n\n\n\nFigure 3.1: Expected returns and volatilities based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nSeveral observations emerge from this figure. First, there is substantial heterogeneity in both expected returns and volatilities across stocks. Second, the relationship between risk and return is far from linear. Some high-volatility stocks have low or even negative expected returns. Third, most individual stocks appear to offer poor risk-return trade-offs. As we will see, portfolios can substantially improve upon these individual positions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-variance-covariance-matrix-capturing-asset-interactions",
    "href": "02_modern_portfolio_theory.html#the-variance-covariance-matrix-capturing-asset-interactions",
    "title": "3  Modern Portfolio Theory",
    "section": "3.2 The Variance-Covariance Matrix: Capturing Asset Interactions",
    "text": "3.2 The Variance-Covariance Matrix: Capturing Asset Interactions\n\n3.2.1 Why Correlations Matter\nA key innovation of MPT is recognizing that portfolio risk depends critically on how assets move together. The variance-covariance matrix \\(\\Sigma\\) captures all pairwise interactions between asset returns.\nTo understand why correlations matter, consider the variance of a two-asset portfolio: \\[\\sigma_p^2 = \\omega_1^2\\sigma_1^2 + \\omega_2^2\\sigma_2^2 + 2\\omega_1\\omega_2\\sigma_{12}\\]\nThe third term involves the covariance \\(\\sigma_{12} = \\rho_{12}\\sigma_1\\sigma_2\\), where \\(\\rho_{12}\\) is the correlation coefficient. When \\(\\rho_{12} &lt; 1\\), the portfolio variance is less than the weighted average of individual variances. When \\(\\rho_{12} &lt; 0\\), the diversification benefit is even more pronounced.\nThis mathematical fact has profound implications: You can reduce risk without reducing expected return by combining assets that don’t move perfectly together. This is sometimes called the “only free lunch in finance.”\n\n\n3.2.2 Computing the Variance-Covariance Matrix\nWe compute the sample covariance matrix as: \\[\\hat{\\sigma}_{ij} = \\frac{1}{T-1} \\sum_{t=1}^{T} (r_{i,t} - \\hat{\\mu}_i)(r_{j,t} - \\hat{\\mu}_j)\\]\nFirst, we reshape the returns data into a wide format with assets as columns:\n\nreturns_wide = (returns_monthly\n  .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n  .reset_index()\n)\n\nsigma = (returns_wide\n  .drop(columns=[\"date\"])\n  .cov()\n)\n\n\n\n3.2.3 Interpreting the Variance-Covariance Matrix\nThe diagonal elements of \\(\\Sigma\\) are the variances of individual assets. The off-diagonal elements are covariances, which can be positive (assets tend to move together), negative (assets tend to move in opposite directions), or zero (no linear relationship).\nFor easier interpretation, we often convert covariances to correlations: \\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sigma_i \\sigma_j}\\]\nCorrelations are bounded between -1 and +1, making them easier to compare across asset pairs.\nFigure 3.2 visualizes the variance-covariance matrix as a heatmap.\n\nsigma_long = (sigma\n  .reset_index()\n  .melt(id_vars=\"symbol\", var_name=\"symbol_b\", value_name=\"value\")\n)\n\nsigma_long[\"symbol_b\"] = pd.Categorical(\n  sigma_long[\"symbol_b\"], \n  categories=sigma_long[\"symbol_b\"].unique()[::-1],\n  ordered=True\n)\n\nsigma_figure = (\n  ggplot(\n    sigma_long, \n    aes(x=\"symbol\", y=\"symbol_b\", fill=\"value\")\n  )\n  + geom_tile()\n  + labs(\n      x=\"\", y=\"\", fill=\"(Co-)Variance\",\n      title=\"Sample variance-covariance matrix of VN30 index constituents\"\n    )\n  + scale_fill_continuous(labels=percent_format())\n  + theme(axis_text_x=element_text(angle=45, hjust=1))\n)\nsigma_figure.show()\n\n\n\n\n\n\n\nFigure 3.2: Variances and covariances based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nThe heatmap reveals important patterns. The diagonal (variances) shows which stocks are most volatile. The off-diagonal patterns show which pairs of stocks tend to move together. In general, stocks within the same sector tend to have higher correlations with each other than with stocks from different sectors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-minimum-variance-portfolio",
    "href": "02_modern_portfolio_theory.html#the-minimum-variance-portfolio",
    "title": "3  Modern Portfolio Theory",
    "section": "3.3 The Minimum-Variance Portfolio",
    "text": "3.3 The Minimum-Variance Portfolio\n\n3.3.1 Motivation: Risk Minimization as a Benchmark\nBefore considering expected returns, let’s find the portfolio that minimizes risk entirely. This minimum-variance portfolio (MVP) serves as an important benchmark and reference point. It represents what an extremely risk-averse investor—one who cares only about minimizing volatility—would choose.\n\n\n3.3.2 The Optimization Problem\nThe minimum-variance investor solves: \\[\n\\min_{\\omega} \\omega^{\\prime}\\Sigma\\omega\n\\]\nsubject to the constraint that weights sum to one:\n\\[\n\\omega^{\\prime}\\iota = 1\n\\]\nwhere \\(\\iota\\) is an \\(N \\times 1\\) vector of ones.\nIn words: minimize portfolio variance, subject to being fully invested.\n\n\n3.3.3 The Analytical Solution\nThis is a classic constrained optimization problem that can be solved using Lagrange multipliers. The Lagrangian is:\n\\[\n\\mathcal{L} = \\omega^{\\prime}\\Sigma\\omega - \\lambda(\\omega^{\\prime}\\iota - 1)\n\\]\nTaking the first-order condition with respect to \\(\\omega\\): \\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda\\iota = 0\n\\]\nSolving for \\(\\omega\\): \\[\n\\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota\n\\]\nUsing the constraint \\(\\omega^{\\prime}\\iota = 1\\) to solve for \\(\\lambda\\): \\[\n\\frac{\\lambda}{2}\\iota^{\\prime}\\Sigma^{-1}\\iota = 1 \\implies \\frac{\\lambda}{2} = \\frac{1}{\\iota^{\\prime}\\Sigma^{-1}\\iota}\n\\]\nSubstituting back: \\[\n\\omega_{\\text{mvp}} = \\frac{\\Sigma^{-1}\\iota}{\\iota^{\\prime}\\Sigma^{-1}\\iota}\n\\]\nThis elegant formula shows that the minimum-variance weights depend only on the covariance matrix—expected returns play no role. The inverse covariance matrix \\(\\Sigma^{-1}\\) determines how much to invest in each asset based on its variance and its covariances with all other assets.\n\n\n3.3.4 Implementation\n\niota = np.ones(sigma.shape[0])\nsigma_inv = np.linalg.inv(sigma.values)\nomega_mvp = (sigma_inv @ iota) / (iota @ sigma_inv @ iota)\n\n\n\n3.3.5 Visualizing the Minimum-Variance Weights\nFigure 3.3 displays the portfolio weights of the minimum-variance portfolio.\n\nassets = assets.assign(omega_mvp=omega_mvp)\n\nassets[\"symbol\"] = pd.Categorical(\n  assets[\"symbol\"],\n  categories=assets.sort_values(\"omega_mvp\")[\"symbol\"],\n  ordered=True\n)\n\nomega_figure = (\n  ggplot(\n    assets,\n    aes(y=\"omega_mvp\", x=\"symbol\", fill=\"omega_mvp&gt;0\")\n  )\n  + geom_col()\n  + coord_flip()\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", \n      y=\"Portfolio Weight\", \n      title=\"Minimum-variance portfolio weights\"\n  )\n  + theme(legend_position=\"none\")\n)\nomega_figure.show()\n\n\n\n\n\n\n\nFigure 3.3: Weights are based on historical moments of monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nSeveral features of the minimum-variance portfolio are noteworthy. First, many stocks receive zero or near-zero weights. Second, some stocks receive negative weights (short positions). These short positions are not a computational artifact, they reflect the optimizer’s attempt to exploit correlations for risk reduction. Third, the weights are quite extreme (both large positive and large negative), which often indicates estimation error amplification, which is a topic we address in later chapters.\n\n\n3.3.6 Portfolio Performance\nLet’s compute the expected return and volatility of the minimum-variance portfolio:\n\nmu = assets[\"mu\"].values\nmu_mvp = omega_mvp @ mu\nsigma_mvp = np.sqrt(omega_mvp @ sigma.values @ omega_mvp)\n\nsummary_mvp = pd.DataFrame({\n  \"mu\": [mu_mvp],\n  \"sigma\": [sigma_mvp],\n  \"type\": [\"Minimum-Variance Portfolio\"]\n})\nsummary_mvp\n\n\n\n\n\n\n\n\nmu\nsigma\ntype\n\n\n\n\n0\n-0.011424\n0.043512\nMinimum-Variance Portfolio\n\n\n\n\n\n\n\n\nmu_mvp_fmt = f\"{mu_mvp:.4f}\"\nsigma_mvp_fmt = f\"{sigma_mvp:.4f}\"\nprint(f\"The MVP return is {mu_mvp_fmt} and volatility is {sigma_mvp_fmt}.\")\n\nThe MVP return is -0.0114 and volatility is 0.0435.\n\n\nIf the expected return is negative, this is not a computational error. The minimum-variance portfolio minimizes risk without regard to expected returns. Because some assets in the sample have negative average returns, the risk-minimizing combination may inherit a negative expected return. This highlights a fundamental limitation of using historical sample means as estimates of expected returns: they are extremely noisy, and can lead to economically unintuitive results even when the optimization mathematics are working correctly.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#efficient-portfolios-balancing-risk-and-return",
    "href": "02_modern_portfolio_theory.html#efficient-portfolios-balancing-risk-and-return",
    "title": "3  Modern Portfolio Theory",
    "section": "3.4 Efficient Portfolios: Balancing Risk and Return",
    "text": "3.4 Efficient Portfolios: Balancing Risk and Return\n\n3.4.1 The Investor’s Trade-off\nIn most cases, minimizing variance is not the investor’s sole objective. A more realistic formulation allows the investor to trade off risk against expected return. The investor might be willing to accept higher portfolio variance in exchange for higher expected returns.\nAn efficient portfolio minimizes variance subject to earning at least some target expected return \\(\\bar{\\mu}\\). Formally:\n\\[\\min_{\\omega} \\omega^{\\prime}\\Sigma\\omega\\]\nsubject to: \\[\\omega^{\\prime}\\iota = 1 \\quad \\text{(fully invested)}\\] \\[\\omega^{\\prime}\\mu \\geq \\bar{\\mu} \\quad \\text{(minimum return)}\\]\nWhen \\(\\bar{\\mu}\\) exceeds the expected return of the minimum-variance portfolio, the investor accepts more risk to earn more return.\n\n\n3.4.2 Setting the Target Return\nFor illustration, suppose the investor wants to earn at least the historical average return of the best-performing stock:\n\nmu_bar = assets[\"mu\"].max()\nprint(f\"Target expected return: {mu_bar:.5f}\")\n\nTarget expected return: 0.01886\n\n\nThis is an ambitious target—it means matching the return of the single highest-returning stock while benefiting from diversification to reduce risk.\n\n\n3.4.3 The Analytical Solution\nThe constrained optimization problem with an inequality constraint on expected returns can be solved using the Karush-Kuhn-Tucker (KKT) conditions. At the optimum (assuming the return constraint binds), the solution is:\n\\[\\omega_{\\text{efp}} = \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu - \\frac{D}{C}\\Sigma^{-1}\\iota\\right)\\]\nwhere:\n\n\\(C = \\iota^{\\prime}\\Sigma^{-1}\\iota\\) (a scalar measuring the “size” of the inverse covariance matrix)\n\\(D = \\iota^{\\prime}\\Sigma^{-1}\\mu\\) (capturing the interaction between expected returns and the inverse covariance matrix)\n\\(E = \\mu^{\\prime}\\Sigma^{-1}\\mu\\) (measuring the “signal” in expected returns weighted by inverse covariances)\n\\(\\lambda^* = 2\\frac{\\bar{\\mu} - D/C}{E - D^2/C}\\) (the shadow price of the return constraint)\n\nAlternatively, we can express the efficient portfolio as a linear combination of the minimum-variance portfolio and an “excess return” portfolio:\n\\[\\omega_{\\text{efp}} = \\omega_{\\text{mvp}} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu - D \\cdot \\omega_{\\text{mvp}}\\right)\\]\nThis representation reveals important intuition: the efficient portfolio starts from the minimum-variance portfolio and tilts toward higher-expected-return assets, with the tilt magnitude determined by \\(\\lambda^*\\).\n\n\n3.4.4 Implementation\n\nC = iota @ sigma_inv @ iota\nD = iota @ sigma_inv @ mu\nE = mu @ sigma_inv @ mu\nlambda_tilde = 2 * (mu_bar - D / C) / (E - (D ** 2) / C)\nomega_efp = omega_mvp + (lambda_tilde / 2) * (sigma_inv @ mu - D * omega_mvp)\n\nmu_efp = omega_efp @ mu\nsigma_efp = np.sqrt(omega_efp @ sigma.values @ omega_efp)\n\nsummary_efp = pd.DataFrame({\n  \"mu\": [mu_efp],\n  \"sigma\": [sigma_efp],\n  \"type\": [\"Efficient Portfolio\"]\n})\n\n\n\n3.4.5 Comparing the Portfolios\nFigure 3.4 plots both portfolios alongside the individual assets.\n\nsummaries = pd.concat(\n  [assets, summary_mvp, summary_efp], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=summaries.query(\"type.isna()\"))\n  + geom_point(data=summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3)\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"Efficient & minimum-variance portfolios\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 3.4: The big dots indicate the location of the minimum-variance and the efficient portfolio that delivers the expected return of the stock with the highest return, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe figure demonstrates the substantial diversification benefits of portfolio optimization. The efficient portfolio achieves the same expected return as the highest-returning individual stock but with substantially lower volatility. This “free lunch” from diversification is the central insight of Modern Portfolio Theory.\n\n\n3.4.6 The Role of Risk Aversion\nThe target return \\(\\bar{\\mu}\\) implicitly reflects the investor’s risk aversion. Less risk-averse investors choose higher \\(\\bar{\\mu}\\), accepting more variance to earn more expected return. More risk-averse investors choose \\(\\bar{\\mu}\\) closer to the minimum-variance portfolio’s expected return.\nEquivalently, the mean-variance framework can be derived from the optimal decisions of an investor with a mean-variance utility function: \\[U(\\omega) = \\omega^{\\prime}\\mu - \\frac{\\gamma}{2}\\omega^{\\prime}\\Sigma\\omega\\]\nwhere \\(\\gamma\\) is the coefficient of relative risk aversion. The Appendix shows there is a one-to-one mapping between \\(\\gamma\\) and \\(\\bar{\\mu}\\), so both formulations yield identical efficient portfolios.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-efficient-frontier-the-menu-of-optimal-portfolios",
    "href": "02_modern_portfolio_theory.html#the-efficient-frontier-the-menu-of-optimal-portfolios",
    "title": "3  Modern Portfolio Theory",
    "section": "3.5 The Efficient Frontier: The Menu of Optimal Portfolios",
    "text": "3.5 The Efficient Frontier: The Menu of Optimal Portfolios\nThe efficient frontier is the set of all portfolios for which no other portfolio offers higher expected return at the same or lower variance. Geometrically, it traces the upper boundary of achievable (volatility, expected return) combinations.\nEvery rational mean-variance investor should hold a portfolio on the efficient frontier. Portfolios below the frontier are “dominated,” there exists another portfolio with either higher return for the same risk, or lower risk for the same return.\n\n3.5.1 The Mutual Fund Separation Theorem\nA remarkable result simplifies the construction of the efficient frontier. The mutual fund separation theorem (sometimes called the two-fund theorem) states that any efficient portfolio can be expressed as a linear combination of any two distinct efficient portfolios.\nFormally, if \\(\\omega_{\\mu_1}\\) and \\(\\omega_{\\mu_2}\\) are efficient portfolios earning expected returns \\(\\mu_1\\) and \\(\\mu_2\\) respectively, then the portfolio: \\[\\omega_{a\\mu_1 + (1-a)\\mu_2} = a \\cdot \\omega_{\\mu_1} + (1-a) \\cdot \\omega_{\\mu_2}\\]\nis also efficient and earns expected return \\(a\\mu_1 + (1-a)\\mu_2\\).\nThis result has profound practical implications: an investor needs access to only two efficient “mutual funds” to construct any portfolio on the efficient frontier. The specific funds don’t matter—any two distinct efficient portfolios span the entire frontier.\n\n\n3.5.2 Proof of the Separation Theorem\nThe proof follows directly from the analytical solution for efficient portfolios. Consider:\n\\[\na \\cdot \\omega_{\\mu_1} + (1-a) \\cdot \\omega_{\\mu_2} = \\left(\\frac{a\\mu_1 + (1-a)\\mu_2 - D/C}{E - D^2/C}\\right)\\left(\\Sigma^{-1}\\mu - \\frac{D}{C}\\Sigma^{-1}\\iota\\right)\n\\]\nThis expression has exactly the form of the efficient portfolio earning expected return \\(a\\mu_1 + (1-a)\\mu_2\\), proving the theorem.\n\n\n3.5.3 Computing the Efficient Frontier\nUsing the minimum-variance portfolio and our efficient portfolio as the two “funds,” we can trace out the entire efficient frontier:\n\nefficient_frontier = (\n  pd.DataFrame({\n    \"a\": np.arange(-1, 2.01, 0.01)\n  })\n  .assign(\n    omega=lambda x: x[\"a\"].map(lambda a: a * omega_efp + (1 - a) * omega_mvp)\n  )\n  .assign(\n    mu=lambda x: x[\"omega\"].map(lambda w: w @ mu),\n    sigma=lambda x: x[\"omega\"].map(lambda w: np.sqrt(w @ sigma @ w))\n  )\n)\n\nNote that we allow \\(a\\) to range from -1 to 2, which means some portfolios involve shorting one of the two basis funds and leveraging into the other. This traces out both the upper and lower portions of the frontier hyperbola.\n\n\n3.5.4 Visualizing the Efficient Frontier\nFigure 3.5 displays the efficient frontier alongside individual assets and the benchmark portfolios.\n\nsummaries = pd.concat(\n  [summaries, efficient_frontier], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=summaries.query(\"type.isna()\"))\n  + geom_line(data=efficient_frontier, color=\"blue\", alpha=0.7)\n  + geom_point(data=summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3)\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"The Efficient Frontier and VN30 Constituents\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 3.5: The big dots indicate the location of the minimum-variance and the efficient portfolio. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe efficient frontier has a characteristic hyperbolic shape. The leftmost point is the minimum-variance portfolio. Moving up and to the right along the frontier, expected return increases but so does volatility. The upper portion of the hyperbola (above the minimum-variance portfolio) is the “efficient” part—these portfolios offer the highest return for each level of risk. The lower portion is “inefficient”—these portfolios are dominated by their mirror images on the upper portion.\nThe figure also reveals how dramatically diversification improves upon individual stock positions. Nearly all individual stocks lie well inside the efficient frontier, meaning investors can achieve the same expected return with much less risk, or much higher expected return with the same risk, simply by diversifying.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#key-takeaways",
    "href": "02_modern_portfolio_theory.html#key-takeaways",
    "title": "3  Modern Portfolio Theory",
    "section": "3.6 Key Takeaways",
    "text": "3.6 Key Takeaways\nThis chapter introduced the concepts of Modern Portfolio Theory. The main insights are:\n\nPortfolio risk depends on correlations: The variance of a portfolio is not simply the weighted average of individual variances. Covariances between assets play a crucial role, creating opportunities for diversification.\nDiversification is the “only free lunch” in finance: By combining assets that don’t move perfectly together, investors can reduce risk without sacrificing expected return. This insight is the cornerstone of modern investment practice.\nThe minimum-variance portfolio minimizes risk: This portfolio depends only on the covariance matrix and serves as an important benchmark. It represents the least risky way to be fully invested in risky assets.\nEfficient portfolios balance risk and return: By accepting more variance, investors can earn higher expected returns. Efficient portfolios are those that offer the best possible trade-off.\nThe efficient frontier characterizes optimal portfolios: This boundary in mean-standard deviation space represents the menu of optimal choices available to mean-variance investors.\nTwo-fund separation simplifies implementation: Any efficient portfolio can be constructed from any two distinct efficient portfolios, reducing the computational burden of portfolio optimization.\n\nLooking ahead, several important complications arise in practice. First, the inputs to portfolio optimization (expected returns and covariances) must be estimated from data, and estimation error can dramatically affect portfolio performance. Second, real-world constraints such as transaction costs, short-sale restrictions, and position limits modify the optimization problem. Third, the assumption that investors care only about mean and variance may be too restrictive when returns are non-normal or when investors have more complex preferences. We address these extensions in subsequent chapters.\n\n\n\n\nMarkowitz, Harry. 1952. “Portfolio selection.” The Journal of Finance 7 (1): 77–91. https://doi.org/10.1111/j.1540-6261.1952.tb01525.x.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "03_capm.html",
    "href": "03_capm.html",
    "title": "4  The Capital Asset Pricing Model",
    "section": "",
    "text": "4.1 From Efficient Portfolios to Equilibrium Prices\nThe previous chapter on Modern Portfolio Theory (MPT) showed how an investor can construct portfolios that optimally trade off risk and expected return. But MPT leaves a crucial question unanswered: What determines the expected returns themselves? Why do some assets command higher risk premiums than others?\nThe Capital Asset Pricing Model (CAPM) answers this question. Developed simultaneously by Sharpe (1964), Lintner (1965), and Mossin (1966), the CAPM extends MPT to explain how assets should be priced in equilibrium when all investors follow mean-variance optimization principles. The CAPM’s central insight is both elegant and counterintuitive: not all risk is rewarded. Only the component of risk that cannot be diversified away (i.e., systematic risk) commands a risk premium in equilibrium.\nThe CAPM remains the cornerstone of asset pricing theory, not because it perfectly describes reality, but because it provides the simplest coherent framework for understanding the relationship between risk and expected return. Every extension and alternative model in asset pricing (e.g., from the Fama-French factors to consumption-based pricing) builds upon or reacts against the CAPM’s foundational logic.\nIn this chapter, we derive the CAPM from first principles, illustrate its theoretical underpinnings, and show how to estimate its parameters empirically. We download stock market data, estimate betas using regression analysis, and evaluate asset performance relative to model predictions.\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#systematic-versus-idiosyncratic-risk",
    "href": "03_capm.html#systematic-versus-idiosyncratic-risk",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.2 Systematic versus Idiosyncratic Risk",
    "text": "4.2 Systematic versus Idiosyncratic Risk\nBefore diving into the mathematics, we need to understand the fundamental distinction that makes the CAPM work: the difference between systematic and idiosyncratic risk.\n\n4.2.1 Idiosyncratic Risk: Diversifiable and Unrewarded\nConsider events that affect individual companies but not the broader market: a CEO resigns unexpectedly, a product launch fails, earnings disappoint analysts, or a factory experiences a fire. These company-specific events can dramatically affect individual stock prices, but they tend to average out across a diversified portfolio. When one company has bad news, another often has good news; the shocks are largely uncorrelated.\nThis idiosyncratic (or firm-specific) risk can be eliminated through diversification. By holding a portfolio of many stocks, an investor can reduce idiosyncratic risk to nearly zero. Since this risk can be avoided at no cost, investors should not expect compensation for bearing it. In equilibrium, idiosyncratic risk earns no premium.\n\n\n4.2.2 Systematic Risk: Undiversifiable and Priced\nSystematic risk, by contrast, affects all assets simultaneously. Recessions, interest rate changes, geopolitical crises, and pandemics impact virtually every company to some degree. No amount of diversification can eliminate exposure to these economy-wide shocks, they are inherent to participating in the market.\nSince systematic risk cannot be diversified away, investors genuinely dislike it. They must be compensated for bearing it. The CAPM formalizes this intuition: expected returns should depend only on systematic risk, not total risk. Two assets with identical total volatility can have very different expected returns if their systematic risk exposures differ.\n\n\n4.2.3 A Simple Illustration\nImagine two stocks with identical 30% annual volatility. Stock A is a gold mining company whose returns move opposite to the overall market: it does well when the economy struggles and poorly when it booms. Stock B is a luxury retailer that amplifies market movements: soaring in good times and crashing in bad times.\nWhich stock should offer higher expected returns? Intuition might suggest they should be equal since both have the same volatility. But the CAPM says Stock B should offer substantially higher returns. Why? Because Stock B performs poorly precisely when investors’ overall wealth is already down (during market crashes), making its returns particularly painful. Stock A, by contrast, provides insurance. Its strong performance during market downturns partially offsets losses elsewhere in the portfolio. Investors value this insurance property and are willing to accept lower expected returns in exchange.\nThis is the CAPM’s core insight: expected returns compensate investors for systematic risk exposure, measured by how an asset’s returns co-move with the market portfolio.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#data-preparation",
    "href": "03_capm.html#data-preparation",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.3 Data Preparation",
    "text": "4.3 Data Preparation\nBuilding on our analysis from the previous chapter, we examine the VN30 constituents as our asset universe. We download and prepare monthly return data:\n\nvn30_symbols = [\n    \"ACB\", \"BCM\", \"BID\", \"BVH\", \"CTG\", \"FPT\", \"GAS\", \"GVR\", \"HDB\", \"HPG\",\n    \"MBB\", \"MSN\", \"MWG\", \"PLX\", \"POW\", \"SAB\", \"SHB\", \"SSB\", \"STB\", \"TCB\",\n    \"TPB\", \"VCB\", \"VHM\", \"VIB\", \"VIC\", \"VJC\", \"VNM\", \"VPB\", \"VRE\", \"EIB\"\n]\n\n\nimport os\nimport boto3\nfrom botocore.client import Config\nfrom io import BytesIO\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\nWe process the raw price data to compute adjusted closing prices and standardize column names:\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\nprices = prices.sort_values([\"symbol\", \"date\"])\n\n\nprices_daily = prices[prices[\"symbol\"].isin(vn30_symbols)]\nprices_daily[[\"date\", \"symbol\", \"adjusted_close\"]].head(3)\n\n\n\n\n\n\n\n\ndate\nsymbol\nadjusted_close\n\n\n\n\n18176\n2010-01-04\nACB\n329.408244\n\n\n18177\n2010-01-05\nACB\n329.408244\n\n\n18178\n2010-01-06\nACB\n320.258015\n\n\n\n\n\n\n\n\n4.3.1 Computing Monthly Returns\nWe aggregate daily prices to monthly frequency. Using monthly returns rather than daily returns offers several advantages for portfolio analysis: monthly returns exhibit less noise, better approximate normality, and reduce the impact of microstructure effects like bid-ask bounce.\n\nreturns_monthly = (prices_daily\n    .assign(\n        date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp(\"M\")\n    )\n    .groupby([\"symbol\", \"date\"], as_index=False)\n    .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n    .sort_values([\"symbol\", \"date\"])\n    .assign(\n        ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    )\n)\n\nreturns_monthly.head(3)\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n0\nACB\n2010-01-31\n291.975489\nNaN\n\n\n1\nACB\n2010-02-28\n303.621235\n0.039886\n\n\n2\nACB\n2010-03-31\n273.784658\n-0.098269",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-risk-free-asset-and-the-investment-opportunity-set",
    "href": "03_capm.html#the-risk-free-asset-and-the-investment-opportunity-set",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.4 The Risk-Free Asset and the Investment Opportunity Set",
    "text": "4.4 The Risk-Free Asset and the Investment Opportunity Set\n\n4.4.1 Adding a Risk-Free Asset\nThe previous chapter on MPT considered portfolios composed entirely of risky assets, requiring that portfolio weights sum to one. The CAPM introduces a crucial new element: a risk-free asset that pays a constant interest rate \\(r_f\\) with zero volatility.\nThis seemingly simple addition fundamentally transforms the investment opportunity set. With a risk-free asset available, investors can choose to park some wealth in the safe asset and invest the remainder in risky assets. They can also borrow at the risk-free rate to leverage their risky positions.\nLet \\(\\omega \\in \\mathbb{R}^N\\) denote the portfolio weights in the \\(N\\) risky assets. Unlike before, these weights need not sum to one. The remainder, \\(1 - \\iota'\\omega\\) (where \\(\\iota\\) is a vector of ones), is invested in the risk-free asset.\n\n\n4.4.2 Portfolio Return with a Risk-Free Asset\nThe expected return on this combined portfolio is:\n\\[\n\\mu_\\omega = \\omega'\\mu + (1 - \\iota'\\omega)r_f = r_f + \\omega'(\\mu - r_f) = r_f + \\omega'\\tilde{\\mu}\n\\]\nwhere \\(\\mu\\) is the vector of expected returns on risky assets and \\(\\tilde{\\mu} = \\mu - r_f\\) denotes the vector of excess returns (returns above the risk-free rate).\nThis expression reveals an important decomposition: the portfolio’s expected return equals the risk-free rate plus a risk premium determined by the exposure to risky assets.\n\n\n4.4.3 Portfolio Variance\nSince the risk-free asset has zero volatility and zero covariance with risky assets, only the risky portion contributes to portfolio variance:\n\\[\n\\sigma_\\omega^2 = \\omega'\\Sigma\\omega\n\\]\nwhere \\(\\Sigma\\) is the variance-covariance matrix of risky asset returns. The portfolio’s volatility (standard deviation) is:\n\\[\n\\sigma_\\omega = \\sqrt{\\omega'\\Sigma\\omega}\n\\]\n\n\n4.4.4 Setting Up the Risk-Free Rate\nFor a realistic proxy of the risk-free rate, we use the Vietnam government bond yield. Government bonds of stable economies are considered “risk-free” because the government can always print money to meet its obligations (though this may cause inflation).\n\nall_dates = pd.date_range(\n    start=returns_monthly[\"date\"].min(), \n    end=returns_monthly[\"date\"].max(), \n    freq=\"ME\"\n)\n\n# Vietnam 10-Year Government Bond Yield (approximately 2.52% annualized)\nrf_annual = 0.0252\nrf_monthly_val = (1 + rf_annual)**(1/12) - 1\n\nrisk_free_monthly = pd.DataFrame({\n    \"date\": all_dates,\n    \"risk_free\": rf_monthly_val\n})\n\nrisk_free_monthly[\"date\"] = (\n    pd.to_datetime(risk_free_monthly[\"date\"])\n    .dt.to_period(\"M\")\n    .dt.to_timestamp(\"M\")\n)\n\nrisk_free_monthly.head(3)\n\n\n\n\n\n\n\n\ndate\nrisk_free\n\n\n\n\n0\n2010-01-31\n0.002076\n\n\n1\n2010-02-28\n0.002076\n\n\n2\n2010-03-31\n0.002076\n\n\n\n\n\n\n\nWe merge the risk-free rate with our returns data and compute excess returns:\n\nreturns_monthly = returns_monthly.merge(\n    risk_free_monthly[[\"date\", \"risk_free\"]], \n    on=\"date\", \n    how=\"left\"\n)\n\nrf = risk_free_monthly[\"risk_free\"].mean()\n\nreturns_monthly = (returns_monthly\n    .assign(\n        ret_excess=lambda x: x[\"ret\"] - x[\"risk_free\"]\n    )\n    .assign(\n        ret_excess=lambda x: x[\"ret_excess\"].clip(lower=-1)\n    )\n)\n\nreturns_monthly.head(3)\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\nrisk_free\nret_excess\n\n\n\n\n0\nACB\n2010-01-31\n291.975489\nNaN\n0.002076\nNaN\n\n\n1\nACB\n2010-02-28\n303.621235\n0.039886\n0.002076\n0.037810\n\n\n2\nACB\n2010-03-31\n273.784658\n-0.098269\n0.002076\n-0.100345",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-tangency-portfolio-where-everyone-invests",
    "href": "03_capm.html#the-tangency-portfolio-where-everyone-invests",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.5 The Tangency Portfolio: Where Everyone Invests",
    "text": "4.5 The Tangency Portfolio: Where Everyone Invests\n\n4.5.1 Deriving the Optimal Risky Portfolio\nWith a risk-free asset available, how should an investor allocate wealth across risky assets? Consider an investor who wants to achieve a target expected excess return \\(\\bar{\\mu}\\) with minimum variance. The optimization problem becomes:\n\\[\n\\min_\\omega \\omega'\\Sigma\\omega \\quad \\text{subject to} \\quad \\omega'\\tilde{\\mu} = \\bar{\\mu}\n\\]\nUsing the Lagrangian method:\n\\[\n\\mathcal{L}(\\omega, \\lambda) = \\omega'\\Sigma\\omega - \\lambda(\\omega'\\tilde{\\mu} - \\bar{\\mu})\n\\]\nThe first-order condition with respect to \\(\\omega\\) is:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda\\tilde{\\mu} = 0\n\\]\nSolving for the optimal weights:\n\\[\n\\omega^* = \\frac{\\lambda}{2}\\Sigma^{-1}\\tilde{\\mu}\n\\]\nThe constraint \\(\\omega'\\tilde{\\mu} = \\bar{\\mu}\\) determines \\(\\lambda\\):\n\\[\n\\bar{\\mu} = \\tilde{\\mu}'\\omega^* = \\frac{\\lambda}{2}\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu} \\implies \\lambda = \\frac{2\\bar{\\mu}}{\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu}}\n\\]\nSubstituting back:\n\\[\n\\omega^* = \\frac{\\bar{\\mu}}{\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu}}\\Sigma^{-1}\\tilde{\\mu}\n\\]\n\n\n4.5.2 The Tangency Portfolio\nNotice something remarkable: the direction of \\(\\omega^*\\) is always \\(\\Sigma^{-1}\\tilde{\\mu}\\), regardless of the target return \\(\\bar{\\mu}\\). Only the scale changes. This means all investors, regardless of their risk preferences, hold the same portfolio of risky assets. They differ only in how much they allocate to this portfolio versus the risk-free asset.\nTo obtain the portfolio of risky assets that is fully invested (weights summing to one), we normalize:\n\\[\n\\omega_{\\text{tan}} = \\frac{\\omega^*}{\\iota'\\omega^*} = \\frac{\\Sigma^{-1}(\\mu - r_f)}{\\iota'\\Sigma^{-1}(\\mu - r_f)}\n\\]\nThis is called the tangency portfolio (or maximum Sharpe ratio portfolio) because it lies at the point where the efficient frontier is tangent to the capital market line.\n\n\n4.5.3 The Sharpe Ratio and the Capital Market Line\nThe Sharpe ratio measures excess return per unit of volatility:\n\\[\n\\text{Sharpe Ratio} = \\frac{\\mu_p - r_f}{\\sigma_p}\n\\]\nThe tangency portfolio maximizes the Sharpe ratio among all possible portfolios. Any combination of the risk-free asset and the tangency portfolio lies on a straight line in mean-standard deviation space, called the Capital Market Line (CML):\n\\[\n\\mu_p = r_f + \\left(\\frac{\\mu_{\\text{tan}} - r_f}{\\sigma_{\\text{tan}}}\\right)\\sigma_p\n\\]\nThe slope of this line equals the Sharpe ratio of the tangency portfolio (i.e., the highest achievable Sharpe ratio).\n\n\n4.5.4 Computing the Tangency Portfolio\nLet’s compute the tangency portfolio for our VN30 universe:\n\nassets = (returns_monthly\n    .groupby(\"symbol\", as_index=False)\n    .agg(\n        mu=(\"ret\", \"mean\"),\n        sigma=(\"ret\", \"std\")\n    )\n)\n\nsigma = (returns_monthly\n    .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n    .cov()\n)\n\nmu = (returns_monthly\n    .groupby(\"symbol\")[\"ret\"]\n    .mean()\n    .values\n)\n\n\n# Compute tangency portfolio weights\nw_tan = np.linalg.solve(sigma, mu - rf)\nw_tan = w_tan / np.sum(w_tan)\n\n# Portfolio performance metrics\nmu_w = w_tan.T @ mu\nsigma_w = np.sqrt(w_tan.T @ sigma @ w_tan)\n\nefficient_portfolios = pd.DataFrame([\n    {\"symbol\": r\"$\\omega_{\\mathrm{tan}}$\", \"mu\": mu_w, \"sigma\": sigma_w},\n    {\"symbol\": r\"$r_f$\", \"mu\": rf, \"sigma\": 0}\n])\n\nsharpe_ratio = (mu_w - rf) / sigma_w\n\nprint(f\"Tangency Portfolio Sharpe Ratio: {sharpe_ratio:.4f}\")\nprint(efficient_portfolios)\n\nTangency Portfolio Sharpe Ratio: -0.5552\n                    symbol        mu     sigma\n0  $\\omega_{\\mathrm{tan}}$ -0.041157  0.077866\n1                    $r_f$  0.002076  0.000000\n\n\n\n\n4.5.5 Visualizing the Efficient Frontier with a Risk-Free Asset\nFigure 4.1 shows the efficient frontier when a risk-free asset is available. The frontier is now a straight line (the Capital Market Line) connecting the risk-free asset to the tangency portfolio and extending beyond.\n\nefficient_portfolios_figure = (\n    ggplot(efficient_portfolios, aes(x=\"sigma\", y=\"mu\"))\n    + geom_point(data=assets)\n    + geom_point(data=efficient_portfolios, color=\"blue\", size=3)\n    + geom_label(\n        aes(label=\"symbol\"), \n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Volatility (Standard Deviation)\", \n        y=\"Expected Return\",\n        title=\"Efficient Frontier with Risk-Free Asset (VN30)\"\n    )\n    + geom_abline(slope=sharpe_ratio, intercept=rf, linetype=\"dotted\")\n)\n\nefficient_portfolios_figure.show()\n\n\n\n\n\n\n\nFigure 4.1: The efficient frontier with a risk-free asset becomes a straight line (the Capital Market Line) connecting the risk-free rate to the tangency portfolio. Individual assets lie below this line, demonstrating the benefits of diversification.\n\n\n\n\n\nYou may notice that estimated expected returns appear quite low, some even negative. This is not a model failure but reflects the realities of estimation:\n\nSample period matters: If the estimation window includes market downturns (such as the 2022-2023 period), realized average returns can be near zero or negative. Mean-variance optimization takes sample means literally.\nEstimation noise in emerging markets: With volatile emerging market data, sample means are dominated by noise. A few extremely bad months can push the average below the risk-free rate even if the long-run equity premium is positive.\n\nThis highlights a fundamental challenge in portfolio optimization: the inputs we observe (historical returns) are noisy estimates of the true parameters we need (expected future returns).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-capm-equation-risk-and-expected-return",
    "href": "03_capm.html#the-capm-equation-risk-and-expected-return",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.6 The CAPM Equation: Risk and Expected Return",
    "text": "4.6 The CAPM Equation: Risk and Expected Return\n\n4.6.1 From Individual Optimization to Market Equilibrium\nSo far, we’ve focused on one investor’s optimization problem. The CAPM’s power comes from considering what happens when all investors optimize simultaneously.\nIf all investors follow mean-variance optimization, they all hold some combination of the risk-free asset and the tangency portfolio. The only difference between investors is their risk tolerance. More risk-averse investors hold more of the risk-free asset, while risk-tolerant investors may even borrow at the risk-free rate to leverage their position in the tangency portfolio.\n\n\n4.6.2 The Market Portfolio\nIn equilibrium, the total demand for each risky asset must equal its supply. Since all investors hold the same portfolio of risky assets (the tangency portfolio), the equilibrium portfolio weights must equal the market capitalization weights. The tangency portfolio is the market portfolio.\nThis insight has enormous practical implications: instead of estimating expected returns and covariances to compute the tangency portfolio, we can simply use the market portfolio (approximated by a broad market index) as a proxy.\n\n\n4.6.3 Deriving the CAPM Equation\nFrom the first-order conditions of the optimization problem, we derived that:\n\\[\n\\tilde{\\mu} = \\frac{2}{\\lambda}\\Sigma\\omega^*\n\\]\nSince \\(\\omega^*\\) is proportional to \\(\\omega_{\\text{tan}}\\), and in equilibrium \\(\\omega_{\\text{tan}}\\) equals the market portfolio \\(\\omega_m\\):\n\\[\n\\tilde{\\mu} = c \\cdot \\Sigma\\omega_m\n\\]\nfor some constant \\(c\\). The \\(i\\)-th element of \\(\\Sigma\\omega_m\\) is:\n\\[\n\\sum_{j=1}^N \\sigma_{ij}\\omega_{m,j} = \\text{Cov}(r_i, r_m)\n\\]\nwhere \\(r_m = \\sum_j \\omega_{m,j} r_j\\) is the return on the market portfolio.\nFor the market portfolio itself:\n\\[\n\\tilde{\\mu}_m = c \\cdot \\text{Var}(r_m) = c \\cdot \\sigma_m^2\n\\]\nTherefore \\(c = \\tilde{\\mu}_m / \\sigma_m^2\\), and for any asset \\(i\\):\n\\[\n\\tilde{\\mu}_i = \\frac{\\tilde{\\mu}_m}{\\sigma_m^2} \\text{Cov}(r_i, r_m) = \\beta_i \\tilde{\\mu}_m\n\\]\nwhere:\n\\[\n\\beta_i = \\frac{\\text{Cov}(r_i, r_m)}{\\text{Var}(r_m)}\n\\]\nThis is the famous CAPM equation:\n\\[\nE(r_i) - r_f = \\beta_i [E(r_m) - r_f]\n\\]\n\n\n4.6.4 Interpreting Beta\nBeta (\\(\\beta_i\\)) measures an asset’s systematic risk (i.e., its sensitivity to market movements). The interpretation is straightforward:\n\n\\(\\beta = 1\\): The asset moves one-for-one with the market (average systematic risk)\n\\(\\beta &gt; 1\\): The asset amplifies market movements (aggressive, high systematic risk)\n\\(\\beta &lt; 1\\): The asset dampens market movements (defensive, low systematic risk)\n\\(\\beta &lt; 0\\): The asset moves opposite to the market (provides insurance)\n\nThe CAPM says that expected excess return is proportional to beta, not to total volatility. This explains why:\n\nAn asset with zero beta earns only the risk-free rate (i.e., its risk is entirely idiosyncratic).\nAn asset with beta of 1 earns the market risk premium\nA negative-beta asset earns less than the risk-free rate (i.e., investors pay for its insurance properties).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-security-market-line",
    "href": "03_capm.html#the-security-market-line",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.7 The Security Market Line",
    "text": "4.7 The Security Market Line\nThe CAPM predicts a linear relationship between beta and expected return. This relationship is called the Security Market Line (SML):\n\\[\nE(r_i) = r_f + \\beta_i [E(r_m) - r_f]\n\\]\nUnlike the Capital Market Line (which plots expected return against total risk), the Security Market Line plots expected return against systematic risk (beta).\n\nbetas = (sigma @ w_tan) / (w_tan.T @ sigma @ w_tan)\nassets[\"beta\"] = betas.values\n\nprice_of_risk = float(w_tan.T @ mu - rf)\n\nassets_figure = (\n    ggplot(assets, aes(x=\"beta\", y=\"mu\"))\n    + geom_point()\n    + geom_abline(intercept=rf, slope=price_of_risk)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Beta (Systematic Risk)\", \n        y=\"Expected Return\",\n        title=\"Security Market Line\"\n    )\n    + annotate(\"text\", x=0.05, y=rf + 0.001, label=\"Risk-free rate\")\n)\n\nassets_figure.show()\n\n\n\n\n\n\n\n\nYou may observe that the estimated SML has a negative slope, which seems to contradict CAPM’s prediction. This reflects a negative estimated market risk premium in our sample period (i.e., the market portfolio earned less than the risk-free rate).\nWhen the market risk premium is negative, CAPM predicts that high-beta stocks should have lower expected returns than low-beta stocks. This is not a model failure, the model is behaving consistently. Rather, it reflects an unusual (but not impossible) sample period where risky assets underperformed safe assets.\nThis observation highlights an important distinction: CAPM describes expected returns in equilibrium, but realized returns over any particular period may differ substantially from expectations due to shocks and surprises.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#empirical-estimation-of-capm-parameters",
    "href": "03_capm.html#empirical-estimation-of-capm-parameters",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.8 Empirical Estimation of CAPM Parameters",
    "text": "4.8 Empirical Estimation of CAPM Parameters\n\n4.8.1 The Regression Framework\nIn practice, we estimate CAPM parameters using time-series regression. The model implies:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i(r_{m,t} - r_{f,t}) + \\varepsilon_{i,t}\n\\]\nwhere:\n\n\\(r_{i,t}\\): Return on asset \\(i\\) at time \\(t\\)\n\\(r_{f,t}\\): Risk-free rate at time \\(t\\)\n\\(r_{m,t}\\): Market return at time \\(t\\)\n\\(\\alpha_i\\): Intercept (should be zero if CAPM holds)\n\\(\\beta_i\\): Systematic risk (slope coefficient)\n\\(\\varepsilon_{i,t}\\): Idiosyncratic shock (residual)\n\n\n\n4.8.2 Alpha: Risk-Adjusted Performance\nThe intercept \\(\\alpha_i\\) measures risk-adjusted performance. If CAPM holds perfectly, alpha should be zero for all assets (i.e., any excess return is exactly compensated by systematic risk).\n\n\\(\\alpha &gt; 0\\): The asset outperformed its CAPM-predicted return (positive abnormal return)\n\\(\\alpha &lt; 0\\): The asset underperformed its CAPM-predicted return (negative abnormal return)\n\nPositive alpha is the holy grail of active management: earning returns beyond what systematic risk exposure would justify.\n\n\n4.8.3 Loading Factor Data\nWe use Fama-French market excess returns as our market portfolio proxy. These data provide a widely accepted benchmark that is already adjusted for the risk-free rate:\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nfactors.head(3)\n\n\n\n\n\n\n\n\ndate\nsmb\nhml\nrmw\ncma\nmkt_excess\n\n\n\n\n0\n2011-07-31\n0.008933\n-0.013099\n0.012139\n-0.009529\n-0.011287\n\n\n1\n2011-08-31\n0.004830\n-0.016656\n0.014516\n-0.003981\n0.007856\n\n\n2\n2011-09-30\n0.004970\n-0.000462\n0.008899\n0.001241\n-0.006501\n\n\n\n\n\n\n\n\n\n4.8.4 Running the Regressions\nWe estimate CAPM regressions for each stock in our universe:\n\nimport statsmodels.formula.api as smf\n\nreturns_excess_monthly = (returns_monthly\n    .merge(factors, on=\"date\", how=\"left\")\n    .assign(ret_excess=lambda x: x[\"ret\"] - x[\"risk_free\"])\n)\n\n\ndef estimate_capm(data):\n    model = smf.ols(\"ret_excess ~ mkt_excess\", data=data).fit()\n    result = pd.DataFrame({\n        \"coefficient\": [\"alpha\", \"beta\"],\n        \"estimate\": model.params.values,\n        \"t_statistic\": model.tvalues.values\n    })\n    return result\n\n\ncapm_results = (returns_excess_monthly\n    .groupby(\"symbol\", group_keys=True)\n    .apply(estimate_capm)\n    .reset_index()\n)\n\ncapm_results.head(4)\n\n\n\n\n\n\n\n\nsymbol\nlevel_1\ncoefficient\nestimate\nt_statistic\n\n\n\n\n0\nACB\n0\nalpha\n-0.007479\n-0.876326\n\n\n1\nACB\n1\nbeta\n-0.240019\n-0.252484\n\n\n2\nBCM\n0\nalpha\n0.027827\n1.737895\n\n\n3\nBCM\n1\nbeta\n4.150186\n2.625283\n\n\n\n\n\n\n\n\n\n4.8.5 Visualizing Alpha Estimates\nFigure 4.2 shows the estimated alphas across our VN30 sample. Statistical significance (at the 95% level) is indicated by color.\n\nalphas = (capm_results\n    .query(\"coefficient == 'alpha'\")\n    .assign(is_significant=lambda x: np.abs(x[\"t_statistic\"]) &gt;= 1.96)\n)\n\nalphas[\"symbol\"] = pd.Categorical(\n    alphas[\"symbol\"],\n    categories=alphas.sort_values(\"estimate\")[\"symbol\"],\n    ordered=True\n)\n\nalphas_figure = (\n    ggplot(alphas, aes(y=\"estimate\", x=\"symbol\", fill=\"is_significant\"))\n    + geom_col()\n    + scale_y_continuous(labels=percent_format())\n    + coord_flip()\n    + labs(\n        x=\"\", \n        y=\"Estimated Alpha (Monthly)\", \n        fill=\"Significant at 95%?\",\n        title=\"Estimated CAPM Alphas for VN30 Index Constituents\"\n    )\n)\n\nalphas_figure.show()\n\n\n\n\n\n\n\nFigure 4.2: Estimated CAPM alphas for VN30 index constituents. Color indicates statistical significance at the 95% confidence level. Most alphas are statistically indistinguishable from zero, consistent with CAPM predictions.\n\n\n\n\n\nThe distribution of alphas provides evidence on CAPM’s empirical validity. If the model holds, we expect:\n\nMost alphas close to zero\nFew statistically significant alphas\nRoughly equal numbers of positive and negative alphas\n\nSystematic patterns in alphas, such as consistently positive alphas for certain types of stocks, would suggest the CAPM is incomplete and that additional risk factors may be needed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#limitations-and-extensions",
    "href": "03_capm.html#limitations-and-extensions",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.9 Limitations and Extensions",
    "text": "4.9 Limitations and Extensions\n\n4.9.1 The Market Portfolio Problem\nA fundamental challenge in testing the CAPM is identifying the market portfolio. The theory requires a portfolio that includes all investable assets, not just stocks, but also bonds, real estate, private businesses, human capital, and even intangible assets. In practice, we use proxies like broad market indices (VNI, S&P 500), but these capture only publicly traded equities.\nThis limitation is profound. As Richard Roll famously argued, the CAPM is essentially untestable because the true market portfolio is unobservable. Any test of the CAPM is simultaneously a test of whether our proxy adequately represents the market.\n\n\n4.9.2 Time-Varying Betas\nThe CAPM assumes that betas are constant over time, but this assumption rarely holds in practice. Companies undergo changes that affect their market sensitivity:\n\nCapital structure changes: Increasing leverage raises beta\nBusiness model evolution: Diversification into new industries can alter systematic risk\nMarket conditions: Betas often increase during market stress\n\nConditional CAPM models (Jagannathan and Wang 1996) address this by allowing risk premiums and betas to vary with the business cycle.\n\n\n4.9.3 Empirical Anomalies\nDecades of empirical research have documented patterns in stock returns that CAPM cannot explain:\n\nSize effect: Small-cap stocks tend to outperform large-cap stocks, even after adjusting for beta\nValue effect: Stocks with high book-to-market ratios outperform growth stocks\nMomentum: Stocks that performed well recently tend to continue performing well\n\nThese anomalies suggest that systematic risk has multiple dimensions beyond market exposure.\n\n\n4.9.4 Multifactor Extensions\nThe limitations of CAPM have led to increasingly sophisticated asset pricing models. The Fama-French three-factor model (Fama and French 1992) adds two factors to capture size and value effects:\n\nSMB (Small Minus Big): Returns on small stocks minus large stocks\nHML (High Minus Low): Returns on value stocks minus growth stocks\n\nThe Fama-French five-factor model (Fama and French 2015) adds two more dimensions:\n\nRMW (Robust Minus Weak): Returns on profitable firms minus unprofitable firms\n\nCMA (Conservative Minus Aggressive): Returns on conservative investors minus aggressive investors\n\nThe Carhart four-factor model (Carhart 1997) adds momentum to the three-factor framework.\nOther theoretical developments include:\n\nConsumption CAPM: Links asset prices to macroeconomic consumption risk\nQ-factor model (Hou, Xue, and Zhang 2014): Derives factors from investment-based asset pricing theory\nArbitrage Pricing Theory: Allows for multiple sources of systematic risk without specifying their identity\n\nDespite its limitations, the CAPM remains valuable as a conceptual benchmark. Its core insight (i.e., only systematic, undiversifiable risk commands a premium) continues to inform how we think about risk and return.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#key-takeaways",
    "href": "03_capm.html#key-takeaways",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.10 Key Takeaways",
    "text": "4.10 Key Takeaways\nThis chapter introduced the Capital Asset Pricing Model and its implications for understanding the relationship between risk and expected return. The main insights are:\n\nNot all risk is rewarded: The CAPM distinguishes between systematic risk (which cannot be diversified away and commands a premium) and idiosyncratic risk (which can be eliminated through diversification and earns no premium).\nThe tangency portfolio is universal: When a risk-free asset exists, all mean-variance investors hold the same portfolio of risky assets (i.e., the tangency or maximum Sharpe ratio portfolio). They differ only in how much they allocate to this portfolio versus the risk-free asset.\nIn equilibrium, the tangency portfolio is the market portfolio: Since all investors hold the same risky portfolio, and total demand must equal supply, the equilibrium portfolio weights are market capitalization weights.\nExpected returns depend on beta: The CAPM equation states that expected excess return equals beta times the market risk premium. Beta measures covariance with the market portfolio, normalized by market variance.\nAlpha measures risk-adjusted performance: Positive alpha indicates returns above what systematic risk would justify; negative alpha indicates underperformance.\nEmpirical challenges exist: Testing the CAPM requires identifying the market portfolio, which is unobservable in practice. Documented anomalies (size, value, momentum) suggest additional risk factors beyond market exposure.\nExtensions abound: Multifactor models like Fama-French extend the CAPM framework by adding factors that capture dimensions of systematic risk the market factor misses.\n\nThe CAPM’s elegance lies in its simplicity: a single factor (i.e., exposure to the market) should explain expected returns in equilibrium. While reality is more complex, this framework provides the foundation for all modern asset pricing theory.\n\n\n\n\nCarhart, Mark M. 1997. “On persistence in mutual fund performance.” The Journal of Finance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock returns.” The Journal of Finance 47 (2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2014. “Digesting anomalies: An investment approach.” Review of Financial Studies 28 (3): 650–705. https://doi.org/10.1093/rfs/hhu068.\n\n\nJagannathan, Ravi, and Zhenyu Wang. 1996. “The conditional CAPM and the cross-section of expected returns.” The Journal of Finance 51 (1): 3–53. https://doi.org/10.2307/2329301.\n\n\nLintner, John. 1965. “Security prices, risk, and maximal gains from diversification.” The Journal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital asset market.” Econometrica 34 (4): 768–83. https://doi.org/10.2307/1910098.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html",
    "href": "04_financial_statement_analysis.html",
    "title": "5  Financial Statement Analysis",
    "section": "",
    "text": "5.1 From Market Prices to Fundamental Value\nThe previous chapters focused on how financial markets price assets in equilibrium. The Capital Asset Pricing Model showed that expected returns depend on systematic risk exposure, while Modern Portfolio Theory demonstrated how to construct efficient portfolios. But these frameworks take expected returns and risk as given, they don’t explain where these expectations come from.\nFinancial statement analysis addresses this gap. By examining a company’s accounting records, investors can form independent assessments of firm value, identify mispriced securities, and understand the economic forces driving business performance. Financial statements provide the primary source of standardized information about a company’s operations, financial position, and cash generation. Their legal requirements and standardized formats make them particularly valuable. Every publicly traded company must file them, creating a level playing field for analysis.\nThis chapter introduces the three primary financial statements: the balance sheet, income statement, and cash flow statement. We then demonstrate how to transform raw accounting data into meaningful financial ratios that facilitate comparison across companies and over time. These ratios serve multiple purposes: they enable investors to benchmark companies against peers, help creditors assess default risk, and provide inputs for asset pricing models like the Fama-French factors we will encounter in later chapters.\nOur analysis combines theoretical frameworks with practical implementation using Vietnamese market data. By the end of this chapter, you will understand how to access financial statements, calculate key ratios across multiple categories, and interpret these metrics in context.\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#the-three-financial-statements",
    "href": "04_financial_statement_analysis.html#the-three-financial-statements",
    "title": "5  Financial Statement Analysis",
    "section": "5.2 The Three Financial Statements",
    "text": "5.2 The Three Financial Statements\nBefore diving into ratios and analysis, we need to understand the three interconnected statements that form the foundation of financial reporting. Each statement answers a different question about the company, and together they provide a comprehensive picture of financial health.\n\n5.2.1 The Balance Sheet: A Snapshot of Financial Position\nThe balance sheet captures a company’s financial position at a specific moment in time, think of it as a photograph rather than a movie. It lists everything the company owns (assets), everything it owes (liabilities), and the residual claim belonging to shareholders (equity). These three components are linked by the fundamental accounting equation:\n\\[\n\\text{Assets} = \\text{Liabilities} + \\text{Equity}\n\\]\nThis equation is not merely a definition, it reflects a core economic principle. A company’s resources (assets) must be financed from somewhere: either borrowed from creditors (liabilities) or contributed by owners (equity). Every transaction affects both sides equally, maintaining the balance.\nAssets represent resources the company controls that are expected to generate future economic benefits:\n\nCurrent assets can be converted to cash within one year: cash and equivalents, short-term investments, accounts receivable (money owed by customers), and inventory (raw materials, work-in-progress, and finished goods)\nNon-current assets support operations beyond one year: property, plant, and equipment (PP&E), long-term investments, and intangible assets like patents, trademarks, and goodwill\n\nLiabilities encompass obligations to external parties:\n\nCurrent liabilities come due within one year: accounts payable (money owed to suppliers), short-term debt, accrued expenses, and the current portion of long-term debt\nNon-current liabilities extend beyond one year: long-term debt, bonds payable, pension obligations, and deferred tax liabilities\n\nShareholders’ equity represents the owners’ residual claim:\n\nCommon stock and additional paid-in capital from share issuance\nRetained earnings (i.e., accumulated profits reinvested rather than distributed as dividends)\nTreasury stock: shares repurchased by the company\n\nUnderstanding these categories is essential for ratio analysis. Current assets and liabilities determine short-term liquidity, while the mix of debt and equity reveals capital structure choices.\n\n\n5.2.2 The Income Statement: Performance Over Time\nWhile the balance sheet provides a snapshot, the income statement (also called the profit and loss statement, or P&L) measures financial performance over a period (e.g., a quarter or year). It follows a hierarchical structure that progressively captures different levels of profitability:\n\\[\n\\text{Revenue} - \\text{COGS} = \\text{Gross Profit}\n\\]\n\\[\n\\text{Gross Profit} - \\text{Operating Expenses} = \\text{Operating Income (EBIT)}\n\\]\n\\[\n\\text{EBIT} - \\text{Interest} - \\text{Taxes} = \\text{Net Income}\n\\]\nEach line reveals something different about the business:\n\nRevenue (Sales): Total income from goods or services sold (i.e., the “top line”)\nCost of Goods Sold (COGS): Direct costs of producing what was sold (materials, direct labor, manufacturing overhead)\nGross Profit: Revenue minus COGS, measuring basic profitability from core operations\nOperating Expenses: Costs of running the business beyond production (selling, general & administrative expenses, research & development)\nOperating Income (EBIT): Earnings Before Interest and Taxes, measuring profitability from operations before financing decisions and taxes\nInterest Expense: The cost of debt financing\nNet Income: The “bottom line” (i.e., total profit after all expenses)\n\nThe income statement’s hierarchical structure allows analysts to identify where profitability problems originate. A company with strong gross margins but weak net income might have bloated overhead costs. One with weak gross margins faces fundamental pricing or production challenges.\n\n\n5.2.3 The Cash Flow Statement: Following the Money\nThe cash flow statement bridges a critical gap: profitable companies can run out of cash, and unprofitable companies can generate positive cash flow. This happens because accrual accounting (used in the income statement) recognizes revenue when earned and expenses when incurred, not when cash changes hands.\nThe cash flow statement tracks actual cash movements, divided into three categories:\n\nOperating activities: Cash generated from core business operations. Starts with net income, then adjusts for non-cash items (depreciation, changes in working capital)\nInvesting activities: Cash spent on or received from long-term investments (e.g., purchasing equipment, acquiring businesses, selling assets)\nFinancing activities: Cash flows from capital structure decisions (e.g., issuing stock, borrowing, repaying debt, paying dividends, buying back shares)\n\nA company can show strong net income while burning cash if it’s building inventory, extending generous credit terms, or making large capital expenditures. Conversely, a company reporting losses might generate positive operating cash flow by collecting receivables faster than it pays suppliers.\n\n\n5.2.4 Illustrating with FPT’s Financial Statements\nTo see these concepts in practice, let’s examine FPT Corporation’s 2023 financial statements. FPT is one of Vietnam’s largest technology companies, providing IT services, telecommunications, and education.\n\n# Placeholder for FPT balance sheet visualization\n# In practice, this would display the actual PDF or cleaned data\n# from DataCore's acquisition pipeline\n\n# Example structure of what the balance sheet data looks like:\n# Assets: Current assets (cash, receivables, inventory) + Non-current assets (PP&E, intangibles)\n# Liabilities: Current liabilities (payables, short-term debt) + Non-current liabilities (long-term debt)\n# Equity: Common stock + Retained earnings\n\nThe balance sheet demonstrates the fundamental accounting equation in action. FPT’s assets (e.g., spanning cash, receivables, technology infrastructure, and intangible assets like software) exactly equal the sum of its liabilities and equity.\n\n# Placeholder for FPT income statement visualization\n# Shows the progression from revenue through various profit measures to net income\n\nFPT’s income statement reveals how the company transforms revenue into profit. The progression from gross profit through operating income to net income shows the impact of operating expenses, interest costs, and taxes.\n\n# Placeholder for FPT cash flow statement visualization\n# Reconciles net income with actual cash generation\n\nThe cash flow statement shows how FPT’s reported profits translate into actual cash. Differences between net income and operating cash flow reveal the impact of working capital management and non-cash expenses.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#loading-financial-statement-data",
    "href": "04_financial_statement_analysis.html#loading-financial-statement-data",
    "title": "5  Financial Statement Analysis",
    "section": "5.3 Loading Financial Statement Data",
    "text": "5.3 Loading Financial Statement Data\nWe now turn to systematic analysis across multiple companies. We load financial statement data for the VN30 index constituents (i.e., the 30 largest and most liquid stocks on Vietnam’s Ho Chi Minh Stock Exchange).\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncomp_vn = pd.read_sql_query(\n    sql=\"SELECT * FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\ncomp_vn.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nxsga\nxint\noancf\ncapx\nbe\nop\nat_lag\ninv\ntotal_debt\nselling_general_and_administrative_expenses\n\n\n\n\n0\nAGF\n1998\n8.845141e+10\nNone\n5.469709e+09\n0.000000e+00\nNone\nNone\n0.0\n1.110705e+10\n...\n1.199765e+10\n0.0\nNaN\nNaN\n2.656020e+10\n0.711195\nNaN\nNaN\n0.000000e+00\n1.990718e+10\n\n\n1\nBBC\n1999\n5.672574e+10\nNone\n5.354939e+09\n5.354939e+09\nNone\nNone\n0.0\n0.000000e+00\n...\n9.396468e+09\n0.0\n2.687635e+10\n1.097031e+10\n3.211410e+10\n0.728193\nNaN\nNaN\n1.505529e+09\n2.387858e+10\n\n\n2\nAGF\n1999\n9.558392e+10\nNone\n2.609276e+09\n0.000000e+00\nNone\nNone\n0.0\n1.008298e+10\n...\n1.595913e+10\n0.0\n1.675607e+10\n3.970966e+09\n3.576596e+10\n0.816972\n1.068410e+11\n0.090477\n0.000000e+00\n2.744458e+10\n\n\n\n\n3 rows × 333 columns\n\n\n\n\nvn30_symbols = [\n    \"ACB\", \"BCM\", \"BID\", \"BVH\", \"CTG\", \"FPT\", \"GAS\", \"GVR\", \"HDB\", \"HPG\",\n    \"MBB\", \"MSN\", \"MWG\", \"PLX\", \"POW\", \"SAB\", \"SHB\", \"SSB\", \"STB\", \"TCB\",\n    \"TPB\", \"VCB\", \"VHM\", \"VIB\", \"VIC\", \"VJC\", \"VNM\", \"VPB\", \"VRE\", \"EIB\"\n]\n\ncomp_vn30 = comp_vn[comp_vn[\"symbol\"].isin(vn30_symbols)]\ncomp_vn30.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nxsga\nxint\noancf\ncapx\nbe\nop\nat_lag\ninv\ntotal_debt\nselling_general_and_administrative_expenses\n\n\n\n\n11\nFPT\n2002\n5.098910e+11\nNone\n1.027470e+11\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n4.178100e+10\n9.008000e+09\n3.203600e+10\n2.202800e+10\n3.125400e+10\n3.293018\nNaN\nNaN\n0.0\n1.235850e+11\n\n\n17\nVNM\n2003\n2.101406e+12\nNone\n6.925924e+11\n6.925924e+11\nNone\nNone\n0.0\n0.0\n...\n9.089413e+10\n1.698909e+10\nNaN\nNaN\n1.560789e+12\n0.663257\nNaN\nNaN\n0.0\n5.037799e+11\n\n\n21\nFPT\n2003\n9.171390e+11\nNone\n7.995600e+10\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n8.584800e+10\n1.286700e+10\n-2.905420e+11\n3.753300e+10\n1.697000e+11\n0.940218\n5.504080e+11\n0.779104\n0.0\n1.968430e+11\n\n\n\n\n3 rows × 333 columns\n\n\n\nThis dataset provides the foundation for calculating financial ratios and conducting cross-sectional comparisons. Each row contains balance sheet, income statement, and cash flow items for a company-year observation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#liquidity-ratios-can-the-company-pay-its-bills",
    "href": "04_financial_statement_analysis.html#liquidity-ratios-can-the-company-pay-its-bills",
    "title": "5  Financial Statement Analysis",
    "section": "5.4 Liquidity Ratios: Can the Company Pay Its Bills?",
    "text": "5.4 Liquidity Ratios: Can the Company Pay Its Bills?\nLiquidity ratios assess a company’s ability to meet short-term obligations. These metrics matter most to creditors, suppliers, and employees who need assurance that the company can pay its bills. They’re calculated using balance sheet items, comparing liquid assets against near-term liabilities.\n\n5.4.1 The Current Ratio\nThe most basic liquidity measure compares all current assets to current liabilities:\n\\[\n\\text{Current Ratio} = \\frac{\\text{Current Assets}}{\\text{Current Liabilities}}\n\\]\nA ratio above one indicates the company has enough current assets to cover obligations due within one year. However, the interpretation depends heavily on the composition of current assets. A company with current assets tied up in slow-moving inventory is less liquid than one holding cash.\n\n\n5.4.2 The Quick Ratio\nThe quick ratio (or “acid test”) provides a more stringent measure by excluding inventory:\n\\[\n\\text{Quick Ratio} = \\frac{\\text{Current Assets} - \\text{Inventory}}{\\text{Current Liabilities}}\n\\]\nWhy exclude inventory? Inventory is typically the least liquid current asset. It must be sold (potentially at a discount) before generating cash. A company facing a liquidity crisis cannot easily convert raw materials or finished goods into immediate cash. The quick ratio answers: “Can we pay our bills without relying on inventory sales?”\n\n\n5.4.3 The Cash Ratio\nThe most conservative liquidity measure focuses solely on the most liquid assets:\n\\[\n\\text{Cash Ratio} = \\frac{\\text{Cash and Cash Equivalents}}{\\text{Current Liabilities}}\n\\]\nWhile a ratio of one indicates robust liquidity, most companies maintain lower cash ratios to avoid holding excessive non-productive assets. Cash sitting in bank accounts could otherwise be invested in growth opportunities, returned to shareholders, or used to pay down costly debt.\n\n\n5.4.4 Calculating Liquidity Ratios\nLet’s compute these ratios for our VN30 sample:\n\nbalance_sheet_statements = (comp_vn30\n    .assign(\n        fiscal_year=lambda x: x[\"year\"].astype(int),\n        \n        # Current Ratio: Current Assets / Current Liabilities\n        current_ratio=lambda x: x[\"act\"] / x[\"lct\"],\n        \n        # Quick Ratio: (Current Assets - Inventory) / Current Liabilities\n        quick_ratio=lambda x: (x[\"act\"] - x[\"inv\"]) / x[\"lct\"],\n        \n        # Cash Ratio: Cash and Equivalents / Current Liabilities\n        cash_ratio=lambda x: x[\"ca_cce\"] / x[\"lct\"],\n        \n        label=lambda x: np.where(\n            x[\"symbol\"].isin(vn30_symbols), x[\"symbol\"], np.nan\n        )\n    )\n)\n\nbalance_sheet_statements.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nop\nat_lag\ninv\ntotal_debt\nselling_general_and_administrative_expenses\nfiscal_year\ncurrent_ratio\nquick_ratio\ncash_ratio\nlabel\n\n\n\n\n11\nFPT\n2002\n5.098910e+11\nNone\n1.027470e+11\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n3.293018\nNaN\nNaN\n0.0\n1.235850e+11\n2002\n1.211413\nNaN\n0.244109\nFPT\n\n\n17\nVNM\n2003\n2.101406e+12\nNone\n6.925924e+11\n6.925924e+11\nNone\nNone\n0.0\n0.0\n...\n0.663257\nNaN\nNaN\n0.0\n5.037799e+11\n2003\n2.195772\nNaN\n0.723694\nVNM\n\n\n21\nFPT\n2003\n9.171390e+11\nNone\n7.995600e+10\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n0.940218\n5.504080e+11\n0.779104\n0.0\n1.968430e+11\n2003\n1.274633\n1.274633\n0.111122\nFPT\n\n\n\n\n3 rows × 338 columns\n\n\n\n\n\n5.4.5 Cross-Sectional Comparison of Liquidity\nFigure 5.1 compares liquidity ratios across companies for the most recent fiscal year. This cross-sectional view reveals how different business models and industries maintain different liquidity profiles.\n\nliquidity_ratios = (balance_sheet_statements\n    .query(\"year == 2023 & label.notna()\")\n    .get([\"symbol\", \"current_ratio\", \"quick_ratio\", \"cash_ratio\"])\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        name=lambda x: x[\"name\"].str.replace(\"_\", \" \").str.title()\n    )\n)\n\nliquidity_ratios_figure = (\n    ggplot(liquidity_ratios, aes(y=\"value\", x=\"name\", fill=\"symbol\"))\n    + geom_col(position=\"dodge\")\n    + coord_flip()\n    + labs(\n        x=\"\", y=\"Ratio Value\", fill=\"\",\n        title=\"Liquidity Ratios for VN30 Stocks (2023)\"\n    )\n)\n\nliquidity_ratios_figure.show()\n\n\n\n\n\n\n\nFigure 5.1: Liquidity ratios measure a company’s ability to meet short-term obligations. Higher values indicate greater liquidity, though excessively high ratios may suggest inefficient use of assets.\n\n\n\n\n\nSeveral patterns emerge from this comparison. Banks and financial institutions typically show different liquidity profiles than industrial companies due to their unique business models. Companies with high inventory (retailers, manufacturers) often show larger gaps between current and quick ratios.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#leverage-ratios-how-is-the-company-financed",
    "href": "04_financial_statement_analysis.html#leverage-ratios-how-is-the-company-financed",
    "title": "5  Financial Statement Analysis",
    "section": "5.5 Leverage Ratios: How Is the Company Financed?",
    "text": "5.5 Leverage Ratios: How Is the Company Financed?\nLeverage ratios examine a company’s capital structure (i.e., the mix of debt and equity financing). These metrics reveal financial risk and long-term solvency, helping investors understand how much of the company’s operations are funded by borrowed money.\n\n5.5.1 Why Capital Structure Matters\nA company’s financing choice involves fundamental trade-offs:\n\nDebt offers tax advantages (interest is deductible) and doesn’t dilute ownership, but creates fixed obligations that must be met regardless of business performance\nEquity provides flexibility (no required payments) but dilutes existing shareholders and may be more expensive than debt\n\nCompanies with high leverage amplify both gains and losses. In good times, shareholders capture more upside because profits aren’t shared with additional equity holders. In bad times, fixed interest payments can push the company toward distress. This is why beta (systematic risk) tends to increase with leverage.\n\n\n5.5.2 Debt-to-Equity Ratio\nThis ratio indicates how much debt financing the company uses relative to shareholder investment:\n\\[\n\\text{Debt-to-Equity} = \\frac{\\text{Total Debt}}{\\text{Total Equity}}\n\\]\nA ratio of 1.0 means equal parts debt and equity financing. Higher ratios indicate more aggressive use of leverage, which can enhance returns in good times but increases bankruptcy risk.\n\n\n5.5.3 Debt-to-Asset Ratio\nThis ratio shows what percentage of assets are financed through debt:\n\\[\n\\text{Debt-to-Asset} = \\frac{\\text{Total Debt}}{\\text{Total Assets}}\n\\]\nA ratio of 0.5 means half the company’s assets are debt-financed. This metric is bounded between 0 and 1 (assuming positive equity), making it easier to compare across companies than the debt-to-equity ratio.\n\n\n5.5.4 Interest Coverage Ratio\nWhile the above ratios measure leverage levels, interest coverage assesses the ability to service that debt:\n\\[\n\\text{Interest Coverage} = \\frac{\\text{EBIT}}{\\text{Interest Expense}}\n\\]\nThis ratio answers: “How many times over can current operating profits cover interest obligations?” A ratio below 1.0 means operating income doesn’t cover interest payments, which is a dangerous position. Ratios above 3-5 generally indicate comfortable coverage.\n\n\n5.5.5 Calculating Leverage Ratios\n\nbalance_sheet_statements = balance_sheet_statements.assign(\n    debt_to_equity=lambda x: x[\"total_debt\"] / x[\"total_equity\"],\n    debt_to_asset=lambda x: x[\"total_debt\"] / x[\"at\"]\n)\n\nincome_statements = (comp_vn30\n    .assign(\n        year=lambda x: x[\"year\"].astype(int),\n        # Handle zero interest expense to avoid infinity\n        interest_coverage=lambda x: np.where(\n            x[\"cfo_interest_expense\"] &gt; 0,\n            x[\"is_net_business_profit\"] / x[\"cfo_interest_expense\"],\n            np.nan\n        ),\n        label=lambda x: np.where(\n            x[\"symbol\"].isin(vn30_symbols), x[\"symbol\"], np.nan\n        )\n    )\n)\n\n\n\n5.5.6 Leverage Trends Over Time\nFigure 5.2 tracks how debt-to-asset ratios have evolved over time. Time-series analysis reveals whether companies are becoming more or less leveraged.\n\ndebt_to_asset = balance_sheet_statements.query(\"symbol in @vn30_symbols\")\n\ndebt_to_asset_figure = (\n    ggplot(debt_to_asset, aes(x=\"year\", y=\"debt_to_asset\", color=\"symbol\"))\n    + geom_line(size=1)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Debt-to-Asset Ratio\", color=\"\",\n        title=\"Debt-to-Asset Ratios of VN30 Stocks Over Time\"\n    )\n)\n\ndebt_to_asset_figure.show()\n\n\n\n\n\n\n\nFigure 5.2: Debt-to-asset ratios show the proportion of assets financed by debt. Changes over time reflect evolving capital structure strategies and market conditions.\n\n\n\n\n\n\n\n5.5.7 Cross-Sectional Leverage Comparison\nFigure 5.3 provides a snapshot of leverage across all VN30 constituents for the most recent year.\n\ndebt_to_asset_comparison = balance_sheet_statements.query(\"year == 2023\")\n\ndebt_to_asset_comparison[\"symbol\"] = pd.Categorical(\n    debt_to_asset_comparison[\"symbol\"],\n    categories=debt_to_asset_comparison.sort_values(\"debt_to_asset\")[\"symbol\"],\n    ordered=True\n)\n\ndebt_to_asset_comparison_figure = (\n    ggplot(\n        debt_to_asset_comparison,\n        aes(y=\"debt_to_asset\", x=\"symbol\", fill=\"label\")\n    )\n    + geom_col()\n    + coord_flip()\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Debt-to-Asset Ratio\", fill=\"\",\n        title=\"Debt-to-Asset Ratios of VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\ndebt_to_asset_comparison_figure.show()\n\n\n\n\n\n\n\nFigure 5.3: Cross-sectional comparison of debt-to-asset ratios reveals industry patterns and company-specific financing strategies.\n\n\n\n\n\n\n\n5.5.8 The Leverage-Coverage Trade-off\nFigure 5.4 examines the relationship between leverage levels and debt-servicing ability. Companies with higher debt loads should ideally have stronger interest coverage to maintain financial stability.\n\ninterest_coverage = (income_statements\n    .query(\"year == 2023\")\n    .get([\"symbol\", \"year\", \"interest_coverage\"])\n    .merge(balance_sheet_statements, on=[\"symbol\", \"year\"], how=\"left\")\n)\n\ninterest_coverage_figure = (\n    ggplot(\n        interest_coverage,\n        aes(x=\"debt_to_asset\", y=\"interest_coverage\", color=\"label\")\n    )\n    + geom_point(size=2)\n    + geom_label(\n        aes(label=\"label\"),\n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + labs(\n        x=\"Debt-to-Asset Ratio\", y=\"Interest Coverage Ratio\",\n        title=\"Leverage versus Interest Coverage for VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\ninterest_coverage_figure.show()\n\n\n\n\n\n\n\nFigure 5.4: The relationship between leverage and interest coverage reveals whether companies can comfortably service their debt. High leverage with low coverage indicates elevated financial risk.\n\n\n\n\n\nThe scatter plot reveals important patterns. Companies in the upper-left quadrant (low leverage, high coverage) have conservative financing with ample debt capacity. Those in the lower-right (high leverage, low coverage) face elevated financial risk.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#efficiency-ratios-how-well-are-assets-managed",
    "href": "04_financial_statement_analysis.html#efficiency-ratios-how-well-are-assets-managed",
    "title": "5  Financial Statement Analysis",
    "section": "5.6 Efficiency Ratios: How Well Are Assets Managed?",
    "text": "5.6 Efficiency Ratios: How Well Are Assets Managed?\nEfficiency ratios measure how effectively a company utilizes its assets and manages operations. These metrics help identify whether management is extracting maximum value from the company’s resource base.\n\n5.6.1 Asset Turnover\nThis ratio measures how efficiently a company uses total assets to generate revenue:\n\\[\n\\text{Asset Turnover} = \\frac{\\text{Revenue}}{\\text{Total Assets}}\n\\]\nA higher ratio indicates more efficient asset utilization: the company generates more sales per dollar of assets. However, optimal levels vary dramatically across industries. Retailers with minimal fixed assets might achieve turnovers above 2.0, while capital-intensive manufacturers might operate below 0.5.\n\n\n5.6.2 Inventory Turnover\nFor companies carrying inventory, this ratio reveals how quickly stock moves through the business:\n\\[\n\\text{Inventory Turnover} = \\frac{\\text{Cost of Goods Sold}}{\\text{Inventory}}\n\\]\nHigher turnover suggests efficient inventory management (i.e., goods don’t sit on shelves collecting dust). However, extremely high turnover might indicate stockout risks, while very low turnover could signal obsolete inventory or overinvestment in working capital.\nWe use COGS rather than revenue in the numerator because inventory is recorded at cost, not selling price. Using revenue would overstate turnover for high-margin businesses.\n\n\n5.6.3 Receivables Turnover\nThis ratio measures how effectively a company collects payments from customers:\n\\[\n\\text{Receivables Turnover} = \\frac{\\text{Revenue}}{\\text{Accounts Receivable}}\n\\]\nHigher turnover indicates faster collection (i.e., customers pay promptly). Converting this to “days sales outstanding” (365 / turnover) gives the average collection period in days. Companies must balance collection efficiency against the sales impact of restrictive credit policies.\n\n\n5.6.4 Calculating Efficiency Ratios\n\ncombined_statements = (balance_sheet_statements\n    .get([\n        \"symbol\", \"year\", \"label\", \"current_ratio\", \"quick_ratio\",\n        \"cash_ratio\", \"debt_to_equity\", \"debt_to_asset\", \"total_asset\",\n        \"total_equity\"\n    ])\n    .merge(\n        (income_statements\n            .get([\n                \"symbol\", \"year\", \"interest_coverage\", \"is_revenue\",\n                \"is_cogs\", \"selling_general_and_administrative_expenses\",\n                \"is_interest_expense\", \"is_gross_profit\", \"is_eat\"\n            ])\n        ),\n        on=[\"symbol\", \"year\"],\n        how=\"left\"\n    )\n    .merge(\n        (comp_vn30\n            .assign(year=lambda x: x[\"year\"].astype(int))\n            .get([\"symbol\", \"year\", \"ca_total_inventory\", \"ca_acc_receiv\"])\n        ),\n        on=[\"symbol\", \"year\"],\n        how=\"left\"\n    )\n)\n\ncombined_statements = combined_statements.assign(\n    asset_turnover=lambda x: x[\"is_revenue\"] / x[\"total_asset\"],\n    inventory_turnover=lambda x: x[\"is_cogs\"] / x[\"ca_total_inventory\"],\n    receivables_turnover=lambda x: x[\"is_revenue\"] / x[\"ca_acc_receiv\"]\n)\n\nEfficiency ratios vary dramatically across industries, making peer comparison essential. A grocery store and a shipbuilder will have fundamentally different asset and inventory dynamics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#profitability-ratios-is-the-company-making-money",
    "href": "04_financial_statement_analysis.html#profitability-ratios-is-the-company-making-money",
    "title": "5  Financial Statement Analysis",
    "section": "5.7 Profitability Ratios: Is the Company Making Money?",
    "text": "5.7 Profitability Ratios: Is the Company Making Money?\nProfitability ratios evaluate how effectively a company converts activity into earnings. These metrics directly measure financial success and are among the most closely watched indicators by investors.\n\n5.7.1 Gross Margin\nThe gross margin reveals what percentage of revenue remains after direct production costs:\n\\[\n\\text{Gross Margin} = \\frac{\\text{Gross Profit}}{\\text{Revenue}} = \\frac{\\text{Revenue} - \\text{COGS}}{\\text{Revenue}}\n\\]\nHigher gross margins indicate stronger pricing power, more efficient production, or a favorable product mix. This metric is particularly useful for comparing companies within an industry, as it reveals relative efficiency in core operations before overhead costs.\n\n\n5.7.2 Profit Margin\nThe profit margin shows what percentage of revenue ultimately becomes net income:\n\\[\n\\text{Profit Margin} = \\frac{\\text{Net Income}}{\\text{Revenue}}\n\\]\nThis comprehensive measure accounts for all costs (e.g., production, operations, interest, and taxes). Higher profit margins suggest effective overall cost management. However, optimal margins vary by industry: software companies routinely achieve 20%+ margins, while grocery stores operate on razor-thin 2-3% margins.\n\n\n5.7.3 Return on Equity (ROE)\nROE measures how efficiently a company uses shareholders’ investment to generate profits:\n\\[\n\\text{Return on Equity} = \\frac{\\text{Net Income}}{\\text{Total Equity}}\n\\]\nThis metric directly addresses what shareholders care about: returns on their invested capital. Higher ROE indicates more effective use of equity, though interpretation requires caution. High leverage can artificially inflate ROE by reducing the equity base (e.g., a company financed 90% by debt will show spectacular ROE on modest profits).\n\n\n5.7.4 The DuPont Decomposition\nThe DuPont framework decomposes ROE into three components that reveal different aspects of performance:\n\\[\n\\text{ROE} = \\underbrace{\\frac{\\text{Net Income}}{\\text{Revenue}}}_{\\text{Profit Margin}} \\times \\underbrace{\\frac{\\text{Revenue}}{\\text{Assets}}}_{\\text{Asset Turnover}} \\times \\underbrace{\\frac{\\text{Assets}}{\\text{Equity}}}_{\\text{Leverage}}\n\\]\nThis decomposition shows that high ROE can come from different sources: strong profit margins (pricing power, cost control), efficient asset use (high turnover), or aggressive leverage. Understanding which driver dominates helps assess sustainability. ROE driven by margins is generally more sustainable than ROE driven by leverage.\n\n\n5.7.5 Calculating Profitability Ratios\n\ncombined_statements = combined_statements.assign(\n    gross_margin=lambda x: x[\"is_gross_profit\"] / x[\"is_revenue\"],\n    profit_margin=lambda x: x[\"is_eat\"] / x[\"is_revenue\"],\n    after_tax_roe=lambda x: x[\"is_eat\"] / x[\"total_equity\"]\n)\n\n\n\n5.7.6 Gross Margin Trends\nFigure 5.5 tracks gross margin evolution over time, revealing whether companies are maintaining pricing power and production efficiency.\n\ngross_margins = combined_statements.query(\"symbol in @vn30_symbols\")\n\ngross_margins_figure = (\n    ggplot(gross_margins, aes(x=\"year\", y=\"gross_margin\", color=\"symbol\"))\n    + geom_line()\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Gross Margin\", color=\"\",\n        title=\"Gross Margins for VN30 Stocks (2019-2023)\"\n    )\n)\n\ngross_margins_figure.show()\n\n\n\n\n\n\n\nFigure 5.5: Gross margin trends reveal changes in pricing power and production efficiency. Declining margins may signal increased competition or rising input costs.\n\n\n\n\n\n\n\n5.7.7 From Gross to Net: Where Do Profits Go?\nFigure 5.6 examines the relationship between gross and profit margins. The gap between them reveals the impact of operating expenses, interest, and taxes.\n\nprofit_margins = combined_statements.query(\"year == 2023\")\n\nprofit_margins_figure = (\n    ggplot(\n        profit_margins,\n        aes(x=\"gross_margin\", y=\"profit_margin\", color=\"label\")\n    )\n    + geom_point(size=2)\n    + geom_label(\n        aes(label=\"label\"),\n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Gross Margin\", y=\"Profit Margin\",\n        title=\"Gross versus Profit Margins for VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\nprofit_margins_figure.show()\n\n\n\n\n\n\n\nFigure 5.6: Comparing gross and profit margins reveals how much of gross profit survives operating expenses, interest, and taxes. Companies far below the diagonal have high overhead relative to gross profit.\n\n\n\n\n\nCompanies along the diagonal convert gross profit to net income efficiently. Those well below the diagonal face high operating costs, interest burdens, or tax rates that erode profitability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#combining-financial-ratios-a-holistic-view",
    "href": "04_financial_statement_analysis.html#combining-financial-ratios-a-holistic-view",
    "title": "5  Financial Statement Analysis",
    "section": "5.8 Combining Financial Ratios: A Holistic View",
    "text": "5.8 Combining Financial Ratios: A Holistic View\nIndividual ratios provide specific insights, but combining them offers a more complete picture. A company might excel in profitability while struggling with liquidity, or maintain conservative leverage while underperforming on efficiency.\n\n5.8.1 Ranking Companies Across Categories\nFigure 5.7 compares company rankings across four ratio categories. Rankings closer to 1 indicate better performance within each category, enabling quick identification of relative strengths and weaknesses.\n\nfinancial_ratios = (combined_statements\n    .query(\"year == 2023\")\n    .filter(\n        items=[\"symbol\"] + [\n            col for col in combined_statements.columns\n            if any(x in col for x in [\n                \"ratio\", \"margin\", \"roe\", \"_to_\", \"turnover\", \"interest_coverage\"\n            ])\n        ]\n    )\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        type=lambda x: np.select(\n            [\n                x[\"name\"].isin([\"current_ratio\", \"quick_ratio\", \"cash_ratio\"]),\n                x[\"name\"].isin([\"debt_to_equity\", \"debt_to_asset\", \"interest_coverage\"]),\n                x[\"name\"].isin([\"asset_turnover\", \"inventory_turnover\", \"receivables_turnover\"]),\n                x[\"name\"].isin([\"gross_margin\", \"profit_margin\", \"after_tax_roe\"]),\n            ],\n            [\n                \"Liquidity Ratios\",\n                \"Leverage Ratios\",\n                \"Efficiency Ratios\",\n                \"Profitability Ratios\"\n            ],\n            default=\"Other\"\n        )\n    )\n)\n\nfinancial_ratios[\"rank\"] = (financial_ratios\n    .sort_values([\"type\", \"name\", \"value\"], ascending=[True, True, False])\n    .groupby([\"type\", \"name\"])\n    .cumcount() + 1\n)\n\nfinal_ranks = (financial_ratios\n    .groupby([\"symbol\", \"type\"], as_index=False)\n    .agg(rank=(\"rank\", \"mean\"))\n    .query(\"symbol in @vn30_symbols\")\n)\n\nfinal_ranks_figure = (\n    ggplot(final_ranks, aes(x=\"rank\", y=\"type\", color=\"symbol\"))\n    + geom_point(shape=\"^\", size=4)\n    + labs(\n        x=\"Average Rank (Lower is Better)\", y=\"\", color=\"\",\n        title=\"Average Rank Across Financial Ratio Categories\"\n    )\n    + coord_cartesian(xlim=[1, 30])\n)\n\nfinal_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 5.7: Ranking companies across multiple ratio categories reveals overall financial profiles. Companies with consistently low ranks across categories demonstrate broad-based financial strength.\n\n\n\n\n\nThe combined view reveals how different business strategies manifest in financial profiles. A company might deliberately accept lower profitability rankings in exchange for stronger liquidity, or use aggressive leverage to boost returns at the cost of financial flexibility.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#financial-ratios-in-asset-pricing",
    "href": "04_financial_statement_analysis.html#financial-ratios-in-asset-pricing",
    "title": "5  Financial Statement Analysis",
    "section": "5.9 Financial Ratios in Asset Pricing",
    "text": "5.9 Financial Ratios in Asset Pricing\nBeyond evaluating individual companies, financial ratios serve as crucial inputs for asset pricing models. The Fama-French five-factor model, which we explore in detail in Fama-French Factors, uses several accounting-based measures to explain cross-sectional variation in stock returns.\n\n5.9.1 The Fama-French Factors\nThe model incorporates four company characteristics derived from financial statements:\nSize is measured as the logarithm of market capitalization: \\[\n\\text{Size} = \\ln(\\text{Market Cap})\n\\]\nThis captures the empirical finding that smaller firms tend to outperform larger firms on a risk-adjusted basis (i.e., the “size premium”).\nBook-to-Market relates accounting value to market value: \\[\n\\text{Book-to-Market} = \\frac{\\text{Book Equity}}{\\text{Market Cap}}\n\\]\nHigh book-to-market stocks (“value” stocks) have historically outperformed low book-to-market stocks (“growth” stocks) (i.e., the “value premium”).\nOperating Profitability measures profit generation relative to equity: \\[\n\\text{Profitability} = \\frac{\\text{Revenue} - \\text{COGS} - \\text{SG\\&A} - \\text{Interest}}{\\text{Book Equity}}\n\\]\nMore profitable firms tend to earn higher returns (i.e., the “profitability premium”).\nInvestment captures asset growth: \\[\n\\text{Investment} = \\frac{\\text{Total Assets}_t}{\\text{Total Assets}_{t-1}} - 1\n\\] Firms investing aggressively tend to underperform conservative investors (i.e., the “investment premium”).\n\n\n5.9.2 Calculating Fama-French Variables\n\nprices_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM prices_monthly\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\n# Use December prices for annual calculations\nprices_december = (prices_monthly\n    .assign(date=lambda x: pd.to_datetime(x[\"date\"]))\n    .query(\"date.dt.month == 12\")\n)\n\n\ncombined_statements_ff = (combined_statements\n    .query(\"year == 2023\")\n    .merge(prices_december, on=[\"symbol\", \"year\"], how=\"left\")\n    .merge(\n        (balance_sheet_statements\n            .query(\"year == 2022\")\n            .get([\"symbol\", \"total_asset\"])\n            .rename(columns={\"total_asset\": \"total_assets_lag\"})\n        ),\n        on=\"symbol\",\n        how=\"left\"\n    )\n    .assign(\n        size=lambda x: np.log(x[\"mktcap\"]),\n        book_to_market=lambda x: x[\"total_equity\"] / x[\"mktcap\"],\n        operating_profitability=lambda x: (\n            (x[\"is_revenue\"] - x[\"is_cogs\"] -\n             x[\"selling_general_and_administrative_expenses\"] -\n             x[\"is_interest_expense\"]) / x[\"total_equity\"]\n        ),\n        investment=lambda x: x[\"total_asset\"] / x[\"total_assets_lag\"] - 1\n    )\n)\n\ncombined_statements_ff.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\nlabel\ncurrent_ratio\nquick_ratio\ncash_ratio\ndebt_to_equity\ndebt_to_asset\ntotal_asset\ntotal_equity\n...\nshrout\nmktcap\nmktcap_lag\nrisk_free\nret_excess\ntotal_assets_lag\nsize\nbook_to_market\noperating_profitability\ninvestment\n\n\n\n\n0\nPOW\n2023\nPOW\n1.084255\n1.084255\n0.315089\n0.0\n0.0\n7.036209e+13\n3.411943e+13\n...\n2.341872e+09\n26346.055367\n26346.055367\n0.003333\n-0.003333\n5.684324e+13\n10.179074\n1.295049e+09\n0.025539\n0.237827\n\n\n1\nHPG\n2023\nHPG\n1.156655\n1.156655\n0.171324\n0.0\n0.0\n1.877826e+14\n1.028364e+14\n...\n5.814786e+09\n162523.260217\n154382.560242\n0.003333\n-0.003333\n1.703355e+14\n11.998576\n6.327489e+08\n0.072798\n0.102428\n\n\n2\nMWG\n2023\nMWG\n1.688604\n1.688604\n0.174408\n0.0\n0.0\n6.011124e+13\n2.335956e+13\n...\n1.462941e+09\n62613.896064\n56323.247628\n0.003333\n-0.009141\n5.583410e+13\n11.044743\n3.730731e+08\n-0.002443\n0.076604\n\n\n\n\n3 rows × 65 columns\n\n\n\n\n\n5.9.3 Fama-French Factor Rankings\nFigure 5.8 shows how VN30 companies rank on each Fama-French variable, connecting fundamental analysis to asset pricing.\n\nfactors_ranks = (combined_statements_ff\n    .get([\"symbol\", \"size\", \"book_to_market\", \"operating_profitability\", \"investment\"])\n    .rename(columns={\n        \"size\": \"Size\",\n        \"book_to_market\": \"Book-to-Market\",\n        \"operating_profitability\": \"Profitability\",\n        \"investment\": \"Investment\"\n    })\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        rank=lambda x: (\n            x.sort_values([\"name\", \"value\"], ascending=[True, False])\n            .groupby(\"name\")\n            .cumcount() + 1\n        )\n    )\n    .query(\"symbol in @vn30_symbols\")\n)\n\nfactors_ranks_figure = (\n    ggplot(factors_ranks, aes(x=\"rank\", y=\"name\", color=\"symbol\"))\n    + geom_point(shape=\"^\", size=4)\n    + labs(\n        x=\"Rank\", y=\"\", color=\"\",\n        title=\"Rank in Fama-French Variables for VN30 Stocks\"\n    )\n    + coord_cartesian(xlim=[1, 30])\n)\n\nfactors_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 5.8: Rankings on Fama-French variables connect financial statement analysis to asset pricing. According to factor models, smaller, higher book-to-market, more profitable, and lower-investment firms should earn higher expected returns.\n\n\n\n\n\nThese rankings have implications for expected returns according to factor models. A small, high book-to-market, highly profitable company with conservative investment should, in theory, earn higher risk-adjusted returns than its opposite.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#limitations-and-practical-considerations",
    "href": "04_financial_statement_analysis.html#limitations-and-practical-considerations",
    "title": "5  Financial Statement Analysis",
    "section": "5.10 Limitations and Practical Considerations",
    "text": "5.10 Limitations and Practical Considerations\nWhile financial ratios provide powerful analytical tools, several limitations deserve attention:\n\n5.10.1 Accounting Discretion\nCompanies have significant discretion in how they apply accounting standards. Revenue recognition timing, depreciation methods, inventory valuation (FIFO vs. LIFO), and capitalization versus expensing decisions all affect reported numbers. Sophisticated analysis requires understanding these choices and their impact.\n\n\n5.10.2 Industry Comparability\nRatios vary dramatically across industries. Comparing a bank’s leverage to a retailer’s is meaningless (e.g., banks naturally operate with much higher leverage due to their business model). Always benchmark against industry peers rather than absolute standards.\n\n\n5.10.3 Point-in-Time Limitations\nBalance sheet ratios capture a single moment, which may not represent typical conditions. Companies often “window dress” by temporarily improving metrics at reporting dates. Trend analysis and quarter-over-quarter comparisons can reveal such practices.\n\n\n5.10.4 Backward-Looking Nature\nFinancial statements report historical results. Past profitability doesn’t guarantee future performance, especially for companies in rapidly changing industries or facing disruption.\n\n\n5.10.5 Quality of Earnings\nNot all profits are created equal. Earnings driven by one-time gains, accounting adjustments, or aggressive revenue recognition may not recur. Cash flow analysis helps assess earnings quality. Profits that don’t convert to cash warrant skepticism.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#key-takeaways",
    "href": "04_financial_statement_analysis.html#key-takeaways",
    "title": "5  Financial Statement Analysis",
    "section": "5.11 Key Takeaways",
    "text": "5.11 Key Takeaways\nThis chapter introduced financial statement analysis as a tool for understanding company fundamentals. The main insights are:\n\nThree statements, three perspectives: The balance sheet shows financial position at a point in time, the income statement measures performance over a period, and the cash flow statement tracks actual cash movements. Together, they provide a complete picture of financial health.\nLiquidity ratios assess short-term survival: Current, quick, and cash ratios measure the ability to meet near-term obligations. Higher ratios indicate greater liquidity but may suggest inefficient asset use.\nLeverage ratios reveal capital structure risk: Debt-to-equity, debt-to-asset, and interest coverage ratios show how the company finances operations and whether it can service its debt. Higher leverage amplifies both returns and risk.\nEfficiency ratios measure management effectiveness: Asset turnover, inventory turnover, and receivables turnover reveal how well the company converts resources into revenue. Industry context is essential for interpretation.\nProfitability ratios quantify financial success: Gross margin, profit margin, and ROE measure the ability to generate earnings. The DuPont decomposition reveals whether ROE comes from margins, turnover, or leverage.\nRatios connect to asset pricing: Financial statement variables like book-to-market, profitability, and investment form the basis of factor models that explain cross-sectional return differences.\nContext matters for interpretation: Ratios must be compared against industry peers, tracked over time, and considered alongside qualitative factors. No single ratio tells the complete story.\n\nLooking ahead, subsequent chapters will explore how these fundamental variables interact with market prices in asset pricing models, and how to construct factor portfolios based on financial statement characteristics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html",
    "href": "05_discounted_cash_flow.html",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "",
    "text": "6.1 What Is a Company Worth?\nThe previous chapters examined how markets price securities in equilibrium and how financial statements reveal company fundamentals. But these approaches leave a central question unanswered: What is the intrinsic value of a business, independent of its current market price?\nDiscounted Cash Flow (DCF) analysis answers this question by valuing a company based on its ability to generate cash for investors. The core insight is simple: a business is worth the present value of all future cash it will produce. This principle that value equals discounted future cash flows underlies virtually all of finance, from bond pricing to real estate valuation.\nDCF analysis stands apart from other valuation approaches in three important ways. First, it explicitly accounts for the time value of money (i.e., the principle that a dollar today is worth more than a dollar tomorrow). By discounting future cash flows at an appropriate rate, we incorporate both time preferences and risk. Second, DCF is forward-looking, making it particularly suitable for companies where historical performance may not reflect future potential. Third, DCF is flexible enough to accommodate various business models and capital structures, making it applicable across industries and company sizes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#what-is-a-company-worth",
    "href": "05_discounted_cash_flow.html#what-is-a-company-worth",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "",
    "text": "6.1.1 Valuation Methods Overview\nCompany valuation methods broadly fall into three categories:\n\nMarket-based approaches compare companies using relative metrics like Price-to-Earnings or EV/EBITDA ratios. These are quick but assume comparable companies are fairly valued.\nAsset-based methods focus on the net value of tangible and intangible assets. These work well for liquidation scenarios but miss going-concern value.\nIncome-based techniques value companies based on their ability to generate future cash flows. DCF is the most rigorous income-based method.\n\nWe focus on DCF because it forces analysts to make explicit assumptions about growth, profitability, and risk. These assumptions are often hidden in other methods. Even when DCF isn’t the final word on valuation, the discipline of building a DCF model deepens understanding of what drives value.\n\n\n6.1.2 The Three Pillars of DCF\nEvery DCF analysis rests on three components:\n\nFree Cash Flow (FCF) forecasts: The expected future cash available for distribution to investors after operating expenses, taxes, and investments\nTerminal value: The company’s value beyond the explicit forecast period, often representing a majority of total valuation\nDiscount rate: Typically the Weighted Average Cost of Capital (WACC), which adjusts future cash flows to present value by incorporating risk and capital structure\n\nWe make simplifying assumptions throughout this chapter. In particular, we assume firms conduct only operating activities (i.e., financial statements do not include non-operating items like excess cash or investment securities). Real-world valuations require valuing these separately. Entire textbooks are devoted to valuation nuances; our goal is to establish the conceptual framework and practical implementation.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\nfrom itertools import product",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#understanding-free-cash-flow",
    "href": "05_discounted_cash_flow.html#understanding-free-cash-flow",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.2 Understanding Free Cash Flow",
    "text": "6.2 Understanding Free Cash Flow\nBefore diving into calculations, we need to understand what Free Cash Flow represents and why it matters for valuation.\n\n6.2.1 Why Free Cash Flow, Not Net Income?\nAccountants report net income, but DCF uses free cash flow. Why the difference?\nNet income includes non-cash items (like depreciation) and ignores cash needs (like capital expenditures and working capital investments). A company can report strong profits while burning cash, or generate substantial cash while reporting losses. Free cash flow captures what actually matters for valuation: the cash available to distribute to all capital providers (both debt holders and equity holders) after funding operations and investments.\n\n\n6.2.2 The Free Cash Flow Formula\nWe calculate FCF using the following formula:\n\\[\n\\text{FCF} = \\text{EBIT} \\times (1 - \\tau) + \\text{D\\&A} - \\Delta\\text{WC} - \\text{CAPEX}\n\\]\nwhere:\n\nEBIT (Earnings Before Interest and Taxes): Core operating profit before financing costs and taxes\n\\(\\tau\\): Corporate tax rate applied to operating profits\nD&A (Depreciation & Amortization): Non-cash charges that reduce reported earnings but don’t consume cash\n\\(\\Delta\\)WC (Change in Working Capital): Cash tied up in (or released from) operations (increases in receivables and inventory consume cash, while increases in payables provide cash)\nCAPEX (Capital Expenditures): Investments in long-term assets required to maintain and grow operations\n\nAn alternative formulation starts from EBIT directly:\n\\[\n\\text{FCF} = \\text{EBIT} + \\text{D\\&A} - \\text{Taxes} - \\Delta\\text{WC} - \\text{CAPEX}\n\\]\nBoth formulations yield the same result when taxes are calculated consistently. The key insight is that FCF represents cash generated from operations after all reinvestment needs (i.e., cash that could theoretically be distributed to investors without impairing the business).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#loading-historical-financial-data",
    "href": "05_discounted_cash_flow.html#loading-historical-financial-data",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.3 Loading Historical Financial Data",
    "text": "6.3 Loading Historical Financial Data\nWe use FPT Corporation, one of Vietnam’s largest technology companies, as our case study. FPT provides IT services, telecommunications, and education. It’s a diversified business with meaningful capital requirements and growth potential.\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncomp_vn = pd.read_sql_query(\n    sql=\"SELECT * FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Filter to FPT and examine the data structure\nfpt_data = comp_vn[comp_vn[\"symbol\"] == \"FPT\"].copy()\nfpt_data[\"year\"] = fpt_data[\"year\"].astype(int)\nfpt_data = fpt_data.sort_values(\"year\").reset_index(drop=True)\n\nprint(f\"Available years: {fpt_data['year'].min()} to {fpt_data['year'].max()}\")\nprint(f\"Number of observations: {len(fpt_data)}\")\n\nAvailable years: 2002 to 2023\nNumber of observations: 22\n\n\n\n6.3.1 Computing Historical Free Cash Flow\nLet’s calculate the components needed for FCF from the financial statement data:\n\n# Extract and compute FCF components\nhistorical_data = (fpt_data\n    .assign(\n        # Revenue for ratio calculations\n        revenue=lambda x: x[\"is_net_revenue\"],\n        \n        # EBIT = Earnings before interest and taxes\n        # Approximate as EBT + Interest Expense\n        ebit=lambda x: x[\"is_ebt\"] + x[\"is_interest_expense\"],\n        \n        # Tax payments (use actual tax expense)\n        taxes=lambda x: x[\"is_cit_expense\"],\n        \n        # Depreciation and amortization (non-cash add-back)\n        depreciation=lambda x: x[\"cfo_depreciation\"],\n        \n        # Change in working capital components\n        # Positive delta_wc means cash is consumed (tied up in working capital)\n        delta_working_capital=lambda x: (\n            x[\"cfo_receive\"] +      # Change in receivables\n            x[\"cfo_inventory\"] -    # Change in inventory  \n            x[\"cfo_payale\"]         # Change in payables (negative = cash source)\n        ),\n        \n        # Capital expenditures\n        capex=lambda x: x[\"capex\"]\n    )\n    .loc[:, [\n        \"year\", \"revenue\", \"ebit\", \"taxes\", \"depreciation\",\n        \"delta_working_capital\", \"capex\"\n    ]]\n)\n\n# Calculate Free Cash Flow\nhistorical_data[\"fcf\"] = (\n    historical_data[\"ebit\"] \n    - historical_data[\"taxes\"]\n    + historical_data[\"depreciation\"]\n    - historical_data[\"delta_working_capital\"]\n    - historical_data[\"capex\"]\n)\n\nhistorical_data\n\n\n\n\n\n\n\n\nyear\nrevenue\nebit\ntaxes\ndepreciation\ndelta_working_capital\ncapex\nfcf\n\n\n\n\n0\n2002\n1.514961e+12\n2.698700e+10\n0.000000e+00\n1.261500e+10\n-2.561760e+11\n2.202800e+10\n2.737500e+11\n\n\n1\n2003\n4.148298e+12\n5.676100e+10\n0.000000e+00\n1.837700e+10\n-5.078740e+11\n3.753300e+10\n5.454790e+11\n\n\n2\n2004\n8.734781e+12\n2.145902e+11\n1.795700e+10\n2.947900e+10\n-4.280270e+11\n5.252100e+10\n6.016182e+11\n\n\n3\n2005\n1.410079e+13\n3.753490e+11\n4.251500e+10\n5.381700e+10\n-4.471110e+11\n1.428320e+11\n6.909300e+11\n\n\n4\n2006\n2.139975e+13\n6.672593e+11\n7.368682e+10\n1.068192e+11\n-1.173099e+12\n2.459780e+11\n1.627513e+12\n\n\n5\n2007\n1.349889e+13\n1.071941e+12\n1.487146e+11\n1.709335e+11\n-1.873794e+12\n4.802762e+11\n2.487677e+12\n\n\n6\n2008\n1.638184e+13\n1.320573e+12\n1.890384e+11\n2.395799e+11\n-1.419506e+11\n6.690461e+11\n8.440192e+11\n\n\n7\n2009\n1.840403e+13\n1.807221e+12\n2.916482e+11\n3.041813e+11\n-8.065011e+11\n7.632280e+11\n1.863027e+12\n\n\n8\n2010\n2.001730e+13\n2.261341e+12\n3.314359e+11\n3.294060e+11\n-2.360993e+12\n8.672138e+11\n3.753090e+12\n\n\n9\n2011\n2.537025e+13\n2.751044e+12\n4.223952e+11\n3.759567e+11\n-2.099380e+12\n4.524081e+11\n4.351578e+12\n\n\n10\n2012\n2.459430e+13\n2.635219e+12\n4.210738e+11\n3.995598e+11\n8.043763e+11\n7.083318e+11\n1.100997e+12\n\n\n11\n2013\n2.702789e+13\n2.690568e+12\n4.503170e+11\n4.429860e+11\n-1.947751e+12\n9.110216e+11\n3.719967e+12\n\n\n12\n2014\n3.264466e+13\n2.625389e+12\n3.800994e+11\n5.472736e+11\n-3.078130e+12\n1.417399e+12\n4.453295e+12\n\n\n13\n2015\n3.795970e+13\n3.113651e+12\n4.130641e+11\n7.328801e+11\n-1.951778e+12\n1.974295e+12\n3.410951e+12\n\n\n14\n2016\n3.953147e+13\n3.388085e+12\n4.382078e+11\n9.334397e+11\n-9.242713e+11\n1.428472e+12\n3.379116e+12\n\n\n15\n2017\n4.265861e+13\n4.623663e+12\n7.270039e+11\n1.039417e+12\n-4.638788e+12\n1.100498e+12\n8.474367e+12\n\n\n16\n2018\n2.321354e+13\n4.095947e+12\n6.236054e+11\n1.164692e+12\n-1.033438e+12\n2.452902e+12\n3.217569e+12\n\n\n17\n2019\n2.771696e+13\n5.023518e+12\n7.528183e+11\n1.354613e+12\n-5.308818e+11\n3.230818e+12\n2.925377e+12\n\n\n18\n2020\n2.983040e+13\n5.648794e+12\n8.397114e+11\n1.490607e+12\n-8.040730e+11\n3.014322e+12\n4.089441e+12\n\n\n19\n2021\n3.565726e+13\n6.821202e+12\n9.879053e+11\n1.643916e+12\n-2.821825e+12\n2.908134e+12\n7.390903e+12\n\n\n20\n2022\n4.400953e+13\n8.308009e+12\n1.170940e+12\n1.833064e+12\n-3.746661e+12\n3.209581e+12\n9.507213e+12\n\n\n21\n2023\n5.261790e+13\n1.003565e+13\n1.414956e+12\n2.286514e+12\n-2.147304e+12\n3.948982e+12\n9.105534e+12\n\n\n\n\n\n\n\n\n\n6.3.2 Understanding the Historical Pattern\nBefore forecasting, we should understand the historical trends in FCF and its components:\n\n# Calculate key ratios relative to revenue\nhistorical_ratios = (historical_data\n    .assign(\n        # Revenue growth (year-over-year)\n        revenue_growth=lambda x: x[\"revenue\"].pct_change(),\n        \n        # Operating margin: EBIT as % of revenue\n        operating_margin=lambda x: x[\"ebit\"] / x[\"revenue\"],\n        \n        # Depreciation as % of revenue\n        depreciation_margin=lambda x: x[\"depreciation\"] / x[\"revenue\"],\n        \n        # Tax rate (taxes as % of revenue, for simplicity)\n        tax_margin=lambda x: x[\"taxes\"] / x[\"revenue\"],\n        \n        # Working capital intensity\n        working_capital_margin=lambda x: x[\"delta_working_capital\"] / x[\"revenue\"],\n        \n        # Capital intensity\n        capex_margin=lambda x: x[\"capex\"] / x[\"revenue\"],\n        \n        # FCF margin\n        fcf_margin=lambda x: x[\"fcf\"] / x[\"revenue\"]\n    )\n)\n\n# Display key metrics\ndisplay_cols = [\n    \"year\", \"revenue_growth\", \"operating_margin\", \"depreciation_margin\",\n    \"tax_margin\", \"working_capital_margin\", \"capex_margin\", \"fcf_margin\"\n]\n\nhistorical_ratios[display_cols].round(3)\n\n\n\n\n\n\n\n\nyear\nrevenue_growth\noperating_margin\ndepreciation_margin\ntax_margin\nworking_capital_margin\ncapex_margin\nfcf_margin\n\n\n\n\n0\n2002\nNaN\n0.018\n0.008\n0.000\n-0.169\n0.015\n0.181\n\n\n1\n2003\n1.738\n0.014\n0.004\n0.000\n-0.122\n0.009\n0.131\n\n\n2\n2004\n1.106\n0.025\n0.003\n0.002\n-0.049\n0.006\n0.069\n\n\n3\n2005\n0.614\n0.027\n0.004\n0.003\n-0.032\n0.010\n0.049\n\n\n4\n2006\n0.518\n0.031\n0.005\n0.003\n-0.055\n0.011\n0.076\n\n\n5\n2007\n-0.369\n0.079\n0.013\n0.011\n-0.139\n0.036\n0.184\n\n\n6\n2008\n0.214\n0.081\n0.015\n0.012\n-0.009\n0.041\n0.052\n\n\n7\n2009\n0.123\n0.098\n0.017\n0.016\n-0.044\n0.041\n0.101\n\n\n8\n2010\n0.088\n0.113\n0.016\n0.017\n-0.118\n0.043\n0.187\n\n\n9\n2011\n0.267\n0.108\n0.015\n0.017\n-0.083\n0.018\n0.172\n\n\n10\n2012\n-0.031\n0.107\n0.016\n0.017\n0.033\n0.029\n0.045\n\n\n11\n2013\n0.099\n0.100\n0.016\n0.017\n-0.072\n0.034\n0.138\n\n\n12\n2014\n0.208\n0.080\n0.017\n0.012\n-0.094\n0.043\n0.136\n\n\n13\n2015\n0.163\n0.082\n0.019\n0.011\n-0.051\n0.052\n0.090\n\n\n14\n2016\n0.041\n0.086\n0.024\n0.011\n-0.023\n0.036\n0.085\n\n\n15\n2017\n0.079\n0.108\n0.024\n0.017\n-0.109\n0.026\n0.199\n\n\n16\n2018\n-0.456\n0.176\n0.050\n0.027\n-0.045\n0.106\n0.139\n\n\n17\n2019\n0.194\n0.181\n0.049\n0.027\n-0.019\n0.117\n0.106\n\n\n18\n2020\n0.076\n0.189\n0.050\n0.028\n-0.027\n0.101\n0.137\n\n\n19\n2021\n0.195\n0.191\n0.046\n0.028\n-0.079\n0.082\n0.207\n\n\n20\n2022\n0.234\n0.189\n0.042\n0.027\n-0.085\n0.073\n0.216\n\n\n21\n2023\n0.196\n0.191\n0.043\n0.027\n-0.041\n0.075\n0.173",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#visualizing-historical-ratios",
    "href": "05_discounted_cash_flow.html#visualizing-historical-ratios",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.4 Visualizing Historical Ratios",
    "text": "6.4 Visualizing Historical Ratios\nFigure 6.1 shows the historical evolution of key financial ratios that drive FCF. Understanding these patterns helps inform our forecasts.\n\n# Prepare data for plotting\nratio_columns = [\n    \"operating_margin\", \"depreciation_margin\", \"tax_margin\",\n    \"working_capital_margin\", \"capex_margin\"\n]\n\nratios_long = (historical_ratios\n    .melt(\n        id_vars=[\"year\"],\n        value_vars=ratio_columns,\n        var_name=\"ratio\",\n        value_name=\"value\"\n    )\n    .assign(\n        ratio=lambda x: x[\"ratio\"].str.replace(\"_\", \" \").str.title()\n    )\n)\n\nratios_figure = (\n    ggplot(ratios_long, aes(x=\"year\", y=\"value\", color=\"ratio\"))\n    + geom_line(size=1)\n    + geom_point(size=2)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Ratio (% of Revenue)\", color=\"\",\n        title=\"Key Financial Ratios of FPT Over Time\"\n    )\n    + theme(legend_position=\"right\")\n)\n\nratios_figure.show()\n\n\n\n\n\n\n\nFigure 6.1: Historical financial ratios reveal the operating characteristics of FPT. These patterns inform our forecast assumptions.\n\n\n\n\n\nSeveral patterns emerge from the historical data. Operating margins show the profitability of core operations. Depreciation margins indicate asset intensity. CAPEX margins reveal investment requirements. Working capital margins can be volatile, reflecting changes in credit terms and inventory management.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#forecasting-free-cash-flow",
    "href": "05_discounted_cash_flow.html#forecasting-free-cash-flow",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.5 Forecasting Free Cash Flow",
    "text": "6.5 Forecasting Free Cash Flow\nWith historical patterns established, we now project FCF into the future. This requires forecasting both revenue growth and the ratios that convert revenue into cash flow.\n\n6.5.1 The Ratio-Based Forecasting Approach\nWe use a ratio-based approach that links all FCF components to revenue. This makes forecasting tractable: rather than projecting absolute dollar amounts for each component, we forecast (1) revenue growth and (2) how each component scales with revenue.\nThis approach embeds a key assumption: that the relationship between revenue and FCF components remains stable. In reality, operating leverage, investment needs, and working capital requirements may change as companies mature. Sophisticated valuations model these dynamics explicitly.\n\n\n6.5.2 Setting Forecast Assumptions\nFor our five-year forecast, we make the following assumptions about FPT’s financial ratios. These should reflect industry analysis, company guidance, and competitive dynamics. Here we use estimates for illustration:\n\n# Define the forecast horizon\nlast_historical_year = historical_data[\"year\"].max()\nforecast_years = list(range(last_historical_year + 1, last_historical_year + 6))\nn_forecast_years = len(forecast_years)\n\nprint(f\"Forecast period: {forecast_years[0]} to {forecast_years[-1]}\")\n\n# Define forecast ratios\n# In practice, these would come from detailed analysis\nforecast_assumptions = pd.DataFrame({\n    \"year\": forecast_years,\n    # Operating margin: slight improvement as scale increases\n    \"operating_margin\": [0.12, 0.125, 0.13, 0.13, 0.135],\n    # Depreciation: stable as % of revenue\n    \"depreciation_margin\": [0.03, 0.03, 0.03, 0.028, 0.028],\n    # Tax rate: stable\n    \"tax_margin\": [0.02, 0.02, 0.02, 0.02, 0.02],\n    # Working capital: modest cash consumption\n    \"working_capital_margin\": [0.01, 0.01, 0.008, 0.008, 0.008],\n    # CAPEX: declining as % of revenue as growth moderates\n    \"capex_margin\": [0.05, 0.048, 0.045, 0.042, 0.04]\n})\n\nforecast_assumptions\n\nForecast period: 2024 to 2028\n\n\n\n\n\n\n\n\n\nyear\noperating_margin\ndepreciation_margin\ntax_margin\nworking_capital_margin\ncapex_margin\n\n\n\n\n0\n2024\n0.120\n0.030\n0.02\n0.010\n0.050\n\n\n1\n2025\n0.125\n0.030\n0.02\n0.010\n0.048\n\n\n2\n2026\n0.130\n0.030\n0.02\n0.008\n0.045\n\n\n3\n2027\n0.130\n0.028\n0.02\n0.008\n0.042\n\n\n4\n2028\n0.135\n0.028\n0.02\n0.008\n0.040\n\n\n\n\n\n\n\n\n\n6.5.3 Forecasting Revenue Growth\nRevenue growth is often the most important and most uncertain assumption in DCF analysis. We demonstrate two approaches: using historical averages and linking growth to macroeconomic forecasts.\nApproach 1: Historical Average\nA simple approach uses the historical average growth rate:\n\nhistorical_growth = historical_ratios[\"revenue_growth\"].dropna()\navg_historical_growth = historical_growth.mean()\n\nprint(f\"Average historical revenue growth: {avg_historical_growth:.1%}\")\n\nAverage historical revenue growth: 25.2%\n\n\nApproach 2: GDP-Linked Growth\nA more sophisticated approach links company growth to GDP forecasts from institutions like the IMF. This captures the intuition that company revenues often move with broader economic activity.\n\n# Vietnam GDP growth forecasts (illustrative, based on IMF WEO style projections)\n# In practice, download from IMF WEO database\ngdp_forecasts = pd.DataFrame({\n    \"year\": forecast_years,\n    \"gdp_growth\": [0.065, 0.063, 0.060, 0.058, 0.055]  # Gradually declining to long-term\n})\n\n# Assume FPT grows at a premium to GDP (tech sector outperformance)\n# This premium should reflect company-specific factors\ngrowth_premium = 0.05  # 5 percentage points above GDP\n\nforecast_assumptions = forecast_assumptions.merge(gdp_forecasts, on=\"year\")\nforecast_assumptions[\"revenue_growth\"] = (\n    forecast_assumptions[\"gdp_growth\"] + growth_premium\n)\n\nforecast_assumptions[[\"year\", \"gdp_growth\", \"revenue_growth\"]]\n\n\n\n\n\n\n\n\nyear\ngdp_growth\nrevenue_growth\n\n\n\n\n0\n2024\n0.065\n0.115\n\n\n1\n2025\n0.063\n0.113\n\n\n2\n2026\n0.060\n0.110\n\n\n3\n2027\n0.058\n0.108\n\n\n4\n2028\n0.055\n0.105\n\n\n\n\n\n\n\n\n\n6.5.4 Building the Forecast\nNow we combine our assumptions to project revenue and FCF:\n\n# Get the last historical revenue as our starting point\nlast_revenue = historical_data.loc[\n    historical_data[\"year\"] == last_historical_year, \"revenue\"\n].values[0]\n\nprint(f\"Last historical revenue ({last_historical_year}): {last_revenue/1e12:.2f} trillion VND\")\n\n# Project revenue forward\nforecast_data = forecast_assumptions.copy()\nforecast_data[\"revenue\"] = None\n\n# Calculate revenue for each forecast year\nfor i, row in forecast_data.iterrows():\n    if i == 0:\n        # First forecast year: grow from last historical\n        forecast_data.loc[i, \"revenue\"] = last_revenue * (1 + row[\"revenue_growth\"])\n    else:\n        # Subsequent years: grow from previous forecast\n        prev_revenue = forecast_data.loc[i-1, \"revenue\"]\n        forecast_data.loc[i, \"revenue\"] = prev_revenue * (1 + row[\"revenue_growth\"])\n\n# Convert revenue to numeric\nforecast_data[\"revenue\"] = forecast_data[\"revenue\"].astype(float)\n\n# Calculate FCF components from ratios\nforecast_data[\"ebit\"] = forecast_data[\"operating_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"depreciation\"] = forecast_data[\"depreciation_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"taxes\"] = forecast_data[\"tax_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"delta_working_capital\"] = forecast_data[\"working_capital_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"capex\"] = forecast_data[\"capex_margin\"] * forecast_data[\"revenue\"]\n\n# Calculate FCF\nforecast_data[\"fcf\"] = (\n    forecast_data[\"ebit\"]\n    - forecast_data[\"taxes\"]\n    + forecast_data[\"depreciation\"]\n    - forecast_data[\"delta_working_capital\"]\n    - forecast_data[\"capex\"]\n)\n\nforecast_data[[\"year\", \"revenue\", \"ebit\", \"fcf\"]].round(0)\n\nLast historical revenue (2023): 52.62 trillion VND\n\n\n\n\n\n\n\n\n\nyear\nrevenue\nebit\nfcf\n\n\n\n\n0\n2024\n5.866896e+13\n7.040275e+12\n4.106827e+12\n\n\n1\n2025\n6.529855e+13\n8.162319e+12\n5.027988e+12\n\n\n2\n2026\n7.248139e+13\n9.422581e+12\n6.305881e+12\n\n\n3\n2027\n8.030938e+13\n1.044022e+13\n7.067226e+12\n\n\n4\n2028\n8.874187e+13\n1.198015e+13\n8.430477e+12",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#visualizing-the-forecast",
    "href": "05_discounted_cash_flow.html#visualizing-the-forecast",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.6 Visualizing the Forecast",
    "text": "6.6 Visualizing the Forecast\nFigure 6.2 compares our forecast ratios with historical values, showing the transition from realized to projected performance.\n\n# Prepare historical data for plotting\nhistorical_plot = (historical_ratios\n    .loc[:, [\"year\", \"operating_margin\", \"depreciation_margin\", \n             \"tax_margin\", \"working_capital_margin\", \"capex_margin\"]]\n    .assign(type=\"Historical\")\n)\n\n# Prepare forecast data for plotting\nforecast_plot = (forecast_data\n    .loc[:, [\"year\", \"operating_margin\", \"depreciation_margin\",\n             \"tax_margin\", \"working_capital_margin\", \"capex_margin\"]]\n    .assign(type=\"Forecast\")\n)\n\n# Combine\ncombined_ratios = pd.concat([historical_plot, forecast_plot], ignore_index=True)\n\n# Reshape for plotting\ncombined_long = combined_ratios.melt(\n    id_vars=[\"year\", \"type\"],\n    var_name=\"ratio\",\n    value_name=\"value\"\n)\n\ncombined_long[\"type\"] = pd.Categorical(\n    combined_long[\"type\"], \n    categories=[\"Historical\", \"Forecast\"]\n)\n\nforecast_ratios_figure = (\n    ggplot(combined_long, aes(x=\"year\", y=\"value\", color=\"ratio\", linetype=\"type\"))\n    + geom_line(size=1)\n    + geom_point(size=2)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Ratio (% of Revenue)\", color=\"\", linetype=\"\",\n        title=\"Historical and Forecast Financial Ratios for FPT\"\n    )\n    + theme(legend_position=\"right\")\n)\n\nforecast_ratios_figure.show()\n\n\n\n\n\n\n\nFigure 6.2: Historical ratios (solid lines) and forecast assumptions (dashed lines) for key financial metrics. The forecast period begins after the last historical observation.\n\n\n\n\n\nFigure 6.3 shows the revenue growth trajectory, comparing historical performance with our GDP-linked forecasts.\n\n# Prepare growth data\nhistorical_growth_df = (historical_ratios\n    .loc[:, [\"year\", \"revenue_growth\"]]\n    .dropna()\n    .assign(type=\"Historical\")\n)\n\nforecast_growth_df = (forecast_data\n    .loc[:, [\"year\", \"revenue_growth\", \"gdp_growth\"]]\n    .assign(type=\"Forecast\")\n)\n\n# Combine for revenue growth\ngrowth_combined = pd.concat([\n    historical_growth_df,\n    forecast_growth_df[[\"year\", \"revenue_growth\", \"type\"]]\n], ignore_index=True)\n\ngrowth_combined[\"type\"] = pd.Categorical(\n    growth_combined[\"type\"],\n    categories=[\"Historical\", \"Forecast\"]\n)\n\ngrowth_figure = (\n    ggplot(growth_combined, aes(x=\"year\", y=\"revenue_growth\", linetype=\"type\"))\n    + geom_line(size=1, color=\"steelblue\")\n    + geom_point(size=2, color=\"steelblue\")\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Revenue Growth Rate\", linetype=\"\",\n        title=\"Historical and Forecast Revenue Growth for FPT\"\n    )\n)\n\ngrowth_figure.show()\n\n\n\n\n\n\n\nFigure 6.3: Revenue growth rates: historical (realized) and forecast (GDP-linked with company premium). The forecast assumes FPT grows at a premium to Vietnam’s GDP growth.\n\n\n\n\n\nFigure 6.4 presents the resulting FCF projections alongside historical values.\n\n# Combine historical and forecast FCF\nfcf_historical = (historical_data\n    .loc[:, [\"year\", \"fcf\"]]\n    .assign(type=\"Historical\")\n)\n\nfcf_forecast = (forecast_data\n    .loc[:, [\"year\", \"fcf\"]]\n    .assign(type=\"Forecast\")\n)\n\nfcf_combined = pd.concat([fcf_historical, fcf_forecast], ignore_index=True)\nfcf_combined[\"type\"] = pd.Categorical(\n    fcf_combined[\"type\"],\n    categories=[\"Historical\", \"Forecast\"]\n)\n\nfcf_figure = (\n    ggplot(fcf_combined, aes(x=\"year\", y=\"fcf/1e12\", fill=\"type\"))\n    + geom_col()\n    + labs(\n        x=\"\", y=\"Free Cash Flow (Trillion VND)\", fill=\"\",\n        title=\"Historical and Forecast Free Cash Flow for FPT\"\n    )\n)\n\nfcf_figure.show()\n\n\n\n\n\n\n\nFigure 6.4: Free Cash Flow: historical (realized) and forecast (projected). The forecast reflects our assumptions about revenue growth and operating ratios.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#terminal-value-capturing-long-term-value",
    "href": "05_discounted_cash_flow.html#terminal-value-capturing-long-term-value",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.7 Terminal Value: Capturing Long-Term Value",
    "text": "6.7 Terminal Value: Capturing Long-Term Value\nA critical component of DCF analysis is the terminal value (or continuation value), which represents the company’s value beyond the explicit forecast period. In most valuations, terminal value constitutes 50-80% of total enterprise value, making its estimation particularly important.\n\n6.7.1 The Perpetuity Growth Model\nThe most common approach is the Perpetuity Growth Model (also called the Gordon Growth Model), which assumes FCF grows at a constant rate forever:\n\\[\nTV_T = \\frac{FCF_{T+1}}{r - g} = \\frac{FCF_T \\times (1 + g)}{r - g}\n\\]\nwhere:\n\n\\(TV_T\\): Terminal value at the end of year \\(T\\)\n\\(FCF_T\\): Free cash flow in the final forecast year\n\\(g\\): Perpetual growth rate\n\\(r\\): Discount rate (WACC)\n\n\n\n6.7.2 Choosing the Perpetual Growth Rate\nThe perpetual growth rate \\(g\\) should reflect long-term sustainable growth. Key considerations:\n\nNo company can grow faster than the economy forever. If it did, the company would eventually become larger than GDP, which is an impossibility. Long-term GDP growth (nominal, including inflation) provides an upper bound.\nMature companies typically grow at or below GDP growth. The perpetual growth rate should reflect the company in its “steady state,” not its current high-growth phase.\nFor Vietnam, long-term nominal GDP growth might be 6-8% given current development stage, but this will moderate over time. A perpetual growth rate of 3-5% is often reasonable.\n\n\ndef compute_terminal_value(last_fcf, growth_rate, discount_rate):\n    \"\"\"\n    Compute terminal value using the perpetuity growth model.\n    \n    Parameters:\n    -----------\n    last_fcf : float\n        Free cash flow in the final forecast year\n    growth_rate : float\n        Perpetual growth rate (g)\n    discount_rate : float\n        Discount rate / WACC (r)\n        \n    Returns:\n    --------\n    float : Terminal value\n    \"\"\"\n    if discount_rate &lt;= growth_rate:\n        raise ValueError(\"Discount rate must exceed growth rate for finite terminal value\")\n    \n    return last_fcf * (1 + growth_rate) / (discount_rate - growth_rate)\n\n\n# Example calculation\nlast_fcf = forecast_data[\"fcf\"].iloc[-1]\nperpetual_growth = 0.04  # 4% perpetual growth\ndiscount_rate = 0.10     # 10% WACC (placeholder)\n\nterminal_value = compute_terminal_value(last_fcf, perpetual_growth, discount_rate)\n\nprint(f\"Last forecast FCF: {last_fcf/1e12:.2f} trillion VND\")\nprint(f\"Terminal value (at {perpetual_growth:.0%} growth, {discount_rate:.0%} WACC): {terminal_value/1e12:.1f} trillion VND\")\n\nLast forecast FCF: 8.43 trillion VND\nTerminal value (at 4% growth, 10% WACC): 146.1 trillion VND\n\n\n\n\n6.7.3 Alternative: Exit Multiple Approach\nPractitioners often cross-check terminal value using the exit multiple approach, which assumes the company is sold at the end of the forecast period at a multiple of EBITDA, EBIT, or revenue comparable to similar companies today.\nFor example, if comparable companies trade at 10x EBITDA, the terminal value would be:\n\\[\nTV_T = \\text{EBITDA}_T \\times \\text{Exit Multiple}\n\\]\nThis approach is simpler but embeds the assumption that current market multiples will persist (a strong assumption that may not hold).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#the-discount-rate-weighted-average-cost-of-capital",
    "href": "05_discounted_cash_flow.html#the-discount-rate-weighted-average-cost-of-capital",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.8 The Discount Rate: Weighted Average Cost of Capital",
    "text": "6.8 The Discount Rate: Weighted Average Cost of Capital\nThe discount rate converts future cash flows to present value. For FCF (which goes to all capital providers), we use the Weighted Average Cost of Capital (WACC):\n\\[\nWACC = \\frac{E}{E+D} \\times r_E + \\frac{D}{E+D} \\times r_D \\times (1 - \\tau)\n\\]\nwhere:\n\n\\(E\\): Market value of equity\n\\(D\\): Market value of debt\n\\(r_E\\): Cost of equity (typically estimated using CAPM)\n\\(r_D\\): Cost of debt (pre-tax)\n\\(\\tau\\): Corporate tax rate\n\nThe \\((1-\\tau)\\) term on debt reflects the tax shield. Interest payments are tax-deductible, reducing the effective cost of debt.\n\n6.8.1 Estimating WACC Components\nCost of Equity is typically estimated using the Capital Asset Pricing Model (see our CAPM chapter):\n\\[\nr_E = r_f + \\beta \\times (r_m - r_f)\n\\]\nwhere \\(r_f\\) is the risk-free rate, \\(\\beta\\) measures systematic risk, and \\((r_m - r_f)\\) is the market risk premium.\nCost of Debt can be estimated from:\n\nInterest expense divided by total debt (effective rate)\nYields on the company’s traded bonds\nYields on bonds with similar credit ratings\n\nCapital Structure Weights should use market values when available. For equity, market capitalization is straightforward. For debt, book value is often used when market values aren’t observable.\n\n\n6.8.2 Using Industry WACC Data\nProfessor Aswath Damodaran at NYU Stern maintains comprehensive industry WACC data. Let’s download and use this resource:\n\nimport requests\nimport os\n\n# Download Damodaran's WACC data\nurl = \"https://pages.stern.nyu.edu/~adamodar/pc/datasets/wacc.xls\"\n\ntry:\n    response = requests.get(url, timeout=10)\n    response.raise_for_status()\n    \n    with open(\"wacc.xls\", \"wb\") as f:\n        f.write(response.content)\n    \n    # Read the data (skip header rows)\n    wacc_data = pd.read_excel(\"wacc.xls\", sheet_name=1, skiprows=18)\n    \n    # Clean up\n    os.remove(\"wacc.xls\")\n    \n    # Find WACC for Computer Services (closest to FPT's business)\n    industry_wacc = wacc_data.loc[\n        wacc_data[\"Industry Name\"] == \"Computer Services\",\n        \"Cost of Capital\"\n    ].values[0]\n    \n    print(f\"Industry WACC (Computer Services): {industry_wacc:.2%}\")\n    \nexcept Exception as e:\n    print(f\"Could not download WACC data: {e}\")\n    # Use a reasonable estimate\n    industry_wacc = 0.10\n    print(f\"Using estimated WACC: {industry_wacc:.2%}\")\n\nwacc = industry_wacc\n\nCould not download WACC data: `Import xlrd` failed. Install xlrd &gt;= 2.0.1 for xls Excel support Use pip or conda to install the xlrd package.\nUsing estimated WACC: 10.00%\n\n\nNote: Industry WACC provides a useful benchmark, but company-specific factors (leverage, business risk, country risk) may warrant adjustments. For Vietnamese companies, adding a country risk premium may be appropriate.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#computing-enterprise-value",
    "href": "05_discounted_cash_flow.html#computing-enterprise-value",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.9 Computing Enterprise Value",
    "text": "6.9 Computing Enterprise Value\nWith all components in place, we can now compute enterprise value. The DCF formula is:\n\\[\n\\text{Enterprise Value} = \\sum_{t=1}^{T} \\frac{FCF_t}{(1 + WACC)^t} + \\frac{TV_T}{(1 + WACC)^T}\n\\]\nThe first term is the present value of forecast-period cash flows; the second is the present value of terminal value.\n\ndef compute_dcf_value(fcf_series, wacc, perpetual_growth):\n    \"\"\"\n    Compute enterprise value using DCF analysis.\n    \n    Parameters:\n    -----------\n    fcf_series : array-like\n        Free cash flows for forecast period\n    wacc : float\n        Weighted average cost of capital\n    perpetual_growth : float\n        Perpetual growth rate for terminal value\n        \n    Returns:\n    --------\n    dict : Components of DCF valuation\n    \"\"\"\n    fcf = np.array(fcf_series)\n    n_years = len(fcf)\n    \n    # Discount factors\n    discount_factors = (1 + wacc) ** np.arange(1, n_years + 1)\n    \n    # Present value of forecast period cash flows\n    pv_fcf = fcf / discount_factors\n    pv_fcf_total = pv_fcf.sum()\n    \n    # Terminal value and its present value\n    terminal_value = compute_terminal_value(fcf[-1], perpetual_growth, wacc)\n    pv_terminal = terminal_value / discount_factors[-1]\n    \n    # Total enterprise value\n    enterprise_value = pv_fcf_total + pv_terminal\n    \n    return {\n        \"pv_fcf\": pv_fcf_total,\n        \"terminal_value\": terminal_value,\n        \"pv_terminal\": pv_terminal,\n        \"enterprise_value\": enterprise_value,\n        \"terminal_pct\": pv_terminal / enterprise_value\n    }\n\n\n# Compute DCF value\nperpetual_growth = 0.04  # 4% perpetual growth\n\ndcf_result = compute_dcf_value(\n    fcf_series=forecast_data[\"fcf\"].values,\n    wacc=wacc,\n    perpetual_growth=perpetual_growth\n)\n\nprint(\"DCF Valuation Results\")\nprint(\"=\" * 50)\nprint(f\"PV of Forecast Period FCF: {dcf_result['pv_fcf']/1e12:.1f} trillion VND\")\nprint(f\"Terminal Value: {dcf_result['terminal_value']/1e12:.1f} trillion VND\")\nprint(f\"PV of Terminal Value: {dcf_result['pv_terminal']/1e12:.1f} trillion VND\")\nprint(f\"Enterprise Value: {dcf_result['enterprise_value']/1e12:.1f} trillion VND\")\nprint(f\"Terminal Value as % of EV: {dcf_result['terminal_pct']:.1%}\")\n\nDCF Valuation Results\n==================================================\nPV of Forecast Period FCF: 22.7 trillion VND\nTerminal Value: 146.1 trillion VND\nPV of Terminal Value: 90.7 trillion VND\nEnterprise Value: 113.4 trillion VND\nTerminal Value as % of EV: 80.0%\n\n\nNote that terminal value often represents 60-80% of enterprise value. This highlights the importance of terminal value assumptions and the inherent uncertainty in DCF analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#sensitivity-analysis",
    "href": "05_discounted_cash_flow.html#sensitivity-analysis",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.10 Sensitivity Analysis",
    "text": "6.10 Sensitivity Analysis\nGiven the uncertainty in DCF inputs, sensitivity analysis is essential. We examine how enterprise value changes with different assumptions about WACC and perpetual growth.\n\n# Define ranges for sensitivity analysis\nwacc_range = np.arange(0.08, 0.14, 0.01)  # 8% to 13%\ngrowth_range = np.arange(0.02, 0.06, 0.01)  # 2% to 5%\n\n# Create all combinations\nsensitivity_results = []\n\nfor w in wacc_range:\n    for g in growth_range:\n        if w &gt; g:  # Must have WACC &gt; growth for valid terminal value\n            result = compute_dcf_value(\n                fcf_series=forecast_data[\"fcf\"].values,\n                wacc=w,\n                perpetual_growth=g\n            )\n            sensitivity_results.append({\n                \"wacc\": w,\n                \"growth_rate\": g,\n                \"enterprise_value\": result[\"enterprise_value\"] / 1e12  # In trillions\n            })\n\nsensitivity_df = pd.DataFrame(sensitivity_results)\n\n# Create heatmap\nsensitivity_figure = (\n    ggplot(sensitivity_df, aes(x=\"wacc\", y=\"growth_rate\", fill=\"enterprise_value\"))\n    + geom_tile()\n    + geom_text(\n        aes(label=\"enterprise_value\"),\n        format_string=\"{:.0f}\",\n        color=\"white\",\n        size=9\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + scale_fill_gradient(low=\"darkblue\", high=\"lightblue\")\n    + labs(\n        x=\"WACC\", y=\"Perpetual Growth Rate\",\n        fill=\"EV\\n(Trillion VND)\",\n        title=\"DCF Sensitivity: Enterprise Value by WACC and Growth Rate\"\n    )\n)\n\nsensitivity_figure.show()\n\n\n\n\n\n\n\nFigure 6.5: Sensitivity of enterprise value to WACC and perpetual growth rate assumptions. Small changes in these inputs can substantially affect valuation.\n\n\n\n\n\nThe sensitivity analysis reveals several important insights:\n\nValuation is highly sensitive to inputs: Small changes in WACC or growth rate produce large changes in enterprise value. A 1 percentage point change in WACC can shift value by 20% or more.\nThe relationship is non-linear: The impact of growth rate changes is amplified at lower WACCs because the terminal value formula has \\((r-g)\\) in the denominator.\nReasonable people can disagree: Given input uncertainty, DCF should be thought of as producing a range of values, not a single precise number.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#from-enterprise-value-to-equity-value",
    "href": "05_discounted_cash_flow.html#from-enterprise-value-to-equity-value",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.11 From Enterprise Value to Equity Value",
    "text": "6.11 From Enterprise Value to Equity Value\nOur DCF analysis yields enterprise value (i.e., the total value of the company’s operations to all capital providers). To determine equity value (what shareholders own), we must adjust for the claims of debt holders and any non-operating assets:\n\\[\n\\text{Equity Value} = \\text{Enterprise Value} + \\text{Non-Operating Assets} - \\text{Debt}\n\\]\nNon-Operating Assets include:\n\nExcess cash beyond operating needs\nMarketable securities\nNon-core real estate or investments\n\nDebt includes:\n\nShort-term debt\nLong-term debt\nCapital lease obligations\nPreferred stock (if treated as debt-like)\n\n\n# Get most recent balance sheet data for FPT\nlatest_year = fpt_data[\"year\"].max()\nlatest_data = fpt_data[fpt_data[\"year\"] == latest_year].iloc[0]\n\n# Extract debt and cash (column names may vary)\ntotal_debt = latest_data.get(\"total_debt\", 0)\ncash = latest_data.get(\"ca_cce\", 0)\n\n# Compute equity value\nenterprise_value = dcf_result[\"enterprise_value\"]\nequity_value = enterprise_value - total_debt + cash\n\nprint(\"From Enterprise Value to Equity Value\")\nprint(\"=\" * 50)\nprint(f\"Enterprise Value: {enterprise_value/1e12:.1f} trillion VND\")\nprint(f\"Less: Total Debt: {total_debt/1e12:.1f} trillion VND\")\nprint(f\"Plus: Cash: {cash/1e12:.1f} trillion VND\")\nprint(f\"Equity Value: {equity_value/1e12:.1f} trillion VND\")\n\nFrom Enterprise Value to Equity Value\n==================================================\nEnterprise Value: 113.4 trillion VND\nLess: Total Debt: 0.0 trillion VND\nPlus: Cash: 8.3 trillion VND\nEquity Value: 121.7 trillion VND\n\n\n\n6.11.1 Implied Share Price\nIf we know the number of shares outstanding, we can compute an implied share price:\n\n# Get shares outstanding (this would come from market data)\n# Using placeholder - in practice, get from exchange data\nshares_outstanding = latest_data.get(\"total_equity\", equity_value) / 25000  # Rough estimate\n\nimplied_price = equity_value / shares_outstanding\n\nprint(f\"\\nImplied Share Price: {implied_price:,.0f} VND\")\n\n\nImplied Share Price: 101,645 VND\n\n\nComparing the implied price to the current market price tells us whether the stock appears under- or overvalued according to our DCF model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#limitations-and-practical-considerations",
    "href": "05_discounted_cash_flow.html#limitations-and-practical-considerations",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.12 Limitations and Practical Considerations",
    "text": "6.12 Limitations and Practical Considerations\nDCF analysis is powerful but has important limitations:\n\n6.12.1 Sensitivity to Assumptions\nAs our sensitivity analysis showed, small changes in inputs produce large changes in value. This is particularly problematic because the most influential inputs (long-term growth, WACC) are the hardest to estimate accurately.\n\n\n6.12.2 Terminal Value Dominance\nTerminal value often represents 60-80% of total value, yet it’s based on assumptions about the very distant future. This concentrates valuation risk in the most uncertain component.\n\n\n6.12.3 Garbage In, Garbage Out\nDCF is only as good as its inputs. Unrealistic growth assumptions, optimistic margins, or inappropriate discount rates produce meaningless valuations. The discipline of DCF lies in forcing analysts to justify their assumptions.\n\n\n6.12.4 Not Suitable for All Companies\nDCF works best for companies with:\n\nPositive and predictable cash flows\nStable or predictably changing margins\nReasonable visibility into future operations\n\nIt struggles with:\n\nEarly-stage companies with no profits\nHighly cyclical businesses\nCompanies undergoing major transitions\nFinancial institutions (which require different approaches)\n\n\n\n6.12.5 Complement with Other Methods\nWise practitioners use DCF alongside other valuation methods:\n\nComparable company analysis: How do similar companies trade?\nPrecedent transactions: What have acquirers paid for similar businesses?\nSum-of-the-parts: Value divisions separately and add\n\nWhen methods converge, confidence increases. When they diverge, it prompts investigation into why.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#key-takeaways",
    "href": "05_discounted_cash_flow.html#key-takeaways",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.13 Key Takeaways",
    "text": "6.13 Key Takeaways\nThis chapter introduced Discounted Cash Flow analysis as a framework for intrinsic valuation. The main insights are:\n\nFree Cash Flow is the foundation: FCF represents cash available to all investors after operating expenses, taxes, and investments. It differs from net income by excluding non-cash items and including investment needs.\nRatio-based forecasting links components to revenue: By expressing FCF components as percentages of revenue, we can systematically forecast cash flows based on revenue growth assumptions and operating ratio projections.\nTerminal value captures long-term value: The perpetuity growth model assumes FCF grows at a constant rate forever. The perpetual growth rate should not exceed long-term economic growth.\nWACC is the appropriate discount rate: The Weighted Average Cost of Capital reflects the blended cost of debt and equity financing, adjusted for the tax shield on interest.\nDCF produces enterprise value: To derive equity value, subtract debt and add non-operating assets. Dividing by shares outstanding yields an implied share price.\nSensitivity analysis is essential: Given input uncertainty, presenting a range of values based on different assumptions is more honest than a single point estimate.\nDCF complements other methods: No single valuation method is definitive. Cross-checking DCF with market multiples and transaction comparables provides a more complete picture.\n\nThe true value of DCF analysis lies not in producing a precise number but in forcing rigorous thinking about what drives company value. The process of building a DCF model (i.e., forecasting growth, projecting margins, estimating risk) develops deep understanding of the business being valued.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html",
    "href": "06_accessing_and_managing_financial_data.html",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "",
    "text": "7.1 Overview of Vietnamese Financial Data Sources\nThis chapter provides a guide to organizing, accessing, and managing financial data specifically tailored for the Vietnamese market. While global financial databases such as CRSP and Compustat serve as standard resources for developed markets, emerging markets like Vietnam require a different approach due to unique data sources, market structures, and regulatory environments. Understanding these nuances is essential for conducting rigorous empirical research on Vietnamese equities, bonds, and macroeconomic indicators.\nVietnam’s financial market has experienced remarkable growth since the establishment of the Ho Chi Minh City Stock Exchange (HOSE) in 2000 and the Hanoi Stock Exchange (HNX) in 2005. Today, the market comprises over 1,600 listed companies across three trading venues: HOSE for large-cap stocks, HNX for mid-cap stocks, and UPCoM (Unlisted Public Company Market) for smaller companies transitioning to formal listing. This diversity creates both opportunities and challenges for financial researchers seeking comprehensive coverage of the Vietnamese equity universe.\nThe Vietnamese market presents several distinctive characteristics that researchers must account for. Foreign ownership limits (typically 49% for most sectors, with exceptions for banking and certain strategic industries), trading band restrictions (e.g., currently \\(\\pm\\) 7% for HOSE and \\(\\pm\\) 10% for HNX), and the T+2 settlement cycle all influence market microstructure and return dynamics. Additionally, the market operates in Vietnamese Dong (VND), requiring careful attention to currency effects when comparing results with international studies.\nWe begin by loading the essential Python packages that facilitate data acquisition and management throughout this chapter.\nWe also define the date range for our data collection, which spans from the early days of the Vietnamese stock market to the present. This extended timeframe allows us to capture the market’s evolution through various economic cycles, including the 2008 global financial crisis, the 2011-2012 domestic banking crisis, and the COVID-19 pandemic period.\nBefore diving into the technical implementation, it is valuable to understand the landscape of financial data providers serving the Vietnamese market. Unlike developed markets where a few dominant providers (Bloomberg, Refinitiv, FactSet) offer comprehensive coverage, Vietnamese financial data has historically been fragmented across multiple sources, each with distinct strengths and limitations.\nThe primary sources of Vietnamese financial data include official exchange feeds from HOSE and HNX, which provide real-time and historical trading data. The State Securities Commission of Vietnam (SSC) publishes regulatory filings, corporate announcements, and market statistics. Commercial data vendors such as FiinGroup, StoxPlus (now part of FiinGroup), and VNDirect offer curated datasets with varying levels of coverage and data quality. Additionally, the State Bank of Vietnam (SBV) and the General Statistics Office (GSO) provide macroeconomic indicators essential for asset pricing research.\nFor academic researchers, this fragmentation traditionally involved difficult trade-offs between cost, coverage, data quality, and ease of access. Commercial providers like FiinGroup offer clean, standardized data but require subscription fees that may be prohibitive for individual researchers and smaller institutions. Open-source alternatives provide free access but often require substantial data cleaning and validation efforts. Manually collecting data from government websites is time-consuming and prone to inconsistencies.\nFortunately, this landscape has improved significantly with the emergence of Datacore as a unified data platform for Vietnamese financial markets. In our experience working with Vietnamese financial data across multiple research projects, Datacore has proven to be the most practical solution for academic research. The platform consolidates data from multiple sources, including stock prices, corporate fundamentals, market indices, macroeconomic indicators, and alternative data, into a single, accessible interface with a well-documented API.\nWhat distinguishes Datacore from traditional commercial providers like FiinGroup extends beyond mere data aggregation. While FiinGroup has long been the institutional incumbent, several factors make Datacore particularly attractive for rigorous empirical research:\nThroughout this chapter, we leverage Datacore as our primary data source. By centralizing our data acquisition through a single platform, we benefit from consistent data formats, reliable corporate action adjustments, and comprehensive market coverage spanning HOSE, HNX, and UPCoM. The code examples that follow demonstrate how straightforward Vietnamese financial research becomes when data access friction is minimized.\nThe following table summarizes the key data sources for Vietnamese financial research:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#overview-of-vietnamese-financial-data-sources",
    "href": "06_accessing_and_managing_financial_data.html#overview-of-vietnamese-financial-data-sources",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "",
    "text": "API-First Architecture: Datacore was built from the ground up for programmatic access, making it seamlessly integrable with Python, R, and other research workflows. FiinGroup’s data access, by contrast, often requires manual downloads or cumbersome Excel-based interfaces that impede reproducibility.\nCost Efficiency: Academic researchers frequently operate under budget constraints. Datacore offers competitive pricing structures that make comprehensive market coverage accessible without the substantial subscription fees associated with legacy providers.\nCorporate Action Handling: One persistent challenge with Vietnamese data is accurate adjustment for stock splits, bonus shares, and rights issues. Datacore implements transparent adjustment methodologies with clear documentation, whereas legacy providers often apply adjustments inconsistently or without adequate explanation.\nUpdate Frequency: Datacore maintains near real-time data updates with clear timestamps, enabling event study research and timely portfolio rebalancing. Traditional providers often suffer from publication lags that can compromise research requiring current data.\nCoverage Breadth: Beyond standard price and fundamental data, Datacore integrates alternative data, and macroeconomic indicators into a unified schema. This eliminates the need to merge datasets from multiple sources, which is a process that introduces potential errors and consumes valuable research time.\n\n\n\n\n\n\nTable 7.1: Vietnamese Financial Data Sources\n\n\n\n\n\n\n\n\n\n\n\n\nData Source\nCoverage\nAccess Type\nKey Strengths\nLimitations\n\n\n\n\nDatacore\nPrices, fundamentals, indices, macro, derivatives\nAPI\nUnified platform, programmatic access, comprehensive coverage, transparent methodology\nNewer platform\n\n\nFiinGroup\nFull market coverage\nCommercial\nEstablished reputation, institutional adoption\nHigh cost, manual access, limited API\n\n\nHOSE/HNX websites\nOfficial exchange data\nFree (manual)\nAuthoritative, real-time\nNo API, manual collection required\n\n\nGSO (gso.gov.vn)\nMacroeconomic indicators\nFree (manual)\nOfficial government statistics\nInfrequent updates, no API\n\n\nSBV (sbv.gov.vn)\nMonetary policy, rates\nFree (manual)\nCentral bank data\nManual download only\n\n\nCafeF/VnExpress\nNews, announcements\nFree\nMarket sentiment, events\nUnstructured, requires NLP processing",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#stock-market-data",
    "href": "06_accessing_and_managing_financial_data.html#stock-market-data",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.2 Stock Market Data",
    "text": "7.2 Stock Market Data\nThe resulting DataFrame contains essential security identifiers including the ticker symbol, company name in both Vietnamese and English, exchange listing, industry classification according to the Vietnam Standard Industrial Classification (VSIC), and various flags indicating special status such as foreign ownership restrictions or trading suspensions.\n\n7.2.1 Historical Price Data\n\n\n7.2.2 Fundamental Data and Financial Statements\nBeyond price data, fundamental analysis requires access to corporate financial statements including balance sheets, income statements, and cash flow statements. Vietnamese publicly listed companies are required to publish quarterly and annual financial reports according to Vietnamese Accounting Standards (VAS), which differ in certain respects from International Financial Reporting Standards (IFRS). Understanding these differences is important when comparing Vietnamese firms with international peers or applying models developed using US or European data.\nKey differences between VAS and IFRS that affect financial analysis include:\n\nRevenue recognition: VAS allows more flexibility in timing of revenue recognition compared to IFRS 15\nFinancial instruments: VAS has less comprehensive guidance on fair value measurement\nLease accounting: VAS does not require operating lease capitalization as under IFRS 16\nGoodwill: VAS requires amortization while IFRS requires impairment testing only\n\n\n\n7.2.3 Corporate Actions and Events\nAccurate treatment of corporate actions is essential for computing correct returns and maintaining data integrity. Vietnamese companies frequently engage in corporate actions including cash dividends, stock dividends (bonus shares), rights issues, and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#market-indices-and-benchmarks",
    "href": "06_accessing_and_managing_financial_data.html#market-indices-and-benchmarks",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.3 Market Indices and Benchmarks",
    "text": "7.3 Market Indices and Benchmarks\nConstructing appropriate benchmarks is fundamental to performance evaluation and factor model estimation. The Vietnamese market features several indices that serve different purposes in financial research.\n\n\n\nTable 7.2: Vietnamese Market Indices\n\n\n\n\n\n\n\n\n\n\n\nIndex\nExchange\nDescription\nUse Case\n\n\n\n\nVN-Index\nHOSE\nAll HOSE-listed stocks\nBroad market benchmark\n\n\nVN30-Index\nHOSE\n30 largest, most liquid\nInvestable benchmark\n\n\nHNX-Index\nHNX\nAll HNX-listed stocks\nMid-cap benchmark\n\n\nHNX30-Index\nHNX\n30 largest HNX stocks\nHNX large-cap\n\n\nVNAllShare\nCombined\nHOSE + HNX\nTotal market\n\n\nVN100\nCombined\nTop 100 stocks\nLarge/mid-cap\n\n\n\n\n\n\nThe VN-Index, which tracks all stocks listed on HOSE, is the most widely followed benchmark and serves as the primary gauge of overall market performance. The HNX-Index covers stocks on the Hanoi exchange, while the VN30-Index tracks the thirty largest and most liquid stocks on HOSE.\nFor asset pricing research, the VN30-Index is particularly valuable as it represents the investable universe for institutional investors and serves as the underlying for Vietnam’s most liquid derivatives contracts. The constituent stocks are reviewed semi-annually based on market capitalization, liquidity, and free-float requirements.\n\n# Retrieve VN-Index historical data\n\n\n7.3.1 Index Constituent Data\nFor factor model construction and portfolio analysis, access to index constituent lists and their weights is essential. While official constituent data requires subscription to exchange data feeds, we can approximate index membership using market capitalization and liquidity filters.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#macroeconomic-data-from-vietnamese-sources",
    "href": "06_accessing_and_managing_financial_data.html#macroeconomic-data-from-vietnamese-sources",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.4 Macroeconomic Data from Vietnamese Sources",
    "text": "7.4 Macroeconomic Data from Vietnamese Sources\nAsset pricing models often incorporate macroeconomic variables as predictors of expected returns or as state variables in conditional models. For the Vietnamese market, relevant macroeconomic data comes primarily from two sources: the General Statistics Office (GSO) and the State Bank of Vietnam (SBV).\n\n7.4.1 Key Macroeconomic Indicators\nThe following macroeconomic variables are particularly relevant for Vietnamese financial research:\n\nConsumer Price Index (CPI): Essential for computing real returns and inflation-adjusted valuations. Vietnam experienced periods of high inflation, particularly during 2008 and 2011 when annual CPI exceeded 20%.\nIndustrial Production Index (IPI): Proxy for economic activity and business cycle conditions.\nMoney Supply (M2): Indicator of monetary policy stance and liquidity conditions.\nCredit Growth: Bank lending growth, a key driver of economic activity in Vietnam’s bank-dominated financial system.\nUSD/VND Exchange Rate: Critical for international investors and companies with foreign currency exposure.\nForeign Direct Investment (FDI): Indicator of international capital flows and economic confidence.\nTrade Balance: Export and import dynamics affecting corporate earnings.\n\nUnfortunately, unlike the US Federal Reserve’s FRED database, Vietnamese macroeconomic data is not available through standardized APIs. Researchers must typically download data manually from GSO and SBV websites or use web scraping techniques.\n\n# Structure for Vietnamese macroeconomic data\n\n\n\n7.4.2 Risk-Free Rate Approximation\nDetermining an appropriate risk-free rate for Vietnam presents challenges not encountered in developed markets. Unlike the US Treasury market, Vietnam’s government bond market is relatively illiquid with limited secondary trading. Several alternatives exist:\n\nSBV Refinancing Rate: The policy rate set by the State Bank of Vietnam. Not directly investable but reflects monetary policy stance.\nGovernment Bond Yields: One-year or longer-term government bond yields from auction results. More investable but less liquid than US Treasuries.\nInterbank Rates: Overnight or term interbank lending rates. Reflect short-term funding costs but include credit risk.\nAdjusted US Rate: US Treasury rate plus expected VND depreciation, following uncovered interest rate parity.\n\n\ndef calculate_risk_free_rate(macro_data, method=\"refinancing\"):\n    \"\"\"\n    Calculate risk-free rate proxy for Vietnamese market.\n    \n    Parameters\n    ----------\n    macro_data : pd.DataFrame\n        DataFrame with macroeconomic data\n    method : str\n        Method for risk-free rate: 'refinancing', 'bond', or 'adjusted_us'\n    \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with date and monthly risk-free rate\n    \"\"\"\n    if method == \"refinancing\":\n        # Use SBV refinancing rate, convert annual to monthly\n        rf = macro_data[[\"date\", \"refinancing_rate\"]].copy()\n        rf[\"rf_monthly\"] = rf[\"refinancing_rate\"] / 12 / 100\n        \n    elif method == \"adjusted_us\":\n        # US rate + expected VND depreciation\n        # Requires additional data on US rates and exchange rate expectations\n        pass\n    \n    return rf[[\"date\", \"rf_monthly\"]]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#setting-up-a-database-for-vietnamese-financial-data",
    "href": "06_accessing_and_managing_financial_data.html#setting-up-a-database-for-vietnamese-financial-data",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.5 Setting Up a Database for Vietnamese Financial Data",
    "text": "7.5 Setting Up a Database for Vietnamese Financial Data\nManaging financial data across multiple sources and formats requires a systematic approach to data storage. We recommend using SQLite as the primary database engine for several reasons: it requires no server setup, stores the entire database in a single portable file, supports standard SQL queries, and integrates seamlessly with Python through the built-in sqlite3 module.\n\n7.5.1 Database Schema Design\nOur database schema is designed to support efficient queries for common research tasks while maintaining data integrity. We create separate tables for different data types with appropriate relationships.\n\nimport os\nimport sqlite3\n\n# Create data directory if it doesn't exist\nif not os.path.exists(\"data\"):\n    os.makedirs(\"data\")\n\n# Initialize SQLite database connection\ntidy_finance_vn = sqlite3.connect(\n    \"data/tidy_finance_vn.sqlite\"\n)\n\n\n\n7.5.2 Storing Data\nWith the database schema established, we can store our collected data using pandas’ to_sql() method.\n\n# Store stock listing data\ncommon_stocks.to_sql(\n    name=\"stock_master\",\n    con=tidy_finance_vn,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store stock price data\nstock_prices.to_sql(\n    name=\"stock_prices_daily\",\n    con=tidy_finance_vn,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store market indices\nvn_index.to_sql(\n    name=\"market_indices\",\n    con=tidy_finance_vn,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store factor returns\nfactors_vietnam.to_sql(\n    name=\"factors_monthly\",\n    con=tidy_finance_vn,\n    if_exists=\"replace\",\n    index=False\n)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#querying-and-updating-the-database",
    "href": "06_accessing_and_managing_financial_data.html#querying-and-updating-the-database",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.6 Querying and Updating the Database",
    "text": "7.6 Querying and Updating the Database\nOnce data is stored in the database, retrieval is straightforward using SQL queries. The pandas read_sql_query() function executes a SQL statement and returns the results as a DataFrame.\n\n# Query stock prices for specific symbols and date range\nquery = \"\"\"\nSELECT date, symbol, close, volume\nFROM stock_prices_daily\nWHERE symbol IN ('VNM', 'VIC', 'FPT', 'VHM', 'VCB')\n  AND date &gt;= '2020-01-01'\nORDER BY symbol, date\n\"\"\"\n\nselected_stocks = pd.read_sql_query(\n    sql=query,\n    con=tidy_finance_vn,\n    parse_dates=[\"date\"]\n)\n\n# Query factor data merged with market returns\nquery_factors = \"\"\"\nSELECT f.date, f.mkt_rf, f.smb, f.hml, f.rf,\n       m.cpi_yoy, m.credit_growth\nFROM factors_monthly f\nLEFT JOIN macro_monthly m ON f.date = m.date\nWHERE f.date &gt;= '2015-01-01'\nORDER BY f.date\n\"\"\"\n\nfactor_data = pd.read_sql_query(\n    sql=query_factors,\n    con=tidy_finance_vn,\n    parse_dates=[\"date\"]\n)\n\n\n7.6.1 Database Maintenance\nRegular database maintenance ensures optimal performance and data integrity.\n\n# Optimize database\ntidy_finance_vn.execute(\"VACUUM\")\n\n# Check database integrity\nintegrity_check = pd.read_sql_query(\n    \"PRAGMA integrity_check\",\n    tidy_finance_vn\n)\nprint(f\"Integrity check: {integrity_check.iloc[0, 0]}\")\n\n# Get database statistics\ntable_stats = pd.read_sql_query(\"\"\"\n    SELECT name, \n           (SELECT COUNT(*) FROM stock_prices_daily) as price_rows,\n           (SELECT COUNT(*) FROM stock_master) as stock_count,\n           (SELECT COUNT(*) FROM factors_monthly) as factor_months\n    FROM sqlite_master\n    WHERE type='table' AND name='stock_master'\n\"\"\", tidy_finance_vn)\n\nprint(table_stats)\n\n# Close connection when done\ntidy_finance_vn.close()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#alternative-data-sources-for-vietnamese-markets",
    "href": "06_accessing_and_managing_financial_data.html#alternative-data-sources-for-vietnamese-markets",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.7 Alternative Data Sources for Vietnamese Markets",
    "text": "7.7 Alternative Data Sources for Vietnamese Markets\nBeyond traditional price and fundamental data, researchers increasingly incorporate alternative data sources to gain unique insights into market dynamics.\n\n7.7.1 Foreign Investor Flow Data\nForeign investor flow data is particularly valuable given the significant role of foreign capital in Vietnamese equity markets. The State Securities Commission publishes daily foreign ownership statistics by security.\n\n\n7.7.2 News and Sentiment Data\nMedia sentiment from Vietnamese financial news sources offers another research avenue. Major outlets such as CafeF, VnExpress Finance, and Vietstock publish real-time news that can be analyzed for market sentiment.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#key-takeaways",
    "href": "06_accessing_and_managing_financial_data.html#key-takeaways",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.8 Key Takeaways",
    "text": "7.8 Key Takeaways\n\nMarket Structure Understanding: The Vietnamese financial market operates across three exchanges (HOSE, HNX, UPCoM) with distinct characteristics including foreign ownership limits, trading band restrictions, and a T+2 settlement cycle. Researchers must account for these institutional features in empirical analysis.\nMacroeconomic Data Challenges: Unlike developed markets with standardized APIs (e.g., FRED), Vietnamese macroeconomic data requires manual collection from government sources (GSO, SBV). Researchers should plan for this additional data gathering effort and implement systematic data management practices.\nDatabase-Centric Workflow: SQLite provides an efficient and portable database solution for managing Vietnamese financial data across research projects. The structured database approach enables reproducible research workflows, efficient queries, and easy data sharing among collaborators.\nData Quality Imperative: Data quality validation is especially important for emerging market data. Implementing systematic checks for missing values, extreme returns, duplicate entries, and cross-source validation helps ensure research reliability and reproducibility.\nAlternative Data Opportunities: Foreign investor flows, corporate announcements, and media sentiment provide unique research opportunities in the Vietnamese market that can complement traditional price and fundamental analysis. These data sources can reveal insights about market dynamics not captured in standard datasets.\nContinuous Maintenance: Financial databases require ongoing maintenance including incremental updates, integrity checks, and optimization. Establishing systematic update procedures ensures data currency and database performance over time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html",
    "href": "07_datacore_data.html",
    "title": "8  DataCore Data",
    "section": "",
    "text": "8.1 Accessing DataCore\nThis chapter shows how to connect to DataCore a provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock and firm characteristics (i.e., Stock Data and Public Companies). Unfortunately, this data is not freely available, but most students and researchers typically have access to DataCore through their university libraries. Assuming that you have access to DataCore, we show you how to prepare and merge the databases and store them in the SQLite database introduced in the previous chapter. We conclude this chapter by providing some tips for working with the DataCore database.\nIf you don’t have access to DataCore but still want to run the code in this book, we refer to DataCore Demo Data, where we show how to create a dummy database that contains the DataCore tables and corresponding columns. With this database at hand, all code chunks in this book can be executed with this dummy database.\nFirst, we load the Python packages that we use throughout this chapter. Later on, we load more packages in the sections where we need them. The last two packages are used for plotting.\nWe use the same date range as in the previous chapter to ensure consistency. However, we have to use the date format that the DataCore database expects.\nDataCore is the most widely used source for asset and firm-specific financial data used in academic settings. DataCore is a data platform that provides data validation, flexible delivery options, and access to many different data sources.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#preparing-company-fundamentals-data",
    "href": "07_datacore_data.html#preparing-company-fundamentals-data",
    "title": "8  DataCore Data",
    "section": "8.2 Preparing Company Fundamentals Data",
    "text": "8.2 Preparing Company Fundamentals Data\nFirm accounting data are an important source of information that we use in portfolio analyses in subsequent chapters.\nTo access Company Fundamentals data, we can again tap DataCore, which hosts the funda data table that contains annual firm-level information on Vietnam companies. We follow the typical filter conventions and pull only data that we actually need: (i) we get only records in industrial data format, which includes companies that are primarily involved in manufacturing, services, and other non-financial business activities,1, (ii) in the standard format (i.e., consolidated information in standard presentation).\n\nimport pandas as pd\nfrom io import BytesIO\nimport datetime as dt\n\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\npaths = [\n    \"fundamental_annual_1767674486317/fundamental_annual_1.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_2.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_3.xlsx\",\n]\n\ndfs = []\nfor key in paths:\n    obj = s3.get_object(Bucket=bucket_name, Key=key)\n    df_tmp = pd.read_excel(BytesIO(obj[\"Body\"].read()))\n    dfs.append(df_tmp)\n\ndf_company_fundamental = pd.concat(dfs, ignore_index=True)\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\n\ndf = df_company_fundamental.copy()\n\n# core keys\ndf[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\ndf[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n\n# drop rows with missing keys\ndf = df.dropna(subset=[\"symbol\", \"year\"])\n\n# if some numeric columns are objects, force numeric for the ones we will use\nneed = [\n    \"total_asset\",\n    \"total_equity\",\n    \"total_liabilities\",\n    \"total_current_liabilities\",\n    \"is_net_revenue\",\n    \"is_cogs\",\n    \"is_manage_expense\",\n    \"is_interest_expense\",\n    \"na_tax_deferred\",\n    \"nl_tax_deferred\",\n    \"e_preferred_stock\",\n    \"capex\",\n    \"total_cfo\",\n    \"is_eat\",\n    \"total_current_asset\", \n    \"ca_cce\", \n    \"total_equity\", \n    \"cfo_interest_expense\",\n    \"ca_total_inventory\", \n    \"ca_acc_receiv\",\n    # The is_net_business_profit field captures the core profitability of a company's business activities before accounting for \"other\" non-core incomes and expenses, and before corporate income tax.\n    \"is_net_business_profit\"\n]\nfor c in need:\n    if c in df.columns:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n\n# Keep the row with the most non missing fields.\ndf[\"_non_missing\"] = df.notna().sum(axis=1)\n\ndf = (\n    df.sort_values([\"symbol\", \"year\", \"_non_missing\"])\n      .drop_duplicates(subset=[\"symbol\", \"year\"], keep=\"last\")\n      .drop(columns=\"_non_missing\")\n      .reset_index(drop=True)\n)\n\nprint(\"Remaining duplicates:\",\n      df.duplicated([\"symbol\", \"year\"]).sum())\n\nRemaining duplicates: 0\n\n\n\n# preferably use Tax ID as Identifer\n# ---- Firm identifier and fiscal date ----\ndf[\"datadate\"] = pd.to_datetime(df[\"year\"].astype(str) + \"-12-31\")  # Fiscal year-end date\n\n# ---- Core balance sheet ----\ndf[\"at\"]  = df[\"total_asset\"]                                      # Total assets\ndf[\"act\"] = df[\"total_current_asset\"] # Total Current Assets\n\ndf[\"lt\"]  = df[\"total_liabilities\"]                                 # Total liabilities\ndf[\"lct\"] = df[\"total_current_liabilities\"] # Total Current Liabilities\ndf[\"seq\"] = df[\"total_equity\"]                                      # Stockholders' equity\ndf[\"ceq\"] = df[\"e_equity\"] if \"e_equity\" in df.columns else np.nan  # Common equity (fallback)\n\n# ---- Deferred taxes ----\ndf[\"txditc\"] = df[\"na_tax_deferred\"] if \"na_tax_deferred\" in df.columns else 0  # Deferred tax assets\ndf[\"txdb\"]   = df[\"nl_tax_deferred\"] if \"nl_tax_deferred\" in df.columns else 0  # Deferred tax liabilities\ndf[\"itcb\"]   = 0                                                                # Investment tax credit (rare, set 0)\n\n# ---- Preferred stock (Compustat has multiple versions, we map one) ----\npref = df[\"e_preferred_stock\"] if \"e_preferred_stock\" in df.columns else 0\ndf[\"pstk\"]   = pref\ndf[\"pstkrv\"] = pref\ndf[\"pstkl\"]  = pref\n\n# ---- Income statement ----\ndf[\"sale\"] = df[\"is_net_revenue\"]                                                # Sales\ndf[\"cogs\"] = df[\"is_cogs\"] if \"is_cogs\" in df.columns else 0                    # Cost of goods sold\ndf[\"xsga\"] = df[\"is_manage_expense\"] if \"is_manage_expense\" in df.columns else 0# SG&A proxy\ndf[\"xint\"] = df[\"is_interest_expense\"] if \"is_interest_expense\" in df.columns else 0  # Interest expense\n\n# ---- Cash flow and investment ----\ndf[\"oancf\"] = df[\"total_cfo\"] if \"total_cfo\" in df.columns else np.nan  # Operating cash flow\ndf[\"capx\"]  = df[\"capex\"] if \"capex\" in df.columns else np.nan          # Capital expenditures\n\ncomp_vn = df\n\ncomp_vn.head()\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nitcb\npstk\npstkrv\npstkl\nsale\ncogs\nxsga\nxint\noancf\ncapx\n\n\n\n\n0\nA32\n2016\n3.432787e+11\nNaN\n1.397735e+11\n1.006804e+10\nNaN\nNaN\n1.297055e+11\n0.0\n...\n0\nNaN\nNaN\nNaN\n6.292595e+11\n5.636144e+11\n3.108361e+10\n0.0\n-3.127942e+10\n2.369143e+10\n\n\n1\nA32\n2017\n3.741267e+11\nNaN\n1.456583e+11\n5.095282e+10\nNaN\nNaN\n9.470550e+10\n0.0\n...\n0\nNaN\nNaN\nNaN\n6.118207e+11\n5.462587e+11\n2.972820e+10\n0.0\n-6.679452e+09\n1.598215e+10\n\n\n2\nA32\n2018\n3.358630e+11\nNaN\n5.829081e+10\n1.229081e+10\nNaN\nNaN\n4.600000e+10\n0.0\n...\n0\nNaN\nNaN\nNaN\n6.461948e+11\n5.837470e+11\n1.852045e+10\n0.0\n5.224003e+10\n1.405200e+10\n\n\n3\nA32\n2019\n2.987680e+11\nNaN\n6.051375e+10\n4.451375e+10\nNaN\nNaN\n1.600000e+10\n0.0\n...\n0\nNaN\nNaN\nNaN\n6.914857e+11\n6.092057e+11\n3.032093e+10\n0.0\n1.120983e+10\n1.296578e+10\n\n\n4\nA32\n2020\n3.566913e+11\nNaN\n4.435908e+10\n2.235908e+10\nNaN\nNaN\n2.200000e+10\n0.0\n...\n0\nNaN\nNaN\nNaN\n7.285810e+11\n6.489849e+11\n3.222692e+10\n0.0\n4.279751e+09\n5.433554e+09\n\n\n\n\n5 rows × 327 columns\n\n\n\n\n# Keep only firm-years with core fundamentals present\n\n# Required for most accounting ratios\nreq = [\"at\", \"lt\", \"seq\", \"sale\"]\n\ncomp_vn = comp_vn.dropna(subset=req)\ncomp_vn = comp_vn[comp_vn[\"at\"] &gt; 0]          # assets must be positive\ncomp_vn = comp_vn[comp_vn[\"sale\"] &gt;= 0]       # sales cannot be negative\n\n# Quick diagnostics\nprint(\"Rows:\", len(comp_vn))\nprint(\"Firms:\", comp_vn[\"symbol\"].nunique())\nprint(\"Years:\", comp_vn[\"datadate\"].dt.year.min(), \"-\", comp_vn[\"datadate\"].dt.year.max())\n\nRows: 20091\nFirms: 1502\nYears: 1998 - 2023\n\n\nNext, we calculate the book value of preferred stock and equity be and the operating profitability op inspired by the variable definitions in Kenneth French’s data library. Note that we set negative or zero equity to missing, which is a common practice when working with book-to-market ratios (see Fama and French 1992 for details).\n\ncomp_vn = (comp_vn\n  .assign(\n    be=lambda x: \n      (x[\"seq\"].combine_first(x[\"ceq\"]+x[\"pstk\"])\n       .combine_first(x[\"at\"]-x[\"lt\"])+\n       x[\"txditc\"].combine_first(x[\"txdb\"]+x[\"itcb\"]).fillna(0)-\n       x[\"pstkrv\"].combine_first(x[\"pstkl\"])\n       .combine_first(x[\"pstk\"]).fillna(0))\n  )\n  .assign(\n    be=lambda x: x[\"be\"].apply(lambda y: np.nan if y &lt;= 0 else y)\n  )\n  .assign(\n    op=lambda x: \n      ((x[\"sale\"]-x[\"cogs\"].fillna(0)- \n        x[\"xsga\"].fillna(0)-x[\"xint\"].fillna(0))/x[\"be\"])\n  )\n)\n\nWe keep only the last available information for each firm-year group (by using the tail(1) pandas function for each group). Note that datadate defines the time the corresponding financial data refers to (e.g., annual report as of December 31, 2022). Therefore, datadate is not the date when data was made available to the public. Check out the Exercises for more insights into the peculiarities of datadate.\n\ncomp_vn = (comp_vn\n  .assign(year=lambda x: pd.DatetimeIndex(x[\"datadate\"]).year)\n  .sort_values(\"datadate\")\n  .groupby([\"symbol\", \"year\"])\n  .tail(1)\n  .reset_index(drop=True)\n)\n\n\ncomp_vn_lag = (comp_vn\n  .get([\"symbol\", \"year\", \"at\"])\n  .assign(year=lambda x: x[\"year\"]+1)\n  .rename(columns={\"at\": \"at_lag\"})\n)\n\ncomp_vn = (comp_vn\n  .merge(comp_vn_lag, how=\"left\", on=[\"symbol\", \"year\"])\n  .assign(inv=lambda x: x[\"at\"]/x[\"at_lag\"]-1)\n  .assign(inv=lambda x: np.where(x[\"at_lag\"] &lt;= 0, np.nan, x[\"inv\"]))\n)\n\nIn a standard Vietnamese financial report, the LIABILITIES section (total_liabilities) includes all forms of debt, including non-interest-bearing items like Accounts Payable (cl_acc_payable) and Taxes payable (cl_tax_state_payable).\nTo get what financial analysts call “Total Debt” (interest-bearing debt), you must manually aggregate the specific loan and lease variables from your dataset:\n\nShort-term interest-bearing debt: Sum of cl_loan and cl_finlease.\nCurrent portion of long-term debt: cl_due_long_debt.\nLong-term interest-bearing debt: Sum of nl_loan and nl_finlease.\n\n\ncomp_vn = comp_vn.assign(\n    total_debt = lambda x: (\n        x[\"cl_loan\"].fillna(0) + \n        x[\"cl_finlease\"].fillna(0) + \n        x[\"cl_due_long_debt\"].fillna(0) + \n        x[\"nl_loan\"].fillna(0) + \n        x[\"nl_finlease\"].fillna(0)\n    ), \n    selling_general_and_administrative_expenses = lambda x: (\n        x[\"is_cos_of_sales\"].fillna(0) + x[\"is_manage_expense\"].fillna(0)\n    )\n)\n\nWith the last step, we are already done preparing the firm fundamentals. Thus, we can store them in our local database.\n\n(comp_vn\n  .to_sql(name=\"comp_vn\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#downloading-and-preparing-stock-data",
    "href": "07_datacore_data.html#downloading-and-preparing-stock-data",
    "title": "8  DataCore Data",
    "section": "8.3 Downloading and Preparing Stock Data",
    "text": "8.3 Downloading and Preparing Stock Data\n\nimport pandas as pd\nfrom io import BytesIO\nimport datetime as dt\n\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\n\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\n\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nprices[\"ret\"] = (\n    prices.groupby(\"symbol\")[\"adjusted_close\"]\n    .pct_change()\n)\n\n# Remove impossible crashes beyond -100 percent\nprices[\"ret\"] = prices[\"ret\"].clip(lower=-0.99)\n\nNow, we have all the relevant daily price data in memory and proceed with preparing the data for future analyses. We perform the preparation step at the current stage since we want to avoid executing the same mutations every time we use the data in subsequent chapters.\nThe first additional variable we create is market capitalization (mktcap), which is the product of the number of outstanding shares (shrout) and the last traded price in a month (prc). Note that in contrast to returns (ret), these two variables are not adjusted ex-post for any corporate actions like stock splits. Therefore, if you want to use a stock’s price, you need to adjust it with a cumulative adjustment factor. We also keep the market cap in millions of VND just for convenience, as we do not want to print huge numbers in our figures and tables. In addition, we set zero market capitalization to missing as it makes conceptually little sense (i.e., the firm would be bankrupt).\n\n# 1. Calculate the Share Count Proxy from Annual Fundamentals\n# df represents your fundamental_annual dataset\ndf[\"shrout\"] = (\n    df[\"is_shareholders_eat\"] / df[\"basic_eps\"]\n)\n\n# 2. Merge with Daily Prices\n# We join on 'symbol' and 'year' so every daily row in 'prices' \n# receives the share count corresponding to that fiscal year.\nprices = prices.merge(\n    df[[\"symbol\", \"year\", \"shrout\"]],\n    on=[\"symbol\", \"year\"],\n    how=\"left\"\n)\n\n# 3. Calculate Daily Market Cap\n# Market Cap = Daily Close Price * Annual Shares Outstanding\n# We use 'close' (unadjusted) to reflect the actual market value.\nprices[\"mktcap\"] = prices[\"close\"] * prices[\"shrout\"]\n\n# 4. Filter and Scale\n# scales values to millions for readability\nprices[\"mktcap\"] = (prices[\"mktcap\"] / 1000000).replace(0, np.nan)\n\n\n# Resample to Monthly Frequency\n# We group by symbol and resample the date to Month End (ME).\n# .last() picks the final available data point for each month.\nprices_monthly = (\n    prices.sort_values([\"symbol\", \"date\"])\n    .groupby(\"symbol\")\n    .resample(\"ME\", on=\"date\")\n    .last()\n)\n\n# After .last(), 'symbol' is index level 0 and 'date' is index level 1.\n# We remove 'symbol' and 'date' from the index to make them regular columns.\nprices_monthly = prices_monthly.drop(columns=[\"symbol\", \"date\"], errors=\"ignore\").reset_index()\n\nThe next variable we frequently use is the one-month lagged market capitalization. Lagged market capitalization is typically used to compute value-weighted portfolio returns, as we demonstrate in a later chapter. The most simple and consistent way to add a column with lagged market cap values is to add one month to each observation and then join the information to our monthly data.\n\n# 3. Lagged Market Capitalization\n# We calculate the market cap at t-1 to use for weighting returns at t.\nprices_monthly[\"mktcap_lag\"] = (\n    prices_monthly.groupby(\"symbol\")[\"mktcap\"]\n    .shift(1)\n)\n\n\n# remove rows missing returns or capitalization data.\nprices_monthly = (\n    prices_monthly\n    .dropna(subset=[\"ret\", \"mktcap\", \"mktcap_lag\"])\n)\n\nNext, we transform primary listing exchange codes to explicit exchange names.\n\ndef map_vn_exchange(row):\n    # This assumes you have an exchange column or can derive it from the symbol\n    # Adjust the logic based on your specific metadata availability.\n    if hasattr(row, 'exchange_code'):\n        if row.exchange_code == \"HOSE\": return \"NYSE_Equiv\"\n        if row.exchange_code == \"HNX\": return \"AMEX_Equiv\"\n    return \"Other\"\n\nNext, we compute excess returns by subtracting the monthly risk-free rate provided by our Fama-French data. As we base all our analyses on the excess returns, we can drop the risk-free rate from our data frame. Note that we ensure excess returns are bounded by -1 from below as a return less than -100% makes no sense conceptually.\nTo implement a placeholder for the Vietnam risk-free rate, we use an annualized value of 4.0% (0.04), which closely approximates the 10-year Vietnam Government Bond yield observed in early 2026. In professional asset pricing research for emerging markets, the 1-year or 10-year government bond rate is a standard proxy when a dedicated monthly Fama-French risk-free rate is unavailable.\n\n# Create a date range covering your prices_monthly sample\nall_dates = pd.date_range(\n    start=prices_monthly['date'].min(), \n    end=prices_monthly['date'].max(), \n    freq='ME'\n)\n\n# Professional Proxy: 4% Annualized (0.04)\n# Monthly rate = 0.04 / 12\nannual_rf = 0.04\nmonthly_rf = annual_rf / 12\n\nrf_monthly = pd.DataFrame({\n    'date': all_dates,\n    'risk_free': monthly_rf\n})\n  \nprices_monthly = (prices_monthly\n  # we don't have Fama-French data for Vietnam\n  .merge(rf_monthly, how=\"left\", on=\"date\")\n  .assign(ret_excess=lambda x: x[\"ret\"]-x[\"risk_free\"])\n  .assign(ret_excess=lambda x: x[\"ret_excess\"].clip(lower=-1))\n  # .drop(columns=[\"risk_free\"])\n)\n\n# Clean the infinite values before replication\nprices_monthly = prices_monthly.replace([np.inf, -np.inf], np.nan).dropna(subset=['ret_excess'])\n\n\n# Check for non-finite values (NaN or Inf)\nprint(\"Missing or Infinite Values:\")\nprint(prices_monthly[['ret_excess', 'mktcap_lag']].isna().sum())\nprint(np.isinf(prices_monthly['ret_excess']).sum())\n\n# Basic summary statistics to catch outliers\nprint(\"\\nRet_excess Summary Statistics:\")\nprint(prices_monthly['ret_excess'].describe())\n\nMissing or Infinite Values:\nret_excess    0\nmktcap_lag    0\ndtype: int64\n0\n\nRet_excess Summary Statistics:\ncount    165468.000000\nmean         -0.001458\nstd           0.041601\nmin          -0.993333\n25%          -0.006044\n50%          -0.003333\n75%           0.002764\nmax           4.139524\nName: ret_excess, dtype: float64\n\n\nInterpretation: Excess returns in a monthly dataset should typically fall between -1.0 and 1.0. If your max value is extremely high, it indicates an error in your return calculation or the risk-free rate merge. Any inf values will cause the np.average in your replication code to fail.\n\nimport matplotlib.pyplot as plt\n\n# Clean data for plotting\nplot_data = prices_monthly['ret_excess'].dropna()\nplot_data = plot_data[np.isfinite(plot_data)]\n\n# Histogram with fixed range\nplt.figure(figsize=(10, 6))\nplt.hist(plot_data, bins=100, range=(-0.5, 0.5), color='skyblue', edgecolor='black')\nplt.title('Distribution of Monthly Excess Returns (Vietnam)')\nplt.xlabel('Excess Return')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: A healthy distribution should be centered slightly above 0 and look roughly “bell-shaped” but with fatter tails (kurtosis). If the distribution is heavily skewed toward one side, check if your monthly_rf_placeholder was applied correctly.\nSince excess returns and market capitalization are crucial for all our analyses, we can safely exclude all observations with missing returns or market capitalization.\n\nprices_monthly = (prices_monthly\n  .dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n)\n\nFinally, we store the monthly Stock prices data file in our database.\n\n(prices_monthly\n  .to_sql(name=\"prices_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#first-glimpse-of-the-stock-sample",
    "href": "07_datacore_data.html#first-glimpse-of-the-stock-sample",
    "title": "8  DataCore Data",
    "section": "8.4 First Glimpse of the Stock Sample",
    "text": "8.4 First Glimpse of the Stock Sample\nBefore we move on to other data sources, let us look at some descriptive statistics of the Stock sample, which is our main source for stock returns.\nFigure 8.1 shows the monthly number of securities by listing exchange over time. NYSE has the longest history in the data, but NASDAQ lists a considerably large number of stocks.\n\n\n\nsecurities_per_exchange = (prices_monthly\n  .groupby([\"exchange\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nsecurities_per_exchange_figure = (\n  ggplot(\n    securities_per_exchange, \n    aes(x=\"date\", y=\"n\", color=\"exchange\", linetype=\"exchange\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly number of securities by listing exchange\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\n\nsecurities_per_exchange_figure.show()\n\n\nFigure 8.1\n\n\n\nNext, we look at the aggregate market capitalization grouped by the respective listing exchanges in Figure 8.2. To ensure that we look at meaningful data that is comparable over time, we adjust the nominal values for inflation. In fact, we can use the tables that are already in our database to calculate aggregate market caps by listing exchange. All values in Figure 8.2 are in terms of the end of 2024 USD to ensure intertemporal comparability. NYSE-listed stocks have by far the largest market capitalization, followed by NASDAQ-listed stocks.\n\n\n\ncpi_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM cpi_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nmarket_cap_per_exchange = (prices_monthly\n  .merge(cpi_monthly, how=\"left\", on=\"date\")\n  .groupby([\"date\", \"exchange\"])\n  .apply(\n    lambda group: pd.Series({\n      \"mktcap\": group[\"mktcap\"].sum()/group[\"cpi\"].mean()\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_exchange_figure = (\n  ggplot(\n    market_cap_per_exchange, \n    aes(x=\"date\", y=\"mktcap/1000\", color=\"exchange\", linetype=\"exchange\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly market cap by listing exchange\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n)\nmarket_cap_per_exchange_figure.show()\n\n\nFigure 8.2\n\n\n\nNext, we look at the same descriptive statistics by industry. Figure 8.3 plots the number of stocks in the sample for each of the VSIC industry classifiers. For most of the sample period, the largest share of stocks is in manufacturing, albeit the number peaked somewhere in the 90s. The number of firms associated with public administration seems to be the only category on the rise in recent years, even surpassing manufacturing at the end of our sample period.\n\n\n\nsecurities_per_industry = (prices_monthly\n  .groupby([\"industry\", \"date\"])\n  .size()\n  .reset_index(name=\"n\")\n)\n\nlinetypes = [\"-\", \"--\", \"-.\", \":\"]\nn_industries = securities_per_industry[\"industry\"].nunique()\n\nsecurities_per_industry_figure = (\n  ggplot(\n    securities_per_industry, \n    aes(x=\"date\", y=\"n\", color=\"industry\", linetype=\"industry\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly number of securities by industry\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n  + scale_linetype_manual(\n      values=[linetypes[l % len(linetypes)] for l in range(n_industries)]\n    ) \n)\nsecurities_per_industry_figure.show()\n\n\nFigure 8.3\n\n\n\nWe also compute the market cap of all stocks belonging to the respective industries and show the evolution over time in Figure 8.4. At all points in time, manufacturing firms comprise of the largest portion of market capitalization. Toward the end of the sample, however, financial firms and services begin to make up a substantial portion of the market cap.\n\n\n\nmarket_cap_per_industry = (prices_monthly\n  .merge(cpi_monthly, how=\"left\", on=\"date\")\n  .groupby([\"date\", \"industry\"])\n  .apply(\n    lambda group: pd.Series({\n      \"mktcap\": (group[\"mktcap\"].sum()/group[\"cpi\"].mean())\n    })\n  )\n  .reset_index()\n)\n\nmarket_cap_per_industry_figure = (\n  ggplot(\n    market_cap_per_industry, \n    aes(x=\"date\", y=\"mktcap/1000\", color=\"industry\", linetype=\"industry\")\n  )\n  + geom_line()\n  + labs(\n      x=\"\", y=\"\", color=\"\", linetype=\"\",\n      title=\"Monthly market cap by industry in billions of Dec 2024 USD\"\n    )\n  + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n  + scale_y_continuous(labels=comma_format())\n  + scale_linetype_manual(\n      values=[linetypes[l % len(linetypes)] for l in range(n_industries)]\n    ) \n)\nmarket_cap_per_industry_figure.show()\n\n\nFigure 8.4",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#daily-stock-data",
    "href": "07_datacore_data.html#daily-stock-data",
    "title": "8  DataCore Data",
    "section": "8.5 Daily Stock Data",
    "text": "8.5 Daily Stock Data\nBefore we turn to accounting data, we provide a proposal for downloading daily Stock data with the same filters used for the monthly data. While the monthly data from above typically fit into your memory and can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily Stock data file is substantially larger than monthly data. This has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire dataset to your local database), and in our experience, the download usually crashes (or never stops).\nThere is a solution to this challenge. As with many big data problems, you can split up the big task into several smaller tasks that are easier to handle. That is, instead of downloading data about all stocks at once, download the data in small batches of stocks consecutively. Such operations can be implemented in for-loops, where we download, prepare, and store the data for a small number of stocks in each iteration. This operation might nonetheless take around 5 minutes, depending on your internet connection. To keep track of the progress, we create ad-hoc progress updates using print(). Notice that we also use the method to_sql() here with the option to append the new data to an existing table, when we process the second and all following batches.\n\nfactors_ff3_daily = pd.read_sql(\n  sql=\"SELECT * FROM factors_ff3_daily\", \n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\npermnos = pd.read_sql(\n  sql=\"SELECT DISTINCT permno FROM crsp.stksecurityinfohist\", \n  con=DataCore,\n  dtype={\"permno\": int}\n)\n\npermnos = list(permnos[\"permno\"].astype(str))\n  \nbatch_size = 500\nbatches = np.ceil(len(permnos)/batch_size).astype(int)\n\nfor j in range(1, batches+1):  \n    \n  permno_batch = permnos[\n    ((j-1)*batch_size):(min(j*batch_size, len(permnos)))\n  ]\n  \n  permno_batch_formatted = (\n    \", \".join(f\"'{permno}'\" for permno in permno_batch)\n  )\n  permno_string = f\"({permno_batch_formatted})\"\n  \n  crsp_daily_sub_query = (\n    \"SELECT dsf.permno, dlycaldt AS date, dlyret AS ret \"\n      \"FROM crsp.dsf_v2 AS dsf \"\n      \"INNER JOIN crsp.stksecurityinfohist AS ssih \"\n      \"ON dsf.permno = ssih.permno AND \"\n         \"ssih.secinfostartdt &lt;= dsf.dlycaldt AND \"\n         \"dsf.dlycaldt &lt;= ssih.secinfoenddt \"\n      f\"WHERE dsf.permno IN {permno_string} \"\n           f\"AND dlycaldt BETWEEN '{start_date}' AND '{end_date}' \"\n            \"AND ssih.sharetype = 'NS' \"\n            \"AND ssih.securitytype = 'EQTY' \"  \n            \"AND ssih.securitysubtype = 'COM' \" \n            \"AND ssih.usincflg = 'Y' \" \n            \"AND ssih.issuertype in ('ACOR', 'CORP') \" \n            \"AND ssih.primaryexch in ('N', 'A', 'Q') \"\n            \"AND ssih.conditionaltype in ('RW', 'NW') \"\n            \"AND ssih.tradingstatusflg = 'A'\"\n  )\n    \n  crsp_daily_sub = (pd.read_sql_query(\n      sql=crsp_daily_sub_query,\n      con=DataCore,\n      dtype={\"permno\": int},\n      parse_dates={\"date\"}\n    )\n    .dropna()\n   )\n\n  if not crsp_daily_sub.empty:\n    \n      crsp_daily_sub = (crsp_daily_sub\n        .merge(factors_ff3_daily[[\"date\", \"risk_free\"]], \n               on=\"date\", how=\"left\")\n        .assign(\n          ret_excess = lambda x: \n            ((x[\"ret\"] - x[\"risk_free\"]).clip(lower=-1))\n        )\n        .get([\"permno\", \"date\", \"ret_excess\"])\n      )\n        \n      if j == 1:\n        if_exists_string = \"replace\"\n      else:\n        if_exists_string = \"append\"\n\n      crsp_daily_sub.to_sql(\n        name=\"crsp_daily\", \n        con=tidy_finance, \n        if_exists=if_exists_string, \n        index=False\n      )\n            \n  print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#merging-stock-with-company-fundamentals",
    "href": "07_datacore_data.html#merging-stock-with-company-fundamentals",
    "title": "8  DataCore Data",
    "section": "8.6 Merging Stock with Company Fundamentals",
    "text": "8.6 Merging Stock with Company Fundamentals\nTo link the two datasets, we need to use the stock symbol.\nBefore we close this chapter, let us look at an interesting descriptive statistic of our data. As the book value of equity plays a crucial role in many asset pricing applications, it is interesting to know for how many of our stocks this information is available. Hence, Figure 8.5 plots the share of securities with book equity values for each exchange.\n\nshare_with_be = (prices_monthly\n  .assign(year=lambda x: pd.DatetimeIndex(x[\"date\"]).year)\n  .sort_values(\"date\")\n  .groupby([\"symbol\", \"year\"])\n  .tail(1)\n  .reset_index()\n  .merge(comp_vn, how=\"left\", on=[\"symbol\", \"year\"])\n  \n  # Group by year only for now; easy to add \"exchange\" later\n  .groupby([\"year\"]) \n  # # .groupby([\"exchange\", \"year\"]) # Uncomment this later\n  .apply(\n    lambda x: pd.Series({\n      \"share\": x[\"symbol\"][x[\"be\"].notnull()].nunique() / x[\"symbol\"].nunique()\n    }),\n    include_groups=False # Recommended for newer pandas versions\n  )\n  .reset_index()\n)\n\n# Professional visualization check\nshare_with_be_figure = (\n  ggplot(\n    share_with_be, \n \n    aes(x=\"year\", y=\"share\") \n    # aes(x=\"year\", y=\"share\", color=\"exchange\", linetype=\"exchange\")\n  )\n  + geom_line(size=1)\n  + labs(\n      x=\"Year\", y=\"Share with BE\",\n      title=\"Share of securities with book equity values\"\n      # title=\"Share of securities with book equity values by exchange\"\n    )\n  + scale_y_continuous(labels=percent_format())\n  + coord_cartesian(ylim=(0, 1))\n  + theme_minimal()\n)\nshare_with_be_figure.show()\n\n\n\n\n\n\n\nFigure 8.5: The figure shows the end-of-year share of securities with book equity values by listing exchange.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#key-takeaways",
    "href": "07_datacore_data.html#key-takeaways",
    "title": "8  DataCore Data",
    "section": "8.7 Key Takeaways",
    "text": "8.7 Key Takeaways\n\nDataCore provides secure access to essential financial databases like Stock and Company Fundamentals, which are critical for empirical finance research.\nStock data provides return, market capitalization and industry data for US common stocks listed.\nCompany Fundamentals provides firm-level accounting data such as book equity, profitability, and investment.\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock returns.” The Journal of Finance 47 (2): 427–65. https://doi.org/2329112.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#footnotes",
    "href": "07_datacore_data.html#footnotes",
    "title": "8  DataCore Data",
    "section": "",
    "text": "Companies that operate in the banking, insurance, or utilities sector typically report in different industry formats that reflect their specific regulatory requirements.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html",
    "href": "08_beta_estimation.html",
    "title": "9  Beta Estimation",
    "section": "",
    "text": "9.1 Estimating Beta Using Monthly Returns\nIn this chapter, we introduce an important concept in financial economics: The exposure of an individual stock to changes in the market portfolio. According to the Capital Asset Pricing Model (CAPM) of Sharpe (1964), Lintner (1965), and Mossin (1966), cross-sectional variation in expected asset returns should be a function of the covariance between the excess return of the asset and the excess return on the market portfolio. The regression coefficient of excess market returns on excess stock returns is usually called the market beta. We show an estimation procedure for the market betas. We provide details about all the functions that we use to compute the results. In particular, we leverage useful computational concepts: Rolling-window estimation and parallelization.\nWe use the following Python packages throughout this chapter:\nCompared to previous chapters, we introduce statsmodels (seabold2010statsmodels?) for regression analysis and for sliding-window regressions and joblib (joblib?) for parallelization.\nThe estimation procedure is based on a rolling-window estimation, where we may use either monthly or daily returns and different window lengths. First, let us start with loading the monthly Stock data from our SQLite database introduced in Accessing and Managing Financial Data and DataCore.\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\n# Prepare accounting data with a 'year' column for merging\ncomp_vn = (pd.read_sql_query(\n    sql=\"SELECT symbol, datadate, icb_name_vi FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"})\n  .assign(year=lambda x: x[\"datadate\"].dt.year)\n  .dropna()\n)\n\n# Ensure prices_monthly has a 'year' column and is merged correctly\nprices_monthly = (pd.read_sql_query(\n    sql=\"SELECT symbol, date, ret_excess FROM prices_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .assign(year=lambda x: x[\"date\"].dt.year)\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nprices_monthly = (prices_monthly\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n  .merge(comp_vn, how=\"left\", on=[\"symbol\", \"year\"])\n  .dropna()\n)\nfrom scipy.stats.mstats import winsorize\n\n# Apply 1% winsorization to returns\nprices_monthly = (prices_monthly\n  .assign(\n    ret_excess = lambda x: winsorize(x[\"ret_excess\"], limits=[0.01, 0.01]),\n    mkt_excess = lambda x: winsorize(x[\"mkt_excess\"], limits=[0.01, 0.01])\n  )\n)\nTo estimate the CAPM regression coefficients\n\\[\nr_{i, t} - r_{f, t} = \\alpha_i + \\beta_i(r_{m, t}-r_{f,t})+\\varepsilon_{i, t},\n\\tag{9.1}\\]\nwe regress stock excess returns ret_excess on excess returns of the market portfolio mkt_excess.\nPython provides a simple solution to estimate (linear) models with the function smf.ols(). The function requires a formula as input that is specified in a compact symbolic form. An expression of the form y ~ model is interpreted as a specification that the response y is modeled by a linear predictor specified symbolically by model. Such a model consists of a series of terms separated by + operators. In addition to standard linear models, smf.ols() provides a lot of flexibility. To start, we restrict the data only to the time series of observations in Stock data that correspond to a company’s stock and compute \\(\\hat\\alpha_i\\) as well as \\(\\hat\\beta_i\\).\nmodel_fit = smf.ols(\n    formula=\"ret_excess ~ mkt_excess\", \n    data=prices_monthly.query(\"symbol == 'VIN'\")\n).fit()\ncoefficients = model_fit.summary2().tables[1]\ncoefficients\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n0.015495\n0.016506\n0.938751\n0.349934\n-0.017220\n0.048210\n\n\nmkt_excess\n3.133326\n1.865697\n1.679440\n0.095931\n-0.564424\n6.831076\nsmf.ols() returns an object of class RegressionModel, which contains all the information we usually care about with linear models. summary2() returns information about the estimated parameters. The output above indicates that Apple moves excessively with the market as the estimated \\(\\hat\\beta_i\\) is above one (\\(\\hat\\beta_i \\approx 1.4\\)).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#rolling-window-estimation",
    "href": "08_beta_estimation.html#rolling-window-estimation",
    "title": "9  Beta Estimation",
    "section": "9.2 Rolling-Window Estimation",
    "text": "9.2 Rolling-Window Estimation\nAfter we estimated the regression coefficients on an example, we scale the estimation of \\(\\beta_i\\) to a whole different level and perform rolling-window estimations for the entire CRSP sample. The following function implements the CAPM regression for a data frame (or a part thereof) containing at least min_obs observations to avoid huge fluctuations if the time series is too short. The function conveniently returns the regression results as a data frame, which ensures that our approach is scalable. If the min_obs-condition is violated, that is, the time series is too short, the function returns an empty data frame for consistency.\n\ndef estimate_capm(data, min_obs=1):\n    if data.shape[0] &lt; min_obs:\n        capm = pd.DataFrame()\n    else:\n        fit = smf.ols(formula=\"ret_excess ~ mkt_excess\", data=data).fit()\n        coefficients = fit.summary2().tables[1]\n\n        capm = pd.DataFrame(\n            {\n                \"coefficient\": coefficients.index,\n                \"estimate\": coefficients[\"Coef.\"],\n                \"t_statistic\": coefficients[\"t\"],\n            }\n        ).assign(\n            coefficient=lambda x: np.where(\n                x[\"coefficient\"] == \"Intercept\", \"alpha\", x[\"coefficient\"]\n            )\n        )\n\n    return capm\n\nNext, we define a function that does the rolling estimation. We use a simple for-loop to implement the sliding window estimation in a straightforward manner. The following function takes input data and slides across the date vector, considering only a total of look_back months. The function essentially performs three steps: (i) arrange all rows, (ii) compute betas by sliding across months, and (iii) return a tibble with dates and corresponding parameter estimates. As we demonstrate further below, we can also apply the same function to daily return data.\n\ndef roll_capm_estimation(data, look_back=60, min_obs=48):\n    results = []\n    dates = data[\"date\"].sort_values().drop_duplicates()\n\n    for i in range(look_back - 1, len(dates)):\n        end_date = dates.iloc[i]\n        start_date = end_date - relativedelta(months=look_back - 1)\n\n        window_data = data.query(\"date &gt;= @start_date & date &lt;= @end_date\")\n\n        result = estimate_capm(window_data, min_obs=min_obs)\n        result[\"date\"] = np.max(window_data[\"date\"])\n        results.append(result)\n\n    if results:\n        rolling_capm_estimation = pd.concat(results, ignore_index=True)\n    else:\n        rolling_capm_estimation = pd.DataFrame()\n\n    return rolling_capm_estimation\n\nBefore we approach the whole Stock sample, let us focus on a couple of examples for well-known firms.\n\nexamples = pd.DataFrame({\n    \"symbol\": [\"FPT\", \"VNM\", \"VIC\", \"HPG\"],\n    \"company\": [\"FPT Corporation\", \"Vinamilk\", \"Vingroup\", \"Hoa Phat Group\"]\n})\n\n\n# Check how many months of data each example firm has\n(prices_monthly\n  .query(\"symbol in @examples['symbol']\")\n  .groupby(\"symbol\")\n  .size()\n  .reset_index(name=\"obs_count\")\n)\n\nprices_monthly[prices_monthly['symbol'] == \"FPT\"]\n\n\n\n\n\n\n\n\nsymbol\ndate\nret_excess\nyear\nmkt_excess\ndatadate\nicb_name_vi\n\n\n\n\n41646\nFPT\n2011-07-31\n0.194583\n2011\n-0.011287\n2011-12-31\nCông nghệ phần mềm\n\n\n41647\nFPT\n2011-08-31\n-0.038116\n2011\n0.007856\n2011-12-31\nCông nghệ phần mềm\n\n\n41648\nFPT\n2011-09-30\n-0.093421\n2011\n-0.006501\n2011-12-31\nCông nghệ phần mềm\n\n\n41649\nFPT\n2011-10-31\n-0.042168\n2011\n-0.005363\n2011-12-31\nCông nghệ phần mềm\n\n\n41650\nFPT\n2011-11-30\n-0.011414\n2011\n-0.009524\n2011-12-31\nCông nghệ phần mềm\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41791\nFPT\n2023-08-31\n0.113277\n2023\n0.004178\n2023-12-31\nCông nghệ phần mềm\n\n\n41792\nFPT\n2023-09-30\n-0.043664\n2023\n-0.000150\n2023-12-31\nCông nghệ phần mềm\n\n\n41793\nFPT\n2023-10-31\n-0.108937\n2023\n-0.024628\n2023-12-31\nCông nghệ phần mềm\n\n\n41794\nFPT\n2023-11-30\n0.103896\n2023\n-0.009693\n2023-12-31\nCông nghệ phần mềm\n\n\n41795\nFPT\n2023-12-31\n0.042369\n2023\n-0.000822\n2023-12-31\nCông nghệ phần mềm\n\n\n\n\n150 rows × 7 columns\n\n\n\nThe main idea is to apply the function to each stock individually and then combine the results into a single data frame. First, we nest the data by symbol. Nested data means we now have a list of symbol with corresponding grouped time series data. We get one row of output for each unique combination of non-nested variables which is only symbol in this case.\n\ncapm_examples_nested = (prices_monthly\n    .query(\"symbol in @examples['symbol']\")\n    .groupby(\"symbol\", group_keys=True)\n)\ncapm_examples_nested\n\n&lt;pandas.api.typing.DataFrameGroupBy object at 0x7fe4337e7250&gt;\n\n\nNext, we want to apply the roll_capm_estimation() function to each stock. This situation is an ideal use case for apply(), which takes a list or vector as input and returns an object of the same length as the input. In our case, apply() returns a single data frame with a time series of beta estimates for each stock. Therefore, we use reset_index() to transform the list of outputs to a tidy data frame.\n\n# use this after fixing the data\ncapm_examples = (capm_examples_nested\n    .apply(lambda x: roll_capm_estimation(x), include_groups=False)\n    .reset_index()\n    .get([\"symbol\", \"date\", \"coefficient\", \"estimate\", \"t_statistic\"])\n)\n\nFigure 9.1 displays the resulting beta estimates, focusing exclusively on the coefficient fo \"mkt_excess\".\n\nbeta_examples_sub = capm_examples.merge(examples, how=\"left\", on=\"symbol\").query(\n    \"coefficient == 'mkt_excess'\"\n)\n\nbeta_figure = (\n    ggplot(\n        beta_examples_sub,\n        aes(x=\"date\", y=\"estimate\", color=\"company\", linetype=\"company\"),\n    )\n    + geom_line()\n    + labs(\n        x=\"\",\n        y=\"\",\n        color=\"\",\n        linetype=\"\",\n        title=\"Monthly beta estimates for example stocks using 5 years of data\",\n    )\n    + scale_x_datetime(date_breaks=\"5 year\", date_labels=\"%Y\")\n)\nbeta_figure.show()\n\n\n\n\n\n\n\nFigure 9.1: The figure shows monthly beta estimates for example stocks using five years of data. The CAPM betas are estimated with monthly data and a rolling window of length five years based on adjusted excess returns from Stock Data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#parallelized-rolling-window-estimation",
    "href": "08_beta_estimation.html#parallelized-rolling-window-estimation",
    "title": "9  Beta Estimation",
    "section": "9.3 Parallelized Rolling-Window Estimation",
    "text": "9.3 Parallelized Rolling-Window Estimation\nEven though we could now just apply the function using .groubby() on the whole Stock sample, we advise against doing it as it is computationally quite expensive. Remember that we have to perform rolling-window estimations across all stocks and time periods. However, this estimation problem is an ideal scenario to employ the power of parallelization. Parallelization means that we split the tasks which perform rolling-window estimations across different workers (or cores on your local machine).\nIf you have a Windows or Mac machine, it makes most sense use the default parallelization backend of joblib, which means that separate Python processes are running in the background on the same machine to perform the individual jobs. If you check out the documentation of joblib.parallel_config(), you can also see other ways to resolve the parallelization in different environments. Note that we use availableCores() to determine the number of cores available for parallelization, but keep one core free for other tasks. Some machines might freeze if all cores are busy with Python jobs.\n\nn_cores = cpu_count() - 1\nn_cores\n\n3\n\n\nYou can speed up things considerably by having more cores available to share the workload or by having more powerful cores. Instead of using .apply() on groups, we use Parallel() to execute multiple tasks concurrently and delayed() to wrap each function call, allowing the calls to be queued and distributed to worker processes rather than executed immediately.\n\nprices_monthly_nested = (prices_monthly\n    .groupby(\"symbol\", group_keys=False)\n)\n\ncapm_monthly = pd.concat(\n    Parallel(n_jobs=n_cores)(\n        delayed(\n            lambda name, group: roll_capm_estimation(group).assign(symbol=name)\n        )(\n            name, group\n        )\n        for name, group in prices_monthly_nested\n    )\n).get([\"symbol\", \"date\", \"coefficient\", \"estimate\", \"t_statistic\"])\ncapm_monthly\n\n\n(capm_monthly\n  .to_sql(name=\"capm_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\n\ncapm_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM capm_monthly\", \n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\ncapm_monthly.head()\n\n\n\n\n\n\n\n\nsymbol\ndate\ncoefficient\nestimate\nt_statistic\n\n\n\n\n0\nA32\n2023-10-31\nalpha\n-0.003802\n-0.735962\n\n\n1\nA32\n2023-10-31\nmkt_excess\n0.685640\n1.218415\n\n\n2\nA32\n2023-11-30\nalpha\n-0.003740\n-0.721261\n\n\n3\nA32\n2023-11-30\nmkt_excess\n0.674715\n1.205106\n\n\n4\nA32\n2023-12-31\nalpha\n-0.003796\n-0.734142",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#estimating-beta-using-daily-returns",
    "href": "08_beta_estimation.html#estimating-beta-using-daily-returns",
    "title": "9  Beta Estimation",
    "section": "9.4 Estimating Beta Using Daily Returns",
    "text": "9.4 Estimating Beta Using Daily Returns\nBefore we provide some descriptive statistics of our beta estimates, we implement the estimation for the daily CRSP sample as well. Depending on the application, you might either use longer horizon beta estimates based on monthly data or shorter horizon estimates based on daily returns. As loading the full daily CRSP data requires relatively large amounts of memory, we split the beta estimation into smaller chunks. The logic follows the approach that we use to download the daily CRSP data (see WRDS, CRSP, and Compustat).\nFirst, we load the daily Fama-French market excess returns and extract the vector of dates.\n\nfactors_ff3_daily = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_daily\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nWe use the stocks from the monthly Stock dataset as our reference point and process them in batches of 500. To estimate the CAPM over a consistent lookback window while accommodating different return frequencies, we adjust the minimum required number of observations accordingly. Specifically, we require at least 1,000 daily returns over a five‑year period for a valid estimation. This threshold is consistent with the monthly requirement of 48 observations out of 60 months, given that there are roughly 252 trading days in a year.\n\nsymbols = list(prices_monthly[\"symbol\"].unique().astype(str))\n\nbatch_size = 500\nbatches = np.ceil(len(symbols)/batch_size).astype(int)\nmin_obs = 1_000\n\nWe then proceed to perform the same steps as with the monthly Stock data, just in batches: Load in daily returns, nest the data by stock, and parallelize the beta estimation across stocks. Note that we also convert the daily date to the beginning of the month so that we can still look back over 60 months and get one beta estimate per month, even though we are using daily data.\n\ncapm_daily = []\n\nfor j in range(1, batches+1):  \n    symbol_batch = symbols[\n      ((j-1)*batch_size):(min(j*batch_size, len(symbols)))\n    ]\n    \n    symbol_batch_formatted = (\n      \", \".join(f\"'{symbol}'\" for symbol in symbol_batch)\n    )\n    symbol_string = f\"({symbol_batch_formatted})\"\n    \n    prices_daily_sub_query = (\n      \"SELECT symbol, date, ret_excess \"\n        \"FROM prices_daily \"\n       f\"WHERE symbol IN {symbol_string}\" \n    )\n      \n    prices_daily_sub = pd.read_sql_query(\n      sql=prices_daily_sub_query,\n      con=tidy_finance,\n      dtype={\"symbol\": int},\n      parse_dates={\"date\"}\n    )\n    \n    prices_daily_sub_nested = (prices_daily_sub\n      .merge(factors_ff3_daily, how=\"inner\", on=\"date\")\n      .assign(\n          date = lambda x: \n            x[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n        )\n      .groupby(\"symbol\", group_keys=False)\n    )\n    \n    results = Parallel(n_jobs=n_cores)(\n        delayed(\n            lambda name, group: roll_capm_estimation(group, min_obs=min_obs).assign(symbol=name)\n        )(\n            name, group\n        )\n        for name, group in prices_daily_sub_nested\n    )\n    \n    if results:\n        capm_daily_sub = pd.concat(results).get(\n            [\"symbol\", \"date\", \"coefficient\", \"estimate\", \"t_statistic\"]\n        )\n        capm_daily.append(capm_daily_sub)\n    else:\n        print(f\"Warning: Batch {j} produced no results (insufficient data)\")\n              \n    print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\\n\")\n  \ncapm_daily = pd.concat(capm_daily)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#comparing-beta-estimates",
    "href": "08_beta_estimation.html#comparing-beta-estimates",
    "title": "9  Beta Estimation",
    "section": "9.5 Comparing Beta Estimates",
    "text": "9.5 Comparing Beta Estimates\nWhat is a typical value for stock betas? First, let us extract the relevant estimates from our CAPM results based on monthly returns.\n\nbeta_monthly = (capm_monthly\n    .query(\"coefficient == 'mkt_excess'\")\n    .get([\"symbol\", \"date\", \"estimate\"])\n    .rename(columns={\"estimate\": \"beta\"})\n    .assign(return_type=\"monthly\")\n)\n\n\n(beta_monthly.to_sql(\n  name=\"beta_monthly\", \n  con=tidy_finance, \n  if_exists=\"replace\",\n  index=False\n  )\n)\n\n71488\n\n\nTo get some feeling, we illustrate the dispersion of the estimated \\(\\hat\\beta_i\\) across different industries and across time below. Figure 9.2 shows that typical business models across industries imply different exposure to the general market economy.\n\nbeta_industries = (beta_monthly\n    .merge(prices_monthly, how=\"inner\", on=[\"symbol\", \"date\"])\n    .dropna(subset=\"beta\")\n    .groupby([\"icb_name_vi\", \"symbol\"])[\"beta\"]\n    .aggregate(\"mean\")\n    .reset_index()\n)\n\nindustry_order = (beta_industries\n    .groupby(\"icb_name_vi\")[\"beta\"]\n    .aggregate(\"median\")\n    .sort_values()\n    .index.tolist()\n)\n\n# To show the 10 industries with the highest median beta\ntop_10_industries = industry_order[-10:]\n\n# To show the 10 industries with the lowest median beta\nbottom_10_industries = industry_order[:10]\n\n# Update the plot call\nbeta_industries_figure = (\n    ggplot(beta_industries, aes(x=\"icb_name_vi\", y=\"beta\"))\n    + geom_boxplot()\n    + coord_flip()\n    + scale_x_discrete(limits=top_10_industries) # Use the sliced list here\n    + labs(title=\"Top 10 Industries by Beta\")\n)\n\n# beta_industries_figure = (\n#     ggplot(\n#         beta_industries, \n#         aes(x=\"icb_name_vi\", y=\"beta\") \n#     )\n#     + geom_boxplot()\n#     + coord_flip()\n#     + labs(\n#         x=\"\",\n#         y=\"Beta\", \n#         title=\"Firm-specific beta distributions by industry\"\n#     )\n#     + scale_x_discrete(limits=industry_order)\n# )\n\nbeta_industries_figure.show()\n\n\n\n\n\n\n\nFigure 9.2: The box plots show the average firm-specific beta estimates by industry.\n\n\n\n\n\nNext, we illustrate the time-variation in the cross-section of estimated betas. Figure 9.3 shows the monthly deciles of estimated betas (based on monthly data) and indicates an interesting pattern: First, betas seem to vary over time in the sense that during some periods, there is a clear trend across all deciles. Second, the sample exhibits periods where the dispersion across stocks increases in the sense that the lower decile decreases and the upper decile increases, which indicates that for some stocks the correlation with the market increases while for others it decreases. Note also here: stocks with negative betas are a rare exception.\n\nbeta_quantiles = (\n    beta_monthly.groupby(\"date\")[\"beta\"]\n    .quantile(q=np.arange(0.1, 1.0, 0.1))\n    .reset_index()\n    .rename(columns={\"level_1\": \"quantile\"})\n    .assign(quantile=lambda x: (x[\"quantile\"] * 100).astype(int))\n    .dropna()\n)\n\nlinetypes = [\"-\", \"--\", \"-.\", \":\"]\nn_quantiles = beta_quantiles[\"quantile\"].nunique()\n\nbeta_quantiles_figure = (\n    ggplot(\n        beta_quantiles,\n        aes(x=\"date\", y=\"beta\", color=\"factor(quantile)\", linetype=\"factor(quantile)\"),\n    )\n    + geom_line()\n    + labs(\n        x=\"\", y=\"\", color=\"\", linetype=\"\", title=\"Monthly deciles of estimated betas\"\n    )\n    + scale_x_datetime(date_breaks=\"5 year\", date_labels=\"%Y\")\n    + scale_linetype_manual(\n        values=[linetypes[l % len(linetypes)] for l in range(n_quantiles)]\n    )\n)\nbeta_quantiles_figure.show()\n\n\n\n\n\n\n\nFigure 9.3: The figure shows monthly deciles of estimated betas. Each line corresponds to the monthly cross-sectional quantile of the estimated CAPM beta.\n\n\n\n\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table.\n\nbeta_daily = (capm_daily\n    .query(\"coefficient == 'mkt_excess'\")\n    .get([\"symbol\", \"date\", \"estimate\"])\n    .rename(columns={\"estimate\": \"beta\"})\n    .assign(return_type=\"daily\")\n)\n\nbeta = pd.concat([beta_monthly, beta_daily], ignore_index=True)\n\nTo compare the difference between daily and monthly data, we combine beta estimates to a single table. Then, we use the table to plot a comparison of beta estimates for our example stocks in Figure 9.4.\n\n\n\nbeta_comparison = beta.merge(examples, how=\"inner\", on=\"symbol\")\n\nbeta_comparison_figure = (\n    ggplot(\n        beta_comparison,\n        aes(x=\"date\", y=\"beta\", color=\"return_type\", linetype=\"return_type\"),\n    )\n    + geom_line()\n    + facet_wrap(\"~company\", ncol=1)\n    + labs(\n        x=\"\",\n        y=\"\",\n        color=\"\",\n        linetype=\"\",\n        title=\"Comparison of beta estimates using monthly and daily data\",\n    )\n    + scale_x_datetime(date_breaks=\"10 years\", date_labels=\"%Y\")\n    + theme(figure_size=(6.4, 6.4))\n)\nbeta_comparison_figure.show()\n\n\nFigure 9.4\n\n\n\nThe estimates in Figure 9.4 look as expected. As you can see, it really depends on the data frequency how your beta estimates turn out because the estimates based on daily data are much smoother due to the higher number of observations in each regression.\nFinally, we write the estimates to our database so that we can use them in later chapters.\n\n(beta.to_sql(\n  name=\"beta\", \n  con=tidy_finance, \n  if_exists=\"replace\",\n  index=False\n  )\n)\n\nWhenever you perform some kind of estimation, it also makes sense to do rough plausibility tests. A possible check is to plot the share of stocks with beta estimates over time. This descriptive helps us discover potential errors in our data preparation or estimation procedure. For instance, suppose there was a gap in our output where we do not have any betas. In this case, we would have to go back and check all previous steps to find out what went wrong.\n\n\n\nreturn_types = pd.DataFrame({\"return_type\": [\"monthly\", \"daily\"]})\n\nbeta_coverage = (\n    prices_monthly.merge(return_types, how=\"cross\")\n    .merge(beta, on=[\"symbol\", \"date\", \"return_type\"], how=\"left\")\n    .groupby([\"date\", \"return_type\"], as_index=False)\n    .apply(lambda x: pd.Series({\"share\": x[\"beta\"].notna().sum() / len(x)}))\n)\n\nbeta_coverage_figure = (\n    ggplot(\n        beta_coverage,\n        aes(x=\"date\", y=\"share\", color=\"return_type\", linetype=\"return_type\"),\n    )\n    + geom_line()\n    + labs(\n        x=\"\",\n        y=\"\",\n        color=\"\",\n        linetype=\"\",\n        title=\"End-of-month share of securities with beta estimates\",\n    )\n    + scale_y_continuous(labels=percent_format())\n    + scale_x_datetime(date_breaks=\"10 year\", date_labels=\"%Y\")\n)\nbeta_coverage_figure.show()\n\n\nFigure 9.5\n\n\n\nFigure 9.5 shows no issues, as the two coverage lines track each other closely, so we can proceed to the next check.\nWe also encourage everyone to always look at the distributional summary statistics of variables. You can easily spot outliers or weird distributions when looking at such tables.\n\n(beta\n    .groupby(\"return_type\")[\"beta\"]\n    .describe()\n    .round(2)\n)\n\nThe summary statistics also look plausible for the two estimation procedures.\nFinally, since we have two different estimators for the same theoretical object, we expect the estimators to be at least positively correlated (although not perfectly as the estimators are based on different sample periods and frequencies).\n\n(beta\n    .pivot_table(index=[\"symbol\", \"date\"], columns=\"return_type\", values=\"beta\")\n    .reset_index()\n    .get([\"monthly\", \"daily\"])\n    .corr()\n    .round(2)\n)\n\nIndeed, we find a positive correlation between our beta estimates. In the subsequent chapters, we mainly use the estimates based on monthly data, as most readers should be able to replicate them due to potential memory limitations that might arise with the daily data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#key-takeaways",
    "href": "08_beta_estimation.html#key-takeaways",
    "title": "9  Beta Estimation",
    "section": "9.6 Key Takeaways",
    "text": "9.6 Key Takeaways\n\nCAPM betas can be estimated using rolling-window estimation and processed in parallel via joblib.\nBoth monthly and daily return data can be used to estimate betas with different frequencies and window lengths, depending on the application.\nSummary statistics, visualization, and plausibility checks help to validate beta estimates across time and industries.\n\n\n\n\n\nLintner, John. 1965. “Security prices, risk, and maximal gains from diversification.” The Journal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital asset market.” Econometrica 34 (4): 768–83. https://doi.org/10.2307/1910098.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html",
    "href": "12_fama_french.html",
    "title": "10  Fama-French Factors",
    "section": "",
    "text": "10.1 Data Preparation\nIn this chapter, we provide a replication of the famous Fama-French factor portfolios. The Fama-French factor models are a cornerstone of empirical asset pricing Fama and French (2015). On top of the market factor represented by the traditional CAPM beta, the three-factor model includes the size and value factors to explain the cross section of returns. Its successor, the five-factor model, additionally includes profitability and investment as explanatory factors.\nWe start with the three-factor model. We already introduced the size and value factors in Value and Bivariate Sorts, and their definition remains the same: size is the SMB factor (small-minus-big) that is long small firms and short large firms. The value factor is HML (high-minus-low) and is long in high book-to-market firms and short in low book-to-market counterparts.\nAfter the replication of the three-factor model, we move to the five-factors by constructing the profitability factor RMW (robust-minus-weak) as the difference between the returns of firms with high and low operating profitability and the investment factor CMA (conservative-minus-aggressive) as the difference between firms with high versus low investment rates.\nThe current chapter relies on this set of Python packages.\nWe use Stock Data and Fundamentals as data sources, as we need exactly the same variables to compute the size and value factors in the way Fama and French do it.1\ntidy_finance = sqlite3.connect(\n  database=\"data/tidy_finance_python.sqlite\"\n)\n\nprices_monthly = (pd.read_sql_query(\n    sql=(\"SELECT symbol, date, ret_excess, mktcap, risk_free,\"\n         \"mktcap_lag FROM prices_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .dropna()\n)\n\ncomp_vn = (pd.read_sql_query(\n    sql=\"SELECT symbol, datadate, be, op, inv FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"})\n  .dropna()\n)\nFollowing Fama-French standards for empirical asset pricing:\nFollowing the Fama and French protocol, we identify the market capitalization in June of year \\(t\\). This represents the firm’s “Size” at the moment of portfolio formation. By assigning a sorting_date of July 1st, we ensure that the portfolio weights are determined using information strictly available before the subsequent July–June return period begins.\nTo calculate the Book-to-Market ratio, we extract the market capitalization from December of year \\(t-1\\). This specific timestamp is used to scale the book equity values. By standardizing this “Market Equity” (me) to the end of the previous calendar year, we maintain consistency across the entire cross-section of stocks, regardless of when their individual fiscal years might end.\nThe Book-to-Market (bm) ratio is constructed using the most recent fiscal year-end book equity (be) from the comp_vn dataset and the preceding December market equity. We applied a scaling factor (\\(10^9\\)) to the book equity to normalize absolute VND accounting values into the Billions-VND scale used by our market data. This ensures the ratio is unit-consistent and economically interpretable.\nIn the final step, we merge the Size and Value components into a single sorting_variables table using the symbol and sorting_date as keys. We apply a dropna() to ensure only firms with both valid price and accounting data are included, and drop_duplicates() to maintain a clean, single observation per stock-year. This structured output serves as the definitive source for calculating the breakpoints needed to categorize stocks into Small/Big and Value/Growth portfolios.\n# 1. Size (June Market Cap)\nsize = (prices_monthly\n  .query(\"date.dt.month == 6\")\n  # Use MonthBegin(1) to set to July 1st\n  .assign(sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(1))\n  .get([\"symbol\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"size\"})\n)\nsize.head(3)\n\n# 2. Market Equity (December Market Cap for BM scaling)\nmarket_equity = (prices_monthly\n  .query(\"date.dt.month == 12\")\n  # Shift December t-1 to July 1st of year t\n  .assign(sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(7))\n  .get([\"symbol\", \"sorting_date\", \"mktcap\"])\n  .rename(columns={\"mktcap\": \"me\"})\n)\nmarket_equity.head(3)\n\n# 3. Calculate Book-to-Market (BM) with Correct Scaling\nbook_to_market = (comp_vn\n    .assign(\n        sorting_date=lambda x: pd.to_datetime((x[\"datadate\"].dt.year + 1).astype(str) + \"-07-01\")\n    )\n    .merge(market_equity, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n    # Adjusted scaling: Dividing BE by 1,000,000,000 to convert absolute VND to Billions\n    # to match the scale of your Market Equity (me)\n    .assign(bm=lambda x: x[\"be\"] / (x[\"me\"] * 1000000000)) \n    [[\"symbol\", \"sorting_date\", \"me\", \"bm\"]]\n)\n\n# SANITY CHECK\nprint(f\"New Median BM Ratio: {book_to_market['bm'].median():.4f}\")\nbook_to_market.head(3)\n\n# Sanity Check: Print the median BM to ensure it is near 1.0\nprint(f\"Median BM Ratio: {book_to_market['bm'].median():.4f}\")\n\n# 4. Final Merge (This should now work)\nsorting_variables = (size\n  .merge(book_to_market, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"symbol\", \"sorting_date\"])\n)\nsorting_variables.head(3)\n\nNew Median BM Ratio: 1.1769\nMedian BM Ratio: 1.1769\n\n\n\n\n\n\n\n\n\nsymbol\nsorting_date\nsize\nme\nbm\n\n\n\n\n0\nA32\n2019-07-01\n153.00\n205.36\n0.977852\n\n\n1\nA32\n2020-07-01\n178.84\n190.40\n1.174437\n\n\n2\nA32\n2021-07-01\n217.60\n234.60\n1.032468",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#data-preparation",
    "href": "12_fama_french.html#data-preparation",
    "title": "10  Fama-French Factors",
    "section": "",
    "text": "Identification of Firm Size (June Market Cap)\n\n\n\nEstablishing the Market Equity Benchmark (December Market Cap)\n\n\n\nConstruction of the Book-to-Market Ratio\n\n\n\nFinal Data Integration and De-duplication\n\n\n\n\n10.1.1 Portfolio Sorts\nNext, we construct our portfolios with an adjusted assign_portfolio() function. Fama and French rely on specific breakpoints to independently form two portfolios in the size dimension at the median and three portfolios in the dimension of book-to-market at the 30 and 70 percentiles. The sorts for book-to-market require an adjustment to the function because we specify the exact percentiles as a list. Additionally, we perform the merge with our return data using a calculated sorting_date to ensure that portfolios formed in July are held constant until June of the following year.\n\ndef assign_portfolio(data, sorting_variable, percentiles):\n    \"\"\"Assign portfolios to a bin according to a sorting variable.\"\"\"\n    \n    # Calculate breakpoints based on quantile sequences\n    breakpoints = (data\n      .get(sorting_variable)\n      .quantile(percentiles, interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    \n    # Ensure the range covers all possible values\n    breakpoints.iloc[0] = -np.inf\n    breakpoints.iloc[breakpoints.size-1] = np.inf\n    \n    # Categorize into bins\n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=pd.Series(range(1, breakpoints.size)),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\n# 1. Assign Portfolios (Annual Sorts)\n# We calculate breakpoints and assign portfolios 1-2 for size and 1-3 for BM\nportfolios_assigned = (sorting_variables\n  .groupby(\"sorting_date\")\n  .apply(lambda x: x.assign(\n      portfolio_size=assign_portfolio(x, \"size\", [0, 0.5, 1]),\n      portfolio_bm=assign_portfolio(x, \"bm\", [0, 0.3, 0.7, 1])\n    ), include_groups=False)\n  .reset_index()\n  # We keep 'size' and 'bm' here so they are available after the merge\n  .get([\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\", \"size\", \"bm\"])\n)\n\n# 2. Merge Portfolios to Monthly Returns\n# Portfolios formed in July are held until June of the following year\nportfolios = (prices_monthly\n  .assign(\n    sorting_date=lambda x: pd.to_datetime(\n        np.where(x[\"date\"].dt.month &lt;= 6,\n                 (x[\"date\"].dt.year - 1).astype(str) + \"0701\",\n                 x[\"date\"].dt.year.astype(str) + \"0701\")\n    )\n  )\n  .merge(portfolios_assigned, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n)\n\n\nBreakpoint Determination: The assign_portfolio function calculates quantiles annually. For firm size, we use the 50th percentile (Median) to bifurcate the market into “Small” and “Big”. For book-to-market, we use the 30th and 70th percentiles to identify “Growth,” “Neutral,” and “Value” stocks.\nIndependent Sorting: We apply these breakpoints independently. This methodology allows a stock to be classified into one of six distinct \\(2 \\times 3\\) portfolios, facilitating the isolation of specific factor premiums.\nTemporal Alignment: Because financial statements in Vietnam are typically released by April, the July 1st sorting date ensures that the accounting information used in the bm ratio is publicly available before the portfolio return period begins.\nHolding Period Persistence: The sorting_date logic ensures that the portfolio assignments made in July remain constant for the next twelve months (July through the following June), consistent with the original Fama-French experimental design.\n\nSanity Check 1: Portfolio Distribution (The 2x3 Grid)\nFirst, we verify that the independent sorts created the expected six portfolios (2 size groups \\(\\times\\) 3 value groups).\n\n# Check the count of stocks in each portfolio combination for the most recent year\nportfolio_counts = (portfolios\n    .query(\"date == date.max()\")\n    .groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)\n    .size()\n    .unstack()\n)\nprint(\"Portfolio Counts (Size x BM):\")\nprint(portfolio_counts)\n\nPortfolio Counts (Size x BM):\nportfolio_bm      1    2    3\nportfolio_size               \n1               112  269  264\n2               275  247  124\n\n\nInterpretation:\nThe Fama-French methodology relies on having a sufficient number of stocks in each of the six bins to diversify idiosyncratic risk. In the Vietnam market, you should see a higher concentration in the “Small” size portfolios compared to “Big”. If any bin is empty or has fewer than 5-10 stocks, the factor returns (SMB and HML) for that period may be overly volatile or driven by a single outlier stock.\nSanity Check 2: Characteristic Monotonicity\nWe check if the average Book-to-Market ratio actually increases as we move from portfolio_bm 1 to 3.\n\n# Verify that higher portfolio numbers correspond to higher BM values\nbm_check = (portfolios\n    .groupby(\"portfolio_bm\", observed=True)\n    .agg({\"bm\": [\"mean\", \"median\", \"min\", \"max\"]})\n)\nprint(\"\\nBM Characteristic Check:\")\nprint(bm_check)\n\n\nBM Characteristic Check:\n                    bm                                \n                  mean    median       min         max\nportfolio_bm                                          \n1             0.597140  0.585077  0.001416    1.435144\n2             1.259447  1.186739  0.584759    2.737556\n3             3.376258  2.454002  1.018822  272.189334\n\n\nInterpretation:\nFor the sort to be valid, the mean and median bm must be strictly increasing across the portfolios (Portfolio 1 &lt; Portfolio 2 &lt; Portfolio 3). Since Portfolio 1 represents “Growth” (low BM) and Portfolio 3 represents “Value” (high BM), this check confirms that our assign_portfolio function correctly utilized the 30th and 70th percentiles.\nSanity Check 3: Holding Period Persistence\nWe verify that for a single stock, the portfolio assignment remains constant between July of one year and June of the next.\n\n# Trace a single symbol (e.g., 'A32') across a formation window\npersistence_check = (portfolios\n    .query(\"symbol == 'A32' & date &gt;= '2022-01-01' & date &lt;= '2023-12-31'\")\n    .sort_values(\"date\")\n    [['symbol', 'date', 'sorting_date', 'portfolio_size', 'portfolio_bm']]\n)\nprint(\"\\nTemporal Persistence Check (Symbol A32):\")\nprint(persistence_check.head(15))\n\n\nTemporal Persistence Check (Symbol A32):\n   symbol       date sorting_date portfolio_size portfolio_bm\n30    A32 2022-01-31   2021-07-01              1            2\n31    A32 2022-02-28   2021-07-01              1            2\n32    A32 2022-03-31   2021-07-01              1            2\n33    A32 2022-04-30   2021-07-01              1            2\n34    A32 2022-05-31   2021-07-01              1            2\n35    A32 2022-06-30   2021-07-01              1            2\n36    A32 2022-07-31   2022-07-01              1            3\n37    A32 2022-08-31   2022-07-01              1            3\n38    A32 2022-09-30   2022-07-01              1            3\n39    A32 2022-10-31   2022-07-01              1            3\n40    A32 2022-11-30   2022-07-01              1            3\n41    A32 2022-12-31   2022-07-01              1            3\n42    A32 2023-01-31   2022-07-01              1            3\n43    A32 2023-02-28   2022-07-01              1            3\n44    A32 2023-03-31   2022-07-01              1            3\n\n\nInterpretation:\nThis check ensures our sorting_date logic is working. You should observe that even as the date (monthly return date) changes, the portfolio_size and portfolio_bm remain identical from July through the following June. A change in assignment should only occur at the July 1st boundary when the new annual accounting data and June market caps are “baked into” the portfolios.\nFinal Summary of the 4-Step Process\n\nBreakpoint Calculation: We calculate the 50th percentile for Size and 30th/70th for BM annually, ensuring the “goalposts” move with the market’s overall valuation.\nIndependent Assignment: By sorting Size and BM separately, we create a matrix that allows us to see how a “Small Value” stock performs relative to a “Small Growth” stock.\nInformation Lag Handling: Using the July 1st sorting_date respects the reality of Vietnamese financial reporting, ensuring we don’t use “future” accounting data that wasn’t public yet.\nPortfolio Rebalancing: The annual rebalancing cycle (July–June) mimics the standard Fama-French experimental design, providing a rigorous framework for testing factor premiums in the Vietnam stock market.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#fama-french-three-factor-model",
    "href": "12_fama_french.html#fama-french-three-factor-model",
    "title": "10  Fama-French Factors",
    "section": "10.2 Fama-French Three-Factor Model",
    "text": "10.2 Fama-French Three-Factor Model\nEquipped with the return data and the assigned portfolios, we can now compute the value-weighted average return for each of the six portfolios. Then, we form the Fama-French factors. For the size factor (i.e., SMB), we go long in the three small portfolios and short the three large portfolios by taking an average across either group. For the value factor (i.e., HML), we go long in the two high book-to-market portfolios and short the two low book-to-market portfolios, again weighting them equally (using the mean() function).\n\nfactors = (portfolios\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"])\n  .apply(lambda x: pd.Series({\n    \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n   )\n  .reset_index()\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"smb\": (\n      x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_size\"] == 2].mean()),\n    \"hml\": (\n      x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() -\n        x[\"ret\"][x[\"portfolio_bm\"] == 1].mean())\n    }))\n  .reset_index()\n)\n\nfactors.head(3)\n\n\n\n\n\n\n\n\ndate\nsmb\nhml\n\n\n\n\n0\n2011-07-31\n-0.007768\n0.002754\n\n\n1\n2011-08-31\n-0.067309\n0.011474\n\n\n2\n2011-09-30\n0.014884\n0.022854\n\n\n\n\n\n\n\nThe market factor (\\(Mkt - RF\\)) is defined as the value-weighted return of all stocks in the investable universe minus the risk-free rate. Since the “market” is independent of how you sort your portfolios (Size, Value, etc.), the calculation remains identical regardless of whether you are building a 3-factor or 5-factor model.\n\n## Fama-French Three-Factor Model\n# --- Calculate Market Factor independently ---\n# This uses the entire prices_monthly universe to represent the broad market\nfactor_market_excess = (prices_monthly\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n      \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    }), include_groups=False)\n  .reset_index()\n)\n\n# --- Merge ---\n# Combine your replicated SMB/HML with the Market Factor\nfactors_ff3_monthly = (factors\n  .merge(factor_market_excess, on=\"date\", how=\"inner\")\n)\n\nfactors_ff3_monthly.head(3)\n\n\n\n\n\n\n\n\ndate\nsmb\nhml\nmkt_excess\n\n\n\n\n0\n2011-07-31\n-0.007768\n0.002754\n-0.067002\n\n\n1\n2011-08-31\n-0.067309\n0.011474\n0.049073\n\n\n2\n2011-09-30\n0.014884\n0.022854\n-0.017362\n\n\n\n\n\n\n\n\n# Remove rows with missing factor values\n# We keep only rows where the characteristic factors are fully populated\nfactors_ff3_monthly = (factors_ff3_monthly\n    .dropna(subset=[\"smb\", \"hml\", \"mkt_excess\"])\n    .reset_index(drop=True)\n)\n\n# Sanity Check\nprint(f\"Factors cleaned. Sample period: {factors_ff3_monthly['date'].min().date()} to {factors_ff3_monthly['date'].max().date()}\")\nprint(\"\\nFirst 3 rows of cleaned factors:\")\nprint(factors_ff3_monthly.head(3))\n\nFactors cleaned. Sample period: 2011-07-31 to 2023-12-31\n\nFirst 3 rows of cleaned factors:\n        date       smb       hml  mkt_excess\n0 2011-07-31 -0.007768  0.002754   -0.067002\n1 2011-08-31 -0.067309  0.011474    0.049073\n2 2011-09-30  0.014884  0.022854   -0.017362\n\n\n\n# --- Save to Database ---\n(factors_ff3_monthly\n  .to_sql(name=\"factors_ff3_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\n150",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#fama-french-five-factor-model",
    "href": "12_fama_french.html#fama-french-five-factor-model",
    "title": "10  Fama-French Factors",
    "section": "10.3 Fama-French Five-Factor Model",
    "text": "10.3 Fama-French Five-Factor Model\nNow, let us move to the replication of the five-factor model. We extend the other_sorting_variables table from above with the additional characteristics operating profitability op and investment inv. Note that the dropna() statement yields different sample sizes, as some firms with be values might not have op or inv values.\n\n# Ensure your BM median is ~1.17 and max is not 272\n# adjust the 1e9 based on your 'me' scale\nother_sorting_variables = (comp_vn\n  .assign(\n    sorting_date=lambda x: (pd.to_datetime(\n      (x[\"datadate\"].dt.year+1).astype(str)+\"0701\", format=\"%Y%m%d\")\n    )\n  )\n  .merge(market_equity, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n  .assign(bm=lambda x: x[\"be\"]/1e9/x[\"me\"])\n  .get([\"symbol\", \"sorting_date\", \"me\", \"bm\", \"op\", \"inv\"])\n)\n\nprint(other_sorting_variables['bm'].median())\n\n# Independent Size Sort\nsorting_variables = (size\n  .merge(other_sorting_variables, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n  .dropna()\n  .drop_duplicates(subset=[\"symbol\", \"sorting_date\"])\n )\n\n1.1769106858249534\n\n\nIn each month, we independently sort all stocks into the two size portfolios. The value, profitability, and investment portfolios, on the other hand, are the results of dependent sorts based on the size portfolios. We then merge the portfolios to the return data for the rest of the year just as above.\n\nfrom scipy.stats.mstats import winsorize\n\n# Winsorize Characteristics (1st and 99th percentiles)\n# This handles your extreme 272.18 BM values.\nvars_to_clean = [\"bm\", \"op\", \"inv\"]\nfor var in vars_to_clean:\n    sorting_variables[var] = winsorize(sorting_variables[var], limits=[0.01, 0.01])\n\n\n# Use transform to keep columns exactly as they are\nsorting_variables['portfolio_size'] = (sorting_variables\n  .groupby('sorting_date', group_keys=False)\n  .apply(lambda x: assign_portfolio(x, 'size', [0, 0.5, 1]))\n  .values\n)\n\n# Dependent Sorts (Sub-grouping)\n# We calculate each characteristic portfolio one by one to avoid KeyError\ndef dependent_sort(df, var, name):\n    return (df.groupby(['sorting_date', 'portfolio_size'], group_keys=False)\n            .apply(lambda x: assign_portfolio(x, var, [0, 0.3, 0.7, 1])))\n\nsorting_variables['portfolio_bm'] = dependent_sort(sorting_variables, 'bm', 'portfolio_bm').values\nsorting_variables['portfolio_op'] = dependent_sort(sorting_variables, 'op', 'portfolio_op').values\nsorting_variables['portfolio_inv'] = dependent_sort(sorting_variables, 'inv', 'portfolio_inv').values\n\n# --- Column Selection for Sorting Variables ---\nportfolios_sorting = sorting_variables.get([\n    \"symbol\", \"sorting_date\", \"portfolio_size\", \n    \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"\n])\nportfolios_sorting.head(3)\n\n\n\n\n\n\n\n\nsymbol\nsorting_date\nportfolio_size\nportfolio_bm\nportfolio_op\nportfolio_inv\n\n\n\n\n0\nA32\n2019-07-01\n2\n2\n3\n2\n\n\n1\nA32\n2020-07-01\n2\n2\n2\n3\n\n\n2\nA32\n2021-07-01\n2\n2\n2\n1\n\n\n\n\n\n\n\nStep-by-Step Interpretation\n\nSize-Based Stratification: We first partition the market into “Small” and “Big” portfolios using the median market capitalization. This provides the primary dimension for the subsequent dependent sorts.\nConditional Characteristic Sorting: Unlike the independent sorts of the 3-factor model, the 5-factor model employs dependent sorts for Value (BM), Profitability (OP), and Investment (INV). By sorting these variables within size groups, we ensure that a firm’s classification (e.g., “Robust Profitability”) is relative to its size peers, which controls for the different financial distributions across small and large Vietnamese firms.\nTemporal Synchronization: We maintain the July 1st formation date to accommodate the Vietnamese corporate reporting landscape, where audited annual results are typically finalized by late April. This creates a conservative two-month buffer, ensuring all accounting data used for sorting was publicly available at the time of portfolio formation.\nPortfolio Persistence: The resulting assignments are held for a 12-month period (July through June). This rebalancing frequency is standard for identifying long-term risk premiums and avoids the excessive transaction costs associated with more frequent turnover.\n\nCheck 1: Characteristic Monotonicity\nThis confirms the sort effectively separated stocks by their economic quality.\n\n# Verify that higher portfolio numbers have higher median characteristics\nsanity_check = (sorting_variables\n    .groupby(\"portfolio_op\")[\"op\"].median()\n)\nprint(\"Median OP by Portfolio (Should be strictly increasing):\")\nprint(sanity_check)\n\nMedian OP by Portfolio (Should be strictly increasing):\nportfolio_op\n1    0.139804\n2    0.133405\n3    0.138398\nName: op, dtype: float64\n\n\nInterpretation: If Portfolio 1 (Weak) has a lower median OP than Portfolio 3 (Robust), your sort is economically valid. If they are the same or reversed, the assign_portfolio logic has failed to identify the “quality” spread in the Vietnam market.\nCheck 2: Bin Diversification\nThis ensures each factor is built on enough stocks to be statistically reliable.\n\n# Check the 2x3 grid for Profitability\nrmw_counts = sorting_variables.groupby([\"portfolio_size\", \"portfolio_op\"]).size().unstack()\nprint(\"\\nStocks per Size/Profitability Bin:\")\nprint(rmw_counts)\n\n\nStocks per Size/Profitability Bin:\nportfolio_op       1     2     3\nportfolio_size                  \n1               1825  2369  1819\n2               1789  2434  1796\n\n\nInterpretation: Your check showed roughly 1,800 stocks per bin. This is excellent for Vietnam and suggests your RMW (Robust Minus Weak) factor will be very stable and well-diversified.\nNow, we want to construct each of the factors, but this time, the size factor actually comes last because it is the result of averaging across all other factor portfolios. This dependency is the reason why we keep the table with value-weighted portfolio returns as a separate object that we reuse later. We construct the value factor, HML, as above by going long the two portfolios with high book-to-market ratios and shorting the two portfolios with low book-to-market.\n\n# --- Merge with FULL monthly return data ---\n# This step is crucial. It adds 'date', 'ret_excess', and 'mktcap_lag'\n# which are missing from your current 'portfolios' object.\nportfolios_full = (prices_monthly\n  .assign(\n    sorting_date=lambda x: pd.to_datetime(\n      np.where(x[\"date\"].dt.month &lt;= 6, \n               (x[\"date\"].dt.year - 1).astype(str) + \"0701\", \n               x[\"date\"].dt.year.astype(str) + \"0701\")\n    )\n  )\n  .merge(portfolios_sorting, how=\"inner\", on=[\"symbol\", \"sorting_date\"])\n)\n\n# --- Construct the Value Factor (HML) ---\nportfolios_value = (portfolios_full\n  .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], group_keys=False)\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_value = (portfolios_value\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"hml\": (\n      x[\"ret\"][x[\"portfolio_bm\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_bm\"] == 1].mean())})\n  , include_groups=False)\n  .reset_index()\n)\n\nFor the profitability factor, RMW (robust-minus-weak), we take a long position in the two high profitability portfolios and a short position in the two low profitability portfolios.\n\n# --- Construct the Profitability Factor (RMW) ---\nportfolios_profitability = (portfolios_full\n  .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], group_keys=False)\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_profitability = (portfolios_profitability\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"rmw\": (\n      x[\"ret\"][x[\"portfolio_op\"] == 3].mean() - \n        x[\"ret\"][x[\"portfolio_op\"] == 1].mean())})\n  , include_groups=False)\n  .reset_index()\n)\n\nFor the investment factor, CMA (conservative-minus-aggressive), we go long the two low investment portfolios and short the two high investment portfolios.\n\n# --- 5. Construct the Investment Factor (CMA) ---\nportfolios_investment = (portfolios_full\n  .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], group_keys=False)\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nfactors_investment = (portfolios_investment\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"cma\": (\n      x[\"ret\"][x[\"portfolio_inv\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_inv\"] == 3].mean())})\n  , include_groups=False)\n  .reset_index()\n)\n\nFinally, the size factor, SMB, is constructed by going long the nine small portfolios and short the nine large portfolios.\n\n# --- Construct the Size Factor (SMB) ---\nfactors_size = (\n  pd.concat(\n    [portfolios_value, portfolios_profitability, portfolios_investment], \n    ignore_index=True\n  )\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"smb\": (\n      x[\"ret\"][x[\"portfolio_size\"] == 1].mean() - \n        x[\"ret\"][x[\"portfolio_size\"] == 2].mean())})\n  , include_groups=False)\n  .reset_index()\n)\n\nThe market factor (\\(Mkt - RF\\)) is defined as the value-weighted return of all stocks in the investable universe minus the risk-free rate. Since the “market” is independent of how you sort your portfolios (Size, Value, etc.), the calculation remains identical regardless of whether you are building a 3-factor or 5-factor model.\n\n# --- Calculate Market Factor independently ---\n# This uses the entire prices_monthly universe\nfactor_market_excess = (prices_monthly\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n      \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    }), include_groups=False)\n  .reset_index()\n)\n\nWe then join all factors together into one dataframe and construct again a suitable table to run tests for evaluating our replication.\n\nfactors = (factors_size\n  .merge(factors_value, how=\"outer\", on=\"date\")\n  .merge(factors_profitability, how=\"outer\", on=\"date\")\n  .merge(factors_investment, how=\"outer\", on=\"date\")\n  .merge(factor_market_excess, how=\"outer\", on=\"date\")\n  \n)\n\n# Check correlations\nprint(\"Factor Correlation Matrix (Vietnam):\")\nprint(factors.drop(columns=\"date\").corr())\n\nFactor Correlation Matrix (Vietnam):\n                 smb       hml       rmw       cma  mkt_excess\nsmb         1.000000  0.215982 -0.045441  0.145660    0.004216\nhml         0.215982  1.000000 -0.402912  0.042314   -0.007630\nrmw        -0.045441 -0.402912  1.000000 -0.237680   -0.077586\ncma         0.145660  0.042314 -0.237680  1.000000    0.274396\nmkt_excess  0.004216 -0.007630 -0.077586  0.274396    1.000000\n\n\nInterpretation: In standard markets, we expect low correlations between factors. If HML and RMW are correlated above 0.8, it may suggest that “Value” and “Profitability” are capturing the same firms in Vietnam, which might happen if the market is less mature.\n\nFactor Volatility and Means Factors should have a low but generally positive mean return over long periods.\n\n\n# Check the average monthly premium of each factor\nprint(\"Average Monthly Factor Premiums:\")\nprint(factors.drop(columns=\"date\").mean() * 100) # In percent\n\nAverage Monthly Factor Premiums:\nsmb          -0.366683\nhml          -0.272497\nrmw           0.066485\ncma           0.166616\nmkt_excess   -1.297145\ndtype: float64\n\n\nIf a factor mean is extremely high (e.g., &gt; 5% per month), it may indicate that a few outliers (like that 272 BM stock) are still leaking into the weighted averages.\n\nVisual Persistence Check\n\nCumulative returns should show the “growth” of $1 invested in the factor.\n\nimport matplotlib.pyplot as plt\n\nfactors_cum = (factors.set_index(\"date\")\n  .add(1).cumprod()\n)\nfactors_cum.plot(figsize=(10, 6), title=\"Cumulative Factor Growth (Vietnam)\")\nplt.ylabel(\"Value of $1 Investment\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Remove rows with missing factor values\n# We keep only rows where the characteristic factors are fully populated\nfactors_ff5_monthly = (factors\n    .dropna(subset=[\"smb\", \"hml\", \"rmw\", \"cma\", \"mkt_excess\"])\n    .reset_index(drop=True)\n)\n\n# Sanity Check\nprint(f\"Factors cleaned. Sample period: {factors_ff5_monthly['date'].min().date()} to {factors_ff5_monthly['date'].max().date()}\")\nprint(\"\\nFirst 3 rows of cleaned factors:\")\nprint(factors_ff5_monthly.head(3))\n\nFactors cleaned. Sample period: 2011-07-31 to 2023-12-31\n\nFirst 3 rows of cleaned factors:\n        date       smb       hml       rmw       cma  mkt_excess\n0 2011-07-31  0.029280 -0.049915  0.013239 -0.025244   -0.067002\n1 2011-08-31  0.022241 -0.021097  0.001543 -0.021929    0.049073\n2 2011-09-30  0.026609  0.014323  0.025125  0.003444   -0.017362\n\n\n\n(factors_ff5_monthly\n  .to_sql(name=\"factors_ff5_monthly\", \n          con=tidy_finance, \n          if_exists=\"replace\",\n          index=False)\n)\n\n150",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#key-takeaways",
    "href": "12_fama_french.html#key-takeaways",
    "title": "10  Fama-French Factors",
    "section": "10.4 Key Takeaways",
    "text": "10.4 Key Takeaways\n\nThe three-factor model adds size (SMB) and value (HML) to the traditional CAPM, while the five-factor model extends this with profitability (RMW) and investment (CMA) factors.\nThe portfolio construction follows the original Fama-French methodology, including NYSE breakpoints, specific time lags, and sorting rules based on firm characteristics.\nThe quality of replication can be evaluated using regression analysis and confirms strong alignment with the original Fama-French data.\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock returns.” The Journal of Finance 47 (2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#footnotes",
    "href": "12_fama_french.html#footnotes",
    "title": "10  Fama-French Factors",
    "section": "",
    "text": "Note that Fama and French (1992) claim to exclude financial firms. To a large extent this happens through using industry format “INDL”. Neither the original paper, nor Ken French’s website, or the WRDS replication contains any indication that financial companies are excluded using additional filters such as industry codes.↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "20_conclusion.html",
    "href": "20_conclusion.html",
    "title": "11  Conclusion",
    "section": "",
    "text": "11.1 What you should take away\nEmpirical finance in emerging and frontier markets is often judged less by the elegance of an estimator than by the credibility of its inputs and the transparency of its decisions. Vietnam makes this point vividly: trading venues and regulatory regimes have evolved quickly, firm coverage can be uneven across time, corporate actions need careful treatment, and accounting conventions require close attention to timing and comparability. Those features do not prevent high-quality research; they simply shift the center of gravity toward reproducible data engineering, auditable transformations, and clear identification of assumptions.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "20_conclusion.html#what-you-should-take-away",
    "href": "20_conclusion.html#what-you-should-take-away",
    "title": "11  Conclusion",
    "section": "",
    "text": "11.1.1 Reproducibility is an identification strategy\nIn textbook settings, identification focuses on variation and exogeneity. In real-world market data, identification also depends on whether your dataset is the same dataset when you rerun the work next month or next year. The practical discipline of versioned inputs, deterministic transformations, and documented filters reduces the scope for accidental \\(p\\)-hacking and silent sample drift (e.g., survivorship bias from symbol changes or late-arriving delistings). Reproducible workflows are not administrative overhead; they are a commitment device that makes results more trustworthy and easier to challenge constructively (Peng 2011; Sandve et al. 2013).\n\n\n11.1.2 Vietnam rewards “microstructure humility”\nThe chapters on returns, beta estimation, and factor construction emphasized that naïve carryover of developed-market defaults can be costly. Thin trading, price limits, lot-size rules, and regime changes mean that decisions like (i) return interval, (ii) stale-price handling, (iii) corporate-action adjustment, and (iv) portfolio formation frequency can materially change inference. This is not a Vietnam-only phenomenon, but it is more visible there, and therefore a useful laboratory for best practices in emerging markets.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "20_conclusion.html#a-reproducibility-checklist-you-can-actually-use",
    "href": "20_conclusion.html#a-reproducibility-checklist-you-can-actually-use",
    "title": "11  Conclusion",
    "section": "11.2 A reproducibility checklist you can actually use",
    "text": "11.2 A reproducibility checklist you can actually use\nThe list below is designed to be operational: each item can be verified in a repository review.\n\n\n\nTable 11.1: Reproducibility deliverables for research\n\n\n\n\n\n\n\n\n\n\nDeliverable\nWhat “done” looks like\nWhere it lives\n\n\n\n\nDeterministic transforms\nSame raw inputs yield identical normalized outputs\nR/transform_*.R (or python/transform_*.py)\n\n\nTest suite\nCoverage, identity, and corporate-action tests run in CI\ntests/ + CI config\n\n\nData dictionary\nTables/fields documented with units, timing, and keys\ndocs/dictionary.qmd\n\n\nResearch log\nAll key design choices recorded (filters, winsorization, periods)\nnotes/research_log.md\n\n\nArtifact registry\nEvery figure/table has a script and a checksum\nartifacts/manifest.json",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Carhart, Mark M. 1997. “On persistence in\nmutual fund performance.” The Journal of\nFinance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock\nreturns.” The Journal of Finance 47\n(2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal\nof Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nGentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for\nthe Social Sciences: A Practitioner’s Guide.” Working Paper,\nUniversity of Chicago.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2014. “Digesting anomalies: An investment\napproach.” Review of Financial\nStudies 28 (3): 650–705. https://doi.org/10.1093/rfs/hhu068.\n\n\nJagannathan, Ravi, and Zhenyu Wang. 1996. “The conditional CAPM and the cross-section of expected\nreturns.” The Journal of Finance 51\n(1): 3–53. https://doi.org/10.2307/2329301.\n\n\nLintner, John. 1965. “Security prices, risk,\nand maximal gains from diversification.” The\nJournal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMarkowitz, Harry. 1952. “Portfolio\nselection.” The Journal of Finance 7\n(1): 77–91. https://doi.org/10.1111/j.1540-6261.1952.tb01525.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital\nasset market.” Econometrica 34 (4):\n768–83. https://doi.org/10.2307/1910098.\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig.\n2013. “Ten Simple Rules for Reproducible Computational\nResearch.” PLoS Computational Biology 9 (10): e1003285.\n\n\nSharpe, William F. 1964. “Capital asset\nprices: A theory of market equilibrium under conditions of risk\n.” The Journal of Finance 19 (3):\n425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.\n\n\nVilhuber, Lars. 2020. “Reproducibility and Replicability in\nEconomics.” Harvard Data Science Review 2 (4): 1–39.",
    "crumbs": [
      "References"
    ]
  }
]