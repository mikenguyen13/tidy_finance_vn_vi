[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidy Finance in Vietnam",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Tidy Finance in Vietnam",
    "section": "Motivation",
    "text": "Motivation\nEmpirical finance has undergone a fundamental transformation over the past two decades. Advances in computational capacity, open-source statistical software, and data availability have reshaped how financial research is conducted, evaluated, and disseminated. Increasingly, credible empirical work is expected to be transparent, replicable, and extensible, with results generated through scripted workflows rather than manual intervention. Reproducibility, defined as the ability for independent researchers to regenerate empirical results using the same data and methods, has thus become a core norm in modern financial economics.\nDespite this progress, the adoption of reproducible research practices has been uneven across markets. In developed financial systems, particularly those with long-established databases and standardized reporting regimes, reproducible empirical workflows are now commonplace. In contrast, research on emerging and frontier markets frequently relies on fragmented datasets, undocumented data cleaning procedures, and implicit institutional assumptions that are difficult to verify or extend. As a result, empirical findings in these markets are often fragile, non-comparable across studies, and costly to update as new data become available.\nThis book addresses that gap.\nIt develops a reproducible empirical finance framework designed explicitly for emerging and frontier markets, using Vietnam as a primary empirical case. Rather than adapting developed-market research pipelines post hoc, the book begins from the institutional and data realities of a fast-growing, retail-dominated, regulation-intensive market and builds methodological solutions accordingly. The objective is not merely to analyze Vietnam’s financial markets, but to demonstrate how reproducible finance principles can be extended, stress-tested, and refined in environments characterized by data scarcity, institutional heterogeneity, and rapid structural change.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-emerging-markets-require-different-empirical-infrastructure",
    "href": "index.html#why-emerging-markets-require-different-empirical-infrastructure",
    "title": "Tidy Finance in Vietnam",
    "section": "Why Emerging Markets Require Different Empirical Infrastructure",
    "text": "Why Emerging Markets Require Different Empirical Infrastructure\nMuch of modern empirical finance implicitly assumes the existence of stable, high-frequency, institutionally harmonized datasets. These assumptions are rarely stated, yet they are deeply embedded in standard research designs: survivorship-free security histories, consistent accounting standards, unrestricted trading mechanisms, and deep institutional liquidity.\nEmerging and frontier markets challenge each of these assumptions.\nIn Vietnam, as in many comparable economies, equity markets exhibit binding daily price limits, episodic trading halts, concentrated state ownership, and a predominance of retail investors. Financial disclosures reflect local accounting standards and evolving regulatory frameworks. Corporate actions are frequent, inconsistently documented, and occasionally revised ex post.\nThese characteristics are not inconveniences to be eliminated through aggressive data cleaning. They shape return dynamics, risk premia, factor construction, and statistical inference itself. An empirical framework that ignores these institutional features risks producing results that are internally inconsistent or externally misleading. A reproducible approach for emerging markets must therefore encode institutional context directly into data schemas, transformation logic, and modeling choices.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#reproducibility-as-a-research-design-principle",
    "href": "index.html#reproducibility-as-a-research-design-principle",
    "title": "Tidy Finance in Vietnam",
    "section": "Reproducibility as a Research Design Principle",
    "text": "Reproducibility as a Research Design Principle\nIn this book, reproducibility extends beyond the narrow notion of code availability. It is treated as an organizing principle governing the entire empirical research lifecycle.\nFirst, all datasets are constructed from raw inputs through documented, deterministic transformations, ensuring clear data provenance. Second, empirical methods are implemented in a manner that makes modeling assumptions explicit and modifiable. Third, results are generated through scripted pipelines rather than interactive analysis, guaranteeing that updates to data or parameters propagate consistently throughout the analysis. Finally, empirical designs are modular, allowing researchers to substitute markets, sample periods, or variable definitions without rewriting entire workflows.\nThis approach draws methodological inspiration from the broader reproducible research movement in economics and finance (e.g., Gentzkow and Shapiro (2014); Vilhuber (2020)), while deliberately extending it beyond its original institutional and data environment. The goal is not to reproduce existing studies, but to enable new ones, particularly those that would otherwise be impractical due to fragmented data and institutional complexity.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#vietnam-as-a-case-not-an-exception",
    "href": "index.html#vietnam-as-a-case-not-an-exception",
    "title": "Tidy Finance in Vietnam",
    "section": "Vietnam as a Case, Not an Exception",
    "text": "Vietnam as a Case, Not an Exception\nVietnam serves as the central empirical case throughout the book, but it is not treated as an idiosyncratic exception. Instead, it is presented as a representative example of a class of markets that occupy an intermediate position between frontier and emerging status: large enough to sustain active equity trading, yet still evolving in terms of regulation, disclosure quality, and investor composition.\nBy grounding methodological development in Vietnam’s market structure, the book aims to produce insights that generalize to other contexts, including Southeast Asia, South Asia, Sub-Saharan Africa, and parts of Latin America. Each empirical chapter emphasizes which components are market-specific and which are portable, encouraging readers to adapt the framework rather than adopt it wholesale.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contribution-and-audience",
    "href": "index.html#contribution-and-audience",
    "title": "Tidy Finance in Vietnam",
    "section": "Contribution and Audience",
    "text": "Contribution and Audience\nThis book makes three primary contributions.\nFirst, it proposes a reproducible empirical finance framework explicitly designed for emerging and frontier markets, integrating institutional detail into data construction and model design. Second, it provides original empirical evidence on asset pricing, liquidity, and market microstructure in Vietnam using consistently constructed datasets. Third, it delivers publication-ready, end-to-end research workflows suitable for academic research, policy analysis, and applied financial work.\nThe intended audience includes graduate students in finance and economics, academic researchers working on non-developed markets, and practitioners interested in systematic analysis of emerging market equities. Familiarity with basic asset pricing theory and statistical programming is assumed, but no prior experience with Vietnam or similar markets is required.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-book",
    "href": "index.html#structure-of-the-book",
    "title": "Tidy Finance in Vietnam",
    "section": "Structure of the Book",
    "text": "Structure of the Book\nThe chapters that follow progress from data infrastructure to empirical application. Early chapters focus on institutional context, data construction, and reproducible workflow design. Subsequent chapters develop asset pricing tests, liquidity measures, and market microstructure analyses tailored to Vietnam’s equity market. Each chapter is designed to be self-contained, yet all are linked through a common data and code architecture to ensure internal consistency.\nThe book concludes by reflecting on the broader implications of reproducible empirical finance for emerging markets research and by outlining directions for future methodological and empirical work.\n\n\n\n\n\n\nGentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for the Social Sciences: A Practitioner’s Guide.” Working Paper, University of Chicago.\n\n\nVilhuber, Lars. 2020. “Reproducibility and Replicability in Economics.” Harvard Data Science Review 2 (4): 1–39.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00_institutional_background.html",
    "href": "00_institutional_background.html",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "",
    "text": "1.1 Evolution of Vietnam’s Equity Market\nEmpirical analysis of financial markets is inseparable from institutional context. Market design, regulatory constraints, ownership structure, and investor composition shape observed prices, volumes, and returns. In developed markets, many of these features are sufficiently stable and standardized that they fade into the background of empirical research. In emerging markets, by contrast, institutional features are often first-order determinants of empirical outcomes.\nThis chapter provides the institutional foundation for the empirical analyses developed later in the book. It describes the structure of Vietnam’s equity market, the regulatory environment governing trading and disclosure, and the characteristics of listed firms and investors. Rather than offering a purely descriptive account, the discussion emphasizes how institutional features map directly into data construction choices, modeling assumptions, and interpretation of empirical results.\nVietnam’s modern equity market is relatively young. Formal stock exchanges were established only in the early 2000s, as part of broader economic reforms aimed at transitioning from a centrally planned system toward a market-oriented economy. Since then, market capitalization, trading volume, and the number of listed firms have grown rapidly, albeit unevenly across sectors and time.\nThe pace of market development has been shaped by a combination of gradual privatization of state-owned enterprises, episodic regulatory reform, and sustained participation by retail investors. Unlike markets that evolved alongside large institutional investor bases, Vietnam’s equity market matured in an environment where individual investors dominate trading activity and informational asymmetries remain substantial.\nThese features have important empirical implications. Return dynamics may reflect behavioral trading patterns, liquidity shocks can be amplified by coordinated retail activity, and firm-level information is incorporated into prices at varying speeds. A reproducible empirical framework must therefore be capable of capturing these dynamics without imposing assumptions derived from institutionally different markets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#exchange-structure-and-trading-mechanisms",
    "href": "00_institutional_background.html#exchange-structure-and-trading-mechanisms",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.2 Exchange Structure and Trading Mechanisms",
    "text": "1.2 Exchange Structure and Trading Mechanisms\nVietnam operates multiple equity exchanges, each with distinct listing requirements and trading rules. Trading is conducted through a centralized limit order book, with price-time priority determining execution. Importantly, daily price limits constrain the maximum allowable price movement for individual securities. These limits vary by exchange and security type and are binding during periods of heightened volatility.\nPrice limits introduce mechanical truncation in observed returns, clustering at upper and lower bounds, and persistence in price movements across days. From an empirical perspective, this challenges standard assumptions about continuous price adjustment and complicates volatility estimation, momentum measurement, and event-study design.\nIn this book, price limits are treated as structural features rather than anomalies. Data pipelines explicitly preserve limit-hit indicators, and empirical models are adapted to account for constrained price dynamics. This design choice reflects a broader principle: reproducibility in emerging markets requires preserving institutional signals rather than smoothing them away.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#listing-requirements-and-firm-characteristics",
    "href": "00_institutional_background.html#listing-requirements-and-firm-characteristics",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.3 Listing Requirements and Firm Characteristics",
    "text": "1.3 Listing Requirements and Firm Characteristics\nListed firms in Vietnam exhibit substantial heterogeneity in size, ownership structure, and disclosure quality. A defining characteristic of the market is the prevalence of firms with significant state ownership, either directly or through affiliated entities. State ownership affects governance, dividend policy, risk-taking behavior, and responsiveness to market signals.\nAccounting disclosures follow Vietnamese Accounting Standards, which differ in important respects from international standards. While convergence efforts are ongoing, historical financial statements often reflect transitional rules, incomplete adoption of fair value accounting, and limited segment reporting. These features complicate cross-firm comparability and longitudinal analysis.\nFrom a reproducible research standpoint, accounting variables cannot be treated as uniform primitives. Variable definitions, reporting lags, and restatement practices must be explicitly documented and encoded into data construction logic. Later chapters demonstrate how accounting data are harmonized in a transparent, version-controlled manner without obscuring underlying institutional differences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#investor-composition-and-trading-behavior",
    "href": "00_institutional_background.html#investor-composition-and-trading-behavior",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.4 Investor Composition and Trading Behavior",
    "text": "1.4 Investor Composition and Trading Behavior\nRetail investors dominate trading volume in Vietnam’s equity market. Institutional investors, including domestic funds and foreign participants, play a growing but still secondary role. This investor composition has implications for liquidity provision, price discovery, and market stability.\nRetail-dominated markets tend to exhibit higher turnover, episodic herding behavior, and sensitivity to non-fundamental information. These patterns affect the interpretation of empirical results, particularly in studies of short-term return predictability, volume-return relations, and volatility clustering.\nRather than assuming institutional trading as the default, this book explicitly models liquidity and trading activity in a retail-centric environment. Measures of liquidity, for example, are chosen and constructed to remain meaningful in the presence of small trade sizes, intermittent trading, and order imbalances driven by individual investors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#regulatory-environment-and-market-frictions",
    "href": "00_institutional_background.html#regulatory-environment-and-market-frictions",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.5 Regulatory Environment and Market Frictions",
    "text": "1.5 Regulatory Environment and Market Frictions\nRegulatory oversight of Vietnam’s equity market has evolved alongside market development. Trading rules, disclosure requirements, and foreign ownership limits have been periodically revised, sometimes with limited backward compatibility. Regulatory changes can induce structural breaks in data that are not immediately apparent in raw time series.\nShort-selling constraints, limited securities lending, and restrictions on derivative usage further distinguish Vietnam’s market from developed counterparts. These frictions affect arbitrage activity and the feasibility of certain trading strategies, influencing observed return patterns and factor realizations.\nA key principle of the empirical framework developed in this book is regulatory awareness. Data pipelines incorporate regulatory timelines, and empirical tests are designed to be robust to rule changes. This ensures that results are interpretable within the institutional regime in which they arise.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#implications-for-empirical-design",
    "href": "00_institutional_background.html#implications-for-empirical-design",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.6 Implications for Empirical Design",
    "text": "1.6 Implications for Empirical Design\nThe institutional features described in this chapter motivate several design choices that recur throughout the book:\n\nData preservation over simplification: Institutional constraints such as price limits and trading halts are retained and explicitly modeled.\nModular variable construction: Accounting and market variables are constructed through transparent functions that can be adjusted as standards evolve.\nRegime sensitivity: Empirical analyses are structured to detect and accommodate regulatory and structural breaks.\nContext-aware interpretation: Results are interpreted in light of market structure rather than benchmarked mechanically against developed-market findings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#summary",
    "href": "00_institutional_background.html#summary",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nVietnam’s equity market combines rapid growth with distinctive institutional features that challenge conventional empirical finance methods. Price limits, retail investor dominance, state ownership, and evolving regulation shape market outcomes in ways that cannot be ignored or abstracted away. For researchers working in such environments, reproducibility requires more than clean code and documented data—it requires embedding institutional context directly into empirical design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html",
    "href": "01_working_with_stock_returns.html",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "",
    "text": "2.1 Data Access and Preparation\nThis chapter develops a practical framework for transforming raw equity price records into return series suitable for empirical financial analysis. The focus is on methodological clarity and reproducibility, with particular attention to data issues that are prevalent in emerging equity markets such as Vietnam.\nThe discussion proceeds from individual stocks to a broad market cross-section, using constituents of the VN30 index as the primary empirical setting.\nWe begin by loading the core numerical and data manipulation libraries. These provide all functionality required for return construction without relying on specialized financial wrappers.\nimport pandas as pd\nimport numpy as np\nHistorical price data are stored in an S3-compatible object storage system. Access credentials are supplied via environment variables, which keeps sensitive information separate from the analysis and supports collaborative reproducibility.\nimport os\nimport boto3\nfrom botocore.client import Config\n\n\nclass ObjectStorage:\n    def __init__(self):\n        self.client = boto3.client(\n            \"s3\",\n            endpoint_url=os.environ[\"MINIO_ENDPOINT\"],\n            aws_access_key_id=os.environ[\"MINIO_ACCESS_KEY\"],\n            aws_secret_access_key=os.environ[\"MINIO_SECRET_KEY\"],\n            region_name=os.getenv(\"MINIO_REGION\", \"us-east-1\"),\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n\nstorage = ObjectStorage()\nbucket = os.environ[\"MINIO_BUCKET\"]\nThe daily price file is read directly into memory. We explicitly parse dates and harmonize variable names to avoid ambiguity in later steps.\nfrom io import BytesIO\n\nprices = pd.read_csv(\n    BytesIO(\n        storage.client.get_object(\n            Bucket=bucket,\n            Key=\"historycal_price/dataset_historical_price.csv\",\n        )[\"Body\"].read()\n    ),\n    low_memory=False,\n)\n\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\n\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\n\nprices = prices.rename(\n    columns={\n        \"vol_total\": \"volume\",\n        \"open_price\": \"open\",\n        \"low_price\": \"low\",\n        \"high_price\": \"high\",\n        \"close_price\": \"close\",\n    }\n)\n\nprices = prices.sort_values([\"symbol\", \"date\"])\nAdjusted closing prices incorporate mechanical changes due to corporate actions such as cash dividends and stock splits. Using adjusted prices ensures that subsequent return calculations reflect investor-relevant performance rather than accounting artifacts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#examining-a-single-equity",
    "href": "01_working_with_stock_returns.html#examining-a-single-equity",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.2 Examining a Single Equity",
    "text": "2.2 Examining a Single Equity\nTo ground the discussion, we isolate the trading history of a single large-cap stock, FPT, over a long sample period.\n\nimport datetime as dt\n\nstart = pd.Timestamp(\"2000-01-01\")\nend = pd.Timestamp(dt.date.today().year - 1, 12, 31)\n\n\nfpt = prices.loc[\n    (prices[\"symbol\"] == \"FPT\")\n    & (prices[\"date\"] &gt;= start)\n    & (prices[\"date\"] &lt;= end),\n    [\"date\", \"symbol\", \"volume\", \"open\", \"low\", \"high\", \"close\", \"adjusted_close\"],\n].copy()\n\nThis subset contains the standard daily market variables required for most empirical studies. Before computing returns, it is good practice to visually inspect the price series.\n\nfrom plotnine import ggplot, aes, geom_line, labs\n\n\n(\n    ggplot(fpt, aes(x=\"date\", y=\"adjusted_close\"))\n    + geom_line()\n    + labs(title=\"Adjusted price path of FPT\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 2.1: Prices are in VND, adjusted for dividend payments and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#from-prices-to-returns",
    "href": "01_working_with_stock_returns.html#from-prices-to-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.3 From Prices to Returns",
    "text": "2.3 From Prices to Returns\nMost empirical asset pricing models are formulated in terms of returns rather than price levels. The simple daily return is defined as\n\\[\nr_t = \\frac{p_t}{p_t - 1} - 1,\n\\]\nwhere \\(p_t\\) denotes the adjusted closing price at the end of trading day \\(t\\).\nBefore computing returns, we must address invalid price observations. In Vietnamese equity data, adjusted prices occasionally take the value zero. These entries typically arise from IPO placeholders, trading suspensions, or historical backfilling conventions and cannot be used to compute meaningful returns.\n\nprices.loc[prices[\"adjusted_close\"] &lt;= 0, [\"symbol\", \"date\", \"adjusted_close\"]].head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\n\n\n\n\n33886\nADP\n2010-02-09\n0.0\n\n\n33887\nADP\n2010-02-24\n0.0\n\n\n33888\nADP\n2010-03-01\n0.0\n\n\n33889\nADP\n2010-03-03\n0.0\n\n\n33890\nADP\n2010-03-12\n0.0\n\n\n\n\n\n\n\nWe therefore exclude non-positive adjusted prices and compute returns stock by stock. Correct chronological ordering is essential.\n\nreturns = (\n    prices\n    .loc[prices[\"adjusted_close\"] &gt; 0]\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n)\nreturns = returns.dropna(subset=[\"ret\"])\n\nThe initial return for each stock is missing by construction, since no lagged price is available. These observations are mechanical and can safely be removed in most applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "href": "01_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.4 Limiting the Influence of Extreme Returns",
    "text": "2.4 Limiting the Influence of Extreme Returns\nDaily return series often contain extreme observations driven by data errors, thin trading, or abrupt price adjustments. A common approach is to winsorize returns using cross-sectional percentile cutoffs.\n\ndef winsorize_cs(df, column=\"ret\", lower_q=0.01, upper_q=0.99):\n    lo = df[column].quantile(lower_q)\n    hi = df[column].quantile(upper_q)\n    out = df.copy()\n    out[column] = out[column].clip(lo, hi)\n    return out\n\nreturns = winsorize_cs(returns)\n\nApplying winsorization across the full cross-section limits the impact of extreme market-wide observations while preserving relative differences between firms. Winsorizing within each stock is rarely appropriate in panel settings and can severely distort illiquid securities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#distributional-features-of-returns",
    "href": "01_working_with_stock_returns.html#distributional-features-of-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.5 Distributional Features of Returns",
    "text": "2.5 Distributional Features of Returns\nWe next examine the empirical distribution of daily returns for FPT. The figure below also marks the historical 5 percent quantile, which provides a simple, non-parametric measure of downside risk.\n\nfrom mizani.formatters import percent_format\nfrom plotnine import geom_histogram, geom_vline, scale_x_continuous\n\n\nfpt_ret = returns.loc[returns[\"symbol\"] == \"FPT\"].copy()\nq05 = fpt_ret[\"ret\"].quantile(0.05)\n\n\n(\n    ggplot(fpt_ret, aes(x=\"ret\"))\n    + geom_histogram(bins=100)\n    + geom_vline(xintercept=q05, linetype=\"dashed\")\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"Distribution of daily FPT returns\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 2.2: The dotted vertical line indicates the historical five percent quantile.\n\n\n\n\n\nSummary statistics offer a compact description of return behavior and should always be inspected before formal modeling.\n\nreturns[\"ret\"].describe().round(3)\n\ncount    4305063.000\nmean           0.000\nstd            0.035\nmin           -0.125\n25%           -0.004\n50%            0.000\n75%            0.003\nmax            0.130\nName: ret, dtype: float64\n\n\nComputing these statistics by calendar year can reveal periods of elevated volatility or structural change.\n\n(\n    returns\n    .assign(year=lambda x: x[\"date\"].dt.year)\n    .groupby(\"year\")[\"ret\"]\n    .describe()\n    .round(3)\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n2010\n131548.0\n-0.001\n0.036\n-0.125\n-0.021\n0.0\n0.018\n0.13\n\n\n2011\n166826.0\n-0.003\n0.033\n-0.125\n-0.020\n0.0\n0.011\n0.13\n\n\n2012\n177938.0\n0.000\n0.033\n-0.125\n-0.012\n0.0\n0.015\n0.13\n\n\n2013\n180417.0\n0.001\n0.033\n-0.125\n-0.004\n0.0\n0.008\n0.13\n\n\n2014\n181907.0\n0.001\n0.034\n-0.125\n-0.008\n0.0\n0.011\n0.13\n\n\n2015\n197881.0\n0.000\n0.033\n-0.125\n-0.006\n0.0\n0.005\n0.13\n\n\n2016\n227896.0\n0.000\n0.035\n-0.125\n-0.005\n0.0\n0.003\n0.13\n\n\n2017\n283642.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.001\n0.13\n\n\n2018\n329887.0\n0.000\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2019\n352754.0\n0.000\n0.033\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2020\n369367.0\n0.001\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2021\n379415.0\n0.002\n0.038\n-0.125\n-0.005\n0.0\n0.007\n0.13\n\n\n2022\n387050.0\n-0.001\n0.038\n-0.125\n-0.008\n0.0\n0.004\n0.13\n\n\n2023\n391605.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.002\n0.13\n\n\n2024\n400379.0\n0.000\n0.031\n-0.125\n-0.002\n0.0\n0.000\n0.13\n\n\n2025\n146551.0\n0.000\n0.037\n-0.125\n-0.004\n0.0\n0.002\n0.13",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "href": "01_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.6 Expanding to a Market Cross-Section",
    "text": "2.6 Expanding to a Market Cross-Section\nThe same procedures apply naturally to a larger universe of stocks. We now restrict attention to the constituents of the VN30 index.\n\nvn30 = [\n    \"ACB\",\"BCM\",\"BID\",\"BVH\",\"CTG\",\"FPT\",\"GAS\",\"GVR\",\"HDB\",\"HPG\",\n    \"MBB\",\"MSN\",\"MWG\",\"PLX\",\"POW\",\"SAB\",\"SHB\",\"SSB\",\"STB\",\"TCB\",\n    \"TPB\",\"VCB\",\"VHM\",\"VIB\",\"VIC\",\"VJC\",\"VNM\",\"VPB\",\"VRE\",\"EIB\",\n]\n\n\nprices_vn30 = prices.loc[prices[\"symbol\"].isin(vn30)]\nfrom plotnine import theme\n\n\n(\n    ggplot(prices_vn30, aes(x=\"date\", y=\"adjusted_close\", color=\"symbol\"))\n    + geom_line()\n    + labs(title=\"Adjusted prices of VN30 constituents\", x=\"\", y=\"\")\n    + theme(legend_position=\"none\")\n)\n\n\n\n\n\n\n\nFigure 2.3: Prices in VND, adjusted for dividend payments and stock splits.\n\n\n\n\n\nReturns for the VN30 universe are computed analogously.\n\nreturns_vn30 = (\n    prices_vn30\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n    .dropna()\n)\n\n\nreturns_vn30.groupby(\"symbol\")[\"ret\"].describe().round(3)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\n\n\nACB\n3822.0\n-0.000\n0.023\n-0.407\n-0.006\n0.0\n0.007\n0.097\n\n\nBCM\n1795.0\n0.001\n0.027\n-0.136\n-0.010\n0.0\n0.010\n0.159\n\n\nBID\n2811.0\n0.000\n0.024\n-0.369\n-0.010\n0.0\n0.011\n0.070\n\n\nBVH\n3825.0\n0.000\n0.024\n-0.097\n-0.012\n0.0\n0.012\n0.070\n\n\nCTG\n3825.0\n0.000\n0.024\n-0.376\n-0.010\n0.0\n0.010\n0.070\n\n\nEIB\n3825.0\n-0.000\n0.022\n-0.302\n-0.008\n0.0\n0.008\n0.070\n\n\nFPT\n3825.0\n-0.000\n0.024\n-0.439\n-0.008\n0.0\n0.009\n0.070\n\n\nGAS\n3236.0\n0.000\n0.022\n-0.289\n-0.009\n0.0\n0.010\n0.070\n\n\nGVR\n1775.0\n0.001\n0.030\n-0.137\n-0.014\n0.0\n0.016\n0.169\n\n\nHDB\n1828.0\n-0.001\n0.028\n-0.391\n-0.009\n0.0\n0.010\n0.070\n\n\nHPG\n3825.0\n-0.001\n0.032\n-0.581\n-0.010\n0.0\n0.011\n0.070\n\n\nMBB\n3371.0\n-0.000\n0.023\n-0.473\n-0.008\n0.0\n0.008\n0.069\n\n\nMSN\n3825.0\n0.000\n0.024\n-0.553\n-0.010\n0.0\n0.010\n0.070\n\n\nMWG\n2701.0\n-0.000\n0.035\n-0.751\n-0.009\n0.0\n0.011\n0.070\n\n\nPLX\n2009.0\n-0.000\n0.021\n-0.140\n-0.010\n0.0\n0.010\n0.070\n\n\nPOW\n1784.0\n0.000\n0.023\n-0.071\n-0.012\n0.0\n0.011\n0.102\n\n\nSAB\n2100.0\n-0.000\n0.024\n-0.745\n-0.008\n0.0\n0.007\n0.070\n\n\nSHB\n3824.0\n-0.000\n0.028\n-0.338\n-0.013\n0.0\n0.013\n0.100\n\n\nSSB\n1029.0\n-0.000\n0.023\n-0.292\n-0.005\n0.0\n0.004\n0.070\n\n\nSTB\n3825.0\n0.000\n0.024\n-0.321\n-0.010\n0.0\n0.010\n0.070\n\n\nTCB\n1732.0\n-0.000\n0.035\n-0.884\n-0.009\n0.0\n0.010\n0.070\n\n\nTPB\n1761.0\n-0.001\n0.029\n-0.477\n-0.009\n0.0\n0.009\n0.070\n\n\nVCB\n3825.0\n-0.000\n0.024\n-0.539\n-0.009\n0.0\n0.009\n0.070\n\n\nVHM\n1744.0\n-0.000\n0.024\n-0.419\n-0.009\n0.0\n0.008\n0.070\n\n\nVIB\n2072.0\n-0.000\n0.031\n-0.489\n-0.009\n0.0\n0.010\n0.109\n\n\nVIC\n3825.0\n-0.000\n0.027\n-0.673\n-0.008\n0.0\n0.008\n0.070\n\n\nVJC\n2046.0\n-0.000\n0.020\n-0.455\n-0.007\n0.0\n0.006\n0.070\n\n\nVNM\n3825.0\n-0.000\n0.023\n-0.547\n-0.007\n0.0\n0.007\n0.070\n\n\nVPB\n1927.0\n-0.000\n0.033\n-0.678\n-0.010\n0.0\n0.010\n0.070\n\n\nVRE\n1871.0\n-0.000\n0.024\n-0.295\n-0.012\n0.0\n0.011\n0.070",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#aggregating-returns-across-time",
    "href": "01_working_with_stock_returns.html#aggregating-returns-across-time",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.7 Aggregating Returns Across Time",
    "text": "2.7 Aggregating Returns Across Time\nFinancial variables are observed at different frequencies. While equity prices are recorded daily, many empirical questions require monthly or annual returns. Lower-frequency returns are constructed by compounding higher-frequency observations.\n\nreturns_monthly = (\n    returns_vn30\n    .assign(month=lambda x: x[\"date\"].dt.to_period(\"M\").dt.to_timestamp())\n    .groupby([\"symbol\", \"month\"], as_index=False)\n    .agg(ret=(\"ret\", lambda x: np.prod(1 + x) - 1))\n)\n\nComparing daily and monthly return distributions illustrates how aggregation dampens volatility and alters tail behavior.\n\nfrom plotnine import facet_wrap\n\nfpt_d = returns_vn30.loc[returns_vn30[\"symbol\"] == \"FPT\"].assign(freq=\"Daily\")\nfpt_m = returns_monthly.loc[returns_monthly[\"symbol\"] == \"FPT\"].assign(freq=\"Monthly\")\n\n\nfpt_both = pd.concat([\n    fpt_d[[\"ret\", \"freq\"]],\n    fpt_m[[\"ret\", \"freq\"]],\n])\n\n\n(\n    ggplot(fpt_both, aes(x=\"ret\"))\n    + geom_histogram(bins=50)\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"FPT returns at different frequencies\", x=\"\", y=\"\")\n    + facet_wrap(\"freq\", scales=\"free\")\n)\n\n\n\n\n\n\n\nFigure 2.4: Returns are based on prices adjusted for dividend payments and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "href": "01_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.8 Aggregation Across Firms: Trading Activity",
    "text": "2.8 Aggregation Across Firms: Trading Activity\nAggregation is not limited to time. In some settings, it is informative to aggregate variables across firms. As an illustration, we compute total daily trading value for VN30 stocks by multiplying share volume by adjusted prices and summing across firms.\n\ntrading_value = (\n    prices_vn30\n    .assign(value=lambda x: x[\"volume\"] * x[\"adjusted_close\"] / 1e9)\n    .groupby(\"date\")[\"value\"]\n    .sum()\n    .reset_index()\n    .assign(value_lag=lambda x: x[\"value\"].shift(1))\n)\n(\n    ggplot(trading_value, aes(x=\"date\", y=\"value\"))\n    + geom_line()\n    + labs(title=\"Aggregate VN30 trading value (billion VND)\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\n\nFinally, we assess persistence in trading activity by comparing trading value on consecutive days.\n\nfrom plotnine import geom_point, geom_abline\n\n\n(\n    ggplot(trading_value, aes(x=\"value_lag\", y=\"value\"))\n    + geom_point()\n    + geom_abline(intercept=0, slope=1, linetype=\"dashed\")\n    + labs(\n        title=\"Persistence in VN30 trading value\",\n        x=\"Previous day\",\n        y=\"Current day\",\n    )\n)\n\n\n\n\n\n\n\nFigure 2.5: Total daily trading volume.\n\n\n\n\n\nA strong alignment along the 45-degree line indicates that high-activity trading days tend to be followed by similarly active days, a common empirical regularity in equity markets.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#summary",
    "href": "01_working_with_stock_returns.html#summary",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nThis chapter established a reproducible workflow for transforming raw price data into return series, diagnosing common data issues, and aggregating information across time and firms. These steps provide the empirical foundation for subsequent analyses of risk, return predictability, and market dynamics in Vietnam’s equity market.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html",
    "href": "02_modern_portfolio_theory.html",
    "title": "3  Modern Portfolio Theory",
    "section": "",
    "text": "3.0.1 The Core Insight: Diversification as a Free Lunch\nIn the previous chapter, we showed how to download and analyze stock market data with figures and summary statistics. Now, we turn to one of the most fundamental questions in finance: How should an investor allocate their wealth across assets that differ in expected returns, variance, and correlations to optimize their portfolio’s performance?\nThis question might seem straightforward at first glance. Why not simply invest everything in the asset with the highest expected return? The answer lies in a profound insight that transformed financial economics: risk matters, and it can be managed through diversification.\nModern Portfolio Theory (MPT), introduced by Markowitz (1952), revolutionized investment decision-making by formalizing the trade-off between risk and expected return. Before Markowitz, investors largely thought about risk on a security-by-security basis. Markowitz’s genius was recognizing that what matters is not the risk of individual securities in isolation, but how they contribute to the risk of the entire portfolio. This insight was so influential that it earned him the Sveriges Riksbank Prize in Economic Sciences in 1990 and laid the foundation for much of modern finance.\nMPT relies on a crucial mathematical fact: portfolio risk depends not only on individual asset volatilities but also on the correlations between asset returns. This insight reveals the power of diversification—combining assets whose returns don’t move in perfect lockstep can reduce overall portfolio risk without necessarily sacrificing expected return.\nConsider a simple analogy: Imagine you run a business selling both sunscreen and umbrellas. On sunny days, sunscreen sales boom but umbrella sales suffer; on rainy days, the reverse happens. By selling both products, your total revenue becomes more stable than if you sold only one. The “correlation” between sunscreen and umbrella sales is negative, and combining them reduces the variance of your overall income. This is precisely the logic behind portfolio diversification.\nThe fruit basket analogy offers another perspective: If all you have are apples and they spoil, you lose everything. With a variety of fruits, some may spoil, but others will stay fresh. Diversification provides insurance against the idiosyncratic risks of individual assets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-asset-universe-setting-up-the-problem",
    "href": "02_modern_portfolio_theory.html#the-asset-universe-setting-up-the-problem",
    "title": "3  Modern Portfolio Theory",
    "section": "3.1 The Asset Universe: Setting Up the Problem",
    "text": "3.1 The Asset Universe: Setting Up the Problem\nSuppose \\(N\\) different risky assets are available to the investor. Each asset \\(i\\) is characterized by:\n\nExpected return \\(\\mu_i\\): The anticipated profit from holding the asset for one period\nVariance \\(\\sigma_i^2\\): The dispersion of returns around the mean\nCovariances \\(\\sigma_{ij}\\): The degree to which asset \\(i\\)’s returns move together with asset \\(j\\)’s returns\n\nThe investor chooses portfolio weights \\(\\omega_i\\) for each asset \\(i\\). These weights represent the fraction of total wealth invested in each asset. We impose the constraint that weights sum to one:\n\\[\n\\sum_{i=1}^N \\omega_i = 1\n\\]\nThis “budget constraint” ensures that the investor is fully invested—there is no outside option such as keeping money under a mattress. Note that we allow weights to be negative (short selling) or greater than one (leverage), though in practice these positions may face constraints.\n\n3.1.1 The Two Stages of Portfolio Selection\nAccording to Markowitz (1952), portfolio selection involves two distinct stages:\n\nEstimation: Forming expectations about future security performance based on observations, experience, and economic reasoning\nOptimization: Using these expectations to choose an optimal portfolio\n\nIn practice, these stages cannot be fully separated. The estimation stage determines the inputs (\\(\\mu\\), \\(\\Sigma\\)) that feed into the optimization stage. Poor estimation leads to poor portfolio choices, regardless of how sophisticated the optimization procedure.\nTo keep things conceptually clear, we focus primarily on the optimization stage in this chapter. We treat the expected returns and variance-covariance matrix as known, using historical data to compute reasonable proxies. In later chapters, we address the substantial challenges that arise from estimation uncertainty.\n\n\n3.1.2 Loading and Preparing the Data\nWe work with the VN30 index constituents—the 30 largest and most liquid stocks on Vietnam’s Ho Chi Minh Stock Exchange. This provides a realistic asset universe for a domestic Vietnamese investor.\n\nvn30_symbols = [\n    \"ACB\",\"BCM\",\"BID\",\"BVH\",\"CTG\",\"FPT\",\"GAS\",\"GVR\",\"HDB\",\"HPG\",\n    \"MBB\",\"MSN\",\"MWG\",\"PLX\",\"POW\",\"SAB\",\"SHB\",\"SSB\",\"STB\",\"TCB\",\n    \"TPB\",\"VCB\",\"VHM\",\"VIB\",\"VIC\",\"VJC\",\"VNM\",\"VPB\",\"VRE\",\"EIB\"\n]\n\nWe load the historical price data:\n\nimport pandas as pd\nfrom io import BytesIO\nimport datetime as dt\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\nWe filter to keep only the VN30 constituents:\n\nprices_daily = prices[prices[\"symbol\"].isin(vn30_symbols)]\nprices_daily[[\"date\", \"symbol\", \"adjusted_close\"]].head(3)\n\n\n\n\n\n\n\n\ndate\nsymbol\nadjusted_close\n\n\n\n\n18176\n2010-01-04\nACB\n329.408244\n\n\n18177\n2010-01-05\nACB\n329.408244\n\n\n18178\n2010-01-06\nACB\n320.258015\n\n\n\n\n\n\n\n\n\n3.1.3 Computing Expected Returns\nThe sample mean return serves as our proxy for expected returns. For each asset \\(i\\), we compute:\n\\[\n\\hat{\\mu}_i = \\frac{1}{T} \\sum_{t=1}^{T} r_{i,t}\n\\]\nwhere \\(r_{i,t}\\) is the return of asset \\(i\\) in period \\(t\\), and \\(T\\) is the total number of periods.\nWhy monthly returns? While daily data provides more observations, monthly returns offer several advantages for portfolio optimization. First, monthly returns are less noisy and exhibit weaker serial correlation. Second, monthly rebalancing is more realistic for most investors, avoiding excessive transaction costs. Third, the estimation error in mean returns is already substantial—using daily data doesn’t materially improve the precision of mean estimates because the mean return scales with the horizon while estimation error scales with the square root of observations.\n\nreturns_monthly = (prices_daily\n  .assign(\n    date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n  )\n  .groupby([\"symbol\", \"date\"], as_index=False)\n  .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n  .assign(\n    ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n  )\n)\n\n\n\n3.1.4 Computing Volatilities\nIndividual asset risk in MPT is quantified using variance (\\(\\sigma^2_i\\)) or its square root, the standard deviation or volatility (\\(\\sigma_i\\)). We use the sample standard deviation as our proxy:\n\\[\n\\hat{\\sigma}_i = \\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} (r_{i,t} - \\hat{\\mu}_i)^2}\n\\]\nAlternative risk measures exist, including Value-at-Risk, Expected Shortfall, and higher-order moments such as skewness and kurtosis. However, variance remains the workhorse measure in portfolio theory because of its mathematical tractability and the central role of the normal distribution in finance.\n\nassets = (returns_monthly\n  .groupby(\"symbol\", as_index=False)\n  .agg(\n    mu=(\"ret\", \"mean\"),\n    sigma=(\"ret\", \"std\")\n  )\n)\n\n\n\n3.1.5 Visualizing the Risk-Return Trade-off\nFigure 3.1 displays each asset’s expected return (vertical axis) against its volatility (horizontal axis). This “mean-standard deviation” space is fundamental to portfolio theory.\n\nassets_figure = (\n  ggplot(\n    assets, \n    aes(x=\"sigma\", y=\"mu\", label=\"symbol\")\n  )\n  + geom_point()\n  + geom_text(adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"Expected returns and volatilities of VN30 index constituents\"\n  )\n)\nassets_figure.show()\n\n\n\n\n\n\n\nFigure 3.1: Expected returns and volatilities based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nSeveral observations emerge from this figure. First, there is substantial heterogeneity in both expected returns and volatilities across stocks. Second, the relationship between risk and return is far from linear. Some high-volatility stocks have low or even negative expected returns. Third, most individual stocks appear to offer poor risk-return trade-offs. As we will see, portfolios can substantially improve upon these individual positions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-variance-covariance-matrix-capturing-asset-interactions",
    "href": "02_modern_portfolio_theory.html#the-variance-covariance-matrix-capturing-asset-interactions",
    "title": "3  Modern Portfolio Theory",
    "section": "3.2 The Variance-Covariance Matrix: Capturing Asset Interactions",
    "text": "3.2 The Variance-Covariance Matrix: Capturing Asset Interactions\n\n3.2.1 Why Correlations Matter\nA key innovation of MPT is recognizing that portfolio risk depends critically on how assets move together. The variance-covariance matrix \\(\\Sigma\\) captures all pairwise interactions between asset returns.\nTo understand why correlations matter, consider the variance of a two-asset portfolio: \\[\\sigma_p^2 = \\omega_1^2\\sigma_1^2 + \\omega_2^2\\sigma_2^2 + 2\\omega_1\\omega_2\\sigma_{12}\\]\nThe third term involves the covariance \\(\\sigma_{12} = \\rho_{12}\\sigma_1\\sigma_2\\), where \\(\\rho_{12}\\) is the correlation coefficient. When \\(\\rho_{12} &lt; 1\\), the portfolio variance is less than the weighted average of individual variances. When \\(\\rho_{12} &lt; 0\\), the diversification benefit is even more pronounced.\nThis mathematical fact has profound implications: You can reduce risk without reducing expected return by combining assets that don’t move perfectly together. This is sometimes called the “only free lunch in finance.”\n\n\n3.2.2 Computing the Variance-Covariance Matrix\nWe compute the sample covariance matrix as: \\[\\hat{\\sigma}_{ij} = \\frac{1}{T-1} \\sum_{t=1}^{T} (r_{i,t} - \\hat{\\mu}_i)(r_{j,t} - \\hat{\\mu}_j)\\]\nFirst, we reshape the returns data into a wide format with assets as columns:\n\nreturns_wide = (returns_monthly\n  .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n  .reset_index()\n)\n\nsigma = (returns_wide\n  .drop(columns=[\"date\"])\n  .cov()\n)\n\n\n\n3.2.3 Interpreting the Variance-Covariance Matrix\nThe diagonal elements of \\(\\Sigma\\) are the variances of individual assets. The off-diagonal elements are covariances, which can be positive (assets tend to move together), negative (assets tend to move in opposite directions), or zero (no linear relationship).\nFor easier interpretation, we often convert covariances to correlations: \\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sigma_i \\sigma_j}\\]\nCorrelations are bounded between -1 and +1, making them easier to compare across asset pairs.\nFigure 3.2 visualizes the variance-covariance matrix as a heatmap.\n\nsigma_long = (sigma\n  .reset_index()\n  .melt(id_vars=\"symbol\", var_name=\"symbol_b\", value_name=\"value\")\n)\n\nsigma_long[\"symbol_b\"] = pd.Categorical(\n  sigma_long[\"symbol_b\"], \n  categories=sigma_long[\"symbol_b\"].unique()[::-1],\n  ordered=True\n)\n\nsigma_figure = (\n  ggplot(\n    sigma_long, \n    aes(x=\"symbol\", y=\"symbol_b\", fill=\"value\")\n  )\n  + geom_tile()\n  + labs(\n      x=\"\", y=\"\", fill=\"(Co-)Variance\",\n      title=\"Sample variance-covariance matrix of VN30 index constituents\"\n    )\n  + scale_fill_continuous(labels=percent_format())\n  + theme(axis_text_x=element_text(angle=45, hjust=1))\n)\nsigma_figure.show()\n\n\n\n\n\n\n\nFigure 3.2: Variances and covariances based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nThe heatmap reveals important patterns. The diagonal (variances) shows which stocks are most volatile. The off-diagonal patterns show which pairs of stocks tend to move together. In general, stocks within the same sector tend to have higher correlations with each other than with stocks from different sectors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-minimum-variance-portfolio",
    "href": "02_modern_portfolio_theory.html#the-minimum-variance-portfolio",
    "title": "3  Modern Portfolio Theory",
    "section": "3.3 The Minimum-Variance Portfolio",
    "text": "3.3 The Minimum-Variance Portfolio\n\n3.3.1 Motivation: Risk Minimization as a Benchmark\nBefore considering expected returns, let’s find the portfolio that minimizes risk entirely. This minimum-variance portfolio (MVP) serves as an important benchmark and reference point. It represents what an extremely risk-averse investor—one who cares only about minimizing volatility—would choose.\n\n\n3.3.2 The Optimization Problem\nThe minimum-variance investor solves: \\[\n\\min_{\\omega} \\omega^{\\prime}\\Sigma\\omega\n\\]\nsubject to the constraint that weights sum to one:\n\\[\n\\omega^{\\prime}\\iota = 1\n\\]\nwhere \\(\\iota\\) is an \\(N \\times 1\\) vector of ones.\nIn words: minimize portfolio variance, subject to being fully invested.\n\n\n3.3.3 The Analytical Solution\nThis is a classic constrained optimization problem that can be solved using Lagrange multipliers. The Lagrangian is:\n\\[\n\\mathcal{L} = \\omega^{\\prime}\\Sigma\\omega - \\lambda(\\omega^{\\prime}\\iota - 1)\n\\]\nTaking the first-order condition with respect to \\(\\omega\\): \\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda\\iota = 0\n\\]\nSolving for \\(\\omega\\): \\[\n\\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota\n\\]\nUsing the constraint \\(\\omega^{\\prime}\\iota = 1\\) to solve for \\(\\lambda\\): \\[\n\\frac{\\lambda}{2}\\iota^{\\prime}\\Sigma^{-1}\\iota = 1 \\implies \\frac{\\lambda}{2} = \\frac{1}{\\iota^{\\prime}\\Sigma^{-1}\\iota}\n\\]\nSubstituting back: \\[\n\\omega_{\\text{mvp}} = \\frac{\\Sigma^{-1}\\iota}{\\iota^{\\prime}\\Sigma^{-1}\\iota}\n\\]\nThis elegant formula shows that the minimum-variance weights depend only on the covariance matrix—expected returns play no role. The inverse covariance matrix \\(\\Sigma^{-1}\\) determines how much to invest in each asset based on its variance and its covariances with all other assets.\n\n\n3.3.4 Implementation\n\niota = np.ones(sigma.shape[0])\nsigma_inv = np.linalg.inv(sigma.values)\nomega_mvp = (sigma_inv @ iota) / (iota @ sigma_inv @ iota)\n\n\n\n3.3.5 Visualizing the Minimum-Variance Weights\nFigure 3.3 displays the portfolio weights of the minimum-variance portfolio.\n\nassets = assets.assign(omega_mvp=omega_mvp)\n\nassets[\"symbol\"] = pd.Categorical(\n  assets[\"symbol\"],\n  categories=assets.sort_values(\"omega_mvp\")[\"symbol\"],\n  ordered=True\n)\n\nomega_figure = (\n  ggplot(\n    assets,\n    aes(y=\"omega_mvp\", x=\"symbol\", fill=\"omega_mvp&gt;0\")\n  )\n  + geom_col()\n  + coord_flip()\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", \n      y=\"Portfolio Weight\", \n      title=\"Minimum-variance portfolio weights\"\n  )\n  + theme(legend_position=\"none\")\n)\nomega_figure.show()\n\n\n\n\n\n\n\nFigure 3.3: Weights are based on historical moments of monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nSeveral features of the minimum-variance portfolio are noteworthy. First, many stocks receive zero or near-zero weights. Second, some stocks receive negative weights (short positions). These short positions are not a computational artifact, they reflect the optimizer’s attempt to exploit correlations for risk reduction. Third, the weights are quite extreme (both large positive and large negative), which often indicates estimation error amplification, which is a topic we address in later chapters.\n\n\n3.3.6 Portfolio Performance\nLet’s compute the expected return and volatility of the minimum-variance portfolio:\n\nmu = assets[\"mu\"].values\nmu_mvp = omega_mvp @ mu\nsigma_mvp = np.sqrt(omega_mvp @ sigma.values @ omega_mvp)\n\nsummary_mvp = pd.DataFrame({\n  \"mu\": [mu_mvp],\n  \"sigma\": [sigma_mvp],\n  \"type\": [\"Minimum-Variance Portfolio\"]\n})\nsummary_mvp\n\n\n\n\n\n\n\n\nmu\nsigma\ntype\n\n\n\n\n0\n-0.011424\n0.043512\nMinimum-Variance Portfolio\n\n\n\n\n\n\n\n\nmu_mvp_fmt = f\"{mu_mvp:.4f}\"\nsigma_mvp_fmt = f\"{sigma_mvp:.4f}\"\nprint(f\"The MVP return is {mu_mvp_fmt} and volatility is {sigma_mvp_fmt}.\")\n\nThe MVP return is -0.0114 and volatility is 0.0435.\n\n\nIf the expected return is negative, this is not a computational error. The minimum-variance portfolio minimizes risk without regard to expected returns. Because some assets in the sample have negative average returns, the risk-minimizing combination may inherit a negative expected return. This highlights a fundamental limitation of using historical sample means as estimates of expected returns: they are extremely noisy, and can lead to economically unintuitive results even when the optimization mathematics are working correctly.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#efficient-portfolios-balancing-risk-and-return",
    "href": "02_modern_portfolio_theory.html#efficient-portfolios-balancing-risk-and-return",
    "title": "3  Modern Portfolio Theory",
    "section": "3.4 Efficient Portfolios: Balancing Risk and Return",
    "text": "3.4 Efficient Portfolios: Balancing Risk and Return\n\n3.4.1 The Investor’s Trade-off\nIn most cases, minimizing variance is not the investor’s sole objective. A more realistic formulation allows the investor to trade off risk against expected return. The investor might be willing to accept higher portfolio variance in exchange for higher expected returns.\nAn efficient portfolio minimizes variance subject to earning at least some target expected return \\(\\bar{\\mu}\\). Formally:\n\\[\\min_{\\omega} \\omega^{\\prime}\\Sigma\\omega\\]\nsubject to: \\[\\omega^{\\prime}\\iota = 1 \\quad \\text{(fully invested)}\\] \\[\\omega^{\\prime}\\mu \\geq \\bar{\\mu} \\quad \\text{(minimum return)}\\]\nWhen \\(\\bar{\\mu}\\) exceeds the expected return of the minimum-variance portfolio, the investor accepts more risk to earn more return.\n\n\n3.4.2 Setting the Target Return\nFor illustration, suppose the investor wants to earn at least the historical average return of the best-performing stock:\n\nmu_bar = assets[\"mu\"].max()\nprint(f\"Target expected return: {mu_bar:.5f}\")\n\nTarget expected return: 0.01886\n\n\nThis is an ambitious target—it means matching the return of the single highest-returning stock while benefiting from diversification to reduce risk.\n\n\n3.4.3 The Analytical Solution\nThe constrained optimization problem with an inequality constraint on expected returns can be solved using the Karush-Kuhn-Tucker (KKT) conditions. At the optimum (assuming the return constraint binds), the solution is:\n\\[\\omega_{\\text{efp}} = \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu - \\frac{D}{C}\\Sigma^{-1}\\iota\\right)\\]\nwhere:\n\n\\(C = \\iota^{\\prime}\\Sigma^{-1}\\iota\\) (a scalar measuring the “size” of the inverse covariance matrix)\n\\(D = \\iota^{\\prime}\\Sigma^{-1}\\mu\\) (capturing the interaction between expected returns and the inverse covariance matrix)\n\\(E = \\mu^{\\prime}\\Sigma^{-1}\\mu\\) (measuring the “signal” in expected returns weighted by inverse covariances)\n\\(\\lambda^* = 2\\frac{\\bar{\\mu} - D/C}{E - D^2/C}\\) (the shadow price of the return constraint)\n\nAlternatively, we can express the efficient portfolio as a linear combination of the minimum-variance portfolio and an “excess return” portfolio:\n\\[\\omega_{\\text{efp}} = \\omega_{\\text{mvp}} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu - D \\cdot \\omega_{\\text{mvp}}\\right)\\]\nThis representation reveals important intuition: the efficient portfolio starts from the minimum-variance portfolio and tilts toward higher-expected-return assets, with the tilt magnitude determined by \\(\\lambda^*\\).\n\n\n3.4.4 Implementation\n\nC = iota @ sigma_inv @ iota\nD = iota @ sigma_inv @ mu\nE = mu @ sigma_inv @ mu\nlambda_tilde = 2 * (mu_bar - D / C) / (E - (D ** 2) / C)\nomega_efp = omega_mvp + (lambda_tilde / 2) * (sigma_inv @ mu - D * omega_mvp)\n\nmu_efp = omega_efp @ mu\nsigma_efp = np.sqrt(omega_efp @ sigma.values @ omega_efp)\n\nsummary_efp = pd.DataFrame({\n  \"mu\": [mu_efp],\n  \"sigma\": [sigma_efp],\n  \"type\": [\"Efficient Portfolio\"]\n})\n\n\n\n3.4.5 Comparing the Portfolios\nFigure 3.4 plots both portfolios alongside the individual assets.\n\nsummaries = pd.concat(\n  [assets, summary_mvp, summary_efp], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=summaries.query(\"type.isna()\"))\n  + geom_point(data=summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3)\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"Efficient & minimum-variance portfolios\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 3.4: The big dots indicate the location of the minimum-variance and the efficient portfolio that delivers the expected return of the stock with the highest return, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe figure demonstrates the substantial diversification benefits of portfolio optimization. The efficient portfolio achieves the same expected return as the highest-returning individual stock but with substantially lower volatility. This “free lunch” from diversification is the central insight of Modern Portfolio Theory.\n\n\n3.4.6 The Role of Risk Aversion\nThe target return \\(\\bar{\\mu}\\) implicitly reflects the investor’s risk aversion. Less risk-averse investors choose higher \\(\\bar{\\mu}\\), accepting more variance to earn more expected return. More risk-averse investors choose \\(\\bar{\\mu}\\) closer to the minimum-variance portfolio’s expected return.\nEquivalently, the mean-variance framework can be derived from the optimal decisions of an investor with a mean-variance utility function: \\[U(\\omega) = \\omega^{\\prime}\\mu - \\frac{\\gamma}{2}\\omega^{\\prime}\\Sigma\\omega\\]\nwhere \\(\\gamma\\) is the coefficient of relative risk aversion. The Appendix shows there is a one-to-one mapping between \\(\\gamma\\) and \\(\\bar{\\mu}\\), so both formulations yield identical efficient portfolios.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-efficient-frontier-the-menu-of-optimal-portfolios",
    "href": "02_modern_portfolio_theory.html#the-efficient-frontier-the-menu-of-optimal-portfolios",
    "title": "3  Modern Portfolio Theory",
    "section": "3.5 The Efficient Frontier: The Menu of Optimal Portfolios",
    "text": "3.5 The Efficient Frontier: The Menu of Optimal Portfolios\nThe efficient frontier is the set of all portfolios for which no other portfolio offers higher expected return at the same or lower variance. Geometrically, it traces the upper boundary of achievable (volatility, expected return) combinations.\nEvery rational mean-variance investor should hold a portfolio on the efficient frontier. Portfolios below the frontier are “dominated,” there exists another portfolio with either higher return for the same risk, or lower risk for the same return.\n\n3.5.1 The Mutual Fund Separation Theorem\nA remarkable result simplifies the construction of the efficient frontier. The mutual fund separation theorem (sometimes called the two-fund theorem) states that any efficient portfolio can be expressed as a linear combination of any two distinct efficient portfolios.\nFormally, if \\(\\omega_{\\mu_1}\\) and \\(\\omega_{\\mu_2}\\) are efficient portfolios earning expected returns \\(\\mu_1\\) and \\(\\mu_2\\) respectively, then the portfolio: \\[\\omega_{a\\mu_1 + (1-a)\\mu_2} = a \\cdot \\omega_{\\mu_1} + (1-a) \\cdot \\omega_{\\mu_2}\\]\nis also efficient and earns expected return \\(a\\mu_1 + (1-a)\\mu_2\\).\nThis result has profound practical implications: an investor needs access to only two efficient “mutual funds” to construct any portfolio on the efficient frontier. The specific funds don’t matter—any two distinct efficient portfolios span the entire frontier.\n\n\n3.5.2 Proof of the Separation Theorem\nThe proof follows directly from the analytical solution for efficient portfolios. Consider:\n\\[\na \\cdot \\omega_{\\mu_1} + (1-a) \\cdot \\omega_{\\mu_2} = \\left(\\frac{a\\mu_1 + (1-a)\\mu_2 - D/C}{E - D^2/C}\\right)\\left(\\Sigma^{-1}\\mu - \\frac{D}{C}\\Sigma^{-1}\\iota\\right)\n\\]\nThis expression has exactly the form of the efficient portfolio earning expected return \\(a\\mu_1 + (1-a)\\mu_2\\), proving the theorem.\n\n\n3.5.3 Computing the Efficient Frontier\nUsing the minimum-variance portfolio and our efficient portfolio as the two “funds,” we can trace out the entire efficient frontier:\n\nefficient_frontier = (\n  pd.DataFrame({\n    \"a\": np.arange(-1, 2.01, 0.01)\n  })\n  .assign(\n    omega=lambda x: x[\"a\"].map(lambda a: a * omega_efp + (1 - a) * omega_mvp)\n  )\n  .assign(\n    mu=lambda x: x[\"omega\"].map(lambda w: w @ mu),\n    sigma=lambda x: x[\"omega\"].map(lambda w: np.sqrt(w @ sigma @ w))\n  )\n)\n\nNote that we allow \\(a\\) to range from -1 to 2, which means some portfolios involve shorting one of the two basis funds and leveraging into the other. This traces out both the upper and lower portions of the frontier hyperbola.\n\n\n3.5.4 Visualizing the Efficient Frontier\nFigure 3.5 displays the efficient frontier alongside individual assets and the benchmark portfolios.\n\nsummaries = pd.concat(\n  [summaries, efficient_frontier], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=summaries.query(\"type.isna()\"))\n  + geom_line(data=efficient_frontier, color=\"blue\", alpha=0.7)\n  + geom_point(data=summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3)\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"The Efficient Frontier and VN30 Constituents\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 3.5: The big dots indicate the location of the minimum-variance and the efficient portfolio. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe efficient frontier has a characteristic hyperbolic shape. The leftmost point is the minimum-variance portfolio. Moving up and to the right along the frontier, expected return increases but so does volatility. The upper portion of the hyperbola (above the minimum-variance portfolio) is the “efficient” part—these portfolios offer the highest return for each level of risk. The lower portion is “inefficient”—these portfolios are dominated by their mirror images on the upper portion.\nThe figure also reveals how dramatically diversification improves upon individual stock positions. Nearly all individual stocks lie well inside the efficient frontier, meaning investors can achieve the same expected return with much less risk, or much higher expected return with the same risk, simply by diversifying.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#key-takeaways",
    "href": "02_modern_portfolio_theory.html#key-takeaways",
    "title": "3  Modern Portfolio Theory",
    "section": "3.6 Key Takeaways",
    "text": "3.6 Key Takeaways\nThis chapter introduced the concepts of Modern Portfolio Theory. The main insights are:\n\nPortfolio risk depends on correlations: The variance of a portfolio is not simply the weighted average of individual variances. Covariances between assets play a crucial role, creating opportunities for diversification.\nDiversification is the “only free lunch” in finance: By combining assets that don’t move perfectly together, investors can reduce risk without sacrificing expected return. This insight is the cornerstone of modern investment practice.\nThe minimum-variance portfolio minimizes risk: This portfolio depends only on the covariance matrix and serves as an important benchmark. It represents the least risky way to be fully invested in risky assets.\nEfficient portfolios balance risk and return: By accepting more variance, investors can earn higher expected returns. Efficient portfolios are those that offer the best possible trade-off.\nThe efficient frontier characterizes optimal portfolios: This boundary in mean-standard deviation space represents the menu of optimal choices available to mean-variance investors.\nTwo-fund separation simplifies implementation: Any efficient portfolio can be constructed from any two distinct efficient portfolios, reducing the computational burden of portfolio optimization.\n\nLooking ahead, several important complications arise in practice. First, the inputs to portfolio optimization (expected returns and covariances) must be estimated from data, and estimation error can dramatically affect portfolio performance. Second, real-world constraints such as transaction costs, short-sale restrictions, and position limits modify the optimization problem. Third, the assumption that investors care only about mean and variance may be too restrictive when returns are non-normal or when investors have more complex preferences. We address these extensions in subsequent chapters.\n\n\n\n\n\n\nMarkowitz, Harry. 1952. “Portfolio selection.” The Journal of Finance 7 (1): 77–91. https://doi.org/10.1111/j.1540-6261.1952.tb01525.x.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "03_capm.html",
    "href": "03_capm.html",
    "title": "4  The Capital Asset Pricing Model",
    "section": "",
    "text": "4.1 From Efficient Portfolios to Equilibrium Prices\nThe previous chapter on Modern Portfolio Theory (MPT) showed how an investor can construct portfolios that optimally trade off risk and expected return. But MPT leaves a crucial question unanswered: What determines the expected returns themselves? Why do some assets command higher risk premiums than others?\nThe Capital Asset Pricing Model (CAPM) answers this question. Developed simultaneously by Sharpe (1964), Lintner (1965), and Mossin (1966), the CAPM extends MPT to explain how assets should be priced in equilibrium when all investors follow mean-variance optimization principles. The CAPM’s central insight is both elegant and counterintuitive: not all risk is rewarded. Only the component of risk that cannot be diversified away (i.e., systematic risk) commands a risk premium in equilibrium.\nThe CAPM remains the cornerstone of asset pricing theory, not because it perfectly describes reality, but because it provides the simplest coherent framework for understanding the relationship between risk and expected return. Every extension and alternative model in asset pricing (e.g., from the Fama-French factors to consumption-based pricing) builds upon or reacts against the CAPM’s foundational logic.\nIn this chapter, we derive the CAPM from first principles, illustrate its theoretical underpinnings, and show how to estimate its parameters empirically. We download stock market data, estimate betas using regression analysis, and evaluate asset performance relative to model predictions.\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#systematic-versus-idiosyncratic-risk",
    "href": "03_capm.html#systematic-versus-idiosyncratic-risk",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.2 Systematic versus Idiosyncratic Risk",
    "text": "4.2 Systematic versus Idiosyncratic Risk\nBefore diving into the mathematics, we need to understand the fundamental distinction that makes the CAPM work: the difference between systematic and idiosyncratic risk.\n\n4.2.1 Idiosyncratic Risk: Diversifiable and Unrewarded\nConsider events that affect individual companies but not the broader market: a CEO resigns unexpectedly, a product launch fails, earnings disappoint analysts, or a factory experiences a fire. These company-specific events can dramatically affect individual stock prices, but they tend to average out across a diversified portfolio. When one company has bad news, another often has good news; the shocks are largely uncorrelated.\nThis idiosyncratic (or firm-specific) risk can be eliminated through diversification. By holding a portfolio of many stocks, an investor can reduce idiosyncratic risk to nearly zero. Since this risk can be avoided at no cost, investors should not expect compensation for bearing it. In equilibrium, idiosyncratic risk earns no premium.\n\n\n4.2.2 Systematic Risk: Undiversifiable and Priced\nSystematic risk, by contrast, affects all assets simultaneously. Recessions, interest rate changes, geopolitical crises, and pandemics impact virtually every company to some degree. No amount of diversification can eliminate exposure to these economy-wide shocks, they are inherent to participating in the market.\nSince systematic risk cannot be diversified away, investors genuinely dislike it. They must be compensated for bearing it. The CAPM formalizes this intuition: expected returns should depend only on systematic risk, not total risk. Two assets with identical total volatility can have very different expected returns if their systematic risk exposures differ.\n\n\n4.2.3 A Simple Illustration\nImagine two stocks with identical 30% annual volatility. Stock A is a gold mining company whose returns move opposite to the overall market: it does well when the economy struggles and poorly when it booms. Stock B is a luxury retailer that amplifies market movements: soaring in good times and crashing in bad times.\nWhich stock should offer higher expected returns? Intuition might suggest they should be equal since both have the same volatility. But the CAPM says Stock B should offer substantially higher returns. Why? Because Stock B performs poorly precisely when investors’ overall wealth is already down (during market crashes), making its returns particularly painful. Stock A, by contrast, provides insurance. Its strong performance during market downturns partially offsets losses elsewhere in the portfolio. Investors value this insurance property and are willing to accept lower expected returns in exchange.\nThis is the CAPM’s core insight: expected returns compensate investors for systematic risk exposure, measured by how an asset’s returns co-move with the market portfolio.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#data-preparation",
    "href": "03_capm.html#data-preparation",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.3 Data Preparation",
    "text": "4.3 Data Preparation\nBuilding on our analysis from the previous chapter, we examine the VN30 constituents as our asset universe. We download and prepare monthly return data:\n\nvn30_symbols = [\n    \"ACB\", \"BCM\", \"BID\", \"BVH\", \"CTG\", \"FPT\", \"GAS\", \"GVR\", \"HDB\", \"HPG\",\n    \"MBB\", \"MSN\", \"MWG\", \"PLX\", \"POW\", \"SAB\", \"SHB\", \"SSB\", \"STB\", \"TCB\",\n    \"TPB\", \"VCB\", \"VHM\", \"VIB\", \"VIC\", \"VJC\", \"VNM\", \"VPB\", \"VRE\", \"EIB\"\n]\n\n\nimport os\nimport boto3\nfrom botocore.client import Config\nfrom io import BytesIO\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\nWe process the raw price data to compute adjusted closing prices and standardize column names:\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\nprices = prices.sort_values([\"symbol\", \"date\"])\n\n\nprices_daily = prices[prices[\"symbol\"].isin(vn30_symbols)]\nprices_daily[[\"date\", \"symbol\", \"adjusted_close\"]].head(3)\n\n\n\n\n\n\n\n\ndate\nsymbol\nadjusted_close\n\n\n\n\n18176\n2010-01-04\nACB\n329.408244\n\n\n18177\n2010-01-05\nACB\n329.408244\n\n\n18178\n2010-01-06\nACB\n320.258015\n\n\n\n\n\n\n\n\n4.3.1 Computing Monthly Returns\nWe aggregate daily prices to monthly frequency. Using monthly returns rather than daily returns offers several advantages for portfolio analysis: monthly returns exhibit less noise, better approximate normality, and reduce the impact of microstructure effects like bid-ask bounce.\n\nreturns_monthly = (prices_daily\n    .assign(\n        date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp(\"M\")\n    )\n    .groupby([\"symbol\", \"date\"], as_index=False)\n    .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n    .sort_values([\"symbol\", \"date\"])\n    .assign(\n        ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    )\n)\n\nreturns_monthly.head(3)\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n0\nACB\n2010-01-31\n291.975489\nNaN\n\n\n1\nACB\n2010-02-28\n303.621235\n0.039886\n\n\n2\nACB\n2010-03-31\n273.784658\n-0.098269",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-risk-free-asset-and-the-investment-opportunity-set",
    "href": "03_capm.html#the-risk-free-asset-and-the-investment-opportunity-set",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.4 The Risk-Free Asset and the Investment Opportunity Set",
    "text": "4.4 The Risk-Free Asset and the Investment Opportunity Set\n\n4.4.1 Adding a Risk-Free Asset\nThe previous chapter on MPT considered portfolios composed entirely of risky assets, requiring that portfolio weights sum to one. The CAPM introduces a crucial new element: a risk-free asset that pays a constant interest rate \\(r_f\\) with zero volatility.\nThis seemingly simple addition fundamentally transforms the investment opportunity set. With a risk-free asset available, investors can choose to park some wealth in the safe asset and invest the remainder in risky assets. They can also borrow at the risk-free rate to leverage their risky positions.\nLet \\(\\omega \\in \\mathbb{R}^N\\) denote the portfolio weights in the \\(N\\) risky assets. Unlike before, these weights need not sum to one. The remainder, \\(1 - \\iota'\\omega\\) (where \\(\\iota\\) is a vector of ones), is invested in the risk-free asset.\n\n\n4.4.2 Portfolio Return with a Risk-Free Asset\nThe expected return on this combined portfolio is:\n\\[\n\\mu_\\omega = \\omega'\\mu + (1 - \\iota'\\omega)r_f = r_f + \\omega'(\\mu - r_f) = r_f + \\omega'\\tilde{\\mu}\n\\]\nwhere \\(\\mu\\) is the vector of expected returns on risky assets and \\(\\tilde{\\mu} = \\mu - r_f\\) denotes the vector of excess returns (returns above the risk-free rate).\nThis expression reveals an important decomposition: the portfolio’s expected return equals the risk-free rate plus a risk premium determined by the exposure to risky assets.\n\n\n4.4.3 Portfolio Variance\nSince the risk-free asset has zero volatility and zero covariance with risky assets, only the risky portion contributes to portfolio variance:\n\\[\n\\sigma_\\omega^2 = \\omega'\\Sigma\\omega\n\\]\nwhere \\(\\Sigma\\) is the variance-covariance matrix of risky asset returns. The portfolio’s volatility (standard deviation) is:\n\\[\n\\sigma_\\omega = \\sqrt{\\omega'\\Sigma\\omega}\n\\]\n\n\n4.4.4 Setting Up the Risk-Free Rate\nFor a realistic proxy of the risk-free rate, we use the Vietnam government bond yield. Government bonds of stable economies are considered “risk-free” because the government can always print money to meet its obligations (though this may cause inflation).\n\nall_dates = pd.date_range(\n    start=returns_monthly[\"date\"].min(), \n    end=returns_monthly[\"date\"].max(), \n    freq=\"ME\"\n)\n\n# Vietnam 10-Year Government Bond Yield (approximately 2.52% annualized)\nrf_annual = 0.0252\nrf_monthly_val = (1 + rf_annual)**(1/12) - 1\n\nrisk_free_monthly = pd.DataFrame({\n    \"date\": all_dates,\n    \"risk_free\": rf_monthly_val\n})\n\nrisk_free_monthly[\"date\"] = (\n    pd.to_datetime(risk_free_monthly[\"date\"])\n    .dt.to_period(\"M\")\n    .dt.to_timestamp(\"M\")\n)\n\nrisk_free_monthly.head(3)\n\n\n\n\n\n\n\n\ndate\nrisk_free\n\n\n\n\n0\n2010-01-31\n0.002076\n\n\n1\n2010-02-28\n0.002076\n\n\n2\n2010-03-31\n0.002076\n\n\n\n\n\n\n\nWe merge the risk-free rate with our returns data and compute excess returns:\n\nreturns_monthly = returns_monthly.merge(\n    risk_free_monthly[[\"date\", \"risk_free\"]], \n    on=\"date\", \n    how=\"left\"\n)\n\nrf = risk_free_monthly[\"risk_free\"].mean()\n\nreturns_monthly = (returns_monthly\n    .assign(\n        ret_excess=lambda x: x[\"ret\"] - x[\"risk_free\"]\n    )\n    .assign(\n        ret_excess=lambda x: x[\"ret_excess\"].clip(lower=-1)\n    )\n)\n\nreturns_monthly.head(3)\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\nrisk_free\nret_excess\n\n\n\n\n0\nACB\n2010-01-31\n291.975489\nNaN\n0.002076\nNaN\n\n\n1\nACB\n2010-02-28\n303.621235\n0.039886\n0.002076\n0.037810\n\n\n2\nACB\n2010-03-31\n273.784658\n-0.098269\n0.002076\n-0.100345",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-tangency-portfolio-where-everyone-invests",
    "href": "03_capm.html#the-tangency-portfolio-where-everyone-invests",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.5 The Tangency Portfolio: Where Everyone Invests",
    "text": "4.5 The Tangency Portfolio: Where Everyone Invests\n\n4.5.1 Deriving the Optimal Risky Portfolio\nWith a risk-free asset available, how should an investor allocate wealth across risky assets? Consider an investor who wants to achieve a target expected excess return \\(\\bar{\\mu}\\) with minimum variance. The optimization problem becomes:\n\\[\n\\min_\\omega \\omega'\\Sigma\\omega \\quad \\text{subject to} \\quad \\omega'\\tilde{\\mu} = \\bar{\\mu}\n\\]\nUsing the Lagrangian method:\n\\[\n\\mathcal{L}(\\omega, \\lambda) = \\omega'\\Sigma\\omega - \\lambda(\\omega'\\tilde{\\mu} - \\bar{\\mu})\n\\]\nThe first-order condition with respect to \\(\\omega\\) is:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda\\tilde{\\mu} = 0\n\\]\nSolving for the optimal weights:\n\\[\n\\omega^* = \\frac{\\lambda}{2}\\Sigma^{-1}\\tilde{\\mu}\n\\]\nThe constraint \\(\\omega'\\tilde{\\mu} = \\bar{\\mu}\\) determines \\(\\lambda\\):\n\\[\n\\bar{\\mu} = \\tilde{\\mu}'\\omega^* = \\frac{\\lambda}{2}\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu} \\implies \\lambda = \\frac{2\\bar{\\mu}}{\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu}}\n\\]\nSubstituting back:\n\\[\n\\omega^* = \\frac{\\bar{\\mu}}{\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu}}\\Sigma^{-1}\\tilde{\\mu}\n\\]\n\n\n4.5.2 The Tangency Portfolio\nNotice something remarkable: the direction of \\(\\omega^*\\) is always \\(\\Sigma^{-1}\\tilde{\\mu}\\), regardless of the target return \\(\\bar{\\mu}\\). Only the scale changes. This means all investors, regardless of their risk preferences, hold the same portfolio of risky assets. They differ only in how much they allocate to this portfolio versus the risk-free asset.\nTo obtain the portfolio of risky assets that is fully invested (weights summing to one), we normalize:\n\\[\n\\omega_{\\text{tan}} = \\frac{\\omega^*}{\\iota'\\omega^*} = \\frac{\\Sigma^{-1}(\\mu - r_f)}{\\iota'\\Sigma^{-1}(\\mu - r_f)}\n\\]\nThis is called the tangency portfolio (or maximum Sharpe ratio portfolio) because it lies at the point where the efficient frontier is tangent to the capital market line.\n\n\n4.5.3 The Sharpe Ratio and the Capital Market Line\nThe Sharpe ratio measures excess return per unit of volatility:\n\\[\n\\text{Sharpe Ratio} = \\frac{\\mu_p - r_f}{\\sigma_p}\n\\]\nThe tangency portfolio maximizes the Sharpe ratio among all possible portfolios. Any combination of the risk-free asset and the tangency portfolio lies on a straight line in mean-standard deviation space, called the Capital Market Line (CML):\n\\[\n\\mu_p = r_f + \\left(\\frac{\\mu_{\\text{tan}} - r_f}{\\sigma_{\\text{tan}}}\\right)\\sigma_p\n\\]\nThe slope of this line equals the Sharpe ratio of the tangency portfolio (i.e., the highest achievable Sharpe ratio).\n\n\n4.5.4 Computing the Tangency Portfolio\nLet’s compute the tangency portfolio for our VN30 universe:\n\nassets = (returns_monthly\n    .groupby(\"symbol\", as_index=False)\n    .agg(\n        mu=(\"ret\", \"mean\"),\n        sigma=(\"ret\", \"std\")\n    )\n)\n\nsigma = (returns_monthly\n    .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n    .cov()\n)\n\nmu = (returns_monthly\n    .groupby(\"symbol\")[\"ret\"]\n    .mean()\n    .values\n)\n\n\n# Compute tangency portfolio weights\nw_tan = np.linalg.solve(sigma, mu - rf)\nw_tan = w_tan / np.sum(w_tan)\n\n# Portfolio performance metrics\nmu_w = w_tan.T @ mu\nsigma_w = np.sqrt(w_tan.T @ sigma @ w_tan)\n\nefficient_portfolios = pd.DataFrame([\n    {\"symbol\": r\"$\\omega_{\\mathrm{tan}}$\", \"mu\": mu_w, \"sigma\": sigma_w},\n    {\"symbol\": r\"$r_f$\", \"mu\": rf, \"sigma\": 0}\n])\n\nsharpe_ratio = (mu_w - rf) / sigma_w\n\nprint(f\"Tangency Portfolio Sharpe Ratio: {sharpe_ratio:.4f}\")\nprint(efficient_portfolios)\n\nTangency Portfolio Sharpe Ratio: -0.5552\n                    symbol        mu     sigma\n0  $\\omega_{\\mathrm{tan}}$ -0.041157  0.077866\n1                    $r_f$  0.002076  0.000000\n\n\n\n\n4.5.5 Visualizing the Efficient Frontier with a Risk-Free Asset\nFigure 4.1 shows the efficient frontier when a risk-free asset is available. The frontier is now a straight line (the Capital Market Line) connecting the risk-free asset to the tangency portfolio and extending beyond.\n\nefficient_portfolios_figure = (\n    ggplot(efficient_portfolios, aes(x=\"sigma\", y=\"mu\"))\n    + geom_point(data=assets)\n    + geom_point(data=efficient_portfolios, color=\"blue\", size=3)\n    + geom_label(\n        aes(label=\"symbol\"), \n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Volatility (Standard Deviation)\", \n        y=\"Expected Return\",\n        title=\"Efficient Frontier with Risk-Free Asset (VN30)\"\n    )\n    + geom_abline(slope=sharpe_ratio, intercept=rf, linetype=\"dotted\")\n)\n\nefficient_portfolios_figure.show()\n\n\n\n\n\n\n\nFigure 4.1: The efficient frontier with a risk-free asset becomes a straight line (the Capital Market Line) connecting the risk-free rate to the tangency portfolio. Individual assets lie below this line, demonstrating the benefits of diversification.\n\n\n\n\n\nYou may notice that estimated expected returns appear quite low, some even negative. This is not a model failure but reflects the realities of estimation:\n\nSample period matters: If the estimation window includes market downturns (such as the 2022-2023 period), realized average returns can be near zero or negative. Mean-variance optimization takes sample means literally.\nEstimation noise in emerging markets: With volatile emerging market data, sample means are dominated by noise. A few extremely bad months can push the average below the risk-free rate even if the long-run equity premium is positive.\n\nThis highlights a fundamental challenge in portfolio optimization: the inputs we observe (historical returns) are noisy estimates of the true parameters we need (expected future returns).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-capm-equation-risk-and-expected-return",
    "href": "03_capm.html#the-capm-equation-risk-and-expected-return",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.6 The CAPM Equation: Risk and Expected Return",
    "text": "4.6 The CAPM Equation: Risk and Expected Return\n\n4.6.1 From Individual Optimization to Market Equilibrium\nSo far, we’ve focused on one investor’s optimization problem. The CAPM’s power comes from considering what happens when all investors optimize simultaneously.\nIf all investors follow mean-variance optimization, they all hold some combination of the risk-free asset and the tangency portfolio. The only difference between investors is their risk tolerance. More risk-averse investors hold more of the risk-free asset, while risk-tolerant investors may even borrow at the risk-free rate to leverage their position in the tangency portfolio.\n\n\n4.6.2 The Market Portfolio\nIn equilibrium, the total demand for each risky asset must equal its supply. Since all investors hold the same portfolio of risky assets (the tangency portfolio), the equilibrium portfolio weights must equal the market capitalization weights. The tangency portfolio is the market portfolio.\nThis insight has enormous practical implications: instead of estimating expected returns and covariances to compute the tangency portfolio, we can simply use the market portfolio (approximated by a broad market index) as a proxy.\n\n\n4.6.3 Deriving the CAPM Equation\nFrom the first-order conditions of the optimization problem, we derived that:\n\\[\n\\tilde{\\mu} = \\frac{2}{\\lambda}\\Sigma\\omega^*\n\\]\nSince \\(\\omega^*\\) is proportional to \\(\\omega_{\\text{tan}}\\), and in equilibrium \\(\\omega_{\\text{tan}}\\) equals the market portfolio \\(\\omega_m\\):\n\\[\n\\tilde{\\mu} = c \\cdot \\Sigma\\omega_m\n\\]\nfor some constant \\(c\\). The \\(i\\)-th element of \\(\\Sigma\\omega_m\\) is:\n\\[\n\\sum_{j=1}^N \\sigma_{ij}\\omega_{m,j} = \\text{Cov}(r_i, r_m)\n\\]\nwhere \\(r_m = \\sum_j \\omega_{m,j} r_j\\) is the return on the market portfolio.\nFor the market portfolio itself:\n\\[\n\\tilde{\\mu}_m = c \\cdot \\text{Var}(r_m) = c \\cdot \\sigma_m^2\n\\]\nTherefore \\(c = \\tilde{\\mu}_m / \\sigma_m^2\\), and for any asset \\(i\\):\n\\[\n\\tilde{\\mu}_i = \\frac{\\tilde{\\mu}_m}{\\sigma_m^2} \\text{Cov}(r_i, r_m) = \\beta_i \\tilde{\\mu}_m\n\\]\nwhere:\n\\[\n\\beta_i = \\frac{\\text{Cov}(r_i, r_m)}{\\text{Var}(r_m)}\n\\]\nThis is the famous CAPM equation:\n\\[\nE(r_i) - r_f = \\beta_i [E(r_m) - r_f]\n\\]\n\n\n4.6.4 Interpreting Beta\nBeta (\\(\\beta_i\\)) measures an asset’s systematic risk (i.e., its sensitivity to market movements). The interpretation is straightforward:\n\n\\(\\beta = 1\\): The asset moves one-for-one with the market (average systematic risk)\n\\(\\beta &gt; 1\\): The asset amplifies market movements (aggressive, high systematic risk)\n\\(\\beta &lt; 1\\): The asset dampens market movements (defensive, low systematic risk)\n\\(\\beta &lt; 0\\): The asset moves opposite to the market (provides insurance)\n\nThe CAPM says that expected excess return is proportional to beta, not to total volatility. This explains why:\n\nAn asset with zero beta earns only the risk-free rate (i.e., its risk is entirely idiosyncratic).\nAn asset with beta of 1 earns the market risk premium\nA negative-beta asset earns less than the risk-free rate (i.e., investors pay for its insurance properties).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-security-market-line",
    "href": "03_capm.html#the-security-market-line",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.7 The Security Market Line",
    "text": "4.7 The Security Market Line\nThe CAPM predicts a linear relationship between beta and expected return. This relationship is called the Security Market Line (SML):\n\\[\nE(r_i) = r_f + \\beta_i [E(r_m) - r_f]\n\\]\nUnlike the Capital Market Line (which plots expected return against total risk), the Security Market Line plots expected return against systematic risk (beta).\n\nbetas = (sigma @ w_tan) / (w_tan.T @ sigma @ w_tan)\nassets[\"beta\"] = betas.values\n\nprice_of_risk = float(w_tan.T @ mu - rf)\n\nassets_figure = (\n    ggplot(assets, aes(x=\"beta\", y=\"mu\"))\n    + geom_point()\n    + geom_abline(intercept=rf, slope=price_of_risk)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Beta (Systematic Risk)\", \n        y=\"Expected Return\",\n        title=\"Security Market Line\"\n    )\n    + annotate(\"text\", x=0.05, y=rf + 0.001, label=\"Risk-free rate\")\n)\n\nassets_figure.show()\n\n\n\n\n\n\n\n\nYou may observe that the estimated SML has a negative slope, which seems to contradict CAPM’s prediction. This reflects a negative estimated market risk premium in our sample period (i.e., the market portfolio earned less than the risk-free rate).\nWhen the market risk premium is negative, CAPM predicts that high-beta stocks should have lower expected returns than low-beta stocks. This is not a model failure, the model is behaving consistently. Rather, it reflects an unusual (but not impossible) sample period where risky assets underperformed safe assets.\nThis observation highlights an important distinction: CAPM describes expected returns in equilibrium, but realized returns over any particular period may differ substantially from expectations due to shocks and surprises.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#empirical-estimation-of-capm-parameters",
    "href": "03_capm.html#empirical-estimation-of-capm-parameters",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.8 Empirical Estimation of CAPM Parameters",
    "text": "4.8 Empirical Estimation of CAPM Parameters\n\n4.8.1 The Regression Framework\nIn practice, we estimate CAPM parameters using time-series regression. The model implies:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i(r_{m,t} - r_{f,t}) + \\varepsilon_{i,t}\n\\]\nwhere:\n\n\\(r_{i,t}\\): Return on asset \\(i\\) at time \\(t\\)\n\\(r_{f,t}\\): Risk-free rate at time \\(t\\)\n\\(r_{m,t}\\): Market return at time \\(t\\)\n\\(\\alpha_i\\): Intercept (should be zero if CAPM holds)\n\\(\\beta_i\\): Systematic risk (slope coefficient)\n\\(\\varepsilon_{i,t}\\): Idiosyncratic shock (residual)\n\n\n\n4.8.2 Alpha: Risk-Adjusted Performance\nThe intercept \\(\\alpha_i\\) measures risk-adjusted performance. If CAPM holds perfectly, alpha should be zero for all assets (i.e., any excess return is exactly compensated by systematic risk).\n\n\\(\\alpha &gt; 0\\): The asset outperformed its CAPM-predicted return (positive abnormal return)\n\\(\\alpha &lt; 0\\): The asset underperformed its CAPM-predicted return (negative abnormal return)\n\nPositive alpha is the holy grail of active management: earning returns beyond what systematic risk exposure would justify.\n\n\n4.8.3 Loading Factor Data\nWe use Fama-French market excess returns as our market portfolio proxy. These data provide a widely accepted benchmark that is already adjusted for the risk-free rate:\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nfactors.head(3)\n\n\n\n\n\n\n\n\ndate\nsmb\nhml\nrmw\ncma\nmkt_excess\n\n\n\n\n0\n2011-07-31\n0.008933\n-0.013099\n0.012139\n-0.009529\n-0.011287\n\n\n1\n2011-08-31\n0.004830\n-0.016656\n0.014516\n-0.003981\n0.007856\n\n\n2\n2011-09-30\n0.004970\n-0.000462\n0.008899\n0.001241\n-0.006501\n\n\n\n\n\n\n\n\n\n4.8.4 Running the Regressions\nWe estimate CAPM regressions for each stock in our universe:\n\nimport statsmodels.formula.api as smf\n\nreturns_excess_monthly = (returns_monthly\n    .merge(factors, on=\"date\", how=\"left\")\n    .assign(ret_excess=lambda x: x[\"ret\"] - x[\"risk_free\"])\n)\n\n\ndef estimate_capm(data):\n    model = smf.ols(\"ret_excess ~ mkt_excess\", data=data).fit()\n    result = pd.DataFrame({\n        \"coefficient\": [\"alpha\", \"beta\"],\n        \"estimate\": model.params.values,\n        \"t_statistic\": model.tvalues.values\n    })\n    return result\n\n\ncapm_results = (returns_excess_monthly\n    .groupby(\"symbol\", group_keys=True)\n    .apply(estimate_capm)\n    .reset_index()\n)\n\ncapm_results.head(4)\n\n\n\n\n\n\n\n\nsymbol\nlevel_1\ncoefficient\nestimate\nt_statistic\n\n\n\n\n0\nACB\n0\nalpha\n-0.007479\n-0.876326\n\n\n1\nACB\n1\nbeta\n-0.240019\n-0.252484\n\n\n2\nBCM\n0\nalpha\n0.027827\n1.737895\n\n\n3\nBCM\n1\nbeta\n4.150186\n2.625283\n\n\n\n\n\n\n\n\n\n4.8.5 Visualizing Alpha Estimates\nFigure 4.2 shows the estimated alphas across our VN30 sample. Statistical significance (at the 95% level) is indicated by color.\n\nalphas = (capm_results\n    .query(\"coefficient == 'alpha'\")\n    .assign(is_significant=lambda x: np.abs(x[\"t_statistic\"]) &gt;= 1.96)\n)\n\nalphas[\"symbol\"] = pd.Categorical(\n    alphas[\"symbol\"],\n    categories=alphas.sort_values(\"estimate\")[\"symbol\"],\n    ordered=True\n)\n\nalphas_figure = (\n    ggplot(alphas, aes(y=\"estimate\", x=\"symbol\", fill=\"is_significant\"))\n    + geom_col()\n    + scale_y_continuous(labels=percent_format())\n    + coord_flip()\n    + labs(\n        x=\"\", \n        y=\"Estimated Alpha (Monthly)\", \n        fill=\"Significant at 95%?\",\n        title=\"Estimated CAPM Alphas for VN30 Index Constituents\"\n    )\n)\n\nalphas_figure.show()\n\n\n\n\n\n\n\nFigure 4.2: Estimated CAPM alphas for VN30 index constituents. Color indicates statistical significance at the 95% confidence level. Most alphas are statistically indistinguishable from zero, consistent with CAPM predictions.\n\n\n\n\n\nThe distribution of alphas provides evidence on CAPM’s empirical validity. If the model holds, we expect:\n\nMost alphas close to zero\nFew statistically significant alphas\nRoughly equal numbers of positive and negative alphas\n\nSystematic patterns in alphas, such as consistently positive alphas for certain types of stocks, would suggest the CAPM is incomplete and that additional risk factors may be needed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#limitations-and-extensions",
    "href": "03_capm.html#limitations-and-extensions",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.9 Limitations and Extensions",
    "text": "4.9 Limitations and Extensions\n\n4.9.1 The Market Portfolio Problem\nA fundamental challenge in testing the CAPM is identifying the market portfolio. The theory requires a portfolio that includes all investable assets, not just stocks, but also bonds, real estate, private businesses, human capital, and even intangible assets. In practice, we use proxies like broad market indices (VNI, S&P 500), but these capture only publicly traded equities.\nThis limitation is profound. As Richard Roll famously argued, the CAPM is essentially untestable because the true market portfolio is unobservable. Any test of the CAPM is simultaneously a test of whether our proxy adequately represents the market.\n\n\n4.9.2 Time-Varying Betas\nThe CAPM assumes that betas are constant over time, but this assumption rarely holds in practice. Companies undergo changes that affect their market sensitivity:\n\nCapital structure changes: Increasing leverage raises beta\nBusiness model evolution: Diversification into new industries can alter systematic risk\nMarket conditions: Betas often increase during market stress\n\nConditional CAPM models (Jagannathan and Wang 1996) address this by allowing risk premiums and betas to vary with the business cycle.\n\n\n4.9.3 Empirical Anomalies\nDecades of empirical research have documented patterns in stock returns that CAPM cannot explain:\n\nSize effect: Small-cap stocks tend to outperform large-cap stocks, even after adjusting for beta\nValue effect: Stocks with high book-to-market ratios outperform growth stocks\nMomentum: Stocks that performed well recently tend to continue performing well\n\nThese anomalies suggest that systematic risk has multiple dimensions beyond market exposure.\n\n\n4.9.4 Multifactor Extensions\nThe limitations of CAPM have led to increasingly sophisticated asset pricing models. The Fama-French three-factor model (Fama and French 1992) adds two factors to capture size and value effects:\n\nSMB (Small Minus Big): Returns on small stocks minus large stocks\nHML (High Minus Low): Returns on value stocks minus growth stocks\n\nThe Fama-French five-factor model (Fama and French 2015) adds two more dimensions:\n\nRMW (Robust Minus Weak): Returns on profitable firms minus unprofitable firms\n\nCMA (Conservative Minus Aggressive): Returns on conservative investors minus aggressive investors\n\nThe Carhart four-factor model (Carhart 1997) adds momentum to the three-factor framework.\nOther theoretical developments include:\n\nConsumption CAPM: Links asset prices to macroeconomic consumption risk\nQ-factor model (Hou, Xue, and Zhang 2014): Derives factors from investment-based asset pricing theory\nArbitrage Pricing Theory: Allows for multiple sources of systematic risk without specifying their identity\n\nDespite its limitations, the CAPM remains valuable as a conceptual benchmark. Its core insight (i.e., only systematic, undiversifiable risk commands a premium) continues to inform how we think about risk and return.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#key-takeaways",
    "href": "03_capm.html#key-takeaways",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.10 Key Takeaways",
    "text": "4.10 Key Takeaways\nThis chapter introduced the Capital Asset Pricing Model and its implications for understanding the relationship between risk and expected return. The main insights are:\n\nNot all risk is rewarded: The CAPM distinguishes between systematic risk (which cannot be diversified away and commands a premium) and idiosyncratic risk (which can be eliminated through diversification and earns no premium).\nThe tangency portfolio is universal: When a risk-free asset exists, all mean-variance investors hold the same portfolio of risky assets (i.e., the tangency or maximum Sharpe ratio portfolio). They differ only in how much they allocate to this portfolio versus the risk-free asset.\nIn equilibrium, the tangency portfolio is the market portfolio: Since all investors hold the same risky portfolio, and total demand must equal supply, the equilibrium portfolio weights are market capitalization weights.\nExpected returns depend on beta: The CAPM equation states that expected excess return equals beta times the market risk premium. Beta measures covariance with the market portfolio, normalized by market variance.\nAlpha measures risk-adjusted performance: Positive alpha indicates returns above what systematic risk would justify; negative alpha indicates underperformance.\nEmpirical challenges exist: Testing the CAPM requires identifying the market portfolio, which is unobservable in practice. Documented anomalies (size, value, momentum) suggest additional risk factors beyond market exposure.\nExtensions abound: Multifactor models like Fama-French extend the CAPM framework by adding factors that capture dimensions of systematic risk the market factor misses.\n\nThe CAPM’s elegance lies in its simplicity: a single factor (i.e., exposure to the market) should explain expected returns in equilibrium. While reality is more complex, this framework provides the foundation for all modern asset pricing theory.\n\n\n\n\n\n\nCarhart, Mark M. 1997. “On persistence in mutual fund performance.” The Journal of Finance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock returns.” The Journal of Finance 47 (2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2014. “Digesting anomalies: An investment approach.” Review of Financial Studies 28 (3): 650–705. https://doi.org/10.1093/rfs/hhu068.\n\n\nJagannathan, Ravi, and Zhenyu Wang. 1996. “The conditional CAPM and the cross-section of expected returns.” The Journal of Finance 51 (1): 3–53. https://doi.org/10.2307/2329301.\n\n\nLintner, John. 1965. “Security prices, risk, and maximal gains from diversification.” The Journal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital asset market.” Econometrica 34 (4): 768–83. https://doi.org/10.2307/1910098.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html",
    "href": "04_financial_statement_analysis.html",
    "title": "5  Financial Statement Analysis",
    "section": "",
    "text": "5.1 From Market Prices to Fundamental Value\nThe previous chapters focused on how financial markets price assets in equilibrium. The Capital Asset Pricing Model showed that expected returns depend on systematic risk exposure, while Modern Portfolio Theory demonstrated how to construct efficient portfolios. But these frameworks take expected returns and risk as given, they don’t explain where these expectations come from.\nFinancial statement analysis addresses this gap. By examining a company’s accounting records, investors can form independent assessments of firm value, identify mispriced securities, and understand the economic forces driving business performance. Financial statements provide the primary source of standardized information about a company’s operations, financial position, and cash generation. Their legal requirements and standardized formats make them particularly valuable. Every publicly traded company must file them, creating a level playing field for analysis.\nThis chapter introduces the three primary financial statements: the balance sheet, income statement, and cash flow statement. We then demonstrate how to transform raw accounting data into meaningful financial ratios that facilitate comparison across companies and over time. These ratios serve multiple purposes: they enable investors to benchmark companies against peers, help creditors assess default risk, and provide inputs for asset pricing models like the Fama-French factors we will encounter in later chapters.\nOur analysis combines theoretical frameworks with practical implementation using Vietnamese market data. By the end of this chapter, you will understand how to access financial statements, calculate key ratios across multiple categories, and interpret these metrics in context.\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#the-three-financial-statements",
    "href": "04_financial_statement_analysis.html#the-three-financial-statements",
    "title": "5  Financial Statement Analysis",
    "section": "5.2 The Three Financial Statements",
    "text": "5.2 The Three Financial Statements\nBefore diving into ratios and analysis, we need to understand the three interconnected statements that form the foundation of financial reporting. Each statement answers a different question about the company, and together they provide a comprehensive picture of financial health.\n\n5.2.1 The Balance Sheet: A Snapshot of Financial Position\nThe balance sheet captures a company’s financial position at a specific moment in time, think of it as a photograph rather than a movie. It lists everything the company owns (assets), everything it owes (liabilities), and the residual claim belonging to shareholders (equity). These three components are linked by the fundamental accounting equation:\n\\[\n\\text{Assets} = \\text{Liabilities} + \\text{Equity}\n\\]\nThis equation is not merely a definition, it reflects a core economic principle. A company’s resources (assets) must be financed from somewhere: either borrowed from creditors (liabilities) or contributed by owners (equity). Every transaction affects both sides equally, maintaining the balance.\nAssets represent resources the company controls that are expected to generate future economic benefits:\n\nCurrent assets can be converted to cash within one year: cash and equivalents, short-term investments, accounts receivable (money owed by customers), and inventory (raw materials, work-in-progress, and finished goods)\nNon-current assets support operations beyond one year: property, plant, and equipment (PP&E), long-term investments, and intangible assets like patents, trademarks, and goodwill\n\nLiabilities encompass obligations to external parties:\n\nCurrent liabilities come due within one year: accounts payable (money owed to suppliers), short-term debt, accrued expenses, and the current portion of long-term debt\nNon-current liabilities extend beyond one year: long-term debt, bonds payable, pension obligations, and deferred tax liabilities\n\nShareholders’ equity represents the owners’ residual claim:\n\nCommon stock and additional paid-in capital from share issuance\nRetained earnings (i.e., accumulated profits reinvested rather than distributed as dividends)\nTreasury stock: shares repurchased by the company\n\nUnderstanding these categories is essential for ratio analysis. Current assets and liabilities determine short-term liquidity, while the mix of debt and equity reveals capital structure choices.\n\n\n5.2.2 The Income Statement: Performance Over Time\nWhile the balance sheet provides a snapshot, the income statement (also called the profit and loss statement, or P&L) measures financial performance over a period (e.g., a quarter or year). It follows a hierarchical structure that progressively captures different levels of profitability:\n\\[\n\\text{Revenue} - \\text{COGS} = \\text{Gross Profit}\n\\]\n\\[\n\\text{Gross Profit} - \\text{Operating Expenses} = \\text{Operating Income (EBIT)}\n\\]\n\\[\n\\text{EBIT} - \\text{Interest} - \\text{Taxes} = \\text{Net Income}\n\\]\nEach line reveals something different about the business:\n\nRevenue (Sales): Total income from goods or services sold (i.e., the “top line”)\nCost of Goods Sold (COGS): Direct costs of producing what was sold (materials, direct labor, manufacturing overhead)\nGross Profit: Revenue minus COGS, measuring basic profitability from core operations\nOperating Expenses: Costs of running the business beyond production (selling, general & administrative expenses, research & development)\nOperating Income (EBIT): Earnings Before Interest and Taxes, measuring profitability from operations before financing decisions and taxes\nInterest Expense: The cost of debt financing\nNet Income: The “bottom line” (i.e., total profit after all expenses)\n\nThe income statement’s hierarchical structure allows analysts to identify where profitability problems originate. A company with strong gross margins but weak net income might have bloated overhead costs. One with weak gross margins faces fundamental pricing or production challenges.\n\n\n5.2.3 The Cash Flow Statement: Following the Money\nThe cash flow statement bridges a critical gap: profitable companies can run out of cash, and unprofitable companies can generate positive cash flow. This happens because accrual accounting (used in the income statement) recognizes revenue when earned and expenses when incurred, not when cash changes hands.\nThe cash flow statement tracks actual cash movements, divided into three categories:\n\nOperating activities: Cash generated from core business operations. Starts with net income, then adjusts for non-cash items (depreciation, changes in working capital)\nInvesting activities: Cash spent on or received from long-term investments (e.g., purchasing equipment, acquiring businesses, selling assets)\nFinancing activities: Cash flows from capital structure decisions (e.g., issuing stock, borrowing, repaying debt, paying dividends, buying back shares)\n\nA company can show strong net income while burning cash if it’s building inventory, extending generous credit terms, or making large capital expenditures. Conversely, a company reporting losses might generate positive operating cash flow by collecting receivables faster than it pays suppliers.\n\n\n5.2.4 Illustrating with FPT’s Financial Statements\nTo see these concepts in practice, let’s examine FPT Corporation’s 2023 financial statements. FPT is one of Vietnam’s largest technology companies, providing IT services, telecommunications, and education.\n\n# Placeholder for FPT balance sheet visualization\n# In practice, this would display the actual PDF or cleaned data\n# from DataCore's acquisition pipeline\n\n# Example structure of what the balance sheet data looks like:\n# Assets: Current assets (cash, receivables, inventory) + Non-current assets (PP&E, intangibles)\n# Liabilities: Current liabilities (payables, short-term debt) + Non-current liabilities (long-term debt)\n# Equity: Common stock + Retained earnings\n\nThe balance sheet demonstrates the fundamental accounting equation in action. FPT’s assets (e.g., spanning cash, receivables, technology infrastructure, and intangible assets like software) exactly equal the sum of its liabilities and equity.\n\n# Placeholder for FPT income statement visualization\n# Shows the progression from revenue through various profit measures to net income\n\nFPT’s income statement reveals how the company transforms revenue into profit. The progression from gross profit through operating income to net income shows the impact of operating expenses, interest costs, and taxes.\n\n# Placeholder for FPT cash flow statement visualization\n# Reconciles net income with actual cash generation\n\nThe cash flow statement shows how FPT’s reported profits translate into actual cash. Differences between net income and operating cash flow reveal the impact of working capital management and non-cash expenses.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#loading-financial-statement-data",
    "href": "04_financial_statement_analysis.html#loading-financial-statement-data",
    "title": "5  Financial Statement Analysis",
    "section": "5.3 Loading Financial Statement Data",
    "text": "5.3 Loading Financial Statement Data\nWe now turn to systematic analysis across multiple companies. We load financial statement data for the VN30 index constituents (i.e., the 30 largest and most liquid stocks on Vietnam’s Ho Chi Minh Stock Exchange).\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncomp_vn = pd.read_sql_query(\n    sql=\"SELECT * FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\ncomp_vn.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nxsga\nxint\noancf\ncapx\nbe\nop\nat_lag\ninv\ntotal_debt\nselling_general_and_administrative_expenses\n\n\n\n\n0\nAGF\n1998\n8.845141e+10\nNone\n5.469709e+09\n0.000000e+00\nNone\nNone\n0.0\n1.110705e+10\n...\n1.199765e+10\n0.0\nNaN\nNaN\n2.656020e+10\n0.711195\nNaN\nNaN\n0.000000e+00\n1.990718e+10\n\n\n1\nBBC\n1999\n5.672574e+10\nNone\n5.354939e+09\n5.354939e+09\nNone\nNone\n0.0\n0.000000e+00\n...\n9.396468e+09\n0.0\n2.687635e+10\n1.097031e+10\n3.211410e+10\n0.728193\nNaN\nNaN\n1.505529e+09\n2.387858e+10\n\n\n2\nAGF\n1999\n9.558392e+10\nNone\n2.609276e+09\n0.000000e+00\nNone\nNone\n0.0\n1.008298e+10\n...\n1.595913e+10\n0.0\n1.675607e+10\n3.970966e+09\n3.576596e+10\n0.816972\n1.068410e+11\n0.090477\n0.000000e+00\n2.744458e+10\n\n\n\n\n3 rows × 333 columns\n\n\n\n\nvn30_symbols = [\n    \"ACB\", \"BCM\", \"BID\", \"BVH\", \"CTG\", \"FPT\", \"GAS\", \"GVR\", \"HDB\", \"HPG\",\n    \"MBB\", \"MSN\", \"MWG\", \"PLX\", \"POW\", \"SAB\", \"SHB\", \"SSB\", \"STB\", \"TCB\",\n    \"TPB\", \"VCB\", \"VHM\", \"VIB\", \"VIC\", \"VJC\", \"VNM\", \"VPB\", \"VRE\", \"EIB\"\n]\n\ncomp_vn30 = comp_vn[comp_vn[\"symbol\"].isin(vn30_symbols)]\ncomp_vn30.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nxsga\nxint\noancf\ncapx\nbe\nop\nat_lag\ninv\ntotal_debt\nselling_general_and_administrative_expenses\n\n\n\n\n11\nFPT\n2002\n5.098910e+11\nNone\n1.027470e+11\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n4.178100e+10\n9.008000e+09\n3.203600e+10\n2.202800e+10\n3.125400e+10\n3.293018\nNaN\nNaN\n0.0\n1.235850e+11\n\n\n17\nVNM\n2003\n2.101406e+12\nNone\n6.925924e+11\n6.925924e+11\nNone\nNone\n0.0\n0.0\n...\n9.089413e+10\n1.698909e+10\nNaN\nNaN\n1.560789e+12\n0.663257\nNaN\nNaN\n0.0\n5.037799e+11\n\n\n21\nFPT\n2003\n9.171390e+11\nNone\n7.995600e+10\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n8.584800e+10\n1.286700e+10\n-2.905420e+11\n3.753300e+10\n1.697000e+11\n0.940218\n5.504080e+11\n0.779104\n0.0\n1.968430e+11\n\n\n\n\n3 rows × 333 columns\n\n\n\nThis dataset provides the foundation for calculating financial ratios and conducting cross-sectional comparisons. Each row contains balance sheet, income statement, and cash flow items for a company-year observation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#liquidity-ratios-can-the-company-pay-its-bills",
    "href": "04_financial_statement_analysis.html#liquidity-ratios-can-the-company-pay-its-bills",
    "title": "5  Financial Statement Analysis",
    "section": "5.4 Liquidity Ratios: Can the Company Pay Its Bills?",
    "text": "5.4 Liquidity Ratios: Can the Company Pay Its Bills?\nLiquidity ratios assess a company’s ability to meet short-term obligations. These metrics matter most to creditors, suppliers, and employees who need assurance that the company can pay its bills. They’re calculated using balance sheet items, comparing liquid assets against near-term liabilities.\n\n5.4.1 The Current Ratio\nThe most basic liquidity measure compares all current assets to current liabilities:\n\\[\n\\text{Current Ratio} = \\frac{\\text{Current Assets}}{\\text{Current Liabilities}}\n\\]\nA ratio above one indicates the company has enough current assets to cover obligations due within one year. However, the interpretation depends heavily on the composition of current assets. A company with current assets tied up in slow-moving inventory is less liquid than one holding cash.\n\n\n5.4.2 The Quick Ratio\nThe quick ratio (or “acid test”) provides a more stringent measure by excluding inventory:\n\\[\n\\text{Quick Ratio} = \\frac{\\text{Current Assets} - \\text{Inventory}}{\\text{Current Liabilities}}\n\\]\nWhy exclude inventory? Inventory is typically the least liquid current asset. It must be sold (potentially at a discount) before generating cash. A company facing a liquidity crisis cannot easily convert raw materials or finished goods into immediate cash. The quick ratio answers: “Can we pay our bills without relying on inventory sales?”\n\n\n5.4.3 The Cash Ratio\nThe most conservative liquidity measure focuses solely on the most liquid assets:\n\\[\n\\text{Cash Ratio} = \\frac{\\text{Cash and Cash Equivalents}}{\\text{Current Liabilities}}\n\\]\nWhile a ratio of one indicates robust liquidity, most companies maintain lower cash ratios to avoid holding excessive non-productive assets. Cash sitting in bank accounts could otherwise be invested in growth opportunities, returned to shareholders, or used to pay down costly debt.\n\n\n5.4.4 Calculating Liquidity Ratios\nLet’s compute these ratios for our VN30 sample:\n\nbalance_sheet_statements = (comp_vn30\n    .assign(\n        fiscal_year=lambda x: x[\"year\"].astype(int),\n        \n        # Current Ratio: Current Assets / Current Liabilities\n        current_ratio=lambda x: x[\"act\"] / x[\"lct\"],\n        \n        # Quick Ratio: (Current Assets - Inventory) / Current Liabilities\n        quick_ratio=lambda x: (x[\"act\"] - x[\"inv\"]) / x[\"lct\"],\n        \n        # Cash Ratio: Cash and Equivalents / Current Liabilities\n        cash_ratio=lambda x: x[\"ca_cce\"] / x[\"lct\"],\n        \n        label=lambda x: np.where(\n            x[\"symbol\"].isin(vn30_symbols), x[\"symbol\"], np.nan\n        )\n    )\n)\n\nbalance_sheet_statements.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nop\nat_lag\ninv\ntotal_debt\nselling_general_and_administrative_expenses\nfiscal_year\ncurrent_ratio\nquick_ratio\ncash_ratio\nlabel\n\n\n\n\n11\nFPT\n2002\n5.098910e+11\nNone\n1.027470e+11\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n3.293018\nNaN\nNaN\n0.0\n1.235850e+11\n2002\n1.211413\nNaN\n0.244109\nFPT\n\n\n17\nVNM\n2003\n2.101406e+12\nNone\n6.925924e+11\n6.925924e+11\nNone\nNone\n0.0\n0.0\n...\n0.663257\nNaN\nNaN\n0.0\n5.037799e+11\n2003\n2.195772\nNaN\n0.723694\nVNM\n\n\n21\nFPT\n2003\n9.171390e+11\nNone\n7.995600e+10\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n0.940218\n5.504080e+11\n0.779104\n0.0\n1.968430e+11\n2003\n1.274633\n1.274633\n0.111122\nFPT\n\n\n\n\n3 rows × 338 columns\n\n\n\n\n\n5.4.5 Cross-Sectional Comparison of Liquidity\nFigure 5.1 compares liquidity ratios across companies for the most recent fiscal year. This cross-sectional view reveals how different business models and industries maintain different liquidity profiles.\n\nliquidity_ratios = (balance_sheet_statements\n    .query(\"year == 2023 & label.notna()\")\n    .get([\"symbol\", \"current_ratio\", \"quick_ratio\", \"cash_ratio\"])\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        name=lambda x: x[\"name\"].str.replace(\"_\", \" \").str.title()\n    )\n)\n\nliquidity_ratios_figure = (\n    ggplot(liquidity_ratios, aes(y=\"value\", x=\"name\", fill=\"symbol\"))\n    + geom_col(position=\"dodge\")\n    + coord_flip()\n    + labs(\n        x=\"\", y=\"Ratio Value\", fill=\"\",\n        title=\"Liquidity Ratios for VN30 Stocks (2023)\"\n    )\n)\n\nliquidity_ratios_figure.show()\n\n\n\n\n\n\n\nFigure 5.1: Liquidity ratios measure a company’s ability to meet short-term obligations. Higher values indicate greater liquidity, though excessively high ratios may suggest inefficient use of assets.\n\n\n\n\n\nSeveral patterns emerge from this comparison. Banks and financial institutions typically show different liquidity profiles than industrial companies due to their unique business models. Companies with high inventory (retailers, manufacturers) often show larger gaps between current and quick ratios.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#leverage-ratios-how-is-the-company-financed",
    "href": "04_financial_statement_analysis.html#leverage-ratios-how-is-the-company-financed",
    "title": "5  Financial Statement Analysis",
    "section": "5.5 Leverage Ratios: How Is the Company Financed?",
    "text": "5.5 Leverage Ratios: How Is the Company Financed?\nLeverage ratios examine a company’s capital structure (i.e., the mix of debt and equity financing). These metrics reveal financial risk and long-term solvency, helping investors understand how much of the company’s operations are funded by borrowed money.\n\n5.5.1 Why Capital Structure Matters\nA company’s financing choice involves fundamental trade-offs:\n\nDebt offers tax advantages (interest is deductible) and doesn’t dilute ownership, but creates fixed obligations that must be met regardless of business performance\nEquity provides flexibility (no required payments) but dilutes existing shareholders and may be more expensive than debt\n\nCompanies with high leverage amplify both gains and losses. In good times, shareholders capture more upside because profits aren’t shared with additional equity holders. In bad times, fixed interest payments can push the company toward distress. This is why beta (systematic risk) tends to increase with leverage.\n\n\n5.5.2 Debt-to-Equity Ratio\nThis ratio indicates how much debt financing the company uses relative to shareholder investment:\n\\[\n\\text{Debt-to-Equity} = \\frac{\\text{Total Debt}}{\\text{Total Equity}}\n\\]\nA ratio of 1.0 means equal parts debt and equity financing. Higher ratios indicate more aggressive use of leverage, which can enhance returns in good times but increases bankruptcy risk.\n\n\n5.5.3 Debt-to-Asset Ratio\nThis ratio shows what percentage of assets are financed through debt:\n\\[\n\\text{Debt-to-Asset} = \\frac{\\text{Total Debt}}{\\text{Total Assets}}\n\\]\nA ratio of 0.5 means half the company’s assets are debt-financed. This metric is bounded between 0 and 1 (assuming positive equity), making it easier to compare across companies than the debt-to-equity ratio.\n\n\n5.5.4 Interest Coverage Ratio\nWhile the above ratios measure leverage levels, interest coverage assesses the ability to service that debt:\n\\[\n\\text{Interest Coverage} = \\frac{\\text{EBIT}}{\\text{Interest Expense}}\n\\]\nThis ratio answers: “How many times over can current operating profits cover interest obligations?” A ratio below 1.0 means operating income doesn’t cover interest payments, which is a dangerous position. Ratios above 3-5 generally indicate comfortable coverage.\n\n\n5.5.5 Calculating Leverage Ratios\n\nbalance_sheet_statements = balance_sheet_statements.assign(\n    debt_to_equity=lambda x: x[\"total_debt\"] / x[\"total_equity\"],\n    debt_to_asset=lambda x: x[\"total_debt\"] / x[\"at\"]\n)\n\nincome_statements = (comp_vn30\n    .assign(\n        year=lambda x: x[\"year\"].astype(int),\n        # Handle zero interest expense to avoid infinity\n        interest_coverage=lambda x: np.where(\n            x[\"cfo_interest_expense\"] &gt; 0,\n            x[\"is_net_business_profit\"] / x[\"cfo_interest_expense\"],\n            np.nan\n        ),\n        label=lambda x: np.where(\n            x[\"symbol\"].isin(vn30_symbols), x[\"symbol\"], np.nan\n        )\n    )\n)\n\n\n\n5.5.6 Leverage Trends Over Time\nFigure 5.2 tracks how debt-to-asset ratios have evolved over time. Time-series analysis reveals whether companies are becoming more or less leveraged.\n\ndebt_to_asset = balance_sheet_statements.query(\"symbol in @vn30_symbols\")\n\ndebt_to_asset_figure = (\n    ggplot(debt_to_asset, aes(x=\"year\", y=\"debt_to_asset\", color=\"symbol\"))\n    + geom_line(size=1)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Debt-to-Asset Ratio\", color=\"\",\n        title=\"Debt-to-Asset Ratios of VN30 Stocks Over Time\"\n    )\n)\n\ndebt_to_asset_figure.show()\n\n\n\n\n\n\n\nFigure 5.2: Debt-to-asset ratios show the proportion of assets financed by debt. Changes over time reflect evolving capital structure strategies and market conditions.\n\n\n\n\n\n\n\n5.5.7 Cross-Sectional Leverage Comparison\nFigure 5.3 provides a snapshot of leverage across all VN30 constituents for the most recent year.\n\ndebt_to_asset_comparison = balance_sheet_statements.query(\"year == 2023\")\n\ndebt_to_asset_comparison[\"symbol\"] = pd.Categorical(\n    debt_to_asset_comparison[\"symbol\"],\n    categories=debt_to_asset_comparison.sort_values(\"debt_to_asset\")[\"symbol\"],\n    ordered=True\n)\n\ndebt_to_asset_comparison_figure = (\n    ggplot(\n        debt_to_asset_comparison,\n        aes(y=\"debt_to_asset\", x=\"symbol\", fill=\"label\")\n    )\n    + geom_col()\n    + coord_flip()\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Debt-to-Asset Ratio\", fill=\"\",\n        title=\"Debt-to-Asset Ratios of VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\ndebt_to_asset_comparison_figure.show()\n\n\n\n\n\n\n\nFigure 5.3: Cross-sectional comparison of debt-to-asset ratios reveals industry patterns and company-specific financing strategies.\n\n\n\n\n\n\n\n5.5.8 The Leverage-Coverage Trade-off\nFigure 5.4 examines the relationship between leverage levels and debt-servicing ability. Companies with higher debt loads should ideally have stronger interest coverage to maintain financial stability.\n\ninterest_coverage = (income_statements\n    .query(\"year == 2023\")\n    .get([\"symbol\", \"year\", \"interest_coverage\"])\n    .merge(balance_sheet_statements, on=[\"symbol\", \"year\"], how=\"left\")\n)\n\ninterest_coverage_figure = (\n    ggplot(\n        interest_coverage,\n        aes(x=\"debt_to_asset\", y=\"interest_coverage\", color=\"label\")\n    )\n    + geom_point(size=2)\n    + geom_label(\n        aes(label=\"label\"),\n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + labs(\n        x=\"Debt-to-Asset Ratio\", y=\"Interest Coverage Ratio\",\n        title=\"Leverage versus Interest Coverage for VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\ninterest_coverage_figure.show()\n\n\n\n\n\n\n\nFigure 5.4: The relationship between leverage and interest coverage reveals whether companies can comfortably service their debt. High leverage with low coverage indicates elevated financial risk.\n\n\n\n\n\nThe scatter plot reveals important patterns. Companies in the upper-left quadrant (low leverage, high coverage) have conservative financing with ample debt capacity. Those in the lower-right (high leverage, low coverage) face elevated financial risk.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#efficiency-ratios-how-well-are-assets-managed",
    "href": "04_financial_statement_analysis.html#efficiency-ratios-how-well-are-assets-managed",
    "title": "5  Financial Statement Analysis",
    "section": "5.6 Efficiency Ratios: How Well Are Assets Managed?",
    "text": "5.6 Efficiency Ratios: How Well Are Assets Managed?\nEfficiency ratios measure how effectively a company utilizes its assets and manages operations. These metrics help identify whether management is extracting maximum value from the company’s resource base.\n\n5.6.1 Asset Turnover\nThis ratio measures how efficiently a company uses total assets to generate revenue:\n\\[\n\\text{Asset Turnover} = \\frac{\\text{Revenue}}{\\text{Total Assets}}\n\\]\nA higher ratio indicates more efficient asset utilization: the company generates more sales per dollar of assets. However, optimal levels vary dramatically across industries. Retailers with minimal fixed assets might achieve turnovers above 2.0, while capital-intensive manufacturers might operate below 0.5.\n\n\n5.6.2 Inventory Turnover\nFor companies carrying inventory, this ratio reveals how quickly stock moves through the business:\n\\[\n\\text{Inventory Turnover} = \\frac{\\text{Cost of Goods Sold}}{\\text{Inventory}}\n\\]\nHigher turnover suggests efficient inventory management (i.e., goods don’t sit on shelves collecting dust). However, extremely high turnover might indicate stockout risks, while very low turnover could signal obsolete inventory or overinvestment in working capital.\nWe use COGS rather than revenue in the numerator because inventory is recorded at cost, not selling price. Using revenue would overstate turnover for high-margin businesses.\n\n\n5.6.3 Receivables Turnover\nThis ratio measures how effectively a company collects payments from customers:\n\\[\n\\text{Receivables Turnover} = \\frac{\\text{Revenue}}{\\text{Accounts Receivable}}\n\\]\nHigher turnover indicates faster collection (i.e., customers pay promptly). Converting this to “days sales outstanding” (365 / turnover) gives the average collection period in days. Companies must balance collection efficiency against the sales impact of restrictive credit policies.\n\n\n5.6.4 Calculating Efficiency Ratios\n\ncombined_statements = (balance_sheet_statements\n    .get([\n        \"symbol\", \"year\", \"label\", \"current_ratio\", \"quick_ratio\",\n        \"cash_ratio\", \"debt_to_equity\", \"debt_to_asset\", \"total_asset\",\n        \"total_equity\"\n    ])\n    .merge(\n        (income_statements\n            .get([\n                \"symbol\", \"year\", \"interest_coverage\", \"is_revenue\",\n                \"is_cogs\", \"selling_general_and_administrative_expenses\",\n                \"is_interest_expense\", \"is_gross_profit\", \"is_eat\"\n            ])\n        ),\n        on=[\"symbol\", \"year\"],\n        how=\"left\"\n    )\n    .merge(\n        (comp_vn30\n            .assign(year=lambda x: x[\"year\"].astype(int))\n            .get([\"symbol\", \"year\", \"ca_total_inventory\", \"ca_acc_receiv\"])\n        ),\n        on=[\"symbol\", \"year\"],\n        how=\"left\"\n    )\n)\n\ncombined_statements = combined_statements.assign(\n    asset_turnover=lambda x: x[\"is_revenue\"] / x[\"total_asset\"],\n    inventory_turnover=lambda x: x[\"is_cogs\"] / x[\"ca_total_inventory\"],\n    receivables_turnover=lambda x: x[\"is_revenue\"] / x[\"ca_acc_receiv\"]\n)\n\nEfficiency ratios vary dramatically across industries, making peer comparison essential. A grocery store and a shipbuilder will have fundamentally different asset and inventory dynamics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#profitability-ratios-is-the-company-making-money",
    "href": "04_financial_statement_analysis.html#profitability-ratios-is-the-company-making-money",
    "title": "5  Financial Statement Analysis",
    "section": "5.7 Profitability Ratios: Is the Company Making Money?",
    "text": "5.7 Profitability Ratios: Is the Company Making Money?\nProfitability ratios evaluate how effectively a company converts activity into earnings. These metrics directly measure financial success and are among the most closely watched indicators by investors.\n\n5.7.1 Gross Margin\nThe gross margin reveals what percentage of revenue remains after direct production costs:\n\\[\n\\text{Gross Margin} = \\frac{\\text{Gross Profit}}{\\text{Revenue}} = \\frac{\\text{Revenue} - \\text{COGS}}{\\text{Revenue}}\n\\]\nHigher gross margins indicate stronger pricing power, more efficient production, or a favorable product mix. This metric is particularly useful for comparing companies within an industry, as it reveals relative efficiency in core operations before overhead costs.\n\n\n5.7.2 Profit Margin\nThe profit margin shows what percentage of revenue ultimately becomes net income:\n\\[\n\\text{Profit Margin} = \\frac{\\text{Net Income}}{\\text{Revenue}}\n\\]\nThis comprehensive measure accounts for all costs (e.g., production, operations, interest, and taxes). Higher profit margins suggest effective overall cost management. However, optimal margins vary by industry: software companies routinely achieve 20%+ margins, while grocery stores operate on razor-thin 2-3% margins.\n\n\n5.7.3 Return on Equity (ROE)\nROE measures how efficiently a company uses shareholders’ investment to generate profits:\n\\[\n\\text{Return on Equity} = \\frac{\\text{Net Income}}{\\text{Total Equity}}\n\\]\nThis metric directly addresses what shareholders care about: returns on their invested capital. Higher ROE indicates more effective use of equity, though interpretation requires caution. High leverage can artificially inflate ROE by reducing the equity base (e.g., a company financed 90% by debt will show spectacular ROE on modest profits).\n\n\n5.7.4 The DuPont Decomposition\nThe DuPont framework decomposes ROE into three components that reveal different aspects of performance:\n\\[\n\\text{ROE} = \\underbrace{\\frac{\\text{Net Income}}{\\text{Revenue}}}_{\\text{Profit Margin}} \\times \\underbrace{\\frac{\\text{Revenue}}{\\text{Assets}}}_{\\text{Asset Turnover}} \\times \\underbrace{\\frac{\\text{Assets}}{\\text{Equity}}}_{\\text{Leverage}}\n\\]\nThis decomposition shows that high ROE can come from different sources: strong profit margins (pricing power, cost control), efficient asset use (high turnover), or aggressive leverage. Understanding which driver dominates helps assess sustainability. ROE driven by margins is generally more sustainable than ROE driven by leverage.\n\n\n5.7.5 Calculating Profitability Ratios\n\ncombined_statements = combined_statements.assign(\n    gross_margin=lambda x: x[\"is_gross_profit\"] / x[\"is_revenue\"],\n    profit_margin=lambda x: x[\"is_eat\"] / x[\"is_revenue\"],\n    after_tax_roe=lambda x: x[\"is_eat\"] / x[\"total_equity\"]\n)\n\n\n\n5.7.6 Gross Margin Trends\nFigure 5.5 tracks gross margin evolution over time, revealing whether companies are maintaining pricing power and production efficiency.\n\ngross_margins = combined_statements.query(\"symbol in @vn30_symbols\")\n\ngross_margins_figure = (\n    ggplot(gross_margins, aes(x=\"year\", y=\"gross_margin\", color=\"symbol\"))\n    + geom_line()\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Gross Margin\", color=\"\",\n        title=\"Gross Margins for VN30 Stocks (2019-2023)\"\n    )\n)\n\ngross_margins_figure.show()\n\n\n\n\n\n\n\nFigure 5.5: Gross margin trends reveal changes in pricing power and production efficiency. Declining margins may signal increased competition or rising input costs.\n\n\n\n\n\n\n\n5.7.7 From Gross to Net: Where Do Profits Go?\nFigure 5.6 examines the relationship between gross and profit margins. The gap between them reveals the impact of operating expenses, interest, and taxes.\n\nprofit_margins = combined_statements.query(\"year == 2023\")\n\nprofit_margins_figure = (\n    ggplot(\n        profit_margins,\n        aes(x=\"gross_margin\", y=\"profit_margin\", color=\"label\")\n    )\n    + geom_point(size=2)\n    + geom_label(\n        aes(label=\"label\"),\n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Gross Margin\", y=\"Profit Margin\",\n        title=\"Gross versus Profit Margins for VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\nprofit_margins_figure.show()\n\n\n\n\n\n\n\nFigure 5.6: Comparing gross and profit margins reveals how much of gross profit survives operating expenses, interest, and taxes. Companies far below the diagonal have high overhead relative to gross profit.\n\n\n\n\n\nCompanies along the diagonal convert gross profit to net income efficiently. Those well below the diagonal face high operating costs, interest burdens, or tax rates that erode profitability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#combining-financial-ratios-a-holistic-view",
    "href": "04_financial_statement_analysis.html#combining-financial-ratios-a-holistic-view",
    "title": "5  Financial Statement Analysis",
    "section": "5.8 Combining Financial Ratios: A Holistic View",
    "text": "5.8 Combining Financial Ratios: A Holistic View\nIndividual ratios provide specific insights, but combining them offers a more complete picture. A company might excel in profitability while struggling with liquidity, or maintain conservative leverage while underperforming on efficiency.\n\n5.8.1 Ranking Companies Across Categories\nFigure 5.7 compares company rankings across four ratio categories. Rankings closer to 1 indicate better performance within each category, enabling quick identification of relative strengths and weaknesses.\n\nfinancial_ratios = (combined_statements\n    .query(\"year == 2023\")\n    .filter(\n        items=[\"symbol\"] + [\n            col for col in combined_statements.columns\n            if any(x in col for x in [\n                \"ratio\", \"margin\", \"roe\", \"_to_\", \"turnover\", \"interest_coverage\"\n            ])\n        ]\n    )\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        type=lambda x: np.select(\n            [\n                x[\"name\"].isin([\"current_ratio\", \"quick_ratio\", \"cash_ratio\"]),\n                x[\"name\"].isin([\"debt_to_equity\", \"debt_to_asset\", \"interest_coverage\"]),\n                x[\"name\"].isin([\"asset_turnover\", \"inventory_turnover\", \"receivables_turnover\"]),\n                x[\"name\"].isin([\"gross_margin\", \"profit_margin\", \"after_tax_roe\"]),\n            ],\n            [\n                \"Liquidity Ratios\",\n                \"Leverage Ratios\",\n                \"Efficiency Ratios\",\n                \"Profitability Ratios\"\n            ],\n            default=\"Other\"\n        )\n    )\n)\n\nfinancial_ratios[\"rank\"] = (financial_ratios\n    .sort_values([\"type\", \"name\", \"value\"], ascending=[True, True, False])\n    .groupby([\"type\", \"name\"])\n    .cumcount() + 1\n)\n\nfinal_ranks = (financial_ratios\n    .groupby([\"symbol\", \"type\"], as_index=False)\n    .agg(rank=(\"rank\", \"mean\"))\n    .query(\"symbol in @vn30_symbols\")\n)\n\nfinal_ranks_figure = (\n    ggplot(final_ranks, aes(x=\"rank\", y=\"type\", color=\"symbol\"))\n    + geom_point(shape=\"^\", size=4)\n    + labs(\n        x=\"Average Rank (Lower is Better)\", y=\"\", color=\"\",\n        title=\"Average Rank Across Financial Ratio Categories\"\n    )\n    + coord_cartesian(xlim=[1, 30])\n)\n\nfinal_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 5.7: Ranking companies across multiple ratio categories reveals overall financial profiles. Companies with consistently low ranks across categories demonstrate broad-based financial strength.\n\n\n\n\n\nThe combined view reveals how different business strategies manifest in financial profiles. A company might deliberately accept lower profitability rankings in exchange for stronger liquidity, or use aggressive leverage to boost returns at the cost of financial flexibility.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#financial-ratios-in-asset-pricing",
    "href": "04_financial_statement_analysis.html#financial-ratios-in-asset-pricing",
    "title": "5  Financial Statement Analysis",
    "section": "5.9 Financial Ratios in Asset Pricing",
    "text": "5.9 Financial Ratios in Asset Pricing\nBeyond evaluating individual companies, financial ratios serve as crucial inputs for asset pricing models. The Fama-French five-factor model, which we explore in detail in Fama-French Factors, uses several accounting-based measures to explain cross-sectional variation in stock returns.\n\n5.9.1 The Fama-French Factors\nThe model incorporates four company characteristics derived from financial statements:\nSize is measured as the logarithm of market capitalization: \\[\n\\text{Size} = \\ln(\\text{Market Cap})\n\\]\nThis captures the empirical finding that smaller firms tend to outperform larger firms on a risk-adjusted basis (i.e., the “size premium”).\nBook-to-Market relates accounting value to market value: \\[\n\\text{Book-to-Market} = \\frac{\\text{Book Equity}}{\\text{Market Cap}}\n\\]\nHigh book-to-market stocks (“value” stocks) have historically outperformed low book-to-market stocks (“growth” stocks) (i.e., the “value premium”).\nOperating Profitability measures profit generation relative to equity: \\[\n\\text{Profitability} = \\frac{\\text{Revenue} - \\text{COGS} - \\text{SG\\&A} - \\text{Interest}}{\\text{Book Equity}}\n\\]\nMore profitable firms tend to earn higher returns (i.e., the “profitability premium”).\nInvestment captures asset growth: \\[\n\\text{Investment} = \\frac{\\text{Total Assets}_t}{\\text{Total Assets}_{t-1}} - 1\n\\] Firms investing aggressively tend to underperform conservative investors (i.e., the “investment premium”).\n\n\n5.9.2 Calculating Fama-French Variables\n\nprices_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM prices_monthly\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\n# Use December prices for annual calculations\nprices_december = (prices_monthly\n    .assign(date=lambda x: pd.to_datetime(x[\"date\"]))\n    .query(\"date.dt.month == 12\")\n)\n\n\ncombined_statements_ff = (combined_statements\n    .query(\"year == 2023\")\n    .merge(prices_december, on=[\"symbol\", \"year\"], how=\"left\")\n    .merge(\n        (balance_sheet_statements\n            .query(\"year == 2022\")\n            .get([\"symbol\", \"total_asset\"])\n            .rename(columns={\"total_asset\": \"total_assets_lag\"})\n        ),\n        on=\"symbol\",\n        how=\"left\"\n    )\n    .assign(\n        size=lambda x: np.log(x[\"mktcap\"]),\n        book_to_market=lambda x: x[\"total_equity\"] / x[\"mktcap\"],\n        operating_profitability=lambda x: (\n            (x[\"is_revenue\"] - x[\"is_cogs\"] -\n             x[\"selling_general_and_administrative_expenses\"] -\n             x[\"is_interest_expense\"]) / x[\"total_equity\"]\n        ),\n        investment=lambda x: x[\"total_asset\"] / x[\"total_assets_lag\"] - 1\n    )\n)\n\ncombined_statements_ff.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\nlabel\ncurrent_ratio\nquick_ratio\ncash_ratio\ndebt_to_equity\ndebt_to_asset\ntotal_asset\ntotal_equity\n...\nshrout\nmktcap\nmktcap_lag\nrisk_free\nret_excess\ntotal_assets_lag\nsize\nbook_to_market\noperating_profitability\ninvestment\n\n\n\n\n0\nPOW\n2023\nPOW\n1.084255\n1.084255\n0.315089\n0.0\n0.0\n7.036209e+13\n3.411943e+13\n...\n2.341872e+09\n26346.055367\n26346.055367\n0.003333\n-0.003333\n5.684324e+13\n10.179074\n1.295049e+09\n0.025539\n0.237827\n\n\n1\nHPG\n2023\nHPG\n1.156655\n1.156655\n0.171324\n0.0\n0.0\n1.877826e+14\n1.028364e+14\n...\n5.814786e+09\n162523.260217\n154382.560242\n0.003333\n-0.003333\n1.703355e+14\n11.998576\n6.327489e+08\n0.072798\n0.102428\n\n\n2\nMWG\n2023\nMWG\n1.688604\n1.688604\n0.174408\n0.0\n0.0\n6.011124e+13\n2.335956e+13\n...\n1.462941e+09\n62613.896064\n56323.247628\n0.003333\n-0.009141\n5.583410e+13\n11.044743\n3.730731e+08\n-0.002443\n0.076604\n\n\n\n\n3 rows × 65 columns\n\n\n\n\n\n5.9.3 Fama-French Factor Rankings\nFigure 5.8 shows how VN30 companies rank on each Fama-French variable, connecting fundamental analysis to asset pricing.\n\nfactors_ranks = (combined_statements_ff\n    .get([\"symbol\", \"size\", \"book_to_market\", \"operating_profitability\", \"investment\"])\n    .rename(columns={\n        \"size\": \"Size\",\n        \"book_to_market\": \"Book-to-Market\",\n        \"operating_profitability\": \"Profitability\",\n        \"investment\": \"Investment\"\n    })\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        rank=lambda x: (\n            x.sort_values([\"name\", \"value\"], ascending=[True, False])\n            .groupby(\"name\")\n            .cumcount() + 1\n        )\n    )\n    .query(\"symbol in @vn30_symbols\")\n)\n\nfactors_ranks_figure = (\n    ggplot(factors_ranks, aes(x=\"rank\", y=\"name\", color=\"symbol\"))\n    + geom_point(shape=\"^\", size=4)\n    + labs(\n        x=\"Rank\", y=\"\", color=\"\",\n        title=\"Rank in Fama-French Variables for VN30 Stocks\"\n    )\n    + coord_cartesian(xlim=[1, 30])\n)\n\nfactors_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 5.8: Rankings on Fama-French variables connect financial statement analysis to asset pricing. According to factor models, smaller, higher book-to-market, more profitable, and lower-investment firms should earn higher expected returns.\n\n\n\n\n\nThese rankings have implications for expected returns according to factor models. A small, high book-to-market, highly profitable company with conservative investment should, in theory, earn higher risk-adjusted returns than its opposite.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#limitations-and-practical-considerations",
    "href": "04_financial_statement_analysis.html#limitations-and-practical-considerations",
    "title": "5  Financial Statement Analysis",
    "section": "5.10 Limitations and Practical Considerations",
    "text": "5.10 Limitations and Practical Considerations\nWhile financial ratios provide powerful analytical tools, several limitations deserve attention:\n\n5.10.1 Accounting Discretion\nCompanies have significant discretion in how they apply accounting standards. Revenue recognition timing, depreciation methods, inventory valuation (FIFO vs. LIFO), and capitalization versus expensing decisions all affect reported numbers. Sophisticated analysis requires understanding these choices and their impact.\n\n\n5.10.2 Industry Comparability\nRatios vary dramatically across industries. Comparing a bank’s leverage to a retailer’s is meaningless (e.g., banks naturally operate with much higher leverage due to their business model). Always benchmark against industry peers rather than absolute standards.\n\n\n5.10.3 Point-in-Time Limitations\nBalance sheet ratios capture a single moment, which may not represent typical conditions. Companies often “window dress” by temporarily improving metrics at reporting dates. Trend analysis and quarter-over-quarter comparisons can reveal such practices.\n\n\n5.10.4 Backward-Looking Nature\nFinancial statements report historical results. Past profitability doesn’t guarantee future performance, especially for companies in rapidly changing industries or facing disruption.\n\n\n5.10.5 Quality of Earnings\nNot all profits are created equal. Earnings driven by one-time gains, accounting adjustments, or aggressive revenue recognition may not recur. Cash flow analysis helps assess earnings quality. Profits that don’t convert to cash warrant skepticism.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#key-takeaways",
    "href": "04_financial_statement_analysis.html#key-takeaways",
    "title": "5  Financial Statement Analysis",
    "section": "5.11 Key Takeaways",
    "text": "5.11 Key Takeaways\nThis chapter introduced financial statement analysis as a tool for understanding company fundamentals. The main insights are:\n\nThree statements, three perspectives: The balance sheet shows financial position at a point in time, the income statement measures performance over a period, and the cash flow statement tracks actual cash movements. Together, they provide a complete picture of financial health.\nLiquidity ratios assess short-term survival: Current, quick, and cash ratios measure the ability to meet near-term obligations. Higher ratios indicate greater liquidity but may suggest inefficient asset use.\nLeverage ratios reveal capital structure risk: Debt-to-equity, debt-to-asset, and interest coverage ratios show how the company finances operations and whether it can service its debt. Higher leverage amplifies both returns and risk.\nEfficiency ratios measure management effectiveness: Asset turnover, inventory turnover, and receivables turnover reveal how well the company converts resources into revenue. Industry context is essential for interpretation.\nProfitability ratios quantify financial success: Gross margin, profit margin, and ROE measure the ability to generate earnings. The DuPont decomposition reveals whether ROE comes from margins, turnover, or leverage.\nRatios connect to asset pricing: Financial statement variables like book-to-market, profitability, and investment form the basis of factor models that explain cross-sectional return differences.\nContext matters for interpretation: Ratios must be compared against industry peers, tracked over time, and considered alongside qualitative factors. No single ratio tells the complete story.\n\nLooking ahead, subsequent chapters will explore how these fundamental variables interact with market prices in asset pricing models, and how to construct factor portfolios based on financial statement characteristics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html",
    "href": "05_discounted_cash_flow.html",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "",
    "text": "6.1 What Is a Company Worth?\nThe previous chapters examined how markets price securities in equilibrium and how financial statements reveal company fundamentals. But these approaches leave a central question unanswered: What is the intrinsic value of a business, independent of its current market price?\nDiscounted Cash Flow (DCF) analysis answers this question by valuing a company based on its ability to generate cash for investors. The core insight is simple: a business is worth the present value of all future cash it will produce. This principle that value equals discounted future cash flows underlies virtually all of finance, from bond pricing to real estate valuation.\nDCF analysis stands apart from other valuation approaches in three important ways. First, it explicitly accounts for the time value of money (i.e., the principle that a dollar today is worth more than a dollar tomorrow). By discounting future cash flows at an appropriate rate, we incorporate both time preferences and risk. Second, DCF is forward-looking, making it particularly suitable for companies where historical performance may not reflect future potential. Third, DCF is flexible enough to accommodate various business models and capital structures, making it applicable across industries and company sizes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#what-is-a-company-worth",
    "href": "05_discounted_cash_flow.html#what-is-a-company-worth",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "",
    "text": "6.1.1 Valuation Methods Overview\nCompany valuation methods broadly fall into three categories:\n\nMarket-based approaches compare companies using relative metrics like Price-to-Earnings or EV/EBITDA ratios. These are quick but assume comparable companies are fairly valued.\nAsset-based methods focus on the net value of tangible and intangible assets. These work well for liquidation scenarios but miss going-concern value.\nIncome-based techniques value companies based on their ability to generate future cash flows. DCF is the most rigorous income-based method.\n\nWe focus on DCF because it forces analysts to make explicit assumptions about growth, profitability, and risk. These assumptions are often hidden in other methods. Even when DCF isn’t the final word on valuation, the discipline of building a DCF model deepens understanding of what drives value.\n\n\n6.1.2 The Three Pillars of DCF\nEvery DCF analysis rests on three components:\n\nFree Cash Flow (FCF) forecasts: The expected future cash available for distribution to investors after operating expenses, taxes, and investments\nTerminal value: The company’s value beyond the explicit forecast period, often representing a majority of total valuation\nDiscount rate: Typically the Weighted Average Cost of Capital (WACC), which adjusts future cash flows to present value by incorporating risk and capital structure\n\nWe make simplifying assumptions throughout this chapter. In particular, we assume firms conduct only operating activities (i.e., financial statements do not include non-operating items like excess cash or investment securities). Real-world valuations require valuing these separately. Entire textbooks are devoted to valuation nuances; our goal is to establish the conceptual framework and practical implementation.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\nfrom itertools import product",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#understanding-free-cash-flow",
    "href": "05_discounted_cash_flow.html#understanding-free-cash-flow",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.2 Understanding Free Cash Flow",
    "text": "6.2 Understanding Free Cash Flow\nBefore diving into calculations, we need to understand what Free Cash Flow represents and why it matters for valuation.\n\n6.2.1 Why Free Cash Flow, Not Net Income?\nAccountants report net income, but DCF uses free cash flow. Why the difference?\nNet income includes non-cash items (like depreciation) and ignores cash needs (like capital expenditures and working capital investments). A company can report strong profits while burning cash, or generate substantial cash while reporting losses. Free cash flow captures what actually matters for valuation: the cash available to distribute to all capital providers (both debt holders and equity holders) after funding operations and investments.\n\n\n6.2.2 The Free Cash Flow Formula\nWe calculate FCF using the following formula:\n\\[\n\\text{FCF} = \\text{EBIT} \\times (1 - \\tau) + \\text{D\\&A} - \\Delta\\text{WC} - \\text{CAPEX}\n\\]\nwhere:\n\nEBIT (Earnings Before Interest and Taxes): Core operating profit before financing costs and taxes\n\\(\\tau\\): Corporate tax rate applied to operating profits\nD&A (Depreciation & Amortization): Non-cash charges that reduce reported earnings but don’t consume cash\n\\(\\Delta\\)WC (Change in Working Capital): Cash tied up in (or released from) operations (increases in receivables and inventory consume cash, while increases in payables provide cash)\nCAPEX (Capital Expenditures): Investments in long-term assets required to maintain and grow operations\n\nAn alternative formulation starts from EBIT directly:\n\\[\n\\text{FCF} = \\text{EBIT} + \\text{D\\&A} - \\text{Taxes} - \\Delta\\text{WC} - \\text{CAPEX}\n\\]\nBoth formulations yield the same result when taxes are calculated consistently. The key insight is that FCF represents cash generated from operations after all reinvestment needs (i.e., cash that could theoretically be distributed to investors without impairing the business).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#loading-historical-financial-data",
    "href": "05_discounted_cash_flow.html#loading-historical-financial-data",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.3 Loading Historical Financial Data",
    "text": "6.3 Loading Historical Financial Data\nWe use FPT Corporation, one of Vietnam’s largest technology companies, as our case study. FPT provides IT services, telecommunications, and education. It’s a diversified business with meaningful capital requirements and growth potential.\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncomp_vn = pd.read_sql_query(\n    sql=\"SELECT * FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Filter to FPT and examine the data structure\nfpt_data = comp_vn[comp_vn[\"symbol\"] == \"FPT\"].copy()\nfpt_data[\"year\"] = fpt_data[\"year\"].astype(int)\nfpt_data = fpt_data.sort_values(\"year\").reset_index(drop=True)\n\nprint(f\"Available years: {fpt_data['year'].min()} to {fpt_data['year'].max()}\")\nprint(f\"Number of observations: {len(fpt_data)}\")\n\nAvailable years: 2002 to 2023\nNumber of observations: 22\n\n\n\n6.3.1 Computing Historical Free Cash Flow\nLet’s calculate the components needed for FCF from the financial statement data:\n\n# Extract and compute FCF components\nhistorical_data = (fpt_data\n    .assign(\n        # Revenue for ratio calculations\n        revenue=lambda x: x[\"is_net_revenue\"],\n        \n        # EBIT = Earnings before interest and taxes\n        # Approximate as EBT + Interest Expense\n        ebit=lambda x: x[\"is_ebt\"] + x[\"is_interest_expense\"],\n        \n        # Tax payments (use actual tax expense)\n        taxes=lambda x: x[\"is_cit_expense\"],\n        \n        # Depreciation and amortization (non-cash add-back)\n        depreciation=lambda x: x[\"cfo_depreciation\"],\n        \n        # Change in working capital components\n        # Positive delta_wc means cash is consumed (tied up in working capital)\n        delta_working_capital=lambda x: (\n            x[\"cfo_receive\"] +      # Change in receivables\n            x[\"cfo_inventory\"] -    # Change in inventory  \n            x[\"cfo_payale\"]         # Change in payables (negative = cash source)\n        ),\n        \n        # Capital expenditures\n        capex=lambda x: x[\"capex\"]\n    )\n    .loc[:, [\n        \"year\", \"revenue\", \"ebit\", \"taxes\", \"depreciation\",\n        \"delta_working_capital\", \"capex\"\n    ]]\n)\n\n# Calculate Free Cash Flow\nhistorical_data[\"fcf\"] = (\n    historical_data[\"ebit\"] \n    - historical_data[\"taxes\"]\n    + historical_data[\"depreciation\"]\n    - historical_data[\"delta_working_capital\"]\n    - historical_data[\"capex\"]\n)\n\nhistorical_data\n\n\n\n\n\n\n\n\nyear\nrevenue\nebit\ntaxes\ndepreciation\ndelta_working_capital\ncapex\nfcf\n\n\n\n\n0\n2002\n1.514961e+12\n2.698700e+10\n0.000000e+00\n1.261500e+10\n-2.561760e+11\n2.202800e+10\n2.737500e+11\n\n\n1\n2003\n4.148298e+12\n5.676100e+10\n0.000000e+00\n1.837700e+10\n-5.078740e+11\n3.753300e+10\n5.454790e+11\n\n\n2\n2004\n8.734781e+12\n2.145902e+11\n1.795700e+10\n2.947900e+10\n-4.280270e+11\n5.252100e+10\n6.016182e+11\n\n\n3\n2005\n1.410079e+13\n3.753490e+11\n4.251500e+10\n5.381700e+10\n-4.471110e+11\n1.428320e+11\n6.909300e+11\n\n\n4\n2006\n2.139975e+13\n6.672593e+11\n7.368682e+10\n1.068192e+11\n-1.173099e+12\n2.459780e+11\n1.627513e+12\n\n\n5\n2007\n1.349889e+13\n1.071941e+12\n1.487146e+11\n1.709335e+11\n-1.873794e+12\n4.802762e+11\n2.487677e+12\n\n\n6\n2008\n1.638184e+13\n1.320573e+12\n1.890384e+11\n2.395799e+11\n-1.419506e+11\n6.690461e+11\n8.440192e+11\n\n\n7\n2009\n1.840403e+13\n1.807221e+12\n2.916482e+11\n3.041813e+11\n-8.065011e+11\n7.632280e+11\n1.863027e+12\n\n\n8\n2010\n2.001730e+13\n2.261341e+12\n3.314359e+11\n3.294060e+11\n-2.360993e+12\n8.672138e+11\n3.753090e+12\n\n\n9\n2011\n2.537025e+13\n2.751044e+12\n4.223952e+11\n3.759567e+11\n-2.099380e+12\n4.524081e+11\n4.351578e+12\n\n\n10\n2012\n2.459430e+13\n2.635219e+12\n4.210738e+11\n3.995598e+11\n8.043763e+11\n7.083318e+11\n1.100997e+12\n\n\n11\n2013\n2.702789e+13\n2.690568e+12\n4.503170e+11\n4.429860e+11\n-1.947751e+12\n9.110216e+11\n3.719967e+12\n\n\n12\n2014\n3.264466e+13\n2.625389e+12\n3.800994e+11\n5.472736e+11\n-3.078130e+12\n1.417399e+12\n4.453295e+12\n\n\n13\n2015\n3.795970e+13\n3.113651e+12\n4.130641e+11\n7.328801e+11\n-1.951778e+12\n1.974295e+12\n3.410951e+12\n\n\n14\n2016\n3.953147e+13\n3.388085e+12\n4.382078e+11\n9.334397e+11\n-9.242713e+11\n1.428472e+12\n3.379116e+12\n\n\n15\n2017\n4.265861e+13\n4.623663e+12\n7.270039e+11\n1.039417e+12\n-4.638788e+12\n1.100498e+12\n8.474367e+12\n\n\n16\n2018\n2.321354e+13\n4.095947e+12\n6.236054e+11\n1.164692e+12\n-1.033438e+12\n2.452902e+12\n3.217569e+12\n\n\n17\n2019\n2.771696e+13\n5.023518e+12\n7.528183e+11\n1.354613e+12\n-5.308818e+11\n3.230818e+12\n2.925377e+12\n\n\n18\n2020\n2.983040e+13\n5.648794e+12\n8.397114e+11\n1.490607e+12\n-8.040730e+11\n3.014322e+12\n4.089441e+12\n\n\n19\n2021\n3.565726e+13\n6.821202e+12\n9.879053e+11\n1.643916e+12\n-2.821825e+12\n2.908134e+12\n7.390903e+12\n\n\n20\n2022\n4.400953e+13\n8.308009e+12\n1.170940e+12\n1.833064e+12\n-3.746661e+12\n3.209581e+12\n9.507213e+12\n\n\n21\n2023\n5.261790e+13\n1.003565e+13\n1.414956e+12\n2.286514e+12\n-2.147304e+12\n3.948982e+12\n9.105534e+12\n\n\n\n\n\n\n\n\n\n6.3.2 Understanding the Historical Pattern\nBefore forecasting, we should understand the historical trends in FCF and its components:\n\n# Calculate key ratios relative to revenue\nhistorical_ratios = (historical_data\n    .assign(\n        # Revenue growth (year-over-year)\n        revenue_growth=lambda x: x[\"revenue\"].pct_change(),\n        \n        # Operating margin: EBIT as % of revenue\n        operating_margin=lambda x: x[\"ebit\"] / x[\"revenue\"],\n        \n        # Depreciation as % of revenue\n        depreciation_margin=lambda x: x[\"depreciation\"] / x[\"revenue\"],\n        \n        # Tax rate (taxes as % of revenue, for simplicity)\n        tax_margin=lambda x: x[\"taxes\"] / x[\"revenue\"],\n        \n        # Working capital intensity\n        working_capital_margin=lambda x: x[\"delta_working_capital\"] / x[\"revenue\"],\n        \n        # Capital intensity\n        capex_margin=lambda x: x[\"capex\"] / x[\"revenue\"],\n        \n        # FCF margin\n        fcf_margin=lambda x: x[\"fcf\"] / x[\"revenue\"]\n    )\n)\n\n# Display key metrics\ndisplay_cols = [\n    \"year\", \"revenue_growth\", \"operating_margin\", \"depreciation_margin\",\n    \"tax_margin\", \"working_capital_margin\", \"capex_margin\", \"fcf_margin\"\n]\n\nhistorical_ratios[display_cols].round(3)\n\n\n\n\n\n\n\n\nyear\nrevenue_growth\noperating_margin\ndepreciation_margin\ntax_margin\nworking_capital_margin\ncapex_margin\nfcf_margin\n\n\n\n\n0\n2002\nNaN\n0.018\n0.008\n0.000\n-0.169\n0.015\n0.181\n\n\n1\n2003\n1.738\n0.014\n0.004\n0.000\n-0.122\n0.009\n0.131\n\n\n2\n2004\n1.106\n0.025\n0.003\n0.002\n-0.049\n0.006\n0.069\n\n\n3\n2005\n0.614\n0.027\n0.004\n0.003\n-0.032\n0.010\n0.049\n\n\n4\n2006\n0.518\n0.031\n0.005\n0.003\n-0.055\n0.011\n0.076\n\n\n5\n2007\n-0.369\n0.079\n0.013\n0.011\n-0.139\n0.036\n0.184\n\n\n6\n2008\n0.214\n0.081\n0.015\n0.012\n-0.009\n0.041\n0.052\n\n\n7\n2009\n0.123\n0.098\n0.017\n0.016\n-0.044\n0.041\n0.101\n\n\n8\n2010\n0.088\n0.113\n0.016\n0.017\n-0.118\n0.043\n0.187\n\n\n9\n2011\n0.267\n0.108\n0.015\n0.017\n-0.083\n0.018\n0.172\n\n\n10\n2012\n-0.031\n0.107\n0.016\n0.017\n0.033\n0.029\n0.045\n\n\n11\n2013\n0.099\n0.100\n0.016\n0.017\n-0.072\n0.034\n0.138\n\n\n12\n2014\n0.208\n0.080\n0.017\n0.012\n-0.094\n0.043\n0.136\n\n\n13\n2015\n0.163\n0.082\n0.019\n0.011\n-0.051\n0.052\n0.090\n\n\n14\n2016\n0.041\n0.086\n0.024\n0.011\n-0.023\n0.036\n0.085\n\n\n15\n2017\n0.079\n0.108\n0.024\n0.017\n-0.109\n0.026\n0.199\n\n\n16\n2018\n-0.456\n0.176\n0.050\n0.027\n-0.045\n0.106\n0.139\n\n\n17\n2019\n0.194\n0.181\n0.049\n0.027\n-0.019\n0.117\n0.106\n\n\n18\n2020\n0.076\n0.189\n0.050\n0.028\n-0.027\n0.101\n0.137\n\n\n19\n2021\n0.195\n0.191\n0.046\n0.028\n-0.079\n0.082\n0.207\n\n\n20\n2022\n0.234\n0.189\n0.042\n0.027\n-0.085\n0.073\n0.216\n\n\n21\n2023\n0.196\n0.191\n0.043\n0.027\n-0.041\n0.075\n0.173",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#visualizing-historical-ratios",
    "href": "05_discounted_cash_flow.html#visualizing-historical-ratios",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.4 Visualizing Historical Ratios",
    "text": "6.4 Visualizing Historical Ratios\nFigure 6.1 shows the historical evolution of key financial ratios that drive FCF. Understanding these patterns helps inform our forecasts.\n\n# Prepare data for plotting\nratio_columns = [\n    \"operating_margin\", \"depreciation_margin\", \"tax_margin\",\n    \"working_capital_margin\", \"capex_margin\"\n]\n\nratios_long = (historical_ratios\n    .melt(\n        id_vars=[\"year\"],\n        value_vars=ratio_columns,\n        var_name=\"ratio\",\n        value_name=\"value\"\n    )\n    .assign(\n        ratio=lambda x: x[\"ratio\"].str.replace(\"_\", \" \").str.title()\n    )\n)\n\nratios_figure = (\n    ggplot(ratios_long, aes(x=\"year\", y=\"value\", color=\"ratio\"))\n    + geom_line(size=1)\n    + geom_point(size=2)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Ratio (% of Revenue)\", color=\"\",\n        title=\"Key Financial Ratios of FPT Over Time\"\n    )\n    + theme(legend_position=\"right\")\n)\n\nratios_figure.show()\n\n\n\n\n\n\n\nFigure 6.1: Historical financial ratios reveal the operating characteristics of FPT. These patterns inform our forecast assumptions.\n\n\n\n\n\nSeveral patterns emerge from the historical data. Operating margins show the profitability of core operations. Depreciation margins indicate asset intensity. CAPEX margins reveal investment requirements. Working capital margins can be volatile, reflecting changes in credit terms and inventory management.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#forecasting-free-cash-flow",
    "href": "05_discounted_cash_flow.html#forecasting-free-cash-flow",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.5 Forecasting Free Cash Flow",
    "text": "6.5 Forecasting Free Cash Flow\nWith historical patterns established, we now project FCF into the future. This requires forecasting both revenue growth and the ratios that convert revenue into cash flow.\n\n6.5.1 The Ratio-Based Forecasting Approach\nWe use a ratio-based approach that links all FCF components to revenue. This makes forecasting tractable: rather than projecting absolute dollar amounts for each component, we forecast (1) revenue growth and (2) how each component scales with revenue.\nThis approach embeds a key assumption: that the relationship between revenue and FCF components remains stable. In reality, operating leverage, investment needs, and working capital requirements may change as companies mature. Sophisticated valuations model these dynamics explicitly.\n\n\n6.5.2 Setting Forecast Assumptions\nFor our five-year forecast, we make the following assumptions about FPT’s financial ratios. These should reflect industry analysis, company guidance, and competitive dynamics. Here we use estimates for illustration:\n\n# Define the forecast horizon\nlast_historical_year = historical_data[\"year\"].max()\nforecast_years = list(range(last_historical_year + 1, last_historical_year + 6))\nn_forecast_years = len(forecast_years)\n\nprint(f\"Forecast period: {forecast_years[0]} to {forecast_years[-1]}\")\n\n# Define forecast ratios\n# In practice, these would come from detailed analysis\nforecast_assumptions = pd.DataFrame({\n    \"year\": forecast_years,\n    # Operating margin: slight improvement as scale increases\n    \"operating_margin\": [0.12, 0.125, 0.13, 0.13, 0.135],\n    # Depreciation: stable as % of revenue\n    \"depreciation_margin\": [0.03, 0.03, 0.03, 0.028, 0.028],\n    # Tax rate: stable\n    \"tax_margin\": [0.02, 0.02, 0.02, 0.02, 0.02],\n    # Working capital: modest cash consumption\n    \"working_capital_margin\": [0.01, 0.01, 0.008, 0.008, 0.008],\n    # CAPEX: declining as % of revenue as growth moderates\n    \"capex_margin\": [0.05, 0.048, 0.045, 0.042, 0.04]\n})\n\nforecast_assumptions\n\nForecast period: 2024 to 2028\n\n\n\n\n\n\n\n\n\nyear\noperating_margin\ndepreciation_margin\ntax_margin\nworking_capital_margin\ncapex_margin\n\n\n\n\n0\n2024\n0.120\n0.030\n0.02\n0.010\n0.050\n\n\n1\n2025\n0.125\n0.030\n0.02\n0.010\n0.048\n\n\n2\n2026\n0.130\n0.030\n0.02\n0.008\n0.045\n\n\n3\n2027\n0.130\n0.028\n0.02\n0.008\n0.042\n\n\n4\n2028\n0.135\n0.028\n0.02\n0.008\n0.040\n\n\n\n\n\n\n\n\n\n6.5.3 Forecasting Revenue Growth\nRevenue growth is often the most important and most uncertain assumption in DCF analysis. We demonstrate two approaches: using historical averages and linking growth to macroeconomic forecasts.\nApproach 1: Historical Average\nA simple approach uses the historical average growth rate:\n\nhistorical_growth = historical_ratios[\"revenue_growth\"].dropna()\navg_historical_growth = historical_growth.mean()\n\nprint(f\"Average historical revenue growth: {avg_historical_growth:.1%}\")\n\nAverage historical revenue growth: 25.2%\n\n\nApproach 2: GDP-Linked Growth\nA more sophisticated approach links company growth to GDP forecasts from institutions like the IMF. This captures the intuition that company revenues often move with broader economic activity.\n\n# Vietnam GDP growth forecasts (illustrative, based on IMF WEO style projections)\n# In practice, download from IMF WEO database\ngdp_forecasts = pd.DataFrame({\n    \"year\": forecast_years,\n    \"gdp_growth\": [0.065, 0.063, 0.060, 0.058, 0.055]  # Gradually declining to long-term\n})\n\n# Assume FPT grows at a premium to GDP (tech sector outperformance)\n# This premium should reflect company-specific factors\ngrowth_premium = 0.05  # 5 percentage points above GDP\n\nforecast_assumptions = forecast_assumptions.merge(gdp_forecasts, on=\"year\")\nforecast_assumptions[\"revenue_growth\"] = (\n    forecast_assumptions[\"gdp_growth\"] + growth_premium\n)\n\nforecast_assumptions[[\"year\", \"gdp_growth\", \"revenue_growth\"]]\n\n\n\n\n\n\n\n\nyear\ngdp_growth\nrevenue_growth\n\n\n\n\n0\n2024\n0.065\n0.115\n\n\n1\n2025\n0.063\n0.113\n\n\n2\n2026\n0.060\n0.110\n\n\n3\n2027\n0.058\n0.108\n\n\n4\n2028\n0.055\n0.105\n\n\n\n\n\n\n\n\n\n6.5.4 Building the Forecast\nNow we combine our assumptions to project revenue and FCF:\n\n# Get the last historical revenue as our starting point\nlast_revenue = historical_data.loc[\n    historical_data[\"year\"] == last_historical_year, \"revenue\"\n].values[0]\n\nprint(f\"Last historical revenue ({last_historical_year}): {last_revenue/1e12:.2f} trillion VND\")\n\n# Project revenue forward\nforecast_data = forecast_assumptions.copy()\nforecast_data[\"revenue\"] = None\n\n# Calculate revenue for each forecast year\nfor i, row in forecast_data.iterrows():\n    if i == 0:\n        # First forecast year: grow from last historical\n        forecast_data.loc[i, \"revenue\"] = last_revenue * (1 + row[\"revenue_growth\"])\n    else:\n        # Subsequent years: grow from previous forecast\n        prev_revenue = forecast_data.loc[i-1, \"revenue\"]\n        forecast_data.loc[i, \"revenue\"] = prev_revenue * (1 + row[\"revenue_growth\"])\n\n# Convert revenue to numeric\nforecast_data[\"revenue\"] = forecast_data[\"revenue\"].astype(float)\n\n# Calculate FCF components from ratios\nforecast_data[\"ebit\"] = forecast_data[\"operating_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"depreciation\"] = forecast_data[\"depreciation_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"taxes\"] = forecast_data[\"tax_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"delta_working_capital\"] = forecast_data[\"working_capital_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"capex\"] = forecast_data[\"capex_margin\"] * forecast_data[\"revenue\"]\n\n# Calculate FCF\nforecast_data[\"fcf\"] = (\n    forecast_data[\"ebit\"]\n    - forecast_data[\"taxes\"]\n    + forecast_data[\"depreciation\"]\n    - forecast_data[\"delta_working_capital\"]\n    - forecast_data[\"capex\"]\n)\n\nforecast_data[[\"year\", \"revenue\", \"ebit\", \"fcf\"]].round(0)\n\nLast historical revenue (2023): 52.62 trillion VND\n\n\n\n\n\n\n\n\n\nyear\nrevenue\nebit\nfcf\n\n\n\n\n0\n2024\n5.866896e+13\n7.040275e+12\n4.106827e+12\n\n\n1\n2025\n6.529855e+13\n8.162319e+12\n5.027988e+12\n\n\n2\n2026\n7.248139e+13\n9.422581e+12\n6.305881e+12\n\n\n3\n2027\n8.030938e+13\n1.044022e+13\n7.067226e+12\n\n\n4\n2028\n8.874187e+13\n1.198015e+13\n8.430477e+12",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#visualizing-the-forecast",
    "href": "05_discounted_cash_flow.html#visualizing-the-forecast",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.6 Visualizing the Forecast",
    "text": "6.6 Visualizing the Forecast\nFigure 6.2 compares our forecast ratios with historical values, showing the transition from realized to projected performance.\n\n# Prepare historical data for plotting\nhistorical_plot = (historical_ratios\n    .loc[:, [\"year\", \"operating_margin\", \"depreciation_margin\", \n             \"tax_margin\", \"working_capital_margin\", \"capex_margin\"]]\n    .assign(type=\"Historical\")\n)\n\n# Prepare forecast data for plotting\nforecast_plot = (forecast_data\n    .loc[:, [\"year\", \"operating_margin\", \"depreciation_margin\",\n             \"tax_margin\", \"working_capital_margin\", \"capex_margin\"]]\n    .assign(type=\"Forecast\")\n)\n\n# Combine\ncombined_ratios = pd.concat([historical_plot, forecast_plot], ignore_index=True)\n\n# Reshape for plotting\ncombined_long = combined_ratios.melt(\n    id_vars=[\"year\", \"type\"],\n    var_name=\"ratio\",\n    value_name=\"value\"\n)\n\ncombined_long[\"type\"] = pd.Categorical(\n    combined_long[\"type\"], \n    categories=[\"Historical\", \"Forecast\"]\n)\n\nforecast_ratios_figure = (\n    ggplot(combined_long, aes(x=\"year\", y=\"value\", color=\"ratio\", linetype=\"type\"))\n    + geom_line(size=1)\n    + geom_point(size=2)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Ratio (% of Revenue)\", color=\"\", linetype=\"\",\n        title=\"Historical and Forecast Financial Ratios for FPT\"\n    )\n    + theme(legend_position=\"right\")\n)\n\nforecast_ratios_figure.show()\n\n\n\n\n\n\n\nFigure 6.2: Historical ratios (solid lines) and forecast assumptions (dashed lines) for key financial metrics. The forecast period begins after the last historical observation.\n\n\n\n\n\nFigure 6.3 shows the revenue growth trajectory, comparing historical performance with our GDP-linked forecasts.\n\n# Prepare growth data\nhistorical_growth_df = (historical_ratios\n    .loc[:, [\"year\", \"revenue_growth\"]]\n    .dropna()\n    .assign(type=\"Historical\")\n)\n\nforecast_growth_df = (forecast_data\n    .loc[:, [\"year\", \"revenue_growth\", \"gdp_growth\"]]\n    .assign(type=\"Forecast\")\n)\n\n# Combine for revenue growth\ngrowth_combined = pd.concat([\n    historical_growth_df,\n    forecast_growth_df[[\"year\", \"revenue_growth\", \"type\"]]\n], ignore_index=True)\n\ngrowth_combined[\"type\"] = pd.Categorical(\n    growth_combined[\"type\"],\n    categories=[\"Historical\", \"Forecast\"]\n)\n\ngrowth_figure = (\n    ggplot(growth_combined, aes(x=\"year\", y=\"revenue_growth\", linetype=\"type\"))\n    + geom_line(size=1, color=\"steelblue\")\n    + geom_point(size=2, color=\"steelblue\")\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Revenue Growth Rate\", linetype=\"\",\n        title=\"Historical and Forecast Revenue Growth for FPT\"\n    )\n)\n\ngrowth_figure.show()\n\n\n\n\n\n\n\nFigure 6.3: Revenue growth rates: historical (realized) and forecast (GDP-linked with company premium). The forecast assumes FPT grows at a premium to Vietnam’s GDP growth.\n\n\n\n\n\nFigure 6.4 presents the resulting FCF projections alongside historical values.\n\n# Combine historical and forecast FCF\nfcf_historical = (historical_data\n    .loc[:, [\"year\", \"fcf\"]]\n    .assign(type=\"Historical\")\n)\n\nfcf_forecast = (forecast_data\n    .loc[:, [\"year\", \"fcf\"]]\n    .assign(type=\"Forecast\")\n)\n\nfcf_combined = pd.concat([fcf_historical, fcf_forecast], ignore_index=True)\nfcf_combined[\"type\"] = pd.Categorical(\n    fcf_combined[\"type\"],\n    categories=[\"Historical\", \"Forecast\"]\n)\n\nfcf_figure = (\n    ggplot(fcf_combined, aes(x=\"year\", y=\"fcf/1e12\", fill=\"type\"))\n    + geom_col()\n    + labs(\n        x=\"\", y=\"Free Cash Flow (Trillion VND)\", fill=\"\",\n        title=\"Historical and Forecast Free Cash Flow for FPT\"\n    )\n)\n\nfcf_figure.show()\n\n\n\n\n\n\n\nFigure 6.4: Free Cash Flow: historical (realized) and forecast (projected). The forecast reflects our assumptions about revenue growth and operating ratios.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#terminal-value-capturing-long-term-value",
    "href": "05_discounted_cash_flow.html#terminal-value-capturing-long-term-value",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.7 Terminal Value: Capturing Long-Term Value",
    "text": "6.7 Terminal Value: Capturing Long-Term Value\nA critical component of DCF analysis is the terminal value (or continuation value), which represents the company’s value beyond the explicit forecast period. In most valuations, terminal value constitutes 50-80% of total enterprise value, making its estimation particularly important.\n\n6.7.1 The Perpetuity Growth Model\nThe most common approach is the Perpetuity Growth Model (also called the Gordon Growth Model), which assumes FCF grows at a constant rate forever:\n\\[\nTV_T = \\frac{FCF_{T+1}}{r - g} = \\frac{FCF_T \\times (1 + g)}{r - g}\n\\]\nwhere:\n\n\\(TV_T\\): Terminal value at the end of year \\(T\\)\n\\(FCF_T\\): Free cash flow in the final forecast year\n\\(g\\): Perpetual growth rate\n\\(r\\): Discount rate (WACC)\n\n\n\n6.7.2 Choosing the Perpetual Growth Rate\nThe perpetual growth rate \\(g\\) should reflect long-term sustainable growth. Key considerations:\n\nNo company can grow faster than the economy forever. If it did, the company would eventually become larger than GDP, which is an impossibility. Long-term GDP growth (nominal, including inflation) provides an upper bound.\nMature companies typically grow at or below GDP growth. The perpetual growth rate should reflect the company in its “steady state,” not its current high-growth phase.\nFor Vietnam, long-term nominal GDP growth might be 6-8% given current development stage, but this will moderate over time. A perpetual growth rate of 3-5% is often reasonable.\n\n\ndef compute_terminal_value(last_fcf, growth_rate, discount_rate):\n    \"\"\"\n    Compute terminal value using the perpetuity growth model.\n    \n    Parameters:\n    -----------\n    last_fcf : float\n        Free cash flow in the final forecast year\n    growth_rate : float\n        Perpetual growth rate (g)\n    discount_rate : float\n        Discount rate / WACC (r)\n        \n    Returns:\n    --------\n    float : Terminal value\n    \"\"\"\n    if discount_rate &lt;= growth_rate:\n        raise ValueError(\"Discount rate must exceed growth rate for finite terminal value\")\n    \n    return last_fcf * (1 + growth_rate) / (discount_rate - growth_rate)\n\n\n# Example calculation\nlast_fcf = forecast_data[\"fcf\"].iloc[-1]\nperpetual_growth = 0.04  # 4% perpetual growth\ndiscount_rate = 0.10     # 10% WACC (placeholder)\n\nterminal_value = compute_terminal_value(last_fcf, perpetual_growth, discount_rate)\n\nprint(f\"Last forecast FCF: {last_fcf/1e12:.2f} trillion VND\")\nprint(f\"Terminal value (at {perpetual_growth:.0%} growth, {discount_rate:.0%} WACC): {terminal_value/1e12:.1f} trillion VND\")\n\nLast forecast FCF: 8.43 trillion VND\nTerminal value (at 4% growth, 10% WACC): 146.1 trillion VND\n\n\n\n\n6.7.3 Alternative: Exit Multiple Approach\nPractitioners often cross-check terminal value using the exit multiple approach, which assumes the company is sold at the end of the forecast period at a multiple of EBITDA, EBIT, or revenue comparable to similar companies today.\nFor example, if comparable companies trade at 10x EBITDA, the terminal value would be:\n\\[\nTV_T = \\text{EBITDA}_T \\times \\text{Exit Multiple}\n\\]\nThis approach is simpler but embeds the assumption that current market multiples will persist (a strong assumption that may not hold).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#the-discount-rate-weighted-average-cost-of-capital",
    "href": "05_discounted_cash_flow.html#the-discount-rate-weighted-average-cost-of-capital",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.8 The Discount Rate: Weighted Average Cost of Capital",
    "text": "6.8 The Discount Rate: Weighted Average Cost of Capital\nThe discount rate converts future cash flows to present value. For FCF (which goes to all capital providers), we use the Weighted Average Cost of Capital (WACC):\n\\[\nWACC = \\frac{E}{E+D} \\times r_E + \\frac{D}{E+D} \\times r_D \\times (1 - \\tau)\n\\]\nwhere:\n\n\\(E\\): Market value of equity\n\\(D\\): Market value of debt\n\\(r_E\\): Cost of equity (typically estimated using CAPM)\n\\(r_D\\): Cost of debt (pre-tax)\n\\(\\tau\\): Corporate tax rate\n\nThe \\((1-\\tau)\\) term on debt reflects the tax shield. Interest payments are tax-deductible, reducing the effective cost of debt.\n\n6.8.1 Estimating WACC Components\nCost of Equity is typically estimated using the Capital Asset Pricing Model (see our CAPM chapter):\n\\[\nr_E = r_f + \\beta \\times (r_m - r_f)\n\\]\nwhere \\(r_f\\) is the risk-free rate, \\(\\beta\\) measures systematic risk, and \\((r_m - r_f)\\) is the market risk premium.\nCost of Debt can be estimated from:\n\nInterest expense divided by total debt (effective rate)\nYields on the company’s traded bonds\nYields on bonds with similar credit ratings\n\nCapital Structure Weights should use market values when available. For equity, market capitalization is straightforward. For debt, book value is often used when market values aren’t observable.\n\n\n6.8.2 Using Industry WACC Data\nProfessor Aswath Damodaran at NYU Stern maintains comprehensive industry WACC data. Let’s download and use this resource:\n\nimport requests\nimport os\n\n# Download Damodaran's WACC data\nurl = \"https://pages.stern.nyu.edu/~adamodar/pc/datasets/wacc.xls\"\n\ntry:\n    response = requests.get(url, timeout=10)\n    response.raise_for_status()\n    \n    with open(\"wacc.xls\", \"wb\") as f:\n        f.write(response.content)\n    \n    # Read the data (skip header rows)\n    wacc_data = pd.read_excel(\"wacc.xls\", sheet_name=1, skiprows=18)\n    \n    # Clean up\n    os.remove(\"wacc.xls\")\n    \n    # Find WACC for Computer Services (closest to FPT's business)\n    industry_wacc = wacc_data.loc[\n        wacc_data[\"Industry Name\"] == \"Computer Services\",\n        \"Cost of Capital\"\n    ].values[0]\n    \n    print(f\"Industry WACC (Computer Services): {industry_wacc:.2%}\")\n    \nexcept Exception as e:\n    print(f\"Could not download WACC data: {e}\")\n    # Use a reasonable estimate\n    industry_wacc = 0.10\n    print(f\"Using estimated WACC: {industry_wacc:.2%}\")\n\nwacc = industry_wacc\n\nCould not download WACC data: `Import xlrd` failed. Install xlrd &gt;= 2.0.1 for xls Excel support Use pip or conda to install the xlrd package.\nUsing estimated WACC: 10.00%\n\n\nNote: Industry WACC provides a useful benchmark, but company-specific factors (leverage, business risk, country risk) may warrant adjustments. For Vietnamese companies, adding a country risk premium may be appropriate.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#computing-enterprise-value",
    "href": "05_discounted_cash_flow.html#computing-enterprise-value",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.9 Computing Enterprise Value",
    "text": "6.9 Computing Enterprise Value\nWith all components in place, we can now compute enterprise value. The DCF formula is:\n\\[\n\\text{Enterprise Value} = \\sum_{t=1}^{T} \\frac{FCF_t}{(1 + WACC)^t} + \\frac{TV_T}{(1 + WACC)^T}\n\\]\nThe first term is the present value of forecast-period cash flows; the second is the present value of terminal value.\n\ndef compute_dcf_value(fcf_series, wacc, perpetual_growth):\n    \"\"\"\n    Compute enterprise value using DCF analysis.\n    \n    Parameters:\n    -----------\n    fcf_series : array-like\n        Free cash flows for forecast period\n    wacc : float\n        Weighted average cost of capital\n    perpetual_growth : float\n        Perpetual growth rate for terminal value\n        \n    Returns:\n    --------\n    dict : Components of DCF valuation\n    \"\"\"\n    fcf = np.array(fcf_series)\n    n_years = len(fcf)\n    \n    # Discount factors\n    discount_factors = (1 + wacc) ** np.arange(1, n_years + 1)\n    \n    # Present value of forecast period cash flows\n    pv_fcf = fcf / discount_factors\n    pv_fcf_total = pv_fcf.sum()\n    \n    # Terminal value and its present value\n    terminal_value = compute_terminal_value(fcf[-1], perpetual_growth, wacc)\n    pv_terminal = terminal_value / discount_factors[-1]\n    \n    # Total enterprise value\n    enterprise_value = pv_fcf_total + pv_terminal\n    \n    return {\n        \"pv_fcf\": pv_fcf_total,\n        \"terminal_value\": terminal_value,\n        \"pv_terminal\": pv_terminal,\n        \"enterprise_value\": enterprise_value,\n        \"terminal_pct\": pv_terminal / enterprise_value\n    }\n\n\n# Compute DCF value\nperpetual_growth = 0.04  # 4% perpetual growth\n\ndcf_result = compute_dcf_value(\n    fcf_series=forecast_data[\"fcf\"].values,\n    wacc=wacc,\n    perpetual_growth=perpetual_growth\n)\n\nprint(\"DCF Valuation Results\")\nprint(\"=\" * 50)\nprint(f\"PV of Forecast Period FCF: {dcf_result['pv_fcf']/1e12:.1f} trillion VND\")\nprint(f\"Terminal Value: {dcf_result['terminal_value']/1e12:.1f} trillion VND\")\nprint(f\"PV of Terminal Value: {dcf_result['pv_terminal']/1e12:.1f} trillion VND\")\nprint(f\"Enterprise Value: {dcf_result['enterprise_value']/1e12:.1f} trillion VND\")\nprint(f\"Terminal Value as % of EV: {dcf_result['terminal_pct']:.1%}\")\n\nDCF Valuation Results\n==================================================\nPV of Forecast Period FCF: 22.7 trillion VND\nTerminal Value: 146.1 trillion VND\nPV of Terminal Value: 90.7 trillion VND\nEnterprise Value: 113.4 trillion VND\nTerminal Value as % of EV: 80.0%\n\n\nNote that terminal value often represents 60-80% of enterprise value. This highlights the importance of terminal value assumptions and the inherent uncertainty in DCF analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#sensitivity-analysis",
    "href": "05_discounted_cash_flow.html#sensitivity-analysis",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.10 Sensitivity Analysis",
    "text": "6.10 Sensitivity Analysis\nGiven the uncertainty in DCF inputs, sensitivity analysis is essential. We examine how enterprise value changes with different assumptions about WACC and perpetual growth.\n\n# Define ranges for sensitivity analysis\nwacc_range = np.arange(0.08, 0.14, 0.01)  # 8% to 13%\ngrowth_range = np.arange(0.02, 0.06, 0.01)  # 2% to 5%\n\n# Create all combinations\nsensitivity_results = []\n\nfor w in wacc_range:\n    for g in growth_range:\n        if w &gt; g:  # Must have WACC &gt; growth for valid terminal value\n            result = compute_dcf_value(\n                fcf_series=forecast_data[\"fcf\"].values,\n                wacc=w,\n                perpetual_growth=g\n            )\n            sensitivity_results.append({\n                \"wacc\": w,\n                \"growth_rate\": g,\n                \"enterprise_value\": result[\"enterprise_value\"] / 1e12  # In trillions\n            })\n\nsensitivity_df = pd.DataFrame(sensitivity_results)\n\n# Create heatmap\nsensitivity_figure = (\n    ggplot(sensitivity_df, aes(x=\"wacc\", y=\"growth_rate\", fill=\"enterprise_value\"))\n    + geom_tile()\n    + geom_text(\n        aes(label=\"enterprise_value\"),\n        format_string=\"{:.0f}\",\n        color=\"white\",\n        size=9\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + scale_fill_gradient(low=\"darkblue\", high=\"lightblue\")\n    + labs(\n        x=\"WACC\", y=\"Perpetual Growth Rate\",\n        fill=\"EV\\n(Trillion VND)\",\n        title=\"DCF Sensitivity: Enterprise Value by WACC and Growth Rate\"\n    )\n)\n\nsensitivity_figure.show()\n\n\n\n\n\n\n\nFigure 6.5: Sensitivity of enterprise value to WACC and perpetual growth rate assumptions. Small changes in these inputs can substantially affect valuation.\n\n\n\n\n\nThe sensitivity analysis reveals several important insights:\n\nValuation is highly sensitive to inputs: Small changes in WACC or growth rate produce large changes in enterprise value. A 1 percentage point change in WACC can shift value by 20% or more.\nThe relationship is non-linear: The impact of growth rate changes is amplified at lower WACCs because the terminal value formula has \\((r-g)\\) in the denominator.\nReasonable people can disagree: Given input uncertainty, DCF should be thought of as producing a range of values, not a single precise number.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#from-enterprise-value-to-equity-value",
    "href": "05_discounted_cash_flow.html#from-enterprise-value-to-equity-value",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.11 From Enterprise Value to Equity Value",
    "text": "6.11 From Enterprise Value to Equity Value\nOur DCF analysis yields enterprise value (i.e., the total value of the company’s operations to all capital providers). To determine equity value (what shareholders own), we must adjust for the claims of debt holders and any non-operating assets:\n\\[\n\\text{Equity Value} = \\text{Enterprise Value} + \\text{Non-Operating Assets} - \\text{Debt}\n\\]\nNon-Operating Assets include:\n\nExcess cash beyond operating needs\nMarketable securities\nNon-core real estate or investments\n\nDebt includes:\n\nShort-term debt\nLong-term debt\nCapital lease obligations\nPreferred stock (if treated as debt-like)\n\n\n# Get most recent balance sheet data for FPT\nlatest_year = fpt_data[\"year\"].max()\nlatest_data = fpt_data[fpt_data[\"year\"] == latest_year].iloc[0]\n\n# Extract debt and cash (column names may vary)\ntotal_debt = latest_data.get(\"total_debt\", 0)\ncash = latest_data.get(\"ca_cce\", 0)\n\n# Compute equity value\nenterprise_value = dcf_result[\"enterprise_value\"]\nequity_value = enterprise_value - total_debt + cash\n\nprint(\"From Enterprise Value to Equity Value\")\nprint(\"=\" * 50)\nprint(f\"Enterprise Value: {enterprise_value/1e12:.1f} trillion VND\")\nprint(f\"Less: Total Debt: {total_debt/1e12:.1f} trillion VND\")\nprint(f\"Plus: Cash: {cash/1e12:.1f} trillion VND\")\nprint(f\"Equity Value: {equity_value/1e12:.1f} trillion VND\")\n\nFrom Enterprise Value to Equity Value\n==================================================\nEnterprise Value: 113.4 trillion VND\nLess: Total Debt: 0.0 trillion VND\nPlus: Cash: 8.3 trillion VND\nEquity Value: 121.7 trillion VND\n\n\n\n6.11.1 Implied Share Price\nIf we know the number of shares outstanding, we can compute an implied share price:\n\n# Get shares outstanding (this would come from market data)\n# Using placeholder - in practice, get from exchange data\nshares_outstanding = latest_data.get(\"total_equity\", equity_value) / 25000  # Rough estimate\n\nimplied_price = equity_value / shares_outstanding\n\nprint(f\"\\nImplied Share Price: {implied_price:,.0f} VND\")\n\n\nImplied Share Price: 101,645 VND\n\n\nComparing the implied price to the current market price tells us whether the stock appears under- or overvalued according to our DCF model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#limitations-and-practical-considerations",
    "href": "05_discounted_cash_flow.html#limitations-and-practical-considerations",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.12 Limitations and Practical Considerations",
    "text": "6.12 Limitations and Practical Considerations\nDCF analysis is powerful but has important limitations:\n\n6.12.1 Sensitivity to Assumptions\nAs our sensitivity analysis showed, small changes in inputs produce large changes in value. This is particularly problematic because the most influential inputs (long-term growth, WACC) are the hardest to estimate accurately.\n\n\n6.12.2 Terminal Value Dominance\nTerminal value often represents 60-80% of total value, yet it’s based on assumptions about the very distant future. This concentrates valuation risk in the most uncertain component.\n\n\n6.12.3 Garbage In, Garbage Out\nDCF is only as good as its inputs. Unrealistic growth assumptions, optimistic margins, or inappropriate discount rates produce meaningless valuations. The discipline of DCF lies in forcing analysts to justify their assumptions.\n\n\n6.12.4 Not Suitable for All Companies\nDCF works best for companies with:\n\nPositive and predictable cash flows\nStable or predictably changing margins\nReasonable visibility into future operations\n\nIt struggles with:\n\nEarly-stage companies with no profits\nHighly cyclical businesses\nCompanies undergoing major transitions\nFinancial institutions (which require different approaches)\n\n\n\n6.12.5 Complement with Other Methods\nWise practitioners use DCF alongside other valuation methods:\n\nComparable company analysis: How do similar companies trade?\nPrecedent transactions: What have acquirers paid for similar businesses?\nSum-of-the-parts: Value divisions separately and add\n\nWhen methods converge, confidence increases. When they diverge, it prompts investigation into why.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#key-takeaways",
    "href": "05_discounted_cash_flow.html#key-takeaways",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.13 Key Takeaways",
    "text": "6.13 Key Takeaways\nThis chapter introduced Discounted Cash Flow analysis as a framework for intrinsic valuation. The main insights are:\n\nFree Cash Flow is the foundation: FCF represents cash available to all investors after operating expenses, taxes, and investments. It differs from net income by excluding non-cash items and including investment needs.\nRatio-based forecasting links components to revenue: By expressing FCF components as percentages of revenue, we can systematically forecast cash flows based on revenue growth assumptions and operating ratio projections.\nTerminal value captures long-term value: The perpetuity growth model assumes FCF grows at a constant rate forever. The perpetual growth rate should not exceed long-term economic growth.\nWACC is the appropriate discount rate: The Weighted Average Cost of Capital reflects the blended cost of debt and equity financing, adjusted for the tax shield on interest.\nDCF produces enterprise value: To derive equity value, subtract debt and add non-operating assets. Dividing by shares outstanding yields an implied share price.\nSensitivity analysis is essential: Given input uncertainty, presenting a range of values based on different assumptions is more honest than a single point estimate.\nDCF complements other methods: No single valuation method is definitive. Cross-checking DCF with market multiples and transaction comparables provides a more complete picture.\n\nThe true value of DCF analysis lies not in producing a precise number but in forcing rigorous thinking about what drives company value. The process of building a DCF model (i.e., forecasting growth, projecting margins, estimating risk) develops deep understanding of the business being valued.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html",
    "href": "06_accessing_and_managing_financial_data.html",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "",
    "text": "7.1 Overview of Vietnamese Financial Data Sources\nThis chapter provides a guide to organizing, accessing, and managing financial data specifically tailored for the Vietnamese market. While global financial databases such as CRSP and Compustat serve as standard resources for developed markets, emerging markets like Vietnam require a different approach due to unique data sources, market structures, and regulatory environments. Understanding these nuances is essential for conducting rigorous empirical research on Vietnamese equities, bonds, and macroeconomic indicators.\nVietnam’s financial market has experienced remarkable growth since the establishment of the Ho Chi Minh City Stock Exchange (HOSE) in 2000 and the Hanoi Stock Exchange (HNX) in 2005. Today, the market comprises over 1,600 listed companies across three trading venues: HOSE for large-cap stocks, HNX for mid-cap stocks, and UPCoM (Unlisted Public Company Market) for smaller companies transitioning to formal listing. This diversity creates both opportunities and challenges for financial researchers seeking comprehensive coverage of the Vietnamese equity universe.\nThe Vietnamese market presents several distinctive characteristics that researchers must account for. Foreign ownership limits (typically 49% for most sectors, with exceptions for banking and certain strategic industries), trading band restrictions (e.g., currently \\(\\pm\\) 7% for HOSE and \\(\\pm\\) 10% for HNX), and the T+2 settlement cycle all influence market microstructure and return dynamics. Additionally, the market operates in Vietnamese Dong (VND), requiring careful attention to currency effects when comparing results with international studies.\nWe begin by loading the essential Python packages that facilitate data acquisition and management throughout this chapter.\nWe also define the date range for our data collection, which spans from the early days of the Vietnamese stock market to the present. This extended timeframe allows us to capture the market’s evolution through various economic cycles, including the 2008 global financial crisis, the 2011-2012 domestic banking crisis, and the COVID-19 pandemic period.\nBefore diving into the technical implementation, it is valuable to understand the landscape of financial data providers serving the Vietnamese market. Unlike developed markets where a few dominant providers (Bloomberg, Refinitiv, FactSet) offer comprehensive coverage, Vietnamese financial data has historically been fragmented across multiple sources, each with distinct strengths and limitations.\nThe primary sources of Vietnamese financial data include official exchange feeds from HOSE and HNX, which provide real-time and historical trading data. The State Securities Commission of Vietnam (SSC) publishes regulatory filings, corporate announcements, and market statistics. Commercial data vendors such as FiinGroup, StoxPlus (now part of FiinGroup), and VNDirect offer curated datasets with varying levels of coverage and data quality. Additionally, the State Bank of Vietnam (SBV) and the General Statistics Office (GSO) provide macroeconomic indicators essential for asset pricing research.\nFor academic researchers, this fragmentation traditionally involved difficult trade-offs between cost, coverage, data quality, and ease of access. Commercial providers like FiinGroup offer clean, standardized data but require subscription fees that may be prohibitive for individual researchers and smaller institutions. Open-source alternatives provide free access but often require substantial data cleaning and validation efforts. Manually collecting data from government websites is time-consuming and prone to inconsistencies.\nFortunately, this landscape has improved significantly with the emergence of Datacore as a unified data platform for Vietnamese financial markets. In our experience working with Vietnamese financial data across multiple research projects, Datacore has proven to be the most practical solution for academic research. The platform consolidates data from multiple sources, including stock prices, corporate fundamentals, market indices, macroeconomic indicators, and alternative data, into a single, accessible interface with a well-documented API.\nWhat distinguishes Datacore from traditional commercial providers like FiinGroup extends beyond mere data aggregation. While FiinGroup has long been the institutional incumbent, several factors make Datacore particularly attractive for rigorous empirical research:\nThroughout this chapter, we leverage Datacore as our primary data source. By centralizing our data acquisition through a single platform, we benefit from consistent data formats, reliable corporate action adjustments, and comprehensive market coverage spanning HOSE, HNX, and UPCoM. The code examples that follow demonstrate how straightforward Vietnamese financial research becomes when data access friction is minimized.\nThe following table summarizes the key data sources for Vietnamese financial research:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#overview-of-vietnamese-financial-data-sources",
    "href": "06_accessing_and_managing_financial_data.html#overview-of-vietnamese-financial-data-sources",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "",
    "text": "API-First Architecture: Datacore was built from the ground up for programmatic access, making it seamlessly integrable with Python, R, and other research workflows. FiinGroup’s data access, by contrast, often requires manual downloads or cumbersome Excel-based interfaces that impede reproducibility.\nCost Efficiency: Academic researchers frequently operate under budget constraints. Datacore offers competitive pricing structures that make comprehensive market coverage accessible without the substantial subscription fees associated with legacy providers.\nCorporate Action Handling: One persistent challenge with Vietnamese data is accurate adjustment for stock splits, bonus shares, and rights issues. Datacore implements transparent adjustment methodologies with clear documentation, whereas legacy providers often apply adjustments inconsistently or without adequate explanation.\nUpdate Frequency: Datacore maintains near real-time data updates with clear timestamps, enabling event study research and timely portfolio rebalancing. Traditional providers often suffer from publication lags that can compromise research requiring current data.\nCoverage Breadth: Beyond standard price and fundamental data, Datacore integrates alternative data, and macroeconomic indicators into a unified schema. This eliminates the need to merge datasets from multiple sources, which is a process that introduces potential errors and consumes valuable research time.\n\n\n\n\n\n\nTable 7.1: Vietnamese Financial Data Sources\n\n\n\n\n\n\n\n\n\n\n\n\nData Source\nCoverage\nAccess Type\nKey Strengths\nLimitations\n\n\n\n\nDatacore\nPrices, fundamentals, indices, macro, derivatives\nAPI\nUnified platform, programmatic access, comprehensive coverage, transparent methodology\nNewer platform\n\n\nFiinGroup\nFull market coverage\nCommercial\nEstablished reputation, institutional adoption\nHigh cost, manual access, limited API\n\n\nHOSE/HNX websites\nOfficial exchange data\nFree (manual)\nAuthoritative, real-time\nNo API, manual collection required\n\n\nGSO (gso.gov.vn)\nMacroeconomic indicators\nFree (manual)\nOfficial government statistics\nInfrequent updates, no API\n\n\nSBV (sbv.gov.vn)\nMonetary policy, rates\nFree (manual)\nCentral bank data\nManual download only\n\n\nCafeF/VnExpress\nNews, announcements\nFree\nMarket sentiment, events\nUnstructured, requires NLP processing",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#stock-market-data",
    "href": "06_accessing_and_managing_financial_data.html#stock-market-data",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.2 Stock Market Data",
    "text": "7.2 Stock Market Data\nThe resulting DataFrame contains essential security identifiers including the ticker symbol, company name in both Vietnamese and English, exchange listing, industry classification according to the Vietnam Standard Industrial Classification (VSIC), and various flags indicating special status such as foreign ownership restrictions or trading suspensions.\n\n7.2.1 Historical Price Data\n\n\n7.2.2 Fundamental Data and Financial Statements\nBeyond price data, fundamental analysis requires access to corporate financial statements including balance sheets, income statements, and cash flow statements. Vietnamese publicly listed companies are required to publish quarterly and annual financial reports according to Vietnamese Accounting Standards (VAS), which differ in certain respects from International Financial Reporting Standards (IFRS). Understanding these differences is important when comparing Vietnamese firms with international peers or applying models developed using US or European data.\nKey differences between VAS and IFRS that affect financial analysis include:\n\nRevenue recognition: VAS allows more flexibility in timing of revenue recognition compared to IFRS 15\nFinancial instruments: VAS has less comprehensive guidance on fair value measurement\nLease accounting: VAS does not require operating lease capitalization as under IFRS 16\nGoodwill: VAS requires amortization while IFRS requires impairment testing only\n\n\n\n7.2.3 Corporate Actions and Events\nAccurate treatment of corporate actions is essential for computing correct returns and maintaining data integrity. Vietnamese companies frequently engage in corporate actions including cash dividends, stock dividends (bonus shares), rights issues, and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#market-indices-and-benchmarks",
    "href": "06_accessing_and_managing_financial_data.html#market-indices-and-benchmarks",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.3 Market Indices and Benchmarks",
    "text": "7.3 Market Indices and Benchmarks\nConstructing appropriate benchmarks is fundamental to performance evaluation and factor model estimation. The Vietnamese market features several indices that serve different purposes in financial research.\n\n\n\nTable 7.2: Vietnamese Market Indices\n\n\n\n\n\n\n\n\n\n\n\nIndex\nExchange\nDescription\nUse Case\n\n\n\n\nVN-Index\nHOSE\nAll HOSE-listed stocks\nBroad market benchmark\n\n\nVN30-Index\nHOSE\n30 largest, most liquid\nInvestable benchmark\n\n\nHNX-Index\nHNX\nAll HNX-listed stocks\nMid-cap benchmark\n\n\nHNX30-Index\nHNX\n30 largest HNX stocks\nHNX large-cap\n\n\nVNAllShare\nCombined\nHOSE + HNX\nTotal market\n\n\nVN100\nCombined\nTop 100 stocks\nLarge/mid-cap\n\n\n\n\n\n\nThe VN-Index, which tracks all stocks listed on HOSE, is the most widely followed benchmark and serves as the primary gauge of overall market performance. The HNX-Index covers stocks on the Hanoi exchange, while the VN30-Index tracks the thirty largest and most liquid stocks on HOSE.\nFor asset pricing research, the VN30-Index is particularly valuable as it represents the investable universe for institutional investors and serves as the underlying for Vietnam’s most liquid derivatives contracts. The constituent stocks are reviewed semi-annually based on market capitalization, liquidity, and free-float requirements.\n\n# Retrieve VN-Index historical data\n\n\n7.3.1 Index Constituent Data\nFor factor model construction and portfolio analysis, access to index constituent lists and their weights is essential. While official constituent data requires subscription to exchange data feeds, we can approximate index membership using market capitalization and liquidity filters.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#macroeconomic-data-from-vietnamese-sources",
    "href": "06_accessing_and_managing_financial_data.html#macroeconomic-data-from-vietnamese-sources",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.4 Macroeconomic Data from Vietnamese Sources",
    "text": "7.4 Macroeconomic Data from Vietnamese Sources\nAsset pricing models often incorporate macroeconomic variables as predictors of expected returns or as state variables in conditional models. For the Vietnamese market, relevant macroeconomic data comes primarily from two sources: the General Statistics Office (GSO) and the State Bank of Vietnam (SBV).\n\n7.4.1 Key Macroeconomic Indicators\nThe following macroeconomic variables are particularly relevant for Vietnamese financial research:\n\nConsumer Price Index (CPI): Essential for computing real returns and inflation-adjusted valuations. Vietnam experienced periods of high inflation, particularly during 2008 and 2011 when annual CPI exceeded 20%.\nIndustrial Production Index (IPI): Proxy for economic activity and business cycle conditions.\nMoney Supply (M2): Indicator of monetary policy stance and liquidity conditions.\nCredit Growth: Bank lending growth, a key driver of economic activity in Vietnam’s bank-dominated financial system.\nUSD/VND Exchange Rate: Critical for international investors and companies with foreign currency exposure.\nForeign Direct Investment (FDI): Indicator of international capital flows and economic confidence.\nTrade Balance: Export and import dynamics affecting corporate earnings.\n\nUnfortunately, unlike the US Federal Reserve’s FRED database, Vietnamese macroeconomic data is not available through standardized APIs. Researchers must typically download data manually from GSO and SBV websites or use web scraping techniques.\n\n# Structure for Vietnamese macroeconomic data\n\n\n\n7.4.2 Risk-Free Rate Approximation\nDetermining an appropriate risk-free rate for Vietnam presents challenges not encountered in developed markets. Unlike the US Treasury market, Vietnam’s government bond market is relatively illiquid with limited secondary trading. Several alternatives exist:\n\nSBV Refinancing Rate: The policy rate set by the State Bank of Vietnam. Not directly investable but reflects monetary policy stance.\nGovernment Bond Yields: One-year or longer-term government bond yields from auction results. More investable but less liquid than US Treasuries.\nInterbank Rates: Overnight or term interbank lending rates. Reflect short-term funding costs but include credit risk.\nAdjusted US Rate: US Treasury rate plus expected VND depreciation, following uncovered interest rate parity.\n\n\ndef calculate_risk_free_rate(macro_data, method=\"refinancing\"):\n    \"\"\"\n    Calculate risk-free rate proxy for Vietnamese market.\n    \n    Parameters\n    ----------\n    macro_data : pd.DataFrame\n        DataFrame with macroeconomic data\n    method : str\n        Method for risk-free rate: 'refinancing', 'bond', or 'adjusted_us'\n    \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with date and monthly risk-free rate\n    \"\"\"\n    if method == \"refinancing\":\n        # Use SBV refinancing rate, convert annual to monthly\n        rf = macro_data[[\"date\", \"refinancing_rate\"]].copy()\n        rf[\"rf_monthly\"] = rf[\"refinancing_rate\"] / 12 / 100\n        \n    elif method == \"adjusted_us\":\n        # US rate + expected VND depreciation\n        # Requires additional data on US rates and exchange rate expectations\n        pass\n    \n    return rf[[\"date\", \"rf_monthly\"]]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#setting-up-a-database-for-vietnamese-financial-data",
    "href": "06_accessing_and_managing_financial_data.html#setting-up-a-database-for-vietnamese-financial-data",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.5 Setting Up a Database for Vietnamese Financial Data",
    "text": "7.5 Setting Up a Database for Vietnamese Financial Data\nManaging financial data across multiple sources and formats requires a systematic approach to data storage. We recommend using SQLite as the primary database engine for several reasons: it requires no server setup, stores the entire database in a single portable file, supports standard SQL queries, and integrates seamlessly with Python through the built-in sqlite3 module.\n\n7.5.1 Database Schema Design\nOur database schema is designed to support efficient queries for common research tasks while maintaining data integrity. We create separate tables for different data types with appropriate relationships.\n\nimport os\nimport sqlite3\n\n# Create data directory if it doesn't exist\nif not os.path.exists(\"data\"):\n    os.makedirs(\"data\")\n\n# Initialize SQLite database connection\ntidy_finance_python = sqlite3.connect(\n    \"data/tidy_finance_python.sqlite\"\n)\n\n\n\n7.5.2 Storing Data\nWith the database schema established, we can store our collected data using pandas’ to_sql() method.\n\n# Store stock listing data\ncommon_stocks.to_sql(\n    name=\"stock_master\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store stock price data\nstock_prices.to_sql(\n    name=\"stock_prices_daily\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store market indices\nvn_index.to_sql(\n    name=\"market_indices\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store factor returns\nfactors_vietnam.to_sql(\n    name=\"factors_monthly\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#querying-and-updating-the-database",
    "href": "06_accessing_and_managing_financial_data.html#querying-and-updating-the-database",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.6 Querying and Updating the Database",
    "text": "7.6 Querying and Updating the Database\nOnce data is stored in the database, retrieval is straightforward using SQL queries. The pandas read_sql_query() function executes a SQL statement and returns the results as a DataFrame.\n\n# Query stock prices for specific symbols and date range\nquery = \"\"\"\nSELECT date, symbol, close, volume\nFROM stock_prices_daily\nWHERE symbol IN ('VNM', 'VIC', 'FPT', 'VHM', 'VCB')\n  AND date &gt;= '2020-01-01'\nORDER BY symbol, date\n\"\"\"\n\nselected_stocks = pd.read_sql_query(\n    sql=query,\n    con=tidy_finance_python,\n    parse_dates=[\"date\"]\n)\n\n# Query factor data merged with market returns\nquery_factors = \"\"\"\nSELECT f.date, f.mkt_rf, f.smb, f.hml, f.rf,\n       m.cpi_yoy, m.credit_growth\nFROM factors_monthly f\nLEFT JOIN macro_monthly m ON f.date = m.date\nWHERE f.date &gt;= '2015-01-01'\nORDER BY f.date\n\"\"\"\n\nfactor_data = pd.read_sql_query(\n    sql=query_factors,\n    con=tidy_finance_python,\n    parse_dates=[\"date\"]\n)\n\n\n7.6.1 Database Maintenance\nRegular database maintenance ensures optimal performance and data integrity.\n\n# Optimize database\ntidy_finance_python.execute(\"VACUUM\")\n\n# Check database integrity\nintegrity_check = pd.read_sql_query(\n    \"PRAGMA integrity_check\",\n    tidy_finance_python\n)\nprint(f\"Integrity check: {integrity_check.iloc[0, 0]}\")\n\n# Get database statistics\ntable_stats = pd.read_sql_query(\"\"\"\n    SELECT name, \n           (SELECT COUNT(*) FROM stock_prices_daily) as price_rows,\n           (SELECT COUNT(*) FROM stock_master) as stock_count,\n           (SELECT COUNT(*) FROM factors_monthly) as factor_months\n    FROM sqlite_master\n    WHERE type='table' AND name='stock_master'\n\"\"\", tidy_finance_python)\n\nprint(table_stats)\n\n# Close connection when done\ntidy_finance_python.close()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#alternative-data-sources-for-vietnamese-markets",
    "href": "06_accessing_and_managing_financial_data.html#alternative-data-sources-for-vietnamese-markets",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.7 Alternative Data Sources for Vietnamese Markets",
    "text": "7.7 Alternative Data Sources for Vietnamese Markets\nBeyond traditional price and fundamental data, researchers increasingly incorporate alternative data sources to gain unique insights into market dynamics.\n\n7.7.1 Foreign Investor Flow Data\nForeign investor flow data is particularly valuable given the significant role of foreign capital in Vietnamese equity markets. The State Securities Commission publishes daily foreign ownership statistics by security.\n\n\n7.7.2 News and Sentiment Data\nMedia sentiment from Vietnamese financial news sources offers another research avenue. Major outlets such as CafeF, VnExpress Finance, and Vietstock publish real-time news that can be analyzed for market sentiment.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#key-takeaways",
    "href": "06_accessing_and_managing_financial_data.html#key-takeaways",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.8 Key Takeaways",
    "text": "7.8 Key Takeaways\n\nMarket Structure Understanding: The Vietnamese financial market operates across three exchanges (HOSE, HNX, UPCoM) with distinct characteristics including foreign ownership limits, trading band restrictions, and a T+2 settlement cycle. Researchers must account for these institutional features in empirical analysis.\nMacroeconomic Data Challenges: Unlike developed markets with standardized APIs (e.g., FRED), Vietnamese macroeconomic data requires manual collection from government sources (GSO, SBV). Researchers should plan for this additional data gathering effort and implement systematic data management practices.\nDatabase-Centric Workflow: SQLite provides an efficient and portable database solution for managing Vietnamese financial data across research projects. The structured database approach enables reproducible research workflows, efficient queries, and easy data sharing among collaborators.\nData Quality Imperative: Data quality validation is especially important for emerging market data. Implementing systematic checks for missing values, extreme returns, duplicate entries, and cross-source validation helps ensure research reliability and reproducibility.\nAlternative Data Opportunities: Foreign investor flows, corporate announcements, and media sentiment provide unique research opportunities in the Vietnamese market that can complement traditional price and fundamental analysis. These data sources can reveal insights about market dynamics not captured in standard datasets.\nContinuous Maintenance: Financial databases require ongoing maintenance including incremental updates, integrity checks, and optimization. Establishing systematic update procedures ensures data currency and database performance over time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html",
    "href": "07_datacore_data.html",
    "title": "8  DataCore Data",
    "section": "",
    "text": "8.1 Setting Up the Environment\nThis chapter demonstrates how to connect to Datacore, Vietnam’s premier provider of financial and economic data for research applications. We use this connection to download the most commonly used data for stock prices and firm characteristics, including historical trading data and company fundamentals. While Datacore requires a subscription, most students and researchers typically have access through their university libraries or research institutions. For those without access, Datacore provides demo datasets that allow you to run the code examples in this book with sample data.\nThe chapter is organized as follows. We first establish the connection to Datacore’s cloud storage infrastructure. Then, we download and prepare company fundamentals data, including balance sheet items, income statement variables, and derived metrics essential for asset pricing research. Next, we retrieve and process stock price data, computing returns, market capitalizations, and excess returns. We conclude by merging these datasets and providing descriptive statistics that characterize the Vietnamese equity market.\nWe begin by loading the Python packages used throughout this chapter. The core packages include pandas for data manipulation, numpy for numerical operations, and sqlite3 for local database management. We also import visualization libraries for creating publication-quality figures.\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom datetime import datetime\nfrom io import BytesIO\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format, percent_format\nWe establish a connection to our local SQLite database, which serves as the central repository for all processed data. This database was introduced in the previous chapter and will store the cleaned datasets for use in subsequent analyses.\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\nWe define the date range for our data collection. The Vietnamese stock market began operations in July 2000 with the establishment of the Ho Chi Minh City Stock Exchange (HOSE), so our sample period starts from 2000 and extends through the end of 2024.\nstart_date = \"2000-01-01\"\nend_date = \"2024-12-31\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#connecting-to-datacore",
    "href": "07_datacore_data.html#connecting-to-datacore",
    "title": "8  DataCore Data",
    "section": "8.2 Connecting to Datacore",
    "text": "8.2 Connecting to Datacore\nDatacore delivers data through a cloud-based object storage system built on MinIO, an S3-compatible storage infrastructure. This architecture enables efficient, programmatic access to large datasets without the limitations of traditional database connections. To access the data, you need credentials provided by Datacore upon subscription: an endpoint URL, access key, and secret key.\nThe following class establishes the connection to Datacore’s storage system. The credentials are stored as environment variables for security, following best practices for credential management in research computing environments.\n\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass DatacoreConnection:\n    \"\"\"\n    Connection handler for Datacore's MinIO-based storage system.\n    \n    This class manages authentication and provides methods for\n    accessing financial datasets stored in Datacore's cloud infrastructure.\n    \n    Attributes\n    ----------\n    s3 : boto3.client\n        S3-compatible client for interacting with Datacore storage\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize connection using environment variables.\"\"\"\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n        \n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n    \n    def test_connection(self):\n        \"\"\"Verify connection by listing available buckets.\"\"\"\n        response = self.s3.list_buckets()\n        print(\"Connected successfully. Available buckets:\")\n        for bucket in response.get(\"Buckets\", []):\n            print(f\"  - {bucket['Name']}\")\n    \n    def list_objects(self, bucket_name, prefix=\"\"):\n        \"\"\"List objects in a bucket with optional prefix filter.\"\"\"\n        response = self.s3.list_objects_v2(\n            Bucket=bucket_name, \n            Prefix=prefix\n        )\n        return [obj[\"Key\"] for obj in response.get(\"Contents\", [])]\n    \n    def read_excel(self, bucket_name, key):\n        \"\"\"Read an Excel file from Datacore storage.\"\"\"\n        obj = self.s3.get_object(Bucket=bucket_name, Key=key)\n        return pd.read_excel(BytesIO(obj[\"Body\"].read()))\n    \n    def read_csv(self, bucket_name, key, **kwargs):\n        \"\"\"Read a CSV file from Datacore storage.\"\"\"\n        obj = self.s3.get_object(Bucket=bucket_name, Key=key)\n        return pd.read_csv(BytesIO(obj[\"Body\"].read()), **kwargs)\n\nWith the connection class defined, we can establish a connection and verify access to Datacore’s data repositories.\n\n# Initialize connection\nconn = DatacoreConnection()\nconn.test_connection()\n\n# Get bucket name from environment\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nConnected successfully. Available buckets:\n  - dsteam-data\n  - rawbctc",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#company-fundamentals-data",
    "href": "07_datacore_data.html#company-fundamentals-data",
    "title": "8  DataCore Data",
    "section": "8.3 Company Fundamentals Data",
    "text": "8.3 Company Fundamentals Data\nFirm accounting data are essential for portfolio analyses, factor construction, and valuation studies. Datacore hosts comprehensive fundamentals data for Vietnamese listed companies, including annual and quarterly financial statements prepared according to Vietnamese Accounting Standards (VAS).\n\n8.3.1 Understanding Vietnamese Financial Statements\nBefore processing the data, it is important to understand the structure of Vietnamese financial reports. Vietnamese companies follow VAS, which shares similarities with International Financial Reporting Standards (IFRS) but has notable differences:\n\nFiscal Year: Most Vietnamese companies use a calendar fiscal year ending December 31, though some companies (particularly in retail and agriculture) use different fiscal year-ends.\nReporting Frequency: Listed companies must publish quarterly financial statements within 20 days of quarter-end and annual audited statements within 90 days of fiscal year-end.\nIndustry-Specific Formats: Companies in banking, insurance, and securities sectors follow specialized reporting formats that differ from the standard industrial format.\nCurrency: All figures are reported in Vietnamese Dong (VND). Given the large nominal values (millions to trillions of VND), we often scale figures to millions or billions for readability.\n\n\n\n8.3.2 Downloading Fundamentals Data\nDatacore organizes fundamentals data in Excel files partitioned by time period for efficient access. We download and concatenate these files to create a comprehensive dataset spanning our sample period.\n\n# Define paths to fundamentals data files\nfundamentals_paths = [\n    \"fundamental_annual_1767674486317/fundamental_annual_1.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_2.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_3.xlsx\",\n]\n\n# Download and combine all files\nfundamentals_list = []\nfor path in fundamentals_paths:\n    df_temp = conn.read_excel(bucket_name, path)\n    fundamentals_list.append(df_temp)\n    print(f\"Downloaded: {path} ({len(df_temp):,} rows)\")\n\ndf_fundamentals_raw = pd.concat(fundamentals_list, ignore_index=True)\nprint(f\"\\nTotal observations: {len(df_fundamentals_raw):,}\")\n\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_1.xlsx (10,000 rows)\n\n\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_2.xlsx (10,000 rows)\n\n\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_3.xlsx (2,821 rows)\n\nTotal observations: 22,821\n\n\n\n\n8.3.3 Cleaning and Standardizing Fundamentals\nThe raw fundamentals data requires several cleaning steps to ensure consistency and usability. We standardize variable names, handle missing values, and create derived variables commonly used in asset pricing research.\n\ndef clean_fundamentals(df):\n    \"\"\"\n    Clean and standardize company fundamentals data.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Raw fundamentals data from Datacore\n    \n    Returns\n    -------\n    pd.DataFrame\n        Cleaned fundamentals with standardized column names\n    \"\"\"\n    df = df.copy()\n    \n    # Standardize identifiers\n    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\n    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n    \n    # Drop rows with missing identifiers\n    df = df.dropna(subset=[\"symbol\", \"year\"])\n    \n    # Define columns that should be numeric\n    numeric_columns = [\n        \"total_asset\", \"total_equity\", \"total_liabilities\",\n        \"total_current_asset\", \"total_current_liabilities\",\n        \"is_net_revenue\", \"is_cogs\", \"is_manage_expense\",\n        \"is_interest_expense\", \"is_eat\", \"is_net_business_profit\",\n        \"na_tax_deferred\", \"nl_tax_deferred\", \"e_preferred_stock\",\n        \"capex\", \"total_cfo\", \"ca_cce\", \"ca_total_inventory\",\n        \"ca_acc_receiv\", \"cfo_interest_expense\", \"basic_eps\",\n        \"is_shareholders_eat\", \"cl_loan\", \"cl_finlease\",\n        \"cl_due_long_debt\", \"nl_loan\", \"nl_finlease\",\n        \"is_cos_of_sales\", \"e_equity\"\n    ]\n    \n    for col in numeric_columns:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n    \n    # Handle duplicates: keep row with most non-missing values\n    df[\"_completeness\"] = df.notna().sum(axis=1)\n    df = (df\n        .sort_values([\"symbol\", \"year\", \"_completeness\"])\n        .drop_duplicates(subset=[\"symbol\", \"year\"], keep=\"last\")\n        .drop(columns=\"_completeness\")\n        .reset_index(drop=True)\n    )\n    \n    return df\n\ndf_fundamentals = clean_fundamentals(df_fundamentals_raw)\nprint(f\"After cleaning: {len(df_fundamentals):,} firm-year observations\")\nprint(f\"Unique firms: {df_fundamentals['symbol'].nunique():,}\")\n\nAfter cleaning: 21,232 firm-year observations\nUnique firms: 1,554\n\n\n\n\n8.3.4 Creating Standardized Variables\nTo facilitate comparison with international studies and ensure compatibility with standard asset pricing methodologies, we create variables following conventions established in the academic literature. We map Vietnamese financial statement items to their Compustat equivalents where possible.\n\ndef create_standard_variables(df):\n    \"\"\"\n    Create standardized financial variables for asset pricing research.\n    \n    This function maps Vietnamese financial statement items to standard\n    variable names used in the academic finance literature, following\n    conventions from Fama and French (1992, 1993, 2015).\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Cleaned fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with standardized variables added\n    \"\"\"\n    df = df.copy()\n    \n    # Fiscal date (assume December year-end)\n    df[\"datadate\"] = pd.to_datetime(df[\"year\"].astype(str) + \"-12-31\")\n    \n    # === Balance Sheet Items ===\n    df[\"at\"] = df[\"total_asset\"]                    # Total assets\n    df[\"lt\"] = df[\"total_liabilities\"]              # Total liabilities\n    df[\"seq\"] = df[\"total_equity\"]                  # Stockholders' equity\n    df[\"act\"] = df[\"total_current_asset\"]           # Current assets\n    df[\"lct\"] = df[\"total_current_liabilities\"]     # Current liabilities\n    \n    # Common equity (fallback to total equity if not available)\n    df[\"ceq\"] = df.get(\"e_equity\", df[\"seq\"])\n    \n    # === Deferred Taxes ===\n    df[\"txditc\"] = df.get(\"na_tax_deferred\", 0).fillna(0)  # Deferred tax assets\n    df[\"txdb\"] = df.get(\"nl_tax_deferred\", 0).fillna(0)    # Deferred tax liab.\n    df[\"itcb\"] = 0  # Investment tax credit (rare in Vietnam)\n    \n    # === Preferred Stock ===\n    pref = df.get(\"e_preferred_stock\", 0)\n    if isinstance(pref, pd.Series):\n        pref = pref.fillna(0)\n    df[\"pstk\"] = pref\n    df[\"pstkrv\"] = pref  # Redemption value\n    df[\"pstkl\"] = pref   # Liquidating value\n    \n    # === Income Statement Items ===\n    df[\"sale\"] = df[\"is_net_revenue\"]                        # Net sales/revenue\n    df[\"cogs\"] = df.get(\"is_cogs\", 0).fillna(0)              # Cost of goods sold\n    df[\"xsga\"] = df.get(\"is_manage_expense\", 0).fillna(0)    # SG&A expenses\n    df[\"xint\"] = df.get(\"is_interest_expense\", 0).fillna(0)  # Interest expense\n    df[\"ni\"] = df.get(\"is_eat\", np.nan)                      # Net income\n    df[\"oibdp\"] = df.get(\"is_net_business_profit\", np.nan)   # Operating income\n    \n    # === Cash Flow Items ===\n    df[\"oancf\"] = df.get(\"total_cfo\", np.nan)  # Operating cash flow\n    df[\"capx\"] = df.get(\"capex\", np.nan)       # Capital expenditures\n    \n    return df\n\ndf_fundamentals = create_standard_variables(df_fundamentals)\n\n\n\n8.3.5 Computing Book Equity and Profitability\nBook equity is a crucial variable for value investing strategies and the construction of HML (High Minus Low) factor portfolios. We follow the definition from Kenneth French’s data library, which accounts for deferred taxes and preferred stock.\n\ndef compute_book_equity(df):\n    \"\"\"\n    Compute book equity following Fama-French conventions.\n    \n    Book equity = Stockholders' equity \n                  + Deferred taxes and investment tax credit\n                  - Preferred stock\n    \n    Negative or zero book equity is set to missing, as book-to-market\n    ratios are undefined for such firms.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals with standardized variables\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with book equity (be) added\n    \"\"\"\n    df = df.copy()\n    \n    # Primary measure: stockholders' equity\n    # Fallback 1: common equity + preferred stock\n    # Fallback 2: total assets - total liabilities\n    seq_measure = (df[\"seq\"]\n        .combine_first(df[\"ceq\"] + df[\"pstk\"])\n        .combine_first(df[\"at\"] - df[\"lt\"])\n    )\n    \n    # Add deferred taxes\n    deferred_taxes = (df[\"txditc\"]\n        .combine_first(df[\"txdb\"] + df[\"itcb\"])\n        .fillna(0)\n    )\n    \n    # Subtract preferred stock (use redemption value as primary)\n    preferred = (df[\"pstkrv\"]\n        .combine_first(df[\"pstkl\"])\n        .combine_first(df[\"pstk\"])\n        .fillna(0)\n    )\n    \n    # Book equity calculation\n    df[\"be\"] = seq_measure + deferred_taxes - preferred\n    \n    # Set non-positive book equity to missing\n    df[\"be\"] = df[\"be\"].where(df[\"be\"] &gt; 0, np.nan)\n    \n    return df\n\ndf_fundamentals = compute_book_equity(df_fundamentals)\n\n# Summary statistics for book equity\nprint(\"Book Equity Summary Statistics (in million VND):\")\nprint(df_fundamentals[\"be\"].describe().round(2))\n\nBook Equity Summary Statistics (in million VND):\ncount    2.023500e+04\nmean     1.031884e+12\nstd      4.705269e+12\nmin      4.404402e+07\n25%      7.267610e+10\n50%      1.803885e+11\n75%      5.304653e+11\nmax      1.836314e+14\nName: be, dtype: float64\n\n\nOperating profitability, introduced by Fama and French (2015), measures a firm’s profits relative to its book equity. Firms with higher operating profitability tend to have higher expected returns.\n\ndef compute_profitability(df):\n    \"\"\"\n    Compute operating profitability following Fama-French (2015).\n    \n    Operating profitability = (Revenue - COGS - SG&A - Interest) / Book Equity\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals with book equity computed\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with operating profitability (op) added\n    \"\"\"\n    df = df.copy()\n    \n    # Operating profit before taxes\n    operating_profit = (\n        df[\"sale\"] \n        - df[\"cogs\"].fillna(0) \n        - df[\"xsga\"].fillna(0) \n        - df[\"xint\"].fillna(0)\n    )\n    \n    # Scale by book equity\n    df[\"op\"] = operating_profit / df[\"be\"]\n    \n    # Winsorize extreme values (outside 1st and 99th percentiles)\n    lower = df[\"op\"].quantile(0.01)\n    upper = df[\"op\"].quantile(0.99)\n    df[\"op\"] = df[\"op\"].clip(lower=lower, upper=upper)\n    \n    return df\n\ndf_fundamentals = compute_profitability(df_fundamentals)\n\n\n\n8.3.6 Computing Investment\nInvestment, measured as asset growth, captures firms’ investment behavior. Fama and French (2015) document that firms with high asset growth (aggressive investment) tend to have lower future returns.\n\ndef compute_investment(df):\n    \"\"\"\n    Compute investment (asset growth) following Fama-French (2015).\n    \n    Investment = (Total Assets_t / Total Assets_{t-1}) - 1\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with investment (inv) added\n    \"\"\"\n    df = df.copy()\n    \n    # Create lagged assets\n    df_lag = (df[[\"symbol\", \"year\", \"at\"]]\n        .assign(year=lambda x: x[\"year\"] + 1)\n        .rename(columns={\"at\": \"at_lag\"})\n    )\n    \n    # Merge lagged values\n    df = df.merge(df_lag, on=[\"symbol\", \"year\"], how=\"left\")\n    \n    # Compute investment (asset growth)\n    df[\"inv\"] = df[\"at\"] / df[\"at_lag\"] - 1\n    \n    # Set to missing if lagged assets non-positive\n    df[\"inv\"] = df[\"inv\"].where(df[\"at_lag\"] &gt; 0, np.nan)\n    \n    return df\n\ndf_fundamentals = compute_investment(df_fundamentals)\n\n\n\n8.3.7 Computing Total Debt\nIn Vietnamese financial statements, total liabilities include non-interest-bearing items such as accounts payable and tax payables. For leverage analysis, we compute total interest-bearing debt by aggregating loan and lease obligations.\n\ndef compute_total_debt(df):\n    \"\"\"\n    Compute total interest-bearing debt.\n    \n    Total Debt = Short-term loans + Finance leases (current)\n                 + Current portion of long-term debt\n                 + Long-term loans + Finance leases (non-current)\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with total_debt added\n    \"\"\"\n    df = df.copy()\n    \n    df[\"total_debt\"] = (\n        df.get(\"cl_loan\", 0).fillna(0) +           # Short-term bank loans\n        df.get(\"cl_finlease\", 0).fillna(0) +       # Current finance leases\n        df.get(\"cl_due_long_debt\", 0).fillna(0) +  # Current portion LT debt\n        df.get(\"nl_loan\", 0).fillna(0) +           # Long-term bank loans\n        df.get(\"nl_finlease\", 0).fillna(0)         # Non-current finance leases\n    )\n    \n    return df\n\ndf_fundamentals = compute_total_debt(df_fundamentals)\n\n\n\n8.3.8 Applying Filters and Final Preparation\nWe apply standard filters to ensure data quality: requiring positive assets, non-negative sales, and presence of core variables needed for portfolio construction.\n\n# Keep only observations with required variables\nrequired_vars = [\"at\", \"lt\", \"seq\", \"sale\"]\ncomp_vn = df_fundamentals.dropna(subset=required_vars)\n\n# Apply quality filters\ncomp_vn = comp_vn.query(\"at &gt; 0\")      # Positive assets\ncomp_vn = comp_vn.query(\"sale &gt;= 0\")   # Non-negative sales\n\n# Keep last observation per firm-year (in case of restatements)\ncomp_vn = (comp_vn\n    .sort_values(\"datadate\")\n    .groupby([\"symbol\", \"year\"])\n    .tail(1)\n    .reset_index(drop=True)\n)\n\n# Diagnostic summary\nprint(f\"Final sample: {len(comp_vn):,} firm-year observations\")\nprint(f\"Unique firms: {comp_vn['symbol'].nunique():,}\")\nprint(f\"Sample period: {comp_vn['year'].min()} - {comp_vn['year'].max()}\")\n\nFinal sample: 20,091 firm-year observations\nUnique firms: 1,502\nSample period: 1998 - 2023\n\n\n\n\n8.3.9 Storing Fundamentals Data\nWe store the prepared fundamentals data in our local SQLite database for use in subsequent chapters.\n\ncomp_vn.to_sql(\n    name=\"comp_vn\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Company fundamentals saved to database.\")\n\nCompany fundamentals saved to database.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#stock-price-data",
    "href": "07_datacore_data.html#stock-price-data",
    "title": "8  DataCore Data",
    "section": "8.4 Stock Price Data",
    "text": "8.4 Stock Price Data\nStock price data forms the foundation of return-based analyses in empirical finance. Datacore provides comprehensive historical price data for all securities traded on HOSE, HNX, and UPCoM, including adjusted prices that account for corporate actions.\n\n8.4.1 Downloading Price Data\nWe download the historical price data from Datacore’s storage system. The data includes daily observations with open, high, low, close prices, trading volume, and adjustment factors.\n\n# Download historical price data\nprices_raw = conn.read_csv(\n    bucket_name,\n    \"historycal_price/dataset_historical_price.csv\",\n    low_memory=False\n)\n\nprint(f\"Downloaded {len(prices_raw):,} daily price observations\")\nprint(f\"Date range: {prices_raw['date'].min()} to {prices_raw['date'].max()}\")\n\nDownloaded 4,307,791 daily price observations\nDate range: 2010-01-04 to 2025-05-12\n\n\n\n\n8.4.2 Processing Price Data\nWe clean the price data and compute adjusted prices that account for stock splits, stock dividends, and other corporate actions.\n\ndef process_price_data(df):\n    \"\"\"\n    Process raw price data from Datacore.\n    \"\"\"\n    df = df.copy()\n    \n    # Parse dates\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    \n    # Standardize column names\n    df = df.rename(columns={\n        \"open_price\": \"open\",\n        \"high_price\": \"high\",\n        \"low_price\": \"low\",\n        \"close_price\": \"close\",\n        \"vol_total\": \"volume\"\n    })\n    \n    # Compute adjusted close price\n    df[\"adjusted_close\"] = df[\"close\"] * df[\"adj_ratio\"]\n    \n    # Standardize symbol\n    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\n    \n    # Sort for return calculation\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Add year and month\n    df[\"year\"] = df[\"date\"].dt.year\n    df[\"month\"] = df[\"date\"].dt.month\n    \n    return df\n\nprices = process_price_data(prices_raw)\n\n\n\n8.4.3 Computing Shares Outstanding and Market Capitalization\nMarket capitalization is computed as the product of price and shares outstanding. Since Datacore provides earnings per share and net income, we can infer shares outstanding from these variables.\n\ndef compute_shares_outstanding(fundamentals_df):\n    \"\"\"\n    Compute shares outstanding from fundamentals.\n    \"\"\"\n    shares = fundamentals_df.copy()\n    shares[\"shrout\"] = shares[\"is_shareholders_eat\"] / shares[\"basic_eps\"]\n    shares = shares[[\"symbol\", \"year\", \"shrout\"]].dropna()\n    \n    return shares\n\nshares_outstanding = compute_shares_outstanding(df_fundamentals)\n\n\ndef add_market_cap(df, shares_df):\n    \"\"\"\n    Add market capitalization to price data.\n    \"\"\"\n    df = df.merge(shares_df, on=[\"symbol\", \"year\"], how=\"left\")\n    \n    # Compute market cap (in million VND)\n    df[\"mktcap\"] = (df[\"close\"] * df[\"shrout\"]) / 1_000_000\n    \n    # Set zero or negative market cap to missing\n    df[\"mktcap\"] = df[\"mktcap\"].where(df[\"mktcap\"] &gt; 0, np.nan)\n    \n    return df\n\nprices = add_market_cap(prices, shares_outstanding)\n\n\n\n8.4.4 Computing Returns and Excess Returns\nWe compute returns using adjusted closing prices to ensure returns correctly reflect total shareholder returns including dividends and corporate actions.\n\n8.4.4.1 Creating Daily Dataset\n\nSequential version\n\n\ndef create_daily_dataset(df, annual_rf=0.04):\n    \"\"\"\n    Create daily price dataset with returns and excess returns.\n    \"\"\"\n    df = df.copy()\n    \n    # Sort by symbol and date (critical for correct return calculation)\n    df = df.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Remove duplicate dates within each symbol (keep last observation)\n    df = df.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Compute daily returns\n    df[\"ret\"] = df.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    \n    # Cap extreme negative returns\n    df[\"ret\"] = df[\"ret\"].clip(lower=-0.99)\n    \n    # Daily risk-free rate (assuming 252 trading days)\n    df[\"risk_free\"] = annual_rf / 252\n    \n    # Excess returns\n    df[\"ret_excess\"] = df[\"ret\"] - df[\"risk_free\"]\n    df[\"ret_excess\"] = df[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    df[\"mktcap_lag\"] = df.groupby(\"symbol\")[\"mktcap\"].shift(1)\n    \n    return df\n\nprices_daily = create_daily_dataset(prices)\n\n\nParallel version\n\n\nfrom joblib import Parallel, delayed\nimport os\n\ndef process_daily_symbol(symbol_df, annual_rf=0.04):\n    \"\"\"\n    Process a single symbol's daily data.\n    \"\"\"\n    df = symbol_df.copy()\n    \n    # Sort by date (critical for correct return calculation)\n    df = df.sort_values(\"date\").reset_index(drop=True)\n    \n    # Remove duplicate dates (keep last observation if duplicates exist)\n    df = df.drop_duplicates(subset=[\"date\"], keep=\"last\")\n    \n    # Compute daily returns\n    df[\"ret\"] = df[\"adjusted_close\"].pct_change()\n\n    # Replace infinite values with NaN\n    df[\"ret\"] = df[\"ret\"].replace([np.inf, -np.inf], np.nan)\n    \n    # Cap extreme negative returns\n    df[\"ret\"] = df[\"ret\"].clip(lower=-0.99)\n    \n    # Daily risk-free rate\n    df[\"risk_free\"] = annual_rf / 252\n    \n    # Excess returns\n    df[\"ret_excess\"] = df[\"ret\"] - df[\"risk_free\"]\n    df[\"ret_excess\"] = df[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    df[\"mktcap_lag\"] = df[\"mktcap\"].shift(1)\n    \n    return df\n\ndef create_daily_dataset_parallel(df, annual_rf=0.04):\n    \"\"\"\n    Create daily price dataset using parallel processing.\n    \"\"\"\n    # Ensure data is sorted before splitting\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Split by symbol\n    symbol_groups = [group for _, group in df.groupby(\"symbol\")]\n    \n    n_jobs = max(1, os.cpu_count() - 1)\n    print(f\"Processing {len(symbol_groups):,} symbols using {n_jobs} cores...\")\n    \n    results = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(process_daily_symbol)(group, annual_rf) \n        for group in symbol_groups\n    )\n    \n    return pd.concat(results, ignore_index=True)\n\nprices_daily = create_daily_dataset_parallel(prices)\n\n# Quick validation\nprint(\"\\nValidation checks:\")\nprint(f\"Any duplicate (symbol, date): {prices_daily.duplicated(subset=['symbol', 'date']).sum()}\")\nprint(f\"Sample of non-zero returns:\")\nprint(prices_daily[prices_daily[\"ret\"] != 0][[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(10))\n\n\nprices_daily.query(\"symbol == 'FPT'\")[[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(3)\n\nProcessing 1,837 symbols using 23 cores...\n\n\n\nValidation checks:\nAny duplicate (symbol, date): 0\nSample of non-zero returns:\n   symbol       date  adjusted_close       ret\n0     A32 2018-10-23       44.574418       NaN\n27    A32 2018-11-29       55.072640  0.235521\n30    A32 2018-12-04       48.188560 -0.125000\n43    A32 2018-12-21       51.974804  0.078571\n49    A32 2019-01-02       55.072640  0.059603\n53    A32 2019-01-08       50.030370 -0.091557\n74    A32 2019-02-13       44.289180 -0.114754\n75    A32 2019-02-14       41.008500 -0.074074\n78    A32 2019-02-19       36.087480 -0.120000\n91    A32 2019-03-08       41.336568  0.145455\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n1146076\nFPT\n2010-01-04\n1170.9885\nNaN\n\n\n1146077\nFPT\n2010-01-05\n1170.9885\n0.000000\n\n\n1146078\nFPT\n2010-01-06\n1149.6978\n-0.018182\n\n\n\n\n\n\n\n\n# Select columns\ndaily_columns = [\n    \"symbol\", \"date\", \"year\", \"month\",\n    \"open\", \"high\", \"low\", \"close\", \"volume\",\n    \"adjusted_close\", \"shrout\", \"mktcap\", \"mktcap_lag\",\n    \"ret\", \"risk_free\", \"ret_excess\"\n]\nprices_daily = prices_daily[daily_columns]\n\n# Remove observations with missing essential variables\nprices_daily = prices_daily.dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n\nprint(\"Daily Return Summary Statistics:\")\nprint(prices_daily[\"ret\"].describe().round(4))\nprint(f\"\\nFinal daily sample: {len(prices_daily):,} observations\")\n\nDaily Return Summary Statistics:\ncount    3.462157e+06\nmean     3.000000e-04\nstd      4.480000e-02\nmin     -9.900000e-01\n25%     -4.900000e-03\n50%      0.000000e+00\n75%      4.000000e-03\nmax      3.250000e+01\nName: ret, dtype: float64\n\nFinal daily sample: 3,462,157 observations\n\n\n\n\n8.4.4.2 Creating Monthly Dataset\nFor monthly returns, we compute returns directly from month-end adjusted prices rather than compounding daily returns. This avoids compounding errors from missing days and is the standard approach in empirical finance.\n\nSequential version\n\n\ndef create_monthly_dataset(df, annual_rf=0.04):\n    \"\"\"\n    Create monthly price dataset with returns computed from \n    month-end to month-end adjusted prices.\n    \"\"\"\n    df = df.copy()\n    \n    # Sort by symbol and date (critical for correct return calculation)\n    df = df.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Remove duplicate dates within each symbol (keep last observation)\n    df = df.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Get month-end observations\n    monthly = (df\n        .groupby(\"symbol\")\n        .resample(\"ME\", on=\"date\")\n        .agg({\n            \"open\": \"first\",           # First day open\n            \"high\": \"max\",             # Monthly high\n            \"low\": \"min\",              # Monthly low\n            \"close\": \"last\",           # Last day close\n            \"volume\": \"sum\",           # Total monthly volume\n            \"adjusted_close\": \"last\",  # Month-end adjusted price\n            \"shrout\": \"last\",          # Month-end shares outstanding\n            \"mktcap\": \"last\",          # Month-end market cap\n            \"year\": \"last\",\n            \"month\": \"last\"\n        })\n        .reset_index()\n    )\n    \n    # Remove duplicate (symbol, date) after resampling (safety check)\n    monthly = monthly.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Sort again after resampling\n    monthly = monthly.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Compute monthly returns from month-end to month-end adjusted prices\n    monthly[\"ret\"] = monthly.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    \n    # Cap extreme returns\n    monthly[\"ret\"] = monthly[\"ret\"].clip(lower=-0.99)\n    \n    # Monthly risk-free rate\n    monthly[\"risk_free\"] = annual_rf / 12\n    \n    # Excess returns\n    monthly[\"ret_excess\"] = monthly[\"ret\"] - monthly[\"risk_free\"]\n    monthly[\"ret_excess\"] = monthly[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap for portfolio weighting\n    monthly[\"mktcap_lag\"] = monthly.groupby(\"symbol\")[\"mktcap\"].shift(1)\n    \n    return monthly\n\nprices_monthly = create_monthly_dataset(prices)\n\n\nParallel version\n\n\nfrom joblib import Parallel, delayed\nimport os\n\ndef process_monthly_symbol(symbol_df, annual_rf=0.04):\n    \"\"\"\n    Process a single symbol's data to monthly frequency.\n    \"\"\"\n    df = symbol_df.copy()\n    \n    # Sort by date (critical for correct return calculation)\n    df = df.sort_values(\"date\").reset_index(drop=True)\n    \n    # Remove duplicate dates (keep last observation if duplicates exist)\n    df = df.drop_duplicates(subset=[\"date\"], keep=\"last\")\n    \n    # Set date as index for resampling\n    df = df.set_index(\"date\")\n    \n    # Resample to monthly\n    monthly = df.resample(\"ME\").agg({\n        \"symbol\": \"last\",\n        \"open\": \"first\",\n        \"high\": \"max\",\n        \"low\": \"min\",\n        \"close\": \"last\",\n        \"volume\": \"sum\",\n        \"adjusted_close\": \"last\",\n        \"shrout\": \"last\",\n        \"mktcap\": \"last\",\n        \"year\": \"last\",\n        \"month\": \"last\"\n    }).reset_index()\n    \n    # Remove rows where symbol is NaN (months with no trading)\n    monthly = monthly.dropna(subset=[\"symbol\"])\n    \n    # Sort by date\n    monthly = monthly.sort_values(\"date\").reset_index(drop=True)\n    \n    # Compute monthly returns\n    monthly[\"ret\"] = monthly[\"adjusted_close\"].pct_change()\n    \n    # Replace infinite values with NaN\n    monthly[\"ret\"] = monthly[\"ret\"].replace([np.inf, -np.inf], np.nan)\n    \n    # Cap extreme returns\n    monthly[\"ret\"] = monthly[\"ret\"].clip(lower=-0.99)\n    \n    # Monthly risk-free rate\n    monthly[\"risk_free\"] = annual_rf / 12\n    \n    # Excess returns\n    monthly[\"ret_excess\"] = monthly[\"ret\"] - monthly[\"risk_free\"]\n    monthly[\"ret_excess\"] = monthly[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    monthly[\"mktcap_lag\"] = monthly[\"mktcap\"].shift(1)\n    \n    return monthly\n\ndef create_monthly_dataset_parallel(df, annual_rf=0.04):\n    \"\"\"\n    Create monthly price dataset using parallel processing.\n    \"\"\"\n    # Ensure data is sorted before splitting\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Split by symbol\n    symbol_groups = [group for _, group in df.groupby(\"symbol\")]\n    \n    n_jobs = max(1, os.cpu_count() - 1)\n    print(f\"Processing {len(symbol_groups):,} symbols using {n_jobs} cores...\")\n    \n    results = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(process_monthly_symbol)(group, annual_rf) \n        for group in symbol_groups\n    )\n    \n    return pd.concat(results, ignore_index=True)\n\nprices_monthly = create_monthly_dataset_parallel(prices)\n\n# Validation checks\nprint(\"\\nValidation checks:\")\nprint(f\"Any duplicate (symbol, date): {prices_monthly.duplicated(subset=['symbol', 'date']).sum()}\")\nprint(f\"\\nSample of non-zero returns:\")\nprint(prices_monthly[prices_monthly[\"ret\"] != 0][[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(10))\n\nprices_monthly.query(\"symbol == 'FPT'\")[[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(3)\n\nProcessing 1,837 symbols using 23 cores...\n\n\n\nValidation checks:\nAny duplicate (symbol, date): 0\n\nSample of non-zero returns:\n   symbol       date  adjusted_close       ret\n0     A32 2018-10-31       44.574418       NaN\n1     A32 2018-11-30       55.072640  0.235521\n2     A32 2018-12-31       51.974804 -0.056250\n3     A32 2019-01-31       50.030370 -0.037411\n4     A32 2019-02-28       36.087480 -0.278689\n5     A32 2019-03-31       41.828670  0.159091\n7     A32 2019-05-31       43.304976  0.035294\n8     A32 2019-06-30       35.929125 -0.170323\n9     A32 2019-07-31       37.525975  0.044444\n10    A32 2019-08-31       38.324400  0.021277\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n55963\nFPT\n2010-01-31\n1092.9226\nNaN\n\n\n55964\nFPT\n2010-02-28\n1107.1164\n0.012987\n\n\n55965\nFPT\n2010-03-31\n1185.1823\n0.070513\n\n\n\n\n\n\n\n\n# Select columns (same structure as daily)\nmonthly_columns = [\n    \"symbol\", \"date\", \"year\", \"month\",\n    \"open\", \"high\", \"low\", \"close\", \"volume\",\n    \"adjusted_close\", \"shrout\", \"mktcap\", \"mktcap_lag\",\n    \"ret\", \"risk_free\", \"ret_excess\"\n]\nprices_monthly = prices_monthly[monthly_columns]\n\n# Remove observations with missing essential variables\nprices_monthly = prices_monthly.dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n\nprint(\"Monthly Return Summary Statistics:\")\nprint(prices_monthly[\"ret\"].describe().round(4))\nprint(f\"\\nFinal monthly sample: {len(prices_monthly):,} observations\")\n\nMonthly Return Summary Statistics:\ncount    165499.0000\nmean          0.0042\nstd           0.1862\nmin          -0.9900\n25%          -0.0703\n50%           0.0000\n75%           0.0553\nmax          12.7500\nName: ret, dtype: float64\n\nFinal monthly sample: 165,499 observations\n\n\n\n\n\n8.4.5 Storing Price Data\n\nprices_daily.to_sql(\n    name=\"prices_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(\"Daily price data saved to database.\")\n\nprices_monthly.to_sql(\n    name=\"prices_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(\"Monthly price data saved to database.\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#descriptive-statistics",
    "href": "07_datacore_data.html#descriptive-statistics",
    "title": "8  DataCore Data",
    "section": "8.5 Descriptive Statistics",
    "text": "8.5 Descriptive Statistics\nBefore proceeding to asset pricing analyses, we examine the characteristics of our sample to understand the Vietnamese equity market’s evolution and composition.\n\n8.5.1 Market Evolution Over Time\nWe first examine how the number of listed securities has grown over time.\n\nsecurities_over_time = (prices_monthly\n    .groupby(\"date\")\n    .agg(\n        n_securities=(\"symbol\", \"nunique\"),\n        total_mktcap=(\"mktcap\", \"sum\")\n    )\n    .reset_index()\n)\n\n\nsecurities_figure = (\n    ggplot(securities_over_time, aes(x=\"date\", y=\"n_securities\"))\n    + geom_line(color=\"steelblue\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Number of Securities\",\n        title=\"Growth of Vietnamese Stock Market\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + scale_y_continuous(labels=comma_format())\n    + theme_minimal()\n)\nsecurities_figure.show()\n\n\n\n\n\n\n\nFigure 8.1: The figure shows the monthly number of securities in the Vietnamese stock market sample.\n\n\n\n\n\n\n\n8.5.2 Market Capitalization Evolution\nThe aggregate market capitalization reflects the overall size and development of the Vietnamese equity market.\n\nmktcap_figure = (\n    ggplot(securities_over_time, aes(x=\"date\", y=\"total_mktcap / 1000\"))\n    + geom_line(color=\"darkgreen\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Market Cap (Trillion VND)\",\n        title=\"Total Market Capitalization of Vietnamese Equities\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + scale_y_continuous(labels=comma_format())\n    + theme_minimal()\n)\nmktcap_figure.show()\n\n\n\n\n\n\n\nFigure 8.2: The figure shows the total market capitalization of Vietnamese listed companies over time.\n\n\n\n\n\n\n\n8.5.3 Return Distribution\nUnderstanding the distribution of monthly returns helps identify potential data quality issues and characterize market risk.\n\nreturn_distribution = (\n    ggplot(prices_monthly, aes(x=\"ret_excess\"))\n    + geom_histogram(\n        binwidth=0.02, \n        fill=\"steelblue\", \n        color=\"white\",\n        alpha=0.7\n    )\n    + labs(\n        x=\"Monthly Excess Return\",\n        y=\"Frequency\",\n        title=\"Distribution of Monthly Excess Returns\"\n    )\n    + scale_x_continuous(limits=(-0.5, 0.5))\n    + theme_minimal()\n)\nreturn_distribution.show()\n\n\n\n\n\n\n\nFigure 8.3: Distribution of monthly excess returns for Vietnamese stocks.\n\n\n\n\n\n\n\n8.5.4 Coverage of Book Equity\nBook equity is essential for constructing value portfolios. We examine what fraction of our sample has book equity data available over time.\n\n# Merge prices with fundamentals\ncoverage_data = (prices_monthly\n    .assign(year=lambda x: x[\"date\"].dt.year)\n    .groupby([\"symbol\", \"year\"])\n    .tail(1)\n    .merge(comp_vn[[\"symbol\", \"year\", \"be\"]], \n           on=[\"symbol\", \"year\"], \n           how=\"left\")\n)\n\n# Compute coverage by year\nbe_coverage = (coverage_data\n    .groupby(\"year\")\n    .apply(lambda x: pd.Series({\n        \"share_with_be\": x[\"be\"].notna().mean()\n    }))\n    .reset_index()\n)\n\ncoverage_figure = (\n    ggplot(be_coverage, aes(x=\"year\", y=\"share_with_be\"))\n    + geom_line(color=\"darkorange\", size=1)\n    + geom_point(color=\"darkorange\", size=2)\n    + labs(\n        x=\"Year\",\n        y=\"Share with Book Equity\",\n        title=\"Coverage of Book Equity Data\"\n    )\n    + scale_y_continuous(labels=percent_format(), limits=(0, 1))\n    + theme_minimal()\n)\ncoverage_figure.show()\n\n\n\n\n\n\n\nFigure 8.4: Share of securities with available book equity data by year.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#merging-stock-and-fundamental-data",
    "href": "07_datacore_data.html#merging-stock-and-fundamental-data",
    "title": "8  DataCore Data",
    "section": "8.6 Merging Stock and Fundamental Data",
    "text": "8.6 Merging Stock and Fundamental Data\nThe final step links price data with fundamental data using the stock symbol as the common identifier. This merged dataset forms the basis for constructing portfolios sorted on firm characteristics.\n\n# Example: Create merged dataset for end-of-June each year\nmerged_data = (prices_monthly\n    .query(\"month == 6\")\n    .merge(\n        comp_vn[[\"symbol\", \"year\", \"be\", \"op\", \"inv\", \"at\"]],\n        on=[\"symbol\", \"year\"],\n        how=\"left\",\n        suffixes=(\"\", \"_fundamental\")\n    )\n)\n\n# Convert BE from VND to BILLION VND\nmerged_data[\"be\"] = merged_data[\"be\"] / 1e9\n\n# Compute book-to-market ratio\nmerged_data[\"bm\"] = merged_data[\"be\"] / merged_data[\"mktcap\"]\n\nmerged_data.loc[\n    (merged_data[\"bm\"] &lt;= 0) |\n    (merged_data[\"bm\"] &gt; 20),\n    \"bm\"\n] = pd.NA\n\n\nmerged_data[\"bm\"].describe(percentiles=[.01, .1, .5, .9, .99])\n\nprint(f\"Merged observations: {len(merged_data):,}\")\nprint(f\"With book-to-market: {merged_data['bm'].notna().sum():,}\")\nmerged_data.head(3)\nmerged_data.describe()\nmerged_data\n\nMerged observations: 13,756\nWith book-to-market: 12,859\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nyear\nmonth\nopen\nhigh\nlow\nclose\nvolume\nadjusted_close\n...\nmktcap\nmktcap_lag\nret\nrisk_free\nret_excess\nbe\nop\ninv\nat\nbm\n\n\n\n\n0\nA32\n2019-06-30\n2019.0\n6.0\n26.4\n26.4\n21.0\n22.5\n3700\n35.929125\n...\n153.000\n179.52\n-0.170323\n0.003333\n-0.173657\n223.612748\n0.232362\n-0.072329\n4.349303e+11\n1.461521\n\n\n1\nA32\n2020-06-30\n2020.0\n6.0\n25.0\n26.3\n24.5\n26.3\n7500\n38.811173\n...\n178.840\n187.00\n-0.067977\n0.003333\n-0.071311\n242.216943\n0.195565\n0.122698\n4.882955e+11\n1.354378\n\n\n2\nA32\n2021-06-30\n2021.0\n6.0\n30.2\n37.0\n29.5\n32.0\n78400\n45.363520\n...\n217.600\n214.20\n0.015873\n0.003333\n0.012540\n238.385190\n0.157723\n0.081581\n5.281309e+11\n1.095520\n\n\n3\nA32\n2022-06-30\n2022.0\n6.0\n30.9\n35.5\n25.0\n35.3\n15200\n47.503210\n...\n240.040\n210.12\n0.142395\n0.003333\n0.139061\n215.399735\n0.172085\n0.036584\n5.474523e+11\n0.897349\n\n\n4\nA32\n2023-06-30\n2023.0\n6.0\n30.1\n33.5\n29.2\n29.4\n2400\n35.064204\n...\n199.920\n204.68\n-0.023256\n0.003333\n-0.026589\n222.024135\n0.174658\n-0.076752\n5.054342e+11\n1.110565\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13751\nYTC\n2019-06-30\n2019.0\n6.0\n70.0\n79.9\n70.0\n79.9\n38900\n171.451817\n...\n246.092\n215.60\n0.141429\n0.003333\n0.138095\n59.901389\n0.738190\n-0.021758\n7.521980e+11\n0.243411\n\n\n13752\nYTC\n2020-06-30\n2020.0\n6.0\n88.5\n88.5\n77.0\n87.0\n150640\n180.966960\n...\n267.960\n272.58\n-0.016949\n0.003333\n-0.020282\n13.459082\n-0.458548\n0.323501\n9.955348e+11\n0.050228\n\n\n13753\nYTC\n2021-06-30\n2021.0\n6.0\n76.0\n115.5\n61.0\n61.0\n34100\n126.884880\n...\n187.880\n234.08\n-0.197368\n0.003333\n-0.200702\n21.746595\n0.539521\n-0.215694\n7.808035e+11\n0.115747\n\n\n13754\nYTC\n2022-06-30\n2022.0\n6.0\n68.0\n68.0\n65.0\n65.5\n200\n136.245240\n...\n201.740\n209.44\n-0.036765\n0.003333\n-0.040098\n32.403055\n0.483088\n0.182911\n9.236206e+11\n0.160618\n\n\n13755\nYTC\n2023-06-30\n2023.0\n6.0\n59.0\n59.0\n59.0\n59.0\n49545\n122.724720\n...\n181.720\n181.72\n0.000000\n0.003333\n-0.003333\n38.976624\n0.450157\n0.017930\n9.401815e+11\n0.214487\n\n\n\n\n13756 rows × 21 columns\n\n\n\n\nfrom plotnine import *\nimport numpy as np\n\nbm_plot_data = (\n    merged_data[[\"bm\"]]\n      .dropna()\n      .assign(bm_plot=lambda x: x[\"bm\"].clip(upper=10))\n)\n\n(\n    ggplot(bm_plot_data, aes(x=\"bm_plot\")) +\n    geom_histogram(bins=80) +\n    labs(\n        title=\"Distribution of Book to Market Ratios\",\n        x=\"Book to Market (capped at 10 for plotting)\",\n        y=\"Number of firms\"\n    ) +\n    theme_minimal()\n)\n\n\n\n\n\n\n\n\n\nsize_plot_data = (\n    merged_data[[\"mktcap_lag\"]]\n      .dropna()\n      .assign(log_size=lambda x: np.log(x[\"mktcap_lag\"]))\n)\n\n(\n    ggplot(size_plot_data, aes(x=\"log_size\")) +\n    geom_histogram(bins=80) +\n    labs(\n        title=\"Distribution of Log Market Capitalization\",\n        x=\"Log Market Cap\",\n        y=\"Number of firms\"\n    ) +\n    theme_minimal()\n)\n\n\n\n\n\n\n\n\n\nscatter_data = (\n    merged_data[[\"be\", \"mktcap_lag\"]]\n      .dropna()\n      .assign(\n          log_be=lambda x: np.log(x[\"be\"]),\n          log_me=lambda x: np.log(x[\"mktcap_lag\"])\n      )\n)\n\n(\n    ggplot(scatter_data, aes(x=\"log_me\", y=\"log_be\")) +\n    geom_point(alpha=0.2) +\n    labs(\n        title=\"Log Book Equity vs Log Market Equity\",\n        x=\"Log Market Cap\",\n        y=\"Log Book Equity\"\n    ) +\n    theme_minimal()\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#key-takeaways",
    "href": "07_datacore_data.html#key-takeaways",
    "title": "8  DataCore Data",
    "section": "8.7 Key Takeaways",
    "text": "8.7 Key Takeaways\n\nDatacore provides unified access to Vietnamese financial data through a modern cloud-based infrastructure, eliminating the need to aggregate data from multiple fragmented sources.\nCompany fundamentals from Datacore include comprehensive balance sheet, income statement, and cash flow data prepared according to Vietnamese Accounting Standards, which we map to standard variables used in international research.\nBook equity computation follows the Fama-French methodology, accounting for deferred taxes and preferred stock to ensure comparability with US-based studies.\nStock price data includes adjustment factors for corporate actions, enabling accurate return calculations over long horizons.\nMonthly frequency is standard for asset pricing research, reducing noise while maintaining sufficient observations for statistical inference.\nRisk-free rate approximation uses Vietnamese government bond yields as a proxy, given the absence of a standardized short-term rate series comparable to US Treasury bills.\nData quality validation through descriptive statistics and visualization helps identify potential issues before conducting formal analyses.\nBatch processing enables efficient handling of large daily datasets that would otherwise exceed memory constraints.\n\n\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>DataCore Data</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html",
    "href": "08_beta_estimation.html",
    "title": "9  Beta Estimation",
    "section": "",
    "text": "9.1 Theoretical Foundation\nThis chapter introduces one of the most fundamental concepts in financial economics: the exposure of an individual stock to systematic market risk. According to the Capital Asset Pricing Model (CAPM) developed by Sharpe (1964), Lintner (1965), and Mossin (1966), cross-sectional variation in expected asset returns should be determined by the covariance between an asset’s excess return and the excess return on the market portfolio. The regression coefficient that captures this relationship (commonly known as market beta) serves as the cornerstone of modern portfolio theory and remains widely used in practice for cost of capital estimation, performance attribution, and risk management.\nIn this chapter, we develop a complete framework for estimating market betas for Vietnamese stocks. We begin with a conceptual overview of the CAPM and its empirical implementation. We then demonstrate beta estimation using ordinary least squares regression, first for individual stocks and then scaled to the entire market using rolling-window estimation. To handle the computational demands of estimating betas for hundreds of stocks across many time periods, we introduce parallelization techniques that dramatically reduce processing time. Finally, we compare beta estimates derived from monthly versus daily returns and examine how betas vary across industries and over time in the Vietnamese market.\nThe chapter leverages several important computational concepts that extend beyond beta estimation itself. Rolling-window estimation is a technique applicable to any time-varying parameter, while parallelization provides a general solution for computationally intensive tasks that can be divided into independent subtasks.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#theoretical-foundation",
    "href": "08_beta_estimation.html#theoretical-foundation",
    "title": "9  Beta Estimation",
    "section": "",
    "text": "9.1.1 The Capital Asset Pricing Model\nThe CAPM provides a theoretical framework linking expected returns to systematic risk. Under the model’s assumptions—including mean-variance optimizing investors, homogeneous expectations, and frictionless markets—the expected excess return on any asset \\(i\\) is proportional to its covariance with the market portfolio:\n\\[\nE[r_i - r_f] = \\beta_i \\cdot E[r_m - r_f]\n\\]\nwhere \\(r_i\\) is the return on asset \\(i\\), \\(r_f\\) is the risk-free rate, \\(r_m\\) is the return on the market portfolio, and \\(\\beta_i\\) is defined as:\n\\[\n\\beta_i = \\frac{\\text{Cov}(r_i, r_m)}{\\text{Var}(r_m)}\n\\]\nThe market beta \\(\\beta_i\\) measures the sensitivity of asset \\(i\\)’s returns to market movements. A beta greater than one indicates the asset amplifies market movements, while a beta less than one indicates dampened sensitivity. A beta of zero would imply no systematic risk exposure, leaving only idiosyncratic risk that can be diversified away.\n\n\n9.1.2 Empirical Implementation\nIn practice, we estimate beta by regressing excess stock returns on excess market returns:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i(r_{m,t} - r_{f,t}) + \\varepsilon_{i,t}\n\\tag{9.1}\\]\nwhere \\(\\alpha_i\\) represents abnormal return (Jensen’s alpha), \\(\\beta_i\\) is the market beta we seek to estimate, and \\(\\varepsilon_{i,t}\\) is the idiosyncratic error term. Under the CAPM, \\(\\alpha_i\\) should equal zero for all assets—any non-zero alpha represents a deviation from the model’s predictions.\nSeveral practical considerations affect beta estimation:\n\nEstimation Window: Longer windows provide more observations and thus more precise estimates, but may include outdated information if betas change over time. Common choices range from 36 to 60 months for monthly data.\nReturn Frequency: Monthly returns reduce noise but provide fewer observations. Daily returns offer more data points but may introduce microstructure effects and non-synchronous trading biases.\nMarket Proxy: The theoretical market portfolio includes all assets, but in practice we use a broad equity index. For Vietnam, we use the value-weighted market return constructed from our stock universe.\nMinimum Observations: Requiring a minimum number of observations (e.g., 48 out of 60 months) helps avoid unreliable estimates from sparse data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#setting-up-the-environment",
    "href": "08_beta_estimation.html#setting-up-the-environment",
    "title": "9  Beta Estimation",
    "section": "9.2 Setting Up the Environment",
    "text": "9.2 Setting Up the Environment\nWe begin by loading the necessary Python packages. The core packages handle data manipulation, statistical modeling, and database operations. We also import parallelization tools that will be essential when scaling our estimation to the full market.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nfrom scipy.stats.mstats import winsorize\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\nfrom joblib import Parallel, delayed, cpu_count\nfrom dateutil.relativedelta import relativedelta\n\nWe connect to our SQLite database containing the processed Vietnamese financial data from previous chapters.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#loading-and-preparing-data",
    "href": "08_beta_estimation.html#loading-and-preparing-data",
    "title": "9  Beta Estimation",
    "section": "9.3 Loading and Preparing Data",
    "text": "9.3 Loading and Preparing Data\n\n9.3.1 Stock Returns Data\nWe load the monthly stock returns data prepared in the Datacore chapter. The data includes excess returns (returns minus the risk-free rate) for all Vietnamese listed stocks.\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess \n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Add year for merging with fundamentals\nprices_monthly[\"year\"] = prices_monthly[\"date\"].dt.year\n\nprint(f\"Loaded {len(prices_monthly):,} monthly observations\")\nprint(f\"Covering {prices_monthly['symbol'].nunique():,} unique stocks\")\nprint(f\"Date range: {prices_monthly['date'].min():%Y-%m} to {prices_monthly['date'].max():%Y-%m}\")\n\nLoaded 209,495 monthly observations\nCovering 1,837 unique stocks\nDate range: 2010-01 to 2025-05\n\n\n\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess \n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n\n\n9.3.2 Company Information\nWe load company information to enable industry-level analysis of beta estimates.\n\ncomp_vn = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, datadate, icb_name_vi \n        FROM comp_vn\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\n# Extract year for merging\ncomp_vn[\"year\"] = comp_vn[\"datadate\"].dt.year\n\nprint(f\"Company data: {comp_vn['symbol'].nunique():,} firms\")\n\nCompany data: 1,502 firms\n\n\n\n\n9.3.3 Market Excess Returns\nFor the market portfolio proxy, we use the value-weighted market excess return. If you have constructed Fama-French factors in a previous chapter, load them here. Otherwise, we can construct a simple market return from our stock data.\n\n# Option 1: Load pre-computed market factor\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Option 2: Construct market return from stock data (if factors not available)\n# This computes the value-weighted average return across all stocks\ndef compute_market_return(prices_df):\n    \"\"\"\n    Compute value-weighted market return from individual stock returns.\n    \n    Parameters\n    ----------\n    prices_df : pd.DataFrame\n        Stock returns with mktcap_lag for weighting\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly market excess returns\n    \"\"\"\n    market_return = (prices_df\n        .groupby(\"date\")\n        .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n        .reset_index(name=\"mkt_excess\")\n    )\n    return market_return\n\n\n\n9.3.4 Merging Datasets\nWe combine the stock returns with market returns and company information to create our estimation dataset.\n\n# Merge stock returns with market returns\nprices_monthly = prices_monthly.merge(\n    factors_ff3_monthly, \n    on=\"date\", \n    how=\"left\"\n)\n\n# Merge with company information for industry classification\nprices_monthly = prices_monthly.merge(\n    comp_vn[[\"symbol\", \"year\", \"icb_name_vi\"]], \n    on=[\"symbol\", \"year\"], \n    how=\"left\"\n)\n\n# Remove observations with missing data\nprices_monthly = prices_monthly.dropna(subset=[\"ret_excess\", \"mkt_excess\"])\n\nprint(f\"Final estimation sample: {len(prices_monthly):,} observations\")\n\nFinal estimation sample: 169,983 observations\n\n\n\n\n9.3.5 Handling Outliers\nExtreme returns can unduly influence regression estimates. We apply winsorization to limit the impact of outliers while preserving the general distribution of returns. Winsorization at the 1% level replaces values below the 1st percentile with the 1st percentile value, and values above the 99th percentile with the 99th percentile value.\n\ndef winsorize_returns(df, columns, limits=(0.01, 0.01)):\n    \"\"\"\n    Apply winsorization to return columns to limit outlier influence.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing return columns\n    columns : list\n        Column names to winsorize\n    limits : tuple\n        Lower and upper percentile limits for winsorization\n        \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with winsorized columns\n    \"\"\"\n    df = df.copy()\n    for col in columns:\n        df[col] = winsorize(df[col], limits=limits)\n    return df\n\nprices_monthly = winsorize_returns(\n    prices_monthly, \n    columns=[\"ret_excess\", \"mkt_excess\"],\n    limits=(0.01, 0.01)\n)\n\nprint(\"Return distributions after winsorization:\")\nprint(prices_monthly[[\"ret_excess\", \"mkt_excess\"]].describe().round(4))\n\nReturn distributions after winsorization:\n        ret_excess   mkt_excess\ncount  169983.0000  169983.0000\nmean        0.0011      -0.0102\nstd         0.1548       0.0579\nmin        -0.4078      -0.1794\n25%        -0.0700      -0.0384\n50%        -0.0033      -0.0084\n75%         0.0531       0.0219\nmax         0.6117       0.1221",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#estimating-beta-for-individual-stocks",
    "href": "08_beta_estimation.html#estimating-beta-for-individual-stocks",
    "title": "9  Beta Estimation",
    "section": "9.4 Estimating Beta for Individual Stocks",
    "text": "9.4 Estimating Beta for Individual Stocks\n\n9.4.1 Single Stock Example\nBefore scaling to the full market, we demonstrate beta estimation for a single well-known Vietnamese stock. We use Vingroup (VIC), one of the largest conglomerates in Vietnam with significant exposure to real estate, retail, and automotive sectors.\n\n# Filter data for Vingroup\nvic_data = prices_monthly.query(\"symbol == 'VIC'\").copy()\n\nprint(f\"VIC observations: {len(vic_data)}\")\nprint(f\"Date range: {vic_data['date'].min():%Y-%m} to {vic_data['date'].max():%Y-%m}\")\n\nVIC observations: 150\nDate range: 2011-07 to 2023-12\n\n\nWe estimate the CAPM regression using ordinary least squares via the statsmodels package. The formula interface provides a convenient way to specify regression models.\n\n# Estimate CAPM for Vingroup\nmodel_vic = smf.ols(\n    formula=\"ret_excess ~ mkt_excess\",\n    data=vic_data\n).fit()\n\n# Display regression results\nprint(model_vic.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             ret_excess   R-squared:                       0.153\nModel:                            OLS   Adj. R-squared:                  0.147\nMethod:                 Least Squares   F-statistic:                     26.67\nDate:                Wed, 04 Feb 2026   Prob (F-statistic):           7.66e-07\nTime:                        10:19:28   Log-Likelihood:                 131.96\nNo. Observations:                 150   AIC:                            -259.9\nDf Residuals:                     148   BIC:                            -253.9\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.0075      0.008     -0.895      0.372      -0.024       0.009\nmkt_excess     0.7503      0.145      5.164      0.000       0.463       1.037\n==============================================================================\nOmnibus:                       39.111   Durbin-Watson:                   2.039\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              107.620\nSkew:                          -1.015   Prob(JB):                     4.27e-24\nKurtosis:                       6.619   Cond. No.                         17.6\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression output provides several important pieces of information:\n\nBeta (mkt_excess coefficient): The estimated market sensitivity. A beta above 1 indicates VIC amplifies market movements.\nAlpha (Intercept): The abnormal return not explained by market exposure. Under CAPM, this should be zero.\nR-squared: The proportion of return variation explained by market movements.\nt-statistics: Test whether coefficients differ significantly from zero.\n\n\n# Extract key estimates\ncoefficients = model_vic.summary2().tables[1]\n\nprint(\"\\nKey estimates for Vingroup (VIC):\")\nprint(f\"  Beta:  {coefficients.loc['mkt_excess', 'Coef.']:.3f}\")\nprint(f\"  Alpha: {coefficients.loc['Intercept', 'Coef.']:.4f}\")\nprint(f\"  R²:    {model_vic.rsquared:.3f}\")\n\n\nKey estimates for Vingroup (VIC):\n  Beta:  0.750\n  Alpha: -0.0075\n  R²:    0.153\n\n\n\n\n9.4.2 CAPM Estimation Function\nWe create a reusable function that estimates the CAPM and returns results in a standardized format. The function includes a minimum observations requirement to avoid unreliable estimates from sparse data.\n\ndef estimate_capm(data, min_obs=48):\n    \"\"\"\n    Estimate CAPM regression and return coefficients.\n    \n    This function regresses excess stock returns on excess market returns\n    and extracts the coefficient estimates along with t-statistics.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        DataFrame with 'ret_excess' and 'mkt_excess' columns\n    min_obs : int\n        Minimum number of observations required for estimation\n        \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with coefficient estimates and t-statistics,\n        or empty DataFrame if insufficient observations\n    \"\"\"\n    if len(data) &lt; min_obs:\n        return pd.DataFrame()\n    \n    try:\n        # Estimate OLS regression\n        model = smf.ols(\n            formula=\"ret_excess ~ mkt_excess\", \n            data=data\n        ).fit()\n        \n        # Extract coefficient table\n        coef_table = model.summary2().tables[1]\n        \n        # Format results\n        results = pd.DataFrame({\n            \"coefficient\": [\"alpha\", \"beta\"],\n            \"estimate\": [\n                coef_table.loc[\"Intercept\", \"Coef.\"],\n                coef_table.loc[\"mkt_excess\", \"Coef.\"]\n            ],\n            \"t_statistic\": [\n                coef_table.loc[\"Intercept\", \"t\"],\n                coef_table.loc[\"mkt_excess\", \"t\"]\n            ],\n            \"r_squared\": model.rsquared\n        })\n        \n        return results\n        \n    except Exception as e:\n        # Return empty DataFrame if estimation fails\n        return pd.DataFrame()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#rolling-window-estimation",
    "href": "08_beta_estimation.html#rolling-window-estimation",
    "title": "9  Beta Estimation",
    "section": "9.5 Rolling-Window Estimation",
    "text": "9.5 Rolling-Window Estimation\n\n9.5.1 Motivation for Rolling Windows\nStock betas are not constant over time. A company’s business mix, leverage, and operating environment evolve, causing its systematic risk exposure to change. To capture this time variation, we use rolling-window estimation: at each point in time, we estimate beta using only data from a fixed lookback period (e.g., the past 60 months).\nRolling-window estimation involves a trade-off:\n\nLonger windows provide more observations and thus more precise estimates, but may include stale information.\nShorter windows are more responsive to changes but produce noisier estimates.\n\nA common choice in academic research is 60 months (5 years) of monthly data, requiring at least 48 valid observations for estimation.\n\n\n9.5.2 Rolling Window Implementation\nThe following function implements rolling-window CAPM estimation. For each month in the sample, it looks back over the specified window and estimates beta using all available data within that window.\n\ndef roll_capm_estimation(data, look_back=60, min_obs=48):\n    \"\"\"\n    Perform rolling-window CAPM estimation.\n    \n    This function slides a window across time, estimating the CAPM\n    regression at each point using the most recent 'look_back' months\n    of data.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        DataFrame with 'date', 'ret_excess', and 'mkt_excess' columns\n    look_back : int\n        Number of months in the estimation window\n    min_obs : int\n        Minimum observations required within each window\n        \n    Returns\n    -------\n    pd.DataFrame\n        Time series of coefficient estimates with dates\n    \"\"\"\n    # Ensure data is sorted by date\n    data = data.sort_values(\"date\").copy()\n    \n    # Get unique dates\n    dates = data[\"date\"].drop_duplicates().sort_values()\n    \n    # Container for results\n    results = []\n    \n    # Slide window across dates\n    for i in range(look_back - 1, len(dates)):\n        # Define window boundaries\n        end_date = dates.iloc[i]\n        start_date = end_date - relativedelta(months=look_back - 1)\n        \n        # Extract data within window\n        window_data = data.query(\"date &gt;= @start_date and date &lt;= @end_date\")\n        \n        # Estimate CAPM for this window\n        window_results = estimate_capm(window_data, min_obs=min_obs)\n        \n        if not window_results.empty:\n            window_results[\"date\"] = end_date\n            results.append(window_results)\n    \n    # Combine all results\n    if results:\n        return pd.concat(results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\n\n9.5.3 Example: Rolling Betas for Selected Stocks\nWe demonstrate rolling-window estimation for several well-known Vietnamese stocks spanning different industries.\n\n# Define example stocks\nexamples = pd.DataFrame({\n    \"symbol\": [\"FPT\", \"VNM\", \"VIC\", \"HPG\", \"VCB\"],\n    \"company\": [\n        \"FPT Corporation\",      # Technology\n        \"Vinamilk\",             # Consumer goods\n        \"Vingroup\",             # Real estate/conglomerate\n        \"Hoa Phat Group\",       # Steel/materials\n        \"Vietcombank\"           # Banking\n    ]\n})\n\n# Check data availability for each example\ndata_availability = (prices_monthly\n    .query(\"symbol in @examples['symbol']\")\n    .groupby(\"symbol\")\n    .agg(\n        n_obs=(\"date\", \"count\"),\n        first_date=(\"date\", \"min\"),\n        last_date=(\"date\", \"max\")\n    )\n    .reset_index()\n)\n\nprint(\"Data availability for example stocks:\")\nprint(data_availability)\n\nData availability for example stocks:\n  symbol  n_obs first_date  last_date\n0    FPT    150 2011-07-31 2023-12-31\n1    HPG    150 2011-07-31 2023-12-31\n2    VCB    150 2011-07-31 2023-12-31\n3    VIC    150 2011-07-31 2023-12-31\n4    VNM    150 2011-07-31 2023-12-31\n\n\n\n# Estimate rolling betas for example stocks\nexample_data = prices_monthly.query(\"symbol in @examples['symbol']\")\n\ncapm_examples = (example_data\n    .groupby(\"symbol\", group_keys=True)\n    .apply(lambda x: roll_capm_estimation(x), include_groups=False)\n    .reset_index()\n    .drop(columns=\"level_1\", errors=\"ignore\")\n)\n\n# Filter to beta estimates only\nbeta_examples = (capm_examples\n    .query(\"coefficient == 'beta'\")\n    .merge(examples, on=\"symbol\")\n)\n\nprint(f\"Rolling beta estimates: {len(beta_examples):,} observations\")\n\nRolling beta estimates: 455 observations\n\n\n\n\n9.5.4 Visualizing Rolling Betas\nFigure 9.1 displays the time series of beta estimates for our example stocks. The figure reveals how systematic risk exposure evolves differently across industries.\n\nrolling_beta_figure = (\n    ggplot(\n        beta_examples,\n        aes(x=\"date\", y=\"estimate\", color=\"company\")\n    )\n    + geom_line(size=0.8)\n    + geom_hline(yintercept=1, linetype=\"dashed\", color=\"gray\", alpha=0.7)\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        color=\"\",\n        title=\"Rolling Beta Estimates (60-Month Window)\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n    + theme(legend_position=\"bottom\")\n)\nrolling_beta_figure.show()\n\n\n\n\n\n\n\nFigure 9.1: Monthly rolling beta estimates for selected Vietnamese stocks using a 60-month estimation window. Different industries exhibit distinct patterns of market sensitivity over time.\n\n\n\n\n\nSeveral patterns emerge from the figure:\n\nIndustry differences: Technology and banking stocks may exhibit different beta patterns than real estate or consumer goods companies.\nTime variation: Betas are not constant. They respond to changes in business conditions, leverage, and market regimes.\nCrisis periods: Market stress periods (e.g., 2008 financial crisis, 2020 COVID-19) often see beta estimates change as correlations across stocks increase.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#parallelized-estimation-for-the-full-market",
    "href": "08_beta_estimation.html#parallelized-estimation-for-the-full-market",
    "title": "9  Beta Estimation",
    "section": "9.6 Parallelized Estimation for the Full Market",
    "text": "9.6 Parallelized Estimation for the Full Market\n\n9.6.1 The Computational Challenge\nEstimating rolling betas for all stocks in our database is computationally intensive. With hundreds of stocks, each requiring rolling estimation across many time periods, sequential processing would take considerable time. Fortunately, beta estimation for different stocks is independent (i.e., the estimate for stock A does not depend on the estimate for stock B). This independence makes the problem ideal for parallelization.\n\n\n9.6.2 Setting Up Parallel Processing\nWe use the joblib library to distribute computation across multiple CPU cores. The Parallel class manages worker processes, while delayed wraps function calls for deferred execution.\n\n# Determine available cores (reserve one for system operations)\nn_cores = max(1, cpu_count() - 1)\nprint(f\"Available cores for parallel processing: {n_cores}\")\n\nAvailable cores for parallel processing: 3\n\n\n\n\n9.6.3 Parallel Beta Estimation\nThe following code estimates rolling betas for all stocks in parallel. Each stock is processed independently by a separate worker.\n\ndef estimate_all_betas_parallel(data, n_cores, look_back=60, min_obs=48):\n    \"\"\"\n    Estimate rolling betas for all stocks using parallel processing.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Full dataset with all stocks\n    n_cores : int\n        Number of CPU cores to use\n    look_back : int\n        Months in estimation window\n    min_obs : int\n        Minimum observations required\n        \n    Returns\n    -------\n    pd.DataFrame\n        Beta estimates for all stocks and dates\n    \"\"\"\n    # Group data by stock\n    grouped = data.groupby(\"symbol\", group_keys=False)\n    \n    # Define worker function\n    def process_stock(name, group):\n        result = roll_capm_estimation(group, look_back=look_back, min_obs=min_obs)\n        if not result.empty:\n            result[\"symbol\"] = name\n        return result\n    \n    # Execute in parallel\n    results = Parallel(n_jobs=n_cores, verbose=1)(\n        delayed(process_stock)(name, group) \n        for name, group in grouped\n    )\n    \n    # Combine results\n    results = [r for r in results if not r.empty]\n    if results:\n        return pd.concat(results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\n# Estimate betas for all stocks\nprint(\"Estimating rolling betas for all stocks...\")\ncapm_monthly = estimate_all_betas_parallel(\n    prices_monthly, \n    n_cores=n_cores,\n    look_back=60,\n    min_obs=48\n)\n\nprint(f\"\\nCompleted: {len(capm_monthly):,} coefficient estimates\")\nprint(f\"Unique stocks: {capm_monthly['symbol'].nunique():,}\")\n\n\n\n9.6.4 Storing Results\nWe save the CAPM estimates to our database for use in subsequent chapters.\n\ncapm_monthly.to_sql(\n    name=\"capm_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"CAPM estimates saved to database.\")\n\nFor subsequent analysis, we load the pre-computed estimates:\n\ncapm_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM capm_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Loaded {len(capm_monthly):,} CAPM estimates\")\n\nLoaded 161,580 CAPM estimates",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#beta-estimation-using-daily-returns",
    "href": "08_beta_estimation.html#beta-estimation-using-daily-returns",
    "title": "9  Beta Estimation",
    "section": "9.7 Beta Estimation Using Daily Returns",
    "text": "9.7 Beta Estimation Using Daily Returns\nWhile monthly returns are standard in academic research, some applications benefit from higher-frequency data:\n\nShorter estimation windows: Daily data allows meaningful estimation over shorter periods (e.g., 3 months rather than 5 years).\nMore responsive estimates: Daily betas capture changes more quickly.\nEvent studies: High-frequency betas are useful for analyzing market reactions to specific events.\n\nHowever, daily data introduces additional challenges:\n\nMicrostructure noise: Bid-ask bounce and other trading frictions add noise to returns.\nNon-synchronous trading: Less liquid stocks may not trade every day, biasing beta estimates downward.\nComputational burden: Daily data is roughly 21 times larger than monthly data.\n\n\n9.7.1 Batch Processing for Daily Data\nGiven the size of daily data, we process stocks in batches to manage memory constraints. This approach loads and processes a subset of stocks, saves results, and proceeds to the next batch.\n\ndef compute_market_return_daily(tidy_finance):\n    \"\"\"\n    Compute daily value-weighted market excess return from stock data.\n    \"\"\"\n    # Load daily prices with market cap for weighting\n    prices_daily_full = pd.read_sql_query(\n        sql=\"\"\"\n            SELECT p.symbol, p.date, p.ret_excess, m.mktcap_lag\n            FROM prices_daily p\n            LEFT JOIN prices_monthly m ON p.symbol = m.symbol \n                AND strftime('%Y-%m', p.date) = strftime('%Y-%m', m.date)\n        \"\"\",\n        con=tidy_finance,\n        parse_dates={\"date\"}\n    )\n    \n    # Compute value-weighted market return each day\n    mkt_daily = (prices_daily_full\n        .dropna(subset=[\"ret_excess\", \"mktcap_lag\"])\n        .groupby(\"date\")\n        .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n        .reset_index(name=\"mkt_excess\")\n    )\n    \n    return mkt_daily\n\n\ndef roll_capm_estimation_daily(data, look_back_days=1260, min_obs=1000):\n    \"\"\"\n    Perform rolling-window CAPM estimation using daily data.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        DataFrame with 'date', 'ret_excess', and 'mkt_excess' columns\n    look_back_days : int\n        Number of trading days in the estimation window\n    min_obs : int\n        Minimum daily observations required within each window\n        \n    Returns\n    -------\n    pd.DataFrame\n        Time series of coefficient estimates with dates\n    \"\"\"\n    data = data.sort_values(\"date\").copy()\n    dates = data[\"date\"].drop_duplicates().sort_values().reset_index(drop=True)\n    \n    results = []\n    \n    for i in range(look_back_days - 1, len(dates)):\n        end_date = dates.iloc[i]\n        start_idx = max(0, i - look_back_days + 1)\n        start_date = dates.iloc[start_idx]\n        \n        window_data = data.query(\"date &gt;= @start_date and date &lt;= @end_date\")\n        window_results = estimate_capm(window_data, min_obs=min_obs)\n        \n        if not window_results.empty:\n            window_results[\"date\"] = end_date\n            results.append(window_results)\n    \n    if results:\n        return pd.concat(results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\ndef estimate_daily_betas_batch(symbols, tidy_finance, n_cores, batch_size=500, \n                                look_back_days=1260, min_obs=1000):\n    \"\"\"\n    Estimate rolling betas from daily data using batch processing.\n    \"\"\"\n    # First, compute or load market return\n    print(\"Computing daily market excess returns...\")\n    mkt_daily = compute_market_return_daily(tidy_finance)\n    print(f\"Market returns: {len(mkt_daily)} days\")\n    \n    n_batches = int(np.ceil(len(symbols) / batch_size))\n    all_results = []\n    \n    for j in range(n_batches):\n        batch_start = j * batch_size\n        batch_end = min((j + 1) * batch_size, len(symbols))\n        batch_symbols = symbols[batch_start:batch_end]\n        \n        symbol_list = \", \".join(f\"'{s}'\" for s in batch_symbols)\n        \n        query = f\"\"\"\n            SELECT symbol, date, ret_excess\n            FROM prices_daily\n            WHERE symbol IN ({symbol_list})\n        \"\"\"\n        \n        prices_daily_batch = pd.read_sql_query(\n            sql=query,\n            con=tidy_finance,\n            parse_dates={\"date\"}\n        )\n        \n        # Merge with market excess return\n        prices_daily_batch = prices_daily_batch.merge(\n            mkt_daily, \n            on=\"date\", \n            how=\"inner\"\n        )\n        \n        # Group by symbol and estimate betas\n        grouped = prices_daily_batch.groupby(\"symbol\", group_keys=False)\n        \n        # Parallel estimation\n        batch_results = Parallel(n_jobs=n_cores)(\n            delayed(lambda name, group: \n                roll_capm_estimation_daily(group, look_back_days=look_back_days, min_obs=min_obs)\n                .assign(symbol=name)\n            )(name, group)\n            for name, group in grouped\n        )\n        \n        batch_results = [r for r in batch_results if r is not None and not r.empty]\n        \n        if batch_results:\n            all_results.append(pd.concat(batch_results, ignore_index=True))\n        \n        print(f\"Batch {j+1}/{n_batches} complete\")\n    \n    if all_results:\n        return pd.concat(all_results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\nsymbols = prices_monthly[\"symbol\"].unique().tolist()\n\ncapm_daily = estimate_daily_betas_batch(\n    symbols=symbols,\n    tidy_finance=tidy_finance,\n    n_cores=n_cores,\n    batch_size=500,\n    look_back_days=1260,  # ~5 years of trading days\n    min_obs=1000\n)\n\nprint(f\"Daily beta estimates: {len(capm_daily):,}\")\n\n\ncapm_daily.to_sql(\n    name=\"capm_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"CAPM estimates saved to database.\")\n\nFor subsequent analysis, we load the pre-computed estimates:\n\ncapm_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM capm_daily\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Loaded {len(capm_daily):,} CAPM estimates\")\n\nLoaded 3,394,490 CAPM estimates",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#analyzing-beta-estimates",
    "href": "08_beta_estimation.html#analyzing-beta-estimates",
    "title": "9  Beta Estimation",
    "section": "9.8 Analyzing Beta Estimates",
    "text": "9.8 Analyzing Beta Estimates\n\n9.8.1 Extracting Beta Estimates\nWe extract the beta coefficient estimates from our CAPM results for analysis.\n\n# Extract monthly betas\nbeta_monthly = (capm_monthly\n    .query(\"coefficient == 'beta'\")\n    .rename(columns={\"estimate\": \"beta\"})\n    [[\"symbol\", \"date\", \"beta\"]]\n    .assign(frequency=\"monthly\")\n)\n\n# Save to database\nbeta_monthly.to_sql(\n    name=\"beta_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(f\"Monthly betas: {len(beta_monthly):,} observations\")\nprint(f\"Unique stocks: {beta_monthly['symbol'].nunique():,}\")\n\nMonthly betas: 80,790 observations\nUnique stocks: 1,383\n\n\n\n# Load pre-computed betas\nbeta_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM beta_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n\n\n9.8.2 Summary Statistics\nWe examine the distribution of beta estimates to verify their reasonableness.\n\nprint(\"Beta Summary Statistics:\")\nprint(beta_monthly[\"beta\"].describe().round(3))\n\n# Additional diagnostics\nprint(f\"\\nStocks with negative average beta: {(beta_monthly.groupby('symbol')['beta'].mean() &lt; 0).sum()}\")\nprint(f\"Stocks with beta &gt; 2: {(beta_monthly.groupby('symbol')['beta'].mean() &gt; 2).sum()}\")\n\nBeta Summary Statistics:\ncount    80790.000\nmean         0.501\nstd          0.539\nmin         -1.345\n25%          0.130\n50%          0.447\n75%          0.832\nmax          2.678\nName: beta, dtype: float64\n\nStocks with negative average beta: 177\nStocks with beta &gt; 2: 5\n\n\n\n\n9.8.3 Beta Distribution Across Industries\nDifferent industries have different exposures to systematic market risk based on their business models, operating leverage, and financial leverage. Figure 9.2 shows the distribution of firm-level average betas across Vietnamese industries.\n\n# Merge betas with industry information\nbeta_with_industry = (beta_monthly\n    .merge(\n        prices_monthly[[\"symbol\", \"date\", \"icb_name_vi\"]].drop_duplicates(),\n        on=[\"symbol\", \"date\"],\n        how=\"left\"\n    )\n    .dropna(subset=[\"icb_name_vi\"])\n)\n\n# Compute firm-level average beta by industry\nbeta_by_industry = (beta_with_industry\n    .groupby([\"icb_name_vi\", \"symbol\"])[\"beta\"]\n    .mean()\n    .reset_index()\n)\n\n# Order industries by median beta\nindustry_order = (beta_by_industry\n    .groupby(\"icb_name_vi\")[\"beta\"]\n    .median()\n    .sort_values()\n    .index.tolist()\n)\n\n# Select top 10 industries by number of firms for clearer visualization\ntop_industries = (beta_by_industry\n    .groupby(\"icb_name_vi\")\n    .size()\n    .nlargest(10)\n    .index.tolist()\n)\n\nbeta_by_industry_filtered = beta_by_industry.query(\"icb_name_vi in @top_industries\")\n\n\nbeta_industry_figure = (\n    ggplot(\n        beta_by_industry_filtered,\n        aes(x=\"icb_name_vi\", y=\"beta\")\n    )\n    + geom_boxplot(fill=\"steelblue\", alpha=0.7)\n    + geom_hline(yintercept=1, linetype=\"dashed\", color=\"red\", alpha=0.7)\n    + coord_flip()\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        title=\"Beta Distribution by Industry\"\n    )\n    + theme_minimal()\n)\nbeta_industry_figure.show()\n\n\n\n\n\n\n\nFigure 9.2: Distribution of firm-level average betas across Vietnamese industries. Box plots show the median, interquartile range, and outliers for each industry.\n\n\n\n\n\n\n\n9.8.4 Time Variation in Cross-Sectional Beta Distribution\nBetas vary not only across stocks but also over time. Figure 9.3 shows how the cross-sectional distribution of betas has evolved in the Vietnamese market.\n\n# Compute monthly quantiles\nbeta_quantiles = (beta_monthly\n    .groupby(\"date\")[\"beta\"]\n    .quantile(q=np.arange(0.1, 1.0, 0.1))\n    .reset_index()\n    .rename(columns={\"level_1\": \"quantile\"})\n    .assign(quantile=lambda x: (x[\"quantile\"] * 100).astype(int).astype(str) + \"%\")\n)\n\nbeta_quantiles_figure = (\n    ggplot(\n        beta_quantiles,\n        aes(x=\"date\", y=\"beta\", color=\"quantile\")\n    )\n    + geom_line(alpha=0.8)\n    + geom_hline(yintercept=1, linetype=\"dashed\", color=\"gray\")\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        color=\"Quantile\",\n        title=\"Cross-Sectional Distribution of Betas Over Time\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n)\nbeta_quantiles_figure.show()\n\n\n\n\n\n\n\nFigure 9.3: Monthly quantiles of beta estimates over time. Each line represents a decile of the cross-sectional beta distribution.\n\n\n\n\n\nThe figure reveals several interesting patterns:\n\nLevel shifts: The entire distribution of betas can shift over time, reflecting changes in market-wide correlation.\nDispersion changes: During market stress, the spread between high and low beta stocks may change as correlations move.\nTrends: Some periods show trending behavior in betas, possibly reflecting structural changes in the economy.\n\n\n\n9.8.5 Coverage Analysis\nWe verify that our estimation procedure produces reasonable coverage across the sample. Figure 9.4 shows the fraction of stocks with available beta estimates over time.\n\n# Count stocks with and without betas\ncoverage = (prices_monthly\n    .groupby(\"date\")[\"symbol\"]\n    .nunique()\n    .reset_index(name=\"total_stocks\")\n    .merge(\n        beta_monthly.groupby(\"date\")[\"symbol\"].nunique().reset_index(name=\"with_beta\"),\n        on=\"date\",\n        how=\"left\"\n    )\n    .fillna(0)\n    .assign(coverage=lambda x: x[\"with_beta\"] / x[\"total_stocks\"])\n)\n\ncoverage_figure = (\n    ggplot(coverage, aes(x=\"date\", y=\"coverage\"))\n    + geom_line(color=\"steelblue\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Share with Beta Estimate\",\n        title=\"Beta Estimation Coverage Over Time\"\n    )\n    + scale_y_continuous(labels=percent_format(), limits=(0, 1))\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n)\ncoverage_figure.show()\n\n\n\n\n\n\n\nFigure 9.4: Share of stocks with available beta estimates over time. Coverage increases as more stocks accumulate sufficient return history.\n\n\n\n\n\nCoverage is lower in early years because stocks need sufficient return history (at least 48 months) before their betas can be estimated. As the market matures and stocks accumulate longer histories, coverage approaches 100%.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#comparing-monthly-and-daily-beta-estimates",
    "href": "08_beta_estimation.html#comparing-monthly-and-daily-beta-estimates",
    "title": "9  Beta Estimation",
    "section": "9.9 Comparing Monthly and Daily Beta Estimates",
    "text": "9.9 Comparing Monthly and Daily Beta Estimates\nWhen both monthly and daily beta estimates are available, we can compare them to understand how estimation frequency affects results.\n\n# Combine monthly and daily estimates\nbeta_daily = (capm_daily\n    .query(\"coefficient == 'beta'\")\n    .rename(columns={\"estimate\": \"beta\"})\n    [[\"symbol\", \"date\", \"beta\"]]\n    .assign(frequency=\"daily\")\n)\n\nbeta_combined = pd.concat([beta_monthly, beta_daily], ignore_index=True)\n\n\n# Filter to example stocks\nbeta_comparison = (beta_combined\n    .merge(examples, on=\"symbol\")\n    .query(\"symbol in ['VIC', 'FPT']\")  # Select two for clarity\n)\n\ncomparison_figure = (\n    ggplot(\n        beta_comparison,\n        aes(x=\"date\", y=\"beta\", color=\"frequency\", linetype=\"frequency\")\n    )\n    + geom_line(size=0.8)\n    + facet_wrap(\"~company\", ncol=1)\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        color=\"Data Frequency\",\n        linetype=\"Data Frequency\",\n        title=\"Monthly vs Daily Beta Estimates\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n    + theme(legend_position=\"bottom\")\n)\ncomparison_figure.show()\n\n\n\n\n\n\n\nFigure 9.5: Comparison of beta estimates using monthly versus daily returns for selected stocks. Daily estimates are smoother due to more observations per estimation window.\n\n\n\n\n\nThe comparison reveals that daily-based estimates are generally smoother due to the larger number of observations in each window. However, the level and trend of estimates are similar across frequencies, providing validation that both approaches capture the same underlying systematic risk exposure.\n\n# Correlation between monthly and daily estimates\ncorrelation_data = (beta_combined\n    .pivot_table(index=[\"symbol\", \"date\"], columns=\"frequency\", values=\"beta\")\n    .dropna()\n)\n\nprint(f\"Correlation between monthly and daily betas: {correlation_data.corr().iloc[0,1]:.3f}\")\n\nCorrelation between monthly and daily betas: 0.745\n\n\n\n\n\nTable 9.1: Theoretical Reasons for Imperfect Correlation\n\n\n\n\n\n\n\n\n\nFactor\nEffect\n\n\n\n\nNon-synchronous trading\nDaily betas can be biased downward for illiquid stocks\n\n\nMicrostructure noise\nBid-ask bounce adds noise to daily estimates\n\n\nDifferent effective windows\nSame calendar period but ~20x more observations for daily\n\n\nMean reversion speed\nDaily captures faster-moving risk dynamics\n\n\n\n\n\n\nTable 9.1 shows several reasons why we might observe imperfect correlation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#key-takeaways",
    "href": "08_beta_estimation.html#key-takeaways",
    "title": "9  Beta Estimation",
    "section": "9.10 Key Takeaways",
    "text": "9.10 Key Takeaways\n\nCAPM beta measures a stock’s sensitivity to systematic market risk and is fundamental to modern portfolio theory, cost of capital estimation, and risk management.\nRolling-window estimation captures time variation in betas, which reflects changes in companies’ business models, leverage, and market conditions.\nParallelization dramatically reduces computation time for large-scale estimation tasks by distributing work across multiple CPU cores.\nEstimation choices matter: Window length, return frequency, and minimum observation requirements all affect beta estimates. Researchers should choose parameters appropriate for their specific application.\nIndustry patterns: Vietnamese stocks show systematic differences in market sensitivity across industries, with cyclical sectors exhibiting higher betas than defensive sectors.\nTime variation: The cross-sectional distribution of betas in Vietnam has evolved over time, with notable shifts during market stress periods.\nFrequency comparison: Monthly and daily beta estimates are positively correlated but not identical. Daily estimates are smoother while monthly estimates may better capture lower-frequency variation.\nData quality checks: Coverage analysis and summary statistics help identify potential issues in estimation procedures before using results in downstream analyses.\n\n\n\n\n\n\n\nLintner, John. 1965. “Security prices, risk, and maximal gains from diversification.” The Journal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital asset market.” Econometrica 34 (4): 768–83. https://doi.org/10.2307/1910098.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html",
    "href": "09_univariate_portfolio_sort.html",
    "title": "10  Univariate Portfolio Sorts",
    "section": "",
    "text": "10.1 Data Preparation\nIn this chapter, we dive into portfolio sorts, one of the most widely used statistical methodologies in empirical asset pricing (e.g., Bali, Engle, and Murray 2016). The key application of portfolio sorts is to examine whether one or more variables can predict future excess returns. In general, the idea is to sort individual stocks into portfolios, where the stocks within each portfolio are similar with respect to a sorting variable, such as firm size. The different portfolios then represent well-diversified investments that differ in the level of the sorting variable. You can then attribute the differences in the return distribution to the impact of the sorting variable. We start by introducing univariate portfolio sorts (which sort based on only one characteristic) and tackle bivariate sorting in Value and Bivariate Sorts.\nA univariate portfolio sort considers only one sorting variable \\(x_{i,t-1}\\). Here, \\(i\\) denotes the stock and \\(t-1\\) indicates that the characteristic is observable by investors at time \\(t\\). The objective is to assess the cross-sectional relation between \\(x_{i,t-1}\\) and, typically, stock excess returns \\(r_{i,t}\\) at time \\(t\\) as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas from the previous chapter as our sorting variable.\nWe start with loading the required data from our SQLite database introduced in Accessing and Managing Financial Data and DataCore Data. In particular, we use the monthly stock price data as our asset universe. Once we form our portfolios, we use the market factor returns to compute the risk-adjusted performance (i.e., alpha). beta is the dataframe with market betas computed in the previous chapter.\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nprices_monthly = (pd.read_sql_query(\n    sql=\"SELECT symbol, date, ret_excess, mktcap_lag FROM prices_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nbeta = pd.read_sql_query(\n  sql=\"SELECT symbol, date, beta FROM beta_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#sorting-by-market-beta",
    "href": "09_univariate_portfolio_sort.html#sorting-by-market-beta",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.2 Sorting by Market Beta",
    "text": "10.2 Sorting by Market Beta\nNext, we merge our sorting variable with the return data. We use the one-month lagged betas as a sorting variable to ensure that the sorts rely only on information available when we create the portfolios. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month \\(t\\) information is available in month \\(t+1\\). You may be tempted to simply use a call such as prices_monthly['beta_lag'] = prices_monthly.groupby('symbol')['beta'].shift(1) instead. This procedure, however, does not work correctly if there are implicit missing values in the time series.\n\nbeta_lag = (beta\n  .assign(date=lambda x: x[\"date\"]+pd.DateOffset(months=1))\n  .get([\"symbol\", \"date\", \"beta\"])\n  .rename(columns={\"beta\": \"beta_lag\"})\n  .dropna()\n)\n\ndata_for_sorts = (prices_monthly\n  .merge(beta_lag, how=\"inner\", on=[\"symbol\", \"date\"])\n  .dropna()\n)\n\nThe first step to conduct portfolio sorts is to calculate periodic breakpoints that you can use to group the stocks into portfolios. For simplicity, we start with the median lagged market beta as the single breakpoint. We then compute the value-weighted returns for each of the two resulting portfolios, which means that the lagged market capitalization determines the weight in np.average().\n\nbeta_portfolios = (\n    data_for_sorts\n    .groupby(\"date\")\n    .apply(lambda x: (\n        x.assign(\n            portfolio=pd.qcut(x[\"beta_lag\"], q=[0, 0.5, 1], labels=[\"low\", \"high\"]),\n            date=x.name\n        )\n    ))\n    .reset_index(drop=True)\n    .groupby([\"portfolio\", \"date\"])\n    .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n    .reset_index(name=\"ret\")\n    .dropna(subset=['ret'])\n)\nbeta_portfolios.head()\n\n\n\n\n\n\n\n\nportfolio\ndate\nret\n\n\n\n\n0\nlow\n2016-08-31\n-0.016507\n\n\n1\nlow\n2016-09-30\n-0.089155\n\n\n2\nlow\n2016-11-30\n-0.015080\n\n\n3\nlow\n2017-01-31\n0.004113\n\n\n4\nlow\n2017-02-28\n0.035101",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#performance-evaluation",
    "href": "09_univariate_portfolio_sort.html#performance-evaluation",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.3 Performance Evaluation",
    "text": "10.3 Performance Evaluation\nWe can construct a long-short strategy based on the two portfolios: buy the high-beta portfolio and, at the same time, short the low-beta portfolio. Thereby, the overall position in the market is net-zero.\n\nbeta_longshort = (beta_portfolios\n  .pivot_table(index=\"date\", columns=\"portfolio\", values=\"ret\")\n  .reset_index()\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n)\n\nWe compute the average return and the corresponding standard error to test whether the long-short portfolio yields on average positive or negative excess returns. In the asset pricing literature, one typically adjusts for autocorrelation by using Newey and West (1987) \\(t\\)-statistics to test the null hypothesis that average portfolio excess returns are equal to zero. One necessary input for Newey-West standard errors is a chosen bandwidth based on the number of lags employed for the estimation. Researchers often default to choosing a pre-specified lag length of six months (which is not a data-driven approach). We do so in the fit() function by indicating the cov_type as HAC and providing the maximum lag length through an additional keywords dictionary.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\",\n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept     0.006       0.006        1.018    0.309\n\nSummary statistics:\n- Number of observations: 53\n- R-squared: 0.000, Adjusted R-squared: 0.000\n- F-statistic not available\n\n\n\nThe results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM predicts that high-beta stocks should yield higher expected returns. Our portfolio sort implicitly mimics an investment strategy that finances high-beta stocks by shorting low-beta stocks. Therefore, one should expect that the average excess returns yield a return that is above the risk-free rate.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#functional-programming-for-portfolio-sorts",
    "href": "09_univariate_portfolio_sort.html#functional-programming-for-portfolio-sorts",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.4 Functional Programming for Portfolio Sorts",
    "text": "10.4 Functional Programming for Portfolio Sorts\nNow, we take portfolio sorts to the next level. We want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming is very handy: we define a function that gives us flexibility concerning which variable to use for the sorting, denoted by sorting_variable. We use np.quantile() to compute breakpoints for n_portfolios. Then, we assign portfolios to stocks using the pd.cut() function. The output of the following function is a new column that contains the number of the portfolio to which a stock belongs.\nIn some applications, the variable used for the sorting might be clustered (e.g., at a lower bound of 0). Then, multiple breakpoints may be identical, leading to empty portfolios. Similarly, some portfolios might have a very small number of stocks at the beginning of the sample. Cases where the number of portfolio constituents differs substantially due to the distribution of the characteristics require careful consideration and, depending on the application, might require customized sorting approaches.\n\ndef assign_portfolio(data, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolios to a bin between breakpoints.\"\"\"\n    \n    breakpoints = np.quantile(\n      data[sorting_variable].dropna(), \n      np.linspace(0, 1, n_portfolios + 1), \n      method=\"linear\"\n    )\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nWe can use the above function to sort stocks into ten portfolios each month using lagged betas and compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.\n\nbeta_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio=assign_portfolio(x, \"beta_lag\", 10)\n    ), include_groups=False\n  )\n  .reset_index(level=\"date\")\n  .groupby([\"portfolio\", \"date\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    }), include_groups=False\n  )\n  .reset_index()\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#more-performance-evaluation",
    "href": "09_univariate_portfolio_sort.html#more-performance-evaluation",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.5 More Performance Evaluation",
    "text": "10.5 More Performance Evaluation\nIn the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns.\n\nbeta_portfolios_summary = (beta_portfolios\n  .groupby(\"portfolio\")\n  .apply(lambda x: pd.Series({\n      \"alpha\": sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[\"Intercept\"],\n      \"beta\": sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[\"mkt_excess\"],\n      \"ret\": x[\"ret\"].mean()\n    }), include_groups=False\n  )\n  .reset_index()\n)\nbeta_portfolios_summary\n\n\n\n\n\n\n\n\nportfolio\nalpha\nbeta\nret\n\n\n\n\n0\n1\n0.007031\n0.356225\n0.003892\n\n\n1\n2\n-0.004215\n0.227882\n-0.005509\n\n\n2\n3\n-0.010169\n0.390216\n-0.011241\n\n\n3\n4\n-0.014205\n0.388342\n-0.015732\n\n\n4\n5\n-0.007220\n0.616833\n-0.009723\n\n\n5\n6\n0.010648\n0.976502\n0.006577\n\n\n6\n7\n-0.010739\n0.679145\n-0.012645\n\n\n7\n8\n-0.002757\n0.991774\n-0.006963\n\n\n8\n9\n-0.003448\n0.904886\n-0.007174\n\n\n9\n10\n0.010675\n1.376356\n0.004944\n\n\n\n\n\n\n\nFigure 10.1 illustrates the CAPM alphas of beta-sorted portfolios.\n\nbeta_portfolios_figure = (\n  ggplot(\n    beta_portfolios_summary, \n    aes(x=\"portfolio\", y=\"alpha\", fill=\"portfolio\")\n  )\n  + geom_bar(stat=\"identity\")\n  + labs(\n      x=\"Portfolio\", y=\"CAPM alpha\", fill=\"Portfolio\",\n      title=\"CAPM alphas of beta-sorted portfolios\"\n    )\n  + scale_y_continuous(labels=percent_format())\n  + theme(legend_position=\"none\")\n)\nbeta_portfolios_figure.show()\n\n\n\n\n\n\n\nFigure 10.1: The figure shows CAPM alphas of beta-sorted portfolios. Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the sample period.\n\n\n\n\n\nUnlike the well-documented “betting against beta” anomaly in US markets, where low-beta portfolios exhibit positive alphas and high-beta portfolios exhibit negative alphas in a monotonic pattern, the Vietnamese market shows no clear relationship between beta and risk-adjusted returns. The alphas fluctuate without a discernible trend across deciles. This lack of pattern likely reflects the limited sample period rather than a definitive conclusion about beta pricing in Vietnam. With such a short time series, the portfolio-level CAPM regressions contain substantial estimation noise, making it difficult to detect subtle anomalies. Longer sample periods would be needed to draw reliable conclusions about whether the low-beta anomaly exists in the Vietnamese equity market.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#security-market-line-and-beta-portfolios",
    "href": "09_univariate_portfolio_sort.html#security-market-line-and-beta-portfolios",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.6 Security Market Line and Beta Portfolios",
    "text": "10.6 Security Market Line and Beta Portfolios\nThe CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time. Figure 10.2 illustrates the security market line: We see that (not surprisingly) the high-beta portfolio returns have a high correlation with the market returns. However, it seems like the average excess returns for high-beta stocks are lower than what the security market line implies would be an “appropriate” compensation for the high market risk.\n\nsml_capm = (sm.OLS.from_formula(\n    formula=\"ret ~ 1 + beta\", \n    data=beta_portfolios_summary\n  )\n  .fit()\n  .params\n)\n\nsml_figure = (\n  ggplot(\n    beta_portfolios_summary,\n    aes(x=\"beta\", y=\"ret\", color=\"factor(portfolio)\")\n  )\n  + geom_point()\n  + geom_abline(\n      intercept=0, slope=factors_ff3_monthly[\"mkt_excess\"].mean(), linetype=\"solid\"\n    )\n  + geom_abline(\n      intercept=sml_capm[\"Intercept\"], slope=sml_capm[\"beta\"], linetype=\"dashed\"\n    )\n  + labs(\n      x=\"Beta\", y=\"Excess return\", color=\"Portfolio\",\n      title=\"Average portfolio excess returns and beta estimates\"\n    )\n  + scale_x_continuous(limits=(0, 2))\n  + scale_y_continuous(labels=percent_format())\n)\nsml_figure.show()\n\n\n\n\n\n\n\nFigure 10.2: The figure shows average portfolio excess returns and beta estimates. Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\n\nTo provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio.\n\nbeta_longshort = (beta_portfolios\n  .assign(\n    portfolio=lambda x: (\n      x[\"portfolio\"].apply(\n        lambda y: \"high\" if y == x[\"portfolio\"].max()\n        else (\"low\" if y == x[\"portfolio\"].min()\n        else y)\n      )\n    )\n  )\n  .query(\"portfolio in ['low', 'high']\")\n  .pivot_table(index=\"date\", columns=\"portfolio\", values=\"ret\")\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n)\n\nAgain, the resulting long-short strategy does not exhibit statistically significant returns.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept     0.001       0.011        0.095    0.924\n\nSummary statistics:\n- Number of observations: 53\n- R-squared: 0.000, Adjusted R-squared: 0.000\n- F-statistic not available\n\n\n\nHowever, controlling for the effect of beta, the long-short portfolio yields a CAPM-adjusted alpha. The results can provide evidence regarding the validity of the CAPM in the Vietnamese market. The betting-against-beta factor has been documented extensively in developed markets (Frazzini and Pedersen 2014). Betting-against-beta corresponds to a strategy that shorts high-beta stocks and takes a (levered) long position in low-beta stocks. If borrowing constraints prevent investors from taking positions on the security market line, they are instead incentivized to buy high-beta stocks, which leads to a relatively higher price (and therefore lower expected returns than implied by the CAPM) for such high-beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital-constrained investors with lower risk aversion.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1 + mkt_excess\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1 + mkt_excess\n\nCoefficients:\n            Estimate  Std. Error  t-Statistic  p-Value\nIntercept      0.004       0.009        0.394    0.694\nmkt_excess     1.020       0.116        8.784    0.000\n\nSummary statistics:\n- Number of observations: 52\n- R-squared: 0.527, Adjusted R-squared: 0.518\n- F-statistic: 77.156 on 1 and 50 DF, p-value: 0.000\n\n\n\nFigure 10.3 shows the annual returns of the extreme beta portfolios we are mainly interested in. The figure illustrates the patterns over the sample period; each portfolio exhibits periods with positive and negative annual returns.\n\nbeta_longshort_year = (beta_longshort\n  .assign(year=lambda x: x[\"date\"].dt.year)\n  .groupby(\"year\")\n  .aggregate(\n    low=(\"low\", lambda x: (1+x).prod()-1),\n    high=(\"high\", lambda x: (1+x).prod()-1),\n    long_short=(\"long_short\", lambda x: (1+x).prod()-1)\n  )\n  .reset_index()\n  .melt(id_vars=\"year\", var_name=\"name\", value_name=\"value\")\n)\n\nbeta_longshort_figure = (\n  ggplot(\n    beta_longshort_year, \n    aes(x=\"year\", y=\"value\", fill=\"name\")\n  )\n  + geom_col(position=\"dodge\")\n  + facet_wrap(\"~name\", ncol=1)\n  + labs(x=\"\", y=\"\", title=\"Annual returns of beta portfolios\")\n  + scale_y_continuous(labels=percent_format())\n  + theme(legend_position=\"none\")\n)\nbeta_longshort_figure.show()\n\n\n\n\n\n\n\nFigure 10.3: The figure shows annual returns of beta portfolios. We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.\n\n\n\n\n\nThe high-beta portfolio and low-beta portfolio both exhibit substantial year-to-year variation. The long-short portfolio, which goes long high-beta stocks and short low-beta stocks, shows no consistent pattern of positive returns. This erratic performance reinforces our earlier finding that the beta-return relationship in the Vietnamese market does not conform to theoretical CAPM predictions during our sample period. The high volatility of annual long-short returns highlights the substantial risk inherent in such a strategy, particularly in an emerging market context with a limited sample period.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#key-takeaways",
    "href": "09_univariate_portfolio_sort.html#key-takeaways",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.7 Key Takeaways",
    "text": "10.7 Key Takeaways\n\nUnivariate portfolio sorts assess whether a single firm characteristic, like lagged market beta, can predict future excess returns.\nPortfolios are formed each month using quantile breakpoints, with returns computed using value-weighted averages to reflect realistic investment strategies.\nA long-short strategy based on beta-sorted portfolios fails to generate significant positive excess returns in the Vietnamese market, contradicting CAPM predictions that higher beta should yield higher returns.\nThe analysis provides a framework for examining the “betting against beta” anomaly, where low-beta portfolios may deliver higher alphas than high-beta portfolios, offering evidence regarding the validity of the CAPM.\nThe functional programming capabilities of Python enable scalable and flexible portfolio sorting, making it easy to analyze multiple characteristics and portfolio configurations.\nEmerging markets like Vietnam may exhibit different beta-return relationships compared to developed markets, highlighting the importance of conducting market-specific empirical analysis rather than assuming universal applicability of asset pricing anomalies.\n\n\n\n\n\n\n\nBali, Turan G, Robert F Engle, and Scott Murray. 2016. Empirical asset pricing: The cross section of stock returns. John Wiley & Sons. https://doi.org/10.1002/9781118445112.stat07954.\n\n\nFrazzini, Andrea, and Lasse Heje Pedersen. 2014. “Betting against beta.” Journal of Financial Economics 111 (1): 1–25. https://doi.org/10.1016/j.jfineco.2013.10.005.\n\n\nNewey, Whitney K., and Kenneth D. West. 1987. “A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance Matrix.” Econometrica 55 (3): 703–8. http://www.jstor.org/stable/1913610.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html",
    "href": "12_fama_french.html",
    "title": "11  Fama-French Factors",
    "section": "",
    "text": "11.1 Theoretical Background\nThis chapter provides a replication of the Fama-French factor portfolios for the Vietnamese stock market. The Fama-French factor models represent a cornerstone of empirical asset pricing, originating from the seminal work of Fama and French (1992) and later extended in Fama and French (2015). These models have transformed how academics and practitioners understand the cross-section of expected stock returns, moving beyond the single-factor Capital Asset Pricing Model to incorporate multiple sources of systematic risk.\nWe construct both the three-factor and five-factor models at monthly and daily frequencies. The monthly factors serve as the foundation for most asset pricing tests and portfolio analyses, while the daily factors enable higher-frequency applications including short-horizon event studies, market microstructure research, and daily beta estimation. By constructing factors at both frequencies, we create a complete toolkit for empirical finance research in the Vietnamese market.\nThe chapter proceeds as follows. We first discuss the theoretical motivation for each factor and the economic intuition behind the Fama-French methodology. We then prepare the necessary data, merging stock returns with accounting characteristics. Next, we implement the portfolio sorting procedures that form the basis of factor construction, carefully following the original Fama-French protocols while adapting them for Vietnamese market characteristics. We construct the three-factor model (market, size, and value) before extending to the five-factor model (adding profitability and investment). Finally, we construct daily factors and validate our replicated factors through various diagnostic checks.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#theoretical-background",
    "href": "12_fama_french.html#theoretical-background",
    "title": "11  Fama-French Factors",
    "section": "",
    "text": "11.1.1 The Evolution from CAPM to Multi-Factor Models\nThe Capital Asset Pricing Model of Sharpe (1964) posits that a single factor—the market portfolio—should explain all cross-sectional variation in expected returns. However, decades of empirical research have documented persistent patterns that CAPM cannot explain. Fama and French (1992) demonstrated that two firm characteristics—size and book-to-market ratio—capture substantial variation in average returns that the market beta leaves unexplained.\nSmall firms tend to earn higher returns than large firms, a pattern known as the size effect. Similarly, firms with high book-to-market ratios (value stocks) tend to outperform firms with low book-to-market ratios (growth stocks), known as the value premium. The three-factor model formalizes these observations by constructing tradeable factor portfolios:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i^{MKT}(r_{m,t} - r_{f,t}) + \\beta_i^{SMB} \\cdot SMB_t + \\beta_i^{HML} \\cdot HML_t + \\varepsilon_{i,t}\n\\tag{11.1}\\]\nwhere:\n\n\\(r_{i,t} - r_{f,t}\\) is the excess return on asset \\(i\\)\n\\(r_{m,t} - r_{f,t}\\) is the market excess return\n\\(SMB_t\\) (Small Minus Big) is the size factor\n\\(HML_t\\) (High Minus Low) is the value factor\n\n\n\n11.1.2 The Five-Factor Extension\nFama and French (2015) extended the model to include two additional factors motivated by the dividend discount model. Firms with higher profitability should have higher expected returns (all else equal), and firms with aggressive investment policies should have lower expected returns:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i^{MKT}MKT_t + \\beta_i^{SMB}SMB_t + \\beta_i^{HML}HML_t + \\beta_i^{RMW}RMW_t + \\beta_i^{CMA}CMA_t + \\varepsilon_{i,t}\n\\tag{11.2}\\]\nwhere:\n\n\\(RMW_t\\) (Robust Minus Weak) is the profitability factor\n\\(CMA_t\\) (Conservative Minus Aggressive) is the investment factor\n\n\n\n11.1.3 Factor Construction Methodology\nThe Fama-French methodology constructs factors through double-sorted portfolios:\n\nSize sorts: Stocks are divided into Small and Big groups based on median market capitalization.\nCharacteristic sorts: Within each size group, stocks are sorted into terciles based on book-to-market (for HML), operating profitability (for RMW), or investment (for CMA).\nFactor returns: Factors are computed as the difference between average returns of portfolios with high versus low characteristic values, averaging across size groups to neutralize size effects.\nTiming: Portfolios are formed at the end of June each year using accounting data from the prior fiscal year, ensuring all information was publicly available at formation.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#setting-up-the-environment",
    "href": "12_fama_french.html#setting-up-the-environment",
    "title": "11  Fama-French Factors",
    "section": "11.2 Setting Up the Environment",
    "text": "11.2 Setting Up the Environment\nWe load the required Python packages for data manipulation, statistical analysis, and visualization.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nfrom scipy.stats.mstats import winsorize\nimport matplotlib.pyplot as plt\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\n\nWe connect to our SQLite database containing the processed Vietnamese financial data.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#data-preparation",
    "href": "12_fama_french.html#data-preparation",
    "title": "11  Fama-French Factors",
    "section": "11.3 Data Preparation",
    "text": "11.3 Data Preparation\n\n11.3.1 Loading Stock Returns\nWe load the monthly stock returns data, which includes excess returns, market capitalization, and the risk-free rate. These variables are essential for computing value-weighted portfolio returns and factor premiums.\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprint(f\"Monthly returns: {len(prices_monthly):,} observations\")\nprint(f\"Unique stocks: {prices_monthly['symbol'].nunique():,}\")\nprint(f\"Date range: {prices_monthly['date'].min():%Y-%m} to {prices_monthly['date'].max():%Y-%m}\")\n\nMonthly returns: 165,499 observations\nUnique stocks: 1,457\nDate range: 2010-02 to 2023-12\n\n\n\n\n11.3.2 Loading Company Fundamentals\nWe load the company fundamentals data containing book equity, operating profitability, and investment—the characteristics needed for constructing the Fama-French factors.\n\ncomp_vn = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, datadate, be, op, inv\n        FROM comp_vn\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n).dropna()\n\nprint(f\"Fundamentals: {len(comp_vn):,} firm-year observations\")\nprint(f\"Unique firms: {comp_vn['symbol'].nunique():,}\")\n\nFundamentals: 18,108 firm-year observations\nUnique firms: 1,496\n\n\n\n\n11.3.3 Constructing Sorting Variables\nFollowing Fama-French conventions, we construct the sorting variables with careful attention to timing. The key principles are:\n\nSize (June Market Cap): We use market capitalization at the end of June of year \\(t\\) to sort stocks into size groups. This ensures we capture the firm’s size at the moment of portfolio formation.\nBook-to-Market Ratio: We use book equity from fiscal year \\(t-1\\) divided by market equity at the end of December \\(t-1\\). This creates a six-month gap between the accounting data and portfolio formation, ensuring the information was publicly available.\nPortfolio Formation Date: Portfolios are formed on July 1st and held for twelve months until the following June.\n\n\ndef construct_sorting_variables(prices_monthly, comp_vn):\n    \"\"\"\n    Construct sorting variables following Fama-French methodology.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns with market cap\n    comp_vn : pd.DataFrame\n        Company fundamentals with book equity, profitability, investment\n        \n    Returns\n    -------\n    pd.DataFrame\n        Sorting variables aligned with July 1st formation dates\n    \"\"\"\n    \n    # 1. Size: June market capitalization\n    # Portfolio formation is July 1st, so we use June market cap\n    size = (prices_monthly\n        .query(\"date.dt.month == 6\")\n        .assign(\n            sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(1)\n        )\n        [[\"symbol\", \"sorting_date\", \"mktcap\"]]\n        .rename(columns={\"mktcap\": \"size\"})\n    )\n    \n    print(f\"Size observations: {len(size):,}\")\n    \n    # 2. Market Equity: December market cap for B/M calculation\n    # December t-1 market cap is used with fiscal year t-1 book equity\n    # This is then used for July t portfolio formation\n    market_equity = (prices_monthly\n        .query(\"date.dt.month == 12\")\n        .assign(\n            # December year t-1 maps to July year t formation\n            sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(7)\n        )\n        [[\"symbol\", \"sorting_date\", \"mktcap\"]]\n        .rename(columns={\"mktcap\": \"me\"})\n    )\n    \n    print(f\"Market equity observations: {len(market_equity):,}\")\n    \n    # 3. Book-to-Market and other characteristics\n    # Fiscal year t-1 data is used for July t portfolio formation\n    book_to_market = (comp_vn\n        .assign(\n            # Fiscal year-end + 6 months = July formation\n            sorting_date=lambda x: pd.to_datetime(\n                (x[\"datadate\"].dt.year + 1).astype(str) + \"-07-01\"\n            )\n        )\n        .merge(market_equity, on=[\"symbol\", \"sorting_date\"], how=\"inner\")\n        .assign(\n            # Scale book equity to match market equity units\n            # BE is in VND, ME is in millions VND\n            bm=lambda x: x[\"be\"] / (x[\"me\"] * 1e9)\n        )\n        [[\"symbol\", \"sorting_date\", \"me\", \"bm\", \"op\", \"inv\"]]\n    )\n    \n    print(f\"Book-to-market observations: {len(book_to_market):,}\")\n    \n    # 4. Merge size with characteristics\n    sorting_variables = (size\n        .merge(book_to_market, on=[\"symbol\", \"sorting_date\"], how=\"inner\")\n        .dropna()\n        .drop_duplicates(subset=[\"symbol\", \"sorting_date\"])\n    )\n    \n    return sorting_variables\n\nsorting_variables = construct_sorting_variables(prices_monthly, comp_vn)\n\nprint(f\"\\nFinal sorting variables: {len(sorting_variables):,} stock-years\")\nprint(f\"Sorting date range: {sorting_variables['sorting_date'].min():%Y-%m} to {sorting_variables['sorting_date'].max():%Y-%m}\")\n\nSize observations: 13,756\nMarket equity observations: 14,286\nBook-to-market observations: 13,389\n\nFinal sorting variables: 12,046 stock-years\nSorting date range: 2011-07 to 2023-07\n\n\n\n\n11.3.4 Validating Sorting Variables\nBefore proceeding, we validate that our sorting variables have reasonable distributions. The book-to-market ratio should center around 1.0 for a typical market, though emerging markets may differ.\n\nprint(\"Sorting Variable Summary Statistics:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\n# Check for extreme values that might indicate data issues\nprint(f\"\\nB/M Median: {sorting_variables['bm'].median():.4f}\")\nprint(f\"B/M 1st percentile: {sorting_variables['bm'].quantile(0.01):.4f}\")\nprint(f\"B/M 99th percentile: {sorting_variables['bm'].quantile(0.99):.4f}\")\n\nSorting Variable Summary Statistics:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.7033      0.1852      0.1322\nstd     14680.4225      3.8683      0.2782      2.5410\nmin         0.4864      0.0014     -0.7529     -0.9569\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817    272.1893      1.4256    261.3355\n\nB/M Median: 1.1849\nB/M 1st percentile: 0.1710\nB/M 99th percentile: 8.0262\n\n\n\n\n11.3.5 Handling Outliers\nExtreme values in sorting characteristics can distort portfolio assignments and factor returns. We apply winsorization to limit the influence of outliers while preserving the general ranking of stocks.\n\n# Check BEFORE winsorization\nprint(\"BEFORE Winsorization:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\n# Apply winsorization\ndef winsorize_characteristics(df, columns, limits=(0.01, 0.99)):\n    \"\"\"\n    Apply winsorization using pandas clip.\n    \"\"\"\n    df = df.copy()\n    for col in columns:\n        if col in df.columns:\n            lower = df[col].quantile(limits[0])\n            upper = df[col].quantile(limits[1])\n            df[col] = df[col].clip(lower=lower, upper=upper)\n            print(f\"  {col}: clipped to [{lower:.4f}, {upper:.4f}]\")\n    return df\n\nsorting_variables = winsorize_characteristics(\n    sorting_variables,\n    columns=[\"bm\", \"op\", \"inv\"],  # Don't winsorize size\n    limits=(0.01, 0.99)\n)\n\n# Check AFTER winsorization\nprint(\"\\nAFTER Winsorization:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\nBEFORE Winsorization:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.7033      0.1852      0.1322\nstd     14680.4225      3.8683      0.2782      2.5410\nmin         0.4864      0.0014     -0.7529     -0.9569\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817    272.1893      1.4256    261.3355\n  bm: clipped to [0.1710, 8.0262]\n  op: clipped to [-0.7319, 1.2192]\n  inv: clipped to [-0.3990, 1.5195]\n\nAFTER Winsorization:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.5544      0.1837      0.0894\nstd     14680.4225      1.2843      0.2705      0.2721\nmin         0.4864      0.1710     -0.7319     -0.3990\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817      8.0262      1.2192      1.5195",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#portfolio-assignment-functions",
    "href": "12_fama_french.html#portfolio-assignment-functions",
    "title": "11  Fama-French Factors",
    "section": "11.4 Portfolio Assignment Functions",
    "text": "11.4 Portfolio Assignment Functions\n\n11.4.1 The Portfolio Assignment Function\nWe create a flexible function for assigning stocks to portfolios based on quantile breakpoints. This function handles both independent sorts (where breakpoints are computed across all stocks) and dependent sorts (where breakpoints are computed within subgroups).\n\ndef assign_portfolio(data, sorting_variable, percentiles):\n    \"\"\"Assign portfolios to a bin according to a sorting variable.\"\"\"\n    \n    # Get the values\n    values = data[sorting_variable].dropna()\n    \n    if len(values) == 0:\n        return pd.Series([np.nan] * len(data), index=data.index)\n    \n    # Calculate breakpoints\n    breakpoints = values.quantile(percentiles, interpolation=\"linear\")\n    \n    # Handle duplicate breakpoints by using unique values\n    unique_breakpoints = np.unique(breakpoints)\n    \n    # If all values are the same, assign all to portfolio 1\n    if len(unique_breakpoints) &lt;= 1:\n        return pd.Series([1] * len(data), index=data.index)\n    \n    # Set boundaries to -inf and +inf\n    unique_breakpoints.iloc[0] = -np.inf\n    unique_breakpoints.iloc[unique_breakpoints.size-1] = np.inf\n    \n    # Assign to bins\n    assigned = pd.cut(\n        data[sorting_variable],\n        bins=unique_breakpoints,\n        labels=pd.Series(range(1, breakpoints.size)),\n        include_lowest=True,\n        right=False\n    )\n    \n    return assigned\n\n\n# Check the distribution of characteristics BEFORE portfolio assignment\nprint(\"Operating Profitability Distribution:\")\nprint(sorting_variables[\"op\"].describe())\nprint(f\"\\nUnique OP values: {sorting_variables['op'].nunique()}\")\n\nprint(\"\\nInvestment Distribution:\")\nprint(sorting_variables[\"inv\"].describe())\nprint(f\"\\nUnique INV values: {sorting_variables['inv'].nunique()}\")\n\n# Check breakpoints for a specific date\ntest_date = sorting_variables[\"sorting_date\"].iloc[0]\ntest_data = sorting_variables.query(\"sorting_date == @test_date\")\n\nprint(f\"\\nBreakpoints for {test_date}:\")\nprint(f\"OP 30th percentile: {test_data['op'].quantile(0.3):.4f}\")\nprint(f\"OP 70th percentile: {test_data['op'].quantile(0.7):.4f}\")\nprint(f\"INV 30th percentile: {test_data['inv'].quantile(0.3):.4f}\")\nprint(f\"INV 70th percentile: {test_data['inv'].quantile(0.7):.4f}\")\n\nOperating Profitability Distribution:\ncount    12046.000000\nmean         0.183738\nstd          0.270509\nmin         -0.731888\n25%          0.030913\n50%          0.136675\n75%          0.295185\nmax          1.219223\nName: op, dtype: float64\n\nUnique OP values: 11804\n\nInvestment Distribution:\ncount    12046.000000\nmean         0.089388\nstd          0.272147\nmin         -0.399042\n25%         -0.049497\n50%          0.034157\n75%          0.158155\nmax          1.519474\nName: inv, dtype: float64\n\nUnique INV values: 11805\n\nBreakpoints for 2019-07-01 00:00:00:\nOP 30th percentile: 0.0541\nOP 70th percentile: 0.2566\nINV 30th percentile: -0.0343\nINV 70th percentile: 0.1116\n\n\n\n\n11.4.2 Assigning Portfolios for Three-Factor Model\nFor the three-factor model, we perform independent double sorts on size and book-to-market. Size is split at the median (2 groups), and book-to-market is split at the 30th and 70th percentiles (3 groups), creating 6 portfolios.\n\ndef assign_ff3_portfolios(sorting_variables):\n    \"\"\"\n    Assign portfolios for Fama-French three-factor model.\n    Independent 2x3 sort on size and book-to-market.\n    \"\"\"\n    df = sorting_variables.copy()\n    \n    # Independent size sort (median split)\n    df[\"portfolio_size\"] = df.groupby(\"sorting_date\")[\"size\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.5, 1], labels=[1, 2], duplicates='drop')\n    )\n    \n    # Independent B/M sort (30/70 split)\n    df[\"portfolio_bm\"] = df.groupby(\"sorting_date\")[\"bm\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    return df\n\n# Assign portfolios\nportfolios_ff3 = assign_ff3_portfolios(sorting_variables)\n\n# Validate\nprint(\"FF3 Book-to-Market by Portfolio (should be INCREASING):\")\nprint(portfolios_ff3.groupby(\"portfolio_bm\", observed=True)[\"bm\"].median().round(4))\n\n\nprint(\"Three-Factor Portfolio Assignments:\")\nprint(portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]].head(10))\n\nFF3 Book-to-Market by Portfolio (should be INCREASING):\nportfolio_bm\n1    0.5836\n2    1.1891\n3    2.4552\nName: bm, dtype: float64\nThree-Factor Portfolio Assignments:\n  symbol sorting_date portfolio_size portfolio_bm\n0    A32   2019-07-01              1            2\n1    A32   2020-07-01              1            2\n2    A32   2021-07-01              1            2\n3    A32   2022-07-01              1            3\n4    A32   2023-07-01              1            2\n5    AAA   2011-07-01              2            2\n6    AAA   2012-07-01              2            3\n7    AAA   2013-07-01              2            3\n8    AAA   2014-07-01              2            2\n9    AAA   2015-07-01              2            3\n\n\n\n\n11.4.3 Validating Portfolio Assignments\nWe verify that the portfolio assignments create the expected 2×3 grid with reasonable stock counts in each cell.\n\n# Check portfolio distribution for most recent year\nlatest_date = portfolios_ff3[\"sorting_date\"].max()\n\nportfolio_counts = (portfolios_ff3\n    .query(\"sorting_date == @latest_date\")\n    .groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)\n    .size()\n    .unstack(fill_value=0)\n)\n\nprint(f\"Portfolio Counts for {latest_date:%Y-%m}:\")\nprint(portfolio_counts)\n\n# Verify characteristic monotonicity\nprint(\"\\nBook-to-Market by Portfolio (should be increasing):\")\nprint(portfolios_ff3.groupby(\"portfolio_bm\", observed=True)[\"bm\"].median().round(4))\n\nPortfolio Counts for 2023-07:\nportfolio_bm      1    2    3\nportfolio_size               \n1               113  271  263\n2               275  246  125\n\nBook-to-Market by Portfolio (should be increasing):\nportfolio_bm\n1    0.5836\n2    1.1891\n3    2.4552\nName: bm, dtype: float64\n\n\nWe verify that for a single stock, the portfolio assignment remains constant between July of one year and June of the next.\n\n# Trace a single symbol (e.g., 'A32') across a formation window\npersistence_check = (portfolios_ff3\n    .query(\"symbol == 'A32' & sorting_date &gt;= '2022-01-01' & sorting_date &lt;= '2023-12-31'\")\n    .sort_values(\"sorting_date\")\n    [['symbol', 'sorting_date', 'portfolio_size', 'portfolio_bm']]\n)\nprint(\"\\nTemporal Persistence Check (Symbol A32):\")\nprint(persistence_check.head(15))\n\n\nTemporal Persistence Check (Symbol A32):\n  symbol sorting_date portfolio_size portfolio_bm\n3    A32   2022-07-01              1            3\n4    A32   2023-07-01              1            2",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#fama-french-three-factor-model-monthly",
    "href": "12_fama_french.html#fama-french-three-factor-model-monthly",
    "title": "11  Fama-French Factors",
    "section": "11.5 Fama-French Three-Factor Model (Monthly)",
    "text": "11.5 Fama-French Three-Factor Model (Monthly)\n\n11.5.1 Merging Portfolios with Returns\nWe merge the portfolio assignments with monthly returns. The key insight is that portfolios formed in July of year \\(t\\) are held through June of year \\(t+1\\). We implement this by computing a sorting_date for each monthly return observation.\n\ndef merge_portfolios_with_returns(prices_monthly, portfolio_assignments):\n    \"\"\"\n    Merge portfolio assignments with monthly returns.\n    \n    Portfolios formed in July t are held through June t+1.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns\n    portfolio_assignments : pd.DataFrame\n        Portfolio assignments with sorting_date\n        \n    Returns\n    -------\n    pd.DataFrame\n        Returns merged with portfolio assignments\n    \"\"\"\n    portfolios = (prices_monthly\n        .assign(\n            # Map each return month to its portfolio formation date\n            sorting_date=lambda x: pd.to_datetime(\n                np.where(\n                    x[\"date\"].dt.month &lt;= 6,\n                    (x[\"date\"].dt.year - 1).astype(str) + \"-07-01\",\n                    x[\"date\"].dt.year.astype(str) + \"-07-01\"\n                )\n            )\n        )\n        .merge(\n            portfolio_assignments,\n            on=[\"symbol\", \"sorting_date\"],\n            how=\"inner\"\n        )\n    )\n    \n    return portfolios\n\nportfolios_monthly_ff3 = merge_portfolios_with_returns(\n    prices_monthly,\n    portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]]\n)\n\n\nprint(f\"Merged observations: {len(portfolios_monthly_ff3):,}\")\n\nMerged observations: 136,444\n\n\n\n\n11.5.2 Computing Value-Weighted Portfolio Returns\nWe compute value-weighted returns for each of the six portfolios. Value-weighting uses lagged market capitalization to avoid look-ahead bias.\n\ndef compute_portfolio_returns(data, grouping_vars):\n    \"\"\"\n    Compute value-weighted portfolio returns.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Returns data with portfolio assignments and mktcap_lag\n    grouping_vars : list\n        Variables defining portfolio groups\n        \n    Returns\n    -------\n    pd.DataFrame\n        Value-weighted returns for each portfolio-date\n    \"\"\"\n    portfolio_returns = (data\n        .groupby(grouping_vars + [\"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]),\n            \"n_stocks\": len(x)\n        }))\n        .reset_index()\n    )\n    \n    return portfolio_returns\n\n\n# Compute portfolio returns\nportfolio_returns_ff3 = compute_portfolio_returns(\n    portfolios_monthly_ff3,\n    [\"portfolio_size\", \"portfolio_bm\"]\n)\n\nprint(\"Portfolio Returns Summary:\")\nprint(portfolio_returns_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)[\"ret\"].describe().round(4))\n\nPortfolio Returns Summary:\n                             count    mean     std     min     25%     50%  \\\nportfolio_size portfolio_bm                                                  \n1              1             150.0 -0.0052  0.0379 -0.1268 -0.0262 -0.0058   \n               2             150.0 -0.0025  0.0414 -0.1080 -0.0236 -0.0053   \n               3             150.0  0.0039  0.0601 -0.1612 -0.0269 -0.0004   \n2              1             150.0 -0.0124  0.0594 -0.2222 -0.0449 -0.0107   \n               2             150.0 -0.0021  0.0671 -0.1701 -0.0403 -0.0046   \n               3             150.0  0.0024  0.0879 -0.2359 -0.0527 -0.0012   \n\n                                75%     max  \nportfolio_size portfolio_bm                  \n1              1             0.0161  0.0849  \n               2             0.0180  0.1195  \n               3             0.0314  0.2015  \n2              1             0.0210  0.1741  \n               2             0.0317  0.1770  \n               3             0.0437  0.2124  \n\n\n\n\n11.5.3 Constructing SMB and HML Factors\nWe now construct the SMB and HML factors from the portfolio returns.\nSMB (Small Minus Big): Average return of three small portfolios minus average return of three big portfolios.\nHML (High Minus Low): Average return of two high B/M portfolios minus average return of two low B/M portfolios.\n\ndef construct_ff3_factors(portfolio_returns):\n    \"\"\"\n    Construct Fama-French three factors from portfolio returns.\n    \n    Parameters\n    ----------\n    portfolio_returns : pd.DataFrame\n        Value-weighted returns for 2x3 portfolios\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly SMB and HML factors\n    \"\"\"\n    factors = (portfolio_returns\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            # SMB: Small minus Big (average across B/M groups)\n            \"smb\": (\n                x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean()\n            ),\n            # HML: High minus Low B/M (average across size groups)\n            \"hml\": (\n                x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean()\n            )\n        }))\n        .reset_index()\n    )\n    \n    return factors\n\nfactors_smb_hml = construct_ff3_factors(portfolio_returns_ff3)\n\nprint(\"SMB and HML Factors:\")\nprint(factors_smb_hml.head(10))\n\nSMB and HML Factors:\n        date       smb       hml\n0 2011-07-31 -0.007768  0.002754\n1 2011-08-31 -0.067309  0.011474\n2 2011-09-30  0.014884  0.022854\n3 2011-10-31 -0.003743  0.001631\n4 2011-11-30  0.063234  0.009103\n5 2011-12-31  0.014571  0.015280\n6 2012-01-31 -0.026080  0.009672\n7 2012-02-29 -0.035721  0.005474\n8 2012-03-31 -0.002344  0.032477\n9 2012-04-30 -0.033391  0.074191\n\n\n\n\n11.5.4 Computing the Market Factor\nThe market factor is the value-weighted return of all stocks minus the risk-free rate. We compute this independently from the sorted portfolios.\n\ndef compute_market_factor(prices_monthly):\n    \"\"\"\n    Compute value-weighted market excess return.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns with mktcap_lag\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly market excess return\n    \"\"\"\n    market_factor = (prices_monthly\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]),\n            \"n_stocks\": len(x)\n        }), include_groups=False)\n        .reset_index()\n    )\n    \n    return market_factor\n\nmarket_factor = compute_market_factor(prices_monthly)\n\nprint(\"Market Factor Summary:\")\nprint(market_factor[\"mkt_excess\"].describe().round(4))\n\nMarket Factor Summary:\ncount    167.0000\nmean      -0.0123\nstd        0.0595\nmin       -0.2149\n25%       -0.0394\n50%       -0.0106\n75%        0.0200\nmax        0.1677\nName: mkt_excess, dtype: float64\n\n\n\n\n11.5.5 Combining Three Factors\nWe combine SMB, HML, and the market factor into the complete three-factor dataset.\n\nfactors_ff3_monthly = (factors_smb_hml\n    .merge(market_factor[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"inner\")\n)\n\n# Add risk-free rate for completeness\nrf_monthly = (prices_monthly\n    .groupby(\"date\")[\"risk_free\"]\n    .first()\n    .reset_index()\n)\n\nfactors_ff3_monthly = factors_ff3_monthly.merge(rf_monthly, on=\"date\", how=\"left\")\n\nprint(\"Fama-French Three Factors (Monthly):\")\nprint(factors_ff3_monthly.head(10))\n\nprint(\"\\nFactor Summary Statistics:\")\nprint(factors_ff3_monthly[[\"mkt_excess\", \"smb\", \"hml\"]].describe().round(4))\n\nFama-French Three Factors (Monthly):\n        date       smb       hml  mkt_excess  risk_free\n0 2011-07-31 -0.007768  0.002754   -0.078748   0.003333\n1 2011-08-31 -0.067309  0.011474    0.029906   0.003333\n2 2011-09-30  0.014884  0.022854   -0.002173   0.003333\n3 2011-10-31 -0.003743  0.001631   -0.014005   0.003333\n4 2011-11-30  0.063234  0.009103   -0.179410   0.003333\n5 2011-12-31  0.014571  0.015280   -0.094802   0.003333\n6 2012-01-31 -0.026080  0.009672    0.081273   0.003333\n7 2012-02-29 -0.035721  0.005474    0.069655   0.003333\n8 2012-03-31 -0.002344  0.032477    0.029005   0.003333\n9 2012-04-30 -0.033391  0.074191    0.048791   0.003333\n\nFactor Summary Statistics:\n       mkt_excess       smb       hml\ncount    150.0000  150.0000  150.0000\nmean      -0.0101    0.0027    0.0120\nstd        0.0586    0.0420    0.0535\nmin       -0.2149   -0.1599   -0.1284\n25%       -0.0380   -0.0175   -0.0160\n50%       -0.0095    0.0070    0.0043\n75%        0.0214    0.0261    0.0340\nmax        0.1677    0.1175    0.1618\n\n\n\n\n11.5.6 Saving Three-Factor Data\n\nfactors_ff3_monthly.to_sql(\n    name=\"factors_ff3_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Three-factor monthly data saved to database.\")\n\nThree-factor monthly data saved to database.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#fama-french-five-factor-model-monthly",
    "href": "12_fama_french.html#fama-french-five-factor-model-monthly",
    "title": "11  Fama-French Factors",
    "section": "11.6 Fama-French Five-Factor Model (Monthly)",
    "text": "11.6 Fama-French Five-Factor Model (Monthly)\n\n11.6.1 Portfolio Assignments with Dependent Sorts\nFor the five-factor model, we use dependent sorts: size is sorted independently, but profitability and investment are sorted within size groups. This controls for the correlation between size and these characteristics.\n\ndef assign_ff5_portfolios(sorting_variables):\n    \"\"\"\n    Assign portfolios for Fama-French five-factor model.\n    \"\"\"\n    df = sorting_variables.copy()\n    \n    # Independent size sort\n    df[\"portfolio_size\"] = df.groupby(\"sorting_date\")[\"size\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.5, 1], labels=[1, 2], duplicates='drop')\n    )\n    \n    # Dependent sorts within size groups\n    df[\"portfolio_bm\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"bm\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    df[\"portfolio_op\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"op\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    df[\"portfolio_inv\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"inv\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    return df\n\n# Run\nportfolios_ff5 = assign_ff5_portfolios(sorting_variables)\n\n\n\n11.6.2 Validating Five-Factor Portfolios\n\n# Check characteristic monotonicity for each dimension\nprint(\"Profitability by Portfolio (should be increasing):\")\nprint(portfolios_ff5.groupby(\"portfolio_op\", observed=True)[\"op\"].median().round(4))\n\nprint(\"\\nInvestment by Portfolio (should be increasing):\")\nprint(portfolios_ff5.groupby(\"portfolio_inv\", observed=True)[\"inv\"].median().round(4))\n\n# Check portfolio counts\nprint(\"\\nStocks per Size/Profitability Bin:\")\nprint(portfolios_ff5.groupby([\"portfolio_size\", \"portfolio_op\"], observed=True).size().unstack(fill_value=0))\n\n# Check number of unique firms per year\n(portfolios_ff5\n .groupby(\"sorting_date\")[\"symbol\"]\n .nunique())\n\nProfitability by Portfolio (should be increasing):\nportfolio_op\n1   -0.0053\n2    0.1366\n3    0.4098\nName: op, dtype: float64\n\nInvestment by Portfolio (should be increasing):\nportfolio_inv\n1   -0.1012\n2    0.0329\n3    0.2568\nName: inv, dtype: float64\n\nStocks per Size/Profitability Bin:\nportfolio_op       1     2     3\nportfolio_size                  \n1               1812  2403  1811\n2               1811  2401  1808\n\n\nsorting_date\n2011-07-01     556\n2012-07-01     632\n2013-07-01     643\n2014-07-01     650\n2015-07-01     669\n2016-07-01     737\n2017-07-01     842\n2018-07-01    1073\n2019-07-01    1183\n2020-07-01    1224\n2021-07-01    1258\n2022-07-01    1286\n2023-07-01    1293\nName: symbol, dtype: int64\n\n\n\n\n11.6.3 Merging and Computing Portfolio Returns\n\n# Merge with returns\nportfolios_monthly_ff5 = merge_portfolios_with_returns(\n    prices_monthly,\n    portfolios_ff5[[\"symbol\", \"sorting_date\", \"portfolio_size\", \n                    \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"]]\n)\n\nprint(f\"Five-factor merged observations: {len(portfolios_monthly_ff5):,}\")\n\nFive-factor merged observations: 136,444\n\n\n\n\n11.6.4 Constructing All Five Factors\nWe construct each factor from the appropriate portfolio sorts.\n\ndef construct_ff5_factors(portfolios_monthly):\n    \"\"\"\n    Construct Fama-French five factors from portfolio data.\n    \n    Parameters\n    ----------\n    portfolios_monthly : pd.DataFrame\n        Monthly returns with all portfolio assignments\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly five-factor returns\n    \"\"\"\n    \n    # HML: Value factor from B/M sorts\n    portfolios_bm = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_hml = (portfolios_bm\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # RMW: Profitability factor from OP sorts\n    portfolios_op = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_rmw = (portfolios_op\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"rmw\": (x.loc[x[\"portfolio_op\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_op\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # CMA: Investment factor from INV sorts\n    # Note: CMA is Conservative minus Aggressive (low inv - high inv)\n    portfolios_inv = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_cma = (portfolios_inv\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"cma\": (x.loc[x[\"portfolio_inv\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_inv\"] == 3, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # SMB: Size factor (average across all characteristic portfolios)\n    all_portfolios = pd.concat([portfolios_bm, portfolios_op, portfolios_inv])\n    \n    factors_smb = (all_portfolios\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # Combine all factors\n    factors = (factors_smb\n        .merge(factors_hml, on=\"date\", how=\"outer\")\n        .merge(factors_rmw, on=\"date\", how=\"outer\")\n        .merge(factors_cma, on=\"date\", how=\"outer\")\n    )\n    \n    return factors\n\nfactors_ff5 = construct_ff5_factors(portfolios_monthly_ff5)\n\n# Add market factor\nfactors_ff5_monthly = (factors_ff5\n    .merge(market_factor[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"inner\")\n    .merge(rf_monthly, on=\"date\", how=\"left\")\n)\n\nprint(\"Fama-French Five Factors (Monthly):\")\nprint(factors_ff5_monthly.head(10))\n\nprint(\"\\nFactor Summary Statistics:\")\nprint(factors_ff5_monthly[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].describe().round(4))\n\nFama-French Five Factors (Monthly):\n        date       smb       hml       rmw       cma  mkt_excess  risk_free\n0 2011-07-31 -0.015907 -0.002812  0.060525  0.045291   -0.078748   0.003333\n1 2011-08-31 -0.061842  0.006189 -0.022700 -0.023177    0.029906   0.003333\n2 2011-09-30  0.014387  0.024301 -0.006005  0.003588   -0.002173   0.003333\n3 2011-10-31 -0.006958 -0.006940  0.026694  0.003649   -0.014005   0.003333\n4 2011-11-30  0.074369  0.015617 -0.058766  0.044214   -0.179410   0.003333\n5 2011-12-31  0.006687  0.022494  0.062655  0.052444   -0.094802   0.003333\n6 2012-01-31 -0.016254  0.010513 -0.042191 -0.067170    0.081273   0.003333\n7 2012-02-29 -0.026606  0.024465 -0.030849 -0.036383    0.069655   0.003333\n8 2012-03-31  0.005096  0.050930 -0.018441  0.043488    0.029005   0.003333\n9 2012-04-30  0.000712  0.058214 -0.061434  0.009233    0.048791   0.003333\n\nFactor Summary Statistics:\n       mkt_excess       smb       hml       rmw       cma\ncount    150.0000  150.0000  150.0000  150.0000  150.0000\nmean      -0.0101    0.0077    0.0115   -0.0047    0.0083\nstd        0.0586    0.0419    0.0518    0.0477    0.0335\nmin       -0.2149   -0.1522   -0.1283   -0.2126   -0.0814\n25%       -0.0380   -0.0137   -0.0126   -0.0308   -0.0131\n50%       -0.0095    0.0104    0.0046    0.0010    0.0067\n75%        0.0214    0.0316    0.0323    0.0178    0.0289\nmax        0.1677    0.1284    0.1510    0.1297    0.1331\n\n\n\n\n11.6.5 Factor Correlations\nWe examine correlations between factors, which should generally be low for the factors to capture distinct sources of risk.\n\nprint(\"Factor Correlation Matrix:\")\ncorrelation_matrix = factors_ff5_monthly[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].corr()\nprint(correlation_matrix.round(3))\n\nFactor Correlation Matrix:\n            mkt_excess    smb    hml    rmw    cma\nmkt_excess       1.000 -0.712  0.230 -0.006 -0.104\nsmb             -0.712  1.000  0.256 -0.373  0.246\nhml              0.230  0.256  1.000 -0.694  0.479\nrmw             -0.006 -0.373 -0.694  1.000 -0.352\ncma             -0.104  0.246  0.479 -0.352  1.000\n\n\n\n\n11.6.6 Saving Five-Factor Data\n\nfactors_ff5_monthly.to_sql(\n    name=\"factors_ff5_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Five-factor monthly data saved to database.\")\n\nFive-factor monthly data saved to database.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#daily-fama-french-factors",
    "href": "12_fama_french.html#daily-fama-french-factors",
    "title": "11  Fama-French Factors",
    "section": "11.7 Daily Fama-French Factors",
    "text": "11.7 Daily Fama-French Factors\n\n11.7.1 Motivation for Daily Factors\nDaily factors are essential for several applications:\n\nDaily beta estimation: CAPM regressions using daily data require daily market excess returns.\nEvent studies: Measuring abnormal returns around corporate events requires daily factor adjustments.\nHigh-frequency research: Market microstructure studies need daily or intraday factor data.\n\nThe construction methodology mirrors the monthly approach, but we compute portfolio returns at daily frequency while maintaining the same annual portfolio formation dates.\n\n\n11.7.2 Loading Daily Returns\n\n# Load daily price data\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess\n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Daily returns: {len(prices_daily):,} observations\")\nprint(f\"Date range: {prices_daily['date'].min():%Y-%m-%d} to {prices_daily['date'].max():%Y-%m-%d}\")\n\nDaily returns: 3,462,157 observations\nDate range: 2010-01-05 to 2023-12-29\n\n\n\n\n11.7.3 Adding Market Cap for Daily Weighting\nFor value-weighted daily returns, we need market capitalization. We use the most recent monthly market cap as the weight for daily returns within that month.\n\n# Get monthly market cap to use as weights for daily returns\nmktcap_monthly = (prices_monthly\n    [[\"symbol\", \"date\", \"mktcap_lag\"]]\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n)\n\n# Add year_month to daily data for merging\nprices_daily = (prices_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .merge(\n        mktcap_monthly[[\"symbol\", \"year_month\", \"mktcap_lag\"]],\n        on=[\"symbol\", \"year_month\"],\n        how=\"left\"\n    )\n    .dropna(subset=[\"ret_excess\", \"mktcap_lag\"])\n)\n\nprint(f\"Daily returns with weights: {len(prices_daily):,} observations\")\n\nDaily returns with weights: 3,443,815 observations\n\n\n\n\n11.7.4 Merging Daily Returns with Portfolios\nWe use the same portfolio assignments (formed annually in July) for daily returns.\n\n# Step 1: Ensure portfolios_ff3 has correct format\nportfolios_ff3_clean = portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]].copy()\nportfolios_ff3_clean[\"sorting_date\"] = pd.to_datetime(portfolios_ff3_clean[\"sorting_date\"])\n\nprint(\"Portfolio sorting dates:\")\nprint(portfolios_ff3_clean[\"sorting_date\"].unique()[:5])\n\n# Step 2: Create sorting_date for daily data\nprices_daily_with_sort = prices_daily.copy()\nprices_daily_with_sort[\"sorting_date\"] = prices_daily_with_sort[\"date\"].apply(\n    lambda x: pd.Timestamp(f\"{x.year}-07-01\") if x.month &gt; 6 else pd.Timestamp(f\"{x.year - 1}-07-01\")\n)\n\nprint(\"\\nDaily sorting dates:\")\nprint(prices_daily_with_sort[\"sorting_date\"].unique()[:5])\n\n# Step 3: Merge\nportfolios_daily_ff3 = prices_daily_with_sort.merge(\n    portfolios_ff3_clean,\n    on=[\"symbol\", \"sorting_date\"],\n    how=\"inner\"\n)\n\nprint(f\"\\nMerged daily observations: {len(portfolios_daily_ff3):,}\")\nprint(f\"Unique dates: {portfolios_daily_ff3['date'].nunique():,}\")\n\n# Step 4: Verify portfolio distribution\nprint(\"\\nPortfolio distribution in daily data:\")\nprint(portfolios_daily_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\nPortfolio sorting dates:\n&lt;DatetimeArray&gt;\n['2019-07-01 00:00:00', '2020-07-01 00:00:00', '2021-07-01 00:00:00',\n '2022-07-01 00:00:00', '2023-07-01 00:00:00']\nLength: 5, dtype: datetime64[us]\n\nDaily sorting dates:\n&lt;DatetimeArray&gt;\n['2018-07-01 00:00:00', '2019-07-01 00:00:00', '2020-07-01 00:00:00',\n '2021-07-01 00:00:00', '2022-07-01 00:00:00']\nLength: 5, dtype: datetime64[us]\n\nMerged daily observations: 2,843,570\nUnique dates: 3,126\n\nPortfolio distribution in daily data:\nportfolio_bm         1       2       3\nportfolio_size                        \n1               218040  585561  617327\n2               636152  552114  234376\n\n\n\n# Diagnostic: Check the daily portfolio merge\nprint(\"=\"*50)\nprint(\"DIAGNOSTIC: Daily Portfolio Merge\")\nprint(\"=\"*50)\n\nprint(f\"\\nDaily prices rows: {len(prices_daily):,}\")\nprint(f\"Daily FF3 portfolios rows: {len(portfolios_daily_ff3):,}\")\nprint(f\"Match rate: {len(portfolios_daily_ff3)/len(prices_daily)*100:.1f}%\")\n\n# Check portfolio distribution in daily data\nprint(\"\\nDaily portfolio distribution:\")\nprint(portfolios_daily_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\n# Check a specific date\ntest_date = portfolios_daily_ff3[\"date\"].iloc[1000]\nprint(f\"\\nSample date: {test_date}\")\nprint(portfolios_daily_ff3.query(\"date == @test_date\").groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\n==================================================\nDIAGNOSTIC: Daily Portfolio Merge\n==================================================\n\nDaily prices rows: 3,443,815\nDaily FF3 portfolios rows: 2,843,570\nMatch rate: 82.6%\n\nDaily portfolio distribution:\nportfolio_bm         1       2       3\nportfolio_size                        \n1               218040  585561  617327\n2               636152  552114  234376\n\nSample date: 2023-06-28 00:00:00\nportfolio_bm      1    2    3\nportfolio_size               \n1                93  232  312\n2               291  280   67\n\n\n\n\n11.7.5 Computing Daily Three Factors\n\ndef compute_daily_ff3_factors(portfolios_daily):\n    \"\"\"\n    Compute daily Fama-French three factors.\n    \n    Parameters\n    ----------\n    portfolios_daily : pd.DataFrame\n        Daily returns with portfolio assignments\n        \n    Returns\n    -------\n    pd.DataFrame\n        Daily SMB and HML factors\n    \"\"\"\n    # Compute daily portfolio returns\n    portfolio_returns = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    # Compute factors\n    factors = (portfolio_returns\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean()),\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    return factors\n\nfactors_daily_smb_hml = compute_daily_ff3_factors(portfolios_daily_ff3)\n\nprint(f\"Daily factor observations: {len(factors_daily_smb_hml):,}\")\nprint(factors_daily_smb_hml.head(10))\n\nDaily factor observations: 3,126\n        date       smb       hml\n0 2011-07-01  0.008587  0.000967\n1 2011-07-04  0.005099 -0.001099\n2 2011-07-05 -0.009088  0.010152\n3 2011-07-06  0.004875 -0.003918\n4 2011-07-07 -0.011239 -0.000584\n5 2011-07-08  0.005636 -0.008003\n6 2011-07-11  0.003940  0.006172\n7 2011-07-12  0.003205  0.006543\n8 2011-07-13 -0.000097 -0.001134\n9 2011-07-14 -0.001248  0.001669\n\n\n\n\n11.7.6 Computing Daily Market Factor\n\ndef compute_daily_market_factor(prices_daily):\n    \"\"\"\n    Compute daily value-weighted market excess return.\n    \n    Parameters\n    ----------\n    prices_daily : pd.DataFrame\n        Daily returns with mktcap_lag\n        \n    Returns\n    -------\n    pd.DataFrame\n        Daily market excess return\n    \"\"\"\n    market_daily = (prices_daily\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    return market_daily\n\nmarket_factor_daily = compute_daily_market_factor(prices_daily)\n\nprint(f\"Daily market factor: {len(market_factor_daily):,} days\")\n\nDaily market factor: 3,474 days\n\n\n\n\n11.7.7 Combining Daily Three Factors\n\nfactors_ff3_daily = (factors_daily_smb_hml\n    .merge(market_factor_daily, on=\"date\", how=\"inner\")\n)\n\n# Add risk-free rate (use monthly rate / 21 as daily approximation, or load actual daily rate)\nfactors_ff3_daily[\"risk_free\"] = 0.04 / 252  # Approximate daily risk-free\n\nprint(\"Daily Fama-French Three Factors:\")\nprint(factors_ff3_daily.head(10))\n\nprint(\"\\nDaily Factor Summary Statistics:\")\nprint(factors_ff3_daily[[\"mkt_excess\", \"smb\", \"hml\"]].describe().round(6))\n\nDaily Fama-French Three Factors:\n        date       smb       hml  mkt_excess  risk_free\n0 2011-07-01  0.008587  0.000967   -0.019862   0.000159\n1 2011-07-04  0.005099 -0.001099   -0.000633   0.000159\n2 2011-07-05 -0.009088  0.010152    0.013314   0.000159\n3 2011-07-06  0.004875 -0.003918   -0.008045   0.000159\n4 2011-07-07 -0.011239 -0.000584    0.003391   0.000159\n5 2011-07-08  0.005636 -0.008003    0.000218   0.000159\n6 2011-07-11  0.003940  0.006172   -0.013393   0.000159\n7 2011-07-12  0.003205  0.006543   -0.017505   0.000159\n8 2011-07-13 -0.000097 -0.001134    0.000767   0.000159\n9 2011-07-14 -0.001248  0.001669   -0.000695   0.000159\n\nDaily Factor Summary Statistics:\n        mkt_excess          smb          hml\ncount  3126.000000  3126.000000  3126.000000\nmean     -0.000479     0.000236     0.000594\nstd       0.011269     0.008488     0.008585\nmin      -0.070268    -0.032671    -0.039418\n25%      -0.005074    -0.004882    -0.003941\n50%       0.000350    -0.000106     0.000522\n75%       0.005531     0.004307     0.005233\nmax       0.043386     0.042686     0.083889\n\n\n\n\n11.7.8 Computing Daily Five Factors\n\n# Step 1: Clean portfolios\nportfolios_ff5_clean = portfolios_ff5[[\"symbol\", \"sorting_date\", \"portfolio_size\", \n                                        \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"]].copy()\nportfolios_ff5_clean[\"sorting_date\"] = pd.to_datetime(portfolios_ff5_clean[\"sorting_date\"])\n\n# Step 2: Merge with daily prices\nportfolios_daily_ff5 = prices_daily_with_sort.merge(\n    portfolios_ff5_clean,\n    on=[\"symbol\", \"sorting_date\"],\n    how=\"inner\"\n)\n\nprint(f\"FF5 Daily merged observations: {len(portfolios_daily_ff5):,}\")\n\nFF5 Daily merged observations: 2,843,570\n\n\n\ndef compute_daily_ff5_factors(portfolios_daily):\n    \"\"\"Compute daily Fama-French five factors.\"\"\"\n    \n    # HML from B/M sorts\n    portfolios_bm = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_hml = (portfolios_bm\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # RMW from OP sorts\n    portfolios_op = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_rmw = (portfolios_op\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"rmw\": (x.loc[x[\"portfolio_op\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_op\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # CMA from INV sorts (note: low minus high)\n    portfolios_inv = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_cma = (portfolios_inv\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"cma\": (x.loc[x[\"portfolio_inv\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_inv\"] == 3, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # SMB from all sorts\n    all_portfolios = pd.concat([portfolios_bm, portfolios_op, portfolios_inv])\n    \n    factors_smb = (all_portfolios\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # Combine\n    factors = (factors_smb\n        .merge(factors_hml, on=\"date\", how=\"outer\")\n        .merge(factors_rmw, on=\"date\", how=\"outer\")\n        .merge(factors_cma, on=\"date\", how=\"outer\")\n    )\n    \n    return factors\n\n# Compute daily FF5 factors\nfactors_daily_ff5 = compute_daily_ff5_factors(portfolios_daily_ff5)\n\n# Add market factor\nfactors_ff5_daily = (factors_daily_ff5\n    .merge(market_factor_daily, on=\"date\", how=\"inner\")\n)\nfactors_ff5_daily[\"risk_free\"] = 0.04 / 252\n\nprint(\"Daily Fama-French Five Factors:\")\nprint(factors_ff5_daily.head(10))\n\nprint(\"\\nDaily Five-Factor Summary Statistics:\")\nprint(factors_ff5_daily[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].describe().round(6))\n\nDaily Fama-French Five Factors:\n        date       smb       hml       rmw       cma  mkt_excess  risk_free\n0 2011-07-01  0.006295  0.002515  0.013140  0.007680   -0.019862   0.000159\n1 2011-07-04  0.002880 -0.002875  0.006560 -0.004886   -0.000633   0.000159\n2 2011-07-05 -0.004260  0.009864 -0.012158 -0.004470    0.013314   0.000159\n3 2011-07-06  0.001544 -0.009847  0.012977  0.006286   -0.008045   0.000159\n4 2011-07-07 -0.009789 -0.003988 -0.000197 -0.006995    0.003391   0.000159\n5 2011-07-08  0.001537 -0.006700  0.010841 -0.007661    0.000218   0.000159\n6 2011-07-11  0.005396  0.004747  0.000655  0.013375   -0.013393   0.000159\n7 2011-07-12  0.004759  0.007367  0.001989  0.014669   -0.017505   0.000159\n8 2011-07-13 -0.000009  0.001110 -0.002052 -0.001633    0.000767   0.000159\n9 2011-07-14 -0.001668  0.002916 -0.005427  0.005388   -0.000695   0.000159\n\nDaily Five-Factor Summary Statistics:\n        mkt_excess          smb          hml          rmw          cma\ncount  3126.000000  3126.000000  3126.000000  3126.000000  3126.000000\nmean     -0.000479     0.000484     0.000549    -0.000136     0.000413\nstd       0.011269     0.008033     0.008312     0.008538     0.006756\nmin      -0.070268    -0.036283    -0.039155    -0.154013    -0.047698\n25%      -0.005074    -0.004122    -0.003681    -0.004212    -0.003364\n50%       0.000350     0.000105     0.000384     0.000030     0.000162\n75%       0.005531     0.004358     0.004819     0.004036     0.003800\nmax       0.043386     0.060307     0.086269     0.102001     0.089907\n\n\n\n\n11.7.9 Saving Daily Factors\n\nfactors_ff3_daily.to_sql(\n    name=\"factors_ff3_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nfactors_ff5_daily.to_sql(\n    name=\"factors_ff5_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Daily factor data saved to database.\")\n\nDaily factor data saved to database.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#factor-validation-and-diagnostics",
    "href": "12_fama_french.html#factor-validation-and-diagnostics",
    "title": "11  Fama-French Factors",
    "section": "11.8 Factor Validation and Diagnostics",
    "text": "11.8 Factor Validation and Diagnostics\n\n# Verify all tables are in database\nprint(\"\\n\" + \"=\"*50)\nprint(\"DATABASE SUMMARY\")\nprint(\"=\"*50)\n\ntables = [\"factors_ff3_monthly\", \"factors_ff5_monthly\", \n          \"factors_ff3_daily\", \"factors_ff5_daily\"]\n\nfor table in tables:\n    df = pd.read_sql_query(f\"SELECT COUNT(*) as n FROM {table}\", con=tidy_finance)\n    print(f\"{table}: {df['n'].iloc[0]:,} observations\")\n\n# Correlation check: Monthly vs Daily (aggregated)\nprint(\"\\n\" + \"=\"*50)\nprint(\"MONTHLY VS DAILY CONSISTENCY CHECK\")\nprint(\"=\"*50)\n\nfactors_daily_agg = (factors_ff3_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .groupby(\"year_month\")[[\"mkt_excess\", \"smb\", \"hml\"]]\n    .sum()\n    .reset_index()\n)\n\nfactors_monthly_check = (factors_ff3_monthly\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n)\n\ncomparison = factors_monthly_check.merge(\n    factors_daily_agg, on=\"year_month\", suffixes=(\"_monthly\", \"_daily\")\n)\n\nfor factor in [\"mkt_excess\", \"smb\", \"hml\"]:\n    corr = comparison[f\"{factor}_monthly\"].corr(comparison[f\"{factor}_daily\"])\n    print(f\"{factor}: Monthly-Daily correlation = {corr:.4f}\")\n\n\n==================================================\nDATABASE SUMMARY\n==================================================\nfactors_ff3_monthly: 150 observations\nfactors_ff5_monthly: 150 observations\nfactors_ff3_daily: 3,126 observations\nfactors_ff5_daily: 3,126 observations\n\n==================================================\nMONTHLY VS DAILY CONSISTENCY CHECK\n==================================================\nmkt_excess: Monthly-Daily correlation = 0.9980\nsmb: Monthly-Daily correlation = 0.9953\nhml: Monthly-Daily correlation = 0.9936\n\n\n\n11.8.1 Cumulative Factor Returns\nWe visualize the cumulative performance of each factor to assess whether the factors generate meaningful premiums over time.\n\n# Compute cumulative returns\nfactors_cumulative = (factors_ff5_monthly\n    .set_index(\"date\")\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .add(1)\n    .cumprod()\n)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nfactors_cumulative.plot(ax=ax)\nax.set_title(\"Cumulative Factor Returns (Vietnam)\")\nax.set_xlabel(\"\")\nax.set_ylabel(\"Growth of $1\")\nax.legend(title=\"Factor\")\nax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 11.1: Cumulative returns of Fama-French factors for the Vietnamese market. The figure shows the growth of $1 invested in each factor portfolio.\n\n\n\n\n\n\n\n11.8.2 Average Factor Premiums\nWe compute annualized average factor premiums and their statistical significance.\n\n# Annualized average returns (monthly returns * 12)\nfactor_premiums = (factors_ff5_monthly\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .mean() * 12 * 100  # Annualized percentage\n)\n\n# Standard errors\nfactor_se = (factors_ff5_monthly\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .std() / np.sqrt(len(factors_ff5_monthly)) * np.sqrt(12) * 100\n)\n\n# T-statistics\nfactor_tstat = factor_premiums / factor_se\n\nprint(\"Annualized Factor Premiums (%):\")\nprint(factor_premiums.round(2))\n\nprint(\"\\nT-Statistics:\")\nprint(factor_tstat.round(2))\n\nAnnualized Factor Premiums (%):\nmkt_excess   -12.09\nsmb            9.19\nhml           13.75\nrmw           -5.69\ncma            9.94\ndtype: float64\n\nT-Statistics:\nmkt_excess    -7.30\nsmb            7.76\nhml            9.38\nrmw           -4.22\ncma           10.49\ndtype: float64\n\n\n\n\n11.8.3 Comparing Monthly and Daily Factors\nWe verify consistency between monthly and daily factors by computing correlations.\n\n# Aggregate daily factors to monthly for comparison\nfactors_daily_monthly = (factors_ff5_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .groupby(\"year_month\")\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .sum()  # Sum daily returns to get monthly\n    .reset_index()\n)\n\n# Merge with actual monthly factors\ncomparison = (factors_ff5_monthly\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .merge(\n        factors_daily_monthly,\n        on=\"year_month\",\n        suffixes=(\"_monthly\", \"_daily\")\n    )\n)\n\n# Correlations\nfor factor in [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]:\n    corr = comparison[f\"{factor}_monthly\"].corr(comparison[f\"{factor}_daily\"])\n    print(f\"{factor}: Monthly-Daily correlation = {corr:.4f}\")\n\nmkt_excess: Monthly-Daily correlation = 0.9980\nsmb: Monthly-Daily correlation = 0.9950\nhml: Monthly-Daily correlation = 0.9948\nrmw: Monthly-Daily correlation = 0.9929\ncma: Monthly-Daily correlation = 0.9884",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#key-takeaways",
    "href": "12_fama_french.html#key-takeaways",
    "title": "11  Fama-French Factors",
    "section": "11.9 Key Takeaways",
    "text": "11.9 Key Takeaways\n\nFactor Models Explained: The Fama-French three-factor model adds size (SMB) and value (HML) factors to the CAPM, while the five-factor model further includes profitability (RMW) and investment (CMA) factors.\nConstruction Methodology: Factors are constructed through double-sorted portfolios with careful attention to timing. Portfolios are formed in July using accounting data from the prior fiscal year to ensure information was publicly available.\nIndependent vs. Dependent Sorts: The three-factor model uses independent sorts on size and book-to-market, creating a 2×3 grid. The five-factor model uses dependent sorts where characteristics are sorted within size groups.\nValue-Weighted Returns: Portfolio returns are computed using value-weighting with lagged market capitalization to avoid look-ahead bias.\nDaily Factors: Daily factors use the same annual portfolio assignments but compute returns at daily frequency, enabling higher-frequency applications like daily beta estimation.\nMarket Factor: The market factor is computed independently as the value-weighted return of all stocks minus the risk-free rate.\nValidation: Factor quality can be assessed through characteristic monotonicity, portfolio diversification, cumulative returns, and consistency between daily and monthly frequencies.\nVietnamese Market Adaptation: While following the original Fama-French methodology, we adapt for Vietnamese market characteristics including VAS accounting standards, reporting timelines, and currency units.\n\n\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock returns.” The Journal of Finance 47 (2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "13_fama_macbeth.html",
    "href": "13_fama_macbeth.html",
    "title": "12  Fama-MacBeth Regressions",
    "section": "",
    "text": "12.1 The Econometric Framework\nIn this chapter, we delve into the implementation of the Fama and MacBeth (1973) regression approach, a cornerstone of empirical asset pricing. While portfolio sorts provide a robust, non-parametric view of the relationship between characteristics and returns, they struggle when we need to control for multiple factors simultaneously. For instance, in the Vietnamese stock market (HOSE and HNX), small-cap stocks often exhibit high illiquidity. Does the “Size effect” exist because small stocks are risky, or simply because they are illiquid? Fama-MacBeth (FM) regressions allow us to disentangle these effects in a linear framework.\nWe will implement a version of the FM procedure, accounting for:\nThe Fama-MacBeth procedure is essentially a two-step filter that separates the cross-sectional variation in returns from the time-series variation.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "13_fama_macbeth.html#the-econometric-framework",
    "href": "13_fama_macbeth.html#the-econometric-framework",
    "title": "12  Fama-MacBeth Regressions",
    "section": "",
    "text": "12.1.1 Intuition: Why not Panel OLS?\nA naive approach would be to pool all data (\\(N\\) stocks \\(\\times\\) \\(T\\) months) and run a single Ordinary Least Squares (OLS) regression:\n\\[\nr_{i,t+1} = \\alpha + \\beta_{i,t} \\lambda + \\epsilon_{i,t+1}\n\\]\nHowever, this assumes that the error terms \\(\\epsilon_{i,t+1}\\) are independent across firms. In reality, stock returns are highly cross-sectionally correlated (if the VN-Index crashes, most stocks fall together). A pooled OLS would underestimate the standard errors, leading to “false positive” discoveries of risk factors. Fama-MacBeth solves this by running \\(T\\) separate cross-sectional regressions, effectively treating each month as a single independent observation of the risk premium.\n\n\n12.1.2 Mathematical Derivation\n\n12.1.2.1 Step 1: Cross-Sectional Regressions\nFor each month \\(t\\), we estimate the premium \\(\\lambda_{k,t}\\) for \\(K\\) factors. Let \\(r_{i,t+1}\\) be the excess return of asset \\(i\\) at time \\(t+1\\). Let \\(\\boldsymbol{\\beta}_{i,t}\\) be a vector of \\(K\\) characteristics (e.g., Market Beta, Book-to-Market, Size) known at time \\(t\\).\nThe model for a specific month \\(t\\) is: \\[\n\\mathbf{r}_{t+1} = \\mathbf{X}_t \\boldsymbol{\\lambda}_{t+1} + \\boldsymbol{\\alpha}_{t+1} + \\boldsymbol{\\epsilon}_{t+1}\n\\]\nWhere:\n\n\\(\\mathbf{r}_{t+1}\\) is an \\(N \\times 1\\) vector of returns.\n\\(\\mathbf{X}_t\\) is an \\(N \\times (K+1)\\) matrix of factor exposures (including a column of ones for the intercept).\n\\(\\boldsymbol{\\lambda}_{t+1}\\) is the vector of risk premiums realized in month \\(t+1\\).\n\nTo use Weighted Least Squares (WLS), We define a weighting matrix \\(\\mathbf{W}_t\\) (typically diagonal with market capitalizations). The estimator for month \\(t\\) is: \\[\n\\hat{\\boldsymbol{\\lambda}}_{t+1} = (\\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{X}_t)^{-1} \\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{r}_{t+1}\n\\]\n\n\n12.1.2.2 Step 2: Time-Series Aggregation\nWe now have a time-series of \\(T\\) estimates: \\(\\hat{\\lambda}_1, \\hat{\\lambda}_2, \\dots, \\hat{\\lambda}_T\\). The final estimate of the risk premium is the time-series average: \\[\n\\hat{\\lambda}_k = \\frac{1}{T} \\sum_{t=1}^T \\hat{\\lambda}_{k,t}\n\\]\nThe standard error is derived from the standard deviation of these monthly estimates: \\[\n\\sigma(\\hat{\\lambda}_k) = \\sqrt{\\frac{1}{T^2} \\sum_{t=1}^T (\\hat{\\lambda}_{k,t} - \\hat{\\lambda}_k)^2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "13_fama_macbeth.html#data-preparation",
    "href": "13_fama_macbeth.html#data-preparation",
    "title": "12  Fama-MacBeth Regressions",
    "section": "12.2 Data Preparation",
    "text": "12.2 Data Preparation\nWe utilize data from our local SQLite database. In Vietnam, the fiscal year typically ends in December, and audited reports are required by April. To ensure no look-ahead bias, we lag accounting data (Book Equity) to match returns starting in July (a 6-month conservative lag, similar to Fama-French, but adapted for Vietnamese reporting delays).\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom pandas.tseries.offsets import MonthEnd\n\n# Connect to the Vietnamese data\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\n# Load Monthly Prices (HOSE & HNX)\nprices_monthly = pd.read_sql_query(\n  sql=\"SELECT symbol, date, ret_excess, mktcap, mktcap_lag FROM prices_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\n# Load Book Equity (derived from Vietnamese Financial Statements)\ncomp_vn = pd.read_sql_query(\n  sql=\"SELECT datadate, symbol, be FROM comp_vn\",\n  con=tidy_finance,\n  parse_dates={\"datadate\"}\n)\n\n# Load Rolling Market Betas (Pre-calculated in Chapter 'Beta Estimation')\nbeta_monthly = pd.read_sql_query(\n  sql=\"SELECT symbol, date, beta FROM beta_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nWe construct our testing characteristics:\n\n(Market Beta): The sensitivity to the VN-Index.\nSize (ln(ME)): The natural log of market capitalization.\nValue (BM): The ratio of Book Equity to Market Equity.\n\n\n# Prepare Characteristics\ncharacteristics = (\n    comp_vn\n    # Align reporting date to month end\n    .assign(date=lambda x: pd.to_datetime(x[\"datadate\"]) + MonthEnd(0))\n    # Merge with price data to get Market Cap at fiscal year end\n    .merge(prices_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n    .merge(beta_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n    .assign(\n        # Compute Book-to-Market\n        bm=lambda x: x[\"be\"] / x[\"mktcap\"],\n        log_mktcap=lambda x: np.log(x[\"mktcap\"]),\n        # Create sorting date: Financials valid from July of year t+1\n        sorting_date=lambda x: x[\"date\"] + pd.DateOffset(months=6) + MonthEnd(0),\n    )\n    .get([\"symbol\", \"bm\", \"beta\", \"sorting_date\"]) \n    .dropna()\n)\n\ncharacteristics.head()\n\n\n\n\n\n\n\n\nsymbol\nbm\nbeta\nsorting_date\n\n\n\n\n8729\nVTV\n7.034945e+08\n0.847809\n2017-06-30\n\n\n8732\nMTG\n2.670306e+09\n1.140066\n2017-06-30\n\n\n8739\nMKV\n6.505031e+08\n-0.448319\n2017-06-30\n\n\n8740\nMIC\n1.243127e+09\n0.772140\n2017-06-30\n\n\n8742\nMCP\n6.657350e+08\n0.348139\n2017-06-30\n\n\n\n\n\n\n\n\n# Merge back to monthly return panel\ndata_fm = (prices_monthly\n  .merge(characteristics, \n         left_on=[\"symbol\", \"date\"], \n         right_on=[\"symbol\", \"sorting_date\"], \n         how=\"left\")\n#   .merge(beta_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n  .sort_values([\"symbol\", \"date\"])\n)\n\n# Forward fill characteristics for 12 months (valid until next report)\ndata_fm[[\"bm\"]] = data_fm.groupby(\"symbol\")[[\"bm\"]].ffill(limit=12)\n\n# Log Market Cap is updated monthly\ndata_fm[\"log_mktcap\"] = np.log(data_fm[\"mktcap\"])\n\n# Lead returns: We use characteristics at t to predict return at t+1\ndata_fm[\"ret_excess_lead\"] = data_fm.groupby(\"symbol\")[\"ret_excess\"].shift(-1)\n\n# Cleaning: Remove rows with missing future returns or characteristics\ndata_fm = data_fm.dropna(subset=[\"ret_excess_lead\", \"beta\", \"log_mktcap\", \"bm\"])\n\nprint(data_fm.head())\n\nprint(f\"Data ready: {len(data_fm):,} observations from {data_fm.date.min().date()} to {data_fm.date.max().date()}\")\n\n    symbol       date  ret_excess       mktcap   mktcap_lag            bm  \\\n163    AAA 2017-06-30    0.129454  2078.455619  1834.816104  7.929854e+08   \n175    AAA 2018-06-30   -0.067690  2758.426126  2948.159140  8.161755e+08   \n187    AAA 2019-06-30    0.030469  3141.519560  3038.799575  1.389438e+09   \n199    AAA 2020-06-30   -0.035462  2311.250278  2387.972279  1.497272e+09   \n211    AAA 2021-06-30    0.275355  5423.280296  4241.283308  1.456989e+09   \n\n         beta sorting_date  log_mktcap  ret_excess_lead  \n163  1.479060   2017-06-30    7.639380        -0.051090  \n175  1.090411   2018-06-30    7.922416        -0.095926  \n187  1.099956   2019-06-30    8.052462        -0.027856  \n199  0.954144   2020-06-30    7.745544        -0.098769  \n211  1.245004   2021-06-30    8.598456        -0.175128  \nData ready: 5,075 observations from 2017-06-30 to 2023-06-30",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "13_fama_macbeth.html#step-1-cross-sectional-regressions-with-wls",
    "href": "13_fama_macbeth.html#step-1-cross-sectional-regressions-with-wls",
    "title": "12  Fama-MacBeth Regressions",
    "section": "12.3 Step 1: Cross-Sectional Regressions with WLS",
    "text": "12.3 Step 1: Cross-Sectional Regressions with WLS\nHou, Xue, and Zhang (2020) argue that micro-cap stocks distorts inference because they have high transaction costs and idiosyncratic volatility. In Vietnam, this is exacerbated by “penny stock” speculation.\nWe implement Weighted Least Squares (WLS) where weights are the market capitalization of the prior month. This tests if the factors are priced in the investable universe, not just the equal-weighted average of tiny stocks.\n\ndef run_cross_section(df):\n    # Standardize inputs for numerical stability\n    # Note: We do NOT standardize the dependent variable (returns)\n    # We standardize regressors to interpret coefficients as \"per 1 SD change\" if desired,\n    # BUT for pure risk premium estimation, we usually keep raw units.\n    # Here we use raw units to interpret lambda as % return per unit of characteristic.\n    \n    # Define Weighted Least Squares\n    model = smf.wls(\n        formula=\"ret_excess_lead ~ beta + log_mktcap + bm\",\n        data=df,\n        weights=df[\"mktcap_lag\"] # Weight by size\n    )\n    results = model.fit()\n    \n    return results.params\n\n# Apply to every month\nrisk_premiums = (data_fm\n  .groupby(\"date\")\n  .apply(run_cross_section)\n  .reset_index()\n)\n\nprint(risk_premiums.head())\n\n        date  Intercept      beta  log_mktcap            bm\n0 2017-06-30  -0.089116 -0.063799    0.010284  2.897813e-11\n1 2018-06-30  -0.023221 -0.008252    0.001890  1.377518e-11\n2 2019-06-30  -0.079373  0.035622    0.006224 -8.139910e-12\n3 2020-06-30  -0.031213 -0.114968    0.008999 -2.306768e-11\n4 2021-06-30   0.081397 -0.011407   -0.007330 -5.211290e-11",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "13_fama_macbeth.html#step-2-time-series-aggregation-hypothesis-testing",
    "href": "13_fama_macbeth.html#step-2-time-series-aggregation-hypothesis-testing",
    "title": "12  Fama-MacBeth Regressions",
    "section": "12.4 Step 2: Time-Series Aggregation & Hypothesis Testing",
    "text": "12.4 Step 2: Time-Series Aggregation & Hypothesis Testing\nWe now possess the time-series of risk premiums. We calculate the arithmetic mean and the -statistics.\nCrucially, we use Newey-West (HAC) standard errors. Risk premiums in Vietnam often exhibit autocorrelation (momentum in factor performance). A simple standard error formula would be invalid.\n\ndef calculate_fama_macbeth_stats(df, lags=6):\n    summary = []\n    \n    for col in [\"Intercept\", \"beta\", \"log_mktcap\", \"bm\"]:\n        series = df[col]\n        \n        # 1. Point Estimate (Average Risk Premium)\n        mean_premium = series.mean()\n        \n        # 2. Newey-West Standard Error\n        # We regress the series on a constant (ones) to get the SE of the mean\n        exog = sm.add_constant(np.ones(len(series)))\n        nw_model = sm.OLS(series, exog).fit(\n            cov_type='HAC', cov_kwds={'maxlags': lags}\n        )\n\n        se = nw_model.bse.iloc[0]\n        t_stat = nw_model.tvalues.iloc[0]\n        \n        summary.append({\n            \"Factor\": col,\n            \"Premium (%)\": mean_premium * 100,\n            \"Std Error\": se * 100,\n            \"t-statistic\": t_stat,\n            \"Significance\": \"*\" if abs(t_stat) &gt; 1.96 else \"\"\n        })\n        \n    return pd.DataFrame(summary)\n\nprice_of_risk = calculate_fama_macbeth_stats(risk_premiums)\nprint(price_of_risk.round(4))\n\n       Factor  Premium (%)  Std Error  t-statistic Significance\n0   Intercept      -1.8174     1.9117      -0.9507             \n1        beta      -1.7859     1.0407      -1.7161             \n2  log_mktcap       0.2347     0.2048       1.1457             \n3          bm      -0.0000     0.0000      -0.0928             \n\n\n\n12.4.1 Visualizing the Time-Varying Risk Premium\nOne major advantage of the FM approach is that we can inspect the volatility of the risk premiums over time. In Vietnam, we expect the “Size” premium to be highly volatile during periods of retail liquidity injection (e.g., 2020-2021).\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# Calculate cumulative returns of the factors (as if they were tradable portfolios)\ncumulative_premiums = (risk_premiums\n    .set_index(\"date\")\n    .drop(columns=[\"Intercept\"])\n    .cumsum()\n)\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncumulative_premiums.plot(ax=ax, linewidth=2)\nax.set_title(\"Cumulative Risk Premiums in Vietnam (Fama-MacBeth)\", fontsize=14)\nax.set_ylabel(\"Cumulative Coefficient Return\")\nax.legend(title=\"Factor\")\nax.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 12.1: Cumulative Risk Premiums in Vietnam.\n\n\n\n\n\n\nMarket Beta: In many empirical studies (including the US), the market beta premium is often insignificant or even negative (the “Betting Against Beta” anomaly). In Vietnam, if the -stat is , it implies the CAPM does not explain the cross-section of returns.\nSize (Log Mktcap): A negative coefficient confirms the “Size Effect”—smaller firms have higher expected returns. However, using WLS often weakens this result compared to OLS, suggesting the size premium is concentrated in micro-caps.\nValue (BM): A positive coefficient confirms the Value premium. In Vietnam, value stocks (high B/M) often outperform growth stocks, particularly in the manufacturing and banking sectors.\n\nFigure 12.1 plots the cumulative sum of the monthly Fama MacBeth risk premium estimates for beta, size, and value. Because these lines cumulate estimated cross sectional prices of risk rather than actual portfolio returns, the figure should be interpreted as showing the time variation and persistence of estimated premia, not investable performance.\nThe beta premium displays a clear regime shift around 2020, with a sharp decline that only partially reverses afterward. This pattern suggests that the pricing of systematic risk in Vietnam is unstable over short samples and may be heavily influenced by episodic market conditions such as the post COVID retail trading boom. The size premium is comparatively smoother but small in magnitude, indicating only weak and time varying evidence that firm size is priced in the cross section during this period. The value premium remains close to zero throughout, implying little consistent cross sectional reward to high book to market firms in this sample window.\nOverall, the figure highlights that estimated risk premia in the Vietnamese market are highly time varying and sensitive to specific macro and market regimes, reinforcing the need for caution when drawing conclusions from short samples.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "13_fama_macbeth.html#sanity-checks",
    "href": "13_fama_macbeth.html#sanity-checks",
    "title": "12  Fama-MacBeth Regressions",
    "section": "12.5 Sanity Checks",
    "text": "12.5 Sanity Checks\n\n12.5.1 Time-Series Volatility Check\nFama-MacBeth relies on the assumption that the risk premium varies over time. If your bm premium is truly near zero every month, the method fails.\nAction: Plot the time series of the estimated coefficients . You want to see “noise” around a mean. If you see a flat line or a single massive spike, your data is corrupted.\n\nimport matplotlib.pyplot as plt\n\n# Plot the time series of the BM risk premium\nfig, ax = plt.subplots(figsize=(10, 5))\nrisk_premiums[\"bm\"].plot(ax=ax, title=\"Monthly Value Premium (BM) Coefficient\")\nax.axhline(0, color=\"black\", linestyle=\"--\")\nax.set_ylabel(\"Slope Coefficient\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n12.5.2 The “Predicted vs. Realized” Scatter Plot\nThe ultimate test of an asset pricing model is whether it can price the test assets. If you group your stocks into portfolios (e.g., 25 portfolios sorted by Size and Beta), the model’s predicted return should match the actual average return.\nAction: Compare the model’s prediction against reality.\n\nCalculate the average realized return for each stock .\nCalculate the predicted return: .\nScatter plot them. They should align along the 45-degree line.\n\n\n# Calculate average characteristics for each stock\nstock_means = data_fm.groupby(\"symbol\")[[\"ret_excess_lead\", \"beta\", \"log_mktcap\", \"bm\"]].mean()\n\n# Note: Ensure you grab the 'Premium (%)' divided by 100 if it was scaled\n# Or use the raw mean from risk_premiums\nlambda_beta = risk_premiums[\"beta\"].mean()\nlambda_size = risk_premiums[\"log_mktcap\"].mean()\nlambda_bm = risk_premiums[\"bm\"].mean()\nconst = risk_premiums[\"Intercept\"].mean()\n\nstock_means[\"predicted_ret\"] = (\n    const +\n    stock_means[\"beta\"] * lambda_beta + \n    stock_means[\"log_mktcap\"] * lambda_size + \n    stock_means[\"bm\"] * lambda_bm\n)\n\n# Plot\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(stock_means[\"predicted_ret\"], stock_means[\"ret_excess_lead\"], alpha=0.3)\nax.plot([0, 0.05], [0, 0.05], color='r', linestyle='--') # 45-degree line\nax.set_xlabel(\"Predicted Average Return\")\nax.set_ylabel(\"Realized Average Return\")\nax.set_title(\"Model Fit: Predicted vs Realized\")\nplt.show()\n\n\n\n\n\n\n\n\nThe scatter plot compares each stock’s average realized excess return to the return predicted by the estimated risk premia and its characteristics. If the model priced assets well, the points would cluster around the 45 degree line. Instead, the cloud is centered near zero on the horizontal axis, while realized returns vary widely on the vertical axis. The fitted line is nearly flat, indicating that differences in predicted returns explain very little of the variation in realized returns across stocks.\nThis pattern implies that, over this sample period, the estimated factor premia have weak cross sectional explanatory power at the individual stock level. Such weak fit is common in emerging markets and in short samples, where idiosyncratic volatility, thin trading, and episodic market regimes dominate the cross section of returns. It also reflects the well known fact that Fama MacBeth tests tend to have low power when applied to individual securities rather than diversified portfolios.\n\n\n12.5.3 Correlation of Characteristics (Multicollinearity)\nIn Vietnam, large-cap stocks (high log_mktcap) are often the ones with high Book-to-Market ratios (banks/utilities) or specific Betas. If your factors are highly correlated, the Fama-MacBeth coefficients will be unstable and insignificant (low t-stats), even if the factors actually matter.\nAction: Check the cross-sectional correlation.\n\n# Check correlation of the characteristics\ncorr_matrix = data_fm[[\"beta\", \"log_mktcap\", \"bm\"]].corr()\nprint(corr_matrix)\n\n                beta  log_mktcap        bm\nbeta        1.000000    0.392776 -0.033748\nlog_mktcap  0.392776    1.000000 -0.203307\nbm         -0.033748   -0.203307  1.000000\n\n\nInterpretation:\n\nIf correlation &gt; 0.7 (absolute value), the regression struggles to distinguish between the two factors.\nFor example, if Size and Liquidity are -0.8 correlated, the model cannot tell which one is driving the return, often resulting in both having insignificant t-stats.\n\n\n\n\n\n\n\nFama, Eugene F., and James D. MacBeth. 1973. “Risk, return, and equilibrium: Empirical tests.” Journal of Political Economy 81 (3): 607–36. https://doi.org/10.1086/260061.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2020. “Replicating anomalies.” Review of Financial Studies 33 (5): 2019–2133. https://doi.org/10.1093/rfs/hhy131.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "20_conclusion.html",
    "href": "20_conclusion.html",
    "title": "13  Conclusion",
    "section": "",
    "text": "13.1 What you should take away\nEmpirical finance in emerging and frontier markets is often judged less by the elegance of an estimator than by the credibility of its inputs and the transparency of its decisions. Vietnam makes this point vividly: trading venues and regulatory regimes have evolved quickly, firm coverage can be uneven across time, corporate actions need careful treatment, and accounting conventions require close attention to timing and comparability. Those features do not prevent high-quality research; they simply shift the center of gravity toward reproducible data engineering, auditable transformations, and clear identification of assumptions.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "20_conclusion.html#what-you-should-take-away",
    "href": "20_conclusion.html#what-you-should-take-away",
    "title": "13  Conclusion",
    "section": "",
    "text": "13.1.1 Reproducibility is an identification strategy\nIn textbook settings, identification focuses on variation and exogeneity. In real-world market data, identification also depends on whether your dataset is the same dataset when you rerun the work next month or next year. The practical discipline of versioned inputs, deterministic transformations, and documented filters reduces the scope for accidental \\(p\\)-hacking and silent sample drift (e.g., survivorship bias from symbol changes or late-arriving delistings). Reproducible workflows are not administrative overhead; they are a commitment device that makes results more trustworthy and easier to challenge constructively (Peng 2011; Sandve et al. 2013).\n\n\n13.1.2 Vietnam rewards “microstructure humility”\nThe chapters on returns, beta estimation, and factor construction emphasized that naïve carryover of developed-market defaults can be costly. Thin trading, price limits, lot-size rules, and regime changes mean that decisions like (i) return interval, (ii) stale-price handling, (iii) corporate-action adjustment, and (iv) portfolio formation frequency can materially change inference. This is not a Vietnam-only phenomenon, but it is more visible there, and therefore a useful laboratory for best practices in emerging markets.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "20_conclusion.html#a-reproducibility-checklist-you-can-actually-use",
    "href": "20_conclusion.html#a-reproducibility-checklist-you-can-actually-use",
    "title": "13  Conclusion",
    "section": "13.2 A reproducibility checklist you can actually use",
    "text": "13.2 A reproducibility checklist you can actually use\nThe list below is designed to be operational: each item can be verified in a repository review.\n\n\n\nTable 13.1: Reproducibility deliverables for research\n\n\n\n\n\n\n\n\n\n\nDeliverable\nWhat “done” looks like\nWhere it lives\n\n\n\n\nDeterministic transforms\nSame raw inputs yield identical normalized outputs\nR/transform_*.R (or python/transform_*.py)\n\n\nTest suite\nCoverage, identity, and corporate-action tests run in CI\ntests/ + CI config\n\n\nData dictionary\nTables/fields documented with units, timing, and keys\ndocs/dictionary.qmd\n\n\nResearch log\nAll key design choices recorded (filters, winsorization, periods)\nnotes/research_log.md\n\n\nArtifact registry\nEvery figure/table has a script and a checksum\nartifacts/manifest.json",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bali, Turan G, Robert F Engle, and Scott Murray. 2016. Empirical asset pricing: The cross section of stock\nreturns. John Wiley & Sons. https://doi.org/10.1002/9781118445112.stat07954.\n\n\nCarhart, Mark M. 1997. “On persistence in\nmutual fund performance.” The Journal of\nFinance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock\nreturns.” The Journal of Finance 47\n(2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal\nof Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nFama, Eugene F., and James D. MacBeth. 1973. “Risk, return, and equilibrium: Empirical\ntests.” Journal of Political Economy\n81 (3): 607–36. https://doi.org/10.1086/260061.\n\n\nFrazzini, Andrea, and Lasse Heje Pedersen. 2014. “Betting against beta.” Journal of\nFinancial Economics 111 (1): 1–25. https://doi.org/10.1016/j.jfineco.2013.10.005.\n\n\nGentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for\nthe Social Sciences: A Practitioner’s Guide.” Working Paper,\nUniversity of Chicago.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2014. “Digesting anomalies: An investment\napproach.” Review of Financial\nStudies 28 (3): 650–705. https://doi.org/10.1093/rfs/hhu068.\n\n\n———. 2020. “Replicating\nanomalies.” Review of Financial\nStudies 33 (5): 2019–2133. https://doi.org/10.1093/rfs/hhy131.\n\n\nJagannathan, Ravi, and Zhenyu Wang. 1996. “The conditional CAPM and the cross-section of expected\nreturns.” The Journal of Finance 51\n(1): 3–53. https://doi.org/10.2307/2329301.\n\n\nLintner, John. 1965. “Security prices, risk,\nand maximal gains from diversification.” The\nJournal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMarkowitz, Harry. 1952. “Portfolio\nselection.” The Journal of Finance 7\n(1): 77–91. https://doi.org/10.1111/j.1540-6261.1952.tb01525.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital\nasset market.” Econometrica 34 (4):\n768–83. https://doi.org/10.2307/1910098.\n\n\nNewey, Whitney K., and Kenneth D. West. 1987. “A simple, positive semi-definite, heteroskedasticity and\nautocorrelation consistent covariance Matrix.”\nEconometrica 55 (3): 703–8. http://www.jstor.org/stable/1913610.\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig.\n2013. “Ten Simple Rules for Reproducible Computational\nResearch.” PLoS Computational Biology 9 (10): e1003285.\n\n\nSharpe, William F. 1964. “Capital asset\nprices: A theory of market equilibrium under conditions of risk\n.” The Journal of Finance 19 (3):\n425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.\n\n\nVilhuber, Lars. 2020. “Reproducibility and Replicability in\nEconomics.” Harvard Data Science Review 2 (4): 1–39.",
    "crumbs": [
      "References"
    ]
  }
]