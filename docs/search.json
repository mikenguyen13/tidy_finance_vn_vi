[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applying Tidy Finance with Python to Vietnam",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Applying Tidy Finance with Python to Vietnam",
    "section": "Motivation",
    "text": "Motivation\nEmpirical finance has undergone a fundamental transformation over the past two decades. Advances in computational capacity, open-source statistical software, and data availability have reshaped how financial research is conducted, evaluated, and disseminated. Increasingly, credible empirical work is expected to be transparent, replicable, and extensible, with results generated through scripted workflows rather than manual intervention. Reproducibility, defined as the ability for independent researchers to regenerate empirical results using the same data and methods, has thus become a core norm in modern financial economics.\nDespite this progress, the adoption of reproducible research practices has been uneven across markets. In developed financial systems, particularly those with long-established databases and standardized reporting regimes, reproducible empirical workflows are now commonplace. In contrast, research on emerging and frontier markets frequently relies on fragmented datasets, undocumented data cleaning procedures, and implicit institutional assumptions that are difficult to verify or extend. As a result, empirical findings in these markets are often fragile, non-comparable across studies, and costly to update as new data become available.\nThis book addresses that gap.\nIt develops a reproducible empirical finance framework designed explicitly for emerging and frontier markets, using Vietnam as a primary empirical case. Rather than adapting developed-market research pipelines post hoc, the book begins from the institutional and data realities of a fast-growing, retail-dominated, regulation-intensive market and builds methodological solutions accordingly. The objective is not merely to analyze Vietnam’s financial markets, but to demonstrate how reproducible finance principles, as developed in the Tidy Finance framework, can be extended, stress-tested, and refined in environments characterized by data scarcity, institutional heterogeneity, and rapid structural change.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#why-emerging-markets-require-different-empirical-infrastructure",
    "href": "index.html#why-emerging-markets-require-different-empirical-infrastructure",
    "title": "Applying Tidy Finance with Python to Vietnam",
    "section": "Why Emerging Markets Require Different Empirical Infrastructure",
    "text": "Why Emerging Markets Require Different Empirical Infrastructure\nMuch of modern empirical finance implicitly assumes the existence of stable, high-frequency, institutionally harmonized datasets. These assumptions are rarely stated, yet they are deeply embedded in standard research designs: survivorship-free security histories, consistent accounting standards, unrestricted trading mechanisms, and deep institutional liquidity.\nEmerging and frontier markets challenge each of these assumptions.\nIn Vietnam, as in many comparable economies, equity markets exhibit binding daily price limits, episodic trading halts, concentrated state ownership, and a predominance of retail investors. Financial disclosures reflect local accounting standards and evolving regulatory frameworks. Corporate actions are frequent, inconsistently documented, and occasionally revised ex post.\nThese characteristics are not inconveniences to be eliminated through aggressive data cleaning. They shape return dynamics, risk premia, factor construction, and statistical inference itself. An empirical framework that ignores these institutional features risks producing results that are internally inconsistent or externally misleading. A reproducible approach for emerging markets must therefore encode institutional context directly into data schemas, transformation logic, and modeling choices.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#reproducibility-as-a-research-design-principle",
    "href": "index.html#reproducibility-as-a-research-design-principle",
    "title": "Applying Tidy Finance with Python to Vietnam",
    "section": "Reproducibility as a Research Design Principle",
    "text": "Reproducibility as a Research Design Principle\nIn this book, reproducibility extends beyond the narrow notion of code availability. It is treated as an organizing principle governing the entire empirical research lifecycle.\nFirst, all datasets are constructed from raw inputs through documented, deterministic transformations, ensuring clear data provenance. Second, empirical methods are implemented in a manner that makes modeling assumptions explicit and modifiable. Third, results are generated through scripted pipelines rather than interactive analysis, guaranteeing that updates to data or parameters propagate consistently throughout the analysis. Finally, empirical designs are modular, allowing researchers to substitute markets, sample periods, or variable definitions without rewriting entire workflows.\nThis approach draws methodological inspiration from the broader reproducible research movement in economics and finance (e.g., Gentzkow and Shapiro 2014; Vilhuber 2020), while deliberately extending it beyond its original institutional and data environment. The goal is not to reproduce existing studies, but to enable new ones—particularly those that would otherwise be impractical due to fragmented data and institutional complexity.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#vietnam-as-a-case-not-an-exception",
    "href": "index.html#vietnam-as-a-case-not-an-exception",
    "title": "Applying Tidy Finance with Python to Vietnam",
    "section": "Vietnam as a Case, Not an Exception",
    "text": "Vietnam as a Case, Not an Exception\nVietnam serves as the central empirical case throughout the book, but it is not treated as an idiosyncratic exception. Instead, it is presented as a representative example of a class of markets that occupy an intermediate position between frontier and emerging status: large enough to sustain active equity trading, yet still evolving in terms of regulation, disclosure quality, and investor composition.\nBy grounding methodological development in Vietnam’s market structure, the book aims to produce insights that generalize to other contexts, including Southeast Asia, South Asia, Sub-Saharan Africa, and parts of Latin America. Each empirical chapter emphasizes which components are market-specific and which are portable, encouraging readers to adapt the framework rather than adopt it wholesale.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#data-access",
    "href": "index.html#data-access",
    "title": "Applying Tidy Finance with Python to Vietnam",
    "section": "Data Access",
    "text": "Data Access\nThe empirical analyses in this book rely on Vietnamese equity market data provided by Datacore. To ensure reproducibility while respecting data licensing constraints, we provide the following resources:\n\nSample datasets: A subset of anonymized data is available in DataCore’s Sample Dataset for readers to run example code.\nData construction scripts: All scripts used to clean and transform raw data are fully documented and available in the repository.\nReplication guidance: Readers with access to Vietnamese market data from commercial providers can use our scripts to construct equivalent datasets.\n\nFor questions about data access or replication, please contact the author.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contribution-and-audience",
    "href": "index.html#contribution-and-audience",
    "title": "Applying Tidy Finance with Python to Vietnam",
    "section": "Contribution and Audience",
    "text": "Contribution and Audience\nThis book makes three primary contributions.\nFirst, it proposes a reproducible empirical finance framework explicitly designed for emerging and frontier markets, integrating institutional detail into data construction and model design. Second, it provides original empirical evidence on asset pricing, liquidity, and market microstructure in Vietnam using consistently constructed datasets. Third, it provides publication-ready, end-to-end research workflows suitable for academic research, policy analysis, and applied finance.\nThe intended audience includes graduate students in finance and economics, academic researchers studying non-developed markets, and practitioners interested in the systematic analysis of emerging-market equities. Familiarity with basic asset pricing theory and statistical programming is assumed, but no prior experience with Vietnam or similar markets is required.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#structure-of-the-book",
    "href": "index.html#structure-of-the-book",
    "title": "Applying Tidy Finance with Python to Vietnam",
    "section": "Structure of the Book",
    "text": "Structure of the Book\nThe chapters that follow progress from data infrastructure to empirical application. The book begins with an introduction to the data sources and infrastructure used throughout, followed by chapters on institutional context, data construction, and reproducible workflow design. Subsequent chapters develop asset pricing tests, liquidity measures, and market microstructure analyses tailored to Vietnam’s equity market. Each chapter is designed to be self-contained, yet all are linked through a common data and code architecture to ensure internal consistency.\nThe book concludes by reflecting on the broader implications of reproducible empirical finance for emerging markets research and by outlining directions for future methodological and empirical work.\n\n\n\n\n\n\nGentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for the Social Sciences: A Practitioner’s Guide.” Working Paper, University of Chicago.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with r. CRC Press.\n\n\nScheuch, Christoph, Stefan Voigt, Patrick Weiss, and Christoph Frey. 2024. Tidy Finance with Python. Chapman; Hall/CRC.\n\n\nVilhuber, Lars. 2020. “Reproducibility and Replicability in Economics.” Harvard Data Science Review 2 (4): 1–39.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "00_institutional_background.html",
    "href": "00_institutional_background.html",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "",
    "text": "1.1 Evolution of Vietnam’s Equity Market\nEmpirical analysis of financial markets is inseparable from institutional context. Market design, regulatory constraints, ownership structure, and investor composition shape observed prices, volumes, and returns. In developed markets, many of these features are sufficiently stable and standardized that they fade into the background of empirical research. In emerging markets, by contrast, institutional features are often first-order determinants of empirical outcomes.\nThis chapter provides the institutional foundation for the empirical analyses developed later in the book. It describes the structure of Vietnam’s equity market, the regulatory environment governing trading and disclosure, and the characteristics of listed firms and investors. Rather than offering a purely descriptive account, the discussion emphasizes how institutional features map directly into data construction choices, modeling assumptions, and interpretation of empirical results.\nVietnam’s modern equity market is relatively young. Formal stock exchanges were established only in the early 2000s, as part of broader economic reforms aimed at transitioning from a centrally planned system toward a market-oriented economy. Since then, market capitalization, trading volume, and the number of listed firms have grown rapidly, albeit unevenly across sectors and time.\nThe pace of market development has been shaped by a combination of gradual privatization of state-owned enterprises, episodic regulatory reform, and sustained participation by retail investors. Unlike markets that evolved alongside large institutional investor bases, Vietnam’s equity market matured in an environment where individual investors dominate trading activity and informational asymmetries remain substantial.\nThese features have important empirical implications. Return dynamics may reflect behavioral trading patterns, liquidity shocks can be amplified by coordinated retail activity, and firm-level information is incorporated into prices at varying speeds. A reproducible empirical framework must therefore be capable of capturing these dynamics without imposing assumptions derived from institutionally different markets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#exchange-structure-and-trading-mechanisms",
    "href": "00_institutional_background.html#exchange-structure-and-trading-mechanisms",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.2 Exchange Structure and Trading Mechanisms",
    "text": "1.2 Exchange Structure and Trading Mechanisms\nVietnam operates multiple equity exchanges, each with distinct listing requirements and trading rules. Trading is conducted through a centralized limit order book, with price-time priority determining execution. Importantly, daily price limits constrain the maximum allowable price movement for individual securities. These limits vary by exchange and security type and are binding during periods of heightened volatility.\nPrice limits introduce mechanical truncation in observed returns, clustering at upper and lower bounds, and persistence in price movements across days. From an empirical perspective, this challenges standard assumptions about continuous price adjustment and complicates volatility estimation, momentum measurement, and event-study design.\nIn this book, price limits are treated as structural features rather than anomalies. Data pipelines explicitly preserve limit-hit indicators, and empirical models are adapted to account for constrained price dynamics. This design choice reflects a broader principle: reproducibility in emerging markets requires preserving institutional signals rather than smoothing them away.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#listing-requirements-and-firm-characteristics",
    "href": "00_institutional_background.html#listing-requirements-and-firm-characteristics",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.3 Listing Requirements and Firm Characteristics",
    "text": "1.3 Listing Requirements and Firm Characteristics\nListed firms in Vietnam exhibit substantial heterogeneity in size, ownership structure, and disclosure quality. A defining characteristic of the market is the prevalence of firms with significant state ownership, either directly or through affiliated entities. State ownership affects governance, dividend policy, risk-taking behavior, and responsiveness to market signals.\nAccounting disclosures follow Vietnamese Accounting Standards, which differ in important respects from international standards. While convergence efforts are ongoing, historical financial statements often reflect transitional rules, incomplete adoption of fair value accounting, and limited segment reporting. These features complicate cross-firm comparability and longitudinal analysis.\nFrom a reproducible research standpoint, accounting variables cannot be treated as uniform primitives. Variable definitions, reporting lags, and restatement practices must be explicitly documented and encoded into data construction logic. Later chapters demonstrate how accounting data are harmonized in a transparent, version-controlled manner without obscuring underlying institutional differences.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#investor-composition-and-trading-behavior",
    "href": "00_institutional_background.html#investor-composition-and-trading-behavior",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.4 Investor Composition and Trading Behavior",
    "text": "1.4 Investor Composition and Trading Behavior\nRetail investors dominate trading volume in Vietnam’s equity market. Institutional investors, including domestic funds and foreign participants, play a growing but still secondary role. This investor composition has implications for liquidity provision, price discovery, and market stability.\nRetail-dominated markets tend to exhibit higher turnover, episodic herding behavior, and sensitivity to non-fundamental information. These patterns affect the interpretation of empirical results, particularly in studies of short-term return predictability, volume-return relations, and volatility clustering.\nRather than assuming institutional trading as the default, this book explicitly models liquidity and trading activity in a retail-centric environment. Measures of liquidity, for example, are chosen and constructed to remain meaningful in the presence of small trade sizes, intermittent trading, and order imbalances driven by individual investors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#regulatory-environment-and-market-frictions",
    "href": "00_institutional_background.html#regulatory-environment-and-market-frictions",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.5 Regulatory Environment and Market Frictions",
    "text": "1.5 Regulatory Environment and Market Frictions\nRegulatory oversight of Vietnam’s equity market has evolved alongside market development. Trading rules, disclosure requirements, and foreign ownership limits have been periodically revised, sometimes with limited backward compatibility. Regulatory changes can induce structural breaks in data that are not immediately apparent in raw time series.\nShort-selling constraints, limited securities lending, and restrictions on derivative usage further distinguish Vietnam’s market from developed counterparts. These frictions affect arbitrage activity and the feasibility of certain trading strategies, influencing observed return patterns and factor realizations.\nA key principle of the empirical framework developed in this book is regulatory awareness. Data pipelines incorporate regulatory timelines, and empirical tests are designed to be robust to rule changes. This ensures that results are interpretable within the institutional regime in which they arise.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#implications-for-empirical-design",
    "href": "00_institutional_background.html#implications-for-empirical-design",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.6 Implications for Empirical Design",
    "text": "1.6 Implications for Empirical Design\nThe institutional features described in this chapter motivate several design choices that recur throughout the book:\n\nData preservation over simplification: Institutional constraints such as price limits and trading halts are retained and explicitly modeled.\nModular variable construction: Accounting and market variables are constructed through transparent functions that can be adjusted as standards evolve.\nRegime sensitivity: Empirical analyses are structured to detect and accommodate regulatory and structural breaks.\nContext-aware interpretation: Results are interpreted in light of market structure rather than benchmarked mechanically against developed-market findings.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#summary",
    "href": "00_institutional_background.html#summary",
    "title": "1  Institutional Background and Market Structure of Vietnam’s Equity Market",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nVietnam’s equity market combines rapid growth with distinctive institutional features that challenge conventional empirical finance methods. Price limits, retail investor dominance, state ownership, and evolving regulation shape market outcomes in ways that cannot be ignored or abstracted away. For researchers working in such environments, reproducibility requires more than clean code and documented data—it requires embedding institutional context directly into empirical design.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Institutional Background and Market Structure of Vietnam’s Equity Market</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html",
    "href": "01_working_with_stock_returns.html",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "",
    "text": "2.1 Data Access and Preparation\nThis chapter develops a practical framework for transforming raw equity price records into return series suitable for empirical financial analysis. The focus is on methodological clarity and reproducibility, with particular attention to data issues that are prevalent in emerging equity markets such as Vietnam.\nThe discussion proceeds from individual stocks to a broad market cross-section, using constituents of the VN30 index as the primary empirical setting.\nWe begin by loading the core numerical and data manipulation libraries. These provide all functionality required for return construction without relying on specialized financial wrappers.\nimport pandas as pd\nimport numpy as np\nFor this project, we retrieve our historical price data using the DataCore API. If you wish to replicate this analysis or use the dataset for your own work, you will need to access the data through their platform.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#data-access-and-preparation",
    "href": "01_working_with_stock_returns.html#data-access-and-preparation",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "",
    "text": "2.1.1 Prerequisites for API Access\nTo run the code below, you need to configure a few things first:\n\nObtain an API Key: You must subscribe to the relevant dataset on the DataCore platform to receive a unique API key.\nWhitelist Your IP Address: The API requires your IP address to be whitelisted for security.\n\nLocal Machine: If you are running this code on your personal computer, you generally need to whitelist your public IP address.\nCloud or Remote Sessions (e.g., HPC Open OnDemand): If you are using a remote server such as those provided by DataCore, the server’s IP address will change with each new session. You must retrieve the server’s private/public IP for that specific session and whitelist it in your DataCore account settings before running the script.\n\nSet Environment Variables: To keep your credentials secure, do not hardcode your API key into your scripts. Instead, save it as an environment variable named datacore_api on your machine.\n\nNote: If you only want to test the code performance, DataCore provides a preview endpoint that does not require an API key, though the data returned is limited.\n\nimport requests\nimport pandas as pd\n\nurl = \"https://gateway.datacore.vn/data/ds/preview\"\nparams = {\n    \"dataSetCode\": \"fundamental_annual\",\n    \"pageSize\": 10000 \n}\nheaders = {\n    \"Accept\": \"application/json\",\n    \"Origin\": \"https://datacore.vn\",\n    \"Referer\": \"https://datacore.vn/\"\n}\n\nresponse = requests.get(url, params=params, headers=headers)\ndata = response.json()\n\ncolumns = data['data']['fields']\nrows = data['data']['dataDetail']\n\ndf = pd.DataFrame(rows, columns=columns)\nprint(df.head())\nprint(\"Total rows:\", len(df))\n\n  symbol  year  total_current_asset ca_fin        ca_cce       ca_cash  \\\n0    CLL  2011         1.078338e+11   None  8.313178e+10  4.131776e+09   \n1    CLL  2012         2.360575e+10   None  8.003560e+09  4.003560e+09   \n2    CLL  2013         5.764370e+10   None  3.496426e+10  9.964256e+09   \n3    CLL  2014         4.973590e+10   None  1.718744e+10  1.718744e+10   \n4    CLL  2015         2.389115e+11   None  1.790364e+11  2.403638e+10   \n\n  ca_cash_inbank ca_cash_attransit  ca_cash_equivalent  ca_fin_invest  ...  \\\n0           None              None        7.900000e+10   0.000000e+00  ...   \n1           None              None        4.000000e+09   0.000000e+00  ...   \n2           None              None        2.500000e+10   0.000000e+00  ...   \n3           None              None        0.000000e+00   1.000000e+09  ...   \n4           None              None        1.550000e+11   1.000000e+09  ...   \n\n   operating_margin      roe      roa sector_pe sector_pb  sector_ps  \\\n0           0.35473  0.19560  0.10649   3.00213   0.26108    0.22170   \n1           0.45369  0.20337  0.13018   2.40335   0.26211    0.21745   \n2           0.45997  0.23459  0.16452   3.11089   0.41013    0.32436   \n3           0.40554  0.19984  0.14747   3.25886   0.46823    0.42146   \n4           0.33774  0.16525  0.12633   6.77337   0.81110    0.68401   \n\n   sector_eps  sector_ros  sector_roe  sector_roa  \n0  3341.54903     0.07385     0.08753     0.05039  \n1  4296.47005     0.09048     0.11392     0.06346  \n2  4024.74648     0.10427     0.13645     0.07415  \n3  5100.81019     0.12933     0.14956     0.08087  \n4  5216.38499     0.10098     0.11815     0.06234  \n\n[5 rows x 308 columns]\nTotal rows: 10\n\n\n\n\n2.1.2 Checking Your IP Address\nIf you need to verify the IP address of the machine running your code (to whitelist it), you can use the following Python snippets.\nTo find your Public IP:\n\nimport requests\n\ntry:\n    public_ip = requests.get(\"https://api.ipify.org\").text\n    print(f\"Public IP: {public_ip}\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"Could not retrieve Public IP: {e}\")\n\nTo find your Private IP (useful for specific remote server setups):\n\nimport socket\n\ndef get_private_ip():\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect((\"8.8.8.8\", 80))\n        private_ip = s.getsockname()[0]\n        s.close()\n        return private_ip\n    except Exception as e:\n        return f\"Error: {e}\"\n\nprint(f\"Private IP: {get_private_ip()}\")\n\n\n\n2.1.3 Fetching the Dataset\nThe following script demonstrates how to securely authenticate and paginate through the DataCore API to retrieve the full dataset_historical_price dataset.\n\n\n\n# Convert the date column to proper datetime objects\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\n# Ensure price and ratio columns are numeric before calculation\nprices[\"close_price\"] = pd.to_numeric(prices[\"close_price\"])\nprices[\"adj_ratio\"] = pd.to_numeric(prices[\"adj_ratio\"])\n\n# Calculate the adjusted close price\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\n# Rename columns to match standard conventions\nprices = prices.rename(\n    columns={\n        \"vol_total\": \"volume\",\n        \"open_price\": \"open\",\n        \"low_price\": \"low\",\n        \"high_price\": \"high\",\n        \"close_price\": \"close\",\n    }\n)\n\n# Sort the dataset logically by symbol and date\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nprint(\"Data manipulation complete. The dataset is ready for analysis.\")\n\nData manipulation complete. The dataset is ready for analysis.\n\n\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\n\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\n\nprices = prices.rename(\n    columns={\n        \"vol_total\": \"volume\",\n        \"open_price\": \"open\",\n        \"low_price\": \"low\",\n        \"high_price\": \"high\",\n        \"close_price\": \"close\",\n    }\n)\n\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nAdjusted closing prices incorporate mechanical changes due to corporate actions such as cash dividends and stock splits. Using adjusted prices ensures that subsequent return calculations reflect investor-relevant performance rather than accounting artifacts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#examining-a-single-equity",
    "href": "01_working_with_stock_returns.html#examining-a-single-equity",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.2 Examining a Single Equity",
    "text": "2.2 Examining a Single Equity\nTo ground the discussion, we isolate the trading history of a single large-cap stock, FPT, over a long sample period.\n\nimport datetime as dt\n\nstart = pd.Timestamp(\"2000-01-01\")\nend = pd.Timestamp(dt.date.today().year - 1, 12, 31)\n\n\nfpt = prices.loc[\n    (prices[\"symbol\"] == \"FPT\")\n    & (prices[\"date\"] &gt;= start)\n    & (prices[\"date\"] &lt;= end),\n    [\"date\", \"symbol\", \"volume\", \"open\", \"low\", \"high\", \"close\", \"adjusted_close\"],\n].copy()\n\nThis subset contains the standard daily market variables required for most empirical studies. Before computing returns, it is good practice to visually inspect the price series.\n\nfrom plotnine import ggplot, aes, geom_line, labs\n\n\n(\n    ggplot(fpt, aes(x=\"date\", y=\"adjusted_close\"))\n    + geom_line()\n    + labs(title=\"Adjusted price path of FPT\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 2.1: Prices are in VND, adjusted for dividend payments and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#from-prices-to-returns",
    "href": "01_working_with_stock_returns.html#from-prices-to-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.3 From Prices to Returns",
    "text": "2.3 From Prices to Returns\nMost empirical asset pricing models are formulated in terms of returns rather than price levels. The simple daily return is defined as\n\\[\nr_t = \\frac{p_t}{p_t - 1} - 1,\n\\]\nwhere \\(p_t\\) denotes the adjusted closing price at the end of trading day \\(t\\).\nBefore computing returns, we must address invalid price observations. In Vietnamese equity data, adjusted prices occasionally take the value zero. These entries typically arise from IPO placeholders, trading suspensions, or historical backfilling conventions and cannot be used to compute meaningful returns.\n\nprices.loc[prices[\"adjusted_close\"] &lt;= 0, [\"symbol\", \"date\", \"adjusted_close\"]].head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\n\n\n\n\n33886\nADP\n2010-02-09\n0.0\n\n\n33887\nADP\n2010-02-24\n0.0\n\n\n33888\nADP\n2010-03-01\n0.0\n\n\n33889\nADP\n2010-03-03\n0.0\n\n\n33890\nADP\n2010-03-12\n0.0\n\n\n\n\n\n\n\nWe therefore exclude non-positive adjusted prices and compute returns stock by stock. Correct chronological ordering is essential.\n\nreturns = (\n    prices\n    .loc[prices[\"adjusted_close\"] &gt; 0]\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n)\nreturns = returns.dropna(subset=[\"ret\"])\n\nThe initial return for each stock is missing by construction, since no lagged price is available. These observations are mechanical and can safely be removed in most applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "href": "01_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.4 Limiting the Influence of Extreme Returns",
    "text": "2.4 Limiting the Influence of Extreme Returns\nDaily return series often contain extreme observations driven by data errors, thin trading, or abrupt price adjustments. A common approach is to winsorize returns using cross-sectional percentile cutoffs.\n\ndef winsorize_cs(df, column=\"ret\", lower_q=0.01, upper_q=0.99):\n    lo = df[column].quantile(lower_q)\n    hi = df[column].quantile(upper_q)\n    out = df.copy()\n    out[column] = out[column].clip(lo, hi)\n    return out\n\nreturns = winsorize_cs(returns)\n\nApplying winsorization across the full cross-section limits the impact of extreme market-wide observations while preserving relative differences between firms. Winsorizing within each stock is rarely appropriate in panel settings and can severely distort illiquid securities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#distributional-features-of-returns",
    "href": "01_working_with_stock_returns.html#distributional-features-of-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.5 Distributional Features of Returns",
    "text": "2.5 Distributional Features of Returns\nWe next examine the empirical distribution of daily returns for FPT. The figure below also marks the historical 5 percent quantile, which provides a simple, non-parametric measure of downside risk.\n\nfrom mizani.formatters import percent_format\nfrom plotnine import geom_histogram, geom_vline, scale_x_continuous\n\n\nfpt_ret = returns.loc[returns[\"symbol\"] == \"FPT\"].copy()\nq05 = fpt_ret[\"ret\"].quantile(0.05)\n\n\n(\n    ggplot(fpt_ret, aes(x=\"ret\"))\n    + geom_histogram(bins=100)\n    + geom_vline(xintercept=q05, linetype=\"dashed\")\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"Distribution of daily FPT returns\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 2.2: The dotted vertical line indicates the historical five percent quantile.\n\n\n\n\n\nSummary statistics offer a compact description of return behavior and should always be inspected before formal modeling.\n\nreturns[\"ret\"].describe().round(3)\n\ncount    4305063.000\nmean           0.000\nstd            0.035\nmin           -0.125\n25%           -0.004\n50%            0.000\n75%            0.003\nmax            0.130\nName: ret, dtype: float64\n\n\nComputing these statistics by calendar year can reveal periods of elevated volatility or structural change.\n\n(\n    returns\n    .assign(year=lambda x: x[\"date\"].dt.year)\n    .groupby(\"year\")[\"ret\"]\n    .describe()\n    .round(3)\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n2010\n131548.0\n-0.001\n0.036\n-0.125\n-0.021\n0.0\n0.018\n0.13\n\n\n2011\n166826.0\n-0.003\n0.033\n-0.125\n-0.020\n0.0\n0.011\n0.13\n\n\n2012\n177938.0\n0.000\n0.033\n-0.125\n-0.012\n0.0\n0.015\n0.13\n\n\n2013\n180417.0\n0.001\n0.033\n-0.125\n-0.004\n0.0\n0.008\n0.13\n\n\n2014\n181907.0\n0.001\n0.034\n-0.125\n-0.008\n0.0\n0.011\n0.13\n\n\n2015\n197881.0\n0.000\n0.033\n-0.125\n-0.006\n0.0\n0.005\n0.13\n\n\n2016\n227896.0\n0.000\n0.035\n-0.125\n-0.005\n0.0\n0.003\n0.13\n\n\n2017\n283642.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.001\n0.13\n\n\n2018\n329887.0\n0.000\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2019\n352754.0\n0.000\n0.033\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2020\n369367.0\n0.001\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2021\n379415.0\n0.002\n0.038\n-0.125\n-0.005\n0.0\n0.007\n0.13\n\n\n2022\n387050.0\n-0.001\n0.038\n-0.125\n-0.008\n0.0\n0.004\n0.13\n\n\n2023\n391605.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.002\n0.13\n\n\n2024\n400379.0\n0.000\n0.031\n-0.125\n-0.002\n0.0\n0.000\n0.13\n\n\n2025\n146551.0\n0.000\n0.037\n-0.125\n-0.004\n0.0\n0.002\n0.13",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "href": "01_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.6 Expanding to a Market Cross-Section",
    "text": "2.6 Expanding to a Market Cross-Section\nThe same procedures apply naturally to a larger universe of stocks. We now restrict attention to the constituents of the VN30 index.\n\nvn30 = [\n    \"ACB\",\"BCM\",\"BID\",\"BVH\",\"CTG\",\"FPT\",\"GAS\",\"GVR\",\"HDB\",\"HPG\",\n    \"MBB\",\"MSN\",\"MWG\",\"PLX\",\"POW\",\"SAB\",\"SHB\",\"SSB\",\"STB\",\"TCB\",\n    \"TPB\",\"VCB\",\"VHM\",\"VIB\",\"VIC\",\"VJC\",\"VNM\",\"VPB\",\"VRE\",\"EIB\",\n]\n\n\nprices_vn30 = prices.loc[prices[\"symbol\"].isin(vn30)]\nfrom plotnine import theme\n\n\n(\n    ggplot(prices_vn30, aes(x=\"date\", y=\"adjusted_close\", color=\"symbol\"))\n    + geom_line()\n    + labs(title=\"Adjusted prices of VN30 constituents\", x=\"\", y=\"\")\n    + theme(legend_position=\"none\")\n)\n\n\n\n\n\n\n\nFigure 2.3: Prices in VND, adjusted for dividend payments and stock splits.\n\n\n\n\n\nReturns for the VN30 universe are computed analogously.\n\nreturns_vn30 = (\n    prices_vn30\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n    .dropna()\n)\n\n\nreturns_vn30.groupby(\"symbol\")[\"ret\"].describe().round(3)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\n\n\nACB\n3822.0\n-0.000\n0.023\n-0.407\n-0.006\n0.0\n0.007\n0.097\n\n\nBCM\n1795.0\n0.001\n0.027\n-0.136\n-0.010\n0.0\n0.010\n0.159\n\n\nBID\n2811.0\n0.000\n0.024\n-0.369\n-0.010\n0.0\n0.011\n0.070\n\n\nBVH\n3825.0\n0.000\n0.024\n-0.097\n-0.012\n0.0\n0.012\n0.070\n\n\nCTG\n3825.0\n0.000\n0.024\n-0.376\n-0.010\n0.0\n0.010\n0.070\n\n\nEIB\n3825.0\n-0.000\n0.022\n-0.302\n-0.008\n0.0\n0.008\n0.070\n\n\nFPT\n3825.0\n-0.000\n0.024\n-0.439\n-0.008\n0.0\n0.009\n0.070\n\n\nGAS\n3236.0\n0.000\n0.022\n-0.289\n-0.009\n0.0\n0.010\n0.070\n\n\nGVR\n1775.0\n0.001\n0.030\n-0.137\n-0.014\n0.0\n0.016\n0.169\n\n\nHDB\n1828.0\n-0.001\n0.028\n-0.391\n-0.009\n0.0\n0.010\n0.070\n\n\nHPG\n3825.0\n-0.001\n0.032\n-0.581\n-0.010\n0.0\n0.011\n0.070\n\n\nMBB\n3371.0\n-0.000\n0.023\n-0.473\n-0.008\n0.0\n0.008\n0.069\n\n\nMSN\n3825.0\n0.000\n0.024\n-0.553\n-0.010\n0.0\n0.010\n0.070\n\n\nMWG\n2701.0\n-0.000\n0.035\n-0.751\n-0.009\n0.0\n0.011\n0.070\n\n\nPLX\n2009.0\n-0.000\n0.021\n-0.140\n-0.010\n0.0\n0.010\n0.070\n\n\nPOW\n1784.0\n0.000\n0.023\n-0.071\n-0.012\n0.0\n0.011\n0.102\n\n\nSAB\n2100.0\n-0.000\n0.024\n-0.745\n-0.008\n0.0\n0.007\n0.070\n\n\nSHB\n3824.0\n-0.000\n0.028\n-0.338\n-0.013\n0.0\n0.013\n0.100\n\n\nSSB\n1029.0\n-0.000\n0.023\n-0.292\n-0.005\n0.0\n0.004\n0.070\n\n\nSTB\n3825.0\n0.000\n0.024\n-0.321\n-0.010\n0.0\n0.010\n0.070\n\n\nTCB\n1732.0\n-0.000\n0.035\n-0.884\n-0.009\n0.0\n0.010\n0.070\n\n\nTPB\n1761.0\n-0.001\n0.029\n-0.477\n-0.009\n0.0\n0.009\n0.070\n\n\nVCB\n3825.0\n-0.000\n0.024\n-0.539\n-0.009\n0.0\n0.009\n0.070\n\n\nVHM\n1744.0\n-0.000\n0.024\n-0.419\n-0.009\n0.0\n0.008\n0.070\n\n\nVIB\n2072.0\n-0.000\n0.031\n-0.489\n-0.009\n0.0\n0.010\n0.109\n\n\nVIC\n3825.0\n-0.000\n0.027\n-0.673\n-0.008\n0.0\n0.008\n0.070\n\n\nVJC\n2046.0\n-0.000\n0.020\n-0.455\n-0.007\n0.0\n0.006\n0.070\n\n\nVNM\n3825.0\n-0.000\n0.023\n-0.547\n-0.007\n0.0\n0.007\n0.070\n\n\nVPB\n1927.0\n-0.000\n0.033\n-0.678\n-0.010\n0.0\n0.010\n0.070\n\n\nVRE\n1871.0\n-0.000\n0.024\n-0.295\n-0.012\n0.0\n0.011\n0.070",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#aggregating-returns-across-time",
    "href": "01_working_with_stock_returns.html#aggregating-returns-across-time",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.7 Aggregating Returns Across Time",
    "text": "2.7 Aggregating Returns Across Time\nFinancial variables are observed at different frequencies. While equity prices are recorded daily, many empirical questions require monthly or annual returns. Lower-frequency returns are constructed by compounding higher-frequency observations.\n\nreturns_monthly = (\n    returns_vn30\n    .assign(month=lambda x: x[\"date\"].dt.to_period(\"M\").dt.to_timestamp())\n    .groupby([\"symbol\", \"month\"], as_index=False)\n    .agg(ret=(\"ret\", lambda x: np.prod(1 + x) - 1))\n)\n\nComparing daily and monthly return distributions illustrates how aggregation dampens volatility and alters tail behavior.\n\nfrom plotnine import facet_wrap\n\nfpt_d = returns_vn30.loc[returns_vn30[\"symbol\"] == \"FPT\"].assign(freq=\"Daily\")\nfpt_m = returns_monthly.loc[returns_monthly[\"symbol\"] == \"FPT\"].assign(freq=\"Monthly\")\n\n\nfpt_both = pd.concat([\n    fpt_d[[\"ret\", \"freq\"]],\n    fpt_m[[\"ret\", \"freq\"]],\n])\n\n\n(\n    ggplot(fpt_both, aes(x=\"ret\"))\n    + geom_histogram(bins=50)\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"FPT returns at different frequencies\", x=\"\", y=\"\")\n    + facet_wrap(\"freq\", scales=\"free\")\n)\n\n\n\n\n\n\n\nFigure 2.4: Returns are based on prices adjusted for dividend payments and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "href": "01_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.8 Aggregation Across Firms: Trading Activity",
    "text": "2.8 Aggregation Across Firms: Trading Activity\nAggregation is not limited to time. In some settings, it is informative to aggregate variables across firms. As an illustration, we compute total daily trading value for VN30 stocks by multiplying share volume by adjusted prices and summing across firms.\n\ntrading_value = (\n    prices_vn30\n    .assign(value=lambda x: x[\"volume\"] * x[\"adjusted_close\"] / 1e9)\n    .groupby(\"date\")[\"value\"]\n    .sum()\n    .reset_index()\n    .assign(value_lag=lambda x: x[\"value\"].shift(1))\n)\n(\n    ggplot(trading_value, aes(x=\"date\", y=\"value\"))\n    + geom_line()\n    + labs(title=\"Aggregate VN30 trading value (billion VND)\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\n\nFinally, we assess persistence in trading activity by comparing trading value on consecutive days.\n\nfrom plotnine import geom_point, geom_abline\n\n\n(\n    ggplot(trading_value, aes(x=\"value_lag\", y=\"value\"))\n    + geom_point()\n    + geom_abline(intercept=0, slope=1, linetype=\"dashed\")\n    + labs(\n        title=\"Persistence in VN30 trading value\",\n        x=\"Previous day\",\n        y=\"Current day\",\n    )\n)\n\n\n\n\n\n\n\nFigure 2.5: Total daily trading volume.\n\n\n\n\n\nA strong alignment along the 45-degree line indicates that high-activity trading days tend to be followed by similarly active days, a common empirical regularity in equity markets.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#summary",
    "href": "01_working_with_stock_returns.html#summary",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nThis chapter established a reproducible workflow for transforming raw price data into return series, diagnosing common data issues, and aggregating information across time and firms. These steps provide the empirical foundation for subsequent analyses of risk, return predictability, and market dynamics in Vietnam’s equity market.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html",
    "href": "02_modern_portfolio_theory.html",
    "title": "3  Modern Portfolio Theory",
    "section": "",
    "text": "3.0.1 The Core Insight: Diversification as a Free Lunch\nIn the previous chapter, we showed how to download and analyze stock market data with figures and summary statistics. Now, we turn to one of the most fundamental questions in finance: How should an investor allocate their wealth across assets that differ in expected returns, variance, and correlations to optimize their portfolio’s performance?\nThis question might seem straightforward at first glance. Why not simply invest everything in the asset with the highest expected return? The answer lies in a profound insight that transformed financial economics: risk matters, and it can be managed through diversification.\nModern Portfolio Theory (MPT), introduced by Markowitz (1952), revolutionized investment decision-making by formalizing the trade-off between risk and expected return. Before Markowitz, investors largely thought about risk on a security-by-security basis. Markowitz’s genius was recognizing that what matters is not the risk of individual securities in isolation, but how they contribute to the risk of the entire portfolio. This insight was so influential that it earned him the Sveriges Riksbank Prize in Economic Sciences in 1990 and laid the foundation for much of modern finance.\nMPT relies on a crucial mathematical fact: portfolio risk depends not only on individual asset volatilities but also on the correlations between asset returns. This insight reveals the power of diversification—combining assets whose returns don’t move in perfect lockstep can reduce overall portfolio risk without necessarily sacrificing expected return.\nConsider a simple analogy: Imagine you run a business selling both sunscreen and umbrellas. On sunny days, sunscreen sales boom but umbrella sales suffer; on rainy days, the reverse happens. By selling both products, your total revenue becomes more stable than if you sold only one. The “correlation” between sunscreen and umbrella sales is negative, and combining them reduces the variance of your overall income. This is precisely the logic behind portfolio diversification.\nThe fruit basket analogy offers another perspective: If all you have are apples and they spoil, you lose everything. With a variety of fruits, some may spoil, but others will stay fresh. Diversification provides insurance against the idiosyncratic risks of individual assets.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-asset-universe-setting-up-the-problem",
    "href": "02_modern_portfolio_theory.html#the-asset-universe-setting-up-the-problem",
    "title": "3  Modern Portfolio Theory",
    "section": "3.1 The Asset Universe: Setting Up the Problem",
    "text": "3.1 The Asset Universe: Setting Up the Problem\nSuppose \\(N\\) different risky assets are available to the investor. Each asset \\(i\\) is characterized by:\n\nExpected return \\(\\mu_i\\): The anticipated profit from holding the asset for one period\nVariance \\(\\sigma_i^2\\): The dispersion of returns around the mean\nCovariances \\(\\sigma_{ij}\\): The degree to which asset \\(i\\)’s returns move together with asset \\(j\\)’s returns\n\nThe investor chooses portfolio weights \\(\\omega_i\\) for each asset \\(i\\). These weights represent the fraction of total wealth invested in each asset. We impose the constraint that weights sum to one:\n\\[\n\\sum_{i=1}^N \\omega_i = 1\n\\]\nThis “budget constraint” ensures that the investor is fully invested—there is no outside option such as keeping money under a mattress. Note that we allow weights to be negative (short selling) or greater than one (leverage), though in practice these positions may face constraints.\n\n3.1.1 The Two Stages of Portfolio Selection\nAccording to Markowitz (1952), portfolio selection involves two distinct stages:\n\nEstimation: Forming expectations about future security performance based on observations, experience, and economic reasoning\nOptimization: Using these expectations to choose an optimal portfolio\n\nIn practice, these stages cannot be fully separated. The estimation stage determines the inputs (\\(\\mu\\), \\(\\Sigma\\)) that feed into the optimization stage. Poor estimation leads to poor portfolio choices, regardless of how sophisticated the optimization procedure.\nTo keep things conceptually clear, we focus primarily on the optimization stage in this chapter. We treat the expected returns and variance-covariance matrix as known, using historical data to compute reasonable proxies. In later chapters, we address the substantial challenges that arise from estimation uncertainty.\n\n\n3.1.2 Loading and Preparing the Data\nWe work with the VN30 index constituents—the 30 largest and most liquid stocks on Vietnam’s Ho Chi Minh Stock Exchange. This provides a realistic asset universe for a domestic Vietnamese investor.\n\nvn30_symbols = [\n    \"ACB\",\"BCM\",\"BID\",\"BVH\",\"CTG\",\"FPT\",\"GAS\",\"GVR\",\"HDB\",\"HPG\",\n    \"MBB\",\"MSN\",\"MWG\",\"PLX\",\"POW\",\"SAB\",\"SHB\",\"SSB\",\"STB\",\"TCB\",\n    \"TPB\",\"VCB\",\"VHM\",\"VIB\",\"VIC\",\"VJC\",\"VNM\",\"VPB\",\"VRE\",\"EIB\"\n]\n\nWe load the historical price data:\n\nimport pandas as pd\nfrom io import BytesIO\nimport datetime as dt\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\nWe filter to keep only the VN30 constituents:\n\nprices_daily = prices[prices[\"symbol\"].isin(vn30_symbols)]\nprices_daily[[\"date\", \"symbol\", \"adjusted_close\"]].head(3)\n\n\n\n\n\n\n\n\ndate\nsymbol\nadjusted_close\n\n\n\n\n18176\n2010-01-04\nACB\n329.408244\n\n\n18177\n2010-01-05\nACB\n329.408244\n\n\n18178\n2010-01-06\nACB\n320.258015\n\n\n\n\n\n\n\n\n\n3.1.3 Computing Expected Returns\nThe sample mean return serves as our proxy for expected returns. For each asset \\(i\\), we compute:\n\\[\n\\hat{\\mu}_i = \\frac{1}{T} \\sum_{t=1}^{T} r_{i,t}\n\\]\nwhere \\(r_{i,t}\\) is the return of asset \\(i\\) in period \\(t\\), and \\(T\\) is the total number of periods.\nWhy monthly returns? While daily data provides more observations, monthly returns offer several advantages for portfolio optimization. First, monthly returns are less noisy and exhibit weaker serial correlation. Second, monthly rebalancing is more realistic for most investors, avoiding excessive transaction costs. Third, the estimation error in mean returns is already substantial—using daily data doesn’t materially improve the precision of mean estimates because the mean return scales with the horizon while estimation error scales with the square root of observations.\n\nreturns_monthly = (prices_daily\n  .assign(\n    date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n  )\n  .groupby([\"symbol\", \"date\"], as_index=False)\n  .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n  .assign(\n    ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n  )\n)\n\n\n\n3.1.4 Computing Volatilities\nIndividual asset risk in MPT is quantified using variance (\\(\\sigma^2_i\\)) or its square root, the standard deviation or volatility (\\(\\sigma_i\\)). We use the sample standard deviation as our proxy:\n\\[\n\\hat{\\sigma}_i = \\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} (r_{i,t} - \\hat{\\mu}_i)^2}\n\\]\nAlternative risk measures exist, including Value-at-Risk, Expected Shortfall, and higher-order moments such as skewness and kurtosis. However, variance remains the workhorse measure in portfolio theory because of its mathematical tractability and the central role of the normal distribution in finance.\n\nassets = (returns_monthly\n  .groupby(\"symbol\", as_index=False)\n  .agg(\n    mu=(\"ret\", \"mean\"),\n    sigma=(\"ret\", \"std\")\n  )\n)\n\n\n\n3.1.5 Visualizing the Risk-Return Trade-off\nFigure 3.1 displays each asset’s expected return (vertical axis) against its volatility (horizontal axis). This “mean-standard deviation” space is fundamental to portfolio theory.\n\nassets_figure = (\n  ggplot(\n    assets, \n    aes(x=\"sigma\", y=\"mu\", label=\"symbol\")\n  )\n  + geom_point()\n  + geom_text(adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"Expected returns and volatilities of VN30 index constituents\"\n  )\n)\nassets_figure.show()\n\n\n\n\n\n\n\nFigure 3.1: Expected returns and volatilities based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nSeveral observations emerge from this figure. First, there is substantial heterogeneity in both expected returns and volatilities across stocks. Second, the relationship between risk and return is far from linear. Some high-volatility stocks have low or even negative expected returns. Third, most individual stocks appear to offer poor risk-return trade-offs. As we will see, portfolios can substantially improve upon these individual positions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-variance-covariance-matrix-capturing-asset-interactions",
    "href": "02_modern_portfolio_theory.html#the-variance-covariance-matrix-capturing-asset-interactions",
    "title": "3  Modern Portfolio Theory",
    "section": "3.2 The Variance-Covariance Matrix: Capturing Asset Interactions",
    "text": "3.2 The Variance-Covariance Matrix: Capturing Asset Interactions\n\n3.2.1 Why Correlations Matter\nA key innovation of MPT is recognizing that portfolio risk depends critically on how assets move together. The variance-covariance matrix \\(\\Sigma\\) captures all pairwise interactions between asset returns.\nTo understand why correlations matter, consider the variance of a two-asset portfolio: \\[\\sigma_p^2 = \\omega_1^2\\sigma_1^2 + \\omega_2^2\\sigma_2^2 + 2\\omega_1\\omega_2\\sigma_{12}\\]\nThe third term involves the covariance \\(\\sigma_{12} = \\rho_{12}\\sigma_1\\sigma_2\\), where \\(\\rho_{12}\\) is the correlation coefficient. When \\(\\rho_{12} &lt; 1\\), the portfolio variance is less than the weighted average of individual variances. When \\(\\rho_{12} &lt; 0\\), the diversification benefit is even more pronounced.\nThis mathematical fact has profound implications: You can reduce risk without reducing expected return by combining assets that don’t move perfectly together. This is sometimes called the “only free lunch in finance.”\n\n\n3.2.2 Computing the Variance-Covariance Matrix\nWe compute the sample covariance matrix as: \\[\\hat{\\sigma}_{ij} = \\frac{1}{T-1} \\sum_{t=1}^{T} (r_{i,t} - \\hat{\\mu}_i)(r_{j,t} - \\hat{\\mu}_j)\\]\nFirst, we reshape the returns data into a wide format with assets as columns:\n\nreturns_wide = (returns_monthly\n  .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n  .reset_index()\n)\n\nsigma = (returns_wide\n  .drop(columns=[\"date\"])\n  .cov()\n)\n\n\n\n3.2.3 Interpreting the Variance-Covariance Matrix\nThe diagonal elements of \\(\\Sigma\\) are the variances of individual assets. The off-diagonal elements are covariances, which can be positive (assets tend to move together), negative (assets tend to move in opposite directions), or zero (no linear relationship).\nFor easier interpretation, we often convert covariances to correlations: \\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sigma_i \\sigma_j}\\]\nCorrelations are bounded between -1 and +1, making them easier to compare across asset pairs.\nFigure 3.2 visualizes the variance-covariance matrix as a heatmap.\n\nsigma_long = (sigma\n  .reset_index()\n  .melt(id_vars=\"symbol\", var_name=\"symbol_b\", value_name=\"value\")\n)\n\nsigma_long[\"symbol_b\"] = pd.Categorical(\n  sigma_long[\"symbol_b\"], \n  categories=sigma_long[\"symbol_b\"].unique()[::-1],\n  ordered=True\n)\n\nsigma_figure = (\n  ggplot(\n    sigma_long, \n    aes(x=\"symbol\", y=\"symbol_b\", fill=\"value\")\n  )\n  + geom_tile()\n  + labs(\n      x=\"\", y=\"\", fill=\"(Co-)Variance\",\n      title=\"Sample variance-covariance matrix of VN30 index constituents\"\n    )\n  + scale_fill_continuous(labels=percent_format())\n  + theme(axis_text_x=element_text(angle=45, hjust=1))\n)\nsigma_figure.show()\n\n\n\n\n\n\n\nFigure 3.2: Variances and covariances based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nThe heatmap reveals important patterns. The diagonal (variances) shows which stocks are most volatile. The off-diagonal patterns show which pairs of stocks tend to move together. In general, stocks within the same sector tend to have higher correlations with each other than with stocks from different sectors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-minimum-variance-portfolio",
    "href": "02_modern_portfolio_theory.html#the-minimum-variance-portfolio",
    "title": "3  Modern Portfolio Theory",
    "section": "3.3 The Minimum-Variance Portfolio",
    "text": "3.3 The Minimum-Variance Portfolio\n\n3.3.1 Motivation: Risk Minimization as a Benchmark\nBefore considering expected returns, let’s find the portfolio that minimizes risk entirely. This minimum-variance portfolio (MVP) serves as an important benchmark and reference point. It represents what an extremely risk-averse investor—one who cares only about minimizing volatility—would choose.\n\n\n3.3.2 The Optimization Problem\nThe minimum-variance investor solves: \\[\n\\min_{\\omega} \\omega^{\\prime}\\Sigma\\omega\n\\]\nsubject to the constraint that weights sum to one:\n\\[\n\\omega^{\\prime}\\iota = 1\n\\]\nwhere \\(\\iota\\) is an \\(N \\times 1\\) vector of ones.\nIn words: minimize portfolio variance, subject to being fully invested.\n\n\n3.3.3 The Analytical Solution\nThis is a classic constrained optimization problem that can be solved using Lagrange multipliers. The Lagrangian is:\n\\[\n\\mathcal{L} = \\omega^{\\prime}\\Sigma\\omega - \\lambda(\\omega^{\\prime}\\iota - 1)\n\\]\nTaking the first-order condition with respect to \\(\\omega\\): \\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda\\iota = 0\n\\]\nSolving for \\(\\omega\\): \\[\n\\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota\n\\]\nUsing the constraint \\(\\omega^{\\prime}\\iota = 1\\) to solve for \\(\\lambda\\): \\[\n\\frac{\\lambda}{2}\\iota^{\\prime}\\Sigma^{-1}\\iota = 1 \\implies \\frac{\\lambda}{2} = \\frac{1}{\\iota^{\\prime}\\Sigma^{-1}\\iota}\n\\]\nSubstituting back: \\[\n\\omega_{\\text{mvp}} = \\frac{\\Sigma^{-1}\\iota}{\\iota^{\\prime}\\Sigma^{-1}\\iota}\n\\]\nThis elegant formula shows that the minimum-variance weights depend only on the covariance matrix—expected returns play no role. The inverse covariance matrix \\(\\Sigma^{-1}\\) determines how much to invest in each asset based on its variance and its covariances with all other assets.\n\n\n3.3.4 Implementation\n\niota = np.ones(sigma.shape[0])\nsigma_inv = np.linalg.inv(sigma.values)\nomega_mvp = (sigma_inv @ iota) / (iota @ sigma_inv @ iota)\n\n\n\n3.3.5 Visualizing the Minimum-Variance Weights\nFigure 3.3 displays the portfolio weights of the minimum-variance portfolio.\n\nassets = assets.assign(omega_mvp=omega_mvp)\n\nassets[\"symbol\"] = pd.Categorical(\n  assets[\"symbol\"],\n  categories=assets.sort_values(\"omega_mvp\")[\"symbol\"],\n  ordered=True\n)\n\nomega_figure = (\n  ggplot(\n    assets,\n    aes(y=\"omega_mvp\", x=\"symbol\", fill=\"omega_mvp&gt;0\")\n  )\n  + geom_col()\n  + coord_flip()\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", \n      y=\"Portfolio Weight\", \n      title=\"Minimum-variance portfolio weights\"\n  )\n  + theme(legend_position=\"none\")\n)\nomega_figure.show()\n\n\n\n\n\n\n\nFigure 3.3: Weights are based on historical moments of monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nSeveral features of the minimum-variance portfolio are noteworthy. First, many stocks receive zero or near-zero weights. Second, some stocks receive negative weights (short positions). These short positions are not a computational artifact, they reflect the optimizer’s attempt to exploit correlations for risk reduction. Third, the weights are quite extreme (both large positive and large negative), which often indicates estimation error amplification, which is a topic we address in later chapters.\n\n\n3.3.6 Portfolio Performance\nLet’s compute the expected return and volatility of the minimum-variance portfolio:\n\nmu = assets[\"mu\"].values\nmu_mvp = omega_mvp @ mu\nsigma_mvp = np.sqrt(omega_mvp @ sigma.values @ omega_mvp)\n\nsummary_mvp = pd.DataFrame({\n  \"mu\": [mu_mvp],\n  \"sigma\": [sigma_mvp],\n  \"type\": [\"Minimum-Variance Portfolio\"]\n})\nsummary_mvp\n\n\n\n\n\n\n\n\nmu\nsigma\ntype\n\n\n\n\n0\n-0.011424\n0.043512\nMinimum-Variance Portfolio\n\n\n\n\n\n\n\n\nmu_mvp_fmt = f\"{mu_mvp:.4f}\"\nsigma_mvp_fmt = f\"{sigma_mvp:.4f}\"\nprint(f\"The MVP return is {mu_mvp_fmt} and volatility is {sigma_mvp_fmt}.\")\n\nThe MVP return is -0.0114 and volatility is 0.0435.\n\n\nIf the expected return is negative, this is not a computational error. The minimum-variance portfolio minimizes risk without regard to expected returns. Because some assets in the sample have negative average returns, the risk-minimizing combination may inherit a negative expected return. This highlights a fundamental limitation of using historical sample means as estimates of expected returns: they are extremely noisy, and can lead to economically unintuitive results even when the optimization mathematics are working correctly.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#efficient-portfolios-balancing-risk-and-return",
    "href": "02_modern_portfolio_theory.html#efficient-portfolios-balancing-risk-and-return",
    "title": "3  Modern Portfolio Theory",
    "section": "3.4 Efficient Portfolios: Balancing Risk and Return",
    "text": "3.4 Efficient Portfolios: Balancing Risk and Return\n\n3.4.1 The Investor’s Trade-off\nIn most cases, minimizing variance is not the investor’s sole objective. A more realistic formulation allows the investor to trade off risk against expected return. The investor might be willing to accept higher portfolio variance in exchange for higher expected returns.\nAn efficient portfolio minimizes variance subject to earning at least some target expected return \\(\\bar{\\mu}\\). Formally:\n\\[\\min_{\\omega} \\omega^{\\prime}\\Sigma\\omega\\]\nsubject to: \\[\\omega^{\\prime}\\iota = 1 \\quad \\text{(fully invested)}\\] \\[\\omega^{\\prime}\\mu \\geq \\bar{\\mu} \\quad \\text{(minimum return)}\\]\nWhen \\(\\bar{\\mu}\\) exceeds the expected return of the minimum-variance portfolio, the investor accepts more risk to earn more return.\n\n\n3.4.2 Setting the Target Return\nFor illustration, suppose the investor wants to earn at least the historical average return of the best-performing stock:\n\nmu_bar = assets[\"mu\"].max()\nprint(f\"Target expected return: {mu_bar:.5f}\")\n\nTarget expected return: 0.01886\n\n\nThis is an ambitious target—it means matching the return of the single highest-returning stock while benefiting from diversification to reduce risk.\n\n\n3.4.3 The Analytical Solution\nThe constrained optimization problem with an inequality constraint on expected returns can be solved using the Karush-Kuhn-Tucker (KKT) conditions. At the optimum (assuming the return constraint binds), the solution is:\n\\[\\omega_{\\text{efp}} = \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu - \\frac{D}{C}\\Sigma^{-1}\\iota\\right)\\]\nwhere:\n\n\\(C = \\iota^{\\prime}\\Sigma^{-1}\\iota\\) (a scalar measuring the “size” of the inverse covariance matrix)\n\\(D = \\iota^{\\prime}\\Sigma^{-1}\\mu\\) (capturing the interaction between expected returns and the inverse covariance matrix)\n\\(E = \\mu^{\\prime}\\Sigma^{-1}\\mu\\) (measuring the “signal” in expected returns weighted by inverse covariances)\n\\(\\lambda^* = 2\\frac{\\bar{\\mu} - D/C}{E - D^2/C}\\) (the shadow price of the return constraint)\n\nAlternatively, we can express the efficient portfolio as a linear combination of the minimum-variance portfolio and an “excess return” portfolio:\n\\[\\omega_{\\text{efp}} = \\omega_{\\text{mvp}} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu - D \\cdot \\omega_{\\text{mvp}}\\right)\\]\nThis representation reveals important intuition: the efficient portfolio starts from the minimum-variance portfolio and tilts toward higher-expected-return assets, with the tilt magnitude determined by \\(\\lambda^*\\).\n\n\n3.4.4 Implementation\n\nC = iota @ sigma_inv @ iota\nD = iota @ sigma_inv @ mu\nE = mu @ sigma_inv @ mu\nlambda_tilde = 2 * (mu_bar - D / C) / (E - (D ** 2) / C)\nomega_efp = omega_mvp + (lambda_tilde / 2) * (sigma_inv @ mu - D * omega_mvp)\n\nmu_efp = omega_efp @ mu\nsigma_efp = np.sqrt(omega_efp @ sigma.values @ omega_efp)\n\nsummary_efp = pd.DataFrame({\n  \"mu\": [mu_efp],\n  \"sigma\": [sigma_efp],\n  \"type\": [\"Efficient Portfolio\"]\n})\n\n\n\n3.4.5 Comparing the Portfolios\nFigure 3.4 plots both portfolios alongside the individual assets.\n\nsummaries = pd.concat(\n  [assets, summary_mvp, summary_efp], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=summaries.query(\"type.isna()\"))\n  + geom_point(data=summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3)\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"Efficient & minimum-variance portfolios\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 3.4: The big dots indicate the location of the minimum-variance and the efficient portfolio that delivers the expected return of the stock with the highest return, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe figure demonstrates the substantial diversification benefits of portfolio optimization. The efficient portfolio achieves the same expected return as the highest-returning individual stock but with substantially lower volatility. This “free lunch” from diversification is the central insight of Modern Portfolio Theory.\n\n\n3.4.6 The Role of Risk Aversion\nThe target return \\(\\bar{\\mu}\\) implicitly reflects the investor’s risk aversion. Less risk-averse investors choose higher \\(\\bar{\\mu}\\), accepting more variance to earn more expected return. More risk-averse investors choose \\(\\bar{\\mu}\\) closer to the minimum-variance portfolio’s expected return.\nEquivalently, the mean-variance framework can be derived from the optimal decisions of an investor with a mean-variance utility function: \\[U(\\omega) = \\omega^{\\prime}\\mu - \\frac{\\gamma}{2}\\omega^{\\prime}\\Sigma\\omega\\]\nwhere \\(\\gamma\\) is the coefficient of relative risk aversion. The Appendix shows there is a one-to-one mapping between \\(\\gamma\\) and \\(\\bar{\\mu}\\), so both formulations yield identical efficient portfolios.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#the-efficient-frontier-the-menu-of-optimal-portfolios",
    "href": "02_modern_portfolio_theory.html#the-efficient-frontier-the-menu-of-optimal-portfolios",
    "title": "3  Modern Portfolio Theory",
    "section": "3.5 The Efficient Frontier: The Menu of Optimal Portfolios",
    "text": "3.5 The Efficient Frontier: The Menu of Optimal Portfolios\nThe efficient frontier is the set of all portfolios for which no other portfolio offers higher expected return at the same or lower variance. Geometrically, it traces the upper boundary of achievable (volatility, expected return) combinations.\nEvery rational mean-variance investor should hold a portfolio on the efficient frontier. Portfolios below the frontier are “dominated,” there exists another portfolio with either higher return for the same risk, or lower risk for the same return.\n\n3.5.1 The Mutual Fund Separation Theorem\nA remarkable result simplifies the construction of the efficient frontier. The mutual fund separation theorem (sometimes called the two-fund theorem) states that any efficient portfolio can be expressed as a linear combination of any two distinct efficient portfolios.\nFormally, if \\(\\omega_{\\mu_1}\\) and \\(\\omega_{\\mu_2}\\) are efficient portfolios earning expected returns \\(\\mu_1\\) and \\(\\mu_2\\) respectively, then the portfolio: \\[\\omega_{a\\mu_1 + (1-a)\\mu_2} = a \\cdot \\omega_{\\mu_1} + (1-a) \\cdot \\omega_{\\mu_2}\\]\nis also efficient and earns expected return \\(a\\mu_1 + (1-a)\\mu_2\\).\nThis result has profound practical implications: an investor needs access to only two efficient “mutual funds” to construct any portfolio on the efficient frontier. The specific funds don’t matter—any two distinct efficient portfolios span the entire frontier.\n\n\n3.5.2 Proof of the Separation Theorem\nThe proof follows directly from the analytical solution for efficient portfolios. Consider:\n\\[\na \\cdot \\omega_{\\mu_1} + (1-a) \\cdot \\omega_{\\mu_2} = \\left(\\frac{a\\mu_1 + (1-a)\\mu_2 - D/C}{E - D^2/C}\\right)\\left(\\Sigma^{-1}\\mu - \\frac{D}{C}\\Sigma^{-1}\\iota\\right)\n\\]\nThis expression has exactly the form of the efficient portfolio earning expected return \\(a\\mu_1 + (1-a)\\mu_2\\), proving the theorem.\n\n\n3.5.3 Computing the Efficient Frontier\nUsing the minimum-variance portfolio and our efficient portfolio as the two “funds,” we can trace out the entire efficient frontier:\n\nefficient_frontier = (\n  pd.DataFrame({\n    \"a\": np.arange(-1, 2.01, 0.01)\n  })\n  .assign(\n    omega=lambda x: x[\"a\"].map(lambda a: a * omega_efp + (1 - a) * omega_mvp)\n  )\n  .assign(\n    mu=lambda x: x[\"omega\"].map(lambda w: w @ mu),\n    sigma=lambda x: x[\"omega\"].map(lambda w: np.sqrt(w @ sigma @ w))\n  )\n)\n\nNote that we allow \\(a\\) to range from -1 to 2, which means some portfolios involve shorting one of the two basis funds and leveraging into the other. This traces out both the upper and lower portions of the frontier hyperbola.\n\n\n3.5.4 Visualizing the Efficient Frontier\nFigure 3.5 displays the efficient frontier alongside individual assets and the benchmark portfolios.\n\nsummaries = pd.concat(\n  [summaries, efficient_frontier], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=summaries.query(\"type.isna()\"))\n  + geom_line(data=efficient_frontier, color=\"blue\", alpha=0.7)\n  + geom_point(data=summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3)\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"The Efficient Frontier and VN30 Constituents\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 3.5: The big dots indicate the location of the minimum-variance and the efficient portfolio. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe efficient frontier has a characteristic hyperbolic shape. The leftmost point is the minimum-variance portfolio. Moving up and to the right along the frontier, expected return increases but so does volatility. The upper portion of the hyperbola (above the minimum-variance portfolio) is the “efficient” part—these portfolios offer the highest return for each level of risk. The lower portion is “inefficient”—these portfolios are dominated by their mirror images on the upper portion.\nThe figure also reveals how dramatically diversification improves upon individual stock positions. Nearly all individual stocks lie well inside the efficient frontier, meaning investors can achieve the same expected return with much less risk, or much higher expected return with the same risk, simply by diversifying.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "02_modern_portfolio_theory.html#key-takeaways",
    "href": "02_modern_portfolio_theory.html#key-takeaways",
    "title": "3  Modern Portfolio Theory",
    "section": "3.6 Key Takeaways",
    "text": "3.6 Key Takeaways\nThis chapter introduced the concepts of Modern Portfolio Theory. The main insights are:\n\nPortfolio risk depends on correlations: The variance of a portfolio is not simply the weighted average of individual variances. Covariances between assets play a crucial role, creating opportunities for diversification.\nDiversification is the “only free lunch” in finance: By combining assets that don’t move perfectly together, investors can reduce risk without sacrificing expected return. This insight is the cornerstone of modern investment practice.\nThe minimum-variance portfolio minimizes risk: This portfolio depends only on the covariance matrix and serves as an important benchmark. It represents the least risky way to be fully invested in risky assets.\nEfficient portfolios balance risk and return: By accepting more variance, investors can earn higher expected returns. Efficient portfolios are those that offer the best possible trade-off.\nThe efficient frontier characterizes optimal portfolios: This boundary in mean-standard deviation space represents the menu of optimal choices available to mean-variance investors.\nTwo-fund separation simplifies implementation: Any efficient portfolio can be constructed from any two distinct efficient portfolios, reducing the computational burden of portfolio optimization.\n\nLooking ahead, several important complications arise in practice. First, the inputs to portfolio optimization (expected returns and covariances) must be estimated from data, and estimation error can dramatically affect portfolio performance. Second, real-world constraints such as transaction costs, short-sale restrictions, and position limits modify the optimization problem. Third, the assumption that investors care only about mean and variance may be too restrictive when returns are non-normal or when investors have more complex preferences. We address these extensions in subsequent chapters.\n\n\n\n\n\n\nMarkowitz, Harry. 1952. “Portfolio selection.” The Journal of Finance 7 (1): 77–91. https://doi.org/10.1111/j.1540-6261.1952.tb01525.x.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "03_capm.html",
    "href": "03_capm.html",
    "title": "4  The Capital Asset Pricing Model",
    "section": "",
    "text": "4.1 From Efficient Portfolios to Equilibrium Prices\nThe previous chapter on Modern Portfolio Theory (MPT) showed how an investor can construct portfolios that optimally trade off risk and expected return. But MPT leaves a crucial question unanswered: What determines the expected returns themselves? Why do some assets command higher risk premiums than others?\nThe Capital Asset Pricing Model (CAPM) answers this question. Developed simultaneously by Sharpe (1964), Lintner (1965), and Mossin (1966), the CAPM extends MPT to explain how assets should be priced in equilibrium when all investors follow mean-variance optimization principles. The CAPM’s central insight is both elegant and counterintuitive: not all risk is rewarded. Only the component of risk that cannot be diversified away (i.e., systematic risk) commands a risk premium in equilibrium.\nThe CAPM remains the cornerstone of asset pricing theory, not because it perfectly describes reality, but because it provides the simplest coherent framework for understanding the relationship between risk and expected return. Every extension and alternative model in asset pricing (e.g., from the Fama-French factors to consumption-based pricing) builds upon or reacts against the CAPM’s foundational logic.\nIn this chapter, we derive the CAPM from first principles, illustrate its theoretical underpinnings, and show how to estimate its parameters empirically. We download stock market data, estimate betas using regression analysis, and evaluate asset performance relative to model predictions.\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#systematic-versus-idiosyncratic-risk",
    "href": "03_capm.html#systematic-versus-idiosyncratic-risk",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.2 Systematic versus Idiosyncratic Risk",
    "text": "4.2 Systematic versus Idiosyncratic Risk\nBefore diving into the mathematics, we need to understand the fundamental distinction that makes the CAPM work: the difference between systematic and idiosyncratic risk.\n\n4.2.1 Idiosyncratic Risk: Diversifiable and Unrewarded\nConsider events that affect individual companies but not the broader market: a CEO resigns unexpectedly, a product launch fails, earnings disappoint analysts, or a factory experiences a fire. These company-specific events can dramatically affect individual stock prices, but they tend to average out across a diversified portfolio. When one company has bad news, another often has good news; the shocks are largely uncorrelated.\nThis idiosyncratic (or firm-specific) risk can be eliminated through diversification. By holding a portfolio of many stocks, an investor can reduce idiosyncratic risk to nearly zero. Since this risk can be avoided at no cost, investors should not expect compensation for bearing it. In equilibrium, idiosyncratic risk earns no premium.\n\n\n4.2.2 Systematic Risk: Undiversifiable and Priced\nSystematic risk, by contrast, affects all assets simultaneously. Recessions, interest rate changes, geopolitical crises, and pandemics impact virtually every company to some degree. No amount of diversification can eliminate exposure to these economy-wide shocks, they are inherent to participating in the market.\nSince systematic risk cannot be diversified away, investors genuinely dislike it. They must be compensated for bearing it. The CAPM formalizes this intuition: expected returns should depend only on systematic risk, not total risk. Two assets with identical total volatility can have very different expected returns if their systematic risk exposures differ.\n\n\n4.2.3 A Simple Illustration\nImagine two stocks with identical 30% annual volatility. Stock A is a gold mining company whose returns move opposite to the overall market: it does well when the economy struggles and poorly when it booms. Stock B is a luxury retailer that amplifies market movements: soaring in good times and crashing in bad times.\nWhich stock should offer higher expected returns? Intuition might suggest they should be equal since both have the same volatility. But the CAPM says Stock B should offer substantially higher returns. Why? Because Stock B performs poorly precisely when investors’ overall wealth is already down (during market crashes), making its returns particularly painful. Stock A, by contrast, provides insurance. Its strong performance during market downturns partially offsets losses elsewhere in the portfolio. Investors value this insurance property and are willing to accept lower expected returns in exchange.\nThis is the CAPM’s core insight: expected returns compensate investors for systematic risk exposure, measured by how an asset’s returns co-move with the market portfolio.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#data-preparation",
    "href": "03_capm.html#data-preparation",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.3 Data Preparation",
    "text": "4.3 Data Preparation\nBuilding on our analysis from the previous chapter, we examine the VN30 constituents as our asset universe. We download and prepare monthly return data:\n\nvn30_symbols = [\n    \"ACB\", \"BCM\", \"BID\", \"BVH\", \"CTG\", \"FPT\", \"GAS\", \"GVR\", \"HDB\", \"HPG\",\n    \"MBB\", \"MSN\", \"MWG\", \"PLX\", \"POW\", \"SAB\", \"SHB\", \"SSB\", \"STB\", \"TCB\",\n    \"TPB\", \"VCB\", \"VHM\", \"VIB\", \"VIC\", \"VJC\", \"VNM\", \"VPB\", \"VRE\", \"EIB\"\n]\n\n\nimport os\nimport boto3\nfrom botocore.client import Config\nfrom io import BytesIO\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\nWe process the raw price data to compute adjusted closing prices and standardize column names:\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\nprices = prices.sort_values([\"symbol\", \"date\"])\n\n\nprices_daily = prices[prices[\"symbol\"].isin(vn30_symbols)]\nprices_daily[[\"date\", \"symbol\", \"adjusted_close\"]].head(3)\n\n\n\n\n\n\n\n\ndate\nsymbol\nadjusted_close\n\n\n\n\n18176\n2010-01-04\nACB\n329.408244\n\n\n18177\n2010-01-05\nACB\n329.408244\n\n\n18178\n2010-01-06\nACB\n320.258015\n\n\n\n\n\n\n\n\n4.3.1 Computing Monthly Returns\nWe aggregate daily prices to monthly frequency. Using monthly returns rather than daily returns offers several advantages for portfolio analysis: monthly returns exhibit less noise, better approximate normality, and reduce the impact of microstructure effects like bid-ask bounce.\n\nreturns_monthly = (prices_daily\n    .assign(\n        date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp(\"M\")\n    )\n    .groupby([\"symbol\", \"date\"], as_index=False)\n    .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n    .sort_values([\"symbol\", \"date\"])\n    .assign(\n        ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    )\n)\n\nreturns_monthly.head(3)\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n0\nACB\n2010-01-31\n291.975489\nNaN\n\n\n1\nACB\n2010-02-28\n303.621235\n0.039886\n\n\n2\nACB\n2010-03-31\n273.784658\n-0.098269",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-risk-free-asset-and-the-investment-opportunity-set",
    "href": "03_capm.html#the-risk-free-asset-and-the-investment-opportunity-set",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.4 The Risk-Free Asset and the Investment Opportunity Set",
    "text": "4.4 The Risk-Free Asset and the Investment Opportunity Set\n\n4.4.1 Adding a Risk-Free Asset\nThe previous chapter on MPT considered portfolios composed entirely of risky assets, requiring that portfolio weights sum to one. The CAPM introduces a crucial new element: a risk-free asset that pays a constant interest rate \\(r_f\\) with zero volatility.\nThis seemingly simple addition fundamentally transforms the investment opportunity set. With a risk-free asset available, investors can choose to park some wealth in the safe asset and invest the remainder in risky assets. They can also borrow at the risk-free rate to leverage their risky positions.\nLet \\(\\omega \\in \\mathbb{R}^N\\) denote the portfolio weights in the \\(N\\) risky assets. Unlike before, these weights need not sum to one. The remainder, \\(1 - \\iota'\\omega\\) (where \\(\\iota\\) is a vector of ones), is invested in the risk-free asset.\n\n\n4.4.2 Portfolio Return with a Risk-Free Asset\nThe expected return on this combined portfolio is:\n\\[\n\\mu_\\omega = \\omega'\\mu + (1 - \\iota'\\omega)r_f = r_f + \\omega'(\\mu - r_f) = r_f + \\omega'\\tilde{\\mu}\n\\]\nwhere \\(\\mu\\) is the vector of expected returns on risky assets and \\(\\tilde{\\mu} = \\mu - r_f\\) denotes the vector of excess returns (returns above the risk-free rate).\nThis expression reveals an important decomposition: the portfolio’s expected return equals the risk-free rate plus a risk premium determined by the exposure to risky assets.\n\n\n4.4.3 Portfolio Variance\nSince the risk-free asset has zero volatility and zero covariance with risky assets, only the risky portion contributes to portfolio variance:\n\\[\n\\sigma_\\omega^2 = \\omega'\\Sigma\\omega\n\\]\nwhere \\(\\Sigma\\) is the variance-covariance matrix of risky asset returns. The portfolio’s volatility (standard deviation) is:\n\\[\n\\sigma_\\omega = \\sqrt{\\omega'\\Sigma\\omega}\n\\]\n\n\n4.4.4 Setting Up the Risk-Free Rate\nFor a realistic proxy of the risk-free rate, we use the Vietnam government bond yield. Government bonds of stable economies are considered “risk-free” because the government can always print money to meet its obligations (though this may cause inflation).\n\nall_dates = pd.date_range(\n    start=returns_monthly[\"date\"].min(), \n    end=returns_monthly[\"date\"].max(), \n    freq=\"ME\"\n)\n\n# Vietnam 10-Year Government Bond Yield (approximately 2.52% annualized)\nrf_annual = 0.0252\nrf_monthly_val = (1 + rf_annual)**(1/12) - 1\n\nrisk_free_monthly = pd.DataFrame({\n    \"date\": all_dates,\n    \"risk_free\": rf_monthly_val\n})\n\nrisk_free_monthly[\"date\"] = (\n    pd.to_datetime(risk_free_monthly[\"date\"])\n    .dt.to_period(\"M\")\n    .dt.to_timestamp(\"M\")\n)\n\nrisk_free_monthly.head(3)\n\n\n\n\n\n\n\n\ndate\nrisk_free\n\n\n\n\n0\n2010-01-31\n0.002076\n\n\n1\n2010-02-28\n0.002076\n\n\n2\n2010-03-31\n0.002076\n\n\n\n\n\n\n\nWe merge the risk-free rate with our returns data and compute excess returns:\n\nreturns_monthly = returns_monthly.merge(\n    risk_free_monthly[[\"date\", \"risk_free\"]], \n    on=\"date\", \n    how=\"left\"\n)\n\nrf = risk_free_monthly[\"risk_free\"].mean()\n\nreturns_monthly = (returns_monthly\n    .assign(\n        ret_excess=lambda x: x[\"ret\"] - x[\"risk_free\"]\n    )\n    .assign(\n        ret_excess=lambda x: x[\"ret_excess\"].clip(lower=-1)\n    )\n)\n\nreturns_monthly.head(3)\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\nrisk_free\nret_excess\n\n\n\n\n0\nACB\n2010-01-31\n291.975489\nNaN\n0.002076\nNaN\n\n\n1\nACB\n2010-02-28\n303.621235\n0.039886\n0.002076\n0.037810\n\n\n2\nACB\n2010-03-31\n273.784658\n-0.098269\n0.002076\n-0.100345",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-tangency-portfolio-where-everyone-invests",
    "href": "03_capm.html#the-tangency-portfolio-where-everyone-invests",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.5 The Tangency Portfolio: Where Everyone Invests",
    "text": "4.5 The Tangency Portfolio: Where Everyone Invests\n\n4.5.1 Deriving the Optimal Risky Portfolio\nWith a risk-free asset available, how should an investor allocate wealth across risky assets? Consider an investor who wants to achieve a target expected excess return \\(\\bar{\\mu}\\) with minimum variance. The optimization problem becomes:\n\\[\n\\min_\\omega \\omega'\\Sigma\\omega \\quad \\text{subject to} \\quad \\omega'\\tilde{\\mu} = \\bar{\\mu}\n\\]\nUsing the Lagrangian method:\n\\[\n\\mathcal{L}(\\omega, \\lambda) = \\omega'\\Sigma\\omega - \\lambda(\\omega'\\tilde{\\mu} - \\bar{\\mu})\n\\]\nThe first-order condition with respect to \\(\\omega\\) is:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda\\tilde{\\mu} = 0\n\\]\nSolving for the optimal weights:\n\\[\n\\omega^* = \\frac{\\lambda}{2}\\Sigma^{-1}\\tilde{\\mu}\n\\]\nThe constraint \\(\\omega'\\tilde{\\mu} = \\bar{\\mu}\\) determines \\(\\lambda\\):\n\\[\n\\bar{\\mu} = \\tilde{\\mu}'\\omega^* = \\frac{\\lambda}{2}\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu} \\implies \\lambda = \\frac{2\\bar{\\mu}}{\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu}}\n\\]\nSubstituting back:\n\\[\n\\omega^* = \\frac{\\bar{\\mu}}{\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu}}\\Sigma^{-1}\\tilde{\\mu}\n\\]\n\n\n4.5.2 The Tangency Portfolio\nNotice something remarkable: the direction of \\(\\omega^*\\) is always \\(\\Sigma^{-1}\\tilde{\\mu}\\), regardless of the target return \\(\\bar{\\mu}\\). Only the scale changes. This means all investors, regardless of their risk preferences, hold the same portfolio of risky assets. They differ only in how much they allocate to this portfolio versus the risk-free asset.\nTo obtain the portfolio of risky assets that is fully invested (weights summing to one), we normalize:\n\\[\n\\omega_{\\text{tan}} = \\frac{\\omega^*}{\\iota'\\omega^*} = \\frac{\\Sigma^{-1}(\\mu - r_f)}{\\iota'\\Sigma^{-1}(\\mu - r_f)}\n\\]\nThis is called the tangency portfolio (or maximum Sharpe ratio portfolio) because it lies at the point where the efficient frontier is tangent to the capital market line.\n\n\n4.5.3 The Sharpe Ratio and the Capital Market Line\nThe Sharpe ratio measures excess return per unit of volatility:\n\\[\n\\text{Sharpe Ratio} = \\frac{\\mu_p - r_f}{\\sigma_p}\n\\]\nThe tangency portfolio maximizes the Sharpe ratio among all possible portfolios. Any combination of the risk-free asset and the tangency portfolio lies on a straight line in mean-standard deviation space, called the Capital Market Line (CML):\n\\[\n\\mu_p = r_f + \\left(\\frac{\\mu_{\\text{tan}} - r_f}{\\sigma_{\\text{tan}}}\\right)\\sigma_p\n\\]\nThe slope of this line equals the Sharpe ratio of the tangency portfolio (i.e., the highest achievable Sharpe ratio).\n\n\n4.5.4 Computing the Tangency Portfolio\nLet’s compute the tangency portfolio for our VN30 universe:\n\nassets = (returns_monthly\n    .groupby(\"symbol\", as_index=False)\n    .agg(\n        mu=(\"ret\", \"mean\"),\n        sigma=(\"ret\", \"std\")\n    )\n)\n\nsigma = (returns_monthly\n    .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n    .cov()\n)\n\nmu = (returns_monthly\n    .groupby(\"symbol\")[\"ret\"]\n    .mean()\n    .values\n)\n\n\n# Compute tangency portfolio weights\nw_tan = np.linalg.solve(sigma, mu - rf)\nw_tan = w_tan / np.sum(w_tan)\n\n# Portfolio performance metrics\nmu_w = w_tan.T @ mu\nsigma_w = np.sqrt(w_tan.T @ sigma @ w_tan)\n\nefficient_portfolios = pd.DataFrame([\n    {\"symbol\": r\"$\\omega_{\\mathrm{tan}}$\", \"mu\": mu_w, \"sigma\": sigma_w},\n    {\"symbol\": r\"$r_f$\", \"mu\": rf, \"sigma\": 0}\n])\n\nsharpe_ratio = (mu_w - rf) / sigma_w\n\nprint(f\"Tangency Portfolio Sharpe Ratio: {sharpe_ratio:.4f}\")\nprint(efficient_portfolios)\n\nTangency Portfolio Sharpe Ratio: -0.5552\n                    symbol        mu     sigma\n0  $\\omega_{\\mathrm{tan}}$ -0.041157  0.077866\n1                    $r_f$  0.002076  0.000000\n\n\n\n\n4.5.5 Visualizing the Efficient Frontier with a Risk-Free Asset\nFigure 4.1 shows the efficient frontier when a risk-free asset is available. The frontier is now a straight line (the Capital Market Line) connecting the risk-free asset to the tangency portfolio and extending beyond.\n\nefficient_portfolios_figure = (\n    ggplot(efficient_portfolios, aes(x=\"sigma\", y=\"mu\"))\n    + geom_point(data=assets)\n    + geom_point(data=efficient_portfolios, color=\"blue\", size=3)\n    + geom_label(\n        aes(label=\"symbol\"), \n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Volatility (Standard Deviation)\", \n        y=\"Expected Return\",\n        title=\"Efficient Frontier with Risk-Free Asset (VN30)\"\n    )\n    + geom_abline(slope=sharpe_ratio, intercept=rf, linetype=\"dotted\")\n)\n\nefficient_portfolios_figure.show()\n\n\n\n\n\n\n\nFigure 4.1: The efficient frontier with a risk-free asset becomes a straight line (the Capital Market Line) connecting the risk-free rate to the tangency portfolio. Individual assets lie below this line, demonstrating the benefits of diversification.\n\n\n\n\n\nYou may notice that estimated expected returns appear quite low, some even negative. This is not a model failure but reflects the realities of estimation:\n\nSample period matters: If the estimation window includes market downturns (such as the 2022-2023 period), realized average returns can be near zero or negative. Mean-variance optimization takes sample means literally.\nEstimation noise in emerging markets: With volatile emerging market data, sample means are dominated by noise. A few extremely bad months can push the average below the risk-free rate even if the long-run equity premium is positive.\n\nThis highlights a fundamental challenge in portfolio optimization: the inputs we observe (historical returns) are noisy estimates of the true parameters we need (expected future returns).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-capm-equation-risk-and-expected-return",
    "href": "03_capm.html#the-capm-equation-risk-and-expected-return",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.6 The CAPM Equation: Risk and Expected Return",
    "text": "4.6 The CAPM Equation: Risk and Expected Return\n\n4.6.1 From Individual Optimization to Market Equilibrium\nSo far, we’ve focused on one investor’s optimization problem. The CAPM’s power comes from considering what happens when all investors optimize simultaneously.\nIf all investors follow mean-variance optimization, they all hold some combination of the risk-free asset and the tangency portfolio. The only difference between investors is their risk tolerance. More risk-averse investors hold more of the risk-free asset, while risk-tolerant investors may even borrow at the risk-free rate to leverage their position in the tangency portfolio.\n\n\n4.6.2 The Market Portfolio\nIn equilibrium, the total demand for each risky asset must equal its supply. Since all investors hold the same portfolio of risky assets (the tangency portfolio), the equilibrium portfolio weights must equal the market capitalization weights. The tangency portfolio is the market portfolio.\nThis insight has enormous practical implications: instead of estimating expected returns and covariances to compute the tangency portfolio, we can simply use the market portfolio (approximated by a broad market index) as a proxy.\n\n\n4.6.3 Deriving the CAPM Equation\nFrom the first-order conditions of the optimization problem, we derived that:\n\\[\n\\tilde{\\mu} = \\frac{2}{\\lambda}\\Sigma\\omega^*\n\\]\nSince \\(\\omega^*\\) is proportional to \\(\\omega_{\\text{tan}}\\), and in equilibrium \\(\\omega_{\\text{tan}}\\) equals the market portfolio \\(\\omega_m\\):\n\\[\n\\tilde{\\mu} = c \\cdot \\Sigma\\omega_m\n\\]\nfor some constant \\(c\\). The \\(i\\)-th element of \\(\\Sigma\\omega_m\\) is:\n\\[\n\\sum_{j=1}^N \\sigma_{ij}\\omega_{m,j} = \\text{Cov}(r_i, r_m)\n\\]\nwhere \\(r_m = \\sum_j \\omega_{m,j} r_j\\) is the return on the market portfolio.\nFor the market portfolio itself:\n\\[\n\\tilde{\\mu}_m = c \\cdot \\text{Var}(r_m) = c \\cdot \\sigma_m^2\n\\]\nTherefore \\(c = \\tilde{\\mu}_m / \\sigma_m^2\\), and for any asset \\(i\\):\n\\[\n\\tilde{\\mu}_i = \\frac{\\tilde{\\mu}_m}{\\sigma_m^2} \\text{Cov}(r_i, r_m) = \\beta_i \\tilde{\\mu}_m\n\\]\nwhere:\n\\[\n\\beta_i = \\frac{\\text{Cov}(r_i, r_m)}{\\text{Var}(r_m)}\n\\]\nThis is the famous CAPM equation:\n\\[\nE(r_i) - r_f = \\beta_i [E(r_m) - r_f]\n\\]\n\n\n4.6.4 Interpreting Beta\nBeta (\\(\\beta_i\\)) measures an asset’s systematic risk (i.e., its sensitivity to market movements). The interpretation is straightforward:\n\n\\(\\beta = 1\\): The asset moves one-for-one with the market (average systematic risk)\n\\(\\beta &gt; 1\\): The asset amplifies market movements (aggressive, high systematic risk)\n\\(\\beta &lt; 1\\): The asset dampens market movements (defensive, low systematic risk)\n\\(\\beta &lt; 0\\): The asset moves opposite to the market (provides insurance)\n\nThe CAPM says that expected excess return is proportional to beta, not to total volatility. This explains why:\n\nAn asset with zero beta earns only the risk-free rate (i.e., its risk is entirely idiosyncratic).\nAn asset with beta of 1 earns the market risk premium\nA negative-beta asset earns less than the risk-free rate (i.e., investors pay for its insurance properties).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#the-security-market-line",
    "href": "03_capm.html#the-security-market-line",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.7 The Security Market Line",
    "text": "4.7 The Security Market Line\nThe CAPM predicts a linear relationship between beta and expected return. This relationship is called the Security Market Line (SML):\n\\[\nE(r_i) = r_f + \\beta_i [E(r_m) - r_f]\n\\]\nUnlike the Capital Market Line (which plots expected return against total risk), the Security Market Line plots expected return against systematic risk (beta).\n\nbetas = (sigma @ w_tan) / (w_tan.T @ sigma @ w_tan)\nassets[\"beta\"] = betas.values\n\nprice_of_risk = float(w_tan.T @ mu - rf)\n\nassets_figure = (\n    ggplot(assets, aes(x=\"beta\", y=\"mu\"))\n    + geom_point()\n    + geom_abline(intercept=rf, slope=price_of_risk)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Beta (Systematic Risk)\", \n        y=\"Expected Return\",\n        title=\"Security Market Line\"\n    )\n    + annotate(\"text\", x=0.05, y=rf + 0.001, label=\"Risk-free rate\")\n)\n\nassets_figure.show()\n\n\n\n\n\n\n\n\nYou may observe that the estimated SML has a negative slope, which seems to contradict CAPM’s prediction. This reflects a negative estimated market risk premium in our sample period (i.e., the market portfolio earned less than the risk-free rate).\nWhen the market risk premium is negative, CAPM predicts that high-beta stocks should have lower expected returns than low-beta stocks. This is not a model failure, the model is behaving consistently. Rather, it reflects an unusual (but not impossible) sample period where risky assets underperformed safe assets.\nThis observation highlights an important distinction: CAPM describes expected returns in equilibrium, but realized returns over any particular period may differ substantially from expectations due to shocks and surprises.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#empirical-estimation-of-capm-parameters",
    "href": "03_capm.html#empirical-estimation-of-capm-parameters",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.8 Empirical Estimation of CAPM Parameters",
    "text": "4.8 Empirical Estimation of CAPM Parameters\n\n4.8.1 The Regression Framework\nIn practice, we estimate CAPM parameters using time-series regression. The model implies:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i(r_{m,t} - r_{f,t}) + \\varepsilon_{i,t}\n\\]\nwhere:\n\n\\(r_{i,t}\\): Return on asset \\(i\\) at time \\(t\\)\n\\(r_{f,t}\\): Risk-free rate at time \\(t\\)\n\\(r_{m,t}\\): Market return at time \\(t\\)\n\\(\\alpha_i\\): Intercept (should be zero if CAPM holds)\n\\(\\beta_i\\): Systematic risk (slope coefficient)\n\\(\\varepsilon_{i,t}\\): Idiosyncratic shock (residual)\n\n\n\n4.8.2 Alpha: Risk-Adjusted Performance\nThe intercept \\(\\alpha_i\\) measures risk-adjusted performance. If CAPM holds perfectly, alpha should be zero for all assets (i.e., any excess return is exactly compensated by systematic risk).\n\n\\(\\alpha &gt; 0\\): The asset outperformed its CAPM-predicted return (positive abnormal return)\n\\(\\alpha &lt; 0\\): The asset underperformed its CAPM-predicted return (negative abnormal return)\n\nPositive alpha is the holy grail of active management: earning returns beyond what systematic risk exposure would justify.\n\n\n4.8.3 Loading Factor Data\nWe use Fama-French market excess returns as our market portfolio proxy. These data provide a widely accepted benchmark that is already adjusted for the risk-free rate:\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nfactors.head(3)\n\n\n\n\n\n\n\n\ndate\nsmb\nhml\nrmw\ncma\nmkt_excess\n\n\n\n\n0\n2011-07-31\n0.008933\n-0.013099\n0.012139\n-0.009529\n-0.011287\n\n\n1\n2011-08-31\n0.004830\n-0.016656\n0.014516\n-0.003981\n0.007856\n\n\n2\n2011-09-30\n0.004970\n-0.000462\n0.008899\n0.001241\n-0.006501\n\n\n\n\n\n\n\n\n\n4.8.4 Running the Regressions\nWe estimate CAPM regressions for each stock in our universe:\n\nimport statsmodels.formula.api as smf\n\nreturns_excess_monthly = (returns_monthly\n    .merge(factors, on=\"date\", how=\"left\")\n    .assign(ret_excess=lambda x: x[\"ret\"] - x[\"risk_free\"])\n)\n\n\ndef estimate_capm(data):\n    model = smf.ols(\"ret_excess ~ mkt_excess\", data=data).fit()\n    result = pd.DataFrame({\n        \"coefficient\": [\"alpha\", \"beta\"],\n        \"estimate\": model.params.values,\n        \"t_statistic\": model.tvalues.values\n    })\n    return result\n\n\ncapm_results = (returns_excess_monthly\n    .groupby(\"symbol\", group_keys=True)\n    .apply(estimate_capm)\n    .reset_index()\n)\n\ncapm_results.head(4)\n\n\n\n\n\n\n\n\nsymbol\nlevel_1\ncoefficient\nestimate\nt_statistic\n\n\n\n\n0\nACB\n0\nalpha\n-0.007479\n-0.876326\n\n\n1\nACB\n1\nbeta\n-0.240019\n-0.252484\n\n\n2\nBCM\n0\nalpha\n0.027827\n1.737895\n\n\n3\nBCM\n1\nbeta\n4.150186\n2.625283\n\n\n\n\n\n\n\n\n\n4.8.5 Visualizing Alpha Estimates\nFigure 4.2 shows the estimated alphas across our VN30 sample. Statistical significance (at the 95% level) is indicated by color.\n\nalphas = (capm_results\n    .query(\"coefficient == 'alpha'\")\n    .assign(is_significant=lambda x: np.abs(x[\"t_statistic\"]) &gt;= 1.96)\n)\n\nalphas[\"symbol\"] = pd.Categorical(\n    alphas[\"symbol\"],\n    categories=alphas.sort_values(\"estimate\")[\"symbol\"],\n    ordered=True\n)\n\nalphas_figure = (\n    ggplot(alphas, aes(y=\"estimate\", x=\"symbol\", fill=\"is_significant\"))\n    + geom_col()\n    + scale_y_continuous(labels=percent_format())\n    + coord_flip()\n    + labs(\n        x=\"\", \n        y=\"Estimated Alpha (Monthly)\", \n        fill=\"Significant at 95%?\",\n        title=\"Estimated CAPM Alphas for VN30 Index Constituents\"\n    )\n)\n\nalphas_figure.show()\n\n\n\n\n\n\n\nFigure 4.2: Estimated CAPM alphas for VN30 index constituents. Color indicates statistical significance at the 95% confidence level. Most alphas are statistically indistinguishable from zero, consistent with CAPM predictions.\n\n\n\n\n\nThe distribution of alphas provides evidence on CAPM’s empirical validity. If the model holds, we expect:\n\nMost alphas close to zero\nFew statistically significant alphas\nRoughly equal numbers of positive and negative alphas\n\nSystematic patterns in alphas, such as consistently positive alphas for certain types of stocks, would suggest the CAPM is incomplete and that additional risk factors may be needed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#limitations-and-extensions",
    "href": "03_capm.html#limitations-and-extensions",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.9 Limitations and Extensions",
    "text": "4.9 Limitations and Extensions\n\n4.9.1 The Market Portfolio Problem\nA fundamental challenge in testing the CAPM is identifying the market portfolio. The theory requires a portfolio that includes all investable assets, not just stocks, but also bonds, real estate, private businesses, human capital, and even intangible assets. In practice, we use proxies like broad market indices (VNI, S&P 500), but these capture only publicly traded equities.\nThis limitation is profound. As Richard Roll famously argued, the CAPM is essentially untestable because the true market portfolio is unobservable. Any test of the CAPM is simultaneously a test of whether our proxy adequately represents the market.\n\n\n4.9.2 Time-Varying Betas\nThe CAPM assumes that betas are constant over time, but this assumption rarely holds in practice. Companies undergo changes that affect their market sensitivity:\n\nCapital structure changes: Increasing leverage raises beta\nBusiness model evolution: Diversification into new industries can alter systematic risk\nMarket conditions: Betas often increase during market stress\n\nConditional CAPM models (Jagannathan and Wang 1996) address this by allowing risk premiums and betas to vary with the business cycle.\n\n\n4.9.3 Empirical Anomalies\nDecades of empirical research have documented patterns in stock returns that CAPM cannot explain:\n\nSize effect: Small-cap stocks tend to outperform large-cap stocks, even after adjusting for beta\nValue effect: Stocks with high book-to-market ratios outperform growth stocks\nMomentum: Stocks that performed well recently tend to continue performing well\n\nThese anomalies suggest that systematic risk has multiple dimensions beyond market exposure.\n\n\n4.9.4 Multifactor Extensions\nThe limitations of CAPM have led to increasingly sophisticated asset pricing models. The Fama-French three-factor model (Fama and French 1992) adds two factors to capture size and value effects:\n\nSMB (Small Minus Big): Returns on small stocks minus large stocks\nHML (High Minus Low): Returns on value stocks minus growth stocks\n\nThe Fama-French five-factor model (Fama and French 2015) adds two more dimensions:\n\nRMW (Robust Minus Weak): Returns on profitable firms minus unprofitable firms\n\nCMA (Conservative Minus Aggressive): Returns on conservative investors minus aggressive investors\n\nThe Carhart four-factor model (Carhart 1997) adds momentum to the three-factor framework.\nOther theoretical developments include:\n\nConsumption CAPM: Links asset prices to macroeconomic consumption risk\nQ-factor model (Hou, Xue, and Zhang 2014): Derives factors from investment-based asset pricing theory\nArbitrage Pricing Theory: Allows for multiple sources of systematic risk without specifying their identity\n\nDespite its limitations, the CAPM remains valuable as a conceptual benchmark. Its core insight (i.e., only systematic, undiversifiable risk commands a premium) continues to inform how we think about risk and return.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "03_capm.html#key-takeaways",
    "href": "03_capm.html#key-takeaways",
    "title": "4  The Capital Asset Pricing Model",
    "section": "4.10 Key Takeaways",
    "text": "4.10 Key Takeaways\nThis chapter introduced the Capital Asset Pricing Model and its implications for understanding the relationship between risk and expected return. The main insights are:\n\nNot all risk is rewarded: The CAPM distinguishes between systematic risk (which cannot be diversified away and commands a premium) and idiosyncratic risk (which can be eliminated through diversification and earns no premium).\nThe tangency portfolio is universal: When a risk-free asset exists, all mean-variance investors hold the same portfolio of risky assets (i.e., the tangency or maximum Sharpe ratio portfolio). They differ only in how much they allocate to this portfolio versus the risk-free asset.\nIn equilibrium, the tangency portfolio is the market portfolio: Since all investors hold the same risky portfolio, and total demand must equal supply, the equilibrium portfolio weights are market capitalization weights.\nExpected returns depend on beta: The CAPM equation states that expected excess return equals beta times the market risk premium. Beta measures covariance with the market portfolio, normalized by market variance.\nAlpha measures risk-adjusted performance: Positive alpha indicates returns above what systematic risk would justify; negative alpha indicates underperformance.\nEmpirical challenges exist: Testing the CAPM requires identifying the market portfolio, which is unobservable in practice. Documented anomalies (size, value, momentum) suggest additional risk factors beyond market exposure.\nExtensions abound: Multifactor models like Fama-French extend the CAPM framework by adding factors that capture dimensions of systematic risk the market factor misses.\n\nThe CAPM’s elegance lies in its simplicity: a single factor (i.e., exposure to the market) should explain expected returns in equilibrium. While reality is more complex, this framework provides the foundation for all modern asset pricing theory.\n\n\n\n\n\n\nCarhart, Mark M. 1997. “On persistence in mutual fund performance.” The Journal of Finance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock returns.” The Journal of Finance 47 (2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2014. “Digesting anomalies: An investment approach.” Review of Financial Studies 28 (3): 650–705. https://doi.org/10.1093/rfs/hhu068.\n\n\nJagannathan, Ravi, and Zhenyu Wang. 1996. “The conditional CAPM and the cross-section of expected returns.” The Journal of Finance 51 (1): 3–53. https://doi.org/10.2307/2329301.\n\n\nLintner, John. 1965. “Security prices, risk, and maximal gains from diversification.” The Journal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital asset market.” Econometrica 34 (4): 768–83. https://doi.org/10.2307/1910098.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html",
    "href": "04_financial_statement_analysis.html",
    "title": "5  Financial Statement Analysis",
    "section": "",
    "text": "5.1 From Market Prices to Fundamental Value\nThe previous chapters focused on how financial markets price assets in equilibrium. The Capital Asset Pricing Model showed that expected returns depend on systematic risk exposure, while Modern Portfolio Theory demonstrated how to construct efficient portfolios. But these frameworks take expected returns and risk as given, they don’t explain where these expectations come from.\nFinancial statement analysis addresses this gap. By examining a company’s accounting records, investors can form independent assessments of firm value, identify mispriced securities, and understand the economic forces driving business performance. Financial statements provide the primary source of standardized information about a company’s operations, financial position, and cash generation. Their legal requirements and standardized formats make them particularly valuable. Every publicly traded company must file them, creating a level playing field for analysis.\nThis chapter introduces the three primary financial statements: the balance sheet, income statement, and cash flow statement. We then demonstrate how to transform raw accounting data into meaningful financial ratios that facilitate comparison across companies and over time. These ratios serve multiple purposes: they enable investors to benchmark companies against peers, help creditors assess default risk, and provide inputs for asset pricing models like the Fama-French factors we will encounter in later chapters.\nOur analysis combines theoretical frameworks with practical implementation using Vietnamese market data. By the end of this chapter, you will understand how to access financial statements, calculate key ratios across multiple categories, and interpret these metrics in context.\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#the-three-financial-statements",
    "href": "04_financial_statement_analysis.html#the-three-financial-statements",
    "title": "5  Financial Statement Analysis",
    "section": "5.2 The Three Financial Statements",
    "text": "5.2 The Three Financial Statements\nBefore diving into ratios and analysis, we need to understand the three interconnected statements that form the foundation of financial reporting. Each statement answers a different question about the company, and together they provide a comprehensive picture of financial health.\n\n5.2.1 The Balance Sheet: A Snapshot of Financial Position\nThe balance sheet captures a company’s financial position at a specific moment in time, think of it as a photograph rather than a movie. It lists everything the company owns (assets), everything it owes (liabilities), and the residual claim belonging to shareholders (equity). These three components are linked by the fundamental accounting equation:\n\\[\n\\text{Assets} = \\text{Liabilities} + \\text{Equity}\n\\]\nThis equation is not merely a definition, it reflects a core economic principle. A company’s resources (assets) must be financed from somewhere: either borrowed from creditors (liabilities) or contributed by owners (equity). Every transaction affects both sides equally, maintaining the balance.\nAssets represent resources the company controls that are expected to generate future economic benefits:\n\nCurrent assets can be converted to cash within one year: cash and equivalents, short-term investments, accounts receivable (money owed by customers), and inventory (raw materials, work-in-progress, and finished goods)\nNon-current assets support operations beyond one year: property, plant, and equipment (PP&E), long-term investments, and intangible assets like patents, trademarks, and goodwill\n\nLiabilities encompass obligations to external parties:\n\nCurrent liabilities come due within one year: accounts payable (money owed to suppliers), short-term debt, accrued expenses, and the current portion of long-term debt\nNon-current liabilities extend beyond one year: long-term debt, bonds payable, pension obligations, and deferred tax liabilities\n\nShareholders’ equity represents the owners’ residual claim:\n\nCommon stock and additional paid-in capital from share issuance\nRetained earnings (i.e., accumulated profits reinvested rather than distributed as dividends)\nTreasury stock: shares repurchased by the company\n\nUnderstanding these categories is essential for ratio analysis. Current assets and liabilities determine short-term liquidity, while the mix of debt and equity reveals capital structure choices.\n\n\n5.2.2 The Income Statement: Performance Over Time\nWhile the balance sheet provides a snapshot, the income statement (also called the profit and loss statement, or P&L) measures financial performance over a period (e.g., a quarter or year). It follows a hierarchical structure that progressively captures different levels of profitability:\n\\[\n\\text{Revenue} - \\text{COGS} = \\text{Gross Profit}\n\\]\n\\[\n\\text{Gross Profit} - \\text{Operating Expenses} = \\text{Operating Income (EBIT)}\n\\]\n\\[\n\\text{EBIT} - \\text{Interest} - \\text{Taxes} = \\text{Net Income}\n\\]\nEach line reveals something different about the business:\n\nRevenue (Sales): Total income from goods or services sold (i.e., the “top line”)\nCost of Goods Sold (COGS): Direct costs of producing what was sold (materials, direct labor, manufacturing overhead)\nGross Profit: Revenue minus COGS, measuring basic profitability from core operations\nOperating Expenses: Costs of running the business beyond production (selling, general & administrative expenses, research & development)\nOperating Income (EBIT): Earnings Before Interest and Taxes, measuring profitability from operations before financing decisions and taxes\nInterest Expense: The cost of debt financing\nNet Income: The “bottom line” (i.e., total profit after all expenses)\n\nThe income statement’s hierarchical structure allows analysts to identify where profitability problems originate. A company with strong gross margins but weak net income might have bloated overhead costs. One with weak gross margins faces fundamental pricing or production challenges.\n\n\n5.2.3 The Cash Flow Statement: Following the Money\nThe cash flow statement bridges a critical gap: profitable companies can run out of cash, and unprofitable companies can generate positive cash flow. This happens because accrual accounting (used in the income statement) recognizes revenue when earned and expenses when incurred, not when cash changes hands.\nThe cash flow statement tracks actual cash movements, divided into three categories:\n\nOperating activities: Cash generated from core business operations. Starts with net income, then adjusts for non-cash items (depreciation, changes in working capital)\nInvesting activities: Cash spent on or received from long-term investments (e.g., purchasing equipment, acquiring businesses, selling assets)\nFinancing activities: Cash flows from capital structure decisions (e.g., issuing stock, borrowing, repaying debt, paying dividends, buying back shares)\n\nA company can show strong net income while burning cash if it’s building inventory, extending generous credit terms, or making large capital expenditures. Conversely, a company reporting losses might generate positive operating cash flow by collecting receivables faster than it pays suppliers.\n\n\n5.2.4 Illustrating with FPT’s Financial Statements\nTo see these concepts in practice, let’s examine FPT Corporation’s 2023 financial statements. FPT is one of Vietnam’s largest technology companies, providing IT services, telecommunications, and education.\n\n# Placeholder for FPT balance sheet visualization\n# In practice, this would display the actual PDF or cleaned data\n# from DataCore's acquisition pipeline\n\n# Example structure of what the balance sheet data looks like:\n# Assets: Current assets (cash, receivables, inventory) + Non-current assets (PP&E, intangibles)\n# Liabilities: Current liabilities (payables, short-term debt) + Non-current liabilities (long-term debt)\n# Equity: Common stock + Retained earnings\n\nThe balance sheet demonstrates the fundamental accounting equation in action. FPT’s assets (e.g., spanning cash, receivables, technology infrastructure, and intangible assets like software) exactly equal the sum of its liabilities and equity.\n\n# Placeholder for FPT income statement visualization\n# Shows the progression from revenue through various profit measures to net income\n\nFPT’s income statement reveals how the company transforms revenue into profit. The progression from gross profit through operating income to net income shows the impact of operating expenses, interest costs, and taxes.\n\n# Placeholder for FPT cash flow statement visualization\n# Reconciles net income with actual cash generation\n\nThe cash flow statement shows how FPT’s reported profits translate into actual cash. Differences between net income and operating cash flow reveal the impact of working capital management and non-cash expenses.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#loading-financial-statement-data",
    "href": "04_financial_statement_analysis.html#loading-financial-statement-data",
    "title": "5  Financial Statement Analysis",
    "section": "5.3 Loading Financial Statement Data",
    "text": "5.3 Loading Financial Statement Data\nWe now turn to systematic analysis across multiple companies. We load financial statement data for the VN30 index constituents (i.e., the 30 largest and most liquid stocks on Vietnam’s Ho Chi Minh Stock Exchange).\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncomp_vn = pd.read_sql_query(\n    sql=\"SELECT * FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\ncomp_vn.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nxsga\nxint\noancf\ncapx\nbe\nop\nat_lag\ninv\ntotal_debt\nselling_general_and_administrative_expenses\n\n\n\n\n0\nAGF\n1998\n8.845141e+10\nNone\n5.469709e+09\n0.000000e+00\nNone\nNone\n0.0\n1.110705e+10\n...\n1.199765e+10\n0.0\nNaN\nNaN\n2.656020e+10\n0.711195\nNaN\nNaN\n0.000000e+00\n1.990718e+10\n\n\n1\nBBC\n1999\n5.672574e+10\nNone\n5.354939e+09\n5.354939e+09\nNone\nNone\n0.0\n0.000000e+00\n...\n9.396468e+09\n0.0\n2.687635e+10\n1.097031e+10\n3.211410e+10\n0.728193\nNaN\nNaN\n1.505529e+09\n2.387858e+10\n\n\n2\nAGF\n1999\n9.558392e+10\nNone\n2.609276e+09\n0.000000e+00\nNone\nNone\n0.0\n1.008298e+10\n...\n1.595913e+10\n0.0\n1.675607e+10\n3.970966e+09\n3.576596e+10\n0.816972\n1.068410e+11\n0.090477\n0.000000e+00\n2.744458e+10\n\n\n\n\n3 rows × 333 columns\n\n\n\n\nvn30_symbols = [\n    \"ACB\", \"BCM\", \"BID\", \"BVH\", \"CTG\", \"FPT\", \"GAS\", \"GVR\", \"HDB\", \"HPG\",\n    \"MBB\", \"MSN\", \"MWG\", \"PLX\", \"POW\", \"SAB\", \"SHB\", \"SSB\", \"STB\", \"TCB\",\n    \"TPB\", \"VCB\", \"VHM\", \"VIB\", \"VIC\", \"VJC\", \"VNM\", \"VPB\", \"VRE\", \"EIB\"\n]\n\ncomp_vn30 = comp_vn[comp_vn[\"symbol\"].isin(vn30_symbols)]\ncomp_vn30.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nxsga\nxint\noancf\ncapx\nbe\nop\nat_lag\ninv\ntotal_debt\nselling_general_and_administrative_expenses\n\n\n\n\n11\nFPT\n2002\n5.098910e+11\nNone\n1.027470e+11\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n4.178100e+10\n9.008000e+09\n3.203600e+10\n2.202800e+10\n3.125400e+10\n3.293018\nNaN\nNaN\n0.0\n1.235850e+11\n\n\n17\nVNM\n2003\n2.101406e+12\nNone\n6.925924e+11\n6.925924e+11\nNone\nNone\n0.0\n0.0\n...\n9.089413e+10\n1.698909e+10\nNaN\nNaN\n1.560789e+12\n0.663257\nNaN\nNaN\n0.0\n5.037799e+11\n\n\n21\nFPT\n2003\n9.171390e+11\nNone\n7.995600e+10\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n8.584800e+10\n1.286700e+10\n-2.905420e+11\n3.753300e+10\n1.697000e+11\n0.940218\n5.504080e+11\n0.779104\n0.0\n1.968430e+11\n\n\n\n\n3 rows × 333 columns\n\n\n\nThis dataset provides the foundation for calculating financial ratios and conducting cross-sectional comparisons. Each row contains balance sheet, income statement, and cash flow items for a company-year observation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#liquidity-ratios-can-the-company-pay-its-bills",
    "href": "04_financial_statement_analysis.html#liquidity-ratios-can-the-company-pay-its-bills",
    "title": "5  Financial Statement Analysis",
    "section": "5.4 Liquidity Ratios: Can the Company Pay Its Bills?",
    "text": "5.4 Liquidity Ratios: Can the Company Pay Its Bills?\nLiquidity ratios assess a company’s ability to meet short-term obligations. These metrics matter most to creditors, suppliers, and employees who need assurance that the company can pay its bills. They’re calculated using balance sheet items, comparing liquid assets against near-term liabilities.\n\n5.4.1 The Current Ratio\nThe most basic liquidity measure compares all current assets to current liabilities:\n\\[\n\\text{Current Ratio} = \\frac{\\text{Current Assets}}{\\text{Current Liabilities}}\n\\]\nA ratio above one indicates the company has enough current assets to cover obligations due within one year. However, the interpretation depends heavily on the composition of current assets. A company with current assets tied up in slow-moving inventory is less liquid than one holding cash.\n\n\n5.4.2 The Quick Ratio\nThe quick ratio (or “acid test”) provides a more stringent measure by excluding inventory:\n\\[\n\\text{Quick Ratio} = \\frac{\\text{Current Assets} - \\text{Inventory}}{\\text{Current Liabilities}}\n\\]\nWhy exclude inventory? Inventory is typically the least liquid current asset. It must be sold (potentially at a discount) before generating cash. A company facing a liquidity crisis cannot easily convert raw materials or finished goods into immediate cash. The quick ratio answers: “Can we pay our bills without relying on inventory sales?”\n\n\n5.4.3 The Cash Ratio\nThe most conservative liquidity measure focuses solely on the most liquid assets:\n\\[\n\\text{Cash Ratio} = \\frac{\\text{Cash and Cash Equivalents}}{\\text{Current Liabilities}}\n\\]\nWhile a ratio of one indicates robust liquidity, most companies maintain lower cash ratios to avoid holding excessive non-productive assets. Cash sitting in bank accounts could otherwise be invested in growth opportunities, returned to shareholders, or used to pay down costly debt.\n\n\n5.4.4 Calculating Liquidity Ratios\nLet’s compute these ratios for our VN30 sample:\n\nbalance_sheet_statements = (comp_vn30\n    .assign(\n        fiscal_year=lambda x: x[\"year\"].astype(int),\n        \n        # Current Ratio: Current Assets / Current Liabilities\n        current_ratio=lambda x: x[\"act\"] / x[\"lct\"],\n        \n        # Quick Ratio: (Current Assets - Inventory) / Current Liabilities\n        quick_ratio=lambda x: (x[\"act\"] - x[\"inv\"]) / x[\"lct\"],\n        \n        # Cash Ratio: Cash and Equivalents / Current Liabilities\n        cash_ratio=lambda x: x[\"ca_cce\"] / x[\"lct\"],\n        \n        label=lambda x: np.where(\n            x[\"symbol\"].isin(vn30_symbols), x[\"symbol\"], np.nan\n        )\n    )\n)\n\nbalance_sheet_statements.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nop\nat_lag\ninv\ntotal_debt\nselling_general_and_administrative_expenses\nfiscal_year\ncurrent_ratio\nquick_ratio\ncash_ratio\nlabel\n\n\n\n\n11\nFPT\n2002\n5.098910e+11\nNone\n1.027470e+11\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n3.293018\nNaN\nNaN\n0.0\n1.235850e+11\n2002\n1.211413\nNaN\n0.244109\nFPT\n\n\n17\nVNM\n2003\n2.101406e+12\nNone\n6.925924e+11\n6.925924e+11\nNone\nNone\n0.0\n0.0\n...\n0.663257\nNaN\nNaN\n0.0\n5.037799e+11\n2003\n2.195772\nNaN\n0.723694\nVNM\n\n\n21\nFPT\n2003\n9.171390e+11\nNone\n7.995600e+10\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n0.940218\n5.504080e+11\n0.779104\n0.0\n1.968430e+11\n2003\n1.274633\n1.274633\n0.111122\nFPT\n\n\n\n\n3 rows × 338 columns\n\n\n\n\n\n5.4.5 Cross-Sectional Comparison of Liquidity\nFigure 5.1 compares liquidity ratios across companies for the most recent fiscal year. This cross-sectional view reveals how different business models and industries maintain different liquidity profiles.\n\nliquidity_ratios = (balance_sheet_statements\n    .query(\"year == 2023 & label.notna()\")\n    .get([\"symbol\", \"current_ratio\", \"quick_ratio\", \"cash_ratio\"])\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        name=lambda x: x[\"name\"].str.replace(\"_\", \" \").str.title()\n    )\n)\n\nliquidity_ratios_figure = (\n    ggplot(liquidity_ratios, aes(y=\"value\", x=\"name\", fill=\"symbol\"))\n    + geom_col(position=\"dodge\")\n    + coord_flip()\n    + labs(\n        x=\"\", y=\"Ratio Value\", fill=\"\",\n        title=\"Liquidity Ratios for VN30 Stocks (2023)\"\n    )\n)\n\nliquidity_ratios_figure.show()\n\n\n\n\n\n\n\nFigure 5.1: Liquidity ratios measure a company’s ability to meet short-term obligations. Higher values indicate greater liquidity, though excessively high ratios may suggest inefficient use of assets.\n\n\n\n\n\nSeveral patterns emerge from this comparison. Banks and financial institutions typically show different liquidity profiles than industrial companies due to their unique business models. Companies with high inventory (retailers, manufacturers) often show larger gaps between current and quick ratios.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#leverage-ratios-how-is-the-company-financed",
    "href": "04_financial_statement_analysis.html#leverage-ratios-how-is-the-company-financed",
    "title": "5  Financial Statement Analysis",
    "section": "5.5 Leverage Ratios: How Is the Company Financed?",
    "text": "5.5 Leverage Ratios: How Is the Company Financed?\nLeverage ratios examine a company’s capital structure (i.e., the mix of debt and equity financing). These metrics reveal financial risk and long-term solvency, helping investors understand how much of the company’s operations are funded by borrowed money.\n\n5.5.1 Why Capital Structure Matters\nA company’s financing choice involves fundamental trade-offs:\n\nDebt offers tax advantages (interest is deductible) and doesn’t dilute ownership, but creates fixed obligations that must be met regardless of business performance\nEquity provides flexibility (no required payments) but dilutes existing shareholders and may be more expensive than debt\n\nCompanies with high leverage amplify both gains and losses. In good times, shareholders capture more upside because profits aren’t shared with additional equity holders. In bad times, fixed interest payments can push the company toward distress. This is why beta (systematic risk) tends to increase with leverage.\n\n\n5.5.2 Debt-to-Equity Ratio\nThis ratio indicates how much debt financing the company uses relative to shareholder investment:\n\\[\n\\text{Debt-to-Equity} = \\frac{\\text{Total Debt}}{\\text{Total Equity}}\n\\]\nA ratio of 1.0 means equal parts debt and equity financing. Higher ratios indicate more aggressive use of leverage, which can enhance returns in good times but increases bankruptcy risk.\n\n\n5.5.3 Debt-to-Asset Ratio\nThis ratio shows what percentage of assets are financed through debt:\n\\[\n\\text{Debt-to-Asset} = \\frac{\\text{Total Debt}}{\\text{Total Assets}}\n\\]\nA ratio of 0.5 means half the company’s assets are debt-financed. This metric is bounded between 0 and 1 (assuming positive equity), making it easier to compare across companies than the debt-to-equity ratio.\n\n\n5.5.4 Interest Coverage Ratio\nWhile the above ratios measure leverage levels, interest coverage assesses the ability to service that debt:\n\\[\n\\text{Interest Coverage} = \\frac{\\text{EBIT}}{\\text{Interest Expense}}\n\\]\nThis ratio answers: “How many times over can current operating profits cover interest obligations?” A ratio below 1.0 means operating income doesn’t cover interest payments, which is a dangerous position. Ratios above 3-5 generally indicate comfortable coverage.\n\n\n5.5.5 Calculating Leverage Ratios\n\nbalance_sheet_statements = balance_sheet_statements.assign(\n    debt_to_equity=lambda x: x[\"total_debt\"] / x[\"total_equity\"],\n    debt_to_asset=lambda x: x[\"total_debt\"] / x[\"at\"]\n)\n\nincome_statements = (comp_vn30\n    .assign(\n        year=lambda x: x[\"year\"].astype(int),\n        # Handle zero interest expense to avoid infinity\n        interest_coverage=lambda x: np.where(\n            x[\"cfo_interest_expense\"] &gt; 0,\n            x[\"is_net_business_profit\"] / x[\"cfo_interest_expense\"],\n            np.nan\n        ),\n        label=lambda x: np.where(\n            x[\"symbol\"].isin(vn30_symbols), x[\"symbol\"], np.nan\n        )\n    )\n)\n\n\n\n5.5.6 Leverage Trends Over Time\nFigure 5.2 tracks how debt-to-asset ratios have evolved over time. Time-series analysis reveals whether companies are becoming more or less leveraged.\n\ndebt_to_asset = balance_sheet_statements.query(\"symbol in @vn30_symbols\")\n\ndebt_to_asset_figure = (\n    ggplot(debt_to_asset, aes(x=\"year\", y=\"debt_to_asset\", color=\"symbol\"))\n    + geom_line(size=1)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Debt-to-Asset Ratio\", color=\"\",\n        title=\"Debt-to-Asset Ratios of VN30 Stocks Over Time\"\n    )\n)\n\ndebt_to_asset_figure.show()\n\n\n\n\n\n\n\nFigure 5.2: Debt-to-asset ratios show the proportion of assets financed by debt. Changes over time reflect evolving capital structure strategies and market conditions.\n\n\n\n\n\n\n\n5.5.7 Cross-Sectional Leverage Comparison\nFigure 5.3 provides a snapshot of leverage across all VN30 constituents for the most recent year.\n\ndebt_to_asset_comparison = balance_sheet_statements.query(\"year == 2023\")\n\ndebt_to_asset_comparison[\"symbol\"] = pd.Categorical(\n    debt_to_asset_comparison[\"symbol\"],\n    categories=debt_to_asset_comparison.sort_values(\"debt_to_asset\")[\"symbol\"],\n    ordered=True\n)\n\ndebt_to_asset_comparison_figure = (\n    ggplot(\n        debt_to_asset_comparison,\n        aes(y=\"debt_to_asset\", x=\"symbol\", fill=\"label\")\n    )\n    + geom_col()\n    + coord_flip()\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Debt-to-Asset Ratio\", fill=\"\",\n        title=\"Debt-to-Asset Ratios of VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\ndebt_to_asset_comparison_figure.show()\n\n\n\n\n\n\n\nFigure 5.3: Cross-sectional comparison of debt-to-asset ratios reveals industry patterns and company-specific financing strategies.\n\n\n\n\n\n\n\n5.5.8 The Leverage-Coverage Trade-off\nFigure 5.4 examines the relationship between leverage levels and debt-servicing ability. Companies with higher debt loads should ideally have stronger interest coverage to maintain financial stability.\n\ninterest_coverage = (income_statements\n    .query(\"year == 2023\")\n    .get([\"symbol\", \"year\", \"interest_coverage\"])\n    .merge(balance_sheet_statements, on=[\"symbol\", \"year\"], how=\"left\")\n)\n\ninterest_coverage_figure = (\n    ggplot(\n        interest_coverage,\n        aes(x=\"debt_to_asset\", y=\"interest_coverage\", color=\"label\")\n    )\n    + geom_point(size=2)\n    + geom_label(\n        aes(label=\"label\"),\n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + labs(\n        x=\"Debt-to-Asset Ratio\", y=\"Interest Coverage Ratio\",\n        title=\"Leverage versus Interest Coverage for VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\ninterest_coverage_figure.show()\n\n\n\n\n\n\n\nFigure 5.4: The relationship between leverage and interest coverage reveals whether companies can comfortably service their debt. High leverage with low coverage indicates elevated financial risk.\n\n\n\n\n\nThe scatter plot reveals important patterns. Companies in the upper-left quadrant (low leverage, high coverage) have conservative financing with ample debt capacity. Those in the lower-right (high leverage, low coverage) face elevated financial risk.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#efficiency-ratios-how-well-are-assets-managed",
    "href": "04_financial_statement_analysis.html#efficiency-ratios-how-well-are-assets-managed",
    "title": "5  Financial Statement Analysis",
    "section": "5.6 Efficiency Ratios: How Well Are Assets Managed?",
    "text": "5.6 Efficiency Ratios: How Well Are Assets Managed?\nEfficiency ratios measure how effectively a company utilizes its assets and manages operations. These metrics help identify whether management is extracting maximum value from the company’s resource base.\n\n5.6.1 Asset Turnover\nThis ratio measures how efficiently a company uses total assets to generate revenue:\n\\[\n\\text{Asset Turnover} = \\frac{\\text{Revenue}}{\\text{Total Assets}}\n\\]\nA higher ratio indicates more efficient asset utilization: the company generates more sales per dollar of assets. However, optimal levels vary dramatically across industries. Retailers with minimal fixed assets might achieve turnovers above 2.0, while capital-intensive manufacturers might operate below 0.5.\n\n\n5.6.2 Inventory Turnover\nFor companies carrying inventory, this ratio reveals how quickly stock moves through the business:\n\\[\n\\text{Inventory Turnover} = \\frac{\\text{Cost of Goods Sold}}{\\text{Inventory}}\n\\]\nHigher turnover suggests efficient inventory management (i.e., goods don’t sit on shelves collecting dust). However, extremely high turnover might indicate stockout risks, while very low turnover could signal obsolete inventory or overinvestment in working capital.\nWe use COGS rather than revenue in the numerator because inventory is recorded at cost, not selling price. Using revenue would overstate turnover for high-margin businesses.\n\n\n5.6.3 Receivables Turnover\nThis ratio measures how effectively a company collects payments from customers:\n\\[\n\\text{Receivables Turnover} = \\frac{\\text{Revenue}}{\\text{Accounts Receivable}}\n\\]\nHigher turnover indicates faster collection (i.e., customers pay promptly). Converting this to “days sales outstanding” (365 / turnover) gives the average collection period in days. Companies must balance collection efficiency against the sales impact of restrictive credit policies.\n\n\n5.6.4 Calculating Efficiency Ratios\n\ncombined_statements = (balance_sheet_statements\n    .get([\n        \"symbol\", \"year\", \"label\", \"current_ratio\", \"quick_ratio\",\n        \"cash_ratio\", \"debt_to_equity\", \"debt_to_asset\", \"total_asset\",\n        \"total_equity\"\n    ])\n    .merge(\n        (income_statements\n            .get([\n                \"symbol\", \"year\", \"interest_coverage\", \"is_revenue\",\n                \"is_cogs\", \"selling_general_and_administrative_expenses\",\n                \"is_interest_expense\", \"is_gross_profit\", \"is_eat\"\n            ])\n        ),\n        on=[\"symbol\", \"year\"],\n        how=\"left\"\n    )\n    .merge(\n        (comp_vn30\n            .assign(year=lambda x: x[\"year\"].astype(int))\n            .get([\"symbol\", \"year\", \"ca_total_inventory\", \"ca_acc_receiv\"])\n        ),\n        on=[\"symbol\", \"year\"],\n        how=\"left\"\n    )\n)\n\ncombined_statements = combined_statements.assign(\n    asset_turnover=lambda x: x[\"is_revenue\"] / x[\"total_asset\"],\n    inventory_turnover=lambda x: x[\"is_cogs\"] / x[\"ca_total_inventory\"],\n    receivables_turnover=lambda x: x[\"is_revenue\"] / x[\"ca_acc_receiv\"]\n)\n\nEfficiency ratios vary dramatically across industries, making peer comparison essential. A grocery store and a shipbuilder will have fundamentally different asset and inventory dynamics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#profitability-ratios-is-the-company-making-money",
    "href": "04_financial_statement_analysis.html#profitability-ratios-is-the-company-making-money",
    "title": "5  Financial Statement Analysis",
    "section": "5.7 Profitability Ratios: Is the Company Making Money?",
    "text": "5.7 Profitability Ratios: Is the Company Making Money?\nProfitability ratios evaluate how effectively a company converts activity into earnings. These metrics directly measure financial success and are among the most closely watched indicators by investors.\n\n5.7.1 Gross Margin\nThe gross margin reveals what percentage of revenue remains after direct production costs:\n\\[\n\\text{Gross Margin} = \\frac{\\text{Gross Profit}}{\\text{Revenue}} = \\frac{\\text{Revenue} - \\text{COGS}}{\\text{Revenue}}\n\\]\nHigher gross margins indicate stronger pricing power, more efficient production, or a favorable product mix. This metric is particularly useful for comparing companies within an industry, as it reveals relative efficiency in core operations before overhead costs.\n\n\n5.7.2 Profit Margin\nThe profit margin shows what percentage of revenue ultimately becomes net income:\n\\[\n\\text{Profit Margin} = \\frac{\\text{Net Income}}{\\text{Revenue}}\n\\]\nThis comprehensive measure accounts for all costs (e.g., production, operations, interest, and taxes). Higher profit margins suggest effective overall cost management. However, optimal margins vary by industry: software companies routinely achieve 20%+ margins, while grocery stores operate on razor-thin 2-3% margins.\n\n\n5.7.3 Return on Equity (ROE)\nROE measures how efficiently a company uses shareholders’ investment to generate profits:\n\\[\n\\text{Return on Equity} = \\frac{\\text{Net Income}}{\\text{Total Equity}}\n\\]\nThis metric directly addresses what shareholders care about: returns on their invested capital. Higher ROE indicates more effective use of equity, though interpretation requires caution. High leverage can artificially inflate ROE by reducing the equity base (e.g., a company financed 90% by debt will show spectacular ROE on modest profits).\n\n\n5.7.4 The DuPont Decomposition\nThe DuPont framework decomposes ROE into three components that reveal different aspects of performance:\n\\[\n\\text{ROE} = \\underbrace{\\frac{\\text{Net Income}}{\\text{Revenue}}}_{\\text{Profit Margin}} \\times \\underbrace{\\frac{\\text{Revenue}}{\\text{Assets}}}_{\\text{Asset Turnover}} \\times \\underbrace{\\frac{\\text{Assets}}{\\text{Equity}}}_{\\text{Leverage}}\n\\]\nThis decomposition shows that high ROE can come from different sources: strong profit margins (pricing power, cost control), efficient asset use (high turnover), or aggressive leverage. Understanding which driver dominates helps assess sustainability. ROE driven by margins is generally more sustainable than ROE driven by leverage.\n\n\n5.7.5 Calculating Profitability Ratios\n\ncombined_statements = combined_statements.assign(\n    gross_margin=lambda x: x[\"is_gross_profit\"] / x[\"is_revenue\"],\n    profit_margin=lambda x: x[\"is_eat\"] / x[\"is_revenue\"],\n    after_tax_roe=lambda x: x[\"is_eat\"] / x[\"total_equity\"]\n)\n\n\n\n5.7.6 Gross Margin Trends\nFigure 5.5 tracks gross margin evolution over time, revealing whether companies are maintaining pricing power and production efficiency.\n\ngross_margins = combined_statements.query(\"symbol in @vn30_symbols\")\n\ngross_margins_figure = (\n    ggplot(gross_margins, aes(x=\"year\", y=\"gross_margin\", color=\"symbol\"))\n    + geom_line()\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Gross Margin\", color=\"\",\n        title=\"Gross Margins for VN30 Stocks (2019-2023)\"\n    )\n)\n\ngross_margins_figure.show()\n\n\n\n\n\n\n\nFigure 5.5: Gross margin trends reveal changes in pricing power and production efficiency. Declining margins may signal increased competition or rising input costs.\n\n\n\n\n\n\n\n5.7.7 From Gross to Net: Where Do Profits Go?\nFigure 5.6 examines the relationship between gross and profit margins. The gap between them reveals the impact of operating expenses, interest, and taxes.\n\nprofit_margins = combined_statements.query(\"year == 2023\")\n\nprofit_margins_figure = (\n    ggplot(\n        profit_margins,\n        aes(x=\"gross_margin\", y=\"profit_margin\", color=\"label\")\n    )\n    + geom_point(size=2)\n    + geom_label(\n        aes(label=\"label\"),\n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Gross Margin\", y=\"Profit Margin\",\n        title=\"Gross versus Profit Margins for VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\nprofit_margins_figure.show()\n\n\n\n\n\n\n\nFigure 5.6: Comparing gross and profit margins reveals how much of gross profit survives operating expenses, interest, and taxes. Companies far below the diagonal have high overhead relative to gross profit.\n\n\n\n\n\nCompanies along the diagonal convert gross profit to net income efficiently. Those well below the diagonal face high operating costs, interest burdens, or tax rates that erode profitability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#combining-financial-ratios-a-holistic-view",
    "href": "04_financial_statement_analysis.html#combining-financial-ratios-a-holistic-view",
    "title": "5  Financial Statement Analysis",
    "section": "5.8 Combining Financial Ratios: A Holistic View",
    "text": "5.8 Combining Financial Ratios: A Holistic View\nIndividual ratios provide specific insights, but combining them offers a more complete picture. A company might excel in profitability while struggling with liquidity, or maintain conservative leverage while underperforming on efficiency.\n\n5.8.1 Ranking Companies Across Categories\nFigure 5.7 compares company rankings across four ratio categories. Rankings closer to 1 indicate better performance within each category, enabling quick identification of relative strengths and weaknesses.\n\nfinancial_ratios = (combined_statements\n    .query(\"year == 2023\")\n    .filter(\n        items=[\"symbol\"] + [\n            col for col in combined_statements.columns\n            if any(x in col for x in [\n                \"ratio\", \"margin\", \"roe\", \"_to_\", \"turnover\", \"interest_coverage\"\n            ])\n        ]\n    )\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        type=lambda x: np.select(\n            [\n                x[\"name\"].isin([\"current_ratio\", \"quick_ratio\", \"cash_ratio\"]),\n                x[\"name\"].isin([\"debt_to_equity\", \"debt_to_asset\", \"interest_coverage\"]),\n                x[\"name\"].isin([\"asset_turnover\", \"inventory_turnover\", \"receivables_turnover\"]),\n                x[\"name\"].isin([\"gross_margin\", \"profit_margin\", \"after_tax_roe\"]),\n            ],\n            [\n                \"Liquidity Ratios\",\n                \"Leverage Ratios\",\n                \"Efficiency Ratios\",\n                \"Profitability Ratios\"\n            ],\n            default=\"Other\"\n        )\n    )\n)\n\nfinancial_ratios[\"rank\"] = (financial_ratios\n    .sort_values([\"type\", \"name\", \"value\"], ascending=[True, True, False])\n    .groupby([\"type\", \"name\"])\n    .cumcount() + 1\n)\n\nfinal_ranks = (financial_ratios\n    .groupby([\"symbol\", \"type\"], as_index=False)\n    .agg(rank=(\"rank\", \"mean\"))\n    .query(\"symbol in @vn30_symbols\")\n)\n\nfinal_ranks_figure = (\n    ggplot(final_ranks, aes(x=\"rank\", y=\"type\", color=\"symbol\"))\n    + geom_point(shape=\"^\", size=4)\n    + labs(\n        x=\"Average Rank (Lower is Better)\", y=\"\", color=\"\",\n        title=\"Average Rank Across Financial Ratio Categories\"\n    )\n    + coord_cartesian(xlim=[1, 30])\n)\n\nfinal_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 5.7: Ranking companies across multiple ratio categories reveals overall financial profiles. Companies with consistently low ranks across categories demonstrate broad-based financial strength.\n\n\n\n\n\nThe combined view reveals how different business strategies manifest in financial profiles. A company might deliberately accept lower profitability rankings in exchange for stronger liquidity, or use aggressive leverage to boost returns at the cost of financial flexibility.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#financial-ratios-in-asset-pricing",
    "href": "04_financial_statement_analysis.html#financial-ratios-in-asset-pricing",
    "title": "5  Financial Statement Analysis",
    "section": "5.9 Financial Ratios in Asset Pricing",
    "text": "5.9 Financial Ratios in Asset Pricing\nBeyond evaluating individual companies, financial ratios serve as crucial inputs for asset pricing models. The Fama-French five-factor model, which we explore in detail in Fama-French Factors, uses several accounting-based measures to explain cross-sectional variation in stock returns.\n\n5.9.1 The Fama-French Factors\nThe model incorporates four company characteristics derived from financial statements:\nSize is measured as the logarithm of market capitalization: \\[\n\\text{Size} = \\ln(\\text{Market Cap})\n\\]\nThis captures the empirical finding that smaller firms tend to outperform larger firms on a risk-adjusted basis (i.e., the “size premium”).\nBook-to-Market relates accounting value to market value: \\[\n\\text{Book-to-Market} = \\frac{\\text{Book Equity}}{\\text{Market Cap}}\n\\]\nHigh book-to-market stocks (“value” stocks) have historically outperformed low book-to-market stocks (“growth” stocks) (i.e., the “value premium”).\nOperating Profitability measures profit generation relative to equity: \\[\n\\text{Profitability} = \\frac{\\text{Revenue} - \\text{COGS} - \\text{SG\\&A} - \\text{Interest}}{\\text{Book Equity}}\n\\]\nMore profitable firms tend to earn higher returns (i.e., the “profitability premium”).\nInvestment captures asset growth: \\[\n\\text{Investment} = \\frac{\\text{Total Assets}_t}{\\text{Total Assets}_{t-1}} - 1\n\\] Firms investing aggressively tend to underperform conservative investors (i.e., the “investment premium”).\n\n\n5.9.2 Calculating Fama-French Variables\n\nprices_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM prices_monthly\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\n# Use December prices for annual calculations\nprices_december = (prices_monthly\n    .assign(date=lambda x: pd.to_datetime(x[\"date\"]))\n    .query(\"date.dt.month == 12\")\n)\n\n\ncombined_statements_ff = (combined_statements\n    .query(\"year == 2023\")\n    .merge(prices_december, on=[\"symbol\", \"year\"], how=\"left\")\n    .merge(\n        (balance_sheet_statements\n            .query(\"year == 2022\")\n            .get([\"symbol\", \"total_asset\"])\n            .rename(columns={\"total_asset\": \"total_assets_lag\"})\n        ),\n        on=\"symbol\",\n        how=\"left\"\n    )\n    .assign(\n        size=lambda x: np.log(x[\"mktcap\"]),\n        book_to_market=lambda x: x[\"total_equity\"] / x[\"mktcap\"],\n        operating_profitability=lambda x: (\n            (x[\"is_revenue\"] - x[\"is_cogs\"] -\n             x[\"selling_general_and_administrative_expenses\"] -\n             x[\"is_interest_expense\"]) / x[\"total_equity\"]\n        ),\n        investment=lambda x: x[\"total_asset\"] / x[\"total_assets_lag\"] - 1\n    )\n)\n\ncombined_statements_ff.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\nlabel\ncurrent_ratio\nquick_ratio\ncash_ratio\ndebt_to_equity\ndebt_to_asset\ntotal_asset\ntotal_equity\n...\nshrout\nmktcap\nmktcap_lag\nrisk_free\nret_excess\ntotal_assets_lag\nsize\nbook_to_market\noperating_profitability\ninvestment\n\n\n\n\n0\nPOW\n2023\nPOW\n1.084255\n1.084255\n0.315089\n0.0\n0.0\n7.036209e+13\n3.411943e+13\n...\n2.341872e+09\n26346.055367\n26346.055367\n0.003333\n-0.003333\n5.684324e+13\n10.179074\n1.295049e+09\n0.025539\n0.237827\n\n\n1\nHPG\n2023\nHPG\n1.156655\n1.156655\n0.171324\n0.0\n0.0\n1.877826e+14\n1.028364e+14\n...\n5.814786e+09\n162523.260217\n154382.560242\n0.003333\n-0.003333\n1.703355e+14\n11.998576\n6.327489e+08\n0.072798\n0.102428\n\n\n2\nMWG\n2023\nMWG\n1.688604\n1.688604\n0.174408\n0.0\n0.0\n6.011124e+13\n2.335956e+13\n...\n1.462941e+09\n62613.896064\n56323.247628\n0.003333\n-0.009141\n5.583410e+13\n11.044743\n3.730731e+08\n-0.002443\n0.076604\n\n\n\n\n3 rows × 65 columns\n\n\n\n\n\n5.9.3 Fama-French Factor Rankings\nFigure 5.8 shows how VN30 companies rank on each Fama-French variable, connecting fundamental analysis to asset pricing.\n\nfactors_ranks = (combined_statements_ff\n    .get([\"symbol\", \"size\", \"book_to_market\", \"operating_profitability\", \"investment\"])\n    .rename(columns={\n        \"size\": \"Size\",\n        \"book_to_market\": \"Book-to-Market\",\n        \"operating_profitability\": \"Profitability\",\n        \"investment\": \"Investment\"\n    })\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        rank=lambda x: (\n            x.sort_values([\"name\", \"value\"], ascending=[True, False])\n            .groupby(\"name\")\n            .cumcount() + 1\n        )\n    )\n    .query(\"symbol in @vn30_symbols\")\n)\n\nfactors_ranks_figure = (\n    ggplot(factors_ranks, aes(x=\"rank\", y=\"name\", color=\"symbol\"))\n    + geom_point(shape=\"^\", size=4)\n    + labs(\n        x=\"Rank\", y=\"\", color=\"\",\n        title=\"Rank in Fama-French Variables for VN30 Stocks\"\n    )\n    + coord_cartesian(xlim=[1, 30])\n)\n\nfactors_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 5.8: Rankings on Fama-French variables connect financial statement analysis to asset pricing. According to factor models, smaller, higher book-to-market, more profitable, and lower-investment firms should earn higher expected returns.\n\n\n\n\n\nThese rankings have implications for expected returns according to factor models. A small, high book-to-market, highly profitable company with conservative investment should, in theory, earn higher risk-adjusted returns than its opposite.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#limitations-and-practical-considerations",
    "href": "04_financial_statement_analysis.html#limitations-and-practical-considerations",
    "title": "5  Financial Statement Analysis",
    "section": "5.10 Limitations and Practical Considerations",
    "text": "5.10 Limitations and Practical Considerations\nWhile financial ratios provide powerful analytical tools, several limitations deserve attention:\n\n5.10.1 Accounting Discretion\nCompanies have significant discretion in how they apply accounting standards. Revenue recognition timing, depreciation methods, inventory valuation (FIFO vs. LIFO), and capitalization versus expensing decisions all affect reported numbers. Sophisticated analysis requires understanding these choices and their impact.\n\n\n5.10.2 Industry Comparability\nRatios vary dramatically across industries. Comparing a bank’s leverage to a retailer’s is meaningless (e.g., banks naturally operate with much higher leverage due to their business model). Always benchmark against industry peers rather than absolute standards.\n\n\n5.10.3 Point-in-Time Limitations\nBalance sheet ratios capture a single moment, which may not represent typical conditions. Companies often “window dress” by temporarily improving metrics at reporting dates. Trend analysis and quarter-over-quarter comparisons can reveal such practices.\n\n\n5.10.4 Backward-Looking Nature\nFinancial statements report historical results. Past profitability doesn’t guarantee future performance, especially for companies in rapidly changing industries or facing disruption.\n\n\n5.10.5 Quality of Earnings\nNot all profits are created equal. Earnings driven by one-time gains, accounting adjustments, or aggressive revenue recognition may not recur. Cash flow analysis helps assess earnings quality. Profits that don’t convert to cash warrant skepticism.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "04_financial_statement_analysis.html#key-takeaways",
    "href": "04_financial_statement_analysis.html#key-takeaways",
    "title": "5  Financial Statement Analysis",
    "section": "5.11 Key Takeaways",
    "text": "5.11 Key Takeaways\nThis chapter introduced financial statement analysis as a tool for understanding company fundamentals. The main insights are:\n\nThree statements, three perspectives: The balance sheet shows financial position at a point in time, the income statement measures performance over a period, and the cash flow statement tracks actual cash movements. Together, they provide a complete picture of financial health.\nLiquidity ratios assess short-term survival: Current, quick, and cash ratios measure the ability to meet near-term obligations. Higher ratios indicate greater liquidity but may suggest inefficient asset use.\nLeverage ratios reveal capital structure risk: Debt-to-equity, debt-to-asset, and interest coverage ratios show how the company finances operations and whether it can service its debt. Higher leverage amplifies both returns and risk.\nEfficiency ratios measure management effectiveness: Asset turnover, inventory turnover, and receivables turnover reveal how well the company converts resources into revenue. Industry context is essential for interpretation.\nProfitability ratios quantify financial success: Gross margin, profit margin, and ROE measure the ability to generate earnings. The DuPont decomposition reveals whether ROE comes from margins, turnover, or leverage.\nRatios connect to asset pricing: Financial statement variables like book-to-market, profitability, and investment form the basis of factor models that explain cross-sectional return differences.\nContext matters for interpretation: Ratios must be compared against industry peers, tracked over time, and considered alongside qualitative factors. No single ratio tells the complete story.\n\nLooking ahead, subsequent chapters will explore how these fundamental variables interact with market prices in asset pricing models, and how to construct factor portfolios based on financial statement characteristics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html",
    "href": "05_discounted_cash_flow.html",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "",
    "text": "6.1 What Is a Company Worth?\nThe previous chapters examined how markets price securities in equilibrium and how financial statements reveal company fundamentals. But these approaches leave a central question unanswered: What is the intrinsic value of a business, independent of its current market price?\nDiscounted Cash Flow (DCF) analysis answers this question by valuing a company based on its ability to generate cash for investors. The core insight is simple: a business is worth the present value of all future cash it will produce. This principle that value equals discounted future cash flows underlies virtually all of finance, from bond pricing to real estate valuation.\nDCF analysis stands apart from other valuation approaches in three important ways. First, it explicitly accounts for the time value of money (i.e., the principle that a dollar today is worth more than a dollar tomorrow). By discounting future cash flows at an appropriate rate, we incorporate both time preferences and risk. Second, DCF is forward-looking, making it particularly suitable for companies where historical performance may not reflect future potential. Third, DCF is flexible enough to accommodate various business models and capital structures, making it applicable across industries and company sizes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#what-is-a-company-worth",
    "href": "05_discounted_cash_flow.html#what-is-a-company-worth",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "",
    "text": "6.1.1 Valuation Methods Overview\nCompany valuation methods broadly fall into three categories:\n\nMarket-based approaches compare companies using relative metrics like Price-to-Earnings or EV/EBITDA ratios. These are quick but assume comparable companies are fairly valued.\nAsset-based methods focus on the net value of tangible and intangible assets. These work well for liquidation scenarios but miss going-concern value.\nIncome-based techniques value companies based on their ability to generate future cash flows. DCF is the most rigorous income-based method.\n\nWe focus on DCF because it forces analysts to make explicit assumptions about growth, profitability, and risk. These assumptions are often hidden in other methods. Even when DCF isn’t the final word on valuation, the discipline of building a DCF model deepens understanding of what drives value.\n\n\n6.1.2 The Three Pillars of DCF\nEvery DCF analysis rests on three components:\n\nFree Cash Flow (FCF) forecasts: The expected future cash available for distribution to investors after operating expenses, taxes, and investments\nTerminal value: The company’s value beyond the explicit forecast period, often representing a majority of total valuation\nDiscount rate: Typically the Weighted Average Cost of Capital (WACC), which adjusts future cash flows to present value by incorporating risk and capital structure\n\nWe make simplifying assumptions throughout this chapter. In particular, we assume firms conduct only operating activities (i.e., financial statements do not include non-operating items like excess cash or investment securities). Real-world valuations require valuing these separately. Entire textbooks are devoted to valuation nuances; our goal is to establish the conceptual framework and practical implementation.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\nfrom itertools import product",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#understanding-free-cash-flow",
    "href": "05_discounted_cash_flow.html#understanding-free-cash-flow",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.2 Understanding Free Cash Flow",
    "text": "6.2 Understanding Free Cash Flow\nBefore diving into calculations, we need to understand what Free Cash Flow represents and why it matters for valuation.\n\n6.2.1 Why Free Cash Flow, Not Net Income?\nAccountants report net income, but DCF uses free cash flow. Why the difference?\nNet income includes non-cash items (like depreciation) and ignores cash needs (like capital expenditures and working capital investments). A company can report strong profits while burning cash, or generate substantial cash while reporting losses. Free cash flow captures what actually matters for valuation: the cash available to distribute to all capital providers (both debt holders and equity holders) after funding operations and investments.\n\n\n6.2.2 The Free Cash Flow Formula\nWe calculate FCF using the following formula:\n\\[\n\\text{FCF} = \\text{EBIT} \\times (1 - \\tau) + \\text{D\\&A} - \\Delta\\text{WC} - \\text{CAPEX}\n\\]\nwhere:\n\nEBIT (Earnings Before Interest and Taxes): Core operating profit before financing costs and taxes\n\\(\\tau\\): Corporate tax rate applied to operating profits\nD&A (Depreciation & Amortization): Non-cash charges that reduce reported earnings but don’t consume cash\n\\(\\Delta\\)WC (Change in Working Capital): Cash tied up in (or released from) operations (increases in receivables and inventory consume cash, while increases in payables provide cash)\nCAPEX (Capital Expenditures): Investments in long-term assets required to maintain and grow operations\n\nAn alternative formulation starts from EBIT directly:\n\\[\n\\text{FCF} = \\text{EBIT} + \\text{D\\&A} - \\text{Taxes} - \\Delta\\text{WC} - \\text{CAPEX}\n\\]\nBoth formulations yield the same result when taxes are calculated consistently. The key insight is that FCF represents cash generated from operations after all reinvestment needs (i.e., cash that could theoretically be distributed to investors without impairing the business).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#loading-historical-financial-data",
    "href": "05_discounted_cash_flow.html#loading-historical-financial-data",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.3 Loading Historical Financial Data",
    "text": "6.3 Loading Historical Financial Data\nWe use FPT Corporation, one of Vietnam’s largest technology companies, as our case study. FPT provides IT services, telecommunications, and education. It’s a diversified business with meaningful capital requirements and growth potential.\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncomp_vn = pd.read_sql_query(\n    sql=\"SELECT * FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Filter to FPT and examine the data structure\nfpt_data = comp_vn[comp_vn[\"symbol\"] == \"FPT\"].copy()\nfpt_data[\"year\"] = fpt_data[\"year\"].astype(int)\nfpt_data = fpt_data.sort_values(\"year\").reset_index(drop=True)\n\nprint(f\"Available years: {fpt_data['year'].min()} to {fpt_data['year'].max()}\")\nprint(f\"Number of observations: {len(fpt_data)}\")\n\nAvailable years: 2002 to 2023\nNumber of observations: 22\n\n\n\n6.3.1 Computing Historical Free Cash Flow\nLet’s calculate the components needed for FCF from the financial statement data:\n\n# Extract and compute FCF components\nhistorical_data = (fpt_data\n    .assign(\n        # Revenue for ratio calculations\n        revenue=lambda x: x[\"is_net_revenue\"],\n        \n        # EBIT = Earnings before interest and taxes\n        # Approximate as EBT + Interest Expense\n        ebit=lambda x: x[\"is_ebt\"] + x[\"is_interest_expense\"],\n        \n        # Tax payments (use actual tax expense)\n        taxes=lambda x: x[\"is_cit_expense\"],\n        \n        # Depreciation and amortization (non-cash add-back)\n        depreciation=lambda x: x[\"cfo_depreciation\"],\n        \n        # Change in working capital components\n        # Positive delta_wc means cash is consumed (tied up in working capital)\n        delta_working_capital=lambda x: (\n            x[\"cfo_receive\"] +      # Change in receivables\n            x[\"cfo_inventory\"] -    # Change in inventory  \n            x[\"cfo_payale\"]         # Change in payables (negative = cash source)\n        ),\n        \n        # Capital expenditures\n        capex=lambda x: x[\"capex\"]\n    )\n    .loc[:, [\n        \"year\", \"revenue\", \"ebit\", \"taxes\", \"depreciation\",\n        \"delta_working_capital\", \"capex\"\n    ]]\n)\n\n# Calculate Free Cash Flow\nhistorical_data[\"fcf\"] = (\n    historical_data[\"ebit\"] \n    - historical_data[\"taxes\"]\n    + historical_data[\"depreciation\"]\n    - historical_data[\"delta_working_capital\"]\n    - historical_data[\"capex\"]\n)\n\nhistorical_data\n\n\n\n\n\n\n\n\nyear\nrevenue\nebit\ntaxes\ndepreciation\ndelta_working_capital\ncapex\nfcf\n\n\n\n\n0\n2002\n1.514961e+12\n2.698700e+10\n0.000000e+00\n1.261500e+10\n-2.561760e+11\n2.202800e+10\n2.737500e+11\n\n\n1\n2003\n4.148298e+12\n5.676100e+10\n0.000000e+00\n1.837700e+10\n-5.078740e+11\n3.753300e+10\n5.454790e+11\n\n\n2\n2004\n8.734781e+12\n2.145902e+11\n1.795700e+10\n2.947900e+10\n-4.280270e+11\n5.252100e+10\n6.016182e+11\n\n\n3\n2005\n1.410079e+13\n3.753490e+11\n4.251500e+10\n5.381700e+10\n-4.471110e+11\n1.428320e+11\n6.909300e+11\n\n\n4\n2006\n2.139975e+13\n6.672593e+11\n7.368682e+10\n1.068192e+11\n-1.173099e+12\n2.459780e+11\n1.627513e+12\n\n\n5\n2007\n1.349889e+13\n1.071941e+12\n1.487146e+11\n1.709335e+11\n-1.873794e+12\n4.802762e+11\n2.487677e+12\n\n\n6\n2008\n1.638184e+13\n1.320573e+12\n1.890384e+11\n2.395799e+11\n-1.419506e+11\n6.690461e+11\n8.440192e+11\n\n\n7\n2009\n1.840403e+13\n1.807221e+12\n2.916482e+11\n3.041813e+11\n-8.065011e+11\n7.632280e+11\n1.863027e+12\n\n\n8\n2010\n2.001730e+13\n2.261341e+12\n3.314359e+11\n3.294060e+11\n-2.360993e+12\n8.672138e+11\n3.753090e+12\n\n\n9\n2011\n2.537025e+13\n2.751044e+12\n4.223952e+11\n3.759567e+11\n-2.099380e+12\n4.524081e+11\n4.351578e+12\n\n\n10\n2012\n2.459430e+13\n2.635219e+12\n4.210738e+11\n3.995598e+11\n8.043763e+11\n7.083318e+11\n1.100997e+12\n\n\n11\n2013\n2.702789e+13\n2.690568e+12\n4.503170e+11\n4.429860e+11\n-1.947751e+12\n9.110216e+11\n3.719967e+12\n\n\n12\n2014\n3.264466e+13\n2.625389e+12\n3.800994e+11\n5.472736e+11\n-3.078130e+12\n1.417399e+12\n4.453295e+12\n\n\n13\n2015\n3.795970e+13\n3.113651e+12\n4.130641e+11\n7.328801e+11\n-1.951778e+12\n1.974295e+12\n3.410951e+12\n\n\n14\n2016\n3.953147e+13\n3.388085e+12\n4.382078e+11\n9.334397e+11\n-9.242713e+11\n1.428472e+12\n3.379116e+12\n\n\n15\n2017\n4.265861e+13\n4.623663e+12\n7.270039e+11\n1.039417e+12\n-4.638788e+12\n1.100498e+12\n8.474367e+12\n\n\n16\n2018\n2.321354e+13\n4.095947e+12\n6.236054e+11\n1.164692e+12\n-1.033438e+12\n2.452902e+12\n3.217569e+12\n\n\n17\n2019\n2.771696e+13\n5.023518e+12\n7.528183e+11\n1.354613e+12\n-5.308818e+11\n3.230818e+12\n2.925377e+12\n\n\n18\n2020\n2.983040e+13\n5.648794e+12\n8.397114e+11\n1.490607e+12\n-8.040730e+11\n3.014322e+12\n4.089441e+12\n\n\n19\n2021\n3.565726e+13\n6.821202e+12\n9.879053e+11\n1.643916e+12\n-2.821825e+12\n2.908134e+12\n7.390903e+12\n\n\n20\n2022\n4.400953e+13\n8.308009e+12\n1.170940e+12\n1.833064e+12\n-3.746661e+12\n3.209581e+12\n9.507213e+12\n\n\n21\n2023\n5.261790e+13\n1.003565e+13\n1.414956e+12\n2.286514e+12\n-2.147304e+12\n3.948982e+12\n9.105534e+12\n\n\n\n\n\n\n\n\n\n6.3.2 Understanding the Historical Pattern\nBefore forecasting, we should understand the historical trends in FCF and its components:\n\n# Calculate key ratios relative to revenue\nhistorical_ratios = (historical_data\n    .assign(\n        # Revenue growth (year-over-year)\n        revenue_growth=lambda x: x[\"revenue\"].pct_change(),\n        \n        # Operating margin: EBIT as % of revenue\n        operating_margin=lambda x: x[\"ebit\"] / x[\"revenue\"],\n        \n        # Depreciation as % of revenue\n        depreciation_margin=lambda x: x[\"depreciation\"] / x[\"revenue\"],\n        \n        # Tax rate (taxes as % of revenue, for simplicity)\n        tax_margin=lambda x: x[\"taxes\"] / x[\"revenue\"],\n        \n        # Working capital intensity\n        working_capital_margin=lambda x: x[\"delta_working_capital\"] / x[\"revenue\"],\n        \n        # Capital intensity\n        capex_margin=lambda x: x[\"capex\"] / x[\"revenue\"],\n        \n        # FCF margin\n        fcf_margin=lambda x: x[\"fcf\"] / x[\"revenue\"]\n    )\n)\n\n# Display key metrics\ndisplay_cols = [\n    \"year\", \"revenue_growth\", \"operating_margin\", \"depreciation_margin\",\n    \"tax_margin\", \"working_capital_margin\", \"capex_margin\", \"fcf_margin\"\n]\n\nhistorical_ratios[display_cols].round(3)\n\n\n\n\n\n\n\n\nyear\nrevenue_growth\noperating_margin\ndepreciation_margin\ntax_margin\nworking_capital_margin\ncapex_margin\nfcf_margin\n\n\n\n\n0\n2002\nNaN\n0.018\n0.008\n0.000\n-0.169\n0.015\n0.181\n\n\n1\n2003\n1.738\n0.014\n0.004\n0.000\n-0.122\n0.009\n0.131\n\n\n2\n2004\n1.106\n0.025\n0.003\n0.002\n-0.049\n0.006\n0.069\n\n\n3\n2005\n0.614\n0.027\n0.004\n0.003\n-0.032\n0.010\n0.049\n\n\n4\n2006\n0.518\n0.031\n0.005\n0.003\n-0.055\n0.011\n0.076\n\n\n5\n2007\n-0.369\n0.079\n0.013\n0.011\n-0.139\n0.036\n0.184\n\n\n6\n2008\n0.214\n0.081\n0.015\n0.012\n-0.009\n0.041\n0.052\n\n\n7\n2009\n0.123\n0.098\n0.017\n0.016\n-0.044\n0.041\n0.101\n\n\n8\n2010\n0.088\n0.113\n0.016\n0.017\n-0.118\n0.043\n0.187\n\n\n9\n2011\n0.267\n0.108\n0.015\n0.017\n-0.083\n0.018\n0.172\n\n\n10\n2012\n-0.031\n0.107\n0.016\n0.017\n0.033\n0.029\n0.045\n\n\n11\n2013\n0.099\n0.100\n0.016\n0.017\n-0.072\n0.034\n0.138\n\n\n12\n2014\n0.208\n0.080\n0.017\n0.012\n-0.094\n0.043\n0.136\n\n\n13\n2015\n0.163\n0.082\n0.019\n0.011\n-0.051\n0.052\n0.090\n\n\n14\n2016\n0.041\n0.086\n0.024\n0.011\n-0.023\n0.036\n0.085\n\n\n15\n2017\n0.079\n0.108\n0.024\n0.017\n-0.109\n0.026\n0.199\n\n\n16\n2018\n-0.456\n0.176\n0.050\n0.027\n-0.045\n0.106\n0.139\n\n\n17\n2019\n0.194\n0.181\n0.049\n0.027\n-0.019\n0.117\n0.106\n\n\n18\n2020\n0.076\n0.189\n0.050\n0.028\n-0.027\n0.101\n0.137\n\n\n19\n2021\n0.195\n0.191\n0.046\n0.028\n-0.079\n0.082\n0.207\n\n\n20\n2022\n0.234\n0.189\n0.042\n0.027\n-0.085\n0.073\n0.216\n\n\n21\n2023\n0.196\n0.191\n0.043\n0.027\n-0.041\n0.075\n0.173",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#visualizing-historical-ratios",
    "href": "05_discounted_cash_flow.html#visualizing-historical-ratios",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.4 Visualizing Historical Ratios",
    "text": "6.4 Visualizing Historical Ratios\nFigure 6.1 shows the historical evolution of key financial ratios that drive FCF. Understanding these patterns helps inform our forecasts.\n\n# Prepare data for plotting\nratio_columns = [\n    \"operating_margin\", \"depreciation_margin\", \"tax_margin\",\n    \"working_capital_margin\", \"capex_margin\"\n]\n\nratios_long = (historical_ratios\n    .melt(\n        id_vars=[\"year\"],\n        value_vars=ratio_columns,\n        var_name=\"ratio\",\n        value_name=\"value\"\n    )\n    .assign(\n        ratio=lambda x: x[\"ratio\"].str.replace(\"_\", \" \").str.title()\n    )\n)\n\nratios_figure = (\n    ggplot(ratios_long, aes(x=\"year\", y=\"value\", color=\"ratio\"))\n    + geom_line(size=1)\n    + geom_point(size=2)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Ratio (% of Revenue)\", color=\"\",\n        title=\"Key Financial Ratios of FPT Over Time\"\n    )\n    + theme(legend_position=\"right\")\n)\n\nratios_figure.show()\n\n\n\n\n\n\n\nFigure 6.1: Historical financial ratios reveal the operating characteristics of FPT. These patterns inform our forecast assumptions.\n\n\n\n\n\nSeveral patterns emerge from the historical data. Operating margins show the profitability of core operations. Depreciation margins indicate asset intensity. CAPEX margins reveal investment requirements. Working capital margins can be volatile, reflecting changes in credit terms and inventory management.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#forecasting-free-cash-flow",
    "href": "05_discounted_cash_flow.html#forecasting-free-cash-flow",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.5 Forecasting Free Cash Flow",
    "text": "6.5 Forecasting Free Cash Flow\nWith historical patterns established, we now project FCF into the future. This requires forecasting both revenue growth and the ratios that convert revenue into cash flow.\n\n6.5.1 The Ratio-Based Forecasting Approach\nWe use a ratio-based approach that links all FCF components to revenue. This makes forecasting tractable: rather than projecting absolute dollar amounts for each component, we forecast (1) revenue growth and (2) how each component scales with revenue.\nThis approach embeds a key assumption: that the relationship between revenue and FCF components remains stable. In reality, operating leverage, investment needs, and working capital requirements may change as companies mature. Sophisticated valuations model these dynamics explicitly.\n\n\n6.5.2 Setting Forecast Assumptions\nFor our five-year forecast, we make the following assumptions about FPT’s financial ratios. These should reflect industry analysis, company guidance, and competitive dynamics. Here we use estimates for illustration:\n\n# Define the forecast horizon\nlast_historical_year = historical_data[\"year\"].max()\nforecast_years = list(range(last_historical_year + 1, last_historical_year + 6))\nn_forecast_years = len(forecast_years)\n\nprint(f\"Forecast period: {forecast_years[0]} to {forecast_years[-1]}\")\n\n# Define forecast ratios\n# In practice, these would come from detailed analysis\nforecast_assumptions = pd.DataFrame({\n    \"year\": forecast_years,\n    # Operating margin: slight improvement as scale increases\n    \"operating_margin\": [0.12, 0.125, 0.13, 0.13, 0.135],\n    # Depreciation: stable as % of revenue\n    \"depreciation_margin\": [0.03, 0.03, 0.03, 0.028, 0.028],\n    # Tax rate: stable\n    \"tax_margin\": [0.02, 0.02, 0.02, 0.02, 0.02],\n    # Working capital: modest cash consumption\n    \"working_capital_margin\": [0.01, 0.01, 0.008, 0.008, 0.008],\n    # CAPEX: declining as % of revenue as growth moderates\n    \"capex_margin\": [0.05, 0.048, 0.045, 0.042, 0.04]\n})\n\nforecast_assumptions\n\nForecast period: 2024 to 2028\n\n\n\n\n\n\n\n\n\nyear\noperating_margin\ndepreciation_margin\ntax_margin\nworking_capital_margin\ncapex_margin\n\n\n\n\n0\n2024\n0.120\n0.030\n0.02\n0.010\n0.050\n\n\n1\n2025\n0.125\n0.030\n0.02\n0.010\n0.048\n\n\n2\n2026\n0.130\n0.030\n0.02\n0.008\n0.045\n\n\n3\n2027\n0.130\n0.028\n0.02\n0.008\n0.042\n\n\n4\n2028\n0.135\n0.028\n0.02\n0.008\n0.040\n\n\n\n\n\n\n\n\n\n6.5.3 Forecasting Revenue Growth\nRevenue growth is often the most important and most uncertain assumption in DCF analysis. We demonstrate two approaches: using historical averages and linking growth to macroeconomic forecasts.\nApproach 1: Historical Average\nA simple approach uses the historical average growth rate:\n\nhistorical_growth = historical_ratios[\"revenue_growth\"].dropna()\navg_historical_growth = historical_growth.mean()\n\nprint(f\"Average historical revenue growth: {avg_historical_growth:.1%}\")\n\nAverage historical revenue growth: 25.2%\n\n\nApproach 2: GDP-Linked Growth\nA more sophisticated approach links company growth to GDP forecasts from institutions like the IMF. This captures the intuition that company revenues often move with broader economic activity.\n\n# Vietnam GDP growth forecasts (illustrative, based on IMF WEO style projections)\n# In practice, download from IMF WEO database\ngdp_forecasts = pd.DataFrame({\n    \"year\": forecast_years,\n    \"gdp_growth\": [0.065, 0.063, 0.060, 0.058, 0.055]  # Gradually declining to long-term\n})\n\n# Assume FPT grows at a premium to GDP (tech sector outperformance)\n# This premium should reflect company-specific factors\ngrowth_premium = 0.05  # 5 percentage points above GDP\n\nforecast_assumptions = forecast_assumptions.merge(gdp_forecasts, on=\"year\")\nforecast_assumptions[\"revenue_growth\"] = (\n    forecast_assumptions[\"gdp_growth\"] + growth_premium\n)\n\nforecast_assumptions[[\"year\", \"gdp_growth\", \"revenue_growth\"]]\n\n\n\n\n\n\n\n\nyear\ngdp_growth\nrevenue_growth\n\n\n\n\n0\n2024\n0.065\n0.115\n\n\n1\n2025\n0.063\n0.113\n\n\n2\n2026\n0.060\n0.110\n\n\n3\n2027\n0.058\n0.108\n\n\n4\n2028\n0.055\n0.105\n\n\n\n\n\n\n\n\n\n6.5.4 Building the Forecast\nNow we combine our assumptions to project revenue and FCF:\n\n# Get the last historical revenue as our starting point\nlast_revenue = historical_data.loc[\n    historical_data[\"year\"] == last_historical_year, \"revenue\"\n].values[0]\n\nprint(f\"Last historical revenue ({last_historical_year}): {last_revenue/1e12:.2f} trillion VND\")\n\n# Project revenue forward\nforecast_data = forecast_assumptions.copy()\nforecast_data[\"revenue\"] = None\n\n# Calculate revenue for each forecast year\nfor i, row in forecast_data.iterrows():\n    if i == 0:\n        # First forecast year: grow from last historical\n        forecast_data.loc[i, \"revenue\"] = last_revenue * (1 + row[\"revenue_growth\"])\n    else:\n        # Subsequent years: grow from previous forecast\n        prev_revenue = forecast_data.loc[i-1, \"revenue\"]\n        forecast_data.loc[i, \"revenue\"] = prev_revenue * (1 + row[\"revenue_growth\"])\n\n# Convert revenue to numeric\nforecast_data[\"revenue\"] = forecast_data[\"revenue\"].astype(float)\n\n# Calculate FCF components from ratios\nforecast_data[\"ebit\"] = forecast_data[\"operating_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"depreciation\"] = forecast_data[\"depreciation_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"taxes\"] = forecast_data[\"tax_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"delta_working_capital\"] = forecast_data[\"working_capital_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"capex\"] = forecast_data[\"capex_margin\"] * forecast_data[\"revenue\"]\n\n# Calculate FCF\nforecast_data[\"fcf\"] = (\n    forecast_data[\"ebit\"]\n    - forecast_data[\"taxes\"]\n    + forecast_data[\"depreciation\"]\n    - forecast_data[\"delta_working_capital\"]\n    - forecast_data[\"capex\"]\n)\n\nforecast_data[[\"year\", \"revenue\", \"ebit\", \"fcf\"]].round(0)\n\nLast historical revenue (2023): 52.62 trillion VND\n\n\n\n\n\n\n\n\n\nyear\nrevenue\nebit\nfcf\n\n\n\n\n0\n2024\n5.866896e+13\n7.040275e+12\n4.106827e+12\n\n\n1\n2025\n6.529855e+13\n8.162319e+12\n5.027988e+12\n\n\n2\n2026\n7.248139e+13\n9.422581e+12\n6.305881e+12\n\n\n3\n2027\n8.030938e+13\n1.044022e+13\n7.067226e+12\n\n\n4\n2028\n8.874187e+13\n1.198015e+13\n8.430477e+12",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#visualizing-the-forecast",
    "href": "05_discounted_cash_flow.html#visualizing-the-forecast",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.6 Visualizing the Forecast",
    "text": "6.6 Visualizing the Forecast\nFigure 6.2 compares our forecast ratios with historical values, showing the transition from realized to projected performance.\n\n# Prepare historical data for plotting\nhistorical_plot = (historical_ratios\n    .loc[:, [\"year\", \"operating_margin\", \"depreciation_margin\", \n             \"tax_margin\", \"working_capital_margin\", \"capex_margin\"]]\n    .assign(type=\"Historical\")\n)\n\n# Prepare forecast data for plotting\nforecast_plot = (forecast_data\n    .loc[:, [\"year\", \"operating_margin\", \"depreciation_margin\",\n             \"tax_margin\", \"working_capital_margin\", \"capex_margin\"]]\n    .assign(type=\"Forecast\")\n)\n\n# Combine\ncombined_ratios = pd.concat([historical_plot, forecast_plot], ignore_index=True)\n\n# Reshape for plotting\ncombined_long = combined_ratios.melt(\n    id_vars=[\"year\", \"type\"],\n    var_name=\"ratio\",\n    value_name=\"value\"\n)\n\ncombined_long[\"type\"] = pd.Categorical(\n    combined_long[\"type\"], \n    categories=[\"Historical\", \"Forecast\"]\n)\n\nforecast_ratios_figure = (\n    ggplot(combined_long, aes(x=\"year\", y=\"value\", color=\"ratio\", linetype=\"type\"))\n    + geom_line(size=1)\n    + geom_point(size=2)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Ratio (% of Revenue)\", color=\"\", linetype=\"\",\n        title=\"Historical and Forecast Financial Ratios for FPT\"\n    )\n    + theme(legend_position=\"right\")\n)\n\nforecast_ratios_figure.show()\n\n\n\n\n\n\n\nFigure 6.2: Historical ratios (solid lines) and forecast assumptions (dashed lines) for key financial metrics. The forecast period begins after the last historical observation.\n\n\n\n\n\nFigure 6.3 shows the revenue growth trajectory, comparing historical performance with our GDP-linked forecasts.\n\n# Prepare growth data\nhistorical_growth_df = (historical_ratios\n    .loc[:, [\"year\", \"revenue_growth\"]]\n    .dropna()\n    .assign(type=\"Historical\")\n)\n\nforecast_growth_df = (forecast_data\n    .loc[:, [\"year\", \"revenue_growth\", \"gdp_growth\"]]\n    .assign(type=\"Forecast\")\n)\n\n# Combine for revenue growth\ngrowth_combined = pd.concat([\n    historical_growth_df,\n    forecast_growth_df[[\"year\", \"revenue_growth\", \"type\"]]\n], ignore_index=True)\n\ngrowth_combined[\"type\"] = pd.Categorical(\n    growth_combined[\"type\"],\n    categories=[\"Historical\", \"Forecast\"]\n)\n\ngrowth_figure = (\n    ggplot(growth_combined, aes(x=\"year\", y=\"revenue_growth\", linetype=\"type\"))\n    + geom_line(size=1, color=\"steelblue\")\n    + geom_point(size=2, color=\"steelblue\")\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Revenue Growth Rate\", linetype=\"\",\n        title=\"Historical and Forecast Revenue Growth for FPT\"\n    )\n)\n\ngrowth_figure.show()\n\n\n\n\n\n\n\nFigure 6.3: Revenue growth rates: historical (realized) and forecast (GDP-linked with company premium). The forecast assumes FPT grows at a premium to Vietnam’s GDP growth.\n\n\n\n\n\nFigure 6.4 presents the resulting FCF projections alongside historical values.\n\n# Combine historical and forecast FCF\nfcf_historical = (historical_data\n    .loc[:, [\"year\", \"fcf\"]]\n    .assign(type=\"Historical\")\n)\n\nfcf_forecast = (forecast_data\n    .loc[:, [\"year\", \"fcf\"]]\n    .assign(type=\"Forecast\")\n)\n\nfcf_combined = pd.concat([fcf_historical, fcf_forecast], ignore_index=True)\nfcf_combined[\"type\"] = pd.Categorical(\n    fcf_combined[\"type\"],\n    categories=[\"Historical\", \"Forecast\"]\n)\n\nfcf_figure = (\n    ggplot(fcf_combined, aes(x=\"year\", y=\"fcf/1e12\", fill=\"type\"))\n    + geom_col()\n    + labs(\n        x=\"\", y=\"Free Cash Flow (Trillion VND)\", fill=\"\",\n        title=\"Historical and Forecast Free Cash Flow for FPT\"\n    )\n)\n\nfcf_figure.show()\n\n\n\n\n\n\n\nFigure 6.4: Free Cash Flow: historical (realized) and forecast (projected). The forecast reflects our assumptions about revenue growth and operating ratios.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#terminal-value-capturing-long-term-value",
    "href": "05_discounted_cash_flow.html#terminal-value-capturing-long-term-value",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.7 Terminal Value: Capturing Long-Term Value",
    "text": "6.7 Terminal Value: Capturing Long-Term Value\nA critical component of DCF analysis is the terminal value (or continuation value), which represents the company’s value beyond the explicit forecast period. In most valuations, terminal value constitutes 50-80% of total enterprise value, making its estimation particularly important.\n\n6.7.1 The Perpetuity Growth Model\nThe most common approach is the Perpetuity Growth Model (also called the Gordon Growth Model), which assumes FCF grows at a constant rate forever:\n\\[\nTV_T = \\frac{FCF_{T+1}}{r - g} = \\frac{FCF_T \\times (1 + g)}{r - g}\n\\]\nwhere:\n\n\\(TV_T\\): Terminal value at the end of year \\(T\\)\n\\(FCF_T\\): Free cash flow in the final forecast year\n\\(g\\): Perpetual growth rate\n\\(r\\): Discount rate (WACC)\n\n\n\n6.7.2 Choosing the Perpetual Growth Rate\nThe perpetual growth rate \\(g\\) should reflect long-term sustainable growth. Key considerations:\n\nNo company can grow faster than the economy forever. If it did, the company would eventually become larger than GDP, which is an impossibility. Long-term GDP growth (nominal, including inflation) provides an upper bound.\nMature companies typically grow at or below GDP growth. The perpetual growth rate should reflect the company in its “steady state,” not its current high-growth phase.\nFor Vietnam, long-term nominal GDP growth might be 6-8% given current development stage, but this will moderate over time. A perpetual growth rate of 3-5% is often reasonable.\n\n\ndef compute_terminal_value(last_fcf, growth_rate, discount_rate):\n    \"\"\"\n    Compute terminal value using the perpetuity growth model.\n    \n    Parameters:\n    -----------\n    last_fcf : float\n        Free cash flow in the final forecast year\n    growth_rate : float\n        Perpetual growth rate (g)\n    discount_rate : float\n        Discount rate / WACC (r)\n        \n    Returns:\n    --------\n    float : Terminal value\n    \"\"\"\n    if discount_rate &lt;= growth_rate:\n        raise ValueError(\"Discount rate must exceed growth rate for finite terminal value\")\n    \n    return last_fcf * (1 + growth_rate) / (discount_rate - growth_rate)\n\n\n# Example calculation\nlast_fcf = forecast_data[\"fcf\"].iloc[-1]\nperpetual_growth = 0.04  # 4% perpetual growth\ndiscount_rate = 0.10     # 10% WACC (placeholder)\n\nterminal_value = compute_terminal_value(last_fcf, perpetual_growth, discount_rate)\n\nprint(f\"Last forecast FCF: {last_fcf/1e12:.2f} trillion VND\")\nprint(f\"Terminal value (at {perpetual_growth:.0%} growth, {discount_rate:.0%} WACC): {terminal_value/1e12:.1f} trillion VND\")\n\nLast forecast FCF: 8.43 trillion VND\nTerminal value (at 4% growth, 10% WACC): 146.1 trillion VND\n\n\n\n\n6.7.3 Alternative: Exit Multiple Approach\nPractitioners often cross-check terminal value using the exit multiple approach, which assumes the company is sold at the end of the forecast period at a multiple of EBITDA, EBIT, or revenue comparable to similar companies today.\nFor example, if comparable companies trade at 10x EBITDA, the terminal value would be:\n\\[\nTV_T = \\text{EBITDA}_T \\times \\text{Exit Multiple}\n\\]\nThis approach is simpler but embeds the assumption that current market multiples will persist (a strong assumption that may not hold).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#the-discount-rate-weighted-average-cost-of-capital",
    "href": "05_discounted_cash_flow.html#the-discount-rate-weighted-average-cost-of-capital",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.8 The Discount Rate: Weighted Average Cost of Capital",
    "text": "6.8 The Discount Rate: Weighted Average Cost of Capital\nThe discount rate converts future cash flows to present value. For FCF (which goes to all capital providers), we use the Weighted Average Cost of Capital (WACC):\n\\[\nWACC = \\frac{E}{E+D} \\times r_E + \\frac{D}{E+D} \\times r_D \\times (1 - \\tau)\n\\]\nwhere:\n\n\\(E\\): Market value of equity\n\\(D\\): Market value of debt\n\\(r_E\\): Cost of equity (typically estimated using CAPM)\n\\(r_D\\): Cost of debt (pre-tax)\n\\(\\tau\\): Corporate tax rate\n\nThe \\((1-\\tau)\\) term on debt reflects the tax shield. Interest payments are tax-deductible, reducing the effective cost of debt.\n\n6.8.1 Estimating WACC Components\nCost of Equity is typically estimated using the Capital Asset Pricing Model. See the CAPM chapter:\n\\[\nr_E = r_f + \\beta \\times (r_m - r_f)\n\\]\nwhere \\(r_f\\) is the risk-free rate, \\(\\beta\\) measures systematic risk, and \\((r_m - r_f)\\) is the market risk premium.\nCost of Debt can be estimated from:\n\nInterest expense divided by total debt (effective rate)\nYields on the company’s traded bonds\nYields on bonds with similar credit ratings\n\nCapital Structure Weights should use market values when available. For equity, market capitalization is straightforward. For debt, book value is often used when market values aren’t observable.\n\n\n6.8.2 Using Industry WACC Data\nProfessor Aswath Damodaran at NYU Stern maintains comprehensive industry WACC and country risk premium data. We use these datasets to estimate appropriate discount rates for Vietnamese companies.\n\n6.8.2.1 Downloading the Data\nThe following code downloads the required datasets. Run this manually when you need to update the data (typically annually), but it is not executed during book rendering to avoid dependency on external servers.\n\nimport requests\nfrom pathlib import Path\n\n# Create data directory if needed\ndata_dir = Path(\"data\")\ndata_dir.mkdir(exist_ok=True)\n\n# Download WACC data\nwacc_url = \"https://pages.stern.nyu.edu/~adamodar/pc/datasets/wacc.xls\"\nresponse = requests.get(wacc_url, timeout=30)\nresponse.raise_for_status()\n(data_dir / \"damodaran_wacc.xls\").write_bytes(response.content)\nprint(\"Downloaded: damodaran_wacc.xls\")\n\n# Download country risk premium data\ncrp_url = \"https://pages.stern.nyu.edu/~adamodar/pc/datasets/ctryprem.xlsx\"\nresponse = requests.get(crp_url, timeout=30)\nresponse.raise_for_status()\n(data_dir / \"damodaran_crp.xlsx\").write_bytes(response.content)\nprint(\"Downloaded: damodaran_crp.xlsx\")\n\n\n\n6.8.2.2 Industry WACC\nWe extract the cost of capital for the Computer Services industry, which most closely matches FPT’s business profile:\n\nimport pandas as pd\n\n# Read local WACC data\nwacc_data = pd.read_excel(\n    \"data/damodaran_wacc.xls\", \n    sheet_name=1, \n    skiprows=18\n)\n\n# Find WACC for Computer Services\nindustry_wacc = wacc_data.loc[\n    wacc_data[\"Industry Name\"] == \"Computer Services\",\n    \"Cost of Capital\"\n].values[0]\n\nprint(f\"Industry WACC (Computer Services): {industry_wacc:.2%}\")\n\nIndustry WACC (Computer Services): 7.83%\n\n\n\n\n6.8.2.3 Country Risk Premium\nFor Vietnamese companies, we must adjust for country-specific risk. Damodaran calculates country risk premiums based on sovereign credit ratings and relative equity market volatility:\n\n# Read local country risk premium data\ncrp_data = pd.read_excel(\n    \"data/damodaran_crp.xlsx\",\n    sheet_name=\"ERPs by country\",\n    skiprows=7\n)\n\n# Find Vietnam's country risk premium\nvietnam_row = crp_data[\n    crp_data.iloc[:, 0].str.contains(\"Vietnam\", case=False, na=False)\n]\n\ncountry_risk_premium = vietnam_row.iloc[0, 8]\nprint(f\"Vietnam Country Risk Premium: {country_risk_premium:.2%}\")\n\nVietnam Country Risk Premium: 2.09%\n\n\n\n\n6.8.2.4 Adjusted WACC for Vietnam\nCombining the industry benchmark with the country risk premium gives us an appropriate discount rate:\n\nwacc_vietnam = industry_wacc + country_risk_premium\n\nprint(f\"Industry WACC (US):        {industry_wacc:.2%}\")\nprint(f\"Country Risk Premium:      {country_risk_premium:.2%}\")\nprint(f\"Adjusted WACC (Vietnam):   {wacc_vietnam:.2%}\")\n\nwacc = wacc_vietnam\n\nIndustry WACC (US):        7.83%\nCountry Risk Premium:      2.09%\nAdjusted WACC (Vietnam):   9.92%\n\n\nNote: This approach assumes Vietnamese companies in the same industry face similar operating risks as their US counterparts, with the country risk premium capturing additional macroeconomic and political risks. For company-specific analysis, further adjustments for leverage differences and firm-specific risk factors may be warranted.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#computing-enterprise-value",
    "href": "05_discounted_cash_flow.html#computing-enterprise-value",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.9 Computing Enterprise Value",
    "text": "6.9 Computing Enterprise Value\nWith all components in place, we can now compute enterprise value. The DCF formula is:\n\\[\n\\text{Enterprise Value} = \\sum_{t=1}^{T} \\frac{FCF_t}{(1 + WACC)^t} + \\frac{TV_T}{(1 + WACC)^T}\n\\]\nThe first term is the present value of forecast-period cash flows; the second is the present value of terminal value.\n\ndef compute_dcf_value(fcf_series, wacc, perpetual_growth):\n    \"\"\"\n    Compute enterprise value using DCF analysis.\n    \n    Parameters:\n    -----------\n    fcf_series : array-like\n        Free cash flows for forecast period\n    wacc : float\n        Weighted average cost of capital\n    perpetual_growth : float\n        Perpetual growth rate for terminal value\n        \n    Returns:\n    --------\n    dict : Components of DCF valuation\n    \"\"\"\n    fcf = np.array(fcf_series)\n    n_years = len(fcf)\n    \n    # Discount factors\n    discount_factors = (1 + wacc) ** np.arange(1, n_years + 1)\n    \n    # Present value of forecast period cash flows\n    pv_fcf = fcf / discount_factors\n    pv_fcf_total = pv_fcf.sum()\n    \n    # Terminal value and its present value\n    terminal_value = compute_terminal_value(fcf[-1], perpetual_growth, wacc)\n    pv_terminal = terminal_value / discount_factors[-1]\n    \n    # Total enterprise value\n    enterprise_value = pv_fcf_total + pv_terminal\n    \n    return {\n        \"pv_fcf\": pv_fcf_total,\n        \"terminal_value\": terminal_value,\n        \"pv_terminal\": pv_terminal,\n        \"enterprise_value\": enterprise_value,\n        \"terminal_pct\": pv_terminal / enterprise_value\n    }\n\n\n# Compute DCF value\nperpetual_growth = 0.04  # 4% perpetual growth\n\ndcf_result = compute_dcf_value(\n    fcf_series=forecast_data[\"fcf\"].values,\n    wacc=wacc,\n    perpetual_growth=perpetual_growth\n)\n\nprint(\"DCF Valuation Results\")\nprint(\"=\" * 50)\nprint(f\"PV of Forecast Period FCF: {dcf_result['pv_fcf']/1e12:.1f} trillion VND\")\nprint(f\"Terminal Value: {dcf_result['terminal_value']/1e12:.1f} trillion VND\")\nprint(f\"PV of Terminal Value: {dcf_result['pv_terminal']/1e12:.1f} trillion VND\")\nprint(f\"Enterprise Value: {dcf_result['enterprise_value']/1e12:.1f} trillion VND\")\nprint(f\"Terminal Value as % of EV: {dcf_result['terminal_pct']:.1%}\")\n\nDCF Valuation Results\n==================================================\nPV of Forecast Period FCF: 22.7 trillion VND\nTerminal Value: 148.1 trillion VND\nPV of Terminal Value: 92.3 trillion VND\nEnterprise Value: 115.1 trillion VND\nTerminal Value as % of EV: 80.2%\n\n\nNote that terminal value often represents 60-80% of enterprise value. This highlights the importance of terminal value assumptions and the inherent uncertainty in DCF analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#sensitivity-analysis",
    "href": "05_discounted_cash_flow.html#sensitivity-analysis",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.10 Sensitivity Analysis",
    "text": "6.10 Sensitivity Analysis\nGiven the uncertainty in DCF inputs, sensitivity analysis is essential. We examine how enterprise value changes with different assumptions about WACC and perpetual growth.\n\n# Define ranges for sensitivity analysis\nwacc_range = np.arange(0.08, 0.14, 0.01)  # 8% to 13%\ngrowth_range = np.arange(0.02, 0.06, 0.01)  # 2% to 5%\n\n# Create all combinations\nsensitivity_results = []\n\nfor w in wacc_range:\n    for g in growth_range:\n        if w &gt; g:  # Must have WACC &gt; growth for valid terminal value\n            result = compute_dcf_value(\n                fcf_series=forecast_data[\"fcf\"].values,\n                wacc=w,\n                perpetual_growth=g\n            )\n            sensitivity_results.append({\n                \"wacc\": w,\n                \"growth_rate\": g,\n                \"enterprise_value\": result[\"enterprise_value\"] / 1e12  # In trillions\n            })\n\nsensitivity_df = pd.DataFrame(sensitivity_results)\n\n# Create heatmap\nsensitivity_figure = (\n    ggplot(sensitivity_df, aes(x=\"wacc\", y=\"growth_rate\", fill=\"enterprise_value\"))\n    + geom_tile()\n    + geom_text(\n        aes(label=\"enterprise_value\"),\n        format_string=\"{:.0f}\",\n        color=\"white\",\n        size=9\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + scale_fill_gradient(low=\"darkblue\", high=\"lightblue\")\n    + labs(\n        x=\"WACC\", y=\"Perpetual Growth Rate\",\n        fill=\"EV\\n(Trillion VND)\",\n        title=\"DCF Sensitivity: Enterprise Value by WACC and Growth Rate\"\n    )\n)\n\nsensitivity_figure.show()\n\n\n\n\n\n\n\nFigure 6.5: Sensitivity of enterprise value to WACC and perpetual growth rate assumptions. Small changes in these inputs can substantially affect valuation.\n\n\n\n\n\nThe sensitivity analysis reveals several important insights:\n\nValuation is highly sensitive to inputs: Small changes in WACC or growth rate produce large changes in enterprise value. A 1 percentage point change in WACC can shift value by 20% or more.\nThe relationship is non-linear: The impact of growth rate changes is amplified at lower WACCs because the terminal value formula has \\((r-g)\\) in the denominator.\nReasonable people can disagree: Given input uncertainty, DCF should be thought of as producing a range of values, not a single precise number.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#from-enterprise-value-to-equity-value",
    "href": "05_discounted_cash_flow.html#from-enterprise-value-to-equity-value",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.11 From Enterprise Value to Equity Value",
    "text": "6.11 From Enterprise Value to Equity Value\nOur DCF analysis yields enterprise value (i.e., the total value of the company’s operations to all capital providers). To determine equity value (what shareholders own), we must adjust for the claims of debt holders and any non-operating assets:\n\\[\n\\text{Equity Value} = \\text{Enterprise Value} + \\text{Non-Operating Assets} - \\text{Debt}\n\\]\nNon-Operating Assets include:\n\nExcess cash beyond operating needs\nMarketable securities\nNon-core real estate or investments\n\nDebt includes:\n\nShort-term debt\nLong-term debt\nCapital lease obligations\nPreferred stock (if treated as debt-like)\n\n\n# Get most recent balance sheet data for FPT\nlatest_year = fpt_data[\"year\"].max()\nlatest_data = fpt_data[fpt_data[\"year\"] == latest_year].iloc[0]\n\n# Extract debt and cash (column names may vary)\ntotal_debt = latest_data.get(\"total_debt\", 0)\ncash = latest_data.get(\"ca_cce\", 0)\n\n# Compute equity value\nenterprise_value = dcf_result[\"enterprise_value\"]\nequity_value = enterprise_value - total_debt + cash\n\nprint(\"From Enterprise Value to Equity Value\")\nprint(\"=\" * 50)\nprint(f\"Enterprise Value: {enterprise_value/1e12:.1f} trillion VND\")\nprint(f\"Less: Total Debt: {total_debt/1e12:.1f} trillion VND\")\nprint(f\"Plus: Cash: {cash/1e12:.1f} trillion VND\")\nprint(f\"Equity Value: {equity_value/1e12:.1f} trillion VND\")\n\nFrom Enterprise Value to Equity Value\n==================================================\nEnterprise Value: 115.1 trillion VND\nLess: Total Debt: 0.0 trillion VND\nPlus: Cash: 8.3 trillion VND\nEquity Value: 123.3 trillion VND\n\n\n\n6.11.1 Implied Share Price\nIf we know the number of shares outstanding, we can compute an implied share price:\n\n# Get shares outstanding (this would come from market data)\n# Using placeholder - in practice, get from exchange data\nshares_outstanding = latest_data.get(\"total_equity\", equity_value) / 25000  # Rough estimate\n\nimplied_price = equity_value / shares_outstanding\n\nprint(f\"\\nImplied Share Price: {implied_price:,.0f} VND\")\n\n\nImplied Share Price: 103,008 VND\n\n\nComparing the implied price to the current market price tells us whether the stock appears under- or overvalued according to our DCF model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#limitations-and-practical-considerations",
    "href": "05_discounted_cash_flow.html#limitations-and-practical-considerations",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.12 Limitations and Practical Considerations",
    "text": "6.12 Limitations and Practical Considerations\nDCF analysis is powerful but has important limitations:\n\n6.12.1 Sensitivity to Assumptions\nAs our sensitivity analysis showed, small changes in inputs produce large changes in value. This is particularly problematic because the most influential inputs (long-term growth, WACC) are the hardest to estimate accurately.\n\n\n6.12.2 Terminal Value Dominance\nTerminal value often represents 60-80% of total value, yet it’s based on assumptions about the very distant future. This concentrates valuation risk in the most uncertain component.\n\n\n6.12.3 Garbage In, Garbage Out\nDCF is only as good as its inputs. Unrealistic growth assumptions, optimistic margins, or inappropriate discount rates produce meaningless valuations. The discipline of DCF lies in forcing analysts to justify their assumptions.\n\n\n6.12.4 Not Suitable for All Companies\nDCF works best for companies with:\n\nPositive and predictable cash flows\nStable or predictably changing margins\nReasonable visibility into future operations\n\nIt struggles with:\n\nEarly-stage companies with no profits\nHighly cyclical businesses\nCompanies undergoing major transitions\nFinancial institutions (which require different approaches)\n\n\n\n6.12.5 Complement with Other Methods\nWise practitioners use DCF alongside other valuation methods:\n\nComparable company analysis: How do similar companies trade?\nPrecedent transactions: What have acquirers paid for similar businesses?\nSum-of-the-parts: Value divisions separately and add\n\nWhen methods converge, confidence increases. When they diverge, it prompts investigation into why.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "05_discounted_cash_flow.html#key-takeaways",
    "href": "05_discounted_cash_flow.html#key-takeaways",
    "title": "6  Discounted Cash Flow Analysis",
    "section": "6.13 Key Takeaways",
    "text": "6.13 Key Takeaways\nThis chapter introduced Discounted Cash Flow analysis as a framework for intrinsic valuation. The main insights are:\n\nFree Cash Flow is the foundation: FCF represents cash available to all investors after operating expenses, taxes, and investments. It differs from net income by excluding non-cash items and including investment needs.\nRatio-based forecasting links components to revenue: By expressing FCF components as percentages of revenue, we can systematically forecast cash flows based on revenue growth assumptions and operating ratio projections.\nTerminal value captures long-term value: The perpetuity growth model assumes FCF grows at a constant rate forever. The perpetual growth rate should not exceed long-term economic growth.\nWACC is the appropriate discount rate: The Weighted Average Cost of Capital reflects the blended cost of debt and equity financing, adjusted for the tax shield on interest.\nDCF produces enterprise value: To derive equity value, subtract debt and add non-operating assets. Dividing by shares outstanding yields an implied share price.\nSensitivity analysis is essential: Given input uncertainty, presenting a range of values based on different assumptions is more honest than a single point estimate.\nDCF complements other methods: No single valuation method is definitive. Cross-checking DCF with market multiples and transaction comparables provides a more complete picture.\n\nThe true value of DCF analysis lies not in producing a precise number but in forcing rigorous thinking about what drives company value. The process of building a DCF model (i.e., forecasting growth, projecting margins, estimating risk) develops deep understanding of the business being valued.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html",
    "href": "06_accessing_and_managing_financial_data.html",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "",
    "text": "7.1 Overview of Vietnamese Financial Data Sources\nThis chapter provides a guide to organizing, accessing, and managing financial data specifically tailored for the Vietnamese market. While global financial databases such as CRSP and Compustat serve as standard resources for developed markets, emerging markets like Vietnam require a different approach due to unique data sources, market structures, and regulatory environments. Understanding these nuances is essential for conducting rigorous empirical research on Vietnamese equities, bonds, and macroeconomic indicators.\nVietnam’s financial market has experienced remarkable growth since the establishment of the Ho Chi Minh City Stock Exchange (HOSE) in 2000 and the Hanoi Stock Exchange (HNX) in 2005. Today, the market comprises over 1,600 listed companies across three trading venues: HOSE for large-cap stocks, HNX for mid-cap stocks, and UPCoM (Unlisted Public Company Market) for smaller companies transitioning to formal listing. This diversity creates both opportunities and challenges for financial researchers seeking comprehensive coverage of the Vietnamese equity universe.\nThe Vietnamese market presents several distinctive characteristics that researchers must account for. Foreign ownership limits (typically 49% for most sectors, with exceptions for banking and certain strategic industries), trading band restrictions (e.g., currently \\(\\pm\\) 7% for HOSE and \\(\\pm\\) 10% for HNX), and the T+2 settlement cycle all influence market microstructure and return dynamics. Additionally, the market operates in Vietnamese Dong (VND), requiring careful attention to currency effects when comparing results with international studies.\nWe begin by loading the essential Python packages that facilitate data acquisition and management throughout this chapter.\nWe also define the date range for our data collection, which spans from the early days of the Vietnamese stock market to the present. This extended timeframe allows us to capture the market’s evolution through various economic cycles, including the 2008 global financial crisis, the 2011-2012 domestic banking crisis, and the COVID-19 pandemic period.\nBefore diving into the technical implementation, it is valuable to understand the landscape of financial data providers serving the Vietnamese market. Unlike developed markets where a few dominant providers (Bloomberg, Refinitiv, FactSet) offer comprehensive coverage, Vietnamese financial data has historically been fragmented across multiple sources, each with distinct strengths and limitations.\nThe primary sources of Vietnamese financial data include official exchange feeds from HOSE and HNX, which provide real-time and historical trading data. The State Securities Commission of Vietnam (SSC) publishes regulatory filings, corporate announcements, and market statistics. Commercial data vendors such as FiinGroup, StoxPlus (now part of FiinGroup), and VNDirect offer curated datasets with varying levels of coverage and data quality. Additionally, the State Bank of Vietnam (SBV) and the General Statistics Office (GSO) provide macroeconomic indicators essential for asset pricing research.\nFor academic researchers, this fragmentation traditionally involved difficult trade-offs between cost, coverage, data quality, and ease of access. Commercial providers like FiinGroup offer clean, standardized data but require subscription fees that may be prohibitive for individual researchers and smaller institutions. Open-source alternatives provide free access but often require substantial data cleaning and validation efforts. Manually collecting data from government websites is time-consuming and prone to inconsistencies.\nFortunately, this landscape has improved significantly with the emergence of Datacore as a unified data platform for Vietnamese financial markets. In our experience working with Vietnamese financial data across multiple research projects, Datacore has proven to be the most practical solution for academic research. The platform consolidates data from multiple sources, including stock prices, corporate fundamentals, market indices, macroeconomic indicators, and alternative data, into a single, accessible interface with a well-documented API.\nWhat distinguishes Datacore from traditional commercial providers like FiinGroup extends beyond mere data aggregation. While FiinGroup has long been the institutional incumbent, several factors make Datacore particularly attractive for rigorous empirical research:\nThroughout this chapter, we leverage Datacore as our primary data source. By centralizing our data acquisition through a single platform, we benefit from consistent data formats, reliable corporate action adjustments, and comprehensive market coverage spanning HOSE, HNX, and UPCoM. The code examples that follow demonstrate how straightforward Vietnamese financial research becomes when data access friction is minimized.\nThe following table summarizes the key data sources for Vietnamese financial research:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#overview-of-vietnamese-financial-data-sources",
    "href": "06_accessing_and_managing_financial_data.html#overview-of-vietnamese-financial-data-sources",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "",
    "text": "API-First Architecture: Datacore was built from the ground up for programmatic access, making it seamlessly integrable with Python, R, and other research workflows. FiinGroup’s data access, by contrast, often requires manual downloads or cumbersome Excel-based interfaces that impede reproducibility.\nCost Efficiency: Academic researchers frequently operate under budget constraints. Datacore offers competitive pricing structures that make comprehensive market coverage accessible without the substantial subscription fees associated with legacy providers.\nCorporate Action Handling: One persistent challenge with Vietnamese data is accurate adjustment for stock splits, bonus shares, and rights issues. Datacore implements transparent adjustment methodologies with clear documentation, whereas legacy providers often apply adjustments inconsistently or without adequate explanation.\nUpdate Frequency: Datacore maintains near real-time data updates with clear timestamps, enabling event study research and timely portfolio rebalancing. Traditional providers often suffer from publication lags that can compromise research requiring current data.\nCoverage Breadth: Beyond standard price and fundamental data, Datacore integrates alternative data, and macroeconomic indicators into a unified schema. This eliminates the need to merge datasets from multiple sources, which is a process that introduces potential errors and consumes valuable research time.\n\n\n\n\n\n\nTable 7.1: Vietnamese Financial Data Sources\n\n\n\n\n\n\n\n\n\n\n\n\nData Source\nCoverage\nAccess Type\nKey Strengths\nLimitations\n\n\n\n\nDatacore\nPrices, fundamentals, indices, macro, derivatives\nAPI\nUnified platform, programmatic access, comprehensive coverage, transparent methodology\nNewer platform\n\n\nFiinGroup\nFull market coverage\nCommercial\nEstablished reputation, institutional adoption\nHigh cost, manual access, limited API\n\n\nHOSE/HNX websites\nOfficial exchange data\nFree (manual)\nAuthoritative, real-time\nNo API, manual collection required\n\n\nGSO (gso.gov.vn)\nMacroeconomic indicators\nFree (manual)\nOfficial government statistics\nInfrequent updates, no API\n\n\nSBV (sbv.gov.vn)\nMonetary policy, rates\nFree (manual)\nCentral bank data\nManual download only\n\n\nCafeF/VnExpress\nNews, announcements\nFree\nMarket sentiment, events\nUnstructured, requires NLP processing",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#stock-market-data",
    "href": "06_accessing_and_managing_financial_data.html#stock-market-data",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.2 Stock Market Data",
    "text": "7.2 Stock Market Data\nThe resulting DataFrame contains essential security identifiers including the ticker symbol, company name in both Vietnamese and English, exchange listing, industry classification according to the Vietnam Standard Industrial Classification (VSIC), and various flags indicating special status such as foreign ownership restrictions or trading suspensions.\n\n7.2.1 Historical Price Data\n\n\n7.2.2 Fundamental Data and Financial Statements\nBeyond price data, fundamental analysis requires access to corporate financial statements including balance sheets, income statements, and cash flow statements. Vietnamese publicly listed companies are required to publish quarterly and annual financial reports according to Vietnamese Accounting Standards (VAS), which differ in certain respects from International Financial Reporting Standards (IFRS). Understanding these differences is important when comparing Vietnamese firms with international peers or applying models developed using US or European data.\nKey differences between VAS and IFRS that affect financial analysis include:\n\nRevenue recognition: VAS allows more flexibility in timing of revenue recognition compared to IFRS 15\nFinancial instruments: VAS has less comprehensive guidance on fair value measurement\nLease accounting: VAS does not require operating lease capitalization as under IFRS 16\nGoodwill: VAS requires amortization while IFRS requires impairment testing only\n\n\n\n7.2.3 Corporate Actions and Events\nAccurate treatment of corporate actions is essential for computing correct returns and maintaining data integrity. Vietnamese companies frequently engage in corporate actions including cash dividends, stock dividends (bonus shares), rights issues, and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#market-indices-and-benchmarks",
    "href": "06_accessing_and_managing_financial_data.html#market-indices-and-benchmarks",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.3 Market Indices and Benchmarks",
    "text": "7.3 Market Indices and Benchmarks\nConstructing appropriate benchmarks is fundamental to performance evaluation and factor model estimation. The Vietnamese market features several indices that serve different purposes in financial research.\n\n\n\nTable 7.2: Vietnamese Market Indices\n\n\n\n\n\n\n\n\n\n\n\nIndex\nExchange\nDescription\nUse Case\n\n\n\n\nVN-Index\nHOSE\nAll HOSE-listed stocks\nBroad market benchmark\n\n\nVN30-Index\nHOSE\n30 largest, most liquid\nInvestable benchmark\n\n\nHNX-Index\nHNX\nAll HNX-listed stocks\nMid-cap benchmark\n\n\nHNX30-Index\nHNX\n30 largest HNX stocks\nHNX large-cap\n\n\nVNAllShare\nCombined\nHOSE + HNX\nTotal market\n\n\nVN100\nCombined\nTop 100 stocks\nLarge/mid-cap\n\n\n\n\n\n\nThe VN-Index, which tracks all stocks listed on HOSE, is the most widely followed benchmark and serves as the primary gauge of overall market performance. The HNX-Index covers stocks on the Hanoi exchange, while the VN30-Index tracks the thirty largest and most liquid stocks on HOSE.\nFor asset pricing research, the VN30-Index is particularly valuable as it represents the investable universe for institutional investors and serves as the underlying for Vietnam’s most liquid derivatives contracts. The constituent stocks are reviewed semi-annually based on market capitalization, liquidity, and free-float requirements.\n\n# Retrieve VN-Index historical data\n\n\n7.3.1 Index Constituent Data\nFor factor model construction and portfolio analysis, access to index constituent lists and their weights is essential. While official constituent data requires subscription to exchange data feeds, we can approximate index membership using market capitalization and liquidity filters.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#macroeconomic-data-from-vietnamese-sources",
    "href": "06_accessing_and_managing_financial_data.html#macroeconomic-data-from-vietnamese-sources",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.4 Macroeconomic Data from Vietnamese Sources",
    "text": "7.4 Macroeconomic Data from Vietnamese Sources\nAsset pricing models often incorporate macroeconomic variables as predictors of expected returns or as state variables in conditional models. For the Vietnamese market, relevant macroeconomic data comes primarily from two sources: the General Statistics Office (GSO) and the State Bank of Vietnam (SBV).\n\n7.4.1 Key Macroeconomic Indicators\nThe following macroeconomic variables are particularly relevant for Vietnamese financial research:\n\nConsumer Price Index (CPI): Essential for computing real returns and inflation-adjusted valuations. Vietnam experienced periods of high inflation, particularly during 2008 and 2011 when annual CPI exceeded 20%.\nIndustrial Production Index (IPI): Proxy for economic activity and business cycle conditions.\nMoney Supply (M2): Indicator of monetary policy stance and liquidity conditions.\nCredit Growth: Bank lending growth, a key driver of economic activity in Vietnam’s bank-dominated financial system.\nUSD/VND Exchange Rate: Critical for international investors and companies with foreign currency exposure.\nForeign Direct Investment (FDI): Indicator of international capital flows and economic confidence.\nTrade Balance: Export and import dynamics affecting corporate earnings.\n\nUnfortunately, unlike the US Federal Reserve’s FRED database, Vietnamese macroeconomic data is not available through standardized APIs. Researchers must typically download data manually from GSO and SBV websites or use web scraping techniques.\n\n# Structure for Vietnamese macroeconomic data\n\n\n\n7.4.2 Risk-Free Rate Approximation\nDetermining an appropriate risk-free rate for Vietnam presents challenges not encountered in developed markets. Unlike the US Treasury market, Vietnam’s government bond market is relatively illiquid with limited secondary trading. Several alternatives exist:\n\nSBV Refinancing Rate: The policy rate set by the State Bank of Vietnam. Not directly investable but reflects monetary policy stance.\nGovernment Bond Yields: One-year or longer-term government bond yields from auction results. More investable but less liquid than US Treasuries.\nInterbank Rates: Overnight or term interbank lending rates. Reflect short-term funding costs but include credit risk.\nAdjusted US Rate: US Treasury rate plus expected VND depreciation, following uncovered interest rate parity.\n\n\ndef calculate_risk_free_rate(macro_data, method=\"refinancing\"):\n    \"\"\"\n    Calculate risk-free rate proxy for Vietnamese market.\n    \n    Parameters\n    ----------\n    macro_data : pd.DataFrame\n        DataFrame with macroeconomic data\n    method : str\n        Method for risk-free rate: 'refinancing', 'bond', or 'adjusted_us'\n    \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with date and monthly risk-free rate\n    \"\"\"\n    if method == \"refinancing\":\n        # Use SBV refinancing rate, convert annual to monthly\n        rf = macro_data[[\"date\", \"refinancing_rate\"]].copy()\n        rf[\"rf_monthly\"] = rf[\"refinancing_rate\"] / 12 / 100\n        \n    elif method == \"adjusted_us\":\n        # US rate + expected VND depreciation\n        # Requires additional data on US rates and exchange rate expectations\n        pass\n    \n    return rf[[\"date\", \"rf_monthly\"]]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#setting-up-a-database-for-vietnamese-financial-data",
    "href": "06_accessing_and_managing_financial_data.html#setting-up-a-database-for-vietnamese-financial-data",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.5 Setting Up a Database for Vietnamese Financial Data",
    "text": "7.5 Setting Up a Database for Vietnamese Financial Data\nManaging financial data across multiple sources and formats requires a systematic approach to data storage. We recommend using SQLite as the primary database engine for several reasons: it requires no server setup, stores the entire database in a single portable file, supports standard SQL queries, and integrates seamlessly with Python through the built-in sqlite3 module.\n\n7.5.1 Database Schema Design\nOur database schema is designed to support efficient queries for common research tasks while maintaining data integrity. We create separate tables for different data types with appropriate relationships.\n\nimport os\nimport sqlite3\n\n# Create data directory if it doesn't exist\nif not os.path.exists(\"data\"):\n    os.makedirs(\"data\")\n\n# Initialize SQLite database connection\ntidy_finance_python = sqlite3.connect(\n    \"data/tidy_finance_python.sqlite\"\n)\n\n\n\n7.5.2 Storing Data\nWith the database schema established, we can store our collected data using pandas’ to_sql() method.\n\n# Store stock listing data\ncommon_stocks.to_sql(\n    name=\"stock_master\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store stock price data\nstock_prices.to_sql(\n    name=\"stock_prices_daily\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store market indices\nvn_index.to_sql(\n    name=\"market_indices\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store factor returns\nfactors_vietnam.to_sql(\n    name=\"factors_monthly\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#querying-and-updating-the-database",
    "href": "06_accessing_and_managing_financial_data.html#querying-and-updating-the-database",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.6 Querying and Updating the Database",
    "text": "7.6 Querying and Updating the Database\nOnce data is stored in the database, retrieval is straightforward using SQL queries. The pandas read_sql_query() function executes a SQL statement and returns the results as a DataFrame.\n\n# Query stock prices for specific symbols and date range\nquery = \"\"\"\nSELECT date, symbol, close, volume\nFROM stock_prices_daily\nWHERE symbol IN ('VNM', 'VIC', 'FPT', 'VHM', 'VCB')\n  AND date &gt;= '2020-01-01'\nORDER BY symbol, date\n\"\"\"\n\nselected_stocks = pd.read_sql_query(\n    sql=query,\n    con=tidy_finance_python,\n    parse_dates=[\"date\"]\n)\n\n# Query factor data merged with market returns\nquery_factors = \"\"\"\nSELECT f.date, f.mkt_rf, f.smb, f.hml, f.rf,\n       m.cpi_yoy, m.credit_growth\nFROM factors_monthly f\nLEFT JOIN macro_monthly m ON f.date = m.date\nWHERE f.date &gt;= '2015-01-01'\nORDER BY f.date\n\"\"\"\n\nfactor_data = pd.read_sql_query(\n    sql=query_factors,\n    con=tidy_finance_python,\n    parse_dates=[\"date\"]\n)\n\n\n7.6.1 Database Maintenance\nRegular database maintenance ensures optimal performance and data integrity.\n\n# Optimize database\ntidy_finance_python.execute(\"VACUUM\")\n\n# Check database integrity\nintegrity_check = pd.read_sql_query(\n    \"PRAGMA integrity_check\",\n    tidy_finance_python\n)\nprint(f\"Integrity check: {integrity_check.iloc[0, 0]}\")\n\n# Get database statistics\ntable_stats = pd.read_sql_query(\"\"\"\n    SELECT name, \n           (SELECT COUNT(*) FROM stock_prices_daily) as price_rows,\n           (SELECT COUNT(*) FROM stock_master) as stock_count,\n           (SELECT COUNT(*) FROM factors_monthly) as factor_months\n    FROM sqlite_master\n    WHERE type='table' AND name='stock_master'\n\"\"\", tidy_finance_python)\n\nprint(table_stats)\n\n# Close connection when done\ntidy_finance_python.close()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#alternative-data-sources-for-vietnamese-markets",
    "href": "06_accessing_and_managing_financial_data.html#alternative-data-sources-for-vietnamese-markets",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.7 Alternative Data Sources for Vietnamese Markets",
    "text": "7.7 Alternative Data Sources for Vietnamese Markets\nBeyond traditional price and fundamental data, researchers increasingly incorporate alternative data sources to gain unique insights into market dynamics.\n\n7.7.1 Foreign Investor Flow Data\nForeign investor flow data is particularly valuable given the significant role of foreign capital in Vietnamese equity markets. The State Securities Commission publishes daily foreign ownership statistics by security.\n\n\n7.7.2 News and Sentiment Data\nMedia sentiment from Vietnamese financial news sources offers another research avenue. Major outlets such as CafeF, VnExpress Finance, and Vietstock publish real-time news that can be analyzed for market sentiment.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "06_accessing_and_managing_financial_data.html#key-takeaways",
    "href": "06_accessing_and_managing_financial_data.html#key-takeaways",
    "title": "7  Accessing and Managing VN Financial Data",
    "section": "7.8 Key Takeaways",
    "text": "7.8 Key Takeaways\n\nMarket Structure Understanding: The Vietnamese financial market operates across three exchanges (HOSE, HNX, UPCoM) with distinct characteristics including foreign ownership limits, trading band restrictions, and a T+2 settlement cycle. Researchers must account for these institutional features in empirical analysis.\nMacroeconomic Data Challenges: Unlike developed markets with standardized APIs (e.g., FRED), Vietnamese macroeconomic data requires manual collection from government sources (GSO, SBV). Researchers should plan for this additional data gathering effort and implement systematic data management practices.\nDatabase-Centric Workflow: SQLite provides an efficient and portable database solution for managing Vietnamese financial data across research projects. The structured database approach enables reproducible research workflows, efficient queries, and easy data sharing among collaborators.\nData Quality Imperative: Data quality validation is especially important for emerging market data. Implementing systematic checks for missing values, extreme returns, duplicate entries, and cross-source validation helps ensure research reliability and reproducibility.\nAlternative Data Opportunities: Foreign investor flows, corporate announcements, and media sentiment provide unique research opportunities in the Vietnamese market that can complement traditional price and fundamental analysis. These data sources can reveal insights about market dynamics not captured in standard datasets.\nContinuous Maintenance: Financial databases require ongoing maintenance including incremental updates, integrity checks, and optimization. Establishing systematic update procedures ensures data currency and database performance over time.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Accessing and Managing VN Financial Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html",
    "href": "07_datacore_data.html",
    "title": "8  Datacore Data",
    "section": "",
    "text": "8.1 Data Access Options\nThis chapter introduces Datacore, Vietnam’s data platform for academic, corporate, and government research. Datacore provides comprehensive financial and economic datasets, including historical trading data, company fundamentals, and macroeconomic indicators essential for reproducible finance research. We use Datacore as the primary data source throughout this book.\nReaders can access the data used in this book through several channels:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#data-access-options",
    "href": "07_datacore_data.html#data-access-options",
    "title": "8  Datacore Data",
    "section": "",
    "text": "Institutional subscription: Many universities and research institutions subscribe to Datacore. Check with your library or research office for access credentials. If your institution does not yet have a subscription, consider requesting one through your library’s acquisition process—Datacore offers institutional pricing for academic use.\nDemo datasets: Datacore provides demo datasets that allow you to run the code examples in this book with sample data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#chapter-overview",
    "href": "07_datacore_data.html#chapter-overview",
    "title": "8  Datacore Data",
    "section": "8.2 Chapter Overview",
    "text": "8.2 Chapter Overview\nThe chapter is organized as follows. We first establish the connection to Datacore’s cloud storage infrastructure. Then, we download and prepare company fundamentals data, including balance sheet items, income statement variables, and derived metrics essential for asset pricing research. Next, we retrieve and process stock price data, computing returns, market capitalizations, and excess returns. We conclude by merging these datasets and providing descriptive statistics that characterize the Vietnamese equity market.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#setting-up-the-environment",
    "href": "07_datacore_data.html#setting-up-the-environment",
    "title": "8  Datacore Data",
    "section": "8.3 Setting Up the Environment",
    "text": "8.3 Setting Up the Environment\nWe begin by loading the Python packages used throughout this chapter. The core packages include pandas for data manipulation, numpy for numerical operations, and sqlite3 for local database management. We also import visualization libraries for creating publication-quality figures.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom datetime import datetime\nfrom io import BytesIO\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format, percent_format\n\nWe establish a connection to our local SQLite database, which serves as the central repository for all processed data. This database was introduced in the previous chapter and will store the cleaned datasets for use in subsequent analyses.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nWe define the date range for our data collection. The Vietnamese stock market began operations in July 2000 with the establishment of the Ho Chi Minh City Stock Exchange (HOSE), so our sample period starts from 2000 and extends through the end of 2024.\n\nstart_date = \"2000-01-01\"\nend_date = \"2024-12-31\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#connecting-to-datacore",
    "href": "07_datacore_data.html#connecting-to-datacore",
    "title": "8  Datacore Data",
    "section": "8.4 Connecting to Datacore",
    "text": "8.4 Connecting to Datacore\nDatacore delivers data through a cloud-based object storage system built on MinIO, an S3-compatible storage infrastructure. This architecture enables efficient, programmatic access to large datasets without the limitations of traditional database connections. To access the data, you need credentials provided by Datacore upon subscription: an endpoint URL, access key, and secret key.\nThe following class establishes the connection to Datacore’s storage system. The credentials are stored as environment variables for security, following best practices for credential management in research computing environments.\n\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass DatacoreConnection:\n    \"\"\"\n    Connection handler for Datacore's MinIO-based storage system.\n    \n    This class manages authentication and provides methods for\n    accessing financial datasets stored in Datacore's cloud infrastructure.\n    \n    Attributes\n    ----------\n    s3 : boto3.client\n        S3-compatible client for interacting with Datacore storage\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize connection using environment variables.\"\"\"\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n        \n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n    \n    def test_connection(self):\n        \"\"\"Verify connection by listing available buckets.\"\"\"\n        response = self.s3.list_buckets()\n        print(\"Connected successfully. Available buckets:\")\n        for bucket in response.get(\"Buckets\", []):\n            print(f\"  - {bucket['Name']}\")\n    \n    def list_objects(self, bucket_name, prefix=\"\"):\n        \"\"\"List objects in a bucket with optional prefix filter.\"\"\"\n        response = self.s3.list_objects_v2(\n            Bucket=bucket_name, \n            Prefix=prefix\n        )\n        return [obj[\"Key\"] for obj in response.get(\"Contents\", [])]\n    \n    def read_excel(self, bucket_name, key):\n        \"\"\"Read an Excel file from Datacore storage.\"\"\"\n        obj = self.s3.get_object(Bucket=bucket_name, Key=key)\n        return pd.read_excel(BytesIO(obj[\"Body\"].read()))\n    \n    def read_csv(self, bucket_name, key, **kwargs):\n        \"\"\"Read a CSV file from Datacore storage.\"\"\"\n        obj = self.s3.get_object(Bucket=bucket_name, Key=key)\n        return pd.read_csv(BytesIO(obj[\"Body\"].read()), **kwargs)\n\nWith the connection class defined, we can establish a connection and verify access to Datacore’s data repositories.\n\n# Initialize connection\nconn = DatacoreConnection()\nconn.test_connection()\n\n# Get bucket name from environment\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nConnected successfully. Available buckets:\n  - dsteam-data\n  - rawbctc",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#company-fundamentals-data",
    "href": "07_datacore_data.html#company-fundamentals-data",
    "title": "8  Datacore Data",
    "section": "8.5 Company Fundamentals Data",
    "text": "8.5 Company Fundamentals Data\nFirm accounting data are essential for portfolio analyses, factor construction, and valuation studies. Datacore hosts comprehensive fundamentals data for Vietnamese listed companies, including annual and quarterly financial statements prepared according to Vietnamese Accounting Standards (VAS).\n\n8.5.1 Understanding Vietnamese Financial Statements\nBefore processing the data, it is important to understand the structure of Vietnamese financial reports. Vietnamese companies follow VAS, which shares similarities with International Financial Reporting Standards (IFRS) but has notable differences:\n\nFiscal Year: Most Vietnamese companies use a calendar fiscal year ending December 31, though some companies (particularly in retail and agriculture) use different fiscal year-ends.\nReporting Frequency: Listed companies must publish quarterly financial statements within 20 days of quarter-end and annual audited statements within 90 days of fiscal year-end.\nIndustry-Specific Formats: Companies in banking, insurance, and securities sectors follow specialized reporting formats that differ from the standard industrial format.\nCurrency: All figures are reported in Vietnamese Dong (VND). Given the large nominal values (millions to trillions of VND), we often scale figures to millions or billions for readability.\n\n\n\n8.5.2 Downloading Fundamentals Data\nDatacore organizes fundamentals data in Excel files partitioned by time period for efficient access. We download and concatenate these files to create a comprehensive dataset spanning our sample period.\n\n# Define paths to fundamentals data files\nfundamentals_paths = [\n    \"fundamental_annual_1767674486317/fundamental_annual_1.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_2.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_3.xlsx\",\n]\n\n# Download and combine all files\nfundamentals_list = []\nfor path in fundamentals_paths:\n    df_temp = conn.read_excel(bucket_name, path)\n    fundamentals_list.append(df_temp)\n    print(f\"Downloaded: {path} ({len(df_temp):,} rows)\")\n\ndf_fundamentals_raw = pd.concat(fundamentals_list, ignore_index=True)\nprint(f\"\\nTotal observations: {len(df_fundamentals_raw):,}\")\n\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_1.xlsx (10,000 rows)\n\n\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_2.xlsx (10,000 rows)\n\n\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_3.xlsx (2,821 rows)\n\nTotal observations: 22,821\n\n\n\n\n8.5.3 Cleaning and Standardizing Fundamentals\nThe raw fundamentals data requires several cleaning steps to ensure consistency and usability. We standardize variable names, handle missing values, and create derived variables commonly used in asset pricing research.\n\ndef clean_fundamentals(df):\n    \"\"\"\n    Clean and standardize company fundamentals data.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Raw fundamentals data from Datacore\n    \n    Returns\n    -------\n    pd.DataFrame\n        Cleaned fundamentals with standardized column names\n    \"\"\"\n    df = df.copy()\n    \n    # Standardize identifiers\n    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\n    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n    \n    # Drop rows with missing identifiers\n    df = df.dropna(subset=[\"symbol\", \"year\"])\n    \n    # Define columns that should be numeric\n    numeric_columns = [\n        \"total_asset\", \"total_equity\", \"total_liabilities\",\n        \"total_current_asset\", \"total_current_liabilities\",\n        \"is_net_revenue\", \"is_cogs\", \"is_manage_expense\",\n        \"is_interest_expense\", \"is_eat\", \"is_net_business_profit\",\n        \"na_tax_deferred\", \"nl_tax_deferred\", \"e_preferred_stock\",\n        \"capex\", \"total_cfo\", \"ca_cce\", \"ca_total_inventory\",\n        \"ca_acc_receiv\", \"cfo_interest_expense\", \"basic_eps\",\n        \"is_shareholders_eat\", \"cl_loan\", \"cl_finlease\",\n        \"cl_due_long_debt\", \"nl_loan\", \"nl_finlease\",\n        \"is_cos_of_sales\", \"e_equity\"\n    ]\n    \n    for col in numeric_columns:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n    \n    # Handle duplicates: keep row with most non-missing values\n    df[\"_completeness\"] = df.notna().sum(axis=1)\n    df = (df\n        .sort_values([\"symbol\", \"year\", \"_completeness\"])\n        .drop_duplicates(subset=[\"symbol\", \"year\"], keep=\"last\")\n        .drop(columns=\"_completeness\")\n        .reset_index(drop=True)\n    )\n    \n    return df\n\ndf_fundamentals = clean_fundamentals(df_fundamentals_raw)\nprint(f\"After cleaning: {len(df_fundamentals):,} firm-year observations\")\nprint(f\"Unique firms: {df_fundamentals['symbol'].nunique():,}\")\n\nAfter cleaning: 21,232 firm-year observations\nUnique firms: 1,554\n\n\n\n\n8.5.4 Creating Standardized Variables\nTo facilitate comparison with international studies and ensure compatibility with standard asset pricing methodologies, we create variables following conventions established in the academic literature. We map Vietnamese financial statement items to their Compustat equivalents where possible.\n\ndef create_standard_variables(df):\n    \"\"\"\n    Create standardized financial variables for asset pricing research.\n    \n    This function maps Vietnamese financial statement items to standard\n    variable names used in the academic finance literature, following\n    conventions from Fama and French (1992, 1993, 2015).\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Cleaned fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with standardized variables added\n    \"\"\"\n    df = df.copy()\n    \n    # Fiscal date (assume December year-end)\n    df[\"datadate\"] = pd.to_datetime(df[\"year\"].astype(str) + \"-12-31\")\n    \n    # === Balance Sheet Items ===\n    df[\"at\"] = df[\"total_asset\"]                    # Total assets\n    df[\"lt\"] = df[\"total_liabilities\"]              # Total liabilities\n    df[\"seq\"] = df[\"total_equity\"]                  # Stockholders' equity\n    df[\"act\"] = df[\"total_current_asset\"]           # Current assets\n    df[\"lct\"] = df[\"total_current_liabilities\"]     # Current liabilities\n    \n    # Common equity (fallback to total equity if not available)\n    df[\"ceq\"] = df.get(\"e_equity\", df[\"seq\"])\n    \n    # === Deferred Taxes ===\n    df[\"txditc\"] = df.get(\"na_tax_deferred\", 0).fillna(0)  # Deferred tax assets\n    df[\"txdb\"] = df.get(\"nl_tax_deferred\", 0).fillna(0)    # Deferred tax liab.\n    df[\"itcb\"] = 0  # Investment tax credit (rare in Vietnam)\n    \n    # === Preferred Stock ===\n    pref = df.get(\"e_preferred_stock\", 0)\n    if isinstance(pref, pd.Series):\n        pref = pref.fillna(0)\n    df[\"pstk\"] = pref\n    df[\"pstkrv\"] = pref  # Redemption value\n    df[\"pstkl\"] = pref   # Liquidating value\n    \n    # === Income Statement Items ===\n    df[\"sale\"] = df[\"is_net_revenue\"]                        # Net sales/revenue\n    df[\"cogs\"] = df.get(\"is_cogs\", 0).fillna(0)              # Cost of goods sold\n    df[\"xsga\"] = df.get(\"is_manage_expense\", 0).fillna(0)    # SG&A expenses\n    df[\"xint\"] = df.get(\"is_interest_expense\", 0).fillna(0)  # Interest expense\n    df[\"ni\"] = df.get(\"is_eat\", np.nan)                      # Net income\n    df[\"oibdp\"] = df.get(\"is_net_business_profit\", np.nan)   # Operating income\n    \n    # === Cash Flow Items ===\n    df[\"oancf\"] = df.get(\"total_cfo\", np.nan)  # Operating cash flow\n    df[\"capx\"] = df.get(\"capex\", np.nan)       # Capital expenditures\n    \n    return df\n\ndf_fundamentals = create_standard_variables(df_fundamentals)\n\n\n\n8.5.5 Computing Book Equity and Profitability\nBook equity is a crucial variable for value investing strategies and the construction of HML (High Minus Low) factor portfolios. We follow the definition from Kenneth French’s data library, which accounts for deferred taxes and preferred stock.\n\ndef compute_book_equity(df):\n    \"\"\"\n    Compute book equity following Fama-French conventions.\n    \n    Book equity = Stockholders' equity \n                  + Deferred taxes and investment tax credit\n                  - Preferred stock\n    \n    Negative or zero book equity is set to missing, as book-to-market\n    ratios are undefined for such firms.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals with standardized variables\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with book equity (be) added\n    \"\"\"\n    df = df.copy()\n    \n    # Primary measure: stockholders' equity\n    # Fallback 1: common equity + preferred stock\n    # Fallback 2: total assets - total liabilities\n    seq_measure = (df[\"seq\"]\n        .combine_first(df[\"ceq\"] + df[\"pstk\"])\n        .combine_first(df[\"at\"] - df[\"lt\"])\n    )\n    \n    # Add deferred taxes\n    deferred_taxes = (df[\"txditc\"]\n        .combine_first(df[\"txdb\"] + df[\"itcb\"])\n        .fillna(0)\n    )\n    \n    # Subtract preferred stock (use redemption value as primary)\n    preferred = (df[\"pstkrv\"]\n        .combine_first(df[\"pstkl\"])\n        .combine_first(df[\"pstk\"])\n        .fillna(0)\n    )\n    \n    # Book equity calculation\n    df[\"be\"] = seq_measure + deferred_taxes - preferred\n    \n    # Set non-positive book equity to missing\n    df[\"be\"] = df[\"be\"].where(df[\"be\"] &gt; 0, np.nan)\n    \n    return df\n\ndf_fundamentals = compute_book_equity(df_fundamentals)\n\n# Summary statistics for book equity\nprint(\"Book Equity Summary Statistics (in million VND):\")\nprint(df_fundamentals[\"be\"].describe().round(2))\n\nBook Equity Summary Statistics (in million VND):\ncount    2.023500e+04\nmean     1.031884e+12\nstd      4.705269e+12\nmin      4.404402e+07\n25%      7.267610e+10\n50%      1.803885e+11\n75%      5.304653e+11\nmax      1.836314e+14\nName: be, dtype: float64\n\n\nOperating profitability, introduced by Fama and French (2015), measures a firm’s profits relative to its book equity. Firms with higher operating profitability tend to have higher expected returns.\n\ndef compute_profitability(df):\n    \"\"\"\n    Compute operating profitability following Fama-French (2015).\n    \n    Operating profitability = (Revenue - COGS - SG&A - Interest) / Book Equity\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals with book equity computed\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with operating profitability (op) added\n    \"\"\"\n    df = df.copy()\n    \n    # Operating profit before taxes\n    operating_profit = (\n        df[\"sale\"] \n        - df[\"cogs\"].fillna(0) \n        - df[\"xsga\"].fillna(0) \n        - df[\"xint\"].fillna(0)\n    )\n    \n    # Scale by book equity\n    df[\"op\"] = operating_profit / df[\"be\"]\n    \n    # Winsorize extreme values (outside 1st and 99th percentiles)\n    lower = df[\"op\"].quantile(0.01)\n    upper = df[\"op\"].quantile(0.99)\n    df[\"op\"] = df[\"op\"].clip(lower=lower, upper=upper)\n    \n    return df\n\ndf_fundamentals = compute_profitability(df_fundamentals)\n\n\n\n8.5.6 Computing Investment\nInvestment, measured as asset growth, captures firms’ investment behavior. Fama and French (2015) document that firms with high asset growth (aggressive investment) tend to have lower future returns.\n\ndef compute_investment(df):\n    \"\"\"\n    Compute investment (asset growth) following Fama-French (2015).\n    \n    Investment = (Total Assets_t / Total Assets_{t-1}) - 1\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with investment (inv) added\n    \"\"\"\n    df = df.copy()\n    \n    # Create lagged assets\n    df_lag = (df[[\"symbol\", \"year\", \"at\"]]\n        .assign(year=lambda x: x[\"year\"] + 1)\n        .rename(columns={\"at\": \"at_lag\"})\n    )\n    \n    # Merge lagged values\n    df = df.merge(df_lag, on=[\"symbol\", \"year\"], how=\"left\")\n    \n    # Compute investment (asset growth)\n    df[\"inv\"] = df[\"at\"] / df[\"at_lag\"] - 1\n    \n    # Set to missing if lagged assets non-positive\n    df[\"inv\"] = df[\"inv\"].where(df[\"at_lag\"] &gt; 0, np.nan)\n    \n    return df\n\ndf_fundamentals = compute_investment(df_fundamentals)\n\n\n\n8.5.7 Computing Total Debt\nIn Vietnamese financial statements, total liabilities include non-interest-bearing items such as accounts payable and tax payables. For leverage analysis, we compute total interest-bearing debt by aggregating loan and lease obligations.\n\ndef compute_total_debt(df):\n    \"\"\"\n    Compute total interest-bearing debt.\n    \n    Total Debt = Short-term loans + Finance leases (current)\n                 + Current portion of long-term debt\n                 + Long-term loans + Finance leases (non-current)\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with total_debt added\n    \"\"\"\n    df = df.copy()\n    \n    df[\"total_debt\"] = (\n        df.get(\"cl_loan\", 0).fillna(0) +           # Short-term bank loans\n        df.get(\"cl_finlease\", 0).fillna(0) +       # Current finance leases\n        df.get(\"cl_due_long_debt\", 0).fillna(0) +  # Current portion LT debt\n        df.get(\"nl_loan\", 0).fillna(0) +           # Long-term bank loans\n        df.get(\"nl_finlease\", 0).fillna(0)         # Non-current finance leases\n    )\n    \n    return df\n\ndf_fundamentals = compute_total_debt(df_fundamentals)\n\n\n\n8.5.8 Applying Filters and Final Preparation\nWe apply standard filters to ensure data quality: requiring positive assets, non-negative sales, and presence of core variables needed for portfolio construction.\n\n# Keep only observations with required variables\nrequired_vars = [\"at\", \"lt\", \"seq\", \"sale\"]\ncomp_vn = df_fundamentals.dropna(subset=required_vars)\n\n# Apply quality filters\ncomp_vn = comp_vn.query(\"at &gt; 0\")      # Positive assets\ncomp_vn = comp_vn.query(\"sale &gt;= 0\")   # Non-negative sales\n\n# Keep last observation per firm-year (in case of restatements)\ncomp_vn = (comp_vn\n    .sort_values(\"datadate\")\n    .groupby([\"symbol\", \"year\"])\n    .tail(1)\n    .reset_index(drop=True)\n)\n\n# Diagnostic summary\nprint(f\"Final sample: {len(comp_vn):,} firm-year observations\")\nprint(f\"Unique firms: {comp_vn['symbol'].nunique():,}\")\nprint(f\"Sample period: {comp_vn['year'].min()} - {comp_vn['year'].max()}\")\n\nFinal sample: 20,091 firm-year observations\nUnique firms: 1,502\nSample period: 1998 - 2023\n\n\n\n\n8.5.9 Storing Fundamentals Data\nWe store the prepared fundamentals data in our local SQLite database for use in subsequent chapters.\n\ncomp_vn.to_sql(\n    name=\"comp_vn\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Company fundamentals saved to database.\")\n\nCompany fundamentals saved to database.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#stock-price-data",
    "href": "07_datacore_data.html#stock-price-data",
    "title": "8  Datacore Data",
    "section": "8.6 Stock Price Data",
    "text": "8.6 Stock Price Data\nStock price data forms the foundation of return-based analyses in empirical finance. Datacore provides comprehensive historical price data for all securities traded on HOSE, HNX, and UPCoM, including adjusted prices that account for corporate actions.\n\n8.6.1 Downloading Price Data\nWe download the historical price data from Datacore’s storage system. The data includes daily observations with open, high, low, close prices, trading volume, and adjustment factors.\n\n# Download historical price data\nprices_raw = conn.read_csv(\n    bucket_name,\n    \"historycal_price/dataset_historical_price.csv\",\n    low_memory=False\n)\n\nprint(f\"Downloaded {len(prices_raw):,} daily price observations\")\nprint(f\"Date range: {prices_raw['date'].min()} to {prices_raw['date'].max()}\")\n\nDownloaded 4,307,791 daily price observations\nDate range: 2010-01-04 to 2025-05-12\n\n\n\n\n8.6.2 Processing Price Data\nWe clean the price data and compute adjusted prices that account for stock splits, stock dividends, and other corporate actions.\n\ndef process_price_data(df):\n    \"\"\"\n    Process raw price data from Datacore.\n    \"\"\"\n    df = df.copy()\n    \n    # Parse dates\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    \n    # Standardize column names\n    df = df.rename(columns={\n        \"open_price\": \"open\",\n        \"high_price\": \"high\",\n        \"low_price\": \"low\",\n        \"close_price\": \"close\",\n        \"vol_total\": \"volume\"\n    })\n    \n    # Compute adjusted close price\n    df[\"adjusted_close\"] = df[\"close\"] * df[\"adj_ratio\"]\n    \n    # Standardize symbol\n    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\n    \n    # Sort for return calculation\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Add year and month\n    df[\"year\"] = df[\"date\"].dt.year\n    df[\"month\"] = df[\"date\"].dt.month\n    \n    return df\n\nprices = process_price_data(prices_raw)\n\n\n\n8.6.3 Computing Shares Outstanding and Market Capitalization\nMarket capitalization is computed as the product of price and shares outstanding. Since Datacore provides earnings per share and net income, we can infer shares outstanding from these variables.\n\ndef compute_shares_outstanding(fundamentals_df):\n    \"\"\"\n    Compute shares outstanding from fundamentals.\n    \"\"\"\n    shares = fundamentals_df.copy()\n    shares[\"shrout\"] = shares[\"is_shareholders_eat\"] / shares[\"basic_eps\"]\n    shares = shares[[\"symbol\", \"year\", \"shrout\"]].dropna()\n    \n    return shares\n\nshares_outstanding = compute_shares_outstanding(df_fundamentals)\n\n\ndef add_market_cap(df, shares_df):\n    \"\"\"\n    Add market capitalization to price data.\n    \"\"\"\n    df = df.merge(shares_df, on=[\"symbol\", \"year\"], how=\"left\")\n    \n    # Compute market cap (in million VND)\n    df[\"mktcap\"] = (df[\"close\"] * df[\"shrout\"]) / 1_000_000\n    \n    # Set zero or negative market cap to missing\n    df[\"mktcap\"] = df[\"mktcap\"].where(df[\"mktcap\"] &gt; 0, np.nan)\n    \n    return df\n\nprices = add_market_cap(prices, shares_outstanding)\n\n\n\n8.6.4 Computing Returns and Excess Returns\nWe compute returns using adjusted closing prices to ensure returns correctly reflect total shareholder returns including dividends and corporate actions.\n\n8.6.4.1 Creating Daily Dataset\n\nSequential version\n\n\ndef create_daily_dataset(df, annual_rf=0.04):\n    \"\"\"\n    Create daily price dataset with returns and excess returns.\n    \"\"\"\n    df = df.copy()\n    \n    # Sort by symbol and date (critical for correct return calculation)\n    df = df.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Remove duplicate dates within each symbol (keep last observation)\n    df = df.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Compute daily returns\n    df[\"ret\"] = df.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    \n    # Cap extreme negative returns\n    df[\"ret\"] = df[\"ret\"].clip(lower=-0.99)\n    \n    # Daily risk-free rate (assuming 252 trading days)\n    df[\"risk_free\"] = annual_rf / 252\n    \n    # Excess returns\n    df[\"ret_excess\"] = df[\"ret\"] - df[\"risk_free\"]\n    df[\"ret_excess\"] = df[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    df[\"mktcap_lag\"] = df.groupby(\"symbol\")[\"mktcap\"].shift(1)\n    \n    return df\n\nprices_daily = create_daily_dataset(prices)\n\n\nParallel version\n\n\nfrom joblib import Parallel, delayed\nimport os\n\ndef process_daily_symbol(symbol_df, annual_rf=0.04):\n    \"\"\"\n    Process a single symbol's daily data.\n    \"\"\"\n    df = symbol_df.copy()\n    \n    # Sort by date (critical for correct return calculation)\n    df = df.sort_values(\"date\").reset_index(drop=True)\n    \n    # Remove duplicate dates (keep last observation if duplicates exist)\n    df = df.drop_duplicates(subset=[\"date\"], keep=\"last\")\n    \n    # Compute daily returns\n    df[\"ret\"] = df[\"adjusted_close\"].pct_change()\n\n    # Replace infinite values with NaN\n    df[\"ret\"] = df[\"ret\"].replace([np.inf, -np.inf], np.nan)\n    \n    # Cap extreme negative returns\n    df[\"ret\"] = df[\"ret\"].clip(lower=-0.99)\n    \n    # Daily risk-free rate\n    df[\"risk_free\"] = annual_rf / 252\n    \n    # Excess returns\n    df[\"ret_excess\"] = df[\"ret\"] - df[\"risk_free\"]\n    df[\"ret_excess\"] = df[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    df[\"mktcap_lag\"] = df[\"mktcap\"].shift(1)\n    \n    return df\n\ndef create_daily_dataset_parallel(df, annual_rf=0.04):\n    \"\"\"\n    Create daily price dataset using parallel processing.\n    \"\"\"\n    # Ensure data is sorted before splitting\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Split by symbol\n    symbol_groups = [group for _, group in df.groupby(\"symbol\")]\n    \n    n_jobs = max(1, os.cpu_count() - 1)\n    print(f\"Processing {len(symbol_groups):,} symbols using {n_jobs} cores...\")\n    \n    results = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(process_daily_symbol)(group, annual_rf) \n        for group in symbol_groups\n    )\n    \n    return pd.concat(results, ignore_index=True)\n\nprices_daily = create_daily_dataset_parallel(prices)\n\n# Quick validation\nprint(\"\\nValidation checks:\")\nprint(f\"Any duplicate (symbol, date): {prices_daily.duplicated(subset=['symbol', 'date']).sum()}\")\nprint(f\"Sample of non-zero returns:\")\nprint(prices_daily[prices_daily[\"ret\"] != 0][[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(10))\n\n\nprices_daily.query(\"symbol == 'FPT'\")[[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(3)\n\nProcessing 1,837 symbols using 23 cores...\n\n\n\nValidation checks:\nAny duplicate (symbol, date): 0\nSample of non-zero returns:\n   symbol       date  adjusted_close       ret\n0     A32 2018-10-23       44.574418       NaN\n27    A32 2018-11-29       55.072640  0.235521\n30    A32 2018-12-04       48.188560 -0.125000\n43    A32 2018-12-21       51.974804  0.078571\n49    A32 2019-01-02       55.072640  0.059603\n53    A32 2019-01-08       50.030370 -0.091557\n74    A32 2019-02-13       44.289180 -0.114754\n75    A32 2019-02-14       41.008500 -0.074074\n78    A32 2019-02-19       36.087480 -0.120000\n91    A32 2019-03-08       41.336568  0.145455\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n1146076\nFPT\n2010-01-04\n1170.9885\nNaN\n\n\n1146077\nFPT\n2010-01-05\n1170.9885\n0.000000\n\n\n1146078\nFPT\n2010-01-06\n1149.6978\n-0.018182\n\n\n\n\n\n\n\n\n# Select columns\ndaily_columns = [\n    \"symbol\", \"date\", \"year\", \"month\",\n    \"open\", \"high\", \"low\", \"close\", \"volume\",\n    \"adjusted_close\", \"shrout\", \"mktcap\", \"mktcap_lag\",\n    \"ret\", \"risk_free\", \"ret_excess\"\n]\nprices_daily = prices_daily[daily_columns]\n\n# Remove observations with missing essential variables\nprices_daily = prices_daily.dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n\nprint(\"Daily Return Summary Statistics:\")\nprint(prices_daily[\"ret\"].describe().round(4))\nprint(f\"\\nFinal daily sample: {len(prices_daily):,} observations\")\n\nDaily Return Summary Statistics:\ncount    3.462157e+06\nmean     3.000000e-04\nstd      4.480000e-02\nmin     -9.900000e-01\n25%     -4.900000e-03\n50%      0.000000e+00\n75%      4.000000e-03\nmax      3.250000e+01\nName: ret, dtype: float64\n\nFinal daily sample: 3,462,157 observations\n\n\n\n\n8.6.4.2 Creating Monthly Dataset\nFor monthly returns, we compute returns directly from month-end adjusted prices rather than compounding daily returns. This avoids compounding errors from missing days and is the standard approach in empirical finance.\n\nSequential version\n\n\ndef create_monthly_dataset(df, annual_rf=0.04):\n    \"\"\"\n    Create monthly price dataset with returns computed from \n    month-end to month-end adjusted prices.\n    \"\"\"\n    df = df.copy()\n    \n    # Sort by symbol and date (critical for correct return calculation)\n    df = df.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Remove duplicate dates within each symbol (keep last observation)\n    df = df.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Get month-end observations\n    monthly = (df\n        .groupby(\"symbol\")\n        .resample(\"ME\", on=\"date\")\n        .agg({\n            \"open\": \"first\",           # First day open\n            \"high\": \"max\",             # Monthly high\n            \"low\": \"min\",              # Monthly low\n            \"close\": \"last\",           # Last day close\n            \"volume\": \"sum\",           # Total monthly volume\n            \"adjusted_close\": \"last\",  # Month-end adjusted price\n            \"shrout\": \"last\",          # Month-end shares outstanding\n            \"mktcap\": \"last\",          # Month-end market cap\n            \"year\": \"last\",\n            \"month\": \"last\"\n        })\n        .reset_index()\n    )\n    \n    # Remove duplicate (symbol, date) after resampling (safety check)\n    monthly = monthly.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Sort again after resampling\n    monthly = monthly.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Compute monthly returns from month-end to month-end adjusted prices\n    monthly[\"ret\"] = monthly.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    \n    # Cap extreme returns\n    monthly[\"ret\"] = monthly[\"ret\"].clip(lower=-0.99)\n    \n    # Monthly risk-free rate\n    monthly[\"risk_free\"] = annual_rf / 12\n    \n    # Excess returns\n    monthly[\"ret_excess\"] = monthly[\"ret\"] - monthly[\"risk_free\"]\n    monthly[\"ret_excess\"] = monthly[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap for portfolio weighting\n    monthly[\"mktcap_lag\"] = monthly.groupby(\"symbol\")[\"mktcap\"].shift(1)\n    \n    return monthly\n\nprices_monthly = create_monthly_dataset(prices)\n\n\nParallel version\n\n\nfrom joblib import Parallel, delayed\nimport os\n\ndef process_monthly_symbol(symbol_df, annual_rf=0.04):\n    \"\"\"\n    Process a single symbol's data to monthly frequency.\n    \"\"\"\n    df = symbol_df.copy()\n    \n    # Sort by date (critical for correct return calculation)\n    df = df.sort_values(\"date\").reset_index(drop=True)\n    \n    # Remove duplicate dates (keep last observation if duplicates exist)\n    df = df.drop_duplicates(subset=[\"date\"], keep=\"last\")\n    \n    # Set date as index for resampling\n    df = df.set_index(\"date\")\n    \n    # Resample to monthly\n    monthly = df.resample(\"ME\").agg({\n        \"symbol\": \"last\",\n        \"open\": \"first\",\n        \"high\": \"max\",\n        \"low\": \"min\",\n        \"close\": \"last\",\n        \"volume\": \"sum\",\n        \"adjusted_close\": \"last\",\n        \"shrout\": \"last\",\n        \"mktcap\": \"last\",\n        \"year\": \"last\",\n        \"month\": \"last\"\n    }).reset_index()\n    \n    # Remove rows where symbol is NaN (months with no trading)\n    monthly = monthly.dropna(subset=[\"symbol\"])\n    \n    # Sort by date\n    monthly = monthly.sort_values(\"date\").reset_index(drop=True)\n    \n    # Compute monthly returns\n    monthly[\"ret\"] = monthly[\"adjusted_close\"].pct_change()\n    \n    # Replace infinite values with NaN\n    monthly[\"ret\"] = monthly[\"ret\"].replace([np.inf, -np.inf], np.nan)\n    \n    # Cap extreme returns\n    monthly[\"ret\"] = monthly[\"ret\"].clip(lower=-0.99)\n    \n    # Monthly risk-free rate\n    monthly[\"risk_free\"] = annual_rf / 12\n    \n    # Excess returns\n    monthly[\"ret_excess\"] = monthly[\"ret\"] - monthly[\"risk_free\"]\n    monthly[\"ret_excess\"] = monthly[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    monthly[\"mktcap_lag\"] = monthly[\"mktcap\"].shift(1)\n    \n    return monthly\n\ndef create_monthly_dataset_parallel(df, annual_rf=0.04):\n    \"\"\"\n    Create monthly price dataset using parallel processing.\n    \"\"\"\n    # Ensure data is sorted before splitting\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Split by symbol\n    symbol_groups = [group for _, group in df.groupby(\"symbol\")]\n    \n    n_jobs = max(1, os.cpu_count() - 1)\n    print(f\"Processing {len(symbol_groups):,} symbols using {n_jobs} cores...\")\n    \n    results = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(process_monthly_symbol)(group, annual_rf) \n        for group in symbol_groups\n    )\n    \n    return pd.concat(results, ignore_index=True)\n\nprices_monthly = create_monthly_dataset_parallel(prices)\n\n# Validation checks\nprint(\"\\nValidation checks:\")\nprint(f\"Any duplicate (symbol, date): {prices_monthly.duplicated(subset=['symbol', 'date']).sum()}\")\nprint(f\"\\nSample of non-zero returns:\")\nprint(prices_monthly[prices_monthly[\"ret\"] != 0][[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(10))\n\nprices_monthly.query(\"symbol == 'FPT'\")[[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(3)\n\nProcessing 1,837 symbols using 23 cores...\n\n\n\nValidation checks:\nAny duplicate (symbol, date): 0\n\nSample of non-zero returns:\n   symbol       date  adjusted_close       ret\n0     A32 2018-10-31       44.574418       NaN\n1     A32 2018-11-30       55.072640  0.235521\n2     A32 2018-12-31       51.974804 -0.056250\n3     A32 2019-01-31       50.030370 -0.037411\n4     A32 2019-02-28       36.087480 -0.278689\n5     A32 2019-03-31       41.828670  0.159091\n7     A32 2019-05-31       43.304976  0.035294\n8     A32 2019-06-30       35.929125 -0.170323\n9     A32 2019-07-31       37.525975  0.044444\n10    A32 2019-08-31       38.324400  0.021277\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n55963\nFPT\n2010-01-31\n1092.9226\nNaN\n\n\n55964\nFPT\n2010-02-28\n1107.1164\n0.012987\n\n\n55965\nFPT\n2010-03-31\n1185.1823\n0.070513\n\n\n\n\n\n\n\n\n# Select columns (same structure as daily)\nmonthly_columns = [\n    \"symbol\", \"date\", \"year\", \"month\",\n    \"open\", \"high\", \"low\", \"close\", \"volume\",\n    \"adjusted_close\", \"shrout\", \"mktcap\", \"mktcap_lag\",\n    \"ret\", \"risk_free\", \"ret_excess\"\n]\nprices_monthly = prices_monthly[monthly_columns]\n\n# Remove observations with missing essential variables\nprices_monthly = prices_monthly.dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n\nprint(\"Monthly Return Summary Statistics:\")\nprint(prices_monthly[\"ret\"].describe().round(4))\nprint(f\"\\nFinal monthly sample: {len(prices_monthly):,} observations\")\n\nMonthly Return Summary Statistics:\ncount    165499.0000\nmean          0.0042\nstd           0.1862\nmin          -0.9900\n25%          -0.0703\n50%           0.0000\n75%           0.0553\nmax          12.7500\nName: ret, dtype: float64\n\nFinal monthly sample: 165,499 observations\n\n\n\n\n\n8.6.5 Storing Price Data\n\nprices_daily.to_sql(\n    name=\"prices_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(\"Daily price data saved to database.\")\n\nprices_monthly.to_sql(\n    name=\"prices_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(\"Monthly price data saved to database.\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#descriptive-statistics",
    "href": "07_datacore_data.html#descriptive-statistics",
    "title": "8  Datacore Data",
    "section": "8.7 Descriptive Statistics",
    "text": "8.7 Descriptive Statistics\nBefore proceeding to asset pricing analyses, we examine the characteristics of our sample to understand the Vietnamese equity market’s evolution and composition.\n\n8.7.1 Market Evolution Over Time\nWe first examine how the number of listed securities has grown over time.\n\nsecurities_over_time = (prices_monthly\n    .groupby(\"date\")\n    .agg(\n        n_securities=(\"symbol\", \"nunique\"),\n        total_mktcap=(\"mktcap\", \"sum\")\n    )\n    .reset_index()\n)\n\n\nsecurities_figure = (\n    ggplot(securities_over_time, aes(x=\"date\", y=\"n_securities\"))\n    + geom_line(color=\"steelblue\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Number of Securities\",\n        title=\"Growth of Vietnamese Stock Market\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + scale_y_continuous(labels=comma_format())\n    + theme_minimal()\n)\nsecurities_figure.show()\n\n\n\n\n\n\n\nFigure 8.1: The figure shows the monthly number of securities in the Vietnamese stock market sample.\n\n\n\n\n\n\n\n8.7.2 Market Capitalization Evolution\nThe aggregate market capitalization reflects the overall size and development of the Vietnamese equity market.\n\nmktcap_figure = (\n    ggplot(securities_over_time, aes(x=\"date\", y=\"total_mktcap / 1000\"))\n    + geom_line(color=\"darkgreen\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Market Cap (Trillion VND)\",\n        title=\"Total Market Capitalization of Vietnamese Equities\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + scale_y_continuous(labels=comma_format())\n    + theme_minimal()\n)\nmktcap_figure.show()\n\n\n\n\n\n\n\nFigure 8.2: The figure shows the total market capitalization of Vietnamese listed companies over time.\n\n\n\n\n\n\n\n8.7.3 Return Distribution\nUnderstanding the distribution of monthly returns helps identify potential data quality issues and characterize market risk.\n\nreturn_distribution = (\n    ggplot(prices_monthly, aes(x=\"ret_excess\"))\n    + geom_histogram(\n        binwidth=0.02, \n        fill=\"steelblue\", \n        color=\"white\",\n        alpha=0.7\n    )\n    + labs(\n        x=\"Monthly Excess Return\",\n        y=\"Frequency\",\n        title=\"Distribution of Monthly Excess Returns\"\n    )\n    + scale_x_continuous(limits=(-0.5, 0.5))\n    + theme_minimal()\n)\nreturn_distribution.show()\n\n\n\n\n\n\n\nFigure 8.3: Distribution of monthly excess returns for Vietnamese stocks.\n\n\n\n\n\n\n\n8.7.4 Coverage of Book Equity\nBook equity is essential for constructing value portfolios. We examine what fraction of our sample has book equity data available over time.\n\n# Merge prices with fundamentals\ncoverage_data = (prices_monthly\n    .assign(year=lambda x: x[\"date\"].dt.year)\n    .groupby([\"symbol\", \"year\"])\n    .tail(1)\n    .merge(comp_vn[[\"symbol\", \"year\", \"be\"]], \n           on=[\"symbol\", \"year\"], \n           how=\"left\")\n)\n\n# Compute coverage by year\nbe_coverage = (coverage_data\n    .groupby(\"year\")\n    .apply(lambda x: pd.Series({\n        \"share_with_be\": x[\"be\"].notna().mean()\n    }))\n    .reset_index()\n)\n\ncoverage_figure = (\n    ggplot(be_coverage, aes(x=\"year\", y=\"share_with_be\"))\n    + geom_line(color=\"darkorange\", size=1)\n    + geom_point(color=\"darkorange\", size=2)\n    + labs(\n        x=\"Year\",\n        y=\"Share with Book Equity\",\n        title=\"Coverage of Book Equity Data\"\n    )\n    + scale_y_continuous(labels=percent_format(), limits=(0, 1))\n    + theme_minimal()\n)\ncoverage_figure.show()\n\n\n\n\n\n\n\nFigure 8.4: Share of securities with available book equity data by year.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#merging-stock-and-fundamental-data",
    "href": "07_datacore_data.html#merging-stock-and-fundamental-data",
    "title": "8  Datacore Data",
    "section": "8.8 Merging Stock and Fundamental Data",
    "text": "8.8 Merging Stock and Fundamental Data\nThe final step links price data with fundamental data using the stock symbol as the common identifier. This merged dataset forms the basis for constructing portfolios sorted on firm characteristics.\n\n# Example: Create merged dataset for end-of-June each year\nmerged_data = (prices_monthly\n    .query(\"month == 6\")\n    .merge(\n        comp_vn[[\"symbol\", \"year\", \"be\", \"op\", \"inv\", \"at\"]],\n        on=[\"symbol\", \"year\"],\n        how=\"left\",\n        suffixes=(\"\", \"_fundamental\")\n    )\n)\n\n# Convert BE from VND to BILLION VND\nmerged_data[\"be\"] = merged_data[\"be\"] / 1e9\n\n# Compute book-to-market ratio\nmerged_data[\"bm\"] = merged_data[\"be\"] / merged_data[\"mktcap\"]\n\nmerged_data.loc[\n    (merged_data[\"bm\"] &lt;= 0) |\n    (merged_data[\"bm\"] &gt; 20),\n    \"bm\"\n] = pd.NA\n\n\nmerged_data[\"bm\"].describe(percentiles=[.01, .1, .5, .9, .99])\n\nprint(f\"Merged observations: {len(merged_data):,}\")\nprint(f\"With book-to-market: {merged_data['bm'].notna().sum():,}\")\nmerged_data.head(3)\nmerged_data.describe()\nmerged_data\n\nMerged observations: 13,756\nWith book-to-market: 12,859\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nyear\nmonth\nopen\nhigh\nlow\nclose\nvolume\nadjusted_close\n...\nmktcap\nmktcap_lag\nret\nrisk_free\nret_excess\nbe\nop\ninv\nat\nbm\n\n\n\n\n0\nA32\n2019-06-30\n2019.0\n6.0\n26.4\n26.4\n21.0\n22.5\n3700\n35.929125\n...\n153.000\n179.52\n-0.170323\n0.003333\n-0.173657\n223.612748\n0.232362\n-0.072329\n4.349303e+11\n1.461521\n\n\n1\nA32\n2020-06-30\n2020.0\n6.0\n25.0\n26.3\n24.5\n26.3\n7500\n38.811173\n...\n178.840\n187.00\n-0.067977\n0.003333\n-0.071311\n242.216943\n0.195565\n0.122698\n4.882955e+11\n1.354378\n\n\n2\nA32\n2021-06-30\n2021.0\n6.0\n30.2\n37.0\n29.5\n32.0\n78400\n45.363520\n...\n217.600\n214.20\n0.015873\n0.003333\n0.012540\n238.385190\n0.157723\n0.081581\n5.281309e+11\n1.095520\n\n\n3\nA32\n2022-06-30\n2022.0\n6.0\n30.9\n35.5\n25.0\n35.3\n15200\n47.503210\n...\n240.040\n210.12\n0.142395\n0.003333\n0.139061\n215.399735\n0.172085\n0.036584\n5.474523e+11\n0.897349\n\n\n4\nA32\n2023-06-30\n2023.0\n6.0\n30.1\n33.5\n29.2\n29.4\n2400\n35.064204\n...\n199.920\n204.68\n-0.023256\n0.003333\n-0.026589\n222.024135\n0.174658\n-0.076752\n5.054342e+11\n1.110565\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13751\nYTC\n2019-06-30\n2019.0\n6.0\n70.0\n79.9\n70.0\n79.9\n38900\n171.451817\n...\n246.092\n215.60\n0.141429\n0.003333\n0.138095\n59.901389\n0.738190\n-0.021758\n7.521980e+11\n0.243411\n\n\n13752\nYTC\n2020-06-30\n2020.0\n6.0\n88.5\n88.5\n77.0\n87.0\n150640\n180.966960\n...\n267.960\n272.58\n-0.016949\n0.003333\n-0.020282\n13.459082\n-0.458548\n0.323501\n9.955348e+11\n0.050228\n\n\n13753\nYTC\n2021-06-30\n2021.0\n6.0\n76.0\n115.5\n61.0\n61.0\n34100\n126.884880\n...\n187.880\n234.08\n-0.197368\n0.003333\n-0.200702\n21.746595\n0.539521\n-0.215694\n7.808035e+11\n0.115747\n\n\n13754\nYTC\n2022-06-30\n2022.0\n6.0\n68.0\n68.0\n65.0\n65.5\n200\n136.245240\n...\n201.740\n209.44\n-0.036765\n0.003333\n-0.040098\n32.403055\n0.483088\n0.182911\n9.236206e+11\n0.160618\n\n\n13755\nYTC\n2023-06-30\n2023.0\n6.0\n59.0\n59.0\n59.0\n59.0\n49545\n122.724720\n...\n181.720\n181.72\n0.000000\n0.003333\n-0.003333\n38.976624\n0.450157\n0.017930\n9.401815e+11\n0.214487\n\n\n\n\n13756 rows × 21 columns\n\n\n\n\nfrom plotnine import *\nimport numpy as np\n\nbm_plot_data = (\n    merged_data[[\"bm\"]]\n      .dropna()\n      .assign(bm_plot=lambda x: x[\"bm\"].clip(upper=10))\n)\n\n(\n    ggplot(bm_plot_data, aes(x=\"bm_plot\")) +\n    geom_histogram(bins=80) +\n    labs(\n        title=\"Distribution of Book to Market Ratios\",\n        x=\"Book to Market (capped at 10 for plotting)\",\n        y=\"Number of firms\"\n    ) +\n    theme_minimal()\n)\n\n\n\n\n\n\n\n\n\nsize_plot_data = (\n    merged_data[[\"mktcap_lag\"]]\n      .dropna()\n      .assign(log_size=lambda x: np.log(x[\"mktcap_lag\"]))\n)\n\n(\n    ggplot(size_plot_data, aes(x=\"log_size\")) +\n    geom_histogram(bins=80) +\n    labs(\n        title=\"Distribution of Log Market Capitalization\",\n        x=\"Log Market Cap\",\n        y=\"Number of firms\"\n    ) +\n    theme_minimal()\n)\n\n\n\n\n\n\n\n\n\nscatter_data = (\n    merged_data[[\"be\", \"mktcap_lag\"]]\n      .dropna()\n      .assign(\n          log_be=lambda x: np.log(x[\"be\"]),\n          log_me=lambda x: np.log(x[\"mktcap_lag\"])\n      )\n)\n\n(\n    ggplot(scatter_data, aes(x=\"log_me\", y=\"log_be\")) +\n    geom_point(alpha=0.2) +\n    labs(\n        title=\"Log Book Equity vs Log Market Equity\",\n        x=\"Log Market Cap\",\n        y=\"Log Book Equity\"\n    ) +\n    theme_minimal()\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "07_datacore_data.html#key-takeaways",
    "href": "07_datacore_data.html#key-takeaways",
    "title": "8  Datacore Data",
    "section": "8.9 Key Takeaways",
    "text": "8.9 Key Takeaways\n\nDatacore provides unified access to Vietnamese financial data through a modern cloud-based infrastructure, eliminating the need to aggregate data from multiple fragmented sources.\nCompany fundamentals from Datacore include comprehensive balance sheet, income statement, and cash flow data prepared according to Vietnamese Accounting Standards, which we map to standard variables used in international research.\nBook equity computation follows the Fama-French methodology, accounting for deferred taxes and preferred stock to ensure comparability with US-based studies.\nStock price data includes adjustment factors for corporate actions, enabling accurate return calculations over long horizons.\nMonthly frequency is standard for asset pricing research, reducing noise while maintaining sufficient observations for statistical inference.\nRisk-free rate approximation uses Vietnamese government bond yields as a proxy, given the absence of a standardized short-term rate series comparable to US Treasury bills.\nData quality validation through descriptive statistics and visualization helps identify potential issues before conducting formal analyses.\nBatch processing enables efficient handling of large daily datasets that would otherwise exceed memory constraints.\n\n\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Datacore Data</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html",
    "href": "08_beta_estimation.html",
    "title": "9  Beta Estimation",
    "section": "",
    "text": "9.1 Theoretical Foundation\nThis chapter introduces one of the most fundamental concepts in financial economics: the exposure of an individual stock to systematic market risk. According to the Capital Asset Pricing Model (CAPM) developed by Sharpe (1964), Lintner (1965), and Mossin (1966), cross-sectional variation in expected asset returns should be determined by the covariance between an asset’s excess return and the excess return on the market portfolio. The regression coefficient that captures this relationship (commonly known as market beta) serves as the cornerstone of modern portfolio theory and remains widely used in practice for cost of capital estimation, performance attribution, and risk management.\nIn this chapter, we develop a complete framework for estimating market betas for Vietnamese stocks. We begin with a conceptual overview of the CAPM and its empirical implementation. We then demonstrate beta estimation using ordinary least squares regression, first for individual stocks and then scaled to the entire market using rolling-window estimation. To handle the computational demands of estimating betas for hundreds of stocks across many time periods, we introduce parallelization techniques that dramatically reduce processing time. Finally, we compare beta estimates derived from monthly versus daily returns and examine how betas vary across industries and over time in the Vietnamese market.\nThe chapter leverages several important computational concepts that extend beyond beta estimation itself. Rolling-window estimation is a technique applicable to any time-varying parameter, while parallelization provides a general solution for computationally intensive tasks that can be divided into independent subtasks.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#theoretical-foundation",
    "href": "08_beta_estimation.html#theoretical-foundation",
    "title": "9  Beta Estimation",
    "section": "",
    "text": "9.1.1 The Capital Asset Pricing Model\nThe CAPM provides a theoretical framework linking expected returns to systematic risk. Under the model’s assumptions—including mean-variance optimizing investors, homogeneous expectations, and frictionless markets—the expected excess return on any asset \\(i\\) is proportional to its covariance with the market portfolio:\n\\[\nE[r_i - r_f] = \\beta_i \\cdot E[r_m - r_f]\n\\]\nwhere \\(r_i\\) is the return on asset \\(i\\), \\(r_f\\) is the risk-free rate, \\(r_m\\) is the return on the market portfolio, and \\(\\beta_i\\) is defined as:\n\\[\n\\beta_i = \\frac{\\text{Cov}(r_i, r_m)}{\\text{Var}(r_m)}\n\\]\nThe market beta \\(\\beta_i\\) measures the sensitivity of asset \\(i\\)’s returns to market movements. A beta greater than one indicates the asset amplifies market movements, while a beta less than one indicates dampened sensitivity. A beta of zero would imply no systematic risk exposure, leaving only idiosyncratic risk that can be diversified away.\n\n\n9.1.2 Empirical Implementation\nIn practice, we estimate beta by regressing excess stock returns on excess market returns:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i(r_{m,t} - r_{f,t}) + \\varepsilon_{i,t}\n\\tag{9.1}\\]\nwhere \\(\\alpha_i\\) represents abnormal return (Jensen’s alpha), \\(\\beta_i\\) is the market beta we seek to estimate, and \\(\\varepsilon_{i,t}\\) is the idiosyncratic error term. Under the CAPM, \\(\\alpha_i\\) should equal zero for all assets—any non-zero alpha represents a deviation from the model’s predictions.\nSeveral practical considerations affect beta estimation:\n\nEstimation Window: Longer windows provide more observations and thus more precise estimates, but may include outdated information if betas change over time. Common choices range from 36 to 60 months for monthly data.\nReturn Frequency: Monthly returns reduce noise but provide fewer observations. Daily returns offer more data points but may introduce microstructure effects and non-synchronous trading biases.\nMarket Proxy: The theoretical market portfolio includes all assets, but in practice we use a broad equity index. For Vietnam, we use the value-weighted market return constructed from our stock universe.\nMinimum Observations: Requiring a minimum number of observations (e.g., 48 out of 60 months) helps avoid unreliable estimates from sparse data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#setting-up-the-environment",
    "href": "08_beta_estimation.html#setting-up-the-environment",
    "title": "9  Beta Estimation",
    "section": "9.2 Setting Up the Environment",
    "text": "9.2 Setting Up the Environment\nWe begin by loading the necessary Python packages. The core packages handle data manipulation, statistical modeling, and database operations. We also import parallelization tools that will be essential when scaling our estimation to the full market.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nfrom scipy.stats.mstats import winsorize\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\nfrom joblib import Parallel, delayed, cpu_count\nfrom dateutil.relativedelta import relativedelta\n\nWe connect to our SQLite database containing the processed Vietnamese financial data from previous chapters.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#loading-and-preparing-data",
    "href": "08_beta_estimation.html#loading-and-preparing-data",
    "title": "9  Beta Estimation",
    "section": "9.3 Loading and Preparing Data",
    "text": "9.3 Loading and Preparing Data\n\n9.3.1 Stock Returns Data\nWe load the monthly stock returns data prepared in the Datacore chapter. The data includes excess returns (returns minus the risk-free rate) for all Vietnamese listed stocks.\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess \n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Add year for merging with fundamentals\nprices_monthly[\"year\"] = prices_monthly[\"date\"].dt.year\n\nprint(f\"Loaded {len(prices_monthly):,} monthly observations\")\nprint(f\"Covering {prices_monthly['symbol'].nunique():,} unique stocks\")\nprint(f\"Date range: {prices_monthly['date'].min():%Y-%m} to {prices_monthly['date'].max():%Y-%m}\")\n\nLoaded 209,495 monthly observations\nCovering 1,837 unique stocks\nDate range: 2010-01 to 2025-05\n\n\n\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess \n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n\n\n9.3.2 Company Information\nWe load company information to enable industry-level analysis of beta estimates.\n\ncomp_vn = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, datadate, icb_name_vi \n        FROM comp_vn\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\n# Extract year for merging\ncomp_vn[\"year\"] = comp_vn[\"datadate\"].dt.year\n\nprint(f\"Company data: {comp_vn['symbol'].nunique():,} firms\")\n\nCompany data: 1,502 firms\n\n\n\n\n9.3.3 Market Excess Returns\nFor the market portfolio proxy, we use the value-weighted market excess return. If you have constructed Fama-French factors in a previous chapter, load them here. Otherwise, we can construct a simple market return from our stock data.\n\n# Option 1: Load pre-computed market factor\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Option 2: Construct market return from stock data (if factors not available)\n# This computes the value-weighted average return across all stocks\ndef compute_market_return(prices_df):\n    \"\"\"\n    Compute value-weighted market return from individual stock returns.\n    \n    Parameters\n    ----------\n    prices_df : pd.DataFrame\n        Stock returns with mktcap_lag for weighting\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly market excess returns\n    \"\"\"\n    market_return = (prices_df\n        .groupby(\"date\")\n        .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n        .reset_index(name=\"mkt_excess\")\n    )\n    return market_return\n\n\n\n9.3.4 Merging Datasets\nWe combine the stock returns with market returns and company information to create our estimation dataset.\n\n# Merge stock returns with market returns\nprices_monthly = prices_monthly.merge(\n    factors_ff3_monthly, \n    on=\"date\", \n    how=\"left\"\n)\n\n# Merge with company information for industry classification\nprices_monthly = prices_monthly.merge(\n    comp_vn[[\"symbol\", \"year\", \"icb_name_vi\"]], \n    on=[\"symbol\", \"year\"], \n    how=\"left\"\n)\n\n# Remove observations with missing data\nprices_monthly = prices_monthly.dropna(subset=[\"ret_excess\", \"mkt_excess\"])\n\nprint(f\"Final estimation sample: {len(prices_monthly):,} observations\")\n\nFinal estimation sample: 169,983 observations\n\n\n\n\n9.3.5 Handling Outliers\nExtreme returns can unduly influence regression estimates. We apply winsorization to limit the impact of outliers while preserving the general distribution of returns. Winsorization at the 1% level replaces values below the 1st percentile with the 1st percentile value, and values above the 99th percentile with the 99th percentile value.\n\ndef winsorize_returns(df, columns, limits=(0.01, 0.01)):\n    \"\"\"\n    Apply winsorization to return columns to limit outlier influence.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing return columns\n    columns : list\n        Column names to winsorize\n    limits : tuple\n        Lower and upper percentile limits for winsorization\n        \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with winsorized columns\n    \"\"\"\n    df = df.copy()\n    for col in columns:\n        df[col] = winsorize(df[col], limits=limits)\n    return df\n\nprices_monthly = winsorize_returns(\n    prices_monthly, \n    columns=[\"ret_excess\", \"mkt_excess\"],\n    limits=(0.01, 0.01)\n)\n\nprint(\"Return distributions after winsorization:\")\nprint(prices_monthly[[\"ret_excess\", \"mkt_excess\"]].describe().round(4))\n\nReturn distributions after winsorization:\n        ret_excess   mkt_excess\ncount  169983.0000  169983.0000\nmean        0.0011      -0.0102\nstd         0.1548       0.0579\nmin        -0.4078      -0.1794\n25%        -0.0700      -0.0384\n50%        -0.0033      -0.0084\n75%         0.0531       0.0219\nmax         0.6117       0.1221",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#estimating-beta-for-individual-stocks",
    "href": "08_beta_estimation.html#estimating-beta-for-individual-stocks",
    "title": "9  Beta Estimation",
    "section": "9.4 Estimating Beta for Individual Stocks",
    "text": "9.4 Estimating Beta for Individual Stocks\n\n9.4.1 Single Stock Example\nBefore scaling to the full market, we demonstrate beta estimation for a single well-known Vietnamese stock. We use Vingroup (VIC), one of the largest conglomerates in Vietnam with significant exposure to real estate, retail, and automotive sectors.\n\n# Filter data for Vingroup\nvic_data = prices_monthly.query(\"symbol == 'VIC'\").copy()\n\nprint(f\"VIC observations: {len(vic_data)}\")\nprint(f\"Date range: {vic_data['date'].min():%Y-%m} to {vic_data['date'].max():%Y-%m}\")\n\nVIC observations: 150\nDate range: 2011-07 to 2023-12\n\n\nWe estimate the CAPM regression using ordinary least squares via the statsmodels package. The formula interface provides a convenient way to specify regression models.\n\n# Estimate CAPM for Vingroup\nmodel_vic = smf.ols(\n    formula=\"ret_excess ~ mkt_excess\",\n    data=vic_data\n).fit()\n\n# Display regression results\nprint(model_vic.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             ret_excess   R-squared:                       0.153\nModel:                            OLS   Adj. R-squared:                  0.147\nMethod:                 Least Squares   F-statistic:                     26.67\nDate:                Wed, 04 Feb 2026   Prob (F-statistic):           7.66e-07\nTime:                        10:19:28   Log-Likelihood:                 131.96\nNo. Observations:                 150   AIC:                            -259.9\nDf Residuals:                     148   BIC:                            -253.9\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.0075      0.008     -0.895      0.372      -0.024       0.009\nmkt_excess     0.7503      0.145      5.164      0.000       0.463       1.037\n==============================================================================\nOmnibus:                       39.111   Durbin-Watson:                   2.039\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              107.620\nSkew:                          -1.015   Prob(JB):                     4.27e-24\nKurtosis:                       6.619   Cond. No.                         17.6\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression output provides several important pieces of information:\n\nBeta (mkt_excess coefficient): The estimated market sensitivity. A beta above 1 indicates VIC amplifies market movements.\nAlpha (Intercept): The abnormal return not explained by market exposure. Under CAPM, this should be zero.\nR-squared: The proportion of return variation explained by market movements.\nt-statistics: Test whether coefficients differ significantly from zero.\n\n\n# Extract key estimates\ncoefficients = model_vic.summary2().tables[1]\n\nprint(\"\\nKey estimates for Vingroup (VIC):\")\nprint(f\"  Beta:  {coefficients.loc['mkt_excess', 'Coef.']:.3f}\")\nprint(f\"  Alpha: {coefficients.loc['Intercept', 'Coef.']:.4f}\")\nprint(f\"  R²:    {model_vic.rsquared:.3f}\")\n\n\nKey estimates for Vingroup (VIC):\n  Beta:  0.750\n  Alpha: -0.0075\n  R²:    0.153\n\n\n\n\n9.4.2 CAPM Estimation Function\nWe create a reusable function that estimates the CAPM and returns results in a standardized format. The function includes a minimum observations requirement to avoid unreliable estimates from sparse data.\n\ndef estimate_capm(data, min_obs=48):\n    \"\"\"\n    Estimate CAPM regression and return coefficients.\n    \n    This function regresses excess stock returns on excess market returns\n    and extracts the coefficient estimates along with t-statistics.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        DataFrame with 'ret_excess' and 'mkt_excess' columns\n    min_obs : int\n        Minimum number of observations required for estimation\n        \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with coefficient estimates and t-statistics,\n        or empty DataFrame if insufficient observations\n    \"\"\"\n    if len(data) &lt; min_obs:\n        return pd.DataFrame()\n    \n    try:\n        # Estimate OLS regression\n        model = smf.ols(\n            formula=\"ret_excess ~ mkt_excess\", \n            data=data\n        ).fit()\n        \n        # Extract coefficient table\n        coef_table = model.summary2().tables[1]\n        \n        # Format results\n        results = pd.DataFrame({\n            \"coefficient\": [\"alpha\", \"beta\"],\n            \"estimate\": [\n                coef_table.loc[\"Intercept\", \"Coef.\"],\n                coef_table.loc[\"mkt_excess\", \"Coef.\"]\n            ],\n            \"t_statistic\": [\n                coef_table.loc[\"Intercept\", \"t\"],\n                coef_table.loc[\"mkt_excess\", \"t\"]\n            ],\n            \"r_squared\": model.rsquared\n        })\n        \n        return results\n        \n    except Exception as e:\n        # Return empty DataFrame if estimation fails\n        return pd.DataFrame()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#rolling-window-estimation",
    "href": "08_beta_estimation.html#rolling-window-estimation",
    "title": "9  Beta Estimation",
    "section": "9.5 Rolling-Window Estimation",
    "text": "9.5 Rolling-Window Estimation\n\n9.5.1 Motivation for Rolling Windows\nStock betas are not constant over time. A company’s business mix, leverage, and operating environment evolve, causing its systematic risk exposure to change. To capture this time variation, we use rolling-window estimation: at each point in time, we estimate beta using only data from a fixed lookback period (e.g., the past 60 months).\nRolling-window estimation involves a trade-off:\n\nLonger windows provide more observations and thus more precise estimates, but may include stale information.\nShorter windows are more responsive to changes but produce noisier estimates.\n\nA common choice in academic research is 60 months (5 years) of monthly data, requiring at least 48 valid observations for estimation.\n\n\n9.5.2 Rolling Window Implementation\nThe following function implements rolling-window CAPM estimation. For each month in the sample, it looks back over the specified window and estimates beta using all available data within that window.\n\ndef roll_capm_estimation(data, look_back=60, min_obs=48):\n    \"\"\"\n    Perform rolling-window CAPM estimation.\n    \n    This function slides a window across time, estimating the CAPM\n    regression at each point using the most recent 'look_back' months\n    of data.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        DataFrame with 'date', 'ret_excess', and 'mkt_excess' columns\n    look_back : int\n        Number of months in the estimation window\n    min_obs : int\n        Minimum observations required within each window\n        \n    Returns\n    -------\n    pd.DataFrame\n        Time series of coefficient estimates with dates\n    \"\"\"\n    # Ensure data is sorted by date\n    data = data.sort_values(\"date\").copy()\n    \n    # Get unique dates\n    dates = data[\"date\"].drop_duplicates().sort_values()\n    \n    # Container for results\n    results = []\n    \n    # Slide window across dates\n    for i in range(look_back - 1, len(dates)):\n        # Define window boundaries\n        end_date = dates.iloc[i]\n        start_date = end_date - relativedelta(months=look_back - 1)\n        \n        # Extract data within window\n        window_data = data.query(\"date &gt;= @start_date and date &lt;= @end_date\")\n        \n        # Estimate CAPM for this window\n        window_results = estimate_capm(window_data, min_obs=min_obs)\n        \n        if not window_results.empty:\n            window_results[\"date\"] = end_date\n            results.append(window_results)\n    \n    # Combine all results\n    if results:\n        return pd.concat(results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\n\n9.5.3 Example: Rolling Betas for Selected Stocks\nWe demonstrate rolling-window estimation for several well-known Vietnamese stocks spanning different industries.\n\n# Define example stocks\nexamples = pd.DataFrame({\n    \"symbol\": [\"FPT\", \"VNM\", \"VIC\", \"HPG\", \"VCB\"],\n    \"company\": [\n        \"FPT Corporation\",      # Technology\n        \"Vinamilk\",             # Consumer goods\n        \"Vingroup\",             # Real estate/conglomerate\n        \"Hoa Phat Group\",       # Steel/materials\n        \"Vietcombank\"           # Banking\n    ]\n})\n\n# Check data availability for each example\ndata_availability = (prices_monthly\n    .query(\"symbol in @examples['symbol']\")\n    .groupby(\"symbol\")\n    .agg(\n        n_obs=(\"date\", \"count\"),\n        first_date=(\"date\", \"min\"),\n        last_date=(\"date\", \"max\")\n    )\n    .reset_index()\n)\n\nprint(\"Data availability for example stocks:\")\nprint(data_availability)\n\nData availability for example stocks:\n  symbol  n_obs first_date  last_date\n0    FPT    150 2011-07-31 2023-12-31\n1    HPG    150 2011-07-31 2023-12-31\n2    VCB    150 2011-07-31 2023-12-31\n3    VIC    150 2011-07-31 2023-12-31\n4    VNM    150 2011-07-31 2023-12-31\n\n\n\n# Estimate rolling betas for example stocks\nexample_data = prices_monthly.query(\"symbol in @examples['symbol']\")\n\ncapm_examples = (example_data\n    .groupby(\"symbol\", group_keys=True)\n    .apply(lambda x: roll_capm_estimation(x), include_groups=False)\n    .reset_index()\n    .drop(columns=\"level_1\", errors=\"ignore\")\n)\n\n# Filter to beta estimates only\nbeta_examples = (capm_examples\n    .query(\"coefficient == 'beta'\")\n    .merge(examples, on=\"symbol\")\n)\n\nprint(f\"Rolling beta estimates: {len(beta_examples):,} observations\")\n\nRolling beta estimates: 455 observations\n\n\n\n\n9.5.4 Visualizing Rolling Betas\nFigure 9.1 displays the time series of beta estimates for our example stocks. The figure reveals how systematic risk exposure evolves differently across industries.\n\nrolling_beta_figure = (\n    ggplot(\n        beta_examples,\n        aes(x=\"date\", y=\"estimate\", color=\"company\")\n    )\n    + geom_line(size=0.8)\n    + geom_hline(yintercept=1, linetype=\"dashed\", color=\"gray\", alpha=0.7)\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        color=\"\",\n        title=\"Rolling Beta Estimates (60-Month Window)\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n    + theme(legend_position=\"bottom\")\n)\nrolling_beta_figure.show()\n\n\n\n\n\n\n\nFigure 9.1: Monthly rolling beta estimates for selected Vietnamese stocks using a 60-month estimation window. Different industries exhibit distinct patterns of market sensitivity over time.\n\n\n\n\n\nSeveral patterns emerge from the figure:\n\nIndustry differences: Technology and banking stocks may exhibit different beta patterns than real estate or consumer goods companies.\nTime variation: Betas are not constant. They respond to changes in business conditions, leverage, and market regimes.\nCrisis periods: Market stress periods (e.g., 2008 financial crisis, 2020 COVID-19) often see beta estimates change as correlations across stocks increase.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#parallelized-estimation-for-the-full-market",
    "href": "08_beta_estimation.html#parallelized-estimation-for-the-full-market",
    "title": "9  Beta Estimation",
    "section": "9.6 Parallelized Estimation for the Full Market",
    "text": "9.6 Parallelized Estimation for the Full Market\n\n9.6.1 The Computational Challenge\nEstimating rolling betas for all stocks in our database is computationally intensive. With hundreds of stocks, each requiring rolling estimation across many time periods, sequential processing would take considerable time. Fortunately, beta estimation for different stocks is independent (i.e., the estimate for stock A does not depend on the estimate for stock B). This independence makes the problem ideal for parallelization.\n\n\n9.6.2 Setting Up Parallel Processing\nWe use the joblib library to distribute computation across multiple CPU cores. The Parallel class manages worker processes, while delayed wraps function calls for deferred execution.\n\n# Determine available cores (reserve one for system operations)\nn_cores = max(1, cpu_count() - 1)\nprint(f\"Available cores for parallel processing: {n_cores}\")\n\nAvailable cores for parallel processing: 3\n\n\n\n\n9.6.3 Parallel Beta Estimation\nThe following code estimates rolling betas for all stocks in parallel. Each stock is processed independently by a separate worker.\n\ndef estimate_all_betas_parallel(data, n_cores, look_back=60, min_obs=48):\n    \"\"\"\n    Estimate rolling betas for all stocks using parallel processing.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Full dataset with all stocks\n    n_cores : int\n        Number of CPU cores to use\n    look_back : int\n        Months in estimation window\n    min_obs : int\n        Minimum observations required\n        \n    Returns\n    -------\n    pd.DataFrame\n        Beta estimates for all stocks and dates\n    \"\"\"\n    # Group data by stock\n    grouped = data.groupby(\"symbol\", group_keys=False)\n    \n    # Define worker function\n    def process_stock(name, group):\n        result = roll_capm_estimation(group, look_back=look_back, min_obs=min_obs)\n        if not result.empty:\n            result[\"symbol\"] = name\n        return result\n    \n    # Execute in parallel\n    results = Parallel(n_jobs=n_cores, verbose=1)(\n        delayed(process_stock)(name, group) \n        for name, group in grouped\n    )\n    \n    # Combine results\n    results = [r for r in results if not r.empty]\n    if results:\n        return pd.concat(results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\n# Estimate betas for all stocks\nprint(\"Estimating rolling betas for all stocks...\")\ncapm_monthly = estimate_all_betas_parallel(\n    prices_monthly, \n    n_cores=n_cores,\n    look_back=60,\n    min_obs=48\n)\n\nprint(f\"\\nCompleted: {len(capm_monthly):,} coefficient estimates\")\nprint(f\"Unique stocks: {capm_monthly['symbol'].nunique():,}\")\n\n\n\n9.6.4 Storing Results\nWe save the CAPM estimates to our database for use in subsequent chapters.\n\ncapm_monthly.to_sql(\n    name=\"capm_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"CAPM estimates saved to database.\")\n\nFor subsequent analysis, we load the pre-computed estimates:\n\ncapm_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM capm_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Loaded {len(capm_monthly):,} CAPM estimates\")\n\nLoaded 161,580 CAPM estimates",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#beta-estimation-using-daily-returns",
    "href": "08_beta_estimation.html#beta-estimation-using-daily-returns",
    "title": "9  Beta Estimation",
    "section": "9.7 Beta Estimation Using Daily Returns",
    "text": "9.7 Beta Estimation Using Daily Returns\nWhile monthly returns are standard in academic research, some applications benefit from higher-frequency data:\n\nShorter estimation windows: Daily data allows meaningful estimation over shorter periods (e.g., 3 months rather than 5 years).\nMore responsive estimates: Daily betas capture changes more quickly.\nEvent studies: High-frequency betas are useful for analyzing market reactions to specific events.\n\nHowever, daily data introduces additional challenges:\n\nMicrostructure noise: Bid-ask bounce and other trading frictions add noise to returns.\nNon-synchronous trading: Less liquid stocks may not trade every day, biasing beta estimates downward.\nComputational burden: Daily data is roughly 21 times larger than monthly data.\n\n\n9.7.1 Batch Processing for Daily Data\nGiven the size of daily data, we process stocks in batches to manage memory constraints. This approach loads and processes a subset of stocks, saves results, and proceeds to the next batch.\n\ndef compute_market_return_daily(tidy_finance):\n    \"\"\"\n    Compute daily value-weighted market excess return from stock data.\n    \"\"\"\n    # Load daily prices with market cap for weighting\n    prices_daily_full = pd.read_sql_query(\n        sql=\"\"\"\n            SELECT p.symbol, p.date, p.ret_excess, m.mktcap_lag\n            FROM prices_daily p\n            LEFT JOIN prices_monthly m ON p.symbol = m.symbol \n                AND strftime('%Y-%m', p.date) = strftime('%Y-%m', m.date)\n        \"\"\",\n        con=tidy_finance,\n        parse_dates={\"date\"}\n    )\n    \n    # Compute value-weighted market return each day\n    mkt_daily = (prices_daily_full\n        .dropna(subset=[\"ret_excess\", \"mktcap_lag\"])\n        .groupby(\"date\")\n        .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n        .reset_index(name=\"mkt_excess\")\n    )\n    \n    return mkt_daily\n\n\ndef roll_capm_estimation_daily(data, look_back_days=1260, min_obs=1000):\n    \"\"\"\n    Perform rolling-window CAPM estimation using daily data.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        DataFrame with 'date', 'ret_excess', and 'mkt_excess' columns\n    look_back_days : int\n        Number of trading days in the estimation window\n    min_obs : int\n        Minimum daily observations required within each window\n        \n    Returns\n    -------\n    pd.DataFrame\n        Time series of coefficient estimates with dates\n    \"\"\"\n    data = data.sort_values(\"date\").copy()\n    dates = data[\"date\"].drop_duplicates().sort_values().reset_index(drop=True)\n    \n    results = []\n    \n    for i in range(look_back_days - 1, len(dates)):\n        end_date = dates.iloc[i]\n        start_idx = max(0, i - look_back_days + 1)\n        start_date = dates.iloc[start_idx]\n        \n        window_data = data.query(\"date &gt;= @start_date and date &lt;= @end_date\")\n        window_results = estimate_capm(window_data, min_obs=min_obs)\n        \n        if not window_results.empty:\n            window_results[\"date\"] = end_date\n            results.append(window_results)\n    \n    if results:\n        return pd.concat(results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\ndef estimate_daily_betas_batch(symbols, tidy_finance, n_cores, batch_size=500, \n                                look_back_days=1260, min_obs=1000):\n    \"\"\"\n    Estimate rolling betas from daily data using batch processing.\n    \"\"\"\n    # First, compute or load market return\n    print(\"Computing daily market excess returns...\")\n    mkt_daily = compute_market_return_daily(tidy_finance)\n    print(f\"Market returns: {len(mkt_daily)} days\")\n    \n    n_batches = int(np.ceil(len(symbols) / batch_size))\n    all_results = []\n    \n    for j in range(n_batches):\n        batch_start = j * batch_size\n        batch_end = min((j + 1) * batch_size, len(symbols))\n        batch_symbols = symbols[batch_start:batch_end]\n        \n        symbol_list = \", \".join(f\"'{s}'\" for s in batch_symbols)\n        \n        query = f\"\"\"\n            SELECT symbol, date, ret_excess\n            FROM prices_daily\n            WHERE symbol IN ({symbol_list})\n        \"\"\"\n        \n        prices_daily_batch = pd.read_sql_query(\n            sql=query,\n            con=tidy_finance,\n            parse_dates={\"date\"}\n        )\n        \n        # Merge with market excess return\n        prices_daily_batch = prices_daily_batch.merge(\n            mkt_daily, \n            on=\"date\", \n            how=\"inner\"\n        )\n        \n        # Group by symbol and estimate betas\n        grouped = prices_daily_batch.groupby(\"symbol\", group_keys=False)\n        \n        # Parallel estimation\n        batch_results = Parallel(n_jobs=n_cores)(\n            delayed(lambda name, group: \n                roll_capm_estimation_daily(group, look_back_days=look_back_days, min_obs=min_obs)\n                .assign(symbol=name)\n            )(name, group)\n            for name, group in grouped\n        )\n        \n        batch_results = [r for r in batch_results if r is not None and not r.empty]\n        \n        if batch_results:\n            all_results.append(pd.concat(batch_results, ignore_index=True))\n        \n        print(f\"Batch {j+1}/{n_batches} complete\")\n    \n    if all_results:\n        return pd.concat(all_results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\nsymbols = prices_monthly[\"symbol\"].unique().tolist()\n\ncapm_daily = estimate_daily_betas_batch(\n    symbols=symbols,\n    tidy_finance=tidy_finance,\n    n_cores=n_cores,\n    batch_size=500,\n    look_back_days=1260,  # ~5 years of trading days\n    min_obs=1000\n)\n\nprint(f\"Daily beta estimates: {len(capm_daily):,}\")\n\n\ncapm_daily.to_sql(\n    name=\"capm_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"CAPM estimates saved to database.\")\n\nFor subsequent analysis, we load the pre-computed estimates:\n\ncapm_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM capm_daily\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Loaded {len(capm_daily):,} CAPM estimates\")\n\nLoaded 3,394,490 CAPM estimates",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#analyzing-beta-estimates",
    "href": "08_beta_estimation.html#analyzing-beta-estimates",
    "title": "9  Beta Estimation",
    "section": "9.8 Analyzing Beta Estimates",
    "text": "9.8 Analyzing Beta Estimates\n\n9.8.1 Extracting Beta Estimates\nWe extract the beta coefficient estimates from our CAPM results for analysis.\n\n# Extract monthly betas\nbeta_monthly = (capm_monthly\n    .query(\"coefficient == 'beta'\")\n    .rename(columns={\"estimate\": \"beta\"})\n    [[\"symbol\", \"date\", \"beta\"]]\n    .assign(frequency=\"monthly\")\n)\n\n# Save to database\nbeta_monthly.to_sql(\n    name=\"beta_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(f\"Monthly betas: {len(beta_monthly):,} observations\")\nprint(f\"Unique stocks: {beta_monthly['symbol'].nunique():,}\")\n\nMonthly betas: 80,790 observations\nUnique stocks: 1,383\n\n\n\n# Load pre-computed betas\nbeta_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM beta_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n\n\n9.8.2 Summary Statistics\nWe examine the distribution of beta estimates to verify their reasonableness.\n\nprint(\"Beta Summary Statistics:\")\nprint(beta_monthly[\"beta\"].describe().round(3))\n\n# Additional diagnostics\nprint(f\"\\nStocks with negative average beta: {(beta_monthly.groupby('symbol')['beta'].mean() &lt; 0).sum()}\")\nprint(f\"Stocks with beta &gt; 2: {(beta_monthly.groupby('symbol')['beta'].mean() &gt; 2).sum()}\")\n\nBeta Summary Statistics:\ncount    80790.000\nmean         0.501\nstd          0.539\nmin         -1.345\n25%          0.130\n50%          0.447\n75%          0.832\nmax          2.678\nName: beta, dtype: float64\n\nStocks with negative average beta: 177\nStocks with beta &gt; 2: 5\n\n\n\n\n9.8.3 Beta Distribution Across Industries\nDifferent industries have different exposures to systematic market risk based on their business models, operating leverage, and financial leverage. Figure 9.2 shows the distribution of firm-level average betas across Vietnamese industries.\n\n# Merge betas with industry information\nbeta_with_industry = (beta_monthly\n    .merge(\n        prices_monthly[[\"symbol\", \"date\", \"icb_name_vi\"]].drop_duplicates(),\n        on=[\"symbol\", \"date\"],\n        how=\"left\"\n    )\n    .dropna(subset=[\"icb_name_vi\"])\n)\n\n# Compute firm-level average beta by industry\nbeta_by_industry = (beta_with_industry\n    .groupby([\"icb_name_vi\", \"symbol\"])[\"beta\"]\n    .mean()\n    .reset_index()\n)\n\n# Order industries by median beta\nindustry_order = (beta_by_industry\n    .groupby(\"icb_name_vi\")[\"beta\"]\n    .median()\n    .sort_values()\n    .index.tolist()\n)\n\n# Select top 10 industries by number of firms for clearer visualization\ntop_industries = (beta_by_industry\n    .groupby(\"icb_name_vi\")\n    .size()\n    .nlargest(10)\n    .index.tolist()\n)\n\nbeta_by_industry_filtered = beta_by_industry.query(\"icb_name_vi in @top_industries\")\n\n\nbeta_industry_figure = (\n    ggplot(\n        beta_by_industry_filtered,\n        aes(x=\"icb_name_vi\", y=\"beta\")\n    )\n    + geom_boxplot(fill=\"steelblue\", alpha=0.7)\n    + geom_hline(yintercept=1, linetype=\"dashed\", color=\"red\", alpha=0.7)\n    + coord_flip()\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        title=\"Beta Distribution by Industry\"\n    )\n    + theme_minimal()\n)\nbeta_industry_figure.show()\n\n\n\n\n\n\n\nFigure 9.2: Distribution of firm-level average betas across Vietnamese industries. Box plots show the median, interquartile range, and outliers for each industry.\n\n\n\n\n\n\n\n9.8.4 Time Variation in Cross-Sectional Beta Distribution\nBetas vary not only across stocks but also over time. Figure 9.3 shows how the cross-sectional distribution of betas has evolved in the Vietnamese market.\n\n# Compute monthly quantiles\nbeta_quantiles = (beta_monthly\n    .groupby(\"date\")[\"beta\"]\n    .quantile(q=np.arange(0.1, 1.0, 0.1))\n    .reset_index()\n    .rename(columns={\"level_1\": \"quantile\"})\n    .assign(quantile=lambda x: (x[\"quantile\"] * 100).astype(int).astype(str) + \"%\")\n)\n\nbeta_quantiles_figure = (\n    ggplot(\n        beta_quantiles,\n        aes(x=\"date\", y=\"beta\", color=\"quantile\")\n    )\n    + geom_line(alpha=0.8)\n    + geom_hline(yintercept=1, linetype=\"dashed\", color=\"gray\")\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        color=\"Quantile\",\n        title=\"Cross-Sectional Distribution of Betas Over Time\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n)\nbeta_quantiles_figure.show()\n\n\n\n\n\n\n\nFigure 9.3: Monthly quantiles of beta estimates over time. Each line represents a decile of the cross-sectional beta distribution.\n\n\n\n\n\nThe figure reveals several interesting patterns:\n\nLevel shifts: The entire distribution of betas can shift over time, reflecting changes in market-wide correlation.\nDispersion changes: During market stress, the spread between high and low beta stocks may change as correlations move.\nTrends: Some periods show trending behavior in betas, possibly reflecting structural changes in the economy.\n\n\n\n9.8.5 Coverage Analysis\nWe verify that our estimation procedure produces reasonable coverage across the sample. Figure 9.4 shows the fraction of stocks with available beta estimates over time.\n\n# Count stocks with and without betas\ncoverage = (prices_monthly\n    .groupby(\"date\")[\"symbol\"]\n    .nunique()\n    .reset_index(name=\"total_stocks\")\n    .merge(\n        beta_monthly.groupby(\"date\")[\"symbol\"].nunique().reset_index(name=\"with_beta\"),\n        on=\"date\",\n        how=\"left\"\n    )\n    .fillna(0)\n    .assign(coverage=lambda x: x[\"with_beta\"] / x[\"total_stocks\"])\n)\n\ncoverage_figure = (\n    ggplot(coverage, aes(x=\"date\", y=\"coverage\"))\n    + geom_line(color=\"steelblue\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Share with Beta Estimate\",\n        title=\"Beta Estimation Coverage Over Time\"\n    )\n    + scale_y_continuous(labels=percent_format(), limits=(0, 1))\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n)\ncoverage_figure.show()\n\n\n\n\n\n\n\nFigure 9.4: Share of stocks with available beta estimates over time. Coverage increases as more stocks accumulate sufficient return history.\n\n\n\n\n\nCoverage is lower in early years because stocks need sufficient return history (at least 48 months) before their betas can be estimated. As the market matures and stocks accumulate longer histories, coverage approaches 100%.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#comparing-monthly-and-daily-beta-estimates",
    "href": "08_beta_estimation.html#comparing-monthly-and-daily-beta-estimates",
    "title": "9  Beta Estimation",
    "section": "9.9 Comparing Monthly and Daily Beta Estimates",
    "text": "9.9 Comparing Monthly and Daily Beta Estimates\nWhen both monthly and daily beta estimates are available, we can compare them to understand how estimation frequency affects results.\n\n# Combine monthly and daily estimates\nbeta_daily = (capm_daily\n    .query(\"coefficient == 'beta'\")\n    .rename(columns={\"estimate\": \"beta\"})\n    [[\"symbol\", \"date\", \"beta\"]]\n    .assign(frequency=\"daily\")\n)\n\nbeta_combined = pd.concat([beta_monthly, beta_daily], ignore_index=True)\n\n\n# Filter to example stocks\nbeta_comparison = (beta_combined\n    .merge(examples, on=\"symbol\")\n    .query(\"symbol in ['VIC', 'FPT']\")  # Select two for clarity\n)\n\ncomparison_figure = (\n    ggplot(\n        beta_comparison,\n        aes(x=\"date\", y=\"beta\", color=\"frequency\", linetype=\"frequency\")\n    )\n    + geom_line(size=0.8)\n    + facet_wrap(\"~company\", ncol=1)\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        color=\"Data Frequency\",\n        linetype=\"Data Frequency\",\n        title=\"Monthly vs Daily Beta Estimates\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n    + theme(legend_position=\"bottom\")\n)\ncomparison_figure.show()\n\n\n\n\n\n\n\nFigure 9.5: Comparison of beta estimates using monthly versus daily returns for selected stocks. Daily estimates are smoother due to more observations per estimation window.\n\n\n\n\n\nThe comparison reveals that daily-based estimates are generally smoother due to the larger number of observations in each window. However, the level and trend of estimates are similar across frequencies, providing validation that both approaches capture the same underlying systematic risk exposure.\n\n# Correlation between monthly and daily estimates\ncorrelation_data = (beta_combined\n    .pivot_table(index=[\"symbol\", \"date\"], columns=\"frequency\", values=\"beta\")\n    .dropna()\n)\n\nprint(f\"Correlation between monthly and daily betas: {correlation_data.corr().iloc[0,1]:.3f}\")\n\nCorrelation between monthly and daily betas: 0.745\n\n\n\n\n\nTable 9.1: Theoretical Reasons for Imperfect Correlation\n\n\n\n\n\n\n\n\n\nFactor\nEffect\n\n\n\n\nNon-synchronous trading\nDaily betas can be biased downward for illiquid stocks\n\n\nMicrostructure noise\nBid-ask bounce adds noise to daily estimates\n\n\nDifferent effective windows\nSame calendar period but ~20x more observations for daily\n\n\nMean reversion speed\nDaily captures faster-moving risk dynamics\n\n\n\n\n\n\nTable 9.1 shows several reasons why we might observe imperfect correlation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "08_beta_estimation.html#key-takeaways",
    "href": "08_beta_estimation.html#key-takeaways",
    "title": "9  Beta Estimation",
    "section": "9.10 Key Takeaways",
    "text": "9.10 Key Takeaways\n\nCAPM beta measures a stock’s sensitivity to systematic market risk and is fundamental to modern portfolio theory, cost of capital estimation, and risk management.\nRolling-window estimation captures time variation in betas, which reflects changes in companies’ business models, leverage, and market conditions.\nParallelization dramatically reduces computation time for large-scale estimation tasks by distributing work across multiple CPU cores.\nEstimation choices matter: Window length, return frequency, and minimum observation requirements all affect beta estimates. Researchers should choose parameters appropriate for their specific application.\nIndustry patterns: Vietnamese stocks show systematic differences in market sensitivity across industries, with cyclical sectors exhibiting higher betas than defensive sectors.\nTime variation: The cross-sectional distribution of betas in Vietnam has evolved over time, with notable shifts during market stress periods.\nFrequency comparison: Monthly and daily beta estimates are positively correlated but not identical. Daily estimates are smoother while monthly estimates may better capture lower-frequency variation.\nData quality checks: Coverage analysis and summary statistics help identify potential issues in estimation procedures before using results in downstream analyses.\n\n\n\n\n\n\n\nLintner, John. 1965. “Security prices, risk, and maximal gains from diversification.” The Journal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital asset market.” Econometrica 34 (4): 768–83. https://doi.org/10.2307/1910098.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html",
    "href": "09_univariate_portfolio_sort.html",
    "title": "10  Univariate Portfolio Sorts",
    "section": "",
    "text": "10.1 Data Preparation\nIn this chapter, we dive into portfolio sorts, one of the most widely used statistical methodologies in empirical asset pricing (e.g., Bali, Engle, and Murray 2016). The key application of portfolio sorts is to examine whether one or more variables can predict future excess returns. In general, the idea is to sort individual stocks into portfolios, where the stocks within each portfolio are similar with respect to a sorting variable, such as firm size. The different portfolios then represent well-diversified investments that differ in the level of the sorting variable. You can then attribute the differences in the return distribution to the impact of the sorting variable. We start by introducing univariate portfolio sorts (which sort based on only one characteristic) and tackle bivariate sorting in Value and Bivariate Sorts.\nA univariate portfolio sort considers only one sorting variable \\(x_{i,t-1}\\). Here, \\(i\\) denotes the stock and \\(t-1\\) indicates that the characteristic is observable by investors at time \\(t\\). The objective is to assess the cross-sectional relation between \\(x_{i,t-1}\\) and, typically, stock excess returns \\(r_{i,t}\\) at time \\(t\\) as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas from the previous chapter as our sorting variable.\nWe start with loading the required data from our SQLite database introduced in Accessing and Managing Financial Data and DataCore Data. In particular, we use the monthly stock price data as our asset universe. Once we form our portfolios, we use the market factor returns to compute the risk-adjusted performance (i.e., alpha). beta is the dataframe with market betas computed in the previous chapter.\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nprices_monthly = (pd.read_sql_query(\n    sql=\"SELECT symbol, date, ret_excess, mktcap_lag FROM prices_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nbeta = pd.read_sql_query(\n  sql=\"SELECT symbol, date, beta FROM beta_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#sorting-by-market-beta",
    "href": "09_univariate_portfolio_sort.html#sorting-by-market-beta",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.2 Sorting by Market Beta",
    "text": "10.2 Sorting by Market Beta\nNext, we merge our sorting variable with the return data. We use the one-month lagged betas as a sorting variable to ensure that the sorts rely only on information available when we create the portfolios. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month \\(t\\) information is available in month \\(t+1\\). You may be tempted to simply use a call such as prices_monthly['beta_lag'] = prices_monthly.groupby('symbol')['beta'].shift(1) instead. This procedure, however, does not work correctly if there are implicit missing values in the time series.\n\nbeta_lag = (beta\n  .assign(date=lambda x: x[\"date\"]+pd.DateOffset(months=1))\n  .get([\"symbol\", \"date\", \"beta\"])\n  .rename(columns={\"beta\": \"beta_lag\"})\n  .dropna()\n)\n\ndata_for_sorts = (prices_monthly\n  .merge(beta_lag, how=\"inner\", on=[\"symbol\", \"date\"])\n  .dropna()\n)\n\nThe first step to conduct portfolio sorts is to calculate periodic breakpoints that you can use to group the stocks into portfolios. For simplicity, we start with the median lagged market beta as the single breakpoint. We then compute the value-weighted returns for each of the two resulting portfolios, which means that the lagged market capitalization determines the weight in np.average().\n\nbeta_portfolios = (\n    data_for_sorts\n    .groupby(\"date\")\n    .apply(lambda x: (\n        x.assign(\n            portfolio=pd.qcut(x[\"beta_lag\"], q=[0, 0.5, 1], labels=[\"low\", \"high\"]),\n            date=x.name\n        )\n    ))\n    .reset_index(drop=True)\n    .groupby([\"portfolio\", \"date\"])\n    .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n    .reset_index(name=\"ret\")\n    .dropna(subset=['ret'])\n)\nbeta_portfolios.head()\n\n\n\n\n\n\n\n\nportfolio\ndate\nret\n\n\n\n\n0\nlow\n2016-08-31\n-0.016507\n\n\n1\nlow\n2016-09-30\n-0.089155\n\n\n2\nlow\n2016-11-30\n-0.015080\n\n\n3\nlow\n2017-01-31\n0.004113\n\n\n4\nlow\n2017-02-28\n0.035101",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#performance-evaluation",
    "href": "09_univariate_portfolio_sort.html#performance-evaluation",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.3 Performance Evaluation",
    "text": "10.3 Performance Evaluation\nWe can construct a long-short strategy based on the two portfolios: buy the high-beta portfolio and, at the same time, short the low-beta portfolio. Thereby, the overall position in the market is net-zero.\n\nbeta_longshort = (beta_portfolios\n  .pivot_table(index=\"date\", columns=\"portfolio\", values=\"ret\")\n  .reset_index()\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n)\n\nWe compute the average return and the corresponding standard error to test whether the long-short portfolio yields on average positive or negative excess returns. In the asset pricing literature, one typically adjusts for autocorrelation by using Newey and West (1987) \\(t\\)-statistics to test the null hypothesis that average portfolio excess returns are equal to zero. One necessary input for Newey-West standard errors is a chosen bandwidth based on the number of lags employed for the estimation. Researchers often default to choosing a pre-specified lag length of six months (which is not a data-driven approach). We do so in the fit() function by indicating the cov_type as HAC and providing the maximum lag length through an additional keywords dictionary.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\",\n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept     0.006       0.006        1.018    0.309\n\nSummary statistics:\n- Number of observations: 53\n- R-squared: 0.000, Adjusted R-squared: 0.000\n- F-statistic not available\n\n\n\nThe results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM predicts that high-beta stocks should yield higher expected returns. Our portfolio sort implicitly mimics an investment strategy that finances high-beta stocks by shorting low-beta stocks. Therefore, one should expect that the average excess returns yield a return that is above the risk-free rate.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#functional-programming-for-portfolio-sorts",
    "href": "09_univariate_portfolio_sort.html#functional-programming-for-portfolio-sorts",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.4 Functional Programming for Portfolio Sorts",
    "text": "10.4 Functional Programming for Portfolio Sorts\nNow, we take portfolio sorts to the next level. We want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming is very handy: we define a function that gives us flexibility concerning which variable to use for the sorting, denoted by sorting_variable. We use np.quantile() to compute breakpoints for n_portfolios. Then, we assign portfolios to stocks using the pd.cut() function. The output of the following function is a new column that contains the number of the portfolio to which a stock belongs.\nIn some applications, the variable used for the sorting might be clustered (e.g., at a lower bound of 0). Then, multiple breakpoints may be identical, leading to empty portfolios. Similarly, some portfolios might have a very small number of stocks at the beginning of the sample. Cases where the number of portfolio constituents differs substantially due to the distribution of the characteristics require careful consideration and, depending on the application, might require customized sorting approaches.\n\ndef assign_portfolio(data, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolios to a bin between breakpoints.\"\"\"\n    \n    breakpoints = np.quantile(\n      data[sorting_variable].dropna(), \n      np.linspace(0, 1, n_portfolios + 1), \n      method=\"linear\"\n    )\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nWe can use the above function to sort stocks into ten portfolios each month using lagged betas and compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.\n\nbeta_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio=assign_portfolio(x, \"beta_lag\", 10)\n    ), include_groups=False\n  )\n  .reset_index(level=\"date\")\n  .groupby([\"portfolio\", \"date\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    }), include_groups=False\n  )\n  .reset_index()\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#more-performance-evaluation",
    "href": "09_univariate_portfolio_sort.html#more-performance-evaluation",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.5 More Performance Evaluation",
    "text": "10.5 More Performance Evaluation\nIn the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns.\n\nbeta_portfolios_summary = (beta_portfolios\n  .groupby(\"portfolio\")\n  .apply(lambda x: pd.Series({\n      \"alpha\": sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[\"Intercept\"],\n      \"beta\": sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[\"mkt_excess\"],\n      \"ret\": x[\"ret\"].mean()\n    }), include_groups=False\n  )\n  .reset_index()\n)\nbeta_portfolios_summary\n\n\n\n\n\n\n\n\nportfolio\nalpha\nbeta\nret\n\n\n\n\n0\n1\n0.007031\n0.356225\n0.003892\n\n\n1\n2\n-0.004215\n0.227882\n-0.005509\n\n\n2\n3\n-0.010169\n0.390216\n-0.011241\n\n\n3\n4\n-0.014205\n0.388342\n-0.015732\n\n\n4\n5\n-0.007220\n0.616833\n-0.009723\n\n\n5\n6\n0.010648\n0.976502\n0.006577\n\n\n6\n7\n-0.010739\n0.679145\n-0.012645\n\n\n7\n8\n-0.002757\n0.991774\n-0.006963\n\n\n8\n9\n-0.003448\n0.904886\n-0.007174\n\n\n9\n10\n0.010675\n1.376356\n0.004944\n\n\n\n\n\n\n\nFigure 10.1 illustrates the CAPM alphas of beta-sorted portfolios.\n\nbeta_portfolios_figure = (\n  ggplot(\n    beta_portfolios_summary, \n    aes(x=\"portfolio\", y=\"alpha\", fill=\"portfolio\")\n  )\n  + geom_bar(stat=\"identity\")\n  + labs(\n      x=\"Portfolio\", y=\"CAPM alpha\", fill=\"Portfolio\",\n      title=\"CAPM alphas of beta-sorted portfolios\"\n    )\n  + scale_y_continuous(labels=percent_format())\n  + theme(legend_position=\"none\")\n)\nbeta_portfolios_figure.show()\n\n\n\n\n\n\n\nFigure 10.1: The figure shows CAPM alphas of beta-sorted portfolios. Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the sample period.\n\n\n\n\n\nUnlike the well-documented “betting against beta” anomaly in US markets, where low-beta portfolios exhibit positive alphas and high-beta portfolios exhibit negative alphas in a monotonic pattern, the Vietnamese market shows no clear relationship between beta and risk-adjusted returns. The alphas fluctuate without a discernible trend across deciles. This lack of pattern likely reflects the limited sample period rather than a definitive conclusion about beta pricing in Vietnam. With such a short time series, the portfolio-level CAPM regressions contain substantial estimation noise, making it difficult to detect subtle anomalies. Longer sample periods would be needed to draw reliable conclusions about whether the low-beta anomaly exists in the Vietnamese equity market.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#security-market-line-and-beta-portfolios",
    "href": "09_univariate_portfolio_sort.html#security-market-line-and-beta-portfolios",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.6 Security Market Line and Beta Portfolios",
    "text": "10.6 Security Market Line and Beta Portfolios\nThe CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time. Figure 10.2 illustrates the security market line: We see that (not surprisingly) the high-beta portfolio returns have a high correlation with the market returns. However, it seems like the average excess returns for high-beta stocks are lower than what the security market line implies would be an “appropriate” compensation for the high market risk.\n\nsml_capm = (sm.OLS.from_formula(\n    formula=\"ret ~ 1 + beta\", \n    data=beta_portfolios_summary\n  )\n  .fit()\n  .params\n)\n\nsml_figure = (\n  ggplot(\n    beta_portfolios_summary,\n    aes(x=\"beta\", y=\"ret\", color=\"factor(portfolio)\")\n  )\n  + geom_point()\n  + geom_abline(\n      intercept=0, slope=factors_ff3_monthly[\"mkt_excess\"].mean(), linetype=\"solid\"\n    )\n  + geom_abline(\n      intercept=sml_capm[\"Intercept\"], slope=sml_capm[\"beta\"], linetype=\"dashed\"\n    )\n  + labs(\n      x=\"Beta\", y=\"Excess return\", color=\"Portfolio\",\n      title=\"Average portfolio excess returns and beta estimates\"\n    )\n  + scale_x_continuous(limits=(0, 2))\n  + scale_y_continuous(labels=percent_format())\n)\nsml_figure.show()\n\n\n\n\n\n\n\nFigure 10.2: The figure shows average portfolio excess returns and beta estimates. Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\n\nTo provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio.\n\nbeta_longshort = (beta_portfolios\n  .assign(\n    portfolio=lambda x: (\n      x[\"portfolio\"].apply(\n        lambda y: \"high\" if y == x[\"portfolio\"].max()\n        else (\"low\" if y == x[\"portfolio\"].min()\n        else y)\n      )\n    )\n  )\n  .query(\"portfolio in ['low', 'high']\")\n  .pivot_table(index=\"date\", columns=\"portfolio\", values=\"ret\")\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n)\n\nAgain, the resulting long-short strategy does not exhibit statistically significant returns.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept     0.001       0.011        0.095    0.924\n\nSummary statistics:\n- Number of observations: 53\n- R-squared: 0.000, Adjusted R-squared: 0.000\n- F-statistic not available\n\n\n\nHowever, controlling for the effect of beta, the long-short portfolio yields a CAPM-adjusted alpha. The results can provide evidence regarding the validity of the CAPM in the Vietnamese market. The betting-against-beta factor has been documented extensively in developed markets (Frazzini and Pedersen 2014). Betting-against-beta corresponds to a strategy that shorts high-beta stocks and takes a (levered) long position in low-beta stocks. If borrowing constraints prevent investors from taking positions on the security market line, they are instead incentivized to buy high-beta stocks, which leads to a relatively higher price (and therefore lower expected returns than implied by the CAPM) for such high-beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital-constrained investors with lower risk aversion.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1 + mkt_excess\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1 + mkt_excess\n\nCoefficients:\n            Estimate  Std. Error  t-Statistic  p-Value\nIntercept      0.004       0.009        0.394    0.694\nmkt_excess     1.020       0.116        8.784    0.000\n\nSummary statistics:\n- Number of observations: 52\n- R-squared: 0.527, Adjusted R-squared: 0.518\n- F-statistic: 77.156 on 1 and 50 DF, p-value: 0.000\n\n\n\nFigure 10.3 shows the annual returns of the extreme beta portfolios we are mainly interested in. The figure illustrates the patterns over the sample period; each portfolio exhibits periods with positive and negative annual returns.\n\nbeta_longshort_year = (beta_longshort\n  .assign(year=lambda x: x[\"date\"].dt.year)\n  .groupby(\"year\")\n  .aggregate(\n    low=(\"low\", lambda x: (1+x).prod()-1),\n    high=(\"high\", lambda x: (1+x).prod()-1),\n    long_short=(\"long_short\", lambda x: (1+x).prod()-1)\n  )\n  .reset_index()\n  .melt(id_vars=\"year\", var_name=\"name\", value_name=\"value\")\n)\n\nbeta_longshort_figure = (\n  ggplot(\n    beta_longshort_year, \n    aes(x=\"year\", y=\"value\", fill=\"name\")\n  )\n  + geom_col(position=\"dodge\")\n  + facet_wrap(\"~name\", ncol=1)\n  + labs(x=\"\", y=\"\", title=\"Annual returns of beta portfolios\")\n  + scale_y_continuous(labels=percent_format())\n  + theme(legend_position=\"none\")\n)\nbeta_longshort_figure.show()\n\n\n\n\n\n\n\nFigure 10.3: The figure shows annual returns of beta portfolios. We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.\n\n\n\n\n\nThe high-beta portfolio and low-beta portfolio both exhibit substantial year-to-year variation. The long-short portfolio, which goes long high-beta stocks and short low-beta stocks, shows no consistent pattern of positive returns. This erratic performance reinforces our earlier finding that the beta-return relationship in the Vietnamese market does not conform to theoretical CAPM predictions during our sample period. The high volatility of annual long-short returns highlights the substantial risk inherent in such a strategy, particularly in an emerging market context with a limited sample period.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "09_univariate_portfolio_sort.html#key-takeaways",
    "href": "09_univariate_portfolio_sort.html#key-takeaways",
    "title": "10  Univariate Portfolio Sorts",
    "section": "10.7 Key Takeaways",
    "text": "10.7 Key Takeaways\n\nUnivariate portfolio sorts assess whether a single firm characteristic, like lagged market beta, can predict future excess returns.\nPortfolios are formed each month using quantile breakpoints, with returns computed using value-weighted averages to reflect realistic investment strategies.\nA long-short strategy based on beta-sorted portfolios fails to generate significant positive excess returns in the Vietnamese market, contradicting CAPM predictions that higher beta should yield higher returns.\nThe analysis provides a framework for examining the “betting against beta” anomaly, where low-beta portfolios may deliver higher alphas than high-beta portfolios, offering evidence regarding the validity of the CAPM.\nThe functional programming capabilities of Python enable scalable and flexible portfolio sorting, making it easy to analyze multiple characteristics and portfolio configurations.\nEmerging markets like Vietnam may exhibit different beta-return relationships compared to developed markets, highlighting the importance of conducting market-specific empirical analysis rather than assuming universal applicability of asset pricing anomalies.\n\n\n\n\n\n\n\nBali, Turan G, Robert F Engle, and Scott Murray. 2016. Empirical asset pricing: The cross section of stock returns. John Wiley & Sons. https://doi.org/10.1002/9781118445112.stat07954.\n\n\nFrazzini, Andrea, and Lasse Heje Pedersen. 2014. “Betting against beta.” Journal of Financial Economics 111 (1): 1–25. https://doi.org/10.1016/j.jfineco.2013.10.005.\n\n\nNewey, Whitney K., and Kenneth D. West. 1987. “A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance Matrix.” Econometrica 55 (3): 703–8. http://www.jstor.org/stable/1913610.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html",
    "href": "12_fama_french.html",
    "title": "11  Fama-French Factors",
    "section": "",
    "text": "11.1 Theoretical Background\nThis chapter provides a replication of the Fama-French factor portfolios for the Vietnamese stock market. The Fama-French factor models represent a cornerstone of empirical asset pricing, originating from the seminal work of Fama and French (1992) and later extended in Fama and French (2015). These models have transformed how academics and practitioners understand the cross-section of expected stock returns, moving beyond the single-factor Capital Asset Pricing Model to incorporate multiple sources of systematic risk.\nWe construct both the three-factor and five-factor models at monthly and daily frequencies. The monthly factors serve as the foundation for most asset pricing tests and portfolio analyses, while the daily factors enable higher-frequency applications including short-horizon event studies, market microstructure research, and daily beta estimation. By constructing factors at both frequencies, we create a complete toolkit for empirical finance research in the Vietnamese market.\nThe chapter proceeds as follows. We first discuss the theoretical motivation for each factor and the economic intuition behind the Fama-French methodology. We then prepare the necessary data, merging stock returns with accounting characteristics. Next, we implement the portfolio sorting procedures that form the basis of factor construction, carefully following the original Fama-French protocols while adapting them for Vietnamese market characteristics. We construct the three-factor model (market, size, and value) before extending to the five-factor model (adding profitability and investment). Finally, we construct daily factors and validate our replicated factors through various diagnostic checks.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#theoretical-background",
    "href": "12_fama_french.html#theoretical-background",
    "title": "11  Fama-French Factors",
    "section": "",
    "text": "11.1.1 The Evolution from CAPM to Multi-Factor Models\nThe Capital Asset Pricing Model of Sharpe (1964) posits that a single factor—the market portfolio—should explain all cross-sectional variation in expected returns. However, decades of empirical research have documented persistent patterns that CAPM cannot explain. Fama and French (1992) demonstrated that two firm characteristics—size and book-to-market ratio—capture substantial variation in average returns that the market beta leaves unexplained.\nSmall firms tend to earn higher returns than large firms, a pattern known as the size effect. Similarly, firms with high book-to-market ratios (value stocks) tend to outperform firms with low book-to-market ratios (growth stocks), known as the value premium. The three-factor model formalizes these observations by constructing tradeable factor portfolios:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i^{MKT}(r_{m,t} - r_{f,t}) + \\beta_i^{SMB} \\cdot SMB_t + \\beta_i^{HML} \\cdot HML_t + \\varepsilon_{i,t}\n\\tag{11.1}\\]\nwhere:\n\n\\(r_{i,t} - r_{f,t}\\) is the excess return on asset \\(i\\)\n\\(r_{m,t} - r_{f,t}\\) is the market excess return\n\\(SMB_t\\) (Small Minus Big) is the size factor\n\\(HML_t\\) (High Minus Low) is the value factor\n\n\n\n11.1.2 The Five-Factor Extension\nFama and French (2015) extended the model to include two additional factors motivated by the dividend discount model. Firms with higher profitability should have higher expected returns (all else equal), and firms with aggressive investment policies should have lower expected returns:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i^{MKT}MKT_t + \\beta_i^{SMB}SMB_t + \\beta_i^{HML}HML_t + \\beta_i^{RMW}RMW_t + \\beta_i^{CMA}CMA_t + \\varepsilon_{i,t}\n\\tag{11.2}\\]\nwhere:\n\n\\(RMW_t\\) (Robust Minus Weak) is the profitability factor\n\\(CMA_t\\) (Conservative Minus Aggressive) is the investment factor\n\n\n\n11.1.3 Factor Construction Methodology\nThe Fama-French methodology constructs factors through double-sorted portfolios:\n\nSize sorts: Stocks are divided into Small and Big groups based on median market capitalization.\nCharacteristic sorts: Within each size group, stocks are sorted into terciles based on book-to-market (for HML), operating profitability (for RMW), or investment (for CMA).\nFactor returns: Factors are computed as the difference between average returns of portfolios with high versus low characteristic values, averaging across size groups to neutralize size effects.\nTiming: Portfolios are formed at the end of June each year using accounting data from the prior fiscal year, ensuring all information was publicly available at formation.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#setting-up-the-environment",
    "href": "12_fama_french.html#setting-up-the-environment",
    "title": "11  Fama-French Factors",
    "section": "11.2 Setting Up the Environment",
    "text": "11.2 Setting Up the Environment\nWe load the required Python packages for data manipulation, statistical analysis, and visualization.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nfrom scipy.stats.mstats import winsorize\nimport matplotlib.pyplot as plt\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\n\nWe connect to our SQLite database containing the processed Vietnamese financial data.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#data-preparation",
    "href": "12_fama_french.html#data-preparation",
    "title": "11  Fama-French Factors",
    "section": "11.3 Data Preparation",
    "text": "11.3 Data Preparation\n\n11.3.1 Loading Stock Returns\nWe load the monthly stock returns data, which includes excess returns, market capitalization, and the risk-free rate. These variables are essential for computing value-weighted portfolio returns and factor premiums.\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprint(f\"Monthly returns: {len(prices_monthly):,} observations\")\nprint(f\"Unique stocks: {prices_monthly['symbol'].nunique():,}\")\nprint(f\"Date range: {prices_monthly['date'].min():%Y-%m} to {prices_monthly['date'].max():%Y-%m}\")\n\nMonthly returns: 165,499 observations\nUnique stocks: 1,457\nDate range: 2010-02 to 2023-12\n\n\n\n\n11.3.2 Loading Company Fundamentals\nWe load the company fundamentals data containing book equity, operating profitability, and investment—the characteristics needed for constructing the Fama-French factors.\n\ncomp_vn = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, datadate, be, op, inv\n        FROM comp_vn\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n).dropna()\n\nprint(f\"Fundamentals: {len(comp_vn):,} firm-year observations\")\nprint(f\"Unique firms: {comp_vn['symbol'].nunique():,}\")\n\nFundamentals: 18,108 firm-year observations\nUnique firms: 1,496\n\n\n\n\n11.3.3 Constructing Sorting Variables\nFollowing Fama-French conventions, we construct the sorting variables with careful attention to timing. The key principles are:\n\nSize (June Market Cap): We use market capitalization at the end of June of year \\(t\\) to sort stocks into size groups. This ensures we capture the firm’s size at the moment of portfolio formation.\nBook-to-Market Ratio: We use book equity from fiscal year \\(t-1\\) divided by market equity at the end of December \\(t-1\\). This creates a six-month gap between the accounting data and portfolio formation, ensuring the information was publicly available.\nPortfolio Formation Date: Portfolios are formed on July 1st and held for twelve months until the following June.\n\n\ndef construct_sorting_variables(prices_monthly, comp_vn):\n    \"\"\"\n    Construct sorting variables following Fama-French methodology.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns with market cap\n    comp_vn : pd.DataFrame\n        Company fundamentals with book equity, profitability, investment\n        \n    Returns\n    -------\n    pd.DataFrame\n        Sorting variables aligned with July 1st formation dates\n    \"\"\"\n    \n    # 1. Size: June market capitalization\n    # Portfolio formation is July 1st, so we use June market cap\n    size = (prices_monthly\n        .query(\"date.dt.month == 6\")\n        .assign(\n            sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(1)\n        )\n        [[\"symbol\", \"sorting_date\", \"mktcap\"]]\n        .rename(columns={\"mktcap\": \"size\"})\n    )\n    \n    print(f\"Size observations: {len(size):,}\")\n    \n    # 2. Market Equity: December market cap for B/M calculation\n    # December t-1 market cap is used with fiscal year t-1 book equity\n    # This is then used for July t portfolio formation\n    market_equity = (prices_monthly\n        .query(\"date.dt.month == 12\")\n        .assign(\n            # December year t-1 maps to July year t formation\n            sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(7)\n        )\n        [[\"symbol\", \"sorting_date\", \"mktcap\"]]\n        .rename(columns={\"mktcap\": \"me\"})\n    )\n    \n    print(f\"Market equity observations: {len(market_equity):,}\")\n    \n    # 3. Book-to-Market and other characteristics\n    # Fiscal year t-1 data is used for July t portfolio formation\n    book_to_market = (comp_vn\n        .assign(\n            # Fiscal year-end + 6 months = July formation\n            sorting_date=lambda x: pd.to_datetime(\n                (x[\"datadate\"].dt.year + 1).astype(str) + \"-07-01\"\n            )\n        )\n        .merge(market_equity, on=[\"symbol\", \"sorting_date\"], how=\"inner\")\n        .assign(\n            # Scale book equity to match market equity units\n            # BE is in VND, ME is in millions VND\n            bm=lambda x: x[\"be\"] / (x[\"me\"] * 1e9)\n        )\n        [[\"symbol\", \"sorting_date\", \"me\", \"bm\", \"op\", \"inv\"]]\n    )\n    \n    print(f\"Book-to-market observations: {len(book_to_market):,}\")\n    \n    # 4. Merge size with characteristics\n    sorting_variables = (size\n        .merge(book_to_market, on=[\"symbol\", \"sorting_date\"], how=\"inner\")\n        .dropna()\n        .drop_duplicates(subset=[\"symbol\", \"sorting_date\"])\n    )\n    \n    return sorting_variables\n\nsorting_variables = construct_sorting_variables(prices_monthly, comp_vn)\n\nprint(f\"\\nFinal sorting variables: {len(sorting_variables):,} stock-years\")\nprint(f\"Sorting date range: {sorting_variables['sorting_date'].min():%Y-%m} to {sorting_variables['sorting_date'].max():%Y-%m}\")\n\nSize observations: 13,756\nMarket equity observations: 14,286\nBook-to-market observations: 13,389\n\nFinal sorting variables: 12,046 stock-years\nSorting date range: 2011-07 to 2023-07\n\n\n\n\n11.3.4 Validating Sorting Variables\nBefore proceeding, we validate that our sorting variables have reasonable distributions. The book-to-market ratio should center around 1.0 for a typical market, though emerging markets may differ.\n\nprint(\"Sorting Variable Summary Statistics:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\n# Check for extreme values that might indicate data issues\nprint(f\"\\nB/M Median: {sorting_variables['bm'].median():.4f}\")\nprint(f\"B/M 1st percentile: {sorting_variables['bm'].quantile(0.01):.4f}\")\nprint(f\"B/M 99th percentile: {sorting_variables['bm'].quantile(0.99):.4f}\")\n\nSorting Variable Summary Statistics:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.7033      0.1852      0.1322\nstd     14680.4225      3.8683      0.2782      2.5410\nmin         0.4864      0.0014     -0.7529     -0.9569\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817    272.1893      1.4256    261.3355\n\nB/M Median: 1.1849\nB/M 1st percentile: 0.1710\nB/M 99th percentile: 8.0262\n\n\n\n\n11.3.5 Handling Outliers\nExtreme values in sorting characteristics can distort portfolio assignments and factor returns. We apply winsorization to limit the influence of outliers while preserving the general ranking of stocks.\n\n# Check BEFORE winsorization\nprint(\"BEFORE Winsorization:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\n# Apply winsorization\ndef winsorize_characteristics(df, columns, limits=(0.01, 0.99)):\n    \"\"\"\n    Apply winsorization using pandas clip.\n    \"\"\"\n    df = df.copy()\n    for col in columns:\n        if col in df.columns:\n            lower = df[col].quantile(limits[0])\n            upper = df[col].quantile(limits[1])\n            df[col] = df[col].clip(lower=lower, upper=upper)\n            print(f\"  {col}: clipped to [{lower:.4f}, {upper:.4f}]\")\n    return df\n\nsorting_variables = winsorize_characteristics(\n    sorting_variables,\n    columns=[\"bm\", \"op\", \"inv\"],  # Don't winsorize size\n    limits=(0.01, 0.99)\n)\n\n# Check AFTER winsorization\nprint(\"\\nAFTER Winsorization:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\nBEFORE Winsorization:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.7033      0.1852      0.1322\nstd     14680.4225      3.8683      0.2782      2.5410\nmin         0.4864      0.0014     -0.7529     -0.9569\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817    272.1893      1.4256    261.3355\n  bm: clipped to [0.1710, 8.0262]\n  op: clipped to [-0.7319, 1.2192]\n  inv: clipped to [-0.3990, 1.5195]\n\nAFTER Winsorization:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.5544      0.1837      0.0894\nstd     14680.4225      1.2843      0.2705      0.2721\nmin         0.4864      0.1710     -0.7319     -0.3990\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817      8.0262      1.2192      1.5195",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#portfolio-assignment-functions",
    "href": "12_fama_french.html#portfolio-assignment-functions",
    "title": "11  Fama-French Factors",
    "section": "11.4 Portfolio Assignment Functions",
    "text": "11.4 Portfolio Assignment Functions\n\n11.4.1 The Portfolio Assignment Function\nWe create a flexible function for assigning stocks to portfolios based on quantile breakpoints. This function handles both independent sorts (where breakpoints are computed across all stocks) and dependent sorts (where breakpoints are computed within subgroups).\n\ndef assign_portfolio(data, sorting_variable, percentiles):\n    \"\"\"Assign portfolios to a bin according to a sorting variable.\"\"\"\n    \n    # Get the values\n    values = data[sorting_variable].dropna()\n    \n    if len(values) == 0:\n        return pd.Series([np.nan] * len(data), index=data.index)\n    \n    # Calculate breakpoints\n    breakpoints = values.quantile(percentiles, interpolation=\"linear\")\n    \n    # Handle duplicate breakpoints by using unique values\n    unique_breakpoints = np.unique(breakpoints)\n    \n    # If all values are the same, assign all to portfolio 1\n    if len(unique_breakpoints) &lt;= 1:\n        return pd.Series([1] * len(data), index=data.index)\n    \n    # Set boundaries to -inf and +inf\n    unique_breakpoints.iloc[0] = -np.inf\n    unique_breakpoints.iloc[unique_breakpoints.size-1] = np.inf\n    \n    # Assign to bins\n    assigned = pd.cut(\n        data[sorting_variable],\n        bins=unique_breakpoints,\n        labels=pd.Series(range(1, breakpoints.size)),\n        include_lowest=True,\n        right=False\n    )\n    \n    return assigned\n\n\n# Check the distribution of characteristics BEFORE portfolio assignment\nprint(\"Operating Profitability Distribution:\")\nprint(sorting_variables[\"op\"].describe())\nprint(f\"\\nUnique OP values: {sorting_variables['op'].nunique()}\")\n\nprint(\"\\nInvestment Distribution:\")\nprint(sorting_variables[\"inv\"].describe())\nprint(f\"\\nUnique INV values: {sorting_variables['inv'].nunique()}\")\n\n# Check breakpoints for a specific date\ntest_date = sorting_variables[\"sorting_date\"].iloc[0]\ntest_data = sorting_variables.query(\"sorting_date == @test_date\")\n\nprint(f\"\\nBreakpoints for {test_date}:\")\nprint(f\"OP 30th percentile: {test_data['op'].quantile(0.3):.4f}\")\nprint(f\"OP 70th percentile: {test_data['op'].quantile(0.7):.4f}\")\nprint(f\"INV 30th percentile: {test_data['inv'].quantile(0.3):.4f}\")\nprint(f\"INV 70th percentile: {test_data['inv'].quantile(0.7):.4f}\")\n\nOperating Profitability Distribution:\ncount    12046.000000\nmean         0.183738\nstd          0.270509\nmin         -0.731888\n25%          0.030913\n50%          0.136675\n75%          0.295185\nmax          1.219223\nName: op, dtype: float64\n\nUnique OP values: 11804\n\nInvestment Distribution:\ncount    12046.000000\nmean         0.089388\nstd          0.272147\nmin         -0.399042\n25%         -0.049497\n50%          0.034157\n75%          0.158155\nmax          1.519474\nName: inv, dtype: float64\n\nUnique INV values: 11805\n\nBreakpoints for 2019-07-01 00:00:00:\nOP 30th percentile: 0.0541\nOP 70th percentile: 0.2566\nINV 30th percentile: -0.0343\nINV 70th percentile: 0.1116\n\n\n\n\n11.4.2 Assigning Portfolios for Three-Factor Model\nFor the three-factor model, we perform independent double sorts on size and book-to-market. Size is split at the median (2 groups), and book-to-market is split at the 30th and 70th percentiles (3 groups), creating 6 portfolios.\n\ndef assign_ff3_portfolios(sorting_variables):\n    \"\"\"\n    Assign portfolios for Fama-French three-factor model.\n    Independent 2x3 sort on size and book-to-market.\n    \"\"\"\n    df = sorting_variables.copy()\n    \n    # Independent size sort (median split)\n    df[\"portfolio_size\"] = df.groupby(\"sorting_date\")[\"size\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.5, 1], labels=[1, 2], duplicates='drop')\n    )\n    \n    # Independent B/M sort (30/70 split)\n    df[\"portfolio_bm\"] = df.groupby(\"sorting_date\")[\"bm\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    return df\n\n# Assign portfolios\nportfolios_ff3 = assign_ff3_portfolios(sorting_variables)\n\n# Validate\nprint(\"FF3 Book-to-Market by Portfolio (should be INCREASING):\")\nprint(portfolios_ff3.groupby(\"portfolio_bm\", observed=True)[\"bm\"].median().round(4))\n\n\nprint(\"Three-Factor Portfolio Assignments:\")\nprint(portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]].head(10))\n\nFF3 Book-to-Market by Portfolio (should be INCREASING):\nportfolio_bm\n1    0.5836\n2    1.1891\n3    2.4552\nName: bm, dtype: float64\nThree-Factor Portfolio Assignments:\n  symbol sorting_date portfolio_size portfolio_bm\n0    A32   2019-07-01              1            2\n1    A32   2020-07-01              1            2\n2    A32   2021-07-01              1            2\n3    A32   2022-07-01              1            3\n4    A32   2023-07-01              1            2\n5    AAA   2011-07-01              2            2\n6    AAA   2012-07-01              2            3\n7    AAA   2013-07-01              2            3\n8    AAA   2014-07-01              2            2\n9    AAA   2015-07-01              2            3\n\n\n\n\n11.4.3 Validating Portfolio Assignments\nWe verify that the portfolio assignments create the expected 2×3 grid with reasonable stock counts in each cell.\n\n# Check portfolio distribution for most recent year\nlatest_date = portfolios_ff3[\"sorting_date\"].max()\n\nportfolio_counts = (portfolios_ff3\n    .query(\"sorting_date == @latest_date\")\n    .groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)\n    .size()\n    .unstack(fill_value=0)\n)\n\nprint(f\"Portfolio Counts for {latest_date:%Y-%m}:\")\nprint(portfolio_counts)\n\n# Verify characteristic monotonicity\nprint(\"\\nBook-to-Market by Portfolio (should be increasing):\")\nprint(portfolios_ff3.groupby(\"portfolio_bm\", observed=True)[\"bm\"].median().round(4))\n\nPortfolio Counts for 2023-07:\nportfolio_bm      1    2    3\nportfolio_size               \n1               113  271  263\n2               275  246  125\n\nBook-to-Market by Portfolio (should be increasing):\nportfolio_bm\n1    0.5836\n2    1.1891\n3    2.4552\nName: bm, dtype: float64\n\n\nWe verify that for a single stock, the portfolio assignment remains constant between July of one year and June of the next.\n\n# Trace a single symbol (e.g., 'A32') across a formation window\npersistence_check = (portfolios_ff3\n    .query(\"symbol == 'A32' & sorting_date &gt;= '2022-01-01' & sorting_date &lt;= '2023-12-31'\")\n    .sort_values(\"sorting_date\")\n    [['symbol', 'sorting_date', 'portfolio_size', 'portfolio_bm']]\n)\nprint(\"\\nTemporal Persistence Check (Symbol A32):\")\nprint(persistence_check.head(15))\n\n\nTemporal Persistence Check (Symbol A32):\n  symbol sorting_date portfolio_size portfolio_bm\n3    A32   2022-07-01              1            3\n4    A32   2023-07-01              1            2",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#fama-french-three-factor-model-monthly",
    "href": "12_fama_french.html#fama-french-three-factor-model-monthly",
    "title": "11  Fama-French Factors",
    "section": "11.5 Fama-French Three-Factor Model (Monthly)",
    "text": "11.5 Fama-French Three-Factor Model (Monthly)\n\n11.5.1 Merging Portfolios with Returns\nWe merge the portfolio assignments with monthly returns. The key insight is that portfolios formed in July of year \\(t\\) are held through June of year \\(t+1\\). We implement this by computing a sorting_date for each monthly return observation.\n\ndef merge_portfolios_with_returns(prices_monthly, portfolio_assignments):\n    \"\"\"\n    Merge portfolio assignments with monthly returns.\n    \n    Portfolios formed in July t are held through June t+1.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns\n    portfolio_assignments : pd.DataFrame\n        Portfolio assignments with sorting_date\n        \n    Returns\n    -------\n    pd.DataFrame\n        Returns merged with portfolio assignments\n    \"\"\"\n    portfolios = (prices_monthly\n        .assign(\n            # Map each return month to its portfolio formation date\n            sorting_date=lambda x: pd.to_datetime(\n                np.where(\n                    x[\"date\"].dt.month &lt;= 6,\n                    (x[\"date\"].dt.year - 1).astype(str) + \"-07-01\",\n                    x[\"date\"].dt.year.astype(str) + \"-07-01\"\n                )\n            )\n        )\n        .merge(\n            portfolio_assignments,\n            on=[\"symbol\", \"sorting_date\"],\n            how=\"inner\"\n        )\n    )\n    \n    return portfolios\n\nportfolios_monthly_ff3 = merge_portfolios_with_returns(\n    prices_monthly,\n    portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]]\n)\n\n\nprint(f\"Merged observations: {len(portfolios_monthly_ff3):,}\")\n\nMerged observations: 136,444\n\n\n\n\n11.5.2 Computing Value-Weighted Portfolio Returns\nWe compute value-weighted returns for each of the six portfolios. Value-weighting uses lagged market capitalization to avoid look-ahead bias.\n\ndef compute_portfolio_returns(data, grouping_vars):\n    \"\"\"\n    Compute value-weighted portfolio returns.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Returns data with portfolio assignments and mktcap_lag\n    grouping_vars : list\n        Variables defining portfolio groups\n        \n    Returns\n    -------\n    pd.DataFrame\n        Value-weighted returns for each portfolio-date\n    \"\"\"\n    portfolio_returns = (data\n        .groupby(grouping_vars + [\"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]),\n            \"n_stocks\": len(x)\n        }))\n        .reset_index()\n    )\n    \n    return portfolio_returns\n\n\n# Compute portfolio returns\nportfolio_returns_ff3 = compute_portfolio_returns(\n    portfolios_monthly_ff3,\n    [\"portfolio_size\", \"portfolio_bm\"]\n)\n\nprint(\"Portfolio Returns Summary:\")\nprint(portfolio_returns_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)[\"ret\"].describe().round(4))\n\nPortfolio Returns Summary:\n                             count    mean     std     min     25%     50%  \\\nportfolio_size portfolio_bm                                                  \n1              1             150.0 -0.0052  0.0379 -0.1268 -0.0262 -0.0058   \n               2             150.0 -0.0025  0.0414 -0.1080 -0.0236 -0.0053   \n               3             150.0  0.0039  0.0601 -0.1612 -0.0269 -0.0004   \n2              1             150.0 -0.0124  0.0594 -0.2222 -0.0449 -0.0107   \n               2             150.0 -0.0021  0.0671 -0.1701 -0.0403 -0.0046   \n               3             150.0  0.0024  0.0879 -0.2359 -0.0527 -0.0012   \n\n                                75%     max  \nportfolio_size portfolio_bm                  \n1              1             0.0161  0.0849  \n               2             0.0180  0.1195  \n               3             0.0314  0.2015  \n2              1             0.0210  0.1741  \n               2             0.0317  0.1770  \n               3             0.0437  0.2124  \n\n\n\n\n11.5.3 Constructing SMB and HML Factors\nWe now construct the SMB and HML factors from the portfolio returns.\nSMB (Small Minus Big): Average return of three small portfolios minus average return of three big portfolios.\nHML (High Minus Low): Average return of two high B/M portfolios minus average return of two low B/M portfolios.\n\ndef construct_ff3_factors(portfolio_returns):\n    \"\"\"\n    Construct Fama-French three factors from portfolio returns.\n    \n    Parameters\n    ----------\n    portfolio_returns : pd.DataFrame\n        Value-weighted returns for 2x3 portfolios\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly SMB and HML factors\n    \"\"\"\n    factors = (portfolio_returns\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            # SMB: Small minus Big (average across B/M groups)\n            \"smb\": (\n                x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean()\n            ),\n            # HML: High minus Low B/M (average across size groups)\n            \"hml\": (\n                x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean()\n            )\n        }))\n        .reset_index()\n    )\n    \n    return factors\n\nfactors_smb_hml = construct_ff3_factors(portfolio_returns_ff3)\n\nprint(\"SMB and HML Factors:\")\nprint(factors_smb_hml.head(10))\n\nSMB and HML Factors:\n        date       smb       hml\n0 2011-07-31 -0.007768  0.002754\n1 2011-08-31 -0.067309  0.011474\n2 2011-09-30  0.014884  0.022854\n3 2011-10-31 -0.003743  0.001631\n4 2011-11-30  0.063234  0.009103\n5 2011-12-31  0.014571  0.015280\n6 2012-01-31 -0.026080  0.009672\n7 2012-02-29 -0.035721  0.005474\n8 2012-03-31 -0.002344  0.032477\n9 2012-04-30 -0.033391  0.074191\n\n\n\n\n11.5.4 Computing the Market Factor\nThe market factor is the value-weighted return of all stocks minus the risk-free rate. We compute this independently from the sorted portfolios.\n\ndef compute_market_factor(prices_monthly):\n    \"\"\"\n    Compute value-weighted market excess return.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns with mktcap_lag\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly market excess return\n    \"\"\"\n    market_factor = (prices_monthly\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]),\n            \"n_stocks\": len(x)\n        }), include_groups=False)\n        .reset_index()\n    )\n    \n    return market_factor\n\nmarket_factor = compute_market_factor(prices_monthly)\n\nprint(\"Market Factor Summary:\")\nprint(market_factor[\"mkt_excess\"].describe().round(4))\n\nMarket Factor Summary:\ncount    167.0000\nmean      -0.0123\nstd        0.0595\nmin       -0.2149\n25%       -0.0394\n50%       -0.0106\n75%        0.0200\nmax        0.1677\nName: mkt_excess, dtype: float64\n\n\n\n\n11.5.5 Combining Three Factors\nWe combine SMB, HML, and the market factor into the complete three-factor dataset.\n\nfactors_ff3_monthly = (factors_smb_hml\n    .merge(market_factor[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"inner\")\n)\n\n# Add risk-free rate for completeness\nrf_monthly = (prices_monthly\n    .groupby(\"date\")[\"risk_free\"]\n    .first()\n    .reset_index()\n)\n\nfactors_ff3_monthly = factors_ff3_monthly.merge(rf_monthly, on=\"date\", how=\"left\")\n\nprint(\"Fama-French Three Factors (Monthly):\")\nprint(factors_ff3_monthly.head(10))\n\nprint(\"\\nFactor Summary Statistics:\")\nprint(factors_ff3_monthly[[\"mkt_excess\", \"smb\", \"hml\"]].describe().round(4))\n\nFama-French Three Factors (Monthly):\n        date       smb       hml  mkt_excess  risk_free\n0 2011-07-31 -0.007768  0.002754   -0.078748   0.003333\n1 2011-08-31 -0.067309  0.011474    0.029906   0.003333\n2 2011-09-30  0.014884  0.022854   -0.002173   0.003333\n3 2011-10-31 -0.003743  0.001631   -0.014005   0.003333\n4 2011-11-30  0.063234  0.009103   -0.179410   0.003333\n5 2011-12-31  0.014571  0.015280   -0.094802   0.003333\n6 2012-01-31 -0.026080  0.009672    0.081273   0.003333\n7 2012-02-29 -0.035721  0.005474    0.069655   0.003333\n8 2012-03-31 -0.002344  0.032477    0.029005   0.003333\n9 2012-04-30 -0.033391  0.074191    0.048791   0.003333\n\nFactor Summary Statistics:\n       mkt_excess       smb       hml\ncount    150.0000  150.0000  150.0000\nmean      -0.0101    0.0027    0.0120\nstd        0.0586    0.0420    0.0535\nmin       -0.2149   -0.1599   -0.1284\n25%       -0.0380   -0.0175   -0.0160\n50%       -0.0095    0.0070    0.0043\n75%        0.0214    0.0261    0.0340\nmax        0.1677    0.1175    0.1618\n\n\n\n\n11.5.6 Saving Three-Factor Data\n\nfactors_ff3_monthly.to_sql(\n    name=\"factors_ff3_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Three-factor monthly data saved to database.\")\n\nThree-factor monthly data saved to database.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#fama-french-five-factor-model-monthly",
    "href": "12_fama_french.html#fama-french-five-factor-model-monthly",
    "title": "11  Fama-French Factors",
    "section": "11.6 Fama-French Five-Factor Model (Monthly)",
    "text": "11.6 Fama-French Five-Factor Model (Monthly)\n\n11.6.1 Portfolio Assignments with Dependent Sorts\nFor the five-factor model, we use dependent sorts: size is sorted independently, but profitability and investment are sorted within size groups. This controls for the correlation between size and these characteristics.\n\ndef assign_ff5_portfolios(sorting_variables):\n    \"\"\"\n    Assign portfolios for Fama-French five-factor model.\n    \"\"\"\n    df = sorting_variables.copy()\n    \n    # Independent size sort\n    df[\"portfolio_size\"] = df.groupby(\"sorting_date\")[\"size\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.5, 1], labels=[1, 2], duplicates='drop')\n    )\n    \n    # Dependent sorts within size groups\n    df[\"portfolio_bm\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"bm\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    df[\"portfolio_op\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"op\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    df[\"portfolio_inv\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"inv\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    return df\n\n# Run\nportfolios_ff5 = assign_ff5_portfolios(sorting_variables)\n\n\n\n11.6.2 Validating Five-Factor Portfolios\n\n# Check characteristic monotonicity for each dimension\nprint(\"Profitability by Portfolio (should be increasing):\")\nprint(portfolios_ff5.groupby(\"portfolio_op\", observed=True)[\"op\"].median().round(4))\n\nprint(\"\\nInvestment by Portfolio (should be increasing):\")\nprint(portfolios_ff5.groupby(\"portfolio_inv\", observed=True)[\"inv\"].median().round(4))\n\n# Check portfolio counts\nprint(\"\\nStocks per Size/Profitability Bin:\")\nprint(portfolios_ff5.groupby([\"portfolio_size\", \"portfolio_op\"], observed=True).size().unstack(fill_value=0))\n\n# Check number of unique firms per year\n(portfolios_ff5\n .groupby(\"sorting_date\")[\"symbol\"]\n .nunique())\n\nProfitability by Portfolio (should be increasing):\nportfolio_op\n1   -0.0053\n2    0.1366\n3    0.4098\nName: op, dtype: float64\n\nInvestment by Portfolio (should be increasing):\nportfolio_inv\n1   -0.1012\n2    0.0329\n3    0.2568\nName: inv, dtype: float64\n\nStocks per Size/Profitability Bin:\nportfolio_op       1     2     3\nportfolio_size                  \n1               1812  2403  1811\n2               1811  2401  1808\n\n\nsorting_date\n2011-07-01     556\n2012-07-01     632\n2013-07-01     643\n2014-07-01     650\n2015-07-01     669\n2016-07-01     737\n2017-07-01     842\n2018-07-01    1073\n2019-07-01    1183\n2020-07-01    1224\n2021-07-01    1258\n2022-07-01    1286\n2023-07-01    1293\nName: symbol, dtype: int64\n\n\n\n\n11.6.3 Merging and Computing Portfolio Returns\n\n# Merge with returns\nportfolios_monthly_ff5 = merge_portfolios_with_returns(\n    prices_monthly,\n    portfolios_ff5[[\"symbol\", \"sorting_date\", \"portfolio_size\", \n                    \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"]]\n)\n\nprint(f\"Five-factor merged observations: {len(portfolios_monthly_ff5):,}\")\n\nFive-factor merged observations: 136,444\n\n\n\n\n11.6.4 Constructing All Five Factors\nWe construct each factor from the appropriate portfolio sorts.\n\ndef construct_ff5_factors(portfolios_monthly):\n    \"\"\"\n    Construct Fama-French five factors from portfolio data.\n    \n    Parameters\n    ----------\n    portfolios_monthly : pd.DataFrame\n        Monthly returns with all portfolio assignments\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly five-factor returns\n    \"\"\"\n    \n    # HML: Value factor from B/M sorts\n    portfolios_bm = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_hml = (portfolios_bm\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # RMW: Profitability factor from OP sorts\n    portfolios_op = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_rmw = (portfolios_op\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"rmw\": (x.loc[x[\"portfolio_op\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_op\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # CMA: Investment factor from INV sorts\n    # Note: CMA is Conservative minus Aggressive (low inv - high inv)\n    portfolios_inv = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_cma = (portfolios_inv\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"cma\": (x.loc[x[\"portfolio_inv\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_inv\"] == 3, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # SMB: Size factor (average across all characteristic portfolios)\n    all_portfolios = pd.concat([portfolios_bm, portfolios_op, portfolios_inv])\n    \n    factors_smb = (all_portfolios\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # Combine all factors\n    factors = (factors_smb\n        .merge(factors_hml, on=\"date\", how=\"outer\")\n        .merge(factors_rmw, on=\"date\", how=\"outer\")\n        .merge(factors_cma, on=\"date\", how=\"outer\")\n    )\n    \n    return factors\n\nfactors_ff5 = construct_ff5_factors(portfolios_monthly_ff5)\n\n# Add market factor\nfactors_ff5_monthly = (factors_ff5\n    .merge(market_factor[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"inner\")\n    .merge(rf_monthly, on=\"date\", how=\"left\")\n)\n\nprint(\"Fama-French Five Factors (Monthly):\")\nprint(factors_ff5_monthly.head(10))\n\nprint(\"\\nFactor Summary Statistics:\")\nprint(factors_ff5_monthly[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].describe().round(4))\n\nFama-French Five Factors (Monthly):\n        date       smb       hml       rmw       cma  mkt_excess  risk_free\n0 2011-07-31 -0.015907 -0.002812  0.060525  0.045291   -0.078748   0.003333\n1 2011-08-31 -0.061842  0.006189 -0.022700 -0.023177    0.029906   0.003333\n2 2011-09-30  0.014387  0.024301 -0.006005  0.003588   -0.002173   0.003333\n3 2011-10-31 -0.006958 -0.006940  0.026694  0.003649   -0.014005   0.003333\n4 2011-11-30  0.074369  0.015617 -0.058766  0.044214   -0.179410   0.003333\n5 2011-12-31  0.006687  0.022494  0.062655  0.052444   -0.094802   0.003333\n6 2012-01-31 -0.016254  0.010513 -0.042191 -0.067170    0.081273   0.003333\n7 2012-02-29 -0.026606  0.024465 -0.030849 -0.036383    0.069655   0.003333\n8 2012-03-31  0.005096  0.050930 -0.018441  0.043488    0.029005   0.003333\n9 2012-04-30  0.000712  0.058214 -0.061434  0.009233    0.048791   0.003333\n\nFactor Summary Statistics:\n       mkt_excess       smb       hml       rmw       cma\ncount    150.0000  150.0000  150.0000  150.0000  150.0000\nmean      -0.0101    0.0077    0.0115   -0.0047    0.0083\nstd        0.0586    0.0419    0.0518    0.0477    0.0335\nmin       -0.2149   -0.1522   -0.1283   -0.2126   -0.0814\n25%       -0.0380   -0.0137   -0.0126   -0.0308   -0.0131\n50%       -0.0095    0.0104    0.0046    0.0010    0.0067\n75%        0.0214    0.0316    0.0323    0.0178    0.0289\nmax        0.1677    0.1284    0.1510    0.1297    0.1331\n\n\n\n\n11.6.5 Factor Correlations\nWe examine correlations between factors, which should generally be low for the factors to capture distinct sources of risk.\n\nprint(\"Factor Correlation Matrix:\")\ncorrelation_matrix = factors_ff5_monthly[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].corr()\nprint(correlation_matrix.round(3))\n\nFactor Correlation Matrix:\n            mkt_excess    smb    hml    rmw    cma\nmkt_excess       1.000 -0.712  0.230 -0.006 -0.104\nsmb             -0.712  1.000  0.256 -0.373  0.246\nhml              0.230  0.256  1.000 -0.694  0.479\nrmw             -0.006 -0.373 -0.694  1.000 -0.352\ncma             -0.104  0.246  0.479 -0.352  1.000\n\n\n\n\n11.6.6 Saving Five-Factor Data\n\nfactors_ff5_monthly.to_sql(\n    name=\"factors_ff5_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Five-factor monthly data saved to database.\")\n\nFive-factor monthly data saved to database.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#daily-fama-french-factors",
    "href": "12_fama_french.html#daily-fama-french-factors",
    "title": "11  Fama-French Factors",
    "section": "11.7 Daily Fama-French Factors",
    "text": "11.7 Daily Fama-French Factors\n\n11.7.1 Motivation for Daily Factors\nDaily factors are essential for several applications:\n\nDaily beta estimation: CAPM regressions using daily data require daily market excess returns.\nEvent studies: Measuring abnormal returns around corporate events requires daily factor adjustments.\nHigh-frequency research: Market microstructure studies need daily or intraday factor data.\n\nThe construction methodology mirrors the monthly approach, but we compute portfolio returns at daily frequency while maintaining the same annual portfolio formation dates.\n\n\n11.7.2 Loading Daily Returns\n\n# Load daily price data\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess\n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Daily returns: {len(prices_daily):,} observations\")\nprint(f\"Date range: {prices_daily['date'].min():%Y-%m-%d} to {prices_daily['date'].max():%Y-%m-%d}\")\n\nDaily returns: 3,462,157 observations\nDate range: 2010-01-05 to 2023-12-29\n\n\n\n\n11.7.3 Adding Market Cap for Daily Weighting\nFor value-weighted daily returns, we need market capitalization. We use the most recent monthly market cap as the weight for daily returns within that month.\n\n# Get monthly market cap to use as weights for daily returns\nmktcap_monthly = (prices_monthly\n    [[\"symbol\", \"date\", \"mktcap_lag\"]]\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n)\n\n# Add year_month to daily data for merging\nprices_daily = (prices_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .merge(\n        mktcap_monthly[[\"symbol\", \"year_month\", \"mktcap_lag\"]],\n        on=[\"symbol\", \"year_month\"],\n        how=\"left\"\n    )\n    .dropna(subset=[\"ret_excess\", \"mktcap_lag\"])\n)\n\nprint(f\"Daily returns with weights: {len(prices_daily):,} observations\")\n\nDaily returns with weights: 3,443,815 observations\n\n\n\n\n11.7.4 Merging Daily Returns with Portfolios\nWe use the same portfolio assignments (formed annually in July) for daily returns.\n\n# Step 1: Ensure portfolios_ff3 has correct format\nportfolios_ff3_clean = portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]].copy()\nportfolios_ff3_clean[\"sorting_date\"] = pd.to_datetime(portfolios_ff3_clean[\"sorting_date\"])\n\nprint(\"Portfolio sorting dates:\")\nprint(portfolios_ff3_clean[\"sorting_date\"].unique()[:5])\n\n# Step 2: Create sorting_date for daily data\nprices_daily_with_sort = prices_daily.copy()\nprices_daily_with_sort[\"sorting_date\"] = prices_daily_with_sort[\"date\"].apply(\n    lambda x: pd.Timestamp(f\"{x.year}-07-01\") if x.month &gt; 6 else pd.Timestamp(f\"{x.year - 1}-07-01\")\n)\n\nprint(\"\\nDaily sorting dates:\")\nprint(prices_daily_with_sort[\"sorting_date\"].unique()[:5])\n\n# Step 3: Merge\nportfolios_daily_ff3 = prices_daily_with_sort.merge(\n    portfolios_ff3_clean,\n    on=[\"symbol\", \"sorting_date\"],\n    how=\"inner\"\n)\n\nprint(f\"\\nMerged daily observations: {len(portfolios_daily_ff3):,}\")\nprint(f\"Unique dates: {portfolios_daily_ff3['date'].nunique():,}\")\n\n# Step 4: Verify portfolio distribution\nprint(\"\\nPortfolio distribution in daily data:\")\nprint(portfolios_daily_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\nPortfolio sorting dates:\n&lt;DatetimeArray&gt;\n['2019-07-01 00:00:00', '2020-07-01 00:00:00', '2021-07-01 00:00:00',\n '2022-07-01 00:00:00', '2023-07-01 00:00:00']\nLength: 5, dtype: datetime64[us]\n\nDaily sorting dates:\n&lt;DatetimeArray&gt;\n['2018-07-01 00:00:00', '2019-07-01 00:00:00', '2020-07-01 00:00:00',\n '2021-07-01 00:00:00', '2022-07-01 00:00:00']\nLength: 5, dtype: datetime64[us]\n\nMerged daily observations: 2,843,570\nUnique dates: 3,126\n\nPortfolio distribution in daily data:\nportfolio_bm         1       2       3\nportfolio_size                        \n1               218040  585561  617327\n2               636152  552114  234376\n\n\n\n# Diagnostic: Check the daily portfolio merge\nprint(\"=\"*50)\nprint(\"DIAGNOSTIC: Daily Portfolio Merge\")\nprint(\"=\"*50)\n\nprint(f\"\\nDaily prices rows: {len(prices_daily):,}\")\nprint(f\"Daily FF3 portfolios rows: {len(portfolios_daily_ff3):,}\")\nprint(f\"Match rate: {len(portfolios_daily_ff3)/len(prices_daily)*100:.1f}%\")\n\n# Check portfolio distribution in daily data\nprint(\"\\nDaily portfolio distribution:\")\nprint(portfolios_daily_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\n# Check a specific date\ntest_date = portfolios_daily_ff3[\"date\"].iloc[1000]\nprint(f\"\\nSample date: {test_date}\")\nprint(portfolios_daily_ff3.query(\"date == @test_date\").groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\n==================================================\nDIAGNOSTIC: Daily Portfolio Merge\n==================================================\n\nDaily prices rows: 3,443,815\nDaily FF3 portfolios rows: 2,843,570\nMatch rate: 82.6%\n\nDaily portfolio distribution:\nportfolio_bm         1       2       3\nportfolio_size                        \n1               218040  585561  617327\n2               636152  552114  234376\n\nSample date: 2023-06-28 00:00:00\nportfolio_bm      1    2    3\nportfolio_size               \n1                93  232  312\n2               291  280   67\n\n\n\n\n11.7.5 Computing Daily Three Factors\n\ndef compute_daily_ff3_factors(portfolios_daily):\n    \"\"\"\n    Compute daily Fama-French three factors.\n    \n    Parameters\n    ----------\n    portfolios_daily : pd.DataFrame\n        Daily returns with portfolio assignments\n        \n    Returns\n    -------\n    pd.DataFrame\n        Daily SMB and HML factors\n    \"\"\"\n    # Compute daily portfolio returns\n    portfolio_returns = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    # Compute factors\n    factors = (portfolio_returns\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean()),\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    return factors\n\nfactors_daily_smb_hml = compute_daily_ff3_factors(portfolios_daily_ff3)\n\nprint(f\"Daily factor observations: {len(factors_daily_smb_hml):,}\")\nprint(factors_daily_smb_hml.head(10))\n\nDaily factor observations: 3,126\n        date       smb       hml\n0 2011-07-01  0.008587  0.000967\n1 2011-07-04  0.005099 -0.001099\n2 2011-07-05 -0.009088  0.010152\n3 2011-07-06  0.004875 -0.003918\n4 2011-07-07 -0.011239 -0.000584\n5 2011-07-08  0.005636 -0.008003\n6 2011-07-11  0.003940  0.006172\n7 2011-07-12  0.003205  0.006543\n8 2011-07-13 -0.000097 -0.001134\n9 2011-07-14 -0.001248  0.001669\n\n\n\n\n11.7.6 Computing Daily Market Factor\n\ndef compute_daily_market_factor(prices_daily):\n    \"\"\"\n    Compute daily value-weighted market excess return.\n    \n    Parameters\n    ----------\n    prices_daily : pd.DataFrame\n        Daily returns with mktcap_lag\n        \n    Returns\n    -------\n    pd.DataFrame\n        Daily market excess return\n    \"\"\"\n    market_daily = (prices_daily\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    return market_daily\n\nmarket_factor_daily = compute_daily_market_factor(prices_daily)\n\nprint(f\"Daily market factor: {len(market_factor_daily):,} days\")\n\nDaily market factor: 3,474 days\n\n\n\n\n11.7.7 Combining Daily Three Factors\n\nfactors_ff3_daily = (factors_daily_smb_hml\n    .merge(market_factor_daily, on=\"date\", how=\"inner\")\n)\n\n# Add risk-free rate (use monthly rate / 21 as daily approximation, or load actual daily rate)\nfactors_ff3_daily[\"risk_free\"] = 0.04 / 252  # Approximate daily risk-free\n\nprint(\"Daily Fama-French Three Factors:\")\nprint(factors_ff3_daily.head(10))\n\nprint(\"\\nDaily Factor Summary Statistics:\")\nprint(factors_ff3_daily[[\"mkt_excess\", \"smb\", \"hml\"]].describe().round(6))\n\nDaily Fama-French Three Factors:\n        date       smb       hml  mkt_excess  risk_free\n0 2011-07-01  0.008587  0.000967   -0.019862   0.000159\n1 2011-07-04  0.005099 -0.001099   -0.000633   0.000159\n2 2011-07-05 -0.009088  0.010152    0.013314   0.000159\n3 2011-07-06  0.004875 -0.003918   -0.008045   0.000159\n4 2011-07-07 -0.011239 -0.000584    0.003391   0.000159\n5 2011-07-08  0.005636 -0.008003    0.000218   0.000159\n6 2011-07-11  0.003940  0.006172   -0.013393   0.000159\n7 2011-07-12  0.003205  0.006543   -0.017505   0.000159\n8 2011-07-13 -0.000097 -0.001134    0.000767   0.000159\n9 2011-07-14 -0.001248  0.001669   -0.000695   0.000159\n\nDaily Factor Summary Statistics:\n        mkt_excess          smb          hml\ncount  3126.000000  3126.000000  3126.000000\nmean     -0.000479     0.000236     0.000594\nstd       0.011269     0.008488     0.008585\nmin      -0.070268    -0.032671    -0.039418\n25%      -0.005074    -0.004882    -0.003941\n50%       0.000350    -0.000106     0.000522\n75%       0.005531     0.004307     0.005233\nmax       0.043386     0.042686     0.083889\n\n\n\n\n11.7.8 Computing Daily Five Factors\n\n# Step 1: Clean portfolios\nportfolios_ff5_clean = portfolios_ff5[[\"symbol\", \"sorting_date\", \"portfolio_size\", \n                                        \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"]].copy()\nportfolios_ff5_clean[\"sorting_date\"] = pd.to_datetime(portfolios_ff5_clean[\"sorting_date\"])\n\n# Step 2: Merge with daily prices\nportfolios_daily_ff5 = prices_daily_with_sort.merge(\n    portfolios_ff5_clean,\n    on=[\"symbol\", \"sorting_date\"],\n    how=\"inner\"\n)\n\nprint(f\"FF5 Daily merged observations: {len(portfolios_daily_ff5):,}\")\n\nFF5 Daily merged observations: 2,843,570\n\n\n\ndef compute_daily_ff5_factors(portfolios_daily):\n    \"\"\"Compute daily Fama-French five factors.\"\"\"\n    \n    # HML from B/M sorts\n    portfolios_bm = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_hml = (portfolios_bm\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # RMW from OP sorts\n    portfolios_op = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_rmw = (portfolios_op\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"rmw\": (x.loc[x[\"portfolio_op\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_op\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # CMA from INV sorts (note: low minus high)\n    portfolios_inv = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_cma = (portfolios_inv\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"cma\": (x.loc[x[\"portfolio_inv\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_inv\"] == 3, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # SMB from all sorts\n    all_portfolios = pd.concat([portfolios_bm, portfolios_op, portfolios_inv])\n    \n    factors_smb = (all_portfolios\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # Combine\n    factors = (factors_smb\n        .merge(factors_hml, on=\"date\", how=\"outer\")\n        .merge(factors_rmw, on=\"date\", how=\"outer\")\n        .merge(factors_cma, on=\"date\", how=\"outer\")\n    )\n    \n    return factors\n\n# Compute daily FF5 factors\nfactors_daily_ff5 = compute_daily_ff5_factors(portfolios_daily_ff5)\n\n# Add market factor\nfactors_ff5_daily = (factors_daily_ff5\n    .merge(market_factor_daily, on=\"date\", how=\"inner\")\n)\nfactors_ff5_daily[\"risk_free\"] = 0.04 / 252\n\nprint(\"Daily Fama-French Five Factors:\")\nprint(factors_ff5_daily.head(10))\n\nprint(\"\\nDaily Five-Factor Summary Statistics:\")\nprint(factors_ff5_daily[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].describe().round(6))\n\nDaily Fama-French Five Factors:\n        date       smb       hml       rmw       cma  mkt_excess  risk_free\n0 2011-07-01  0.006295  0.002515  0.013140  0.007680   -0.019862   0.000159\n1 2011-07-04  0.002880 -0.002875  0.006560 -0.004886   -0.000633   0.000159\n2 2011-07-05 -0.004260  0.009864 -0.012158 -0.004470    0.013314   0.000159\n3 2011-07-06  0.001544 -0.009847  0.012977  0.006286   -0.008045   0.000159\n4 2011-07-07 -0.009789 -0.003988 -0.000197 -0.006995    0.003391   0.000159\n5 2011-07-08  0.001537 -0.006700  0.010841 -0.007661    0.000218   0.000159\n6 2011-07-11  0.005396  0.004747  0.000655  0.013375   -0.013393   0.000159\n7 2011-07-12  0.004759  0.007367  0.001989  0.014669   -0.017505   0.000159\n8 2011-07-13 -0.000009  0.001110 -0.002052 -0.001633    0.000767   0.000159\n9 2011-07-14 -0.001668  0.002916 -0.005427  0.005388   -0.000695   0.000159\n\nDaily Five-Factor Summary Statistics:\n        mkt_excess          smb          hml          rmw          cma\ncount  3126.000000  3126.000000  3126.000000  3126.000000  3126.000000\nmean     -0.000479     0.000484     0.000549    -0.000136     0.000413\nstd       0.011269     0.008033     0.008312     0.008538     0.006756\nmin      -0.070268    -0.036283    -0.039155    -0.154013    -0.047698\n25%      -0.005074    -0.004122    -0.003681    -0.004212    -0.003364\n50%       0.000350     0.000105     0.000384     0.000030     0.000162\n75%       0.005531     0.004358     0.004819     0.004036     0.003800\nmax       0.043386     0.060307     0.086269     0.102001     0.089907\n\n\n\n\n11.7.9 Saving Daily Factors\n\nfactors_ff3_daily.to_sql(\n    name=\"factors_ff3_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nfactors_ff5_daily.to_sql(\n    name=\"factors_ff5_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Daily factor data saved to database.\")\n\nDaily factor data saved to database.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#factor-validation-and-diagnostics",
    "href": "12_fama_french.html#factor-validation-and-diagnostics",
    "title": "11  Fama-French Factors",
    "section": "11.8 Factor Validation and Diagnostics",
    "text": "11.8 Factor Validation and Diagnostics\n\n# Verify all tables are in database\nprint(\"\\n\" + \"=\"*50)\nprint(\"DATABASE SUMMARY\")\nprint(\"=\"*50)\n\ntables = [\"factors_ff3_monthly\", \"factors_ff5_monthly\", \n          \"factors_ff3_daily\", \"factors_ff5_daily\"]\n\nfor table in tables:\n    df = pd.read_sql_query(f\"SELECT COUNT(*) as n FROM {table}\", con=tidy_finance)\n    print(f\"{table}: {df['n'].iloc[0]:,} observations\")\n\n# Correlation check: Monthly vs Daily (aggregated)\nprint(\"\\n\" + \"=\"*50)\nprint(\"MONTHLY VS DAILY CONSISTENCY CHECK\")\nprint(\"=\"*50)\n\nfactors_daily_agg = (factors_ff3_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .groupby(\"year_month\")[[\"mkt_excess\", \"smb\", \"hml\"]]\n    .sum()\n    .reset_index()\n)\n\nfactors_monthly_check = (factors_ff3_monthly\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n)\n\ncomparison = factors_monthly_check.merge(\n    factors_daily_agg, on=\"year_month\", suffixes=(\"_monthly\", \"_daily\")\n)\n\nfor factor in [\"mkt_excess\", \"smb\", \"hml\"]:\n    corr = comparison[f\"{factor}_monthly\"].corr(comparison[f\"{factor}_daily\"])\n    print(f\"{factor}: Monthly-Daily correlation = {corr:.4f}\")\n\n\n==================================================\nDATABASE SUMMARY\n==================================================\nfactors_ff3_monthly: 150 observations\nfactors_ff5_monthly: 150 observations\nfactors_ff3_daily: 3,126 observations\nfactors_ff5_daily: 3,126 observations\n\n==================================================\nMONTHLY VS DAILY CONSISTENCY CHECK\n==================================================\nmkt_excess: Monthly-Daily correlation = 0.9980\nsmb: Monthly-Daily correlation = 0.9953\nhml: Monthly-Daily correlation = 0.9936\n\n\n\n11.8.1 Cumulative Factor Returns\nWe visualize the cumulative performance of each factor to assess whether the factors generate meaningful premiums over time.\n\n# Compute cumulative returns\nfactors_cumulative = (factors_ff5_monthly\n    .set_index(\"date\")\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .add(1)\n    .cumprod()\n)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nfactors_cumulative.plot(ax=ax)\nax.set_title(\"Cumulative Factor Returns (Vietnam)\")\nax.set_xlabel(\"\")\nax.set_ylabel(\"Growth of $1\")\nax.legend(title=\"Factor\")\nax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 11.1: Cumulative returns of Fama-French factors for the Vietnamese market. The figure shows the growth of $1 invested in each factor portfolio.\n\n\n\n\n\n\n\n11.8.2 Average Factor Premiums\nWe compute annualized average factor premiums and their statistical significance.\n\n# Annualized average returns (monthly returns * 12)\nfactor_premiums = (factors_ff5_monthly\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .mean() * 12 * 100  # Annualized percentage\n)\n\n# Standard errors\nfactor_se = (factors_ff5_monthly\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .std() / np.sqrt(len(factors_ff5_monthly)) * np.sqrt(12) * 100\n)\n\n# T-statistics\nfactor_tstat = factor_premiums / factor_se\n\nprint(\"Annualized Factor Premiums (%):\")\nprint(factor_premiums.round(2))\n\nprint(\"\\nT-Statistics:\")\nprint(factor_tstat.round(2))\n\nAnnualized Factor Premiums (%):\nmkt_excess   -12.09\nsmb            9.19\nhml           13.75\nrmw           -5.69\ncma            9.94\ndtype: float64\n\nT-Statistics:\nmkt_excess    -7.30\nsmb            7.76\nhml            9.38\nrmw           -4.22\ncma           10.49\ndtype: float64\n\n\n\n\n11.8.3 Comparing Monthly and Daily Factors\nWe verify consistency between monthly and daily factors by computing correlations.\n\n# Aggregate daily factors to monthly for comparison\nfactors_daily_monthly = (factors_ff5_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .groupby(\"year_month\")\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .sum()  # Sum daily returns to get monthly\n    .reset_index()\n)\n\n# Merge with actual monthly factors\ncomparison = (factors_ff5_monthly\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .merge(\n        factors_daily_monthly,\n        on=\"year_month\",\n        suffixes=(\"_monthly\", \"_daily\")\n    )\n)\n\n# Correlations\nfor factor in [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]:\n    corr = comparison[f\"{factor}_monthly\"].corr(comparison[f\"{factor}_daily\"])\n    print(f\"{factor}: Monthly-Daily correlation = {corr:.4f}\")\n\nmkt_excess: Monthly-Daily correlation = 0.9980\nsmb: Monthly-Daily correlation = 0.9950\nhml: Monthly-Daily correlation = 0.9948\nrmw: Monthly-Daily correlation = 0.9929\ncma: Monthly-Daily correlation = 0.9884",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "12_fama_french.html#key-takeaways",
    "href": "12_fama_french.html#key-takeaways",
    "title": "11  Fama-French Factors",
    "section": "11.9 Key Takeaways",
    "text": "11.9 Key Takeaways\n\nFactor Models Explained: The Fama-French three-factor model adds size (SMB) and value (HML) factors to the CAPM, while the five-factor model further includes profitability (RMW) and investment (CMA) factors.\nConstruction Methodology: Factors are constructed through double-sorted portfolios with careful attention to timing. Portfolios are formed in July using accounting data from the prior fiscal year to ensure information was publicly available.\nIndependent vs. Dependent Sorts: The three-factor model uses independent sorts on size and book-to-market, creating a 2×3 grid. The five-factor model uses dependent sorts where characteristics are sorted within size groups.\nValue-Weighted Returns: Portfolio returns are computed using value-weighting with lagged market capitalization to avoid look-ahead bias.\nDaily Factors: Daily factors use the same annual portfolio assignments but compute returns at daily frequency, enabling higher-frequency applications like daily beta estimation.\nMarket Factor: The market factor is computed independently as the value-weighted return of all stocks minus the risk-free rate.\nValidation: Factor quality can be assessed through characteristic monotonicity, portfolio diversification, cumulative returns, and consistency between daily and monthly frequencies.\nVietnamese Market Adaptation: While following the original Fama-French methodology, we adapt for Vietnamese market characteristics including VAS accounting standards, reporting timelines, and currency units.\n\n\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock returns.” The Journal of Finance 47 (2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html",
    "href": "13_momemtum.html",
    "title": "12  Momentum Strategies",
    "section": "",
    "text": "12.1 Theoretical Background\nMomentum is one of the most robust and pervasive anomalies in financial economics. Stocks that have performed well over the past three to twelve months tend to continue performing well over the subsequent three to twelve months, and stocks that have performed poorly tend to continue underperforming. This pattern, first documented by Jegadeesh and Titman (1993), has been replicated across virtually every equity market, asset class, and time period examined, earning it a central place in the canon of empirical asset pricing.\nIn this chapter, we provide an implementation of momentum strategies following the methodology of Jegadeesh and Titman (1993). We construct momentum portfolios based on past cumulative returns, evaluate their performance across different formation and holding period combinations, and examine whether the momentum premium exists in the Vietnamese equity market..",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#theoretical-background",
    "href": "13_momemtum.html#theoretical-background",
    "title": "12  Momentum Strategies",
    "section": "",
    "text": "12.1.1 The Momentum Effect\nThe momentum effect refers to the tendency of assets with high recent returns to continue generating high returns, and assets with low recent returns to continue generating low returns. More formally, if we define the cumulative return of stock \\(i\\) over the past \\(J\\) months as\n\\[\nR_{i,t-J:t-1} = \\prod_{s=t-J}^{t-1} (1 + r_{i,s}) - 1\n\\tag{12.1}\\]\nthen momentum predicts a positive cross-sectional relationship between \\(R_{i,t-J:t-1}\\) and future returns \\(r_{i,t}\\). That is, stocks in the top decile of past returns (winners) should outperform stocks in the bottom decile (losers) in subsequent months.\nThe Jegadeesh and Titman (1993) framework parameterizes momentum strategies by two key dimensions:\n\nFormation period (\\(J\\)): The number of months over which past returns are computed to rank stocks. Typical values range from 3 to 12 months.\nHolding period (\\(K\\)): The number of months for which the momentum portfolios are held after formation. Typical values also range from 3 to 12 months.\n\nA strategy is therefore characterized by the pair \\((J, K)\\). For example, a \\((6, 6)\\) strategy ranks stocks based on their cumulative returns over the past 6 months and holds the resulting portfolios for 6 months.\n\n\n12.1.2 Overlapping Portfolios and Calendar-Time Returns\nA critical implementation detail in Jegadeesh and Titman (1993) is the use of overlapping portfolios. In each month \\(t\\), a new set of portfolios is formed based on the most recent \\(J\\)-month returns. However, portfolios formed in months \\(t-1\\), \\(t-2\\), \\(\\ldots\\), \\(t-K+1\\) are still within their holding periods and therefore remain active. The return of the momentum strategy in month \\(t\\) is thus the equally weighted average across all \\(K\\) active cohorts:\n\\[\nr_{p,t} = \\frac{1}{K} \\sum_{k=0}^{K-1} r_{p,t}^{(t-k)}\n\\tag{12.2}\\]\nwhere \\(r_{p,t}^{(t-k)}\\) denotes the return in month \\(t\\) of the portfolio formed in month \\(t-k\\). This overlapping portfolio approach serves two purposes. First, it reduces the impact of any single formation date on the strategy’s performance. Second, it produces a monthly return series that can be analyzed using standard time-series methods, even when the holding period \\(K\\) exceeds one month.\n\n\n12.1.3 Theoretical Explanations\nThe academic literature offers two broad classes of explanations for the momentum effect.\n\nBehavioral explanations attribute momentum to systematic cognitive biases among investors. Daniel, Hirshleifer, and Subrahmanyam (1998) propose a model based on investor overconfidence and biased self-attribution: investors overweight private signals and attribute confirming outcomes to their own skill, leading to initial underreaction followed by delayed overreaction. Hong and Stein (1999) develop a model in which information diffuses gradually across heterogeneous investors, generating underreaction to firm-specific news. Barberis, Shleifer, and Vishny (1998) formalize a model combining conservatism bias (slow updating of beliefs in response to new evidence) and representativeness heuristic (extrapolation of recent trends).\nRisk-based explanations argue that momentum profits represent compensation for systematic risk that varies over time. Johnson (2002) show that momentum can arise in a rational framework if expected returns are stochastic and time-varying. Grundy and Martin (2001) document that momentum portfolios have substantial time-varying factor exposures, suggesting that at least part of the momentum premium reflects dynamic risk. However, standard risk models such as the Fama-French three-factor model have generally struggled to explain momentum returns, which motivated the development of explicit momentum factors such as the Winners-Minus-Losers (WML) factor in Carhart (1997).\n\n\n\n12.1.4 Momentum in Emerging Markets\nThe behavior of momentum in emerging markets differs substantially from developed markets, and understanding these differences is essential for interpreting our Vietnamese market results.\nRouwenhorst (1998) provides early evidence that momentum profits exist in European markets. Chan, Hameed, and Tong (2000) extend the analysis to international equity markets and find that momentum profits are present in most developed markets but are weaker or absent in several Asian markets. Chui, Titman, and Wei (2010) offer a cultural explanation, documenting that momentum profits are positively related to a country’s degree of individualism as measured by Hofstede (2001). Countries with collectivist cultures, including many in East and Southeast Asia, tend to exhibit weaker momentum effects.\nSeveral market microstructure features common in emerging markets may attenuate momentum:\n\nTrading band limits constrain daily price movements, potentially slowing the adjustment process that generates momentum. Vietnam’s HOSE imposes a \\(\\pm 7\\%\\) daily limit, while HNX allows \\(\\pm 10\\%\\).\nLower liquidity and higher transaction costs can erode momentum profits, as documented by Lesmond, Schill, and Zhou (2004).\nForeign ownership limits segment the investor base, potentially altering the information diffusion dynamics that underlie momentum.\nShorter market history limits the statistical power available to detect the effect.\n\nThese considerations motivate our careful empirical analysis of whether, and to what extent, momentum manifests in Vietnamese equities.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#setting-up-the-environment",
    "href": "13_momemtum.html#setting-up-the-environment",
    "title": "12  Momentum Strategies",
    "section": "12.2 Setting Up the Environment",
    "text": "12.2 Setting Up the Environment\nWe begin by loading the necessary Python packages. The core packages include pandas for data manipulation, numpy for numerical operations, and sqlite3 for database connectivity. We also import plotnine for creating publication-quality figures and scipy for statistical tests.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom datetime import datetime\nfrom itertools import product\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format, percent_format\nfrom scipy import stats\n\nWe connect to our SQLite database, which stores the cleaned datasets prepared in Accessing and Managing Financial Data and DataCore Data.\n\ntidy_finance = sqlite3.connect(\n    database=\"data/tidy_finance_python.sqlite\"\n)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#data-preparation",
    "href": "13_momemtum.html#data-preparation",
    "title": "12  Momentum Strategies",
    "section": "12.3 Data Preparation",
    "text": "12.3 Data Preparation\n\n12.3.1 Loading Monthly Stock Returns\nWe load the monthly stock price data from our database. The prices_monthly table contains adjusted returns, market capitalizations, and other variables for all stocks listed on HOSE and HNX.\n\nprices_monthly = pd.read_sql_query(\n    # can add \"exchange\" variable\n    sql=\"\"\"\n        SELECT symbol, date, ret, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprint(f\"Total monthly observations: {len(prices_monthly):,}\")\nprint(f\"Unique stocks: {prices_monthly['symbol'].nunique():,}\")\nprint(f\"Date range: {prices_monthly['date'].min().date()} \"\n      f\"to {prices_monthly['date'].max().date()}\")\n\nTotal monthly observations: 165,499\nUnique stocks: 1,457\nDate range: 2010-02-28 to 2023-12-31\n\n\n\n\n12.3.2 Inspecting the Data\nBefore proceeding with portfolio construction, we examine the key variables in our dataset. Table 12.1 presents the summary statistics for the main variables used in momentum portfolio construction.\n\nsummary_stats = (prices_monthly\n    [[\"ret\", \"ret_excess\", \"mktcap\"]]\n    .describe()\n    .T\n    .round(4)\n)\nsummary_stats\n\n\n\nTable 12.1: Summary statistics for monthly stock return data. The table reports the count, mean, standard deviation, minimum, 25th percentile, median, 75th percentile, and maximum for monthly returns (ret), excess returns (ret_excess), and market capitalization (mktcap, in billion VND).\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nret\n165499.0\n0.0042\n0.1862\n-0.9900\n-0.0703\n0.0000\n0.0553\n12.7500\n\n\nret_excess\n165499.0\n0.0008\n0.1862\n-0.9933\n-0.0736\n-0.0033\n0.0519\n12.7467\n\n\nmktcap\n165499.0\n2183.1646\n13983.9977\n0.3536\n60.3728\n180.6224\n660.0000\n463886.6454\n\n\n\n\n\n\n\n\n\n\nWe also examine the cross-sectional distribution of stocks over time, which is important for understanding whether we have sufficient breadth for decile portfolio construction.\n\nstock_counts = (prices_monthly\n    .groupby(\"date\")[\"symbol\"]\n    .nunique()\n    .reset_index()\n    .rename(columns={\"symbol\": \"n_stocks\"})\n)\n\nplot_stock_counts = (\n    ggplot(stock_counts, aes(x=\"date\", y=\"n_stocks\")) +\n    geom_line(color=\"#1f77b4\") +\n    labs(\n        x=\"\", y=\"Number of stocks\",\n        title=\"Cross-Sectional Breadth Over Time\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_stock_counts\n\n\n\n\n\n\n\nFigure 12.1: Number of stocks with available return data over time. The figure shows the monthly cross-section of stocks available for momentum portfolio construction. A minimum number of stocks is needed to form meaningful decile portfolios.\n\n\n\n\n\n\n\n12.3.3 Loading Factor Data\nWe also load the Fama-French factor returns for risk-adjusted performance evaluation. These factors were constructed in Fama-French Factors.\n\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT date, mkt_excess, smb, hml, risk_free FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Factor observations: {len(factors_ff3_monthly):,}\")\nprint(f\"Date range: {factors_ff3_monthly['date'].min().date()} \"\n      f\"to {factors_ff3_monthly['date'].max().date()}\")\n\nFactor observations: 150\nDate range: 2011-07-31 to 2023-12-31\n\n\n\n\n12.3.4 Data Quality Filters\nMomentum strategies require continuous return histories for the formation period. We apply several filters to ensure data quality:\n\nMinimum price filter: We exclude penny stocks with prices below 1,000 VND, which are subject to extreme microstructure noise.\nReturn availability: We require non-missing returns for the entire formation period.\nMarket capitalization: We require positive lagged market capitalization for portfolio weighting.\n\n\n# Filter for stocks with positive market cap and non-missing returns\nprices_clean = (prices_monthly\n    .dropna(subset=[\"ret\", \"mktcap\"])\n    .query(\"mktcap &gt; 0\")\n    .sort_values([\"symbol\", \"date\"])\n    .reset_index(drop=True)\n)\n\nprint(f\"Observations after filtering: {len(prices_clean):,}\")\nprint(f\"Stocks after filtering: {prices_clean['symbol'].nunique():,}\")\n\nObservations after filtering: 165,499\nStocks after filtering: 1,457",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#momentum-portfolio-construction",
    "href": "13_momemtum.html#momentum-portfolio-construction",
    "title": "12  Momentum Strategies",
    "section": "12.4 Momentum Portfolio Construction",
    "text": "12.4 Momentum Portfolio Construction\n\n12.4.1 Computing Formation Period Returns\nThe first step in constructing momentum portfolios is to compute cumulative returns over the formation period. For a formation period of \\(J\\) months, we compute the cumulative return for each stock \\(i\\) at the end of month \\(t\\) as specified in Equation 12.1.\nWe implement this using a rolling product of gross returns. A key detail is that we require non-missing returns for all \\(J\\) months in the formation window. If any monthly return is missing, the cumulative return is set to missing, and the stock is excluded from portfolio formation in that month.\n\ndef compute_formation_returns(data, J):\n    \"\"\"\n    Compute J-month cumulative returns for momentum portfolio formation.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Panel data with columns 'symbol', 'date', 'ret'.\n    J : int\n        Formation period length in months (typically 3-12).\n    \n    Returns\n    -------\n    pd.DataFrame\n        Original data augmented with 'cum_return' column.\n    \"\"\"\n    df = data.sort_values([\"symbol\", \"date\"]).copy()\n    \n    # Compute rolling J-month cumulative return\n    # Using gross returns: (1+r1)*(1+r2)*...*(1+rJ) - 1\n    df[\"gross_ret\"] = 1 + df[\"ret\"]\n    \n    df[\"cum_return\"] = (\n        df.groupby(\"symbol\")[\"gross_ret\"]\n        .rolling(window=J, min_periods=J)\n        .apply(np.prod, raw=True)\n        .reset_index(level=0, drop=True)\n        - 1\n    )\n    \n    df = df.drop(columns=[\"gross_ret\"])\n    \n    return df\n\nLet us apply this function for our baseline case with \\(J = 6\\) months:\n\nJ = 6  # Formation period: 6 months\n\nprices_with_cumret = compute_formation_returns(prices_clean, J)\n\n# Check the result\nprint(f\"Observations with valid {J}-month cumulative returns: \"\n      f\"{prices_with_cumret['cum_return'].notna().sum():,}\")\nprint(f\"\\nCumulative return distribution:\")\nprint(prices_with_cumret[\"cum_return\"].describe().round(4))\n\nObservations with valid 6-month cumulative returns: 158,227\n\nCumulative return distribution:\ncount    158227.0000\nmean          0.0171\nstd           0.5053\nmin          -0.9999\n25%          -0.2196\n50%          -0.0400\n75%           0.1404\nmax          35.7136\nName: cum_return, dtype: float64\n\n\n\n\n12.4.2 Assigning Momentum Decile Portfolios\nEach month, we sort all stocks with valid cumulative returns into decile portfolios based on their past \\(J\\)-month performance. Portfolio 1 contains the bottom decile (losers) and portfolio 10 contains the top decile (winners).\n\ndef assign_momentum_portfolios(data, n_portfolios=10):\n    \"\"\"\n    Assign stocks to momentum portfolios based on formation-period returns.\n\n    For each cross-section (month), stocks are sorted into \n    n_portfolios quantile groups according to their cumulative return.\n    \n    Portfolio 1 = lowest past returns (Losers)\n    Portfolio n_portfolios = highest past returns (Winners)\n\n    This implementation uses `groupby().transform()` rather than \n    `groupby().apply()` to preserve the original DataFrame structure \n    and avoid index mutation issues.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        Panel data containing at minimum:\n        - 'symbol' : stock identifier\n        - 'date' : formation month\n        - 'cum_return' : cumulative return over formation window\n\n    n_portfolios : int, optional\n        Number of portfolios to form (default = 10 for deciles).\n\n    Returns\n    -------\n    pd.DataFrame\n        Original data augmented with:\n        - 'momr' : integer momentum rank (1 to n_portfolios)\n    \"\"\"\n\n    # Drop observations without formation-period returns\n    # These cannot be ranked into portfolios\n    df = data.dropna(subset=[\"cum_return\"]).copy()\n\n    def safe_qcut(x):\n        \"\"\"\n        Assign quantile portfolio labels within a single cross-section.\n\n        Uses pd.qcut to create approximately equal-sized portfolios.\n        If too few unique return values exist (which can happen in \n        small markets or illiquid samples), fall back to rank-based \n        binning to ensure portfolios are still formed.\n        \"\"\"\n        try:\n            # Standard quantile sorting\n            return pd.qcut(\n                x,\n                q=n_portfolios,\n                labels=range(1, n_portfolios + 1),\n                duplicates=\"drop\"  # prevents crash if quantile edges duplicate\n            )\n        except ValueError:\n            # Fallback: rank then evenly cut into bins\n            return pd.cut(\n                x.rank(method=\"first\"),\n                bins=n_portfolios,\n                labels=range(1, n_portfolios + 1)\n            )\n\n    # Cross-sectional portfolio assignment:\n    # For each month, compute portfolio ranks based on cum_return.\n    # transform ensures:\n    # - original row order preserved\n    # - no index mutation\n    # - no loss of 'date' column\n    df[\"momr\"] = (\n        df.groupby(\"date\")[\"cum_return\"]\n          .transform(safe_qcut)\n          .astype(int)\n    )\n\n    return df\n\n\nportfolios = assign_momentum_portfolios(prices_with_cumret)\n\nprint(f\"Stocks assigned to portfolios: {len(portfolios):,}\")\nprint(f\"\\nPortfolio distribution (should be approximately equal):\")\nprint(portfolios.groupby(\"momr\")[\"symbol\"].count().to_frame(\"count\"))\n\nStocks assigned to portfolios: 158,227\n\nPortfolio distribution (should be approximately equal):\n      count\nmomr       \n1     15894\n2     15815\n3     16021\n4     15759\n5     15738\n6     16568\n7     15288\n8     15582\n9     15697\n10    15865\n\n\n\n\n12.4.3 Defining Holding Period Dates\nAfter forming portfolios at the end of month \\(t\\), we hold them from the beginning of month \\(t+1\\) through the end of month \\(t+K\\). This one-month gap between the formation period and the start of the holding period is standard in the literature and avoids the well-documented short-term reversal effect at the one-month horizon (Jegadeesh 1990).\n\nK = 6  # Holding period: 6 months\n\nportfolios_with_dates = portfolios.copy()\nportfolios_with_dates = portfolios_with_dates.rename(\n    columns={\"date\": \"form_date\"}\n)\n\n# Define holding period start and end dates\nportfolios_with_dates[\"hdate1\"] = (\n    portfolios_with_dates[\"form_date\"] + pd.offsets.MonthBegin(1)\n)\nportfolios_with_dates[\"hdate2\"] = (\n    portfolios_with_dates[\"form_date\"] + pd.offsets.MonthEnd(K)\n)\n\nprint(portfolios_with_dates[\n    [\"symbol\", \"form_date\", \"cum_return\", \"momr\", \"hdate1\", \"hdate2\"]\n].head(10))\n\n   symbol  form_date  cum_return  momr     hdate1     hdate2\n5     A32 2019-04-30   -0.061599     4 2019-05-01 2019-10-31\n6     A32 2019-05-31   -0.213675     2 2019-06-01 2019-11-30\n7     A32 2019-06-30   -0.308720     2 2019-07-01 2019-12-31\n8     A32 2019-07-31   -0.249936     2 2019-08-01 2020-01-31\n9     A32 2019-08-31    0.061986     8 2019-09-01 2020-02-29\n10    A32 2019-09-30    0.030751     8 2019-10-01 2020-03-31\n11    A32 2019-10-31    0.030751     8 2019-11-01 2020-04-30\n12    A32 2019-11-30    0.032486     8 2019-12-01 2020-05-31\n13    A32 2019-12-31    0.180073     9 2020-01-01 2020-06-30\n14    A32 2020-01-31    0.412322    10 2020-02-01 2020-07-31\n\n\n\n\n12.4.4 Computing Portfolio Holding Period Returns\nWe now compute the returns of each portfolio during its holding period. For each stock-formation date combination, we merge in the actual returns realized during the holding window. This produces a panel of stock returns indexed by formation date, holding date, and momentum portfolio rank.\n\n# Merge: for each portfolio assignment, get all returns during holding period\nportfolio_returns = portfolios_with_dates.merge(\n    prices_clean[[\"symbol\", \"date\", \"ret\"]].rename(\n        columns={\"ret\": \"hret\", \"date\": \"hdate\"}\n    ),\n    on=\"symbol\",\n    how=\"inner\"\n)\n\n# Keep only returns within the holding period\nportfolio_returns = portfolio_returns.query(\n    \"hdate &gt;= hdate1 and hdate &lt;= hdate2\"\n)\n\nprint(f\"Portfolio-holding period observations: {len(portfolio_returns):,}\")\n\nPortfolio-holding period observations: 918,072\n\n\n\n\n12.4.5 Computing Equally Weighted Portfolio Returns\nThe Jegadeesh and Titman (1993) methodology uses equally weighted portfolios. For each holding month, each momentum decile has \\(K\\) active cohorts (one formed in each of the past \\(K\\) months). We first compute the equally weighted return within each cohort, then average across cohorts to get the final monthly portfolio return.\n\n# Step 1: Average return within each cohort (form_date × momr × hdate)\ncohort_returns = (portfolio_returns\n    .groupby([\"hdate\", \"momr\", \"form_date\"])\n    .agg(cohort_ret=(\"hret\", \"mean\"))\n    .reset_index()\n)\n\n# Step 2: Average across cohorts for each momentum portfolio × month\newret = (cohort_returns\n    .groupby([\"hdate\", \"momr\"])\n    .agg(\n        ewret=(\"cohort_ret\", \"mean\"),\n        ewret_std=(\"cohort_ret\", \"std\"),\n        n_cohorts=(\"cohort_ret\", \"count\")\n    )\n    .reset_index()\n    .rename(columns={\"hdate\": \"date\"})\n)\n\nprint(f\"Monthly portfolio return observations: {len(ewret):,}\")\nprint(f\"Date range: {ewret['date'].min().date()} to {ewret['date'].max().date()}\")\n\nMonthly portfolio return observations: 1,610\nDate range: 2010-08-31 to 2023-12-31\n\n\nWe should verify that we have the expected number of active cohorts per month. Once the strategy has been running for at least \\(K\\) months, each momentum decile should have exactly \\(K\\) active cohorts.\n\ncohort_check = (ewret\n    .groupby(\"date\")[\"n_cohorts\"]\n    .mean()\n    .reset_index()\n    .rename(columns={\"n_cohorts\": \"avg_cohorts\"})\n)\n\nplot_cohorts = (\n    ggplot(cohort_check, aes(x=\"date\", y=\"avg_cohorts\")) +\n    geom_line(color=\"#1f77b4\") +\n    geom_hline(yintercept=K, linetype=\"dashed\", color=\"red\") +\n    labs(\n        x=\"\", y=\"Average number of cohorts\",\n        title=f\"Active Cohorts per Portfolio (K={K})\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 4))\n)\nplot_cohorts\n\n\n\n\n\n\n\nFigure 12.2: Number of active cohorts per momentum portfolio over time. After an initial ramp-up period of K months, each portfolio should have exactly K active cohorts contributing to its monthly return.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#baseline-results-j6-k6-strategy",
    "href": "13_momemtum.html#baseline-results-j6-k6-strategy",
    "title": "12  Momentum Strategies",
    "section": "12.5 Baseline Results: J=6, K=6 Strategy",
    "text": "12.5 Baseline Results: J=6, K=6 Strategy\n\n12.5.1 Summary Statistics by Momentum Decile\nWe now examine the average monthly returns for each momentum decile portfolio. Table 12.2 presents the mean, \\(t\\)-statistic, and \\(p\\)-value for each of the ten portfolios.\n\ndef compute_portfolio_stats(group):\n    \"\"\"Compute mean, t-stat, and p-value for a return series.\"\"\"\n    n = len(group)\n    mean_ret = group.mean()\n    std_ret = group.std()\n    t_stat = mean_ret / (std_ret / np.sqrt(n)) if std_ret &gt; 0 else np.nan\n    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1)) if not np.isnan(t_stat) else np.nan\n    return pd.Series({\n        \"N\": n,\n        \"Mean (%)\": mean_ret * 100,\n        \"Std (%)\": std_ret * 100,\n        \"t-stat\": t_stat,\n        \"p-value\": p_val\n    })\n\nmomentum_stats = (ewret\n    .groupby(\"momr\")[\"ewret\"]\n    .apply(compute_portfolio_stats)\n    .unstack()\n    .round(4)\n)\n\nmomentum_stats\n\n\n\nTable 12.2: Average monthly returns of momentum decile portfolios for the J=6, K=6 strategy. Portfolio 1 contains past losers and portfolio 10 contains past winners. The table reports the number of months, mean monthly return, t-statistic, and p-value for each decile.\n\n\n\n\n\n\n\n\n\n\nN\nMean (%)\nStd (%)\nt-stat\np-value\n\n\nmomr\n\n\n\n\n\n\n\n\n\n1\n161.0\n1.4190\n6.5319\n2.7565\n0.0065\n\n\n2\n161.0\n0.6602\n5.9895\n1.3985\n0.1639\n\n\n3\n161.0\n0.3658\n5.5080\n0.8427\n0.4007\n\n\n4\n161.0\n0.1623\n5.0085\n0.4112\n0.6815\n\n\n5\n161.0\n0.0111\n4.7561\n0.0297\n0.9763\n\n\n6\n161.0\n0.0408\n4.6864\n0.1104\n0.9122\n\n\n7\n161.0\n-0.0674\n4.8881\n-0.1749\n0.8614\n\n\n8\n161.0\n-0.2066\n4.9208\n-0.5329\n0.5949\n\n\n9\n161.0\n-0.2228\n5.3013\n-0.5332\n0.5946\n\n\n10\n161.0\n-0.6812\n5.7131\n-1.5130\n0.1323\n\n\n\n\n\n\n\n\n\n\n\n\n12.5.2 The Long-Short Momentum Portfolio\nThe key test of the momentum effect is whether the spread between winners and losers—the long-short momentum portfolio—generates statistically significant positive returns. We construct this spread portfolio by going long the top decile (portfolio 10) and short the bottom decile (portfolio 1).\n\n# Pivot to wide format\newret_wide = ewret.pivot(\n    index=\"date\", columns=\"momr\", values=\"ewret\"\n).reset_index()\n\newret_wide.columns = [\"date\"] + [f\"port{i}\" for i in range(1, 11)]\n\n# Compute long-short return\newret_wide[\"winners\"] = ewret_wide[\"port10\"]\newret_wide[\"losers\"] = ewret_wide[\"port1\"]\newret_wide[\"long_short\"] = ewret_wide[\"winners\"] - ewret_wide[\"losers\"]\n\n\nls_stats = pd.DataFrame()\nfor col in [\"winners\", \"losers\", \"long_short\"]:\n    series = ewret_wide[col].dropna()\n    n = len(series)\n    mean_val = series.mean()\n    std_val = series.std()\n    t_val = mean_val / (std_val / np.sqrt(n))\n    p_val = 2 * (1 - stats.t.cdf(abs(t_val), df=n-1))\n    ls_stats[col] = [n, mean_val * 100, std_val * 100, t_val, p_val]\n\nls_stats.index = [\"N\", \"Mean (%)\", \"Std (%)\", \"t-stat\", \"p-value\"]\nls_stats.columns = [\"Winners\", \"Losers\", \"Long-Short\"]\nls_stats = ls_stats.round(4)\nls_stats\n\n\n\nTable 12.3: Performance of the momentum long-short strategy (J=6, K=6). Winners is the top decile, Losers is the bottom decile, and Long-Short is Winners minus Losers. The table reports the number of months, mean monthly return, t-statistic, and p-value.\n\n\n\n\n\n\n\n\n\n\nWinners\nLosers\nLong-Short\n\n\n\n\nN\n161.0000\n161.0000\n161.0000\n\n\nMean (%)\n-0.6812\n1.4190\n-2.1002\n\n\nStd (%)\n5.7131\n6.5319\n4.8969\n\n\nt-stat\n-1.5130\n2.7565\n-5.4420\n\n\np-value\n0.1323\n0.0065\n0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n12.5.3 Cumulative Returns\nThe time-series evolution of cumulative returns provides visual evidence of the momentum strategy’s performance. Figure 12.3 shows the cumulative returns of the winner and loser portfolios, while Figure 12.4 shows the cumulative return of the long-short momentum strategy.\n\n# Compute cumulative returns\newret_wide = ewret_wide.sort_values(\"date\").reset_index(drop=True)\n\nfor col in [\"winners\", \"losers\", \"long_short\"]:\n    ewret_wide[f\"cumret_{col}\"] = (1 + ewret_wide[col]).cumprod() - 1\n\n\ncumret_plot_data = (ewret_wide\n    [[\"date\", \"cumret_winners\", \"cumret_losers\"]]\n    .melt(id_vars=\"date\", var_name=\"portfolio\", value_name=\"cumret\")\n)\ncumret_plot_data[\"portfolio\"] = cumret_plot_data[\"portfolio\"].map({\n    \"cumret_winners\": \"Winners (P10)\",\n    \"cumret_losers\": \"Losers (P1)\"\n})\n\nplot_cumret = (\n    ggplot(cumret_plot_data, \n           aes(x=\"date\", y=\"cumret\", color=\"portfolio\")) +\n    geom_line(size=1) +\n    scale_y_continuous(labels=percent_format()) +\n    scale_color_manual(values=[\"#d62728\", \"#1f77b4\"]) +\n    labs(\n        x=\"\", y=\"Cumulative return\",\n        color=\"Portfolio\",\n        title=\"Cumulative Returns: Winners vs. Losers\"\n    ) +\n    theme_minimal() +\n    theme(\n        figure_size=(10, 6),\n        legend_position=\"bottom\"\n    )\n)\nplot_cumret\n\n\n\n\n\n\n\nFigure 12.3: Cumulative returns of momentum winner and loser portfolios. The winner portfolio (blue) contains stocks in the top decile of past 6-month returns, and the loser portfolio (red) contains stocks in the bottom decile. The spread between the two lines reflects the cumulative momentum premium.\n\n\n\n\n\n\nplot_ls = (\n    ggplot(ewret_wide, aes(x=\"date\", y=\"cumret_long_short\")) +\n    geom_line(size=1, color=\"#2ca02c\") +\n    geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\") +\n    scale_y_continuous(labels=percent_format()) +\n    labs(\n        x=\"\", y=\"Cumulative return\",\n        title=\"Cumulative Return: Long-Short Momentum Strategy\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 6))\n)\nplot_ls\n\n\n\n\n\n\n\nFigure 12.4: Cumulative return of the momentum long-short strategy (Winners minus Losers). Upward-sloping periods indicate momentum profits; sharp drawdowns often coincide with market reversals or crisis episodes.\n\n\n\n\n\n\n\n12.5.4 Monthly Return Distribution\nBeyond the mean, the full distribution of monthly long-short returns provides insight into the risk profile of the momentum strategy. Figure 12.5 shows the histogram of monthly returns.\n\nmean_ls = ewret_wide[\"long_short\"].mean()\n\nplot_dist = (\n    ggplot(ewret_wide, aes(x=\"long_short\")) +\n    geom_histogram(bins=50, fill=\"#1f77b4\", alpha=0.7) +\n    geom_vline(xintercept=mean_ls, linetype=\"dashed\", color=\"red\") +\n    scale_x_continuous(labels=percent_format()) +\n    labs(\n        x=\"Monthly long-short return\",\n        y=\"Frequency\",\n        title=\"Distribution of Monthly Momentum Returns\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_dist\n\n\n\n\n\n\n\nFigure 12.5: Distribution of monthly long-short momentum returns. The histogram shows the frequency distribution of monthly Winners-minus-Losers returns. The vertical dashed line indicates the mean monthly return.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#extending-to-multiple-formation-and-holding-periods",
    "href": "13_momemtum.html#extending-to-multiple-formation-and-holding-periods",
    "title": "12  Momentum Strategies",
    "section": "12.6 Extending to Multiple Formation and Holding Periods",
    "text": "12.6 Extending to Multiple Formation and Holding Periods\n\n12.6.1 The J × K Grid\nJegadeesh and Titman (1993) evaluate momentum strategies across a comprehensive grid of formation periods \\(J \\in \\{3, 6, 9, 12\\}\\) and holding periods \\(K \\in \\{3, 6, 9, 12\\}\\). This produces 16 different strategy specifications, allowing us to assess the robustness of the momentum effect across different horizons.\nWe now implement a function that computes the full momentum strategy for any given \\((J, K)\\) pair, wrapping the steps developed above into a single reusable pipeline.\n\nfrom joblib import Parallel, delayed\nimport time\n\ndef momentum_strategy(data, J, K, n_portfolios=10):\n    \"\"\"\n    Implement the full Jegadeesh-Titman momentum strategy.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Panel of stock returns with columns: symbol, date, ret.\n    J : int\n        Formation period in months.\n    K : int\n        Holding period in months.\n    n_portfolios : int\n        Number of portfolios (default: 10 for deciles).\n    \n    Returns\n    -------\n    pd.DataFrame\n        Monthly returns for each momentum portfolio and the long-short spread.\n    \"\"\"\n    # Step 1: Compute formation period cumulative returns\n    df = data.sort_values([\"symbol\", \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[\"ret\"]\n    df[\"cum_return\"] = (\n        df.groupby(\"symbol\")[\"gross_ret\"]\n        .rolling(window=J, min_periods=J)\n        .apply(np.prod, raw=True)\n        .reset_index(level=0, drop=True)\n        - 1\n    )\n    df = df.drop(columns=[\"gross_ret\"]).dropna(subset=[\"cum_return\"])\n    \n    # Step 2: Assign to momentum portfolios using transform\n    df[\"momr\"] = df.groupby(\"date\", observed=True)[\"cum_return\"].transform(\n        lambda x: pd.qcut(\n            x,\n            q=n_portfolios,\n            labels=range(1, n_portfolios + 1),\n            duplicates=\"drop\"\n        )\n    ).astype('Int64')\n    \n    # If any NaNs in momr, fill with rank-based assignment\n    mask = df[\"momr\"].isna()\n    if mask.any():\n        df.loc[mask, \"momr\"] = df.loc[mask].groupby(\"date\")[\"cum_return\"].transform(\n            lambda x: pd.qcut(\n                x.rank(method=\"first\"),\n                q=min(n_portfolios, len(x.unique())),\n                labels=False,\n                duplicates=\"drop\"\n            )\n        ).astype('Int64') + 1\n    \n    # Step 3: Define holding period dates\n    df = df.rename(columns={\"date\": \"form_date\"})\n    df[\"hdate1\"] = df[\"form_date\"] + pd.offsets.MonthBegin(1)\n    df[\"hdate2\"] = df[\"form_date\"] + pd.offsets.MonthEnd(K)\n    \n    # Step 4: Merge with holding period returns\n    port_ret = df[[\"symbol\", \"form_date\", \"momr\", \"hdate1\", \"hdate2\"]].merge(\n        data[[\"symbol\", \"date\", \"ret\"]].rename(\n            columns={\"ret\": \"hret\", \"date\": \"hdate\"}\n        ),\n        on=\"symbol\",\n        how=\"inner\"\n    )\n    \n    # Use boolean indexing\n    port_ret = port_ret[\n        (port_ret[\"hdate\"] &gt;= port_ret[\"hdate1\"]) & \n        (port_ret[\"hdate\"] &lt;= port_ret[\"hdate2\"])\n    ]\n    \n    # Step 5: Compute equally weighted returns (two-stage averaging)\n    cohort_ret = (port_ret\n        .groupby([\"hdate\", \"momr\", \"form_date\"])\n        .agg(cohort_ret=(\"hret\", \"mean\"))\n        .reset_index()\n    )\n    \n    monthly_ret = (cohort_ret\n        .groupby([\"hdate\", \"momr\"])\n        .agg(ewret=(\"cohort_ret\", \"mean\"))\n        .reset_index()\n        .rename(columns={\"hdate\": \"date\"})\n    )\n    \n    # Step 6: Compute long-short spread\n    wide = monthly_ret.pivot(\n        index=\"date\", columns=\"momr\", values=\"ewret\"\n    ).reset_index()\n    \n    # Handle variable number of portfolios\n    n_cols = len(wide.columns) - 1\n    wide.columns = [\"date\"] + [f\"port{i}\" for i in range(1, n_cols + 1)]\n    wide[\"winners\"] = wide[f\"port{n_cols}\"]\n    wide[\"losers\"] = wide[\"port1\"]\n    wide[\"long_short\"] = wide[\"winners\"] - wide[\"losers\"]\n    \n    return monthly_ret, wide\n\n\n\n12.6.2 Running the Full Grid\nWe now run the momentum strategy for all 16 \\((J, K)\\) combinations. This is computationally intensive, so we store the key summary statistics for each specification.\n\n12.6.2.1 Sequential Calculation\n\nJ_values = [3, 6, 9, 12]\nK_values = [3, 6, 9, 12]\n\nresults_grid = []\n\nfor J_val, K_val in product(J_values, K_values):\n    print(f\"Computing J={J_val}, K={K_val}...\", end=\" \")\n    \n    try:\n        _, wide_result = momentum_strategy(prices_clean, J_val, K_val)\n        \n        for portfolio_name in [\"winners\", \"losers\", \"long_short\"]:\n            series = wide_result[portfolio_name].dropna()\n            n = len(series)\n            mean_ret = series.mean()\n            std_ret = series.std()\n            t_stat = mean_ret / (std_ret / np.sqrt(n)) if std_ret &gt; 0 else np.nan\n            p_val = (2 * (1 - stats.t.cdf(abs(t_stat), df=n-1)) \n                     if not np.isnan(t_stat) else np.nan)\n            \n            results_grid.append({\n                \"J\": J_val,\n                \"K\": K_val,\n                \"portfolio\": portfolio_name,\n                \"n_months\": n,\n                \"mean_ret\": mean_ret,\n                \"std_ret\": std_ret,\n                \"t_stat\": t_stat,\n                \"p_value\": p_val\n            })\n        \n        print(\"Done.\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nresults_df = pd.DataFrame(results_grid)\n\n\n\n12.6.2.2 Parallel Calculation\n\ndef compute_single_strategy(data, J, K):\n    \"\"\"\n    Compute statistics for a single (J, K) strategy.\n    Returns a list of result dicts, one per portfolio.\n    \"\"\"\n    try:\n        _, wide_result = momentum_strategy(data, J, K)\n        \n        results = []\n        for portfolio_name in [\"winners\", \"losers\", \"long_short\"]:\n            series = wide_result[portfolio_name].dropna()\n            n = len(series)\n            mean_ret = series.mean()\n            std_ret = series.std()\n            t_stat = mean_ret / (std_ret / np.sqrt(n)) if std_ret &gt; 0 else np.nan\n            p_val = (2 * (1 - stats.t.cdf(abs(t_stat), df=n-1)) \n                     if not np.isnan(t_stat) else np.nan)\n            \n            results.append({\n                \"J\": J,\n                \"K\": K,\n                \"portfolio\": portfolio_name,\n                \"n_months\": n,\n                \"mean_ret\": mean_ret,\n                \"std_ret\": std_ret,\n                \"t_stat\": t_stat,\n                \"p_value\": p_val\n            })\n        return results\n    except Exception as e:\n        print(f\"Error in J={J}, K={K}: {e}\")\n        return []\n\n\nJ_values = [3, 6, 9, 12]\nK_values = [3, 6, 9, 12]\n\n# Create list of (J, K) pairs\nparams = list(product(J_values, K_values))\n\nprint(f\"Running {len(params)} momentum strategies in parallel with 4 cores...\")\nstart_time = time.time()\n\n# Parallel execution with 4 workers\nresults_list = Parallel(n_jobs=4, verbose=10)(\n    delayed(compute_single_strategy)(prices_clean, J, K)\n    for J, K in params\n)\n\n# Flatten results\nresults_grid = [item for sublist in results_list for item in sublist]\nresults_df = pd.DataFrame(results_grid)\n\nelapsed = time.time() - start_time\nprint(f\"\\nCompleted in {elapsed:.2f} seconds\")\n\nRunning 16 momentum strategies in parallel with 4 cores...\n\n\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    8.9s\n[Parallel(n_jobs=4)]: Done  11 out of  16 | elapsed:   14.6s remaining:    6.6s\n[Parallel(n_jobs=4)]: Done  13 out of  16 | elapsed:   19.4s remaining:    4.5s\n\n\n\nCompleted in 20.20 seconds\n\n\n[Parallel(n_jobs=4)]: Done  16 out of  16 | elapsed:   20.1s finished\n\n\n\n# Summary statistics\nprint(\"\\nResults Summary by Formation Period:\")\nprint(results_df.groupby(\"J\").agg({\n    \"mean_ret\": \"mean\",\n    \"std_ret\": \"mean\",\n    \"t_stat\": [\"mean\", lambda x: (x.abs() &gt; 1.96).sum()],\n    \"n_months\": \"first\"\n}).round(6))\n\n\nResults Summary by Formation Period:\n    mean_ret   std_ret    t_stat            n_months\n        mean      mean      mean &lt;lambda_0&gt;    first\nJ                                                   \n6  -0.004661  0.055672 -1.517507          9      161\n9  -0.003265  0.056520 -1.071904          9      158\n12 -0.002759  0.058194 -0.931273          9      155\n\n\n\n\n\n12.6.3 Winners Portfolio Returns\nTable 12.4 presents the average monthly returns of the winner portfolio (portfolio 10) across all \\((J, K)\\) combinations.\n\nwinners_grid = (results_df\n    .query(\"portfolio == 'winners'\")\n    .assign(\n        display=lambda x: (\n            x[\"mean_ret\"].apply(lambda v: f\"{v*100:.2f}\") + \n            \"\\n(\" + x[\"t_stat\"].apply(lambda v: f\"{v:.2f}\") + \")\"\n        )\n    )\n    .pivot(index=\"J\", columns=\"K\", values=\"display\")\n)\nwinners_grid.columns = [f\"K={k}\" for k in winners_grid.columns]\nwinners_grid.index = [f\"J={j}\" for j in winners_grid.index]\nwinners_grid\n\n\n\nTable 12.4: Average monthly returns (%) of the winner portfolio across formation periods (J) and holding periods (K). t-statistics are reported in parentheses.\n\n\n\n\n\n\n\n\n\n\nK=3\nK=6\nK=9\nK=12\n\n\n\n\nJ=6\n-1.18\\n(-2.65)\n-0.68\\n(-1.51)\n-0.55\\n(-1.23)\n-0.39\\n(-0.87)\n\n\nJ=9\n-1.00\\n(-2.36)\n-0.51\\n(-1.22)\n-0.28\\n(-0.68)\n-0.16\\n(-0.39)\n\n\nJ=12\n-0.88\\n(-2.09)\n-0.39\\n(-0.91)\n-0.24\\n(-0.56)\n-0.14\\n(-0.34)\n\n\n\n\n\n\n\n\n\n\n\n\n12.6.4 Losers Portfolio Returns\nTable 12.5 presents the corresponding results for the loser portfolio.\n\nlosers_grid = (results_df\n    .query(\"portfolio == 'losers'\")\n    .assign(\n        display=lambda x: (\n            x[\"mean_ret\"].apply(lambda v: f\"{v*100:.2f}\") + \n            \"\\n(\" + x[\"t_stat\"].apply(lambda v: f\"{v:.2f}\") + \")\"\n        )\n    )\n    .pivot(index=\"J\", columns=\"K\", values=\"display\")\n)\nlosers_grid.columns = [f\"K={k}\" for k in losers_grid.columns]\nlosers_grid.index = [f\"J={j}\" for j in losers_grid.index]\nlosers_grid\n\n\n\nTable 12.5: Average monthly returns (%) of the loser portfolio across formation periods (J) and holding periods (K). t-statistics are reported in parentheses.\n\n\n\n\n\n\n\n\n\n\nK=3\nK=6\nK=9\nK=12\n\n\n\n\nJ=6\n1.87\\n(3.49)\n1.42\\n(2.76)\n1.10\\n(2.19)\n0.99\\n(1.99)\n\n\nJ=9\n1.94\\n(3.51)\n1.38\\n(2.56)\n1.21\\n(2.31)\n1.15\\n(2.22)\n\n\nJ=12\n1.57\\n(2.71)\n1.29\\n(2.28)\n1.19\\n(2.13)\n1.15\\n(2.09)\n\n\n\n\n\n\n\n\n\n\n\n\n12.6.5 Long-Short Momentum Returns\nTable 12.6 presents the most important results: the average monthly returns of the long-short momentum portfolio across all specifications.\n\nls_grid = (results_df\n    .query(\"portfolio == 'long_short'\")\n    .assign(\n        display=lambda x: (\n            x[\"mean_ret\"].apply(lambda v: f\"{v*100:.2f}\") + \n            \"\\n(\" + x[\"t_stat\"].apply(lambda v: f\"{v:.2f}\") + \")\"\n        )\n    )\n    .pivot(index=\"J\", columns=\"K\", values=\"display\")\n)\nls_grid.columns = [f\"K={k}\" for k in ls_grid.columns]\nls_grid.index = [f\"J={j}\" for j in ls_grid.index]\nls_grid\n\n\n\nTable 12.6: Average monthly returns (%) of the momentum long-short portfolio (Winners minus Losers) across formation periods (J) and holding periods (K). t-statistics are reported in parentheses. This table replicates the format of Table 1 in Jegadeesh and Titman (1993).\n\n\n\n\n\n\n\n\n\n\nK=3\nK=6\nK=9\nK=12\n\n\n\n\nJ=6\n-3.04\\n(-7.38)\n-2.10\\n(-5.44)\n-1.65\\n(-4.94)\n-1.37\\n(-4.61)\n\n\nJ=9\n-2.94\\n(-6.41)\n-1.89\\n(-4.55)\n-1.49\\n(-3.98)\n-1.31\\n(-3.87)\n\n\nJ=12\n-2.45\\n(-5.42)\n-1.68\\n(-3.92)\n-1.43\\n(-3.60)\n-1.30\\n(-3.55)\n\n\n\n\n\n\n\n\n\n\n\n\n12.6.6 Visualizing the Momentum Premium Across Specifications\nFigure 12.6 provides a visual summary of the long-short momentum premium across all \\((J, K)\\) combinations.\n\nls_heatmap_data = (results_df\n    .query(\"portfolio == 'long_short'\")\n    .assign(\n        mean_pct=lambda x: x[\"mean_ret\"] * 100,\n        significant=lambda x: x[\"p_value\"] &lt; 0.05,\n        label=lambda x: x.apply(\n            lambda row: f\"{row['mean_ret']*100:.2f}{'*' if row['p_value'] &lt; 0.05 else ''}\", \n            axis=1\n        )\n    )\n)\n\nplot_heatmap = (\n    ggplot(ls_heatmap_data, \n           aes(x=\"K.astype(str)\", y=\"J.astype(str)\", fill=\"mean_pct\")) +\n    geom_tile(color=\"white\", size=2) +\n    geom_text(aes(label=\"label\"), size=10, color=\"white\") +\n    scale_fill_gradient2(\n        low=\"#d62728\", mid=\"#f7f7f7\", high=\"#1f77b4\", midpoint=0,\n        name=\"Monthly\\nReturn (%)\"\n    ) +\n    labs(\n        x=\"Holding Period (K months)\",\n        y=\"Formation Period (J months)\",\n        title=\"Momentum Premium Across J×K Specifications\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(8, 6))\n)\nplot_heatmap\n\n\n\n\n\n\n\nFigure 12.6: Heatmap of average monthly long-short momentum returns (%) across formation periods (J) and holding periods (K). Darker shades indicate higher momentum profits. Asterisks denote statistical significance at the 5% level.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#risk-adjusted-performance",
    "href": "13_momemtum.html#risk-adjusted-performance",
    "title": "12  Momentum Strategies",
    "section": "12.7 Risk-Adjusted Performance",
    "text": "12.7 Risk-Adjusted Performance\n\n12.7.1 CAPM Alpha\nA natural question is whether the momentum premium is explained by exposure to the market factor. We estimate the CAPM alpha of the long-short momentum portfolio by regressing its returns on the market excess return:\n\\[\nr_{\\text{WML},t} = \\alpha + \\beta \\cdot r_{\\text{MKT},t} + \\epsilon_t\n\\tag{12.3}\\]\n\n# Merge momentum returns with factor data (baseline J=6, K=6)\newret_factors = (ewret_wide\n    [[\"date\", \"winners\", \"losers\", \"long_short\"]]\n    .merge(factors_ff3_monthly, on=\"date\", how=\"inner\")\n)\n\n# CAPM regression for long-short portfolio\nfrom statsmodels.formula.api import ols as ols_formula\nimport statsmodels.api as sm\n\nX_capm = sm.add_constant(ewret_factors[\"mkt_excess\"])\ny_ls = ewret_factors[\"long_short\"]\n\ncapm_model = sm.OLS(y_ls, X_capm).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n\nprint(\"CAPM Regression: Long-Short Momentum Returns\")\nprint(\"=\" * 60)\nprint(f\"Alpha (monthly):  {capm_model.params['const']*100:.4f}% \"\n      f\"(t={capm_model.tvalues['const']:.2f})\")\nprint(f\"Market Beta:      {capm_model.params['mkt_excess']:.4f} \"\n      f\"(t={capm_model.tvalues['mkt_excess']:.2f})\")\nprint(f\"R-squared:        {capm_model.rsquared:.4f}\")\nprint(f\"N observations:   {capm_model.nobs:.0f}\")\n\nCAPM Regression: Long-Short Momentum Returns\n============================================================\nAlpha (monthly):  -2.2703% (t=-5.05)\nMarket Beta:      -0.1779 (t=-1.75)\nR-squared:        0.0470\nN observations:   150\n\n\n\n\n12.7.2 Fama-French Three-Factor Alpha\nWe extend the risk adjustment to the Fama-French three-factor model, which includes the size (SMB) and value (HML) factors in addition to the market factor:\n\\[\nr_{\\text{WML},t} = \\alpha + \\beta_1 \\cdot r_{\\text{MKT},t} + \\beta_2 \\cdot \\text{SMB}_t + \\beta_3 \\cdot \\text{HML}_t + \\epsilon_t\n\\tag{12.4}\\]\n\nX_ff3 = sm.add_constant(\n    ewret_factors[[\"mkt_excess\", \"smb\", \"hml\"]]\n)\ny_ls = ewret_factors[\"long_short\"]\n\nff3_model = sm.OLS(y_ls, X_ff3).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n\n# Display results as a clean table\nff3_results = pd.DataFrame({\n    \"Coefficient\": ff3_model.params,\n    \"Std Error\": ff3_model.bse,\n    \"t-stat\": ff3_model.tvalues,\n    \"p-value\": ff3_model.pvalues\n}).round(4)\n\nff3_results.index = [\"Alpha\", \"MKT\", \"SMB\", \"HML\"]\nff3_results\n\n\n\nTable 12.7: Fama-French three-factor regression for the momentum long-short portfolio. The dependent variable is the monthly return of the Winners-minus-Losers portfolio. Alpha is the intercept, representing the risk-adjusted abnormal return. HAC standard errors with 6 lags are used.\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd Error\nt-stat\np-value\n\n\n\n\nAlpha\n-0.0234\n0.0041\n-5.7189\n0.0000\n\n\nMKT\n-0.1269\n0.1558\n-0.8145\n0.4154\n\n\nSMB\n0.1123\n0.1882\n0.5969\n0.5506\n\n\nHML\n0.0716\n0.1414\n0.5065\n0.6125\n\n\n\n\n\n\n\n\n\n\n\nprint(f\"\\nR-squared: {ff3_model.rsquared:.4f}\")\nprint(f\"Adjusted R-squared: {ff3_model.rsquared_adj:.4f}\")\nprint(f\"Alpha (annualized): {ff3_model.params['const'] * 12 * 100:.2f}%\")\n\n\nR-squared: 0.0553\nAdjusted R-squared: 0.0359\nAlpha (annualized): -28.02%\n\n\n\n\n12.7.3 Interpretation of Risk Exposures\nThe factor loadings from the three-factor regression reveal the risk characteristics of the momentum strategy in the Vietnamese market. Several patterns are commonly observed:\n\nMarket beta: Momentum portfolios typically have moderate market exposure. In the U.S., Grundy and Martin (2001) document that the market beta of the long-short portfolio is close to zero on average but highly time-varying, spiking during market reversals.\nSize exposure (SMB): Momentum strategies often load positively on the size factor, reflecting the tendency for smaller stocks to exhibit stronger momentum patterns.\nValue exposure (HML): The long-short momentum portfolio typically loads negatively on HML, indicating that winners tend to be growth stocks while losers tend to be value stocks. This creates a natural tension between momentum and value strategies.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#momentum-and-market-states",
    "href": "13_momemtum.html#momentum-and-market-states",
    "title": "12  Momentum Strategies",
    "section": "12.8 Momentum and Market States",
    "text": "12.8 Momentum and Market States\n\n12.8.1 Conditional Performance\nAn important finding in the momentum literature is that momentum profits vary with market conditions. Cooper, Gutierrez Jr, and Hameed (2004) document that momentum strategies perform well following market gains (UP markets) but experience severe losses following market declines (DOWN markets). This asymmetry is particularly relevant for emerging markets, which experience more extreme market states.\nWe define market states based on the cumulative market return over the prior 12 months:\n\\[\n\\text{Market State}_t = \\begin{cases} \\text{UP} & \\text{if } \\prod_{s=t-12}^{t-1}(1 + r_{m,s}) - 1 &gt; 0 \\\\ \\text{DOWN} & \\text{otherwise} \\end{cases}\n\\tag{12.5}\\]\n\n# Compute 12-month lagged market return\nmarket_returns = factors_ff3_monthly[[\"date\", \"mkt_excess\"]].copy()\nmarket_returns = market_returns.sort_values(\"date\")\nmarket_returns[\"mkt_cum_12m\"] = (\n    (1 + market_returns[\"mkt_excess\"])\n    .rolling(window=12, min_periods=12)\n    .apply(np.prod, raw=True)\n    - 1\n)\nmarket_returns[\"market_state\"] = np.where(\n    market_returns[\"mkt_cum_12m\"] &gt; 0, \"UP\", \"DOWN\"\n)\n\n# Merge with momentum returns\newret_states = ewret_wide[[\"date\", \"long_short\"]].merge(\n    market_returns[[\"date\", \"market_state\"]], on=\"date\", how=\"inner\"\n).dropna()\n\n\nstate_stats = []\nfor state in [\"UP\", \"DOWN\"]:\n    subset = ewret_states.query(f\"market_state == '{state}'\")[\"long_short\"]\n    n = len(subset)\n    mean_ret = subset.mean()\n    std_ret = subset.std()\n    t_stat = mean_ret / (std_ret / np.sqrt(n)) if std_ret &gt; 0 else np.nan\n    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1)) if not np.isnan(t_stat) else np.nan\n    state_stats.append({\n        \"Market State\": state,\n        \"N Months\": n,\n        \"Mean (%)\": mean_ret * 100,\n        \"Std (%)\": std_ret * 100,\n        \"t-stat\": t_stat,\n        \"p-value\": p_val\n    })\n\nstate_stats_df = pd.DataFrame(state_stats).round(4)\nstate_stats_df\n\n\n\nTable 12.8: Momentum long-short returns conditional on market states. UP markets are defined as periods where the cumulative market return over the prior 12 months is positive; DOWN markets are periods with negative prior 12-month returns.\n\n\n\n\n\n\n\n\n\n\nMarket State\nN Months\nMean (%)\nStd (%)\nt-stat\np-value\n\n\n\n\n0\nUP\n39\n-1.1972\n4.5814\n-1.6320\n0.1109\n\n\n1\nDOWN\n111\n-2.4050\n4.8669\n-5.2063\n0.0000\n\n\n\n\n\n\n\n\n\n\n\nplot_states = (\n    ggplot(ewret_states, aes(x=\"market_state\", y=\"long_short\", \n                              fill=\"market_state\")) +\n    geom_boxplot(alpha=0.7) +\n    geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\") +\n    scale_y_continuous(labels=percent_format()) +\n    scale_fill_manual(values={\"UP\": \"#1f77b4\", \"DOWN\": \"#d62728\"}) +\n    labs(\n        x=\"Market State (Prior 12-Month Return)\",\n        y=\"Monthly Long-Short Return\",\n        title=\"Momentum Returns by Market State\"\n    ) +\n    theme_minimal() +\n    theme(\n        figure_size=(8, 6),\n        legend_position=\"none\"\n    )\n)\nplot_states\n\n\n\n\n\n\n\nFigure 12.7: Distribution of monthly momentum returns by market state. The box plots show the distribution of long-short momentum returns separately for UP and DOWN market states, defined by the sign of the prior 12-month cumulative market return.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#momentum-crashes",
    "href": "13_momemtum.html#momentum-crashes",
    "title": "12  Momentum Strategies",
    "section": "12.9 Momentum Crashes",
    "text": "12.9 Momentum Crashes\n\n12.9.1 Understanding Momentum Drawdowns\nOne of the most important risk characteristics of momentum strategies is their susceptibility to sudden, severe losses—known as momentum crashes. Daniel and Moskowitz (2016) document that momentum strategies experience infrequent but extreme losses, typically during market rebounds following bear markets. These crashes occur because the loser portfolio, which has been short, is heavily loaded with high-beta stocks that surge when markets reverse.\nWe identify the worst drawdowns of the momentum strategy and examine their market context.\n\nworst_months = (ewret_wide\n    [[\"date\", \"long_short\", \"winners\", \"losers\"]]\n    .merge(factors_ff3_monthly[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"left\")\n    .sort_values(\"long_short\")\n    .head(10)\n    .assign(\n        long_short_pct=lambda x: (x[\"long_short\"] * 100).round(2),\n        winners_pct=lambda x: (x[\"winners\"] * 100).round(2),\n        losers_pct=lambda x: (x[\"losers\"] * 100).round(2),\n        mkt_pct=lambda x: (x[\"mkt_excess\"] * 100).round(2)\n    )\n    [[\"date\", \"long_short_pct\", \"winners_pct\", \"losers_pct\", \"mkt_pct\"]]\n    .rename(columns={\n        \"date\": \"Date\",\n        \"long_short_pct\": \"L/S (%)\",\n        \"winners_pct\": \"Winners (%)\",\n        \"losers_pct\": \"Losers (%)\",\n        \"mkt_pct\": \"Market (%)\"\n    })\n)\nworst_months\n\n\n\nTable 12.9: Ten worst months for the momentum long-short strategy. The table reports the date, long-short return, winner return, loser return, and concurrent market return for the ten months with the largest momentum losses.\n\n\n\n\n\n\n\n\n\n\nDate\nL/S (%)\nWinners (%)\nLosers (%)\nMarket (%)\n\n\n\n\n153\n2023-05-31\n-16.39\n-2.69\n13.70\n2.78\n\n\n39\n2013-11-30\n-14.26\n4.24\n18.50\n2.57\n\n\n20\n2012-04-30\n-13.68\n1.51\n15.19\n4.88\n\n\n78\n2017-02-28\n-13.56\n2.10\n15.67\n1.49\n\n\n107\n2019-07-31\n-13.15\n-2.46\n10.69\n1.61\n\n\n140\n2022-04-30\n-11.71\n-17.57\n-5.87\n-11.19\n\n\n149\n2023-01-31\n-11.53\n1.37\n12.90\n6.96\n\n\n28\n2012-12-31\n-11.52\n4.03\n15.54\n-1.61\n\n\n0\n2010-08-31\n-11.31\n-25.03\n-13.72\nNaN\n\n\n10\n2011-06-30\n-10.44\n-3.15\n7.29\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n12.9.2 Maximum Drawdown Analysis\nThe maximum drawdown provides a measure of the worst peak-to-trough decline experienced by the strategy. This metric is particularly relevant for practitioners evaluating the risk of momentum strategies.\n\n# Compute running maximum and drawdown\newret_wide[\"cum_wealth\"] = (1 + ewret_wide[\"long_short\"]).cumprod()\newret_wide[\"running_max\"] = ewret_wide[\"cum_wealth\"].cummax()\newret_wide[\"drawdown\"] = (\n    ewret_wide[\"cum_wealth\"] / ewret_wide[\"running_max\"] - 1\n)\n\nmax_dd = ewret_wide[\"drawdown\"].min()\nmax_dd_date = ewret_wide.loc[ewret_wide[\"drawdown\"].idxmin(), \"date\"]\n\nprint(f\"Maximum drawdown: {max_dd*100:.2f}%\")\nprint(f\"Date of maximum drawdown: {max_dd_date.date()}\")\n\nMaximum drawdown: -97.08%\nDate of maximum drawdown: 2023-10-31\n\n\n\nplot_dd = (\n    ggplot(ewret_wide, aes(x=\"date\", y=\"drawdown\")) +\n    geom_area(fill=\"#d62728\", alpha=0.5) +\n    geom_line(color=\"#d62728\", size=0.5) +\n    scale_y_continuous(labels=percent_format()) +\n    labs(\n        x=\"\", y=\"Drawdown\",\n        title=\"Momentum Strategy Drawdown\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_dd\n\n\n\n\n\n\n\nFigure 12.8: Drawdown of the momentum long-short strategy over time. The chart shows the percentage decline from the previous peak in cumulative wealth. Deeper drawdowns represent more severe momentum crashes.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#value-weighted-momentum-portfolios",
    "href": "13_momemtum.html#value-weighted-momentum-portfolios",
    "title": "12  Momentum Strategies",
    "section": "12.10 Value-Weighted Momentum Portfolios",
    "text": "12.10 Value-Weighted Momentum Portfolios\nThe baseline Jegadeesh and Titman (1993) implementation uses equally weighted portfolios. However, equally weighted returns can be dominated by small, illiquid stocks that may be difficult to trade in practice. Value-weighted portfolios, where each stock’s contribution is proportional to its market capitalization, provide a more investable benchmark and are more representative of the returns that large investors could actually achieve.\n\ndef momentum_strategy_vw(data, J, K, n_portfolios=10):\n    \"\"\"\n    Value-weighted momentum strategy implementation.\n    \n    Same as the equally-weighted version but uses lagged market \n    capitalization as weights when computing portfolio returns.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Panel with columns: symbol, date, ret, mktcap_lag.\n    J : int\n        Formation period in months.\n    K : int\n        Holding period in months.\n    n_portfolios : int\n        Number of portfolios.\n    \n    Returns\n    -------\n    pd.DataFrame\n        Monthly value-weighted portfolio returns.\n    \"\"\"\n    # Step 1: Formation period returns\n    df = data.sort_values([\"symbol\", \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[\"ret\"]\n    df[\"cum_return\"] = (\n        df.groupby(\"symbol\")[\"gross_ret\"]\n        .rolling(window=J, min_periods=J)\n        .apply(np.prod, raw=True)\n        .reset_index(level=0, drop=True)\n        - 1\n    )\n    df = df.drop(columns=[\"gross_ret\"]).dropna(subset=[\"cum_return\"])\n    \n    # Step 2: Portfolio assignment using transform (fast)\n    df[\"momr\"] = df.groupby(\"date\", observed=True)[\"cum_return\"].transform(\n        lambda x: pd.qcut(\n            x,\n            q=n_portfolios,\n            labels=range(1, n_portfolios + 1),\n            duplicates=\"drop\"\n        )\n    ).astype('Int64')\n    \n    # Fill NaNs with rank-based assignment\n    mask = df[\"momr\"].isna()\n    if mask.any():\n        df.loc[mask, \"momr\"] = df.loc[mask].groupby(\"date\")[\"cum_return\"].transform(\n            lambda x: pd.qcut(\n                x.rank(method=\"first\"),\n                q=min(n_portfolios, len(x.unique())),\n                labels=False,\n                duplicates=\"drop\"\n            )\n        ).astype('Int64') + 1\n    \n    # Step 3: Holding period\n    df = df.rename(columns={\"date\": \"form_date\"})\n    df[\"hdate1\"] = df[\"form_date\"] + pd.offsets.MonthBegin(1)\n    df[\"hdate2\"] = df[\"form_date\"] + pd.offsets.MonthEnd(K)\n    \n    # Step 4: Merge with holding period returns AND weights\n    port_ret = df[\n        [\"symbol\", \"form_date\", \"momr\", \"hdate1\", \"hdate2\"]\n    ].merge(\n        data[[\"symbol\", \"date\", \"ret\", \"mktcap_lag\"]].rename(\n            columns={\"ret\": \"hret\", \"date\": \"hdate\"}\n        ),\n        on=\"symbol\",\n        how=\"inner\"\n    )\n    \n    # Use boolean indexing instead of query (faster)\n    port_ret = port_ret[\n        (port_ret[\"hdate\"] &gt;= port_ret[\"hdate1\"]) & \n        (port_ret[\"hdate\"] &lt;= port_ret[\"hdate2\"])\n    ]\n    port_ret = port_ret.dropna(subset=[\"mktcap_lag\"])\n    port_ret = port_ret[port_ret[\"mktcap_lag\"] &gt; 0]\n    \n    # Step 5: Value-weighted returns within each cohort\n    def vw_mean(group):\n        weights = group[\"mktcap_lag\"]\n        if weights.sum() == 0:\n            return np.nan\n        return np.average(group[\"hret\"], weights=weights)\n    \n    cohort_ret = (port_ret\n        .groupby([\"hdate\", \"momr\", \"form_date\"])\n        .apply(vw_mean, include_groups=False)\n        .reset_index(name=\"cohort_ret\")\n    )\n    \n    monthly_ret = (cohort_ret\n        .groupby([\"hdate\", \"momr\"])\n        .agg(vwret=(\"cohort_ret\", \"mean\"))\n        .reset_index()\n        .rename(columns={\"hdate\": \"date\"})\n    )\n    \n    # Step 6: Long-short\n    wide = monthly_ret.pivot(\n        index=\"date\", columns=\"momr\", values=\"vwret\"\n    ).reset_index()\n    \n    # Handle variable number of portfolios\n    n_cols = len(wide.columns) - 1\n    wide.columns = [\"date\"] + [f\"port{i}\" for i in range(1, n_cols + 1)]\n    wide[\"winners\"] = wide[f\"port{n_cols}\"]\n    wide[\"losers\"] = wide[\"port1\"]\n    wide[\"long_short\"] = wide[\"winners\"] - wide[\"losers\"]\n    \n    return monthly_ret, wide\n\n\n# Run value-weighted J=6, K=6 strategy\n_, vw_results = momentum_strategy_vw(prices_clean, J=6, K=6)\n\nprint(\"Value-Weighted Momentum Strategy (J=6, K=6)\")\nprint(\"=\" * 50)\nprint(\"\\nPortfolio Statistics:\")\nprint(vw_results[[\"winners\", \"losers\", \"long_short\"]].describe().round(4))\n\nValue-Weighted Momentum Strategy (J=6, K=6)\n==================================================\n\nPortfolio Statistics:\n        winners    losers  long_short\ncount  161.0000  161.0000    161.0000\nmean    -0.0129    0.0006     -0.0135\nstd      0.0763    0.0713      0.0653\nmin     -0.2519   -0.2178     -0.3813\n25%     -0.0540   -0.0397     -0.0465\n50%     -0.0067    0.0010     -0.0126\n75%      0.0354    0.0443      0.0173\nmax      0.1765    0.2498      0.2498\n\n\n\ncomparison = []\nfor scheme, df in [(\"EW\", ewret_wide), (\"VW\", vw_results)]:\n    for col in [\"winners\", \"losers\", \"long_short\"]:\n        series = df[col].dropna()\n        n = len(series)\n        mean_ret = series.mean()\n        std_ret = series.std()\n        t_stat = mean_ret / (std_ret / np.sqrt(n))\n        p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1))\n        comparison.append({\n            \"Weighting\": scheme,\n            \"Portfolio\": col.replace(\"_\", \" \").title(),\n            \"Mean (%)\": round(mean_ret * 100, 4),\n            \"Std (%)\": round(std_ret * 100, 4),\n            \"t-stat\": round(t_stat, 2),\n            \"p-value\": round(p_val, 4)\n        })\n\npd.DataFrame(comparison)\n\n\n\nTable 12.10: Comparison of equally weighted (EW) and value-weighted (VW) momentum strategies for J=6, K=6. The table reports mean monthly returns, t-statistics, and p-values for winners, losers, and long-short portfolios under both weighting schemes.\n\n\n\n\n\n\n\n\n\n\nWeighting\nPortfolio\nMean (%)\nStd (%)\nt-stat\np-value\n\n\n\n\n0\nEW\nWinners\n-0.6812\n5.7131\n-1.51\n0.1323\n\n\n1\nEW\nLosers\n1.4190\n6.5319\n2.76\n0.0065\n\n\n2\nEW\nLong Short\n-2.1002\n4.8969\n-5.44\n0.0000\n\n\n3\nVW\nWinners\n-1.2943\n7.6274\n-2.15\n0.0328\n\n\n4\nVW\nLosers\n0.0589\n7.1275\n0.10\n0.9166\n\n\n5\nVW\nLong Short\n-1.3533\n6.5312\n-2.63\n0.0094\n\n\n\n\n\n\n\n\n\n\n\nvw_results = vw_results.sort_values(\"date\")\nvw_results[\"cumret_ls_vw\"] = (1 + vw_results[\"long_short\"]).cumprod() - 1\n\new_data = (ewret_wide[[\"date\", \"long_short\"]]\n    .rename(columns={\"long_short\": \"cumret\"})\n    .assign(scheme=\"Equally Weighted\")\n)\new_data[\"cumret\"] = (1 + ew_data[\"cumret\"]).cumprod() - 1\n\nvw_data = (vw_results[[\"date\", \"cumret_ls_vw\"]]\n    .rename(columns={\"cumret_ls_vw\": \"cumret\"})\n    .assign(scheme=\"Value Weighted\")\n)\n\new_vs_vw = pd.concat([ew_data, vw_data], ignore_index=True)\n\nplot_ew_vw = (\n    ggplot(ew_vs_vw, aes(x=\"date\", y=\"cumret\", color=\"scheme\")) +\n    geom_line(size=1) +\n    scale_y_continuous(labels=percent_format()) +\n    scale_color_manual(values=[\"#1f77b4\", \"#ff7f0e\"]) +\n    labs(\n        x=\"\", y=\"Cumulative return\",\n        color=\"Weighting Scheme\",\n        title=\"EW vs. VW Momentum Long-Short Strategy\"\n    ) +\n    theme_minimal() +\n    theme(\n        figure_size=(10, 6),\n        legend_position=\"bottom\"\n    )\n)\nplot_ew_vw\n\n\n\n\n\n\n\nFigure 12.9: Cumulative returns of equally weighted (EW) versus value-weighted (VW) long-short momentum strategies. Differences between the two lines reflect the impact of firm size on momentum profitability.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#daily-momentum-analysis",
    "href": "13_momemtum.html#daily-momentum-analysis",
    "title": "12  Momentum Strategies",
    "section": "12.11 Daily Momentum Analysis",
    "text": "12.11 Daily Momentum Analysis\nWhile momentum strategies are typically evaluated at the monthly frequency following Jegadeesh and Titman (1993), analyzing daily return patterns provides additional insights into the dynamics of momentum profits. Daily data allows us to examine how momentum profits accrue within the holding period, measure intra-month volatility of the strategy, and compute more precise risk measures.\n\n12.11.1 Loading Daily Data\n\nprices_daily = pd.read_sql_query(\n    sql=(\"SELECT symbol, date, ret, ret_excess, mktcap_lag \"\n         \"FROM prices_daily\"),\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Daily observations: {len(prices_daily):,}\")\nprint(f\"Unique stocks: {prices_daily['symbol'].nunique():,}\")\nprint(f\"Date range: {prices_daily['date'].min().date()} \"\n      f\"to {prices_daily['date'].max().date()}\")\n\nDaily observations: 3,462,157\nUnique stocks: 1,459\nDate range: 2010-01-05 to 2023-12-29\n\n\n\n\n12.11.2 Daily Returns of Monthly Momentum Portfolios\nRather than forming momentum portfolios at the daily frequency (which would require daily rebalancing and is impractical), we use the monthly portfolio assignments and track their daily returns. This gives us the daily return series of the monthly momentum strategy.\n\n# # # Use the monthly portfolio assignments from the J=6 baseline\nmonthly_assignments = portfolios_with_dates[\n    [\"symbol\", \"form_date\", \"momr\", \"hdate1\", \"hdate2\"]\n].copy()\n\n# Merge with daily returns\ndaily_mom_returns = monthly_assignments.merge(\n    prices_daily[[\"symbol\", \"date\", \"ret\"]].rename(\n        columns={\"ret\": \"dret\", \"date\": \"ddate\"}\n    ),\n    on=\"symbol\",\n    how=\"inner\"\n)\n\n# Use boolean indexing instead of query\ndaily_mom_returns = daily_mom_returns[\n    (daily_mom_returns[\"ddate\"] &gt;= daily_mom_returns[\"hdate1\"]) & \n    (daily_mom_returns[\"ddate\"] &lt;= daily_mom_returns[\"hdate2\"])\n]\n\nprint(f\"Daily portfolio return observations: {len(daily_mom_returns):,}\")\n\nDaily portfolio return observations: 19,112,536\n\n\n\n# Compute daily equally weighted portfolio returns\n# Stage 1: Average within each cohort\ndaily_cohort_ret = (daily_mom_returns\n    .groupby([\"ddate\", \"momr\", \"form_date\"])\n    .agg(cohort_ret=(\"dret\", \"mean\"))\n    .reset_index()\n)\n\n# Stage 2: Average across cohorts\ndaily_ewret = (daily_cohort_ret\n    .groupby([\"ddate\", \"momr\"])\n    .agg(ewret=(\"cohort_ret\", \"mean\"))\n    .reset_index()\n    .rename(columns={\"ddate\": \"date\"})\n)\n\n# Compute daily long-short returns\ndaily_wide = daily_ewret.pivot(\n    index=\"date\", columns=\"momr\", values=\"ewret\"\n).reset_index()\ndaily_wide.columns = [\"date\"] + [f\"port{i}\" for i in range(1, 11)]\ndaily_wide[\"winners\"] = daily_wide[\"port10\"]\ndaily_wide[\"losers\"] = daily_wide[\"port1\"]\ndaily_wide[\"long_short\"] = daily_wide[\"winners\"] - daily_wide[\"losers\"]\n\nprint(f\"Daily long-short return observations: {len(daily_wide):,}\")\n\nDaily long-short return observations: 3,352\n\n\n\n\n12.11.3 Daily Cumulative Returns\n\ndaily_wide = daily_wide.sort_values(\"date\")\ndaily_wide[\"cumret_ls\"] = (1 + daily_wide[\"long_short\"]).cumprod() - 1\n\nplot_daily_cumret = (\n    ggplot(daily_wide, aes(x=\"date\", y=\"cumret_ls\")) +\n    geom_line(size=0.5, color=\"#2ca02c\") +\n    geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\") +\n    scale_y_continuous(labels=percent_format()) +\n    labs(\n        x=\"\", y=\"Cumulative return\",\n        title=\"Daily Cumulative Return: Momentum Long-Short Strategy\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 6))\n)\nplot_daily_cumret\n\n\n\n\n\n\n\nFigure 12.10: Cumulative daily returns of the momentum long-short strategy. This figure shows the same strategy as the monthly analysis but tracked at daily frequency, revealing intra-month dynamics and the precise timing of momentum gains and losses.\n\n\n\n\n\n\n\n12.11.4 Annualized Risk Metrics from Daily Data\nDaily data enables more precise estimation of risk metrics through higher-frequency sampling.\n\ndaily_ls = daily_wide[\"long_short\"].dropna()\n\n# Annualized metrics\nann_mean = daily_ls.mean() * 252\nann_vol = daily_ls.std() * np.sqrt(252)\nsharpe = ann_mean / ann_vol if ann_vol &gt; 0 else np.nan\n\n# Drawdown\ndaily_wealth = (1 + daily_ls).cumprod()\ndaily_running_max = daily_wealth.cummax()\ndaily_dd = (daily_wealth / daily_running_max - 1).min()\n\n# Higher moments\nskew = daily_ls.skew()\nkurt = daily_ls.kurtosis()\n\n# VaR and CVaR\nvar_95 = daily_ls.quantile(0.05)\ncvar_95 = daily_ls[daily_ls &lt;= var_95].mean()\n\nrisk_metrics = pd.DataFrame({\n    \"Metric\": [\n        \"Annualized Mean Return\",\n        \"Annualized Volatility\", \n        \"Sharpe Ratio\",\n        \"Maximum Drawdown\",\n        \"Skewness\",\n        \"Excess Kurtosis\",\n        \"Daily VaR (5%)\",\n        \"Daily CVaR (5%)\"\n    ],\n    \"Value\": [\n        f\"{ann_mean*100:.2f}%\",\n        f\"{ann_vol*100:.2f}%\",\n        f\"{sharpe:.2f}\",\n        f\"{daily_dd*100:.2f}%\",\n        f\"{skew:.2f}\",\n        f\"{kurt:.2f}\",\n        f\"{var_95*100:.2f}%\",\n        f\"{cvar_95*100:.2f}%\"\n    ]\n})\nrisk_metrics\n\n\n\nTable 12.11: Annualized risk metrics for the momentum long-short strategy computed from daily returns. Volatility is annualized using the square root of 252 rule. The Sharpe ratio, maximum drawdown, skewness, and kurtosis provide a comprehensive risk profile.\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nAnnualized Mean Return\n-26.90%\n\n\n1\nAnnualized Volatility\n15.01%\n\n\n2\nSharpe Ratio\n-1.79\n\n\n3\nMaximum Drawdown\n-97.62%\n\n\n4\nSkewness\n-11.23\n\n\n5\nExcess Kurtosis\n378.11\n\n\n6\nDaily VaR (5%)\n-1.33%\n\n\n7\nDaily CVaR (5%)\n-2.10%\n\n\n\n\n\n\n\n\n\n\n\n\n12.11.5 Realized Volatility of Momentum Returns\nUsing daily returns, we can compute the monthly realized volatility of the momentum strategy and examine how it varies over time.\n\ndaily_wide[\"year_month\"] = daily_wide[\"date\"].dt.to_period(\"M\")\n\nrealized_vol = (daily_wide\n    .groupby(\"year_month\")[\"long_short\"]\n    .std()\n    .reset_index()\n    .rename(columns={\"long_short\": \"realized_vol\"})\n)\nrealized_vol[\"date\"] = realized_vol[\"year_month\"].dt.to_timestamp()\nrealized_vol[\"realized_vol_ann\"] = realized_vol[\"realized_vol\"] * np.sqrt(252)\n\nplot_rvol = (\n    ggplot(realized_vol, aes(x=\"date\", y=\"realized_vol_ann\")) +\n    geom_line(color=\"#d62728\", size=0.8) +\n    scale_y_continuous(labels=percent_format()) +\n    labs(\n        x=\"\", y=\"Annualized Realized Volatility\",\n        title=\"Realized Volatility of Momentum Strategy\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_rvol\n\n\n\n\n\n\n\nFigure 12.11: Monthly realized volatility of the momentum long-short strategy, computed from daily returns within each month. Higher values indicate periods of greater uncertainty in momentum profits, often coinciding with market stress.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#saving-results-to-the-database",
    "href": "13_momemtum.html#saving-results-to-the-database",
    "title": "12  Momentum Strategies",
    "section": "12.12 Saving Results to the Database",
    "text": "12.12 Saving Results to the Database\nWe save the momentum portfolio returns to our database for use in subsequent chapters, including factor model construction and portfolio optimization.\n\n# Save monthly equally weighted momentum portfolio returns\newret_to_save = ewret[[\"date\", \"momr\", \"ewret\"]].copy()\newret_to_save.to_sql(\n    name=\"momentum_portfolios_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(f\"Saved {len(ewret_to_save):,} monthly momentum portfolio observations.\")\n\n# Save the long-short return series\nmomentum_factor = ewret_wide[[\"date\", \"long_short\"]].dropna().copy()\nmomentum_factor = momentum_factor.rename(columns={\"long_short\": \"wml\"})\nmomentum_factor.to_sql(\n    name=\"momentum_factor_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(f\"Saved {len(momentum_factor):,} monthly WML factor observations.\")\n\n# Save the daily long-short return series\ndaily_momentum_factor = daily_wide[[\"date\", \"long_short\"]].dropna().copy()\ndaily_momentum_factor = daily_momentum_factor.rename(\n    columns={\"long_short\": \"wml\"}\n)\ndaily_momentum_factor.to_sql(\n    name=\"momentum_factor_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(f\"Saved {len(daily_momentum_factor):,} daily WML factor observations.\")\n\nSaved 1,610 monthly momentum portfolio observations.\nSaved 161 monthly WML factor observations.\nSaved 3,352 daily WML factor observations.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#practical-considerations",
    "href": "13_momemtum.html#practical-considerations",
    "title": "12  Momentum Strategies",
    "section": "12.13 Practical Considerations",
    "text": "12.13 Practical Considerations\n\n12.13.1 Transaction Costs\nMomentum strategies involve substantial portfolio turnover, as stocks enter and exit the extreme decile portfolios each month. Korajczyk and Sadka (2004) examine whether momentum profits survive transaction costs and find that profitability declines significantly for large institutional investors, though smaller portfolios can still capture meaningful returns.\nIn the Vietnamese market, transaction costs include:\n\nBrokerage commissions: Typically 0.15%–0.25% of transaction value for institutional investors.\nExchange fees: Approximately 0.03% per trade.\nMarket impact: Particularly relevant for smaller, less liquid stocks that dominate the extreme momentum portfolios. Vietnam’s lower liquidity compared to developed markets may amplify this cost.\nTrading band limits: The \\(\\pm 7\\%\\) daily price limit on HOSE can prevent immediate execution of trades, introducing tracking error relative to the theoretical portfolio.\n\n\n\n12.13.2 Implementation Lag\nOur baseline implementation assumes that portfolios can be formed and rebalanced instantaneously at the end of each month. In practice, there is a lag between observing the formation period returns and executing the portfolio trades. The one-month gap between the formation period and the start of the holding period (Jegadeesh (1990)) partially addresses this concern, but practitioners should consider additional implementation delays.\n\n\n12.13.3 Survivorship Bias\nOur dataset from DataCore includes both active and delisted stocks, which mitigates survivorship bias. However, the treatment of delisted stocks can affect momentum results. Stocks that are delisted during the holding period may generate extreme returns (both positive for acquisitions and negative for failures). We retain delisted returns as reported in the database, which is consistent with the treatment in Jegadeesh and Titman (1993).\n\n\n12.13.4 Small Sample Considerations\nThe Vietnamese stock market has a relatively short history compared to the U.S. market studied in Jegadeesh and Titman (1993). Our sample spans approximately two decades, compared to the nearly three decades in the original study. This shorter sample period implies wider confidence intervals and greater sensitivity to specific episodes (such as the 2007–2009 financial crisis, which had a severe impact on Vietnamese equities). Results should be interpreted with this caveat in mind.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "13_momemtum.html#key-takeaways",
    "href": "13_momemtum.html#key-takeaways",
    "title": "12  Momentum Strategies",
    "section": "12.14 Key Takeaways",
    "text": "12.14 Key Takeaways\nThis chapter has provided an implementation and analysis of momentum strategies in the Vietnamese equity market, following the methodology of Jegadeesh and Titman (1993). The main findings and methodological contributions are:\n\nMethodology: We implemented the full Jegadeesh and Titman (1993) overlapping portfolio methodology, including the two-stage averaging procedure (within-cohort, then across-cohort) that handles the \\(K\\) active portfolios in each month.\nBaseline results: The \\(J=6\\), \\(K=6\\) strategy provides a natural benchmark. The spread between winner and loser portfolios reveals whether cross-sectional momentum exists in Vietnamese equities.\nRobustness across horizons: By computing the full \\(J \\times K\\) grid with \\(J, K \\in \\{3, 6, 9, 12\\}\\), we assessed whether the momentum premium is robust to the choice of formation and holding periods, following the approach in Jegadeesh and Titman (1993) Table 1.\nRisk adjustment: CAPM and Fama-French three-factor alphas measure whether the momentum premium is explained by standard risk factors, building on the analysis in Fama and French (1996) who show that their three-factor model fails to explain momentum.\nMarket state dependence: Following Cooper, Gutierrez Jr, and Hameed (2004), we examined whether momentum profits vary with market conditions, which is particularly relevant in emerging markets with pronounced boom-bust cycles.\nValue weighting: The comparison of equally weighted and value-weighted strategies addresses practical implementability and isolates the role of firm size in driving momentum profits.\nDaily analysis: By tracking momentum portfolios at the daily frequency, we computed precise risk metrics including realized volatility, maximum drawdown, VaR, and CVaR, providing a complete risk profile of the strategy.\nEmerging market context: Throughout the analysis, we have highlighted features specific to the Vietnamese market—trading band limits, foreign ownership restrictions, shorter sample history, and higher transaction costs—that affect the interpretation and practical viability of momentum strategies.\n\n\n\n\n\n\n\n\nBarberis, Nicholas, Andrei Shleifer, and Robert Vishny. 1998. “A Model of Investor Sentiment.” Journal of Financial Economics 49 (3): 307–43.\n\n\nCarhart, Mark M. 1997. “On Persistence in Mutual Fund Performance.” The Journal of Finance 52 (1): 57–82.\n\n\nChan, Kalok, Allaudeen Hameed, and Wilson Tong. 2000. “Profitability of Momentum Strategies in the International Equity Markets.” Journal of Financial and Quantitative Analysis, 153–72.\n\n\nChui, Andy CW, Sheridan Titman, and KC John Wei. 2010. “Individualism and Momentum Around the World.” The Journal of Finance 65 (1): 361–92.\n\n\nCooper, Michael J, Roberto C Gutierrez Jr, and Allaudeen Hameed. 2004. “Market States and Momentum.” The Journal of Finance 59 (3): 1345–65.\n\n\nDaniel, Kent, David Hirshleifer, and Avanidhar Subrahmanyam. 1998. “Investor Psychology and Security Market Under-and Overreactions.” The Journal of Finance 53 (6): 1839–85.\n\n\nDaniel, Kent, and Tobias J Moskowitz. 2016. “Momentum Crashes.” Journal of Financial Economics 122 (2): 221–47.\n\n\nFama, Eugene F, and Kenneth R French. 1996. “Multifactor Explanations of Asset Pricing Anomalies.” The Journal of Finance 51 (1): 55–84.\n\n\nGrundy, Bruce D, and J Spencer Martin Martin. 2001. “Understanding the Nature of the Risks and the Source of the Rewards to Momentum Investing.” The Review of Financial Studies 14 (1): 29–78.\n\n\nHofstede, Geert. 2001. “Culture’s Consequences: Comparing Values, Behaviors, Institutions and Organizations Across Nations.” International Educational and Professional.\n\n\nHong, Harrison, and Jeremy C Stein. 1999. “A Unified Theory of Underreaction, Momentum Trading, and Overreaction in Asset Markets.” The Journal of Finance 54 (6): 2143–84.\n\n\nJegadeesh, Narasimhan. 1990. “Evidence of Predictable Behavior of Security Returns.” The Journal of Finance 45 (3): 881–98.\n\n\nJegadeesh, Narasimhan, and Sheridan Titman. 1993. “Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency.” The Journal of Finance 48 (1): 65–91.\n\n\nJohnson, Timothy C. 2002. “Rational Momentum Effects.” The Journal of Finance 57 (2): 585–608.\n\n\nKorajczyk, Robert A, and Ronnie Sadka. 2004. “Are Momentum Profits Robust to Trading Costs?” The Journal of Finance 59 (3): 1039–82.\n\n\nLesmond, David A, Michael J Schill, and Chunsheng Zhou. 2004. “The Illusory Nature of Momentum Profits.” Journal of Financial Economics 71 (2): 349–80.\n\n\nRouwenhorst, K Geert. 1998. “International Momentum Strategies.” The Journal of Finance 53 (1): 267–84.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "14_fama_macbeth.html",
    "href": "14_fama_macbeth.html",
    "title": "13  Fama-MacBeth Regressions",
    "section": "",
    "text": "13.1 The Econometric Framework\nIn this chapter, we delve into the implementation of the Fama and MacBeth (1973) regression approach, a cornerstone of empirical asset pricing. While portfolio sorts provide a robust, non-parametric view of the relationship between characteristics and returns, they struggle when we need to control for multiple factors simultaneously. For instance, in the Vietnamese stock market (HOSE and HNX), small-cap stocks often exhibit high illiquidity. Does the “Size effect” exist because small stocks are risky, or simply because they are illiquid? Fama-MacBeth (FM) regressions allow us to disentangle these effects in a linear framework.\nWe will implement a version of the FM procedure, accounting for:\nThe Fama-MacBeth procedure is essentially a two-step filter that separates the cross-sectional variation in returns from the time-series variation.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "14_fama_macbeth.html#the-econometric-framework",
    "href": "14_fama_macbeth.html#the-econometric-framework",
    "title": "13  Fama-MacBeth Regressions",
    "section": "",
    "text": "13.1.1 Intuition: Why not Panel OLS?\nA naive approach would be to pool all data (\\(N\\) stocks \\(\\times\\) \\(T\\) months) and run a single Ordinary Least Squares (OLS) regression:\n\\[\nr_{i,t+1} = \\alpha + \\beta_{i,t} \\lambda + \\epsilon_{i,t+1}\n\\]\nHowever, this assumes that the error terms \\(\\epsilon_{i,t+1}\\) are independent across firms. In reality, stock returns are highly cross-sectionally correlated (if the VN-Index crashes, most stocks fall together). A pooled OLS would underestimate the standard errors, leading to “false positive” discoveries of risk factors. Fama-MacBeth solves this by running \\(T\\) separate cross-sectional regressions, effectively treating each month as a single independent observation of the risk premium.\n\n\n13.1.2 Mathematical Derivation\n\n13.1.2.1 Step 1: Cross-Sectional Regressions\nFor each month \\(t\\), we estimate the premium \\(\\lambda_{k,t}\\) for \\(K\\) factors. Let \\(r_{i,t+1}\\) be the excess return of asset \\(i\\) at time \\(t+1\\). Let \\(\\boldsymbol{\\beta}_{i,t}\\) be a vector of \\(K\\) characteristics (e.g., Market Beta, Book-to-Market, Size) known at time \\(t\\).\nThe model for a specific month \\(t\\) is: \\[\n\\mathbf{r}_{t+1} = \\mathbf{X}_t \\boldsymbol{\\lambda}_{t+1} + \\boldsymbol{\\alpha}_{t+1} + \\boldsymbol{\\epsilon}_{t+1}\n\\]\nWhere:\n\n\\(\\mathbf{r}_{t+1}\\) is an \\(N \\times 1\\) vector of returns.\n\\(\\mathbf{X}_t\\) is an \\(N \\times (K+1)\\) matrix of factor exposures (including a column of ones for the intercept).\n\\(\\boldsymbol{\\lambda}_{t+1}\\) is the vector of risk premiums realized in month \\(t+1\\).\n\nTo use Weighted Least Squares (WLS), We define a weighting matrix \\(\\mathbf{W}_t\\) (typically diagonal with market capitalizations). The estimator for month \\(t\\) is: \\[\n\\hat{\\boldsymbol{\\lambda}}_{t+1} = (\\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{X}_t)^{-1} \\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{r}_{t+1}\n\\]\n\n\n13.1.2.2 Step 2: Time-Series Aggregation\nWe now have a time-series of \\(T\\) estimates: \\(\\hat{\\lambda}_1, \\hat{\\lambda}_2, \\dots, \\hat{\\lambda}_T\\). The final estimate of the risk premium is the time-series average: \\[\n\\hat{\\lambda}_k = \\frac{1}{T} \\sum_{t=1}^T \\hat{\\lambda}_{k,t}\n\\]\nThe standard error is derived from the standard deviation of these monthly estimates: \\[\n\\sigma(\\hat{\\lambda}_k) = \\sqrt{\\frac{1}{T^2} \\sum_{t=1}^T (\\hat{\\lambda}_{k,t} - \\hat{\\lambda}_k)^2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "14_fama_macbeth.html#data-preparation",
    "href": "14_fama_macbeth.html#data-preparation",
    "title": "13  Fama-MacBeth Regressions",
    "section": "13.2 Data Preparation",
    "text": "13.2 Data Preparation\nWe utilize data from our local SQLite database. In Vietnam, the fiscal year typically ends in December, and audited reports are required by April. To ensure no look-ahead bias, we lag accounting data (Book Equity) to match returns starting in July (a 6-month conservative lag, similar to Fama-French, but adapted for Vietnamese reporting delays).\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom pandas.tseries.offsets import MonthEnd\n\n# Connect to the Vietnamese data\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\n# Load Monthly Prices (HOSE & HNX)\nprices_monthly = pd.read_sql_query(\n  sql=\"SELECT symbol, date, ret_excess, mktcap, mktcap_lag FROM prices_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\n# Load Book Equity (derived from Vietnamese Financial Statements)\ncomp_vn = pd.read_sql_query(\n  sql=\"SELECT datadate, symbol, be FROM comp_vn\",\n  con=tidy_finance,\n  parse_dates={\"datadate\"}\n)\n\n# Load Rolling Market Betas (Pre-calculated in Chapter 'Beta Estimation')\nbeta_monthly = pd.read_sql_query(\n  sql=\"SELECT symbol, date, beta FROM beta_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nWe construct our testing characteristics:\n\n(Market Beta): The sensitivity to the VN-Index.\nSize (ln(ME)): The natural log of market capitalization.\nValue (BM): The ratio of Book Equity to Market Equity.\n\n\n# Prepare Characteristics\ncharacteristics = (\n    comp_vn\n    # Align reporting date to month end\n    .assign(date=lambda x: pd.to_datetime(x[\"datadate\"]) + MonthEnd(0))\n    # Merge with price data to get Market Cap at fiscal year end\n    .merge(prices_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n    .merge(beta_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n    .assign(\n        # Compute Book-to-Market\n        bm=lambda x: x[\"be\"] / x[\"mktcap\"],\n        log_mktcap=lambda x: np.log(x[\"mktcap\"]),\n        # Create sorting date: Financials valid from July of year t+1\n        sorting_date=lambda x: x[\"date\"] + pd.DateOffset(months=6) + MonthEnd(0),\n    )\n    .get([\"symbol\", \"bm\", \"beta\", \"sorting_date\"]) \n    .dropna()\n)\n\ncharacteristics.head()\n\n\n\n\n\n\n\n\nsymbol\nbm\nbeta\nsorting_date\n\n\n\n\n8729\nVTV\n7.034945e+08\n0.847809\n2017-06-30\n\n\n8732\nMTG\n2.670306e+09\n1.140066\n2017-06-30\n\n\n8739\nMKV\n6.505031e+08\n-0.448319\n2017-06-30\n\n\n8740\nMIC\n1.243127e+09\n0.772140\n2017-06-30\n\n\n8742\nMCP\n6.657350e+08\n0.348139\n2017-06-30\n\n\n\n\n\n\n\n\n# Merge back to monthly return panel\ndata_fm = (prices_monthly\n  .merge(characteristics, \n         left_on=[\"symbol\", \"date\"], \n         right_on=[\"symbol\", \"sorting_date\"], \n         how=\"left\")\n#   .merge(beta_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n  .sort_values([\"symbol\", \"date\"])\n)\n\n# Forward fill characteristics for 12 months (valid until next report)\ndata_fm[[\"bm\"]] = data_fm.groupby(\"symbol\")[[\"bm\"]].ffill(limit=12)\n\n# Log Market Cap is updated monthly\ndata_fm[\"log_mktcap\"] = np.log(data_fm[\"mktcap\"])\n\n# Lead returns: We use characteristics at t to predict return at t+1\ndata_fm[\"ret_excess_lead\"] = data_fm.groupby(\"symbol\")[\"ret_excess\"].shift(-1)\n\n# Cleaning: Remove rows with missing future returns or characteristics\ndata_fm = data_fm.dropna(subset=[\"ret_excess_lead\", \"beta\", \"log_mktcap\", \"bm\"])\n\nprint(data_fm.head())\n\nprint(f\"Data ready: {len(data_fm):,} observations from {data_fm.date.min().date()} to {data_fm.date.max().date()}\")\n\n    symbol       date  ret_excess       mktcap   mktcap_lag            bm  \\\n163    AAA 2017-06-30    0.129454  2078.455619  1834.816104  7.929854e+08   \n175    AAA 2018-06-30   -0.067690  2758.426126  2948.159140  8.161755e+08   \n187    AAA 2019-06-30    0.030469  3141.519560  3038.799575  1.389438e+09   \n199    AAA 2020-06-30   -0.035462  2311.250278  2387.972279  1.497272e+09   \n211    AAA 2021-06-30    0.275355  5423.280296  4241.283308  1.456989e+09   \n\n         beta sorting_date  log_mktcap  ret_excess_lead  \n163  1.479060   2017-06-30    7.639380        -0.051090  \n175  1.090411   2018-06-30    7.922416        -0.095926  \n187  1.099956   2019-06-30    8.052462        -0.027856  \n199  0.954144   2020-06-30    7.745544        -0.098769  \n211  1.245004   2021-06-30    8.598456        -0.175128  \nData ready: 5,075 observations from 2017-06-30 to 2023-06-30",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "14_fama_macbeth.html#step-1-cross-sectional-regressions-with-wls",
    "href": "14_fama_macbeth.html#step-1-cross-sectional-regressions-with-wls",
    "title": "13  Fama-MacBeth Regressions",
    "section": "13.3 Step 1: Cross-Sectional Regressions with WLS",
    "text": "13.3 Step 1: Cross-Sectional Regressions with WLS\nHou, Xue, and Zhang (2020) argue that micro-cap stocks distorts inference because they have high transaction costs and idiosyncratic volatility. In Vietnam, this is exacerbated by “penny stock” speculation.\nWe implement Weighted Least Squares (WLS) where weights are the market capitalization of the prior month. This tests if the factors are priced in the investable universe, not just the equal-weighted average of tiny stocks.\n\ndef run_cross_section(df):\n    # Standardize inputs for numerical stability\n    # Note: We do NOT standardize the dependent variable (returns)\n    # We standardize regressors to interpret coefficients as \"per 1 SD change\" if desired,\n    # BUT for pure risk premium estimation, we usually keep raw units.\n    # Here we use raw units to interpret lambda as % return per unit of characteristic.\n    \n    # Define Weighted Least Squares\n    model = smf.wls(\n        formula=\"ret_excess_lead ~ beta + log_mktcap + bm\",\n        data=df,\n        weights=df[\"mktcap_lag\"] # Weight by size\n    )\n    results = model.fit()\n    \n    return results.params\n\n# Apply to every month\nrisk_premiums = (data_fm\n  .groupby(\"date\")\n  .apply(run_cross_section)\n  .reset_index()\n)\n\nprint(risk_premiums.head())\n\n        date  Intercept      beta  log_mktcap            bm\n0 2017-06-30  -0.089116 -0.063799    0.010284  2.897813e-11\n1 2018-06-30  -0.023221 -0.008252    0.001890  1.377518e-11\n2 2019-06-30  -0.079373  0.035622    0.006224 -8.139910e-12\n3 2020-06-30  -0.031213 -0.114968    0.008999 -2.306768e-11\n4 2021-06-30   0.081397 -0.011407   -0.007330 -5.211290e-11",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "14_fama_macbeth.html#step-2-time-series-aggregation-hypothesis-testing",
    "href": "14_fama_macbeth.html#step-2-time-series-aggregation-hypothesis-testing",
    "title": "13  Fama-MacBeth Regressions",
    "section": "13.4 Step 2: Time-Series Aggregation & Hypothesis Testing",
    "text": "13.4 Step 2: Time-Series Aggregation & Hypothesis Testing\nWe now possess the time-series of risk premiums. We calculate the arithmetic mean and the -statistics.\nCrucially, we use Newey-West (HAC) standard errors. Risk premiums in Vietnam often exhibit autocorrelation (momentum in factor performance). A simple standard error formula would be invalid.\n\ndef calculate_fama_macbeth_stats(df, lags=6):\n    summary = []\n    \n    for col in [\"Intercept\", \"beta\", \"log_mktcap\", \"bm\"]:\n        series = df[col]\n        \n        # 1. Point Estimate (Average Risk Premium)\n        mean_premium = series.mean()\n        \n        # 2. Newey-West Standard Error\n        # We regress the series on a constant (ones) to get the SE of the mean\n        exog = sm.add_constant(np.ones(len(series)))\n        nw_model = sm.OLS(series, exog).fit(\n            cov_type='HAC', cov_kwds={'maxlags': lags}\n        )\n\n        se = nw_model.bse.iloc[0]\n        t_stat = nw_model.tvalues.iloc[0]\n        \n        summary.append({\n            \"Factor\": col,\n            \"Premium (%)\": mean_premium * 100,\n            \"Std Error\": se * 100,\n            \"t-statistic\": t_stat,\n            \"Significance\": \"*\" if abs(t_stat) &gt; 1.96 else \"\"\n        })\n        \n    return pd.DataFrame(summary)\n\nprice_of_risk = calculate_fama_macbeth_stats(risk_premiums)\nprint(price_of_risk.round(4))\n\n       Factor  Premium (%)  Std Error  t-statistic Significance\n0   Intercept      -1.8174     1.9117      -0.9507             \n1        beta      -1.7859     1.0407      -1.7161             \n2  log_mktcap       0.2347     0.2048       1.1457             \n3          bm      -0.0000     0.0000      -0.0928             \n\n\n\n13.4.1 Visualizing the Time-Varying Risk Premium\nOne major advantage of the FM approach is that we can inspect the volatility of the risk premiums over time. In Vietnam, we expect the “Size” premium to be highly volatile during periods of retail liquidity injection (e.g., 2020-2021).\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# Calculate cumulative returns of the factors (as if they were tradable portfolios)\ncumulative_premiums = (risk_premiums\n    .set_index(\"date\")\n    .drop(columns=[\"Intercept\"])\n    .cumsum()\n)\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncumulative_premiums.plot(ax=ax, linewidth=2)\nax.set_title(\"Cumulative Risk Premiums in Vietnam (Fama-MacBeth)\", fontsize=14)\nax.set_ylabel(\"Cumulative Coefficient Return\")\nax.legend(title=\"Factor\")\nax.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 13.1: Cumulative Risk Premiums in Vietnam.\n\n\n\n\n\n\nMarket Beta: In many empirical studies (including the US), the market beta premium is often insignificant or even negative (the “Betting Against Beta” anomaly). In Vietnam, if the -stat is , it implies the CAPM does not explain the cross-section of returns.\nSize (Log Mktcap): A negative coefficient confirms the “Size Effect”—smaller firms have higher expected returns. However, using WLS often weakens this result compared to OLS, suggesting the size premium is concentrated in micro-caps.\nValue (BM): A positive coefficient confirms the Value premium. In Vietnam, value stocks (high B/M) often outperform growth stocks, particularly in the manufacturing and banking sectors.\n\nFigure 13.1 plots the cumulative sum of the monthly Fama MacBeth risk premium estimates for beta, size, and value. Because these lines cumulate estimated cross sectional prices of risk rather than actual portfolio returns, the figure should be interpreted as showing the time variation and persistence of estimated premia, not investable performance.\nThe beta premium displays a clear regime shift around 2020, with a sharp decline that only partially reverses afterward. This pattern suggests that the pricing of systematic risk in Vietnam is unstable over short samples and may be heavily influenced by episodic market conditions such as the post COVID retail trading boom. The size premium is comparatively smoother but small in magnitude, indicating only weak and time varying evidence that firm size is priced in the cross section during this period. The value premium remains close to zero throughout, implying little consistent cross sectional reward to high book to market firms in this sample window.\nOverall, the figure highlights that estimated risk premia in the Vietnamese market are highly time varying and sensitive to specific macro and market regimes, reinforcing the need for caution when drawing conclusions from short samples.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "14_fama_macbeth.html#sanity-checks",
    "href": "14_fama_macbeth.html#sanity-checks",
    "title": "13  Fama-MacBeth Regressions",
    "section": "13.5 Sanity Checks",
    "text": "13.5 Sanity Checks\n\n13.5.1 Time-Series Volatility Check\nFama-MacBeth relies on the assumption that the risk premium varies over time. If your bm premium is truly near zero every month, the method fails.\nAction: Plot the time series of the estimated coefficients . You want to see “noise” around a mean. If you see a flat line or a single massive spike, your data is corrupted.\n\nimport matplotlib.pyplot as plt\n\n# Plot the time series of the BM risk premium\nfig, ax = plt.subplots(figsize=(10, 5))\nrisk_premiums[\"bm\"].plot(ax=ax, title=\"Monthly Value Premium (BM) Coefficient\")\nax.axhline(0, color=\"black\", linestyle=\"--\")\nax.set_ylabel(\"Slope Coefficient\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n13.5.2 Correlation of Characteristics (Multicollinearity)\nIn Vietnam, large-cap stocks (high log_mktcap) are often the ones with high Book-to-Market ratios (banks/utilities) or specific Betas. If your factors are highly correlated, the Fama-MacBeth coefficients will be unstable and insignificant (low t-stats), even if the factors actually matter.\nAction: Check the cross-sectional correlation.\n\n# Check correlation of the characteristics\ncorr_matrix = data_fm[[\"beta\", \"log_mktcap\", \"bm\"]].corr()\nprint(corr_matrix)\n\n                beta  log_mktcap        bm\nbeta        1.000000    0.392776 -0.033748\nlog_mktcap  0.392776    1.000000 -0.203307\nbm         -0.033748   -0.203307  1.000000\n\n\nInterpretation:\n\nIf correlation &gt; 0.7 (absolute value), the regression struggles to distinguish between the two factors.\nFor example, if Size and Liquidity are -0.8 correlated, the model cannot tell which one is driving the return, often resulting in both having insignificant t-stats.\n\n\n\n\n\n\n\nFama, Eugene F., and James D. MacBeth. 1973. “Risk, return, and equilibrium: Empirical tests.” Journal of Political Economy 81 (3): 607–36. https://doi.org/10.1086/260061.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2020. “Replicating anomalies.” Review of Financial Studies 33 (5): 2019–2133. https://doi.org/10.1093/rfs/hhy131.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "17_event_studies.html",
    "href": "17_event_studies.html",
    "title": "14  Event Studies in Finance",
    "section": "",
    "text": "14.0.1 Why Event Studies Matter\nEvent studies constitute one of the most enduring and widely deployed empirical methodologies in financial economics. At their core, event studies measure the impact of a specific event on the value of a firm by examining abnormal security returns around the time the event occurs. The methodology rests on a simple premise: if capital markets are informationally efficient, the effect of an event will be reflected immediately in security prices, and any deviation from “normal” expected returns can be attributed to the event itself.\nSince the pioneering work of Eugene F. Fama et al. (1969), who studied how stock prices adjust to new information around stock splits, event studies have become a cornerstone of empirical research across finance, accounting, economics, and law. Ball and Brown (2013) demonstrated that accounting earnings announcements convey information to the market, a finding that launched decades of research in accounting and disclosure. The methodology has since been refined through contributions by Brown and Warner (1980) and Brown and Warner (1985), who established the statistical properties of event study methods, and MacKinlay (1997) codified best practices that remain standard today.\nThe breadth of applications is remarkable. Event studies have been used to examine the wealth effects of mergers and acquisitions (Jensen and Ruback 1983; Andrade, Mitchell, and Stafford 2001), earnings announcements (Bernard and Thomas 1989), dividend changes (Aharony and Swary 1980), regulatory changes (Schwert 1981), executive turnover (Warner, Watts, and Wruck 1988), and macroeconomic announcements (Flannery and Protopapadakis 2002). In law and economics, event studies serve as the primary tool for measuring damages in securities fraud litigation (Mitchell and Netter 1993) and assessing the impact of regulatory interventions (Binder 1998). Kothari and Warner (2007) documented over 500 published event studies in the top five finance journals alone between 1974 and 2000.\nThe enduring popularity of event studies stems from several compelling properties:",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "17_event_studies.html#literature-review-and-methodological-evolution",
    "href": "17_event_studies.html#literature-review-and-methodological-evolution",
    "title": "14  Event Studies in Finance",
    "section": "14.1 Literature Review and Methodological Evolution",
    "text": "14.1 Literature Review and Methodological Evolution\n\n14.1.1 The Classical Framework (1969-1985)\nThe modern event study traces its origins to Eugene F. Fama et al. (1969), hereafter FFJR, who examined monthly stock returns around 940 stock splits between 1927 and 1959. Their key innovation was the use of the market model to decompose returns into expected (normal) and unexpected (abnormal) components.\nBall and Brown (2013) independently developed a similar approach to study earnings announcements, establishing the information content of accounting data. It was a finding with profound implications for both the efficient markets hypothesis and the relevance of financial reporting.\nBrown and Warner (1980) provided the first systematic analysis of event study methodology using simulation. Their study of monthly data established several important results: (i) the simple market model performs at least as well as more complex models, (ii) value-weighted market indices can lead to misspecification when the sample is tilted toward smaller firms, and (iii) the standard cross-sectional test has well-specified size under the null hypothesis. Their follow-up study (Brown and Warner 1985) extended the analysis to daily data, documenting the importance of non-normality in daily returns and the increased power of daily versus monthly studies.\n\n\n14.1.2 Risk Model Refinements (1992-2015)\nThe advent of the Fama-French three-factor model (Eugene F. Fama and French 1993) represented a major advance in modeling expected returns. Adding size (SMB) and value (HML) factors to the market model improved the cross-sectional fit of expected returns considerably. Carhart (1997) augmented this with a momentum factor (UMD), yielding the four-factor model that became standard in event studies through the 2000s. Eugene F. Fama and French (2015) subsequently introduced profitability (RMW) and investment (CMA) factors in their five-factor model.\nThe choice of risk model matters for event studies primarily in long-horizon settings. Kothari and Warner (2007) showed that for short-window studies (3-5 days), the market model and multi-factor models produce virtually identical results because the incremental factors explain very little daily return variation for individual firms. However, for event windows exceeding 20 trading days, model choice can materially affect inferences.\n\n\n14.1.3 Testing for Abnormal Returns (1976-2010)\nThe statistical testing of abnormal returns has evolved considerably:\n\n\n\nTable 14.1: Summary of major event study test statistics\n\n\n\n\n\n\n\n\n\n\n\nTest\nYear\nKey Property\nReference\n\n\n\n\nPatell Z\n1976\nStandardizes by estimation-period \\(\\sigma\\); weights firms inversely by volatility\nPatell (1976)\n\n\nCross-Sectional \\(t\\)\n1980\nAllows event-induced variance change\nBrown and Warner (1980)\n\n\nBMP\n1991\nRobust to event-induced variance\nBoehmer, Masumeci, and Poulsen (1991)\n\n\nCorrado Rank\n1989\nNon-parametric; robust to non-normality\nCorrado (1989)\n\n\nGeneralized Sign\n1992\nNon-parametric; uses estimation-window baseline\nCowan (1992)\n\n\nKolari-Pynnönen\n2010\nAccounts for cross-sectional dependence\nKolari and Pynnönen (2010)\n\n\nSkewness-Adjusted\n1992\nCorrects for BHAR skewness\nHall (1992)\n\n\n\n\n\n\n\n\n14.1.4 CARs versus BHARs\nCumulative abnormal returns (CARs) sum daily abnormal returns, while buy-and-hold abnormal returns (BHARs) compound returns and subtract the compounded benchmark. Barber and Lyon (1997) demonstrated that BHARs better capture the actual investor experience, since investors earn compound, not cumulative, returns. However, Eugene F. Fama (1998) and Mitchell and Stafford (2000) showed that BHARs exhibit severe cross-sectional dependence and positive skewness. For short event windows (under 10 days), the difference between CARs and BHARs is negligible. For longer windows, both should be reported.\n\n\n14.1.5 Emerging Market Considerations\nEvent studies in emerging markets face distinct challenges:\n\nThin trading. Many emerging market securities trade infrequently, inducing bias in market model beta estimates. Scholes and Williams (1977) and Dimson (1979) proposed corrections using leading and lagging market returns.\nFactor availability. While Fama-French factors are readily available for developed markets, emerging market factors must often be constructed locally.\nMarket microstructure. Price limits (\\(\\pm\\) 7% on HOSE, \\(\\pm\\) 10% on HNX, \\(\\pm\\) 15% on UPCOM in Vietnam), T+2 settlement, and the absence of short-selling affect the speed of price adjustment. Researchers should consider wider event windows to accommodate slower information incorporation (Bhattacharya et al. 2000; Griffin, Kelly, and Nardari 2010).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "17_event_studies.html#mathematical-framework",
    "href": "17_event_studies.html#mathematical-framework",
    "title": "14  Event Studies in Finance",
    "section": "14.2 Mathematical Framework",
    "text": "14.2 Mathematical Framework\nThis section presents the complete mathematical specification of the event study methodology. We follow the notation conventions of Campbell et al. (1998) and Kothari and Warner (2007).\n\n14.2.1 Timeline and Windows\nThe event study timeline is defined relative to the event date, denoted \\(\\tau = 0\\). All dates are measured in trading days:\n\\[\n\\underbrace{T_0 + 1, \\ldots, T_1}_{\\text{Estimation Window (L₁ days)}} \\quad \\underbrace{\\quad}_{\\text{Gap (G days)}} \\quad \\underbrace{\\tau_1, \\ldots, 0, \\ldots, \\tau_2}_{\\text{Event Window (L₂ days)}}\n\\]\nwhere:\n\nEstimation window: \\(L_1\\) trading days over which the risk model parameters are estimated\nGap: \\(G\\) trading days separating estimation and event windows, preventing contamination by pre-event information leakage\nEvent window: \\(L_2 = \\tau_2 - \\tau_1 + 1\\) trading days centered around the event date\n\nFor example, with \\(L_1 = 150\\), \\(G = 15\\), \\(\\tau_1 = -10\\), \\(\\tau_2 = +10\\): the estimation window covers trading days \\([-175, -25]\\) relative to the event, and the event window covers \\([-10, +10]\\).\n\n\n14.2.2 Normal Return Models\nLet \\(R_{it}\\) denote the return on security \\(i\\) on trading day \\(t\\), \\(R_{ft}\\) the risk-free rate, and \\(R_{mt}\\) the market return. We implement six models:\nModel 0: Market-Adjusted Returns. Assumes \\(\\beta_i = 1\\) and \\(\\alpha_i = 0\\) for all firms:\n\\[\nAR_{it}^{MA} = R_{it} - R_{mt}\n\\]\nModel 1: Market Model (Sharpe 1964):\n\\[\nR_{it} = \\alpha_i + \\beta_i R_{mt} + \\varepsilon_{it}, \\quad E[\\varepsilon_{it}] = 0, \\quad \\text{Var}[\\varepsilon_{it}] = \\sigma^2_{\\varepsilon_i}\n\\]\n\\[\nAR_{it}^{MM} = R_{it} - \\hat{\\alpha}_i - \\hat{\\beta}_i R_{mt}\n\\]\nModel 2: Fama-French Three-Factor (Eugene F. Fama and French 1993):\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\varepsilon_{it}\n\\]\nModel 3: Carhart Four-Factor (Carhart 1997):\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\beta_{i,4} \\cdot UMD_t + \\varepsilon_{it}\n\\]\nModel 4: Fama-French Five-Factor (Eugene F. Fama and French 2015):\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\beta_{i,4} \\cdot RMW_t + \\beta_{i,5} \\cdot CMA_t + \\varepsilon_{it}\n\\]\nModel 5: User-Specified Factor Model:\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\sum_{k=1}^{K} \\beta_{i,k} F_{k,t} + \\varepsilon_{it}\n\\]\n\n\n14.2.3 Aggregation: CARs and BHARs\nCumulative Abnormal Returns sum daily abnormal returns:\n\\[\nCAR_i(\\tau_1, \\tau_2) = \\sum_{t=\\tau_1}^{\\tau_2} AR_{it}, \\qquad \\overline{CAR}(\\tau_1, \\tau_2) = \\frac{1}{N} \\sum_{i=1}^{N} CAR_i(\\tau_1, \\tau_2)\n\\]\nBuy-and-Hold Abnormal Returns compound returns:\n\\[\nBHAR_i(\\tau_1, \\tau_2) = \\prod_{t=\\tau_1}^{\\tau_2}(1 + R_{it}) - \\prod_{t=\\tau_1}^{\\tau_2}(1 + \\hat{E}[R_{it}])\n\\]\n\n\n14.2.4 Standardized Returns\nThe standardized abnormal return for firm \\(i\\) on day \\(t\\) is:\n\\[\nSAR_{it} = \\frac{AR_{it}}{\\hat{\\sigma}_{\\varepsilon_i}}\n\\]\nThe standardized cumulative abnormal return is:\n\\[\nSCAR_i(\\tau_1, \\tau_2) = \\frac{CAR_i(\\tau_1, \\tau_2)}{\\hat{\\sigma}_{\\varepsilon_i} \\sqrt{L_2}}\n\\]\n\n\n14.2.5 Test Statistics\nLet \\(N\\) denote the number of firm-event observations.\nTest 1: Cross-Sectional \\(t\\)-Test. Allows event-induced variance; assumes cross-sectional independence:\n\\[\nt_{CS} = \\frac{\\overline{CAR}}{s_{CAR}/\\sqrt{N}}, \\quad s_{CAR} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(CAR_i - \\overline{CAR})^2}\n\\]\nTest 2: Patell Z-Test (Patell 1976). Weights firms inversely by volatility:\n\\[\nZ_{Patell} = \\frac{\\sum_{i=1}^{N} SCAR_i}{\\sqrt{\\sum_{i=1}^{N} \\frac{K_i - 2}{K_i - 4}}}\n\\]\nTest 3: BMP Test (Boehmer, Masumeci, and Poulsen 1991). Robust to event-induced variance:\n\\[\nt_{BMP} = \\frac{\\overline{SCAR}}{s_{SCAR}/\\sqrt{N}}\n\\]\nTest 4: Kolari-Pynnönen Adjusted BMP (Kolari and Pynnönen 2010). Accounts for cross-sectional dependence:\n\\[\nt_{KP} = t_{BMP} \\times \\sqrt{\\frac{1}{1 + (N-1)\\bar{r}}}\n\\]\nwhere \\(\\bar{r}\\) is the mean pairwise cross-correlation of estimation-period residuals.\nTest 5: Generalized Sign Test (Cowan 1992):\n\\[\nZ_{GSign} = \\frac{\\hat{p} - \\hat{p}_0}{\\sqrt{\\hat{p}_0(1-\\hat{p}_0)/N}}\n\\]\nTest 6: Sign Test:\n\\[\nZ_{Sign} = \\frac{N^{+} - 0.5N}{\\sqrt{0.25N}}\n\\]\nTest 7: Skewness-Adjusted \\(t\\)-Test (Hall 1992):\n\\[\nt_{SA} = \\sqrt{N}\\left(\\bar{z} + \\frac{1}{3}\\hat{\\gamma}\\bar{z}^2 + \\frac{1}{27}\\hat{\\gamma}^2\\bar{z}^3 + \\frac{1}{6N}\\hat{\\gamma}\\right)\n\\]\nTest 8: Wilcoxon Signed-Rank Test: A non-parametric test of whether the median CAR differs from zero.\nThe table below summarizes the assumptions of each test:\n\n\n\nTable 14.2: Assumption requirements for event study test statistics\n\n\n\n\n\n\n\n\n\n\n\nTest\nEvent-Induced Variance\nCross-Sectional Independence\nNormality\n\n\n\n\nCross-Sectional \\(t\\)\nRobust\nAssumes\nAssumes\n\n\nPatell Z\nAssumes no change\nAssumes\nAssumes\n\n\nBMP\nRobust\nAssumes\nAssumes\n\n\nKolari-Pynnönen\nRobust\nRobust\nAssumes\n\n\nGeneralized Sign\nRobust\nAssumes\nRobust\n\n\nCorrado Rank\nRobust\nAssumes\nRobust\n\n\nSkewness-Adjusted\nRobust\nAssumes\nPartially\n\n\nWilcoxon\nRobust\nAssumes\nRobust",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "17_event_studies.html#python-implementation",
    "href": "17_event_studies.html#python-implementation",
    "title": "14  Event Studies in Finance",
    "section": "14.3 Python Implementation",
    "text": "14.3 Python Implementation\n\n14.3.1 Design Philosophy\nOur implementation follows these principles:\n\nModularity: Each component (calendar, estimation, AR computation, testing) is a separate function.\nVectorization: All operations use pandas/numpy for performance on large datasets.\nConfigurability: All parameters are user-configurable via a dataclass.\nTransparency: Intermediate outputs are preserved for inspection.\nProduction-ready: Comprehensive input validation, missing data handling, and edge cases.\n\n\n\n14.3.2 Setup and Imports\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Tuple\nfrom enum import Enum\nimport warnings\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', '{:.6f}'.format)\nprint(\"All libraries loaded.\")\n\nAll libraries loaded.\n\n\n\nimport pandas as pd\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_ff3_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff3_daily\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff5_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_daily\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff5_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\n\n\n14.3.3 Configuration\n\nclass RiskModel(Enum):\n    \"\"\"Supported risk models for expected return computation.\"\"\"\n    MARKET_ADJ = \"market_adjusted\"\n    MARKET_MODEL = \"market_model\"\n    FF3 = \"ff3\"\n    CARHART = \"carhart\"\n    FF5 = \"ff5\"\n    CUSTOM = \"custom\"\n\n@dataclass\nclass EventStudyConfig:\n    \"\"\"Complete configuration for an event study.\n    \n    Attributes\n    ----------\n    estimation_window : int\n        Length of estimation period in trading days. Brown and Warner (1985)\n        suggest ≥100 days; MacKinlay (1997) recommends 120 as standard.\n    event_window_start : int\n        Start of event window relative to event date (e.g., -10).\n    event_window_end : int\n        End of event window relative to event date (e.g., +10).\n    gap : int\n        Trading days between estimation and event windows. Prevents\n        contamination from pre-event information leakage.\n    min_estimation_obs : int\n        Minimum non-missing returns required in estimation period.\n    risk_model : RiskModel\n        Risk model for computing expected returns.\n    custom_factors : list\n        Column names for user-specified factors (CUSTOM model only).\n    thin_trading_adj : str or None\n        None, 'scholes_williams', or 'dimson'.\n    dimson_lags : int\n        Number of leads/lags for Dimson (1979) correction.\n    \"\"\"\n    estimation_window: int = 150\n    event_window_start: int = -10\n    event_window_end: int = 10\n    gap: int = 15\n    min_estimation_obs: int = 120\n    risk_model: RiskModel = RiskModel.MARKET_MODEL\n    custom_factors: List[str] = field(default_factory=list)\n    thin_trading_adj: Optional[str] = None\n    dimson_lags: int = 1\n    \n    @property\n    def event_window_length(self) -&gt; int:\n        return self.event_window_end - self.event_window_start + 1\n    \n    def validate(self):\n        assert self.estimation_window &gt; 0\n        assert self.event_window_start &lt;= self.event_window_end\n        assert self.gap &gt;= 0\n        assert self.min_estimation_obs &lt;= self.estimation_window\n        if self.risk_model == RiskModel.CUSTOM:\n            assert len(self.custom_factors) &gt; 0\n        return True\n\n# Demonstrate\nconfig_demo = EventStudyConfig(\n    estimation_window=150, event_window_start=-10, event_window_end=10,\n    gap=15, min_estimation_obs=120, risk_model=RiskModel.FF3\n)\nconfig_demo.validate()\nprint(f\"Event window length: {config_demo.event_window_length} days\")\nprint(f\"Model: {config_demo.risk_model.value}\")\n\nEvent window length: 21 days\nModel: ff3\n\n\n\n\n14.3.4 Step 1: Trading Calendar Construction\nA correct trading calendar is fundamental. It maps any event date to the exact calendar dates for the start/end of estimation and event windows, accounting for weekends, holidays, and non-trading days.\n\ndef build_trading_calendar(trading_dates, config):\n    \"\"\"Build a trading calendar mapping event dates to window boundaries.\n    \n    For each potential event date, identifies the calendar dates for the\n    start/end of the estimation period and event window using only actual\n    trading days.\n    \n    Parameters\n    ----------\n    trading_dates : array-like\n        Sorted unique trading dates in the market.\n    config : EventStudyConfig\n    \n    Returns\n    -------\n    pd.DataFrame with columns: estper_beg, estper_end, evtwin_beg,\n        evtdate, evtwin_end, cal_index\n    \"\"\"\n    dates = pd.Series(sorted(pd.to_datetime(trading_dates).unique()))\n    n = len(dates)\n    \n    L1 = config.estimation_window\n    G = config.gap\n    s = config.event_window_start\n    L2 = config.event_window_length\n    \n    # Offsets (FIRSTOBS logic)\n    o0 = 0                      # estper_beg\n    o1 = L1 - 1                 # estper_end\n    o2 = L1 + G                 # evtwin_beg\n    o3 = L1 + G - s             # evtdate\n    o4 = L1 + G + L2 - 1        # evtwin_end\n    \n    max_offset = o4\n    valid = n - max_offset\n    if valid &lt;= 0:\n        raise ValueError(f\"Need ≥{max_offset+1} trading dates, have {n}\")\n    \n    cal = pd.DataFrame({\n        'estper_beg': dates.iloc[o0:o0+valid].values,\n        'estper_end': dates.iloc[o1:o1+valid].values,\n        'evtwin_beg': dates.iloc[o2:o2+valid].values,\n        'evtdate':    dates.iloc[o3:o3+valid].values,\n        'evtwin_end': dates.iloc[o4:o4+valid].values,\n    })\n    cal['cal_index'] = range(1, len(cal)+1)\n    \n    # Validate window lengths using a sample row\n    idx = min(10, len(cal)-1)\n    row = cal.iloc[idx]\n    est_n = dates[(dates &gt;= row['estper_beg']) & (dates &lt;= row['estper_end'])].shape[0]\n    evt_n = dates[(dates &gt;= row['evtwin_beg']) & (dates &lt;= row['evtwin_end'])].shape[0]\n    assert est_n == L1, f\"Estimation window: {est_n} ≠ {L1}\"\n    assert evt_n == L2, f\"Event window: {evt_n} ≠ {L2}\"\n    \n    return cal\n\n# Demo\ndemo_dates = pd.bdate_range('2018-01-01', '2023-12-31', freq='B')\ndemo_cal = build_trading_calendar(demo_dates, config_demo)\nprint(f\"Calendar: {len(demo_cal)} potential event dates\")\nprint(demo_cal.head(3).to_string(index=False))\n\nCalendar: 1380 potential event dates\nestper_beg estper_end evtwin_beg    evtdate evtwin_end  cal_index\n2018-01-01 2018-07-27 2018-08-20 2018-09-03 2018-09-17          1\n2018-01-02 2018-07-30 2018-08-21 2018-09-04 2018-09-18          2\n2018-01-03 2018-07-31 2018-08-22 2018-09-05 2018-09-19          3\n\n\n\n\n14.3.5 Step 2: Event Date Alignment\nWhen an event occurs on a non-trading day, align to the next available trading day.\n\ndef align_events(events, calendar, id_col='symbol', date_col='event_date'):\n    \"\"\"Align event dates to trading calendar.\n    \n    Non-trading-day events are shifted forward to the next trading day.\n    \n    Parameters\n    ----------\n    events : pd.DataFrame with [id_col, date_col] and optional 'group'\n    calendar : pd.DataFrame from build_trading_calendar()\n    \n    Returns\n    -------\n    pd.DataFrame with window boundaries for each firm-event\n    \"\"\"\n    events = events.copy()\n    events[date_col] = pd.to_datetime(events[date_col])\n    \n    cal_dates = calendar[['evtdate']].drop_duplicates().sort_values('evtdate')\n    \n    merged = pd.merge_asof(\n        events.sort_values(date_col),\n        cal_dates.rename(columns={'evtdate': 'aligned_date'}),\n        left_on=date_col, right_on='aligned_date',\n        direction='forward'\n    )\n    \n    result = merged.merge(calendar, left_on='aligned_date', right_on='evtdate', how='inner')\n    \n    shifted = (result[date_col] != result['evtdate']).sum()\n    if shifted &gt; 0:\n        print(f\"  {shifted} event(s) shifted to next trading day\")\n    \n    result = result.rename(columns={date_col: 'original_date'})\n    result = result.drop_duplicates(subset=[id_col, 'evtdate'])\n    \n    return result\n\n\n\n14.3.6 Step 3: Data Extraction and Factor Merging\nExtract returns for each security-event across the full estimation + event window and merge risk factors.\n\ndef extract_returns(aligned_events, prices, factors, config,\n                    id_col='symbol', date_col='date', ret_col='ret',\n                    mkt_col='mkt_excess', rf_col='risk_free'):\n    \"\"\"Extract stock returns and merge risk factors for each event.\n    \n    For each security-event, retrieves daily returns from estper_beg\n    through evtwin_end and merges appropriate risk factors.\n    \"\"\"\n    prices = prices.copy()\n    factors = factors.copy()\n    prices[date_col] = pd.to_datetime(prices[date_col])\n    factors[date_col] = pd.to_datetime(factors[date_col])\n    \n    # Recover raw return from excess return if needed\n    if ret_col not in prices.columns and 'ret_excess' in prices.columns:\n        if rf_col in factors.columns:\n            prices = prices.merge(factors[[date_col, rf_col]].drop_duplicates(),\n                                  on=date_col, how='left')\n        prices[ret_col] = prices['ret_excess'] + prices[rf_col]\n    \n    # Factor columns based on model\n    model = config.risk_model\n    fac_cols = [mkt_col] if mkt_col in factors.columns else []\n    if rf_col in factors.columns:\n        fac_cols.append(rf_col)\n    \n    model_factors = {\n        RiskModel.FF3: ['smb', 'hml'],\n        RiskModel.CARHART: ['smb', 'hml', 'umd'],\n        RiskModel.FF5: ['smb', 'hml', 'rmw', 'cma'],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n    for f in model_factors.get(model, []):\n        if f in factors.columns:\n            fac_cols.append(f)\n    \n    fac_cols = list(set([date_col] + fac_cols))\n    \n    # Vectorized merge approach: join events with prices on id + date range\n    frames = []\n    for _, evt in aligned_events.iterrows():\n        mask = ((prices[id_col] == evt[id_col]) &\n                (prices[date_col] &gt;= evt['estper_beg']) &\n                (prices[date_col] &lt;= evt['evtwin_end']))\n        fd = prices.loc[mask, [id_col, date_col, ret_col]].copy()\n        if len(fd) == 0:\n            continue\n        fd['evtdate'] = evt['evtdate']\n        fd['estper_beg'] = evt['estper_beg']\n        fd['estper_end'] = evt['estper_end']\n        fd['evtwin_beg'] = evt['evtwin_beg']\n        fd['evtwin_end'] = evt['evtwin_end']\n        if 'group' in evt.index:\n            fd['group'] = evt['group']\n        frames.append(fd)\n    \n    if not frames:\n        raise ValueError(\"No return data found for any events\")\n    \n    result = pd.concat(frames, ignore_index=True)\n    result = result.merge(factors[fac_cols].drop_duplicates(), on=date_col, how='left')\n    \n    # Excess and market-adjusted returns\n    if rf_col in result.columns:\n        result['ret_excess'] = result[ret_col] - result[rf_col]\n    else:\n        result['ret_excess'] = result[ret_col]\n    if mkt_col in result.columns:\n        result['ret_mktadj'] = result['ret_excess'] - result[mkt_col]\n    \n    result = result.sort_values([id_col, 'evtdate', date_col]).reset_index(drop=True)\n    n_evts = result.groupby([id_col, 'evtdate']).ngroups\n    print(f\"  Extracted {len(result):,} obs for {n_evts} firm-events\")\n    return result\n\n\n\n14.3.7 Step 4: Risk Model Estimation\nEstimate risk model parameters over the estimation window.\n\ndef estimate_model(\n    event_returns, config, id_col=\"symbol\", date_col=\"date\", ret_col=\"ret\"\n):\n    \"\"\"Estimate risk model parameters for each firm-event.\n\n    Runs OLS over the estimation window. Returns alpha, betas, sigma,\n    R^2, nobs, and residuals for cross-correlation computation.\n    \"\"\"\n    model = config.risk_model\n\n    # Define regression specification\n    dep_var_map = {\n        RiskModel.MARKET_ADJ: \"ret_mktadj\",\n        RiskModel.MARKET_MODEL: ret_col,\n        RiskModel.FF3: \"ret_excess\",\n        RiskModel.CARHART: \"ret_excess\",\n        RiskModel.FF5: \"ret_excess\",\n        RiskModel.CUSTOM: \"ret_excess\",\n    }\n    indep_var_map = {\n        RiskModel.MARKET_ADJ: [],\n        RiskModel.MARKET_MODEL: [\"mkt_excess\"],\n        RiskModel.FF3: [\"mkt_excess\", \"smb\", \"hml\"],\n        RiskModel.CARHART: [\"mkt_excess\", \"smb\", \"hml\", \"umd\"],\n        RiskModel.FF5: [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n\n    dep_var = dep_var_map[model]\n    indep_vars = indep_var_map[model]\n\n    est = event_returns[\n        (event_returns[date_col] &gt;= event_returns[\"estper_beg\"])\n        & (event_returns[date_col] &lt;= event_returns[\"estper_end\"])\n    ].copy()\n\n    params_list = []\n\n    for (firm, evtdate), grp in est.groupby([id_col, \"evtdate\"]):\n        valid = grp.dropna(subset=[dep_var] + indep_vars)\n        nobs = len(valid)\n        if nobs &lt; config.min_estimation_obs:\n            continue\n\n        y = valid[dep_var].values\n\n        if len(indep_vars) == 0:\n            # Market-adjusted: intercept-only for variance\n            p = {\n                id_col: firm,\n                \"evtdate\": evtdate,\n                \"alpha\": y.mean(),\n                \"sigma\": y.std(ddof=1),\n                \"variance\": y.var(ddof=1),\n                \"nobs\": nobs,\n                \"r_squared\": 0.0,\n                \"_residuals\": y - y.mean(),\n            }\n        else:\n            X = sm.add_constant(valid[indep_vars].values)\n            res = sm.OLS(y, X).fit()\n            p = {\n                id_col: firm,\n                \"evtdate\": evtdate,\n                \"alpha\": res.params[0],\n                \"sigma\": np.sqrt(res.mse_resid),\n                \"variance\": res.mse_resid,\n                \"nobs\": nobs,\n                \"r_squared\": res.rsquared if np.isfinite(res.rsquared) else np.nan,\n                \"_residuals\": res.resid,\n            }\n            for j, var in enumerate(indep_vars):\n                p[f\"beta_{var}\"] = res.params[j + 1]\n\n        # Skip degenerate firms (zero or near-zero variance)\n        if p[\"sigma\"] &lt; 1e-6:\n            continue\n\n        params_list.append(p)\n\n    if not params_list:\n        raise ValueError(\"No firm-events passed minimum observation filter\")\n\n    params_df = pd.DataFrame(params_list)\n    n_total = event_returns.groupby([id_col, \"evtdate\"]).ngroups\n    print(\n        f\"  Estimated {len(params_df)}/{n_total} firm-events \"\n        f\"(mean R^2 = {params_df['r_squared'].dropna().mean():.4f})\"\n    )\n    return params_df\n\n\n\n14.3.8 Step 5: Abnormal Return Computation\nCompute AR, CAR, BHAR, SAR, SCAR for each firm-event-date.\n\ndef compute_abnormal_returns(\n    event_returns, params, config, id_col=\"symbol\", date_col=\"date\", ret_col=\"ret\"\n):\n    \"\"\"Compute abnormal returns and aggregate to CARs/BHARs.\n\n    Returns\n    -------\n    daily_ar : pd.DataFrame - daily AR/SAR/CAR/BHAR per firm-event-date\n    event_ar : pd.DataFrame - event-level CAR/BHAR/SCAR per firm-event\n    \"\"\"\n    model = config.risk_model\n\n    factor_map = {\n        RiskModel.MARKET_ADJ: [],\n        RiskModel.MARKET_MODEL: [\"mkt_excess\"],\n        RiskModel.FF3: [\"mkt_excess\", \"smb\", \"hml\"],\n        RiskModel.CARHART: [\"mkt_excess\", \"smb\", \"hml\", \"umd\"],\n        RiskModel.FF5: [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n    factor_cols = factor_map[model]\n\n    # Filter to event window\n    evt = event_returns[\n        (event_returns[date_col] &gt;= event_returns[\"evtwin_beg\"])\n        & (event_returns[date_col] &lt;= event_returns[\"evtwin_end\"])\n    ].copy()\n\n    # Merge params (drop residuals column for merge)\n    merge_cols = [c for c in params.columns if c != \"_residuals\"]\n    evt = evt.merge(params[merge_cols], on=[id_col, \"evtdate\"], how=\"inner\")\n\n    # Expected returns\n    if model == RiskModel.MARKET_ADJ:\n        evt[\"expected_ret\"] = evt.get(\"mkt_excess\", 0) + evt.get(\"risk_free\", 0)\n        evt[\"AR\"] = evt[ret_col] - evt[\"expected_ret\"]\n    else:\n        evt[\"expected_ret\"] = evt[\"alpha\"]\n        for fc in factor_cols:\n            bcol = f\"beta_{fc}\"\n            if bcol in evt.columns:\n                evt[\"expected_ret\"] += evt[bcol] * evt[fc]\n\n        if model == RiskModel.MARKET_MODEL:\n            evt[\"AR\"] = evt[ret_col] - evt[\"expected_ret\"]\n        else:\n            evt[\"AR\"] = evt[\"ret_excess\"] - evt[\"expected_ret\"]\n\n    evt[\"SAR\"] = evt[\"AR\"] / evt[\"sigma\"]\n    evt = evt.sort_values([id_col, \"evtdate\", date_col])\n\n    # Compute event time\n    all_dates = sorted(event_returns[date_col].unique())\n    d2i = {d: i for i, d in enumerate(all_dates)}\n    evt[\"evttime\"] = evt[date_col].map(d2i) - evt[\"evtdate\"].map(d2i)\n\n    # Cumulative measures per firm-event\n    daily_recs = []\n    event_recs = []\n\n    for (firm, evtdate), g in evt.groupby([id_col, \"evtdate\"]):\n        g = g.sort_values(date_col).copy()\n        nd = len(g)\n\n        g[\"CAR\"] = g[\"AR\"].cumsum()\n        g[\"cum_ret\"] = (1 + g[ret_col]).cumprod() - 1\n        g[\"cum_expected\"] = (1 + g[\"expected_ret\"]).cumprod() - 1\n        g[\"BHAR\"] = g[\"cum_ret\"] - g[\"cum_expected\"]\n        g[\"SCAR\"] = g[\"CAR\"] / (g[\"sigma\"].iloc[0] * np.sqrt(np.arange(1, nd + 1)))\n\n        daily_recs.append(g)\n\n        last = g.iloc[-1]\n        sigma = g[\"sigma\"].iloc[0]\n        nobs = g[\"nobs\"].iloc[0]\n\n        rec = {\n            id_col: firm,\n            \"evtdate\": evtdate,\n            \"CAR\": last[\"CAR\"],\n            \"BHAR\": last[\"BHAR\"],\n            \"cum_ret\": last[\"cum_ret\"],\n            \"SCAR\": last[\"CAR\"] / (sigma * np.sqrt(nd)),\n            \"sigma\": sigma,\n            \"variance\": g[\"variance\"].iloc[0],\n            \"nobs\": nobs,\n            \"n_event_days\": nd,\n            \"alpha\": g[\"alpha\"].iloc[0],\n            \"pat_scale\": (nobs - 2) / (nobs - 4) if nobs &gt; 4 else np.nan,\n            \"pos_car\": int(last[\"CAR\"] &gt; 0),\n        }\n\n        for fc in factor_cols:\n            bcol = f\"beta_{fc}\"\n            if bcol in g.columns:\n                rec[bcol] = g[bcol].iloc[0]\n        if \"group\" in g.columns:\n            rec[\"group\"] = g[\"group\"].iloc[0]\n\n        event_recs.append(rec)\n\n    daily_ar = pd.concat(daily_recs, ignore_index=True)\n    event_ar = pd.DataFrame(event_recs)\n\n    print(\n        f\"  {len(event_ar)} firm-events | Mean CAR: {event_ar['CAR'].mean():.6f} | \"\n        f\"Mean BHAR: {event_ar['BHAR'].mean():.6f} | \"\n        f\"% positive: {event_ar['pos_car'].mean():.1%}\"\n    )\n    return daily_ar, event_ar\n\n\n\n14.3.9 Step 6: Comprehensive Test Statistics\nEight tests covering parametric, non-parametric, and cross-correlation-robust approaches.\n\ndef compute_test_statistics(event_ar, params=None, group_col=None):\n    \"\"\"Compute comprehensive test statistics for abnormal returns.\n    \n    Implements 8 tests with varying assumptions about variance,\n    cross-dependence, and distributional form.\n    \"\"\"\n    def _stats(data, label=None):\n        N = len(data)\n        if N &lt; 3:\n            return None\n        \n        cars = data['CAR'].values\n        bhars = data['BHAR'].values\n        scars = data['SCAR'].values\n        pos = data['pos_car'].values\n        \n        m_car, s_car = np.mean(cars), np.std(cars, ddof=1)\n        m_scar, s_scar = np.mean(scars), np.std(scars, ddof=1)\n        \n        r = {'group': label or 'All', 'N': N,\n             'mean_CAR': m_car, 'median_CAR': np.median(cars),\n             'std_CAR': s_car, 'mean_BHAR': np.mean(bhars),\n             'pct_positive': np.mean(pos)}\n        \n        # 1. Cross-sectional t\n        t1 = m_car / (s_car / np.sqrt(N)) if s_car &gt; 0 else np.nan\n        r['t_CS'] = t1\n        r['p_CS'] = 2 * (1 - stats.t.cdf(abs(t1), N-1)) if np.isfinite(t1) else np.nan\n        \n        # 2. Patell Z\n        if 'pat_scale' in data.columns:\n            ps = data['pat_scale'].dropna().values\n            z2 = np.sum(scars[:len(ps)]) / np.sqrt(np.sum(ps)) if len(ps) &gt; 0 else np.nan\n        else:\n            z2 = m_scar * np.sqrt(N)\n        r['Z_Patell'] = z2\n        r['p_Patell'] = 2*(1-stats.norm.cdf(abs(z2))) if np.isfinite(z2) else np.nan\n        \n        # 3. BMP\n        t3 = m_scar / (s_scar / np.sqrt(N)) if s_scar &gt; 0 else np.nan\n        r['t_BMP'] = t3\n        r['p_BMP'] = 2*(1-stats.t.cdf(abs(t3), N-1)) if np.isfinite(t3) else np.nan\n        \n        # 4. Kolari-Pynnönen\n        rbar = 0.0\n        if params is not None and '_residuals' in params.columns:\n            resids = [row['_residuals'] for _, row in params.iterrows()\n                      if isinstance(row.get('_residuals'), np.ndarray)]\n            if len(resids) &gt; 1:\n                ml = min(len(x) for x in resids)\n                aligned = np.column_stack([x[:ml] for x in resids])\n                cm = np.corrcoef(aligned.T)\n                np.fill_diagonal(cm, 0)\n                rbar = cm.sum() / (len(resids) * (len(resids)-1))\n        \n        adj = np.sqrt(1/(1+(N-1)*rbar)) if (1+(N-1)*rbar) &gt; 0 else 1\n        t4 = t3 * adj if np.isfinite(t3) else np.nan\n        r['t_KP'] = t4\n        r['p_KP'] = 2*(1-stats.t.cdf(abs(t4), N-1)) if np.isfinite(t4) else np.nan\n        r['r_bar'] = rbar\n        \n        # 5. Generalized sign test\n        p_hat = np.mean(pos)\n        z5 = (p_hat - 0.5) / np.sqrt(0.25 / N)\n        r['Z_GSign'] = z5\n        r['p_GSign'] = 2*(1-stats.norm.cdf(abs(z5)))\n        \n        # 6. Sign test\n        r['Z_Sign'] = z5  # Same formula with p0=0.5\n        r['p_Sign'] = r['p_GSign']\n        \n        # 7. Skewness-adjusted t\n        if s_scar &gt; 0:\n            zb = m_scar / s_scar\n            gam = stats.skew(scars)\n            t7 = np.sqrt(N) * (zb + gam*zb**2/3 + gam**2*zb**3/27 + gam/(6*N))\n            r['t_SkAdj'] = t7\n            r['p_SkAdj'] = 2*(1-stats.t.cdf(abs(t7), N-1)) if np.isfinite(t7) else np.nan\n        \n        # 8. Wilcoxon signed-rank\n        try:\n            w, pw = stats.wilcoxon(cars, alternative='two-sided')\n            r['W_Wilcoxon'] = w\n            r['p_Wilcoxon'] = pw\n        except:\n            r['W_Wilcoxon'] = r['p_Wilcoxon'] = np.nan\n        \n        return r\n    \n    results = [_stats(event_ar)]\n    if group_col and group_col in event_ar.columns:\n        for gv, gd in event_ar.groupby(group_col):\n            s = _stats(gd, label=gv)\n            if s:\n                results.append(s)\n    \n    return pd.DataFrame([r for r in results if r is not None])\n\n\ndef compute_daily_stats(daily_ar, id_col='symbol'):\n    \"\"\"Compute test statistics at each event time t.\"\"\"\n    rows = []\n    for t, g in daily_ar.groupby('evttime'):\n        n = g[id_col].nunique()\n        if n &lt; 2:\n            continue\n        m_ar = g['AR'].mean()\n        s_ar = g['AR'].std(ddof=1)\n        t_ar = m_ar / (s_ar/np.sqrt(n)) if s_ar &gt; 0 else np.nan\n        rows.append({'evttime': t, 'N': n, 'mean_AR': m_ar,\n                     'mean_CAR': g['CAR'].mean(), 'mean_BHAR': g['BHAR'].mean(),\n                     'mean_cum_ret': g.get('cum_ret', pd.Series()).mean(),\n                     't_AR': t_ar})\n    return pd.DataFrame(rows).sort_values('evttime')\n\n\n\n14.3.10 Step 7: Publication-Ready Visualization\n\ndef plot_event_study(daily_stats, title=\"Cumulative Abnormal Returns Around Event Date\",\n                     figsize=(12, 7), save_path=None):\n    \"\"\"Publication-ready event study plot with CAR, BHAR, and daily AR panels.\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=figsize, height_ratios=[3, 1],\n                              gridspec_kw={'hspace': 0.05})\n    ds = daily_stats.sort_values('evttime')\n    t = ds['evttime'].values\n    \n    # Top: cumulative returns\n    ax = axes[0]\n    ax.plot(t, ds['mean_CAR']*100, color='#2166AC', lw=2.5, label='Mean CAR')\n    ax.plot(t, ds['mean_BHAR']*100, color='#B2182B', lw=2, ls='--', label='Mean BHAR')\n    if 'mean_cum_ret' in ds.columns:\n        ax.plot(t, ds['mean_cum_ret']*100, color='#666', lw=1.5, ls=':', \n                label='Mean Cum. Return', alpha=0.7)\n    ax.axvline(0, color='k', lw=0.8, alpha=0.5)\n    ax.axhline(0, color='k', lw=0.5, alpha=0.3)\n    ax.set_ylabel('Cumulative Return (%)', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.legend(loc='upper left', fontsize=10)\n    ax.grid(True, alpha=0.2)\n    ax.set_xticklabels([])\n    \n    # Bottom: daily AR bars\n    ax2 = axes[1]\n    colors = ['#2166AC' if v &gt;= 0 else '#B2182B' for v in ds['mean_AR']]\n    ax2.bar(t, ds['mean_AR']*100, color=colors, alpha=0.7, width=0.8)\n    if 't_AR' in ds.columns:\n        sig = np.abs(ds['t_AR'].values) &gt; 1.96\n        if sig.any():\n            ax2.scatter(t[sig], ds['mean_AR'].values[sig]*100, \n                       color='gold', s=40, marker='*', zorder=4, label='p&lt;0.05')\n            ax2.legend(fontsize=9)\n    ax2.axvline(0, color='k', lw=0.8, alpha=0.5)\n    ax2.axhline(0, color='k', lw=0.5, alpha=0.3)\n    ax2.set_xlabel('Event Time (Trading Periods)', fontsize=12)\n    ax2.set_ylabel('Mean AR (%)', fontsize=10)\n    ax2.grid(True, alpha=0.2)\n    \n    for a in axes:\n        a.spines['top'].set_visible(False)\n        a.spines['right'].set_visible(False)\n    plt.tight_layout()\n    if save_path:\n        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n    return fig\n\n\ndef plot_car_distribution(event_ar, var='CAR', figsize=(12, 5)):\n    \"\"\"Cross-sectional distribution of CARs with histogram and QQ plot.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n    data = event_ar[var].dropna() * 100\n    \n    ax1.hist(data, bins=50, density=True, alpha=0.6, color='#2166AC', edgecolor='white')\n    ax1.axvline(data.mean(), color='k', ls='--', lw=1.5, \n                label=f'Mean={data.mean():.2f}%')\n    ax1.axvline(data.median(), color='gray', ls=':', lw=1.5,\n                label=f'Median={data.median():.2f}%')\n    ax1.set_xlabel(f'{var} (%)')\n    ax1.set_ylabel('Density')\n    ax1.set_title(f'Distribution of {var}', fontweight='bold')\n    ax1.legend()\n    ax1.spines['top'].set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    \n    # QQ plot\n    (osm, osr), (slope, intercept, r) = stats.probplot(data, dist='norm')\n    ax2.scatter(osm, osr, alpha=0.4, s=10, color='#2166AC')\n    ax2.plot(osm, slope*np.array(osm)+intercept, 'r--', lw=1)\n    ax2.set_xlabel('Theoretical Quantiles')\n    ax2.set_ylabel('Sample Quantiles')\n    ax2.set_title('Q-Q Plot (Normal)', fontweight='bold')\n    ax2.spines['top'].set_visible(False)\n    ax2.spines['right'].set_visible(False)\n    \n    plt.tight_layout()\n    return fig\n\n\n\n14.3.11 The Master Pipeline\nCombine all components into one function:\n\ndef run_event_study(events, prices, factors, config,\n                    id_col='symbol', date_col='date', ret_col='ret',\n                    event_date_col='event_date', mkt_col='mkt_excess',\n                    rf_col='risk_free', group_col=None, verbose=True):\n    \"\"\"Run a complete event study from raw inputs to test statistics.\n    \n    This is the main entry point. Provide your events, price data,\n    factor data, and configuration—get back everything you need.\n    \n    Parameters\n    ----------\n    events : pd.DataFrame\n        Columns: [id_col, event_date_col], optional 'group'.\n    prices : pd.DataFrame\n        Daily returns: [id_col, date_col, ret_col or 'ret_excess', rf_col].\n    factors : pd.DataFrame\n        Factor returns: [date_col, mkt_col, 'smb', 'hml', ...].\n    config : EventStudyConfig\n    \n    Returns\n    -------\n    dict with keys: 'config', 'daily_ar', 'event_ar', 'daily_stats',\n        'test_stats', 'params'\n    \"\"\"\n    config.validate()\n    \n    if verbose:\n        print(f\"═══ Event Study: {config.risk_model.value} model ═══\")\n        print(f\"  Windows: estimation={config.estimation_window}, \"\n              f\"gap={config.gap}, event=({config.event_window_start},{config.event_window_end})\")\n        print(f\"  Min obs: {config.min_estimation_obs}\\n\")\n    \n    # 1. Trading calendar\n    if verbose: print(\"Step 1: Building trading calendar...\")\n    trading_dates = pd.Series(sorted(prices[date_col].unique()))\n    calendar = build_trading_calendar(trading_dates, config)\n    if verbose: print(f\"  {len(calendar)} potential event dates\\n\")\n    \n    # 2. Align events\n    if verbose: print(\"Step 2: Aligning events to trading calendar...\")\n    aligned = align_events(events, calendar, id_col, event_date_col)\n    if verbose: print(f\"  {len(aligned)} aligned events\\n\")\n    \n    # 3. Extract returns\n    if verbose: print(\"Step 3: Extracting returns and merging factors...\")\n    evt_rets = extract_returns(aligned, prices, factors, config,\n                               id_col, date_col, ret_col, mkt_col, rf_col)\n    if verbose: print()\n    \n    # 4. Estimate model\n    if verbose: print(\"Step 4: Estimating risk model parameters...\")\n    params = estimate_model(evt_rets, config, id_col, date_col, ret_col)\n    if verbose: print()\n    \n    # 5. Compute abnormal returns\n    if verbose: print(\"Step 5: Computing abnormal returns...\")\n    daily_ar, event_ar = compute_abnormal_returns(\n        evt_rets, params, config, id_col, date_col, ret_col)\n    if verbose: print()\n    \n    # 6. Test statistics\n    if verbose: print(\"Step 6: Computing test statistics...\")\n    test_stats = compute_test_statistics(event_ar, params, group_col)\n    daily_stats = compute_daily_stats(daily_ar, id_col)\n    if verbose:\n        print(f\"  Done.\\n\")\n        print(\"═══ Results Summary ═══\")\n        cols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 't_BMP', 'p_BMP', 't_KP', 'p_KP']\n        avail = [c for c in cols if c in test_stats.columns]\n        print(test_stats[avail].to_string(index=False))\n    \n    return {\n        'config': config,\n        'params': params,\n        'daily_ar': daily_ar,\n        'event_ar': event_ar,\n        'daily_stats': daily_stats,\n        'test_stats': test_stats,\n        'calendar': calendar,\n    }\n\nprint(\"Master pipeline ready.\")\n\nMaster pipeline ready.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "17_event_studies.html#demonstration-with-simulated-data",
    "href": "17_event_studies.html#demonstration-with-simulated-data",
    "title": "14  Event Studies in Finance",
    "section": "14.4 Demonstration with Simulated Data",
    "text": "14.4 Demonstration with Simulated Data\nSince we are building a general-purpose framework (the actual event data will be supplied later), we demonstrate the full pipeline with realistic simulated data.\n\nnp.random.seed(2024)\n\n# --- Simulated trading calendar (Vietnamese market: ~245 days/year) ---\ndates = pd.bdate_range('2019-01-01', '2023-12-31', freq='B')\n# Remove Tet + national holidays (simplified)\ntet_holidays = pd.to_datetime([\n    '2019-02-04','2019-02-05','2019-02-06','2019-02-07','2019-02-08',\n    '2020-01-23','2020-01-24','2020-01-27','2020-01-28','2020-01-29',\n    '2021-02-10','2021-02-11','2021-02-12','2021-02-15','2021-02-16',\n    '2022-01-31','2022-02-01','2022-02-02','2022-02-03','2022-02-04',\n    '2023-01-20','2023-01-23','2023-01-24','2023-01-25','2023-01-26',\n])\ndates = dates.difference(tet_holidays)\nT = len(dates)\n\n# --- Simulated factors (realistic Vietnamese market parameters) ---\nrf_daily = 0.04 / 252  # ~4% annual risk-free\nmkt_excess = np.random.normal(0.0003, 0.012, T)  # ~7.5% annual, ~19% vol\nsmb = np.random.normal(0.0001, 0.006, T)\nhml = np.random.normal(0.0001, 0.005, T)\nrmw = np.random.normal(0.00005, 0.004, T)\ncma = np.random.normal(0.00005, 0.004, T)\n\nfactors_sim = pd.DataFrame({\n    'date': dates, 'mkt_excess': mkt_excess, 'smb': smb, 'hml': hml,\n    'rmw': rmw, 'cma': cma, 'risk_free': rf_daily\n})\n\n# --- 100 simulated stocks ---\nn_stocks = 100\nsymbols = [f'SIM{i:03d}' for i in range(n_stocks)]\nbetas = np.random.uniform(0.5, 1.5, n_stocks)\nalphas = np.random.normal(0, 0.0002, n_stocks)\nidio_vols = np.random.uniform(0.015, 0.035, n_stocks)\n\nprice_rows = []\nfor i, sym in enumerate(symbols):\n    eps = np.random.normal(0, idio_vols[i], T)\n    rets = alphas[i] + betas[i] * mkt_excess + 0.3*smb + 0.2*hml + eps\n    for j in range(T):\n        price_rows.append({\n            'symbol': sym, 'date': dates[j], 'ret': rets[j],\n            'ret_excess': rets[j] - rf_daily,\n            'risk_free': rf_daily,\n            'mktcap': np.random.uniform(100, 5000),\n        })\n\nprices_sim = pd.DataFrame(price_rows)\n\n# --- Simulated events: 50 random firm-dates with KNOWN positive AR ---\nevent_indices = np.random.choice(range(250, T-50), 50, replace=False)\nevent_firms = np.random.choice(symbols, 50, replace=True)\nevent_dates_sim = [dates[i] for i in event_indices]\n\n# Inject abnormal returns on event date (2% positive shock)\nfor firm, edate in zip(event_firms, event_dates_sim):\n    mask = (prices_sim['symbol'] == firm) & (prices_sim['date'] == edate)\n    prices_sim.loc[mask, 'ret'] += 0.02\n    prices_sim.loc[mask, 'ret_excess'] += 0.02\n\nevents_sim = pd.DataFrame({\n    'symbol': event_firms,\n    'event_date': event_dates_sim,\n    'group': np.random.choice([1, 2], 50)\n})\n\nprint(f\"Simulated data: {n_stocks} stocks × {T} days = {len(prices_sim):,} obs\")\nprint(f\"Events: {len(events_sim)} firm-event pairs\")\nprint(f\"Injected abnormal return: +2% on event date\")\n\nSimulated data: 100 stocks × 1279 days = 127,900 obs\nEvents: 50 firm-event pairs\nInjected abnormal return: +2% on event date\n\n\n\n14.4.1 Running the Full Pipeline\n\nconfig = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\nresults = run_event_study(\n    events=events_sim,\n    prices=prices_sim,\n    factors=factors_sim,\n    config=config,\n    group_col='group'\n)\n\n═══ Event Study: ff3 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  1094 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 9,300 obs for 50 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 50/50 firm-events (mean R^2 = 0.2368)\n\nStep 5: Computing abnormal returns...\n  50 firm-events | Mean CAR: 0.033009 | Mean BHAR: 0.032498 | % positive: 60.0%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\ngroup  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n  All 50  0.033009   0.032498      0.600000 2.288362 0.026468 2.157106 0.035929 2.161291 0.035587\n    1 20  0.030704   0.028326      0.650000 1.568676 0.133227 1.248382 0.227056 1.249320 0.226720\n    2 30  0.034545   0.035280      0.566667 1.688848 0.101975 1.734699 0.093413 1.736689 0.093055\n\n\n\n\n14.4.2 Visualizing Results\n\nfig1 = plot_event_study(\n    results['daily_stats'],\n    title=\"Event Study: FF3 Model — Simulated Vietnamese Market\"\n)\nplt.show()\n\n\n\n\n\n\n\nFigure 14.1: Dynamics of cumulative abnormal returns (CARs) and buy-and-hold abnormal returns (BHARs) around the event date. The positive jump at t=0 reflects the injected 2% abnormal return.\n\n\n\n\n\n\nfig2 = plot_car_distribution(results['event_ar'], 'CAR')\nplt.show()\n\n\n\n\n\n\n\nFigure 14.2: Cross-sectional distribution of cumulative abnormal returns. The rightward shift from zero and positive skewness are consistent with the injected positive event effect.\n\n\n\n\n\n\n\n14.4.3 Complete Test Statistics\n\n\n\nTable 14.3: Event study test statistics for the full sample and by subgroup\n\n\n# Format for display\nts = results['test_stats'].copy()\n\n# Select key columns\ndisplay_cols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 'Z_Patell', 'p_Patell',\n                't_BMP', 'p_BMP', 't_KP', 'p_KP',\n                'Z_GSign', 'p_GSign', 't_SkAdj', 'p_SkAdj']\navail = [c for c in display_cols if c in ts.columns]\ndisplay_df = ts[avail].copy()\n\n# Format\nfor c in display_df.columns:\n    if c in ['N']:\n        display_df[c] = display_df[c].astype(int)\n    elif c.startswith('p_'):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4f}' if pd.notna(x) else '')\n    elif c in ['mean_CAR', 'mean_BHAR']:\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4%}' if pd.notna(x) else '')\n    elif c == 'pct_positive':\n        display_df[c] = display_df[c].map(lambda x: f'{x:.1%}' if pd.notna(x) else '')\n    elif isinstance(display_df[c].iloc[0], (int, float, np.floating)):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(display_df.to_string(index=False))\n\ngroup  N mean_CAR mean_BHAR pct_positive  t_CS   p_CS Z_Patell p_Patell t_BMP  p_BMP  t_KP   p_KP Z_GSign p_GSign t_SkAdj p_SkAdj\n  All 50  3.3009%   3.2498%        60.0% 2.288 0.0265    1.832   0.0669 2.157 0.0359 2.161 0.0356   1.414  0.1573   2.074  0.0434\n    1 20  3.0704%   2.8326%        65.0% 1.569 0.1332    1.020   0.3075 1.248 0.2271 1.249 0.2267   1.342  0.1797   1.103  0.2836\n    2 30  3.4545%   3.5280%        56.7% 1.689 0.1020    1.532   0.1255 1.735 0.0934 1.737 0.0931   0.730  0.4652   1.727  0.0949\n\n\n\n\n\n\n14.4.4 Running Multiple Models for Robustness\nA key best practice is to report results across multiple risk models. If conclusions are robust across models, this strengthens the findings:\n\nmodels_to_run = [\n    (\"Market-Adjusted\", RiskModel.MARKET_ADJ),\n    (\"Market Model\", RiskModel.MARKET_MODEL),\n    (\"Fama-French 3\", RiskModel.FF3),\n    (\"Fama-French 5\", RiskModel.FF5),\n]\n\nrobustness = []\nfor name, mdl in models_to_run:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=-10, event_window_end=10,\n        gap=15, min_estimation_obs=120, risk_model=mdl\n    )\n    res = run_event_study(events_sim, prices_sim, factors_sim, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    robustness.append({\n        'Model': name,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.2f}\",\n        't (BMP)': f\"{full['t_BMP']:.2f}\",\n        't (KP)': f\"{full.get('t_KP', np.nan):.2f}\",\n    })\n\nrob_df = pd.DataFrame(robustness)\nprint(\"Robustness Across Risk Models:\")\nprint(rob_df.to_string(index=False))\n\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.0000)\n  50 firm-events | Mean CAR: 0.029672 | Mean BHAR: 0.026468 | % positive: 62.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2198)\n  50 firm-events | Mean CAR: 0.033785 | Mean BHAR: 0.029974 | % positive: 62.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2368)\n  50 firm-events | Mean CAR: 0.033009 | Mean BHAR: 0.032498 | % positive: 60.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2516)\n  50 firm-events | Mean CAR: 0.036479 | Mean BHAR: 0.036000 | % positive: 64.0%\nRobustness Across Risk Models:\n          Model  N Mean CAR Mean BHAR % Positive t (CS) t (BMP) t (KP)\nMarket-Adjusted 50  2.9672%   2.6468%      62.0%   2.12    2.03   2.01\n   Market Model 50  3.3785%   2.9974%      62.0%   2.37    2.26   2.28\n  Fama-French 3 50  3.3009%   3.2498%      60.0%   2.29    2.16   2.16\n  Fama-French 5 50  3.6479%   3.6000%      64.0%   2.51    2.36   2.41",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "17_event_studies.html#sec-usage",
    "href": "17_event_studies.html#sec-usage",
    "title": "14  Event Studies in Finance",
    "section": "14.5 How to Use This Framework with Your Data",
    "text": "14.5 How to Use This Framework with Your Data\n\n14.5.1 Required Data Format\nTo run the event study on real Vietnamese market data, prepare three inputs:\n1. Stock Returns (prices DataFrame):\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nsymbol\nStock ticker\n'VNM'\n\n\ndate\nTrading date\n2023-06-15\n\n\nret or ret_excess\nDaily return (decimal)\n0.0123\n\n\nrisk_free\nDaily risk-free rate\n0.000159\n\n\n\n2. Factor Returns (factors DataFrame):\n\n\n\nColumn\nDescription\n\n\n\n\ndate\nTrading date\n\n\nmkt_excess\nMarket excess return\n\n\nsmb\nSize factor (FF3/FF5)\n\n\nhml\nValue factor (FF3/FF5)\n\n\nrmw\nProfitability factor (FF5)\n\n\ncma\nInvestment factor (FF5)\n\n\nrisk_free\nRisk-free rate\n\n\n\n3. Event File (events DataFrame):\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nsymbol\nStock ticker\n'VNM'\n\n\nevent_date\nEvent date\n2023-03-15\n\n\ngroup\n(Optional) subgroup\n1\n\n\n\n\n\n14.5.2 Minimal Usage Example\n# Load your data\nprices = pd.read_csv('prices_daily.csv', parse_dates=['date'])\nfactors = pd.read_csv('factors_ff3_daily.csv', parse_dates=['date'])\nevents = pd.read_csv('my_events.csv', parse_dates=['event_date'])\n\n# Configure\nconfig = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-5,\n    event_window_end=5,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\n# Run\nresults = run_event_study(events, prices, factors, config)\n\n# Access outputs\nresults['test_stats']    # Test statistics\nresults['event_ar']      # Firm-level CARs/BHARs\nresults['daily_ar']      # Daily abnormal returns\nresults['daily_stats']   # Event-time aggregates\n\n# Plot\nplot_event_study(results['daily_stats'], title=\"My Event Study\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "17_event_studies.html#demonstration-with-vietnamese-market-data",
    "href": "17_event_studies.html#demonstration-with-vietnamese-market-data",
    "title": "14  Event Studies in Finance",
    "section": "14.6 Demonstration with Vietnamese Market Data",
    "text": "14.6 Demonstration with Vietnamese Market Data\nWe now demonstrate the full event study pipeline using actual Vietnamese stock market data. The datasets available are:\n\nprices_daily: symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\nprices_monthly: same structure\nfactors_ff3_daily: date, smb, hml, mkt_excess, risk_free\nfactors_ff3_monthly — monthly frequency version\nfactors_ff5_daily: date, smb, hml, mkt_excess, risk_free, rmw, cma\nfactors_ff5_monthly\n\nSince our data provides ret_excess rather than raw returns, we recover raw returns as \\(R_{it} = R^e_{it} + R_{f,t}\\), and the market return as \\(R_{m,t} = R^e_{m,t} + R_{f,t}\\). The extract_event_returns() function handles this automatically.\n\n14.6.1 Loading the Data\n\n# --- Recover raw returns ---\n# ret = ret_excess + risk_free\nprices_daily['ret'] = prices_daily['ret_excess'] + prices_daily['risk_free']\nprices_monthly['ret'] = prices_monthly['ret_excess'] + prices_monthly['risk_free']\n\n# --- Inspect the data ---\nprint(\"=\" * 70)\nprint(\"VIETNAMESE MARKET DATA SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"\\nprices_daily: {prices_daily.shape[0]:,} rows, \"\n      f\"{prices_daily['symbol'].nunique()} stocks, \"\n      f\"{prices_daily['date'].min().date()} to {prices_daily['date'].max().date()}\")\nprint(f\"prices_monthly: {prices_monthly.shape[0]:,} rows, \"\n      f\"{prices_monthly['symbol'].nunique()} stocks\")\nprint(f\"\\nfactors_ff3_daily: {factors_ff3_daily.shape[0]:,} trading days\")\nprint(f\"  Columns: {list(factors_ff3_daily.columns)}\")\nprint(f\"factors_ff5_daily: {factors_ff5_daily.shape[0]:,} trading days\")\nprint(f\"  Columns: {list(factors_ff5_daily.columns)}\")\nprint(f\"\\nSample daily returns:\")\nprint(prices_daily[['symbol', 'date', 'ret_excess', 'ret', 'risk_free', 'mktcap']]\n      .head(5).to_string(index=False))\nprint(f\"\\nSample daily factors:\")\nprint(factors_ff3_daily.head(5).to_string(index=False))\n\n======================================================================\nVIETNAMESE MARKET DATA SUMMARY\n======================================================================\n\nprices_daily: 3,462,157 rows, 1459 stocks, 2010-01-05 to 2023-12-29\nprices_monthly: 165,499 rows, 1457 stocks\n\nfactors_ff3_daily: 3,126 trading days\n  Columns: ['date', 'smb', 'hml', 'mkt_excess', 'risk_free']\nfactors_ff5_daily: 3,126 trading days\n  Columns: ['date', 'smb', 'hml', 'rmw', 'cma', 'mkt_excess', 'risk_free']\n\nSample daily returns:\nsymbol       date  ret_excess      ret  risk_free     mktcap\n   A32 2018-10-24   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-25   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-26   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-29   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-30   -0.000159 0.000000   0.000159 176.120000\n\nSample daily factors:\n      date       smb       hml  mkt_excess  risk_free\n2011-07-01  0.008587  0.000967   -0.019862   0.000159\n2011-07-04  0.005099 -0.001099   -0.000633   0.000159\n2011-07-05 -0.009088  0.010152    0.013314   0.000159\n2011-07-06  0.004875 -0.003918   -0.008045   0.000159\n2011-07-07 -0.011239 -0.000584    0.003391   0.000159\n\n\n\n\n14.6.2 Creating Sample Events\nFor this demonstration, we create a sample event file. In practice, events would come from corporate announcements (earnings, M&A, dividends), regulatory changes, or other information shocks. Here we select 50 large-cap Vietnamese stocks and assign random event dates from the most recent two years of data to illustrate the pipeline mechanics.\n\nnp.random.seed(2024)\n\n# Select the 50 largest stocks by median market cap\nlargest = (prices_daily.groupby('symbol')['mktcap']\n           .median()\n           .nlargest(50)\n           .index.tolist())\n\n# Date range for events: last 2 years of data, with buffer for windows\ndate_range = prices_daily['date'].sort_values().unique()\nn_dates = len(date_range)\n# Events from the middle portion (need room for estimation + event windows)\nevent_eligible = date_range[int(n_dates * 0.3):int(n_dates * 0.85)]\n\n# Generate 50 random firm-event pairs\nevent_firms = np.random.choice(largest, 50, replace=True)\nevent_dates = np.random.choice(event_eligible, 50, replace=False)\n\nevents_demo = pd.DataFrame({\n    'symbol': event_firms,\n    'event_date': pd.to_datetime(event_dates),\n    'group': np.random.choice(['Group_A', 'Group_B'], 50)\n})\n\n# Remove any duplicate firm-date pairs\nevents_demo = events_demo.drop_duplicates(subset=['symbol', 'event_date'])\n\nprint(f\"Sample event file: {len(events_demo)} firm-event observations\")\nprint(f\"Unique firms: {events_demo['symbol'].nunique()}\")\nprint(f\"Date range: {events_demo['event_date'].min().date()} to \"\n      f\"{events_demo['event_date'].max().date()}\")\nprint(f\"\\nGroup distribution:\")\nprint(events_demo['group'].value_counts().to_string())\nprint(f\"\\nFirst 10 events:\")\nprint(events_demo.sort_values('event_date').head(10).to_string(index=False))\n\nSample event file: 50 firm-event observations\nUnique firms: 35\nDate range: 2014-06-25 to 2021-10-29\n\nGroup distribution:\ngroup\nGroup_B    26\nGroup_A    24\n\nFirst 10 events:\nsymbol event_date   group\n   MCH 2014-06-25 Group_B\n   SIP 2014-10-23 Group_B\n   VRE 2014-11-14 Group_B\n   QNS 2014-12-25 Group_A\n   FOX 2015-01-16 Group_B\n   THD 2015-01-26 Group_A\n   QNS 2015-02-12 Group_A\n   HNG 2015-05-07 Group_B\n   MML 2015-08-17 Group_B\n   ACV 2015-10-15 Group_A\n\n\n\n\n14.6.3 Daily Event Study: Fama-French 3-Factor Model\n\nconfig_ff3 = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\nresults_ff3 = run_event_study(\n    events=events_demo,\n    prices=prices_daily,\n    factors=factors_ff3_daily,\n    config=config_ff3,\n    group_col='group'\n)\n\n═══ Event Study: ff3 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  3308 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 5,421 obs for 30 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n\nStep 5: Computing abnormal returns...\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n    All 26  0.021513   0.024885      0.615385 0.774999 0.445609 0.726797 0.474101 0.699973 0.490407\nGroup_A 13  0.008601   0.006783      0.538462 0.211606 0.835966 0.577574 0.574229 0.567041 0.581138\nGroup_B 13  0.034425   0.042987      0.692308 0.879892 0.396197 0.437902 0.669236 0.429917 0.674875\n\n\n\n\n14.6.4 Visualizing Daily Results\n\nfig1 = plot_event_study(\n    results_ff3['daily_stats'],\n    title=\"Event Study: Fama-French 3-Factor Model — Vietnamese Market (Daily)\"\n)\nplt.show()\n\n\n\n\n\n\n\nFigure 14.3: Cumulative abnormal returns around event dates for Vietnamese stocks using the Fama-French 3-factor model. The event window spans [-10, +10] trading days.\n\n\n\n\n\n\nfig2 = plot_car_distribution(results_ff3['event_ar'], 'CAR')\nplt.show()\n\n\n\n\n\n\n\nFigure 14.4: Cross-sectional distribution of cumulative abnormal returns (CARs) across firm-events. The histogram and Q-Q plot assess normality assumptions underlying parametric tests.\n\n\n\n\n\n\n\n14.6.5 Complete Test Statistics (Daily)\n\n\n\nTable 14.4: Event study test statistics for the full sample and by subgroup — Daily frequency, FF3 model\n\n\nts = results_ff3['test_stats'].copy()\n\ndisplay_cols = ['group', 'N', 'mean_CAR', 'median_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 'Z_Patell', 'p_Patell',\n                't_BMP', 'p_BMP', 't_KP', 'p_KP',\n                'Z_GSign', 'p_GSign', 't_SkAdj', 'p_SkAdj',\n                'W_Wilcoxon', 'p_Wilcoxon']\navail = [c for c in display_cols if c in ts.columns]\ndisplay_df = ts[avail].copy()\n\nfor c in display_df.columns:\n    if c in ['N']:\n        display_df[c] = display_df[c].astype(int)\n    elif c == 'group':\n        continue\n    elif c.startswith('p_'):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4f}' if pd.notna(x) else '')\n    elif c in ['mean_CAR', 'median_CAR', 'mean_BHAR']:\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4%}' if pd.notna(x) else '')\n    elif c == 'pct_positive':\n        display_df[c] = display_df[c].map(lambda x: f'{x:.1%}' if pd.notna(x) else '')\n    elif isinstance(display_df[c].iloc[0], (int, float, np.floating)):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(display_df.to_string(index=False))\n\n  group  N mean_CAR median_CAR mean_BHAR pct_positive  t_CS   p_CS Z_Patell p_Patell t_BMP  p_BMP  t_KP   p_KP Z_GSign p_GSign t_SkAdj p_SkAdj W_Wilcoxon p_Wilcoxon\n    All 26  2.1513%    1.5701%   2.4885%        61.5% 0.775 0.4456    0.961   0.3364 0.727 0.4741 0.700 0.4904   1.177  0.2393   0.738  0.4676    158.000     0.6710\nGroup_A 13  0.8601%    2.5330%   0.6783%        53.8% 0.212 0.8360    0.739   0.4596 0.578 0.5742 0.567 0.5811   0.277  0.7815   0.597  0.5618     45.000     1.0000\nGroup_B 13  3.4425%    1.4169%   4.2987%        69.2% 0.880 0.3962    0.620   0.5352 0.438 0.6692 0.430 0.6749   1.387  0.1655   0.444  0.6647     35.000     0.4973\n\n\n\n\n\n\n14.6.6 Robustness: Multiple Risk Models (Daily)\n\n\n\nTable 14.5: Robustness of event study results across risk models — Daily frequency\n\n\nmodels_daily = [\n    (\"Market-Adjusted\",  RiskModel.MARKET_ADJ,    factors_ff3_daily),\n    (\"Market Model\",     RiskModel.MARKET_MODEL,   factors_ff3_daily),\n    (\"Fama-French 3\",    RiskModel.FF3,            factors_ff3_daily),\n    (\"Fama-French 5\",    RiskModel.FF5,            factors_ff5_daily),\n]\n\nrobustness_daily = []\nfor name, mdl, facs in models_daily:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=-10, event_window_end=10,\n        gap=15, min_estimation_obs=120, risk_model=mdl\n    )\n    res = run_event_study(events_demo, prices_daily, facs, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    robustness_daily.append({\n        'Model': name,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Median CAR': f\"{full['median_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.3f}\",\n        'p (CS)': f\"{full['p_CS']:.4f}\",\n        't (BMP)': f\"{full['t_BMP']:.3f}\",\n        'p (BMP)': f\"{full['p_BMP']:.4f}\",\n        't (KP)': f\"{full.get('t_KP', np.nan):.3f}\",\n        'p (KP)': f\"{full.get('p_KP', np.nan):.4f}\",\n    })\n\nrob_daily_df = pd.DataFrame(robustness_daily)\nprint(\"Robustness Across Risk Models (Daily Frequency)\")\nprint(\"=\" * 100)\nprint(rob_daily_df.to_string(index=False))\n\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 28/30 firm-events (mean R^2 = 0.0000)\n  28 firm-events | Mean CAR: 0.035338 | Mean BHAR: 0.036936 | % positive: 50.0%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.1960)\n  26 firm-events | Mean CAR: 0.032107 | Mean BHAR: 0.033221 | % positive: 61.5%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2675)\n  26 firm-events | Mean CAR: 0.025684 | Mean BHAR: 0.028399 | % positive: 57.7%\nRobustness Across Risk Models (Daily Frequency)\n====================================================================================================\n          Model  N Mean CAR Median CAR Mean BHAR % Positive t (CS) p (CS) t (BMP) p (BMP) t (KP) p (KP)\nMarket-Adjusted 28  3.5338%    0.2610%   3.6936%      50.0%  1.390 0.1758   1.103  0.2798  1.062 0.2975\n   Market Model 26  3.2107%    0.6925%   3.3221%      61.5%  1.198 0.2422   1.146  0.2625  1.107 0.2789\n  Fama-French 3 26  2.1513%    1.5701%   2.4885%      61.5%  0.775 0.4456   0.727  0.4741  0.700 0.4904\n  Fama-French 5 26  2.5684%    2.3619%   2.8399%      57.7%  0.968 0.3422   0.987  0.3332  0.962 0.3451\n\n\n\n\n\n\n14.6.7 Robustness: Multiple Event Windows\nA key practice is to examine sensitivity to the event window specification:\n\n\n\nTable 14.6: Sensitivity of results to event window specification\n\n\nwindows = [\n    (\"(-1, +1)\",  -1, 1),\n    (\"(-3, +3)\",  -3, 3),\n    (\"(-5, +5)\",  -5, 5),\n    (\"(-10, +10)\", -10, 10),\n    (\"(-1, +5)\",  -1, 5),\n    (\"(-5, +1)\",  -5, 1),\n    (\"(0, 0)\",     0, 0),\n]\n\nwindow_results = []\nfor label, ws, we in windows:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=ws, event_window_end=we,\n        gap=15, min_estimation_obs=120, risk_model=RiskModel.FF3\n    )\n    res = run_event_study(events_demo, prices_daily, factors_ff3_daily, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    window_results.append({\n        'Window': label,\n        'Days': we - ws + 1,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.3f}\",\n        't (BMP)': f\"{full['t_BMP']:.3f}\",\n        'p (BMP)': f\"{full['p_BMP']:.4f}\",\n    })\n\nwin_df = pd.DataFrame(window_results)\nprint(\"Sensitivity to Event Window Specification (FF3 Model)\")\nprint(\"=\" * 90)\nprint(win_df.to_string(index=False))\n\n  Extracted 4,899 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2147)\n  26 firm-events | Mean CAR: 0.004074 | Mean BHAR: 0.004648 | % positive: 50.0%\n  Extracted 5,015 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2155)\n  26 firm-events | Mean CAR: 0.003761 | Mean BHAR: 0.004327 | % positive: 42.3%\n  Extracted 5,131 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2217)\n  26 firm-events | Mean CAR: -0.001133 | Mean BHAR: 0.001027 | % positive: 42.3%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n  Extracted 5,019 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2147)\n  26 firm-events | Mean CAR: -0.005096 | Mean BHAR: -0.005148 | % positive: 42.3%\n  Extracted 5,011 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2217)\n  26 firm-events | Mean CAR: 0.008600 | Mean BHAR: 0.010441 | % positive: 46.2%\n  Extracted 4,841 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2150)\n  26 firm-events | Mean CAR: 0.000344 | Mean BHAR: 0.000502 | % positive: 46.2%\nSensitivity to Event Window Specification (FF3 Model)\n==========================================================================================\n    Window  Days  N Mean CAR Mean BHAR % Positive t (CS) t (BMP) p (BMP)\n  (-1, +1)     3 26  0.4074%   0.4648%      50.0%  0.456   0.601  0.5535\n  (-3, +3)     7 26  0.3761%   0.4327%      42.3%  0.198   0.361  0.7211\n  (-5, +5)    11 26 -0.1133%   0.1027%      42.3% -0.049   0.030  0.9761\n(-10, +10)    21 26  2.1513%   2.4885%      61.5%  0.775   0.727  0.4741\n  (-1, +5)     7 26 -0.5096%  -0.5148%      42.3% -0.385  -0.068  0.9460\n  (-5, +1)     7 26  0.8600%   1.0441%      46.2%  0.439   0.429  0.6715\n    (0, 0)     1 26  0.0344%   0.0502%      46.2%  0.064  -0.020  0.9840\n\n\n\n\n\n\n14.6.8 Monthly Event Study: Fama-French 3-Factor Model\nFor longer-horizon studies, monthly frequency is appropriate. Note that the estimation window is specified in months rather than days:\n\n# Create monthly events aligned to the monthly data\n# Map daily event dates to the corresponding month-end\nevents_monthly = events_demo.copy()\nevents_monthly['event_date'] = events_monthly['event_date'].dt.to_period('M').dt.to_timestamp('M')\n\n# Use month-end dates from monthly prices\nmonthly_dates = prices_monthly['date'].sort_values().unique()\n\n# Filter events to dates present in monthly data\nevents_monthly = events_monthly[events_monthly['event_date'].isin(monthly_dates)]\nevents_monthly = events_monthly.drop_duplicates(subset=['symbol', 'event_date'])\n\nconfig_monthly = EventStudyConfig(\n    estimation_window=36,     # 36 months\n    event_window_start=-3,    # 3 months before\n    event_window_end=3,       # 3 months after\n    gap=3,                    # 3-month gap\n    min_estimation_obs=24,    # At least 24 months\n    risk_model=RiskModel.FF3\n)\n\nif len(events_monthly) &gt; 0:\n    results_monthly = run_event_study(\n        events=events_monthly,\n        prices=prices_monthly,\n        factors=factors_ff3_monthly,\n        config=config_monthly,\n        group_col='group'\n    )\n    \n    print(\"\\n--- Monthly Test Statistics ---\")\n    ts_m = results_monthly['test_stats']\n    mcols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n             't_CS', 'p_CS', 't_BMP', 'p_BMP']\n    mavail = [c for c in mcols if c in ts_m.columns]\n    print(ts_m[mavail].to_string(index=False))\nelse:\n    print(\"No monthly events could be aligned. Skipping monthly study.\")\n\n═══ Event Study: ff3 model ═══\n  Windows: estimation=36, gap=3, event=(-3,3)\n  Min obs: 24\n\nStep 1: Building trading calendar...\n  122 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 1,036 obs for 33 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 18/33 firm-events (mean R^2 = 0.3218)\n\nStep 5: Computing abnormal returns...\n  18 firm-events | Mean CAR: -0.005257 | Mean BHAR: -0.014576 | % positive: 55.6%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive      t_CS     p_CS     t_BMP    p_BMP      t_KP     p_KP\n    All 18 -0.005257  -0.014576      0.555556 -0.081058 0.936342 -0.249547 0.805928 -0.320212 0.752709\nGroup_A  7 -0.141756  -0.135084      0.428571 -1.102110 0.312648 -1.357452 0.223472 -1.462577 0.193905\nGroup_B 11  0.081606   0.062111      0.636364  1.390317 0.194599  1.177372 0.266309  1.342593 0.209085\n\n--- Monthly Test Statistics ---\n  group  N  mean_CAR  mean_BHAR  pct_positive      t_CS     p_CS     t_BMP    p_BMP\n    All 18 -0.005257  -0.014576      0.555556 -0.081058 0.936342 -0.249547 0.805928\nGroup_A  7 -0.141756  -0.135084      0.428571 -1.102110 0.312648 -1.357452 0.223472\nGroup_B 11  0.081606   0.062111      0.636364  1.390317 0.194599  1.177372 0.266309\n\n\n\nif len(events_monthly) &gt; 0 and 'daily_stats' in results_monthly:\n    fig3 = plot_event_study(\n        results_monthly['daily_stats'],\n        title=\"Event Study: FF3 Model — Vietnamese Market (Monthly)\"\n    )\n    plt.show()\n\n\n\n\n\n\n\nFigure 14.5: Monthly cumulative abnormal returns around event dates. Wider windows capture slower information incorporation typical of emerging markets.\n\n\n\n\n\n\n\n14.6.9 Daily Event Study: Fama-French 5-Factor Model\n\nconfig_ff5 = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF5\n)\n\nresults_ff5 = run_event_study(\n    events=events_demo,\n    prices=prices_daily,\n    factors=factors_ff5_daily,\n    config=config_ff5,\n    group_col='group'\n)\n\n═══ Event Study: ff5 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  3308 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 5,421 obs for 30 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 26/30 firm-events (mean R^2 = 0.2675)\n\nStep 5: Computing abnormal returns...\n  26 firm-events | Mean CAR: 0.025684 | Mean BHAR: 0.028399 | % positive: 57.7%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n    All 26  0.025684   0.028399      0.576923 0.968332 0.342154 0.986788 0.333201 0.962252 0.345139\nGroup_A 13  0.015352   0.013489      0.461538 0.393655 0.700741 0.786232 0.446980 0.776663 0.452395\nGroup_B 13  0.036016   0.043309      0.692308 0.965100 0.353542 0.585915 0.568789 0.578785 0.573438\n\n\n\n\n14.6.10 Comparing FF3 vs FF5 Estimation Quality\n\n\n\nTable 14.7: Comparison of estimation quality between FF3 and FF5 models\n\n\nparams_ff3 = results_ff3['params']\nparams_ff5 = results_ff5['params']\n\nprint(\"Model Estimation Diagnostics\")\nprint(\"=\" * 60)\nprint(f\"\\n{'Metric':&lt;30} {'FF3':&gt;12} {'FF5':&gt;12}\")\nprint(\"-\" * 54)\nprint(f\"{'Firm-events estimated':&lt;30} {len(params_ff3):&gt;12} {len(params_ff5):&gt;12}\")\nprint(f\"{'Mean R^2':&lt;30} {params_ff3['r_squared'].mean():&gt;12.4f} {params_ff5['r_squared'].mean():&gt;12.4f}\")\nprint(f\"{'Median R^2':&lt;30} {params_ff3['r_squared'].median():&gt;12.4f} {params_ff5['r_squared'].median():&gt;12.4f}\")\nprint(f\"{'Mean σ(ε)':&lt;30} {params_ff3['sigma'].mean():&gt;12.6f} {params_ff5['sigma'].mean():&gt;12.6f}\")\nprint(f\"{'Mean |α|':&lt;30} {params_ff3['alpha'].abs().mean():&gt;12.6f} {params_ff5['alpha'].abs().mean():&gt;12.6f}\")\nprint(f\"{'Mean β(MKT)':&lt;30} {params_ff3['beta_mkt_excess'].mean():&gt;12.4f} {params_ff5['beta_mkt_excess'].mean():&gt;12.4f}\")\nif 'beta_smb' in params_ff3.columns:\n    print(f\"{'Mean β(SMB)':&lt;30} {params_ff3['beta_smb'].mean():&gt;12.4f} {params_ff5['beta_smb'].mean():&gt;12.4f}\")\nif 'beta_hml' in params_ff3.columns:\n    print(f\"{'Mean β(HML)':&lt;30} {params_ff3['beta_hml'].mean():&gt;12.4f} {params_ff5['beta_hml'].mean():&gt;12.4f}\")\nif 'beta_rmw' in params_ff5.columns:\n    print(f\"{'Mean β(RMW)':&lt;30} {'—':&gt;12} {params_ff5['beta_rmw'].mean():&gt;12.4f}\")\nif 'beta_cma' in params_ff5.columns:\n    print(f\"{'Mean β(CMA)':&lt;30} {'—':&gt;12} {params_ff5['beta_cma'].mean():&gt;12.4f}\")\n\nModel Estimation Diagnostics\n============================================================\n\nMetric                                  FF3          FF5\n------------------------------------------------------\nFirm-events estimated                    26           26\nMean R^2                             0.2245       0.2675\nMedian R^2                           0.1943       0.2692\nMean σ(ε)                          0.021753     0.021351\nMean |α|                           0.001022     0.001130\nMean β(MKT)                          0.8867       0.9721\nMean β(SMB)                         -0.0434       0.0265\nMean β(HML)                          0.2489       0.1493\nMean β(RMW)                               —      -0.0934\nMean β(CMA)                               —       0.1070\n\n\n\n\n\n\n14.6.11 Event-Level Detail\n\n\n\nTable 14.8: Event-level detail: CARs and BHARs for each firm-event (FF3 model)\n\n\ndetail = results_ff3['event_ar'].copy()\ndetail_cols = ['symbol', 'evtdate', 'CAR', 'BHAR', 'SCAR', 'sigma',\n               'nobs', 'alpha', 'beta_mkt_excess']\ndetail_avail = [c for c in detail_cols if c in detail.columns]\ndetail_show = detail[detail_avail].copy()\ndetail_show['CAR'] = detail_show['CAR'].map(lambda x: f'{x:.4%}')\ndetail_show['BHAR'] = detail_show['BHAR'].map(lambda x: f'{x:.4%}')\ndetail_show['SCAR'] = detail_show['SCAR'].map(lambda x: f'{x:.3f}')\n\nprint(\"Event-Level Results (first 20 firm-events)\")\nprint(\"=\" * 100)\nprint(detail_show.head(20).to_string(index=False))\n\nEvent-Level Results (first 20 firm-events)\n====================================================================================================\nsymbol    evtdate       CAR      BHAR   SCAR    sigma  nobs     alpha  beta_mkt_excess\n   BVH 2016-10-20 -12.6456% -11.6522% -1.603 0.017219   150  0.001193         1.449182\n   DHG 2016-01-25  16.1151%  17.1149%  2.273 0.015472   150 -0.000224         0.754245\n   DNH 2019-10-14   1.7233%   1.8068%  0.104 0.036035   150 -0.000781         1.861996\n   DPM 2020-07-30  -5.9576%  -6.2025% -0.545 0.023873   150  0.001279         0.313932\n   FOX 2021-01-13   2.7478%   2.9194%  0.329 0.018243   150 -0.000696         0.148058\n   GAS 2020-02-11   0.5371%   0.7801%  0.100 0.011694   150  0.000501         1.851155\n   GEX 2020-08-20  11.9985%  13.5843%  1.197 0.021883   150  0.000944         1.583418\n   IDC 2018-10-01   1.3889%   0.5651%  0.094 0.032120   150 -0.000674         0.809305\n   MML 2021-10-29 -12.0139% -12.3759% -1.141 0.022985   150  0.002976         0.260588\n   MSN 2015-10-23  -5.7205%  -5.5645% -0.718 0.017375   150  0.001177         0.453951\n   PGV 2019-06-26  -7.5892%  -9.1366% -0.359 0.046165   150  0.000502         0.151377\n   PLX 2020-01-07   2.5330%   2.7847%  0.423 0.013067   150 -0.000176         0.917578\n   PLX 2020-06-01  -6.1517%  -6.0085% -0.739 0.018168   150 -0.000465         1.815178\n   POW 2020-12-23   7.7751%   9.1225%  1.130 0.015014   150 -0.000575         0.774423\n   PVD 2020-05-25   4.8827%   5.6674%  0.510 0.020875   150 -0.001522         1.316102\n   PVS 2017-08-07   2.1360%   2.1918%  0.297 0.015715   150 -0.000341         1.136273\n   QNS 2018-01-18   4.3001%   3.4011%  0.530 0.017703   150 -0.003716         0.128328\n   SAB 2017-08-24   6.0347%   6.7732%  0.748 0.017614   150  0.003284         2.123751\n   SNZ 2019-03-12  34.5230%  33.1105%  2.145 0.035121   150  0.000235        -1.015554\n   VCI 2020-01-20  -3.3191%  -2.9072% -0.458 0.015797   150  0.000335        -0.086268\n\n\n\n\n\n\n14.6.12 Daily Abnormal Return Dynamics\n\n\n\nTable 14.9: Daily dynamics of mean abnormal returns and test statistics within the event window\n\n\nds = results_ff3['daily_stats'].copy()\nds_cols = ['evttime', 'N', 'mean_AR', 'mean_CAR', 'mean_BHAR', 't_AR_CS', 't_AR_BMP']\nds_avail = [c for c in ds_cols if c in ds.columns]\nds_show = ds[ds_avail].copy()\n\nfor c in ['mean_AR', 'mean_CAR', 'mean_BHAR']:\n    if c in ds_show.columns:\n        ds_show[c] = ds_show[c].map(lambda x: f'{x:.4%}')\nfor c in ['t_AR_CS', 't_AR_BMP']:\n    if c in ds_show.columns:\n        ds_show[c] = ds_show[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(\"Daily Event-Window Dynamics (FF3 Model)\")\nprint(\"=\" * 80)\nprint(ds_show.to_string(index=False))\n\nDaily Event-Window Dynamics (FF3 Model)\n================================================================================\n evttime  N  mean_AR mean_CAR mean_BHAR\n     -10 23  0.3571%  0.3571%   0.3729%\n      -9 23  0.0337%  0.3907%   0.4209%\n      -8 23 -0.1581%  0.2326%   0.2766%\n      -7 23  0.3691%  0.6018%   0.6581%\n      -6 23  1.3416%  1.9433%   2.0278%\n      -5 23  0.1509%  2.0942%   2.2313%\n      -4 23  0.5512%  2.6454%   2.9187%\n      -3 23 -0.4641%  2.1814%   2.4297%\n      -2 23  0.2412%  2.4225%   2.8028%\n      -1 23  0.5660%  2.9885%   3.3240%\n       0 23 -0.0281%  2.9604%   3.4926%\n       1 23 -0.2564%  2.7040%   3.1039%\n       2 23 -0.4421%  2.2619%   2.5764%\n       3 23  0.6337%  2.8956%   3.4659%\n       4 23 -0.8432%  2.0524%   2.4738%\n       5 23 -0.4174%  1.6350%   2.1339%\n       6 23 -0.2272%  1.4078%   1.8085%\n       7 23 -0.2085%  1.1993%   1.6819%\n       8 23  0.0893%  1.2886%   1.8609%\n       9 23  0.3307%  1.6193%   2.1658%\n      10 23  0.5320%  2.1513%   2.4885%\n\n\n\n\n\n\n14.6.13 Summary of Key Findings\n\nprint(\"=\" * 70)\nprint(\"EVENT STUDY RESULTS SUMMARY\")\nprint(\"=\" * 70)\n\nff3_all = results_ff3['test_stats'][results_ff3['test_stats']['group'] == 'All'].iloc[0]\n\nprint(f\"\\nSample: {int(ff3_all['N'])} firm-event observations\")\nprint(f\"Frequency: Daily\")\nprint(f\"Primary Model: Fama-French 3-Factor\")\nprint(f\"Estimation Window: {config_ff3.estimation_window} trading days\")\nprint(f\"Event Window: ({config_ff3.event_window_start}, {config_ff3.event_window_end})\")\nprint(f\"Gap: {config_ff3.gap} trading days\")\nprint(f\"\\n--- Abnormal Return Measures ---\")\nprint(f\"Mean CAR({config_ff3.event_window_start},{config_ff3.event_window_end}): \"\n      f\"{ff3_all['mean_CAR']:.4%}\")\nprint(f\"Median CAR: {ff3_all['median_CAR']:.4%}\")\nprint(f\"Mean BHAR: {ff3_all['mean_BHAR']:.4%}\")\nprint(f\"Fraction positive CARs: {ff3_all['pct_positive']:.1%}\")\nprint(f\"\\n--- Statistical Significance ---\")\nprint(f\"Cross-Sectional t: {ff3_all['t_CS']:.3f} (p = {ff3_all['p_CS']:.4f})\")\nprint(f\"Patell Z: {ff3_all['Z_Patell']:.3f} (p = {ff3_all['p_Patell']:.4f})\")\nprint(f\"BMP t: {ff3_all['t_BMP']:.3f} (p = {ff3_all['p_BMP']:.4f})\")\nprint(f\"Kolari-Pynnönen t: {ff3_all['t_KP']:.3f} (p = {ff3_all['p_KP']:.4f})\")\nprint(f\"Generalized Sign Z: {ff3_all['Z_GSign']:.3f} (p = {ff3_all['p_GSign']:.4f})\")\n\nsig_005 = sum(1 for k in ['p_CS','p_Patell','p_BMP','p_KP','p_GSign','p_SkAdj','p_Wilcoxon']\n              if k in ff3_all and pd.notna(ff3_all[k]) and ff3_all[k] &lt; 0.05)\ntotal_tests = sum(1 for k in ['p_CS','p_Patell','p_BMP','p_KP','p_GSign','p_SkAdj','p_Wilcoxon']\n                  if k in ff3_all and pd.notna(ff3_all[k]))\nprint(f\"\\n{sig_005}/{total_tests} tests significant at 5% level\")\n\n# Robustness note\nprint(f\"\\nRobustness: Results checked across {len(models_daily)} risk models \"\n      f\"and {len(windows)} event windows\")\n\n======================================================================\nEVENT STUDY RESULTS SUMMARY\n======================================================================\n\nSample: 26 firm-event observations\nFrequency: Daily\nPrimary Model: Fama-French 3-Factor\nEstimation Window: 150 trading days\nEvent Window: (-10, 10)\nGap: 15 trading days\n\n--- Abnormal Return Measures ---\nMean CAR(-10,10): 2.1513%\nMedian CAR: 1.5701%\nMean BHAR: 2.4885%\nFraction positive CARs: 61.5%\n\n--- Statistical Significance ---\nCross-Sectional t: 0.775 (p = 0.4456)\nPatell Z: 0.961 (p = 0.3364)\nBMP t: 0.727 (p = 0.4741)\nKolari-Pynnönen t: 0.700 (p = 0.4904)\nGeneralized Sign Z: 1.177 (p = 0.2393)\n\n0/7 tests significant at 5% level\n\nRobustness: Results checked across 4 risk models and 7 event windows",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "17_event_studies.html#practical-recommendations",
    "href": "17_event_studies.html#practical-recommendations",
    "title": "14  Event Studies in Finance",
    "section": "14.7 Practical Recommendations",
    "text": "14.7 Practical Recommendations\nBased on the literature and our implementation experience:\n\nEstimation window: Use 150 trading days (~7 months) for daily studies. This balances parameter precision against structural breaks. For monthly studies, 60 months is standard (Kothari and Warner 2007).\nGap: 15 trading days is standard. Increase to 30 if information leakage is a concern.\nEvent window: Start with (-1, +1) for short-window tests, then expand to (-5, +5) and (-10, +10) for robustness. Report all windows.\nModel choice: Always report market model as the baseline. Add FF3 or FF5 for robustness. For Vietnam, local factors are preferable to global factors.\nTest statistics: Report at minimum: cross-sectional t (for ease of interpretation), BMP (robust to event-induced variance), and one non-parametric test (sign or Wilcoxon). Report Kolari-Pynnönen if events cluster in calendar time.\nThin trading: For Vietnamese small-caps, consider Dimson (1979) with 1 lead/lag or increase min_estimation_obs to filter out illiquid stocks.\nMultiple testing: If testing multiple event windows or subgroups, apply Bonferroni or Holm corrections to control family-wise error rate.\n\n\n\n\n\n\n\nAharony, Joseph, and Itzhak Swary. 1980. “Quarterly Dividend and Earnings Announcements and Stockholders’ Returns: An Empirical Analysis.” The Journal of Finance 35 (1): 1–12.\n\n\nAndrade, Gregor, Mark Mitchell, and Erik Stafford. 2001. “New Evidence and Perspectives on Mergers.” Journal of Economic Perspectives 15 (2): 103–20.\n\n\nBall, Ray, and Philip Brown. 2013. “An Empirical Evaluation of Accounting Income Numbers.” In Financial Accounting and Equity Markets, 27–46. Routledge.\n\n\nBarber, Brad M, and John D Lyon. 1997. “Detecting Long-Run Abnormal Stock Returns: The Empirical Power and Specification of Test Statistics.” Journal of Financial Economics 43 (3): 341–72.\n\n\nBernard, Victor L, and Jacob K Thomas. 1989. “Post-Earnings-Announcement Drift: Delayed Price Response or Risk Premium?” Journal of Accounting Research 27: 1–36.\n\n\nBhattacharya, Utpal, Hazem Daouk, Brian Jorgenson, and Carl-Heinrich Kehr. 2000. “When an Event Is Not an Event: The Curious Case of an Emerging Market.” Journal of Financial Economics 55 (1): 69–101.\n\n\nBinder, John. 1998. “The Event Study Methodology Since 1969.” Review of Quantitative Finance and Accounting 11 (2): 111–37.\n\n\nBoehmer, Ekkehart, Jim Masumeci, and Annette B Poulsen. 1991. “Event-Study Methodology Under Conditions of Event-Induced Variance.” Journal of Financial Economics 30 (2): 253–72.\n\n\nBrown, Stephen J, and Jerold B Warner. 1980. “Measuring Security Price Performance.” Journal of Financial Economics 8 (3): 205–58.\n\n\n———. 1985. “Using Daily Stock Returns: The Case of Event Studies.” Journal of Financial Economics 14 (1): 3–31.\n\n\nCampbell, John Y, Andrew W Lo, A Craig MacKinlay, and Robert F Whitelaw. 1998. “The Econometrics of Financial Markets.” Macroeconomic Dynamics 2 (4): 559–62.\n\n\nCarhart, Mark M. 1997. “On persistence in mutual fund performance.” The Journal of Finance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nCorrado, Charles J. 1989. “A Nonparametric Test for Abnormal Security-Price Performance in Event Studies.” Journal of Financial Economics 23 (2): 385–95.\n\n\nCowan, Arnold Richard. 1992. “Nonparametric Event Study Tests.” Review of Quantitative Finance and Accounting 2 (4): 343–58.\n\n\nDimson, Elroy. 1979. “Risk Measurement When Shares Are Subject to Infrequent Trading.” Journal of Financial Economics 7 (2): 197–226.\n\n\nFama, Eugene F. 1998. “Market Efficiency, Long-Term Returns, and Behavioral Finance.” Journal of Financial Economics 49 (3): 283–306.\n\n\nFama, Eugene F, Lawrence Fisher, Michael C Jensen, and Richard Roll. 1969. “The Adjustment of Stock Prices to New Information.” International Economic Review 10 (1): 1–21.\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nFlannery, Mark J, and Aris A Protopapadakis. 2002. “Macroeconomic Factors Do Influence Aggregate Stock Returns.” The Review of Financial Studies 15 (3): 751–82.\n\n\nGriffin, John M, Patrick J Kelly, and Federico Nardari. 2010. “Do Market Efficiency Measures Yield Correct Inferences? A Comparison of Developed and Emerging Markets.” The Review of Financial Studies 23 (8): 3225–77.\n\n\nHall, Peter. 1992. “On the Removal of Skewness by Transformation.” Journal of the Royal Statistical Society Series B: Statistical Methodology 54 (1): 221–28.\n\n\nJensen, Michael C, and Richard S Ruback. 1983. “The Market for Corporate Control: The Scientific Evidence.” Journal of Financial Economics 11 (1-4): 5–50.\n\n\nKolari, James W, and Seppo Pynnönen. 2010. “Event Study Testing with Cross-Sectional Correlation of Abnormal Returns.” The Review of Financial Studies 23 (11): 3996–4025.\n\n\nKothari, Sagar P, and Jerold B Warner. 2007. “Econometrics of Event Studies.” In Handbook of Empirical Corporate Finance, 3–36. Elsevier.\n\n\nMacKinlay, A Craig. 1997. “Event Studies in Economics and Finance.” Journal of Economic Literature 35 (1): 13–39.\n\n\nMitchell, Mark L, and Jeffry M Netter. 1993. “The Role of Financial Economics in Securities Fraud Cases: Applications at the Securities and Exchange Commission.” Bus. Law. 49: 545.\n\n\nMitchell, Mark L, and Erik Stafford. 2000. “Managerial Decisions and Long-Term Stock Price Performance.” The Journal of Business 73 (3): 287–329.\n\n\nPatell, James M. 1976. “Corporate Forecasts of Earnings Per Share and Stock Price Behavior: Empirical Test.” Journal of Accounting Research, 246–76.\n\n\nScholes, Myron, and Joseph Williams. 1977. “Estimating Betas from Nonsynchronous Data.” Journal of Financial Economics 5 (3): 309–27.\n\n\nSchwert, G William. 1981. “Using Financial Data to Measure Effects of Regulation.” The Journal of Law and Economics 24 (1): 121–58.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.\n\n\nWarner, Jerold B, Ross L Watts, and Karen H Wruck. 1988. “Stock Prices and Top Management Changes.” Journal of Financial Economics 20: 461–92.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "18_sue.html",
    "href": "18_sue.html",
    "title": "15  Standardized Earnings Surprises (SUE)",
    "section": "",
    "text": "15.1 Methodology\nIn the context of the Ho Chi Minh Stock Exchange (HOSE) and the Hanoi Stock Exchange (HNX), earnings announcements represent critical information events. Investors and quantitative analysts continuously monitor the deviation between reported earnings and market expectations. This deviation is quantified as the Standardized Earnings Surprise (SUE).\nThis chapter details the methodology for calculating SUE using three distinct approaches frequently utilized in academic literature and institutional research. We apply these methods to a dataset of Vietnamese large-cap equities to illustrate the mechanics of the calculation. The goal is to isolate the “surprise” component of earnings, which is a known predictor of post-earnings announcement drift (PEAD) (Bernard and Thomas 1989; Livnat and Mendenhall 2006).\nWe define three primary methods for calculating SUE. Each method differs in how it establishes the “expected” earnings value.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "18_sue.html#methodology",
    "href": "18_sue.html#methodology",
    "title": "15  Standardized Earnings Surprises (SUE)",
    "section": "",
    "text": "15.1.1 Method 1: Seasonal Random Walk\nThis method assumes that earnings follow a seasonal pattern. The best predictor for the current quarter’s earnings per share (EPS) is the EPS from the same quarter in the previous year. This controls for the seasonality often seen in Vietnamese sectors like retail and agriculture.\n\\[SUE_{1} = \\frac{EPS_{t} - EPS_{t-4}}{P_{t}}\\]\nWhere:\n\n\\(EPS_{t}\\) is the current quarterly Earnings Per Share.\n\\(EPS_{t-4}\\) is the Earnings Per Share from the same quarter of the prior fiscal year.\n\\(P_{t}\\) is the stock price at the end of the quarter (used as a deflator).\n\n\n\n15.1.2 Method 2: Exclusion of Special Items\nReported earnings often contain non-recurring items (e.g., asset sales, one-time write-offs) that distort the true operating performance. This method adjusts the reported EPS by removing the after-tax impact of special items.\nIn Vietnam, the standard Corporate Income Tax (CIT) rate is generally 20%. We adjust special items to reflect their impact on net income.\n\\[Adjusted \\ EPS = Reported \\ EPS - \\frac{Special \\ Items \\times (1 - CIT)}{Shares \\ Outstanding}\\]\nThe SUE calculation then follows the seasonal logic but uses the adjusted EPS figures:\n\\[SUE_{2} = \\frac{Adj \\ EPS_{t} - Adj \\ EPS_{t-4}}{P_{t}}\\]\n\n\n15.1.3 Method 3: Analyst Consensus\nThis method relies on market consensus rather than historical time series. It compares the actual reported earnings against the median analyst forecast provided prior to the announcement.\n\\[SUE_{3} = \\frac{Actual \\ EPS - Median \\ Estimate}{P_{t}}\\]",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "18_sue.html#data-description",
    "href": "18_sue.html#data-description",
    "title": "15  Standardized Earnings Surprises (SUE)",
    "section": "15.2 Data Description",
    "text": "15.2 Data Description\nFor this analysis, we utilize a dataset covering the fiscal years 2023 through 2025. The data includes quarterly financial statements and analyst consensus estimates for a selection of VN30 index constituents.\nThe dataset, vietnam_fin_data.csv, contains the following columns:\n\nticker: Stock symbol (e.g., VNM, VCB, HPG).\nfiscal_year: The financial year.\nfiscal_qtr: The financial quarter (1-4).\neps_basic: Basic Earnings Per Share (VND).\nprice_close: Closing price at quarter end (VND).\nspecial_items: Pre-tax special items value (VND millions). (i.e., is_other_profit in DataCore).\nshares_out: Shares outstanding (millions).\nanalyst_med: Median analyst EPS estimate (VND).\n\n\n15.2.1 Visualizing the Core Data\nBelow is a tabular representation of the raw data we have ingested for the analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nticker\nfiscal_year\nfiscal_qtr\neps_basic\nprice_close\nspecial_items\nshares_out\nanalyst_med\n\n\n\n\nVNM\n2023\n1\n1200\n68000\n0\n2090\n1150\n\n\nVNM\n2023\n2\n1350\n71000\n50000\n2090\n1300\n\n\nVNM\n2023\n3\n1400\n74000\n0\n2090\n1450\n\n\nVNM\n2023\n4\n1100\n69000\n-20000\n2090\n1150\n\n\nVNM\n2024\n1\n1300\n72000\n0\n2090\n1250\n\n\nVNM\n2024\n2\n1500\n75000\n0\n2090\n1400\n\n\nVCB\n2023\n1\n1800\n85000\n10000\n5500\n1700\n\n\nVCB\n2024\n1\n2100\n92000\n0\n5500\n2000",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "18_sue.html#implementation",
    "href": "18_sue.html#implementation",
    "title": "15  Standardized Earnings Surprises (SUE)",
    "section": "15.3 Implementation",
    "text": "15.3 Implementation\n\n15.3.1 Python Setup and Data Loading\nFirst, we establish our environment and load the dataset. We ensure the data is sorted by ticker and time to allow for accurate lag calculations.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Creating the dataset directly for this chapter's demonstration\ndata = {\n    'ticker': ['VNM']*8 + ['VCB']*8 + ['HPG']*8,\n    'fiscal_year': [2023, 2023, 2023, 2023, 2024, 2024, 2024, 2024] * 3,\n    'fiscal_qtr': [1, 2, 3, 4, 1, 2, 3, 4] * 3,\n    'eps_basic': [\n        1200, 1350, 1400, 1100, 1300, 1500, 1450, 1250, # VNM\n        1800, 1900, 2000, 2200, 2100, 2300, 2400, 2600, # VCB\n        500, 600, 550, 400, 700, 800, 750, 600          # HPG\n    ],\n    'price_close': [\n        68000, 71000, 74000, 69000, 72000, 75000, 73000, 70000, # VNM\n        85000, 88000, 90000, 95000, 92000, 96000, 98000, 102000, # VCB\n        20000, 22000, 21000, 19000, 25000, 28000, 27000, 24000 # HPG\n    ],\n    'special_items': [\n        0, 50000, 0, -20000, 0, 0, 10000, 0, # VNM (VND Millions)\n        10000, 0, 0, 50000, 0, 20000, 0, 0, # VCB\n        0, 0, -50000, 0, 100000, 0, 0, 0 # HPG\n    ],\n    'shares_out': [2090]*8 + [5580]*8 + [5810]*8, # In Millions\n    'analyst_med': [\n        1150, 1300, 1450, 1150, 1250, 1400, 1480, 1200, # VNM\n        1700, 1850, 1950, 2150, 2000, 2250, 2450, 2550, # VCB\n        450, 550, 600, 450, 650, 750, 800, 650 # HPG\n    ]\n}\n\ndf = pd.DataFrame(data)\n\n# Sort strictly to ensure shift operations work on correct temporal sequence\ndf = df.sort_values(by=['ticker', 'fiscal_year', 'fiscal_qtr'])\nprint(df.head())\n\n   ticker  fiscal_year  fiscal_qtr  eps_basic  price_close  special_items  \\\n16    HPG         2023           1        500        20000              0   \n17    HPG         2023           2        600        22000              0   \n18    HPG         2023           3        550        21000         -50000   \n19    HPG         2023           4        400        19000              0   \n20    HPG         2024           1        700        25000         100000   \n\n    shares_out  analyst_med  \n16        5810          450  \n17        5810          550  \n18        5810          600  \n19        5810          450  \n20        5810          650  \n\n\n\n\n15.3.2 Calculation Logic\nWe now apply the functions to calculate the three variations of SUE.\nStep 1: Handling Seasonality (Lags)\nFor Methods 1 and 2, we require the data from the same quarter of the previous year (lag 4).\n\n# Group by ticker to ensure we don't shift data between companies\ndf['eps_lag4'] = df.groupby('ticker')['eps_basic'].shift(4)\n\nStep 2: Adjusting for Special Items\nFor Method 2, we must strip out non-recurring items. We apply the Vietnamese Corporate Income Tax (CIT) rate of 20%.\nThe formula for the adjustment per share is: \\[ \\text{Adjustment} = \\frac{\\text{Special Items} \\times (1 - 0.20)}{\\text{Shares Outstanding}} \\]\n\n# Constants\nCIT_RATE_VN = 0.20\n\n# Calculate impact per share\n# Note: special_items are in millions, shares_out are in millions\n# The units cancel out, leaving the result in VND per share.\ndf['spi_impact_per_share'] = (df['special_items'] * (1 - CIT_RATE_VN)) / df['shares_out']\n\n# Calculate Adjusted EPS\ndf['eps_adjusted'] = df['eps_basic'] - df['spi_impact_per_share']\n\n# Create lag for Adjusted EPS\ndf['eps_adj_lag4'] = df.groupby('ticker')['eps_adjusted'].shift(4)\n\nStep 3: Computing SUE Variants\nWe finalize the calculation by computing the difference between actual (or adjusted) and expected values, deflated by the stock price.\n\n# Method 1: Seasonal Random Walk (Standard EPS)\ndf['sue_1'] = (df['eps_basic'] - df['eps_lag4']) / df['price_close']\n\n# Method 2: Seasonal Random Walk (Excluding Special Items)\ndf['sue_2'] = (df['eps_adjusted'] - df['eps_adj_lag4']) / df['price_close']\n\n# Method 3: Analyst Forecasts (IBES Equivalent)\ndf['sue_3'] = (df['eps_basic'] - df['analyst_med']) / df['price_close']\n\n# Scaling for readability (converting to percentage)\ndf['sue_1_pct'] = df['sue_1'] * 100\ndf['sue_2_pct'] = df['sue_2'] * 100\ndf['sue_3_pct'] = df['sue_3'] * 100",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "18_sue.html#results-and-analysis",
    "href": "18_sue.html#results-and-analysis",
    "title": "15  Standardized Earnings Surprises (SUE)",
    "section": "15.4 Results and Analysis",
    "text": "15.4 Results and Analysis\nWe present the calculated standardized earnings surprises for the fiscal year 2024. Positive values indicate a positive surprise (beating expectations), while negative values indicate a miss.\n\n15.4.1 Tabular Results (FY 2024)\n\n# Filter for 2024 results where lag data exists\nresults_2024 = df[df['fiscal_year'] == 2024][['ticker', 'fiscal_qtr', 'sue_1_pct', 'sue_2_pct', 'sue_3_pct']]\n\n# Display formatted table\nfrom IPython.display import display, Markdown\nmarkdown_table = results_2024.to_markdown(index=False, floatfmt=\".4f\")\ndisplay(Markdown(markdown_table))\n\n\n\n\nticker\nfiscal_qtr\nsue_1_pct\nsue_2_pct\nsue_3_pct\n\n\n\n\nHPG\n1\n0.8000\n0.7449\n0.2000\n\n\nHPG\n2\n0.7143\n0.7143\n0.1786\n\n\nHPG\n3\n0.7407\n0.7152\n-0.1852\n\n\nHPG\n4\n0.8333\n0.8333\n-0.2083\n\n\nVCB\n1\n0.3261\n0.3276\n0.1087\n\n\nVCB\n2\n0.4167\n0.4137\n0.0521\n\n\nVCB\n3\n0.4082\n0.4082\n-0.0510\n\n\nVCB\n4\n0.3922\n0.3992\n0.0490\n\n\nVNM\n1\n0.1389\n0.1389\n0.0694\n\n\nVNM\n2\n0.2000\n0.2255\n0.1333\n\n\nVNM\n3\n0.0685\n0.0632\n-0.0411\n\n\nVNM\n4\n0.2143\n0.2033\n0.0714\n\n\n\n\n\n\n\n\n\n15.4.2 Visualization\nThe following figure plots the Analyst-based SUE (Method 3) for the selected tickers over the 2024 fiscal year.\n\npivot_sue = results_2024.pivot(index='fiscal_qtr', columns='ticker', values='sue_3_pct')\n\nplt.figure(figsize=(10, 6))\nfor column in pivot_sue.columns:\n    plt.plot(pivot_sue.index, pivot_sue[column], marker='o', label=column)\n\nplt.title('Method 3: Analyst Based SUE (FY 2024)')\nplt.xlabel('Fiscal Quarter')\nplt.ylabel('SUE (%)')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.legend(title='Ticker')\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.xticks([1, 2, 3, 4])\nplt.show()\n\n\n\n\n\n\n\nFigure 15.1",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "18_sue.html#conclusion",
    "href": "18_sue.html#conclusion",
    "title": "15  Standardized Earnings Surprises (SUE)",
    "section": "15.5 Conclusion",
    "text": "15.5 Conclusion\nIn this chapter, we have formalized the calculation of Standardized Earnings Surprises for the Vietnamese market. By implementing three distinct methods using Python, we demonstrated that relying solely on raw EPS growth (Method 1) can be misleading in the presence of non-recurring items. Furthermore, analyst-based surprises (Method 3) often provide a cleaner signal of new information reaching the market.\nFor robust quantitative modeling in Vietnam, we recommend using Method 2 when analyst data is sparse (common in small-cap stocks) and Method 3 for VN30 constituents where analyst coverage is deep and liquid.\n\n\n\n\n\n\nBernard, Victor L, and Jacob K Thomas. 1989. “Post-Earnings-Announcement Drift: Delayed Price Response or Risk Premium?” Journal of Accounting Research 27: 1–36.\n\n\nLivnat, Joshua, and Richard R Mendenhall. 2006. “Comparing the Post–Earnings Announcement Drift for Surprises Calculated from Analyst and Time Series Forecasts.” Journal of Accounting Research 44 (1): 177–205.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "19_divop.html",
    "href": "19_divop.html",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "",
    "text": "17 Theoretical Framework\nA foundational question in financial economics concerns how differences in investor beliefs affect asset prices and trading activity. In markets where investors hold heterogeneous expectations about a firm’s future cash flows, the aggregation of these divergent views into a single market price becomes a non-trivial exercise with profound implications for asset valuation, return predictability, and market efficiency. The concept of divergence of investor opinion (hereafter DIVOP) has emerged as a central construct in both the accounting and finance literatures, serving as a lens through which researchers examine the information environment of firms, the dynamics of uncertainty resolution, and the nature of market reactions to news.\nThe theoretical foundations of the DIVOP literature trace back to Miller (1977), who proposed that when investors disagree about the value of a security and short-sale constraints prevent pessimistic investors from fully expressing their views, the market price will reflect the valuation of the most optimistic investors. This leads to systematic overpricing that is increasing in the degree of opinion divergence. The overpricing persists until information events, such as earnings announcements, reduce disagreement and prices converge toward fundamental values (Berkman et al. 2009). Varian (1985) offers an alternative perspective in which divergence of opinion represents an additional risk factor, leading to higher rather than lower expected returns, creating a theoretical tension that has motivated extensive empirical investigation.\nThe empirical literature on DIVOP has expanded considerably since these seminal contributions. Researchers have documented that divergence of opinion helps explain a range of asset pricing anomalies, including post-earnings announcement drift (Garfinkel and Sokobin 2006; K. L. Anderson, Harris, and So 2007), the cross-sectional return difference between value and growth stocks (Doukas, Kim, and Pantzalis 2004), short- and long-run post-IPO returns (Houge et al. 2001), pre- and post-acquisition stock returns (Alexandridis, Antoniou, and Petmezas 2007), takeover premia (Chatterjee, John, and Yan 2012), and the broad cross-section of stock returns (Diether, Malloy, and Scherbina 2002; Doukas, Kim, and Pantzalis 2006). The explanatory power of DIVOP has been demonstrated using a rich set of empirical proxies, ranging from analyst forecast dispersion and abnormal trading volume to bid-ask spreads and idiosyncratic volatility.\nDespite the maturity of the DIVOP literature in developed markets, particularly the United States, its application to emerging markets remains remarkably thin. This gap is especially notable given that the theoretical conditions under which divergence of opinion matters most (namely, binding short-sale constraints, information asymmetry, and heterogeneous investor sophistication) are arguably more prevalent in emerging markets than in their developed counterparts. The Vietnamese equity market presents a compelling laboratory for studying investor disagreement. The market is characterized by several features that amplify the relevance of the DIVOP framework:\nThis chapter provides a methodology for constructing multiple proxies for divergence of investor opinion adapted to the institutional characteristics of the Vietnamese market. We draw on the methodological frameworks established by Garfinkel (2009) and Diether, Malloy, and Scherbina (2002), while introducing modifications that account for the microstructure of Vietnamese exchanges, the \\(T+2\\) settlement cycle, the absence (until recently) of short selling, and the availability of data through domestic financial platforms. Specifically, we construct and analyze the following DIVOP proxies:\nFor each proxy, we describe the theoretical motivation, the data requirements, the construction methodology adapted for Vietnamese data, the empirical properties observed in the Vietnamese cross-section, and the practical considerations that researchers should bear in mind when employing these measures. We pay particular attention to issues that are specific to emerging markets, including thin trading, corporate action adjustments, exchange-specific microstructure effects, and the interplay between foreign ownership constraints and measures of investor disagreement.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#the-miller-1977-overpricing-hypothesis",
    "href": "19_divop.html#the-miller-1977-overpricing-hypothesis",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "17.1 The Miller (1977) Overpricing Hypothesis",
    "text": "17.1 The Miller (1977) Overpricing Hypothesis\nThe canonical model of divergence of opinion and asset pricing begins with Miller (1977). Miller’s central insight is simple: in a market where investors hold heterogeneous beliefs about the future payoffs of a risky asset and short-sale constraints prevent some investors from acting on their pessimistic views, the equilibrium price will be set by the subset of investors who are most optimistic about the asset’s value. The severity of overpricing is increasing in both the degree of opinion divergence and the stringency of short-sale constraints. Formally, if investor \\(i\\) assigns a valuation \\(V_i\\) to a security, the market price \\(P\\) satisfies:\n\\[\nP = E[V_i \\mid V_i \\geq V^*]\n\\]\nwhere \\(V^*\\) is the marginal investor’s valuation, which exceeds the unconditional mean valuation \\(E[V_i]\\) whenever short-sale constraints bind for some investors. The degree of overpricing is:\n\\[\n\\text{Overpricing} = P - E[V_i] = E[V_i \\mid V_i \\geq V^*] - E[V_i]\n\\]\nwhich is positive and increasing in the dispersion of the distribution of \\(V_i\\) (i.e., divergence of opinion) and in \\(V^*\\) (i.e., the severity of short-sale constraints).\nMiller’s model generates several testable predictions:\n\nCross-sectional prediction: Stocks with higher divergence of opinion should have lower subsequent returns as prices gradually correct toward fundamental values.\nTime-series prediction: Information events that reduce disagreement (e.g., earnings announcements) should be associated with negative abnormal returns for high-DIVOP stocks, as the “optimism premium” dissipates.\nInteraction prediction: The overpricing effect should be strongest among stocks that simultaneously exhibit high divergence of opinion and binding short-sale constraints.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#alternative-theoretical-perspectives",
    "href": "19_divop.html#alternative-theoretical-perspectives",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "17.2 Alternative Theoretical Perspectives",
    "text": "17.2 Alternative Theoretical Perspectives\nVarian (1985) proposes an alternative framework in which divergence of opinion acts as a risk factor. If investors are risk-averse and disagreement represents genuine uncertainty about future payoffs, then higher dispersion of beliefs should be associated with higher expected returns as compensation for bearing the additional risk. This creates a sharp empirical dichotomy: the Miller hypothesis predicts a negative DIVOP-return relation, whereas the Varian model predicts a positive relation.\nThe distinction between these theories hinges critically on the market microstructure and institutional setting (@tbl-divop-theories).\n\n\n\nTable 17.1: Summary of theoretical predictions for the DIVOP-return relation under different assumptions\n\n\n\n\n\n\n\n\n\n\n\nTheoretical Framework\nShort-Sale Constraints\nDIVOP-Return Relation\nKey Mechanism\n\n\n\n\nMiller (1977)\nBinding\nNegative\nOptimistic bias in price\n\n\nVarian (1985)\nNon-binding\nPositive\nRisk premium for uncertainty\n\n\nHong and Stein (2003)\nBinding, gradual info\nNegative, time-varying\nSlow diffusion of bearish views\n\n\nScheinkman and Xiong (2003)\nBinding, overconfidence\nNegative\nSpeculative bubble premium\n\n\n\n\n\n\nHong and Stein (2003) extend Miller’s framework by incorporating gradual information diffusion. In their model, bearish information is impounded into prices more slowly than bullish information because short-sale constraints raise the cost of acting on negative views. This generates momentum-like patterns in which high-DIVOP stocks exhibit positive short-run returns (as optimists push prices up) followed by negative long-run returns (as bearish information eventually reaches the market).\nScheinkman and Xiong (2003) introduce an additional dimension by noting that when investors are overconfident about their private signals and short-sale constraints bind, stock prices contain a “speculative bubble” component that reflects the option value of reselling the asset to a future investor who may be even more optimistic. This model predicts that both high trading volume and high price volatility should be associated with overpricing, providing a theoretical basis for using volume-based and volatility-based DIVOP proxies.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#relevance-to-the-vietnamese-market",
    "href": "19_divop.html#relevance-to-the-vietnamese-market",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "17.3 Relevance to the Vietnamese Market",
    "text": "17.3 Relevance to the Vietnamese Market\nThe Vietnamese equity market provides an unusually clean setting for testing the Miller hypothesis. Vietnam’s equity market operated without any short-selling mechanism from its inception in 2000 through January 2025, which was a full quarter-century in which the first necessary condition of Miller’s model (binding short-sale constraints) was satisfied by regulation rather than by market frictions. Even after the introduction of covered short selling in 2025, the mechanism remains restricted to securities meeting specific liquidity and market capitalization thresholds, and the regulatory environment imposes borrowing requirements that significantly raise the cost of shorting relative to developed markets.\nThe dominance of retail investors amplifies the second necessary condition (i.e., heterogeneous beliefs). Research on the Vietnamese market has documented significant herding behavior (Vo and Phan 2017; Vo 2015), sentiment-driven trading (Phan et al. 2023; Nguyen and Pham 2018), and information asymmetry between domestic and foreign investors (Vo 2017). These behavioral characteristics naturally generate wider dispersion of investor valuations compared to markets dominated by institutional investors with access to similar analytical frameworks and information sources.\nTable 17.2 compares key institutional features relevant to the DIVOP framework between Vietnam and the United States.\n\n\n\nTable 17.2: Institutional comparison of Vietnam and the United States relevant to divergence of opinion\n\n\n\n\n\n\n\n\n\n\nFeature\nVietnam (HOSE/HNX)\nUnited States (NYSE/NASDAQ)\n\n\n\n\nShort selling\nIntroduced Jan 2025 (limited)\nPermitted (Reg SHO since 2005)\n\n\nRetail investor share of volume\n~80-85%\n~25%\n\n\nSettlement cycle\nT+2 (T+1 planned for 2026)\nT+1 (since May 2024)\n\n\nDaily price limits\n\\(\\pm\\) 7% (HOSE), \\(\\pm\\) 10% (HNX)\nNone\n\n\nForeign ownership cap\n49% (most sectors)\nNone\n\n\nAverage analyst coverage (VN30)\n5-10 analysts\n15-25 analysts\n\n\nMandatory quarterly reporting\nYes (since 2012)\nYes\n\n\nOptions/derivatives market\nVN30 Index Futures (since 2017)\nExtensive options/futures\n\n\n\n\n\n\nThe presence of daily price limits (\\(\\pm\\) 7% on HOSE and \\(\\pm\\) 10% on HNX) creates an additional mechanism through which divergence of opinion can be amplified. When a stock hits its price limit, investors who wish to trade in the direction of the limit are unable to do so, leading to accumulated unfilled orders and delayed price discovery. This institutional feature may create short-term spikes in measured DIVOP that reflect limit-induced friction rather than genuine disagreement. We address this issue in our empirical methodology by flagging limit-hit days and conducting robustness checks that exclude these observations.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#data-sources",
    "href": "19_divop.html#data-sources",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "18.1 Data Sources",
    "text": "18.1 Data Sources\nThe construction of DIVOP proxies for the Vietnamese market requires daily stock-level trading data and, for the analyst dispersion measures, individual analyst forecast data. We source all data from DataCore.vn, which provides coverage of all securities listed on HOSE, HNX, and the UPCoM (Unlisted Public Company Market) exchange. Table 18.1 summarizes the datasets and key variables used in this study.\n\n\n\nTable 18.1: Data sources and key variables for DIVOP proxy construction\n\n\n\n\n\n\n\n\n\n\nDataset\nKey Variables\nFrequency\n\n\n\n\nDaily Stock Trading\nClose price, high, low, open, volume, shares outstanding, adjusted price, bid, ask\nDaily\n\n\nCorporate Actions\nDividends, stock splits, bonus issues, rights offerings\nEvent-based\n\n\nCompany Information\nExchange code, industry classification (ICB), listing date, delisting date\nStatic/Periodic\n\n\nAnalyst Forecasts\nIndividual analyst EPS forecasts, announcement dates, fiscal period end, analyst ID, broker name\nPer estimate\n\n\nMarket Index\nVN-Index daily returns, VN30 returns, HNX-Index returns\nDaily\n\n\nForeign Ownership\nForeign buy/sell volume, foreign ownership percentage, remaining foreign room\nDaily",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#sample-construction",
    "href": "19_divop.html#sample-construction",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "18.2 Sample Construction",
    "text": "18.2 Sample Construction\nWe construct our sample using the following filters, applied sequentially:\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats as scipy_stats\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# =============================================================================\n# Configuration Parameters\n# =============================================================================\n# Users can modify these parameters to adjust the methodology\nCONFIG = {\n    # Sample period\n    'beg_date': '2007-01-01',\n    'end_date': '2024-12-31',\n    \n    # Estimation windows (in trading days)\n    'est_window': 60,          # Rolling window for SUV and volatility\n    'detrend_window': 180,     # Window for DTO detrending median\n    'lag': 7,                  # Lag for DTO detrending\n    'gap': 5,                  # Gap between estimation period and event date\n    \n    # Filters\n    'min_price': 1000,         # Minimum price in VND\n    'min_volume_days': 0.8,    # Min fraction of non-zero volume days in window\n    'min_analysts': 3,         # Minimum number of analysts for DISP\n    'max_spread_pct': 0.50,    # Maximum bid-ask spread as fraction of midpoint\n    'forecast_carry_days': 105,# Days to carry forward stale analyst forecasts\n    \n    # Exchange identifiers\n    'exchanges': ['HOSE', 'HNX'],\n    \n    # Price limit thresholds (for flagging)\n    'price_limit_hose': 0.07,\n    'price_limit_hnx': 0.10,\n}\n\nprint(\"Configuration parameters loaded successfully.\")\nprint(f\"Sample period: {CONFIG['beg_date']} to {CONFIG['end_date']}\")\nprint(f\"Estimation window: {CONFIG['est_window']} trading days\")\nprint(f\"Detrending window: {CONFIG['detrend_window']} trading days\")\n\nConfiguration parameters loaded successfully.\nSample period: 2007-01-01 to 2024-12-31\nEstimation window: 60 trading days\nDetrending window: 180 trading days\n\n\nThe sample universe includes all common stocks (ordinary shares) listed on HOSE and HNX during the period January 2007 through December 2024. We begin in 2007 rather than at market inception (2000 for HOSE, 2005 for HNX) for two reasons. First, the early years of the Vietnamese market were characterized by an extremely small number of listed firms (fewer than 30 on HOSE through 2005), making cross-sectional analysis unreliable. Second, data quality and consistency improve substantially after the market expansion of 2006-2007, during which the number of listed firms on HOSE grew from approximately 40 to over 100.\nWe apply the following filters to construct the analysis sample:\n\nSecurity type filter. We retain only common stocks (ordinary shares), excluding preferred shares, exchange-traded funds (ETFs), covered warrants, and certificates of deposit. This is analogous to the standard filter in the U.S. literature that restricts to CRSP share codes 10 and 11.\nExchange filter. We include stocks listed on HOSE and HNX but exclude UPCoM securities in our baseline analysis. UPCoM is a registration-based trading venue with less stringent listing requirements and substantially lower liquidity, which may introduce noise into volume-based and spread-based measures. We include UPCoM in robustness checks.\nPrice filter. We exclude stock-day observations with closing prices below 1,000 VND. This threshold serves the same purpose as the “penny stock” exclusion common in U.S. studies (typically $1 or $5 thresholds) and helps mitigate the influence of extreme percentage returns and spreads at very low price levels.\nMinimum trading activity. For volume-based measures, we require that a stock has non-zero trading volume on at least 80% of trading days within each estimation window. This filter eliminates the most thinly traded securities for which turnover-based measures would be unreliable.\n\n\ndef load_daily_data(config):\n    \"\"\"\n    Load daily stock trading data from DataCore.vn.\n    \n    In practice, this function connects to the DataCore API or reads\n    from a local database/CSV. Here we document the expected schema.\n    \n    Expected columns:\n    - ticker: str, stock ticker symbol (e.g., 'VCB', 'HPG', 'VNM')\n    - date: datetime, trading date\n    - open, high, low, close: float, daily OHLC prices (VND)\n    - volume: int, trading volume (shares)\n    - shares_outstanding: int, total shares outstanding\n    - adjusted_close: float, price adjusted for corporate actions\n    - adj_factor: float, cumulative adjustment factor\n    - bid, ask: float, best bid/ask at close\n    - exchange: str, exchange code ('HOSE', 'HNX', 'UPCOM')\n    - industry_icb: str, ICB industry classification code\n    - foreign_buy_vol, foreign_sell_vol: int, foreign investor volumes\n    - foreign_ownership_pct: float, foreign ownership percentage\n    \"\"\"\n    # =========================================================================\n    # Replace with actual DataCore API call:\n    # from datacore import Client\n    # client = Client(api_key='YOUR_KEY')\n    # df = client.daily_stock(\n    #     start=config['beg_date'], end=config['end_date'],\n    #     exchanges=config['exchanges']\n    # )\n    # =========================================================================\n    print(\"Connect to DataCore.vn and load daily stock data.\")\n    print(\"Expected schema: ticker, date, open, high, low, close, volume,\")\n    print(\"  shares_outstanding, adjusted_close, adj_factor, bid, ask,\")\n    print(\"  exchange, industry_icb, foreign_buy_vol, foreign_sell_vol,\")\n    print(\"  foreign_ownership_pct\")\n    return None  # Replace with actual data\n\n\ndef apply_sample_filters(df, config):\n    \"\"\"Apply sequential sample construction filters.\"\"\"\n    print(\"\\n=== Sample Construction ===\")\n    n0 = len(df)\n    \n    # Date filter\n    df = df[(df['date'] &gt;= config['beg_date']) &\n            (df['date'] &lt;= config['end_date'])].copy()\n    print(f\"[1] Date filter: {len(df):,} obs (from {n0:,})\")\n    \n    # Exchange filter\n    df = df[df['exchange'].isin(config['exchanges'])].copy()\n    print(f\"[2] Exchange filter ({config['exchanges']}): {len(df):,} obs\")\n    \n    # Price filter\n    df = df[df['close'] &gt;= config['min_price']].copy()\n    print(f\"[3] Price &gt;= {config['min_price']:,} VND: {len(df):,} obs\")\n    \n    # Compute daily return from adjusted prices\n    df = df.sort_values(['ticker', 'date'])\n    df['ret'] = df.groupby('ticker')['adjusted_close'].pct_change()\n    \n    # Flag price limit hits\n    df['limit_hit'] = (\n        ((df['exchange'] == 'HOSE') &\n         (df['ret'].abs() &gt;= config['price_limit_hose'] - 0.001)) |\n        ((df['exchange'] == 'HNX') &\n         (df['ret'].abs() &gt;= config['price_limit_hnx'] - 0.001))\n    )\n    \n    n_tickers = df['ticker'].nunique()\n    print(f\"\\nFinal sample: {len(df):,} stock-day obs, \"\n          f\"{n_tickers} unique tickers\")\n    print(f\"Limit-hit days: {df['limit_hit'].sum():,} \"\n          f\"({100*df['limit_hit'].mean():.2f}%)\")\n    return df",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#sec-corp-actions",
    "href": "19_divop.html#sec-corp-actions",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "18.3 Corporate Action Adjustments",
    "text": "18.3 Corporate Action Adjustments\nProper adjustment for corporate actions is critical for volume-based DIVOP measures, as events such as stock splits, bonus share issues, and rights offerings change the number of shares outstanding and can create artificial spikes in measured turnover. We need to use cumulative adjustment factors that account for stock dividends (bonus shares), stock splits, rights offerings, and cash dividends (price adjustment only). We use these to construct adjusted volume and adjusted shares outstanding:\n\\[\n\\text{AdjVolume}_{i,t} = \\text{Volume}_{i,t} \\times \\text{CumAdjFactor}_{i,t}\n\\]\n\\[\n\\text{AdjSharesOut}_{i,t} = \\text{SharesOut}_{i,t} \\times \\text{CumAdjFactor}_{i,t}\n\\]\nThis ensures that the turnover ratio is consistent across corporate action events.\n\ndef adjust_for_corporate_actions(df):\n    \"\"\"Apply cumulative adjustment factors to volume and shares outstanding.\"\"\"\n    df = df.copy()\n    df['adj_volume'] = df['volume'] * df['adj_factor']\n    df['adj_shares_out'] = df['shares_outstanding'] * df['adj_factor']\n    \n    # Daily turnover ratio\n    df['turnover'] = np.where(\n        df['adj_shares_out'] &gt; 0,\n        df['adj_volume'] / df['adj_shares_out'],\n        np.nan\n    )\n    \n    # Flag extreme turnover (&gt; 50% of float)\n    extreme = df['turnover'] &gt; 0.50\n    if extreme.any():\n        print(f\"Warning: {extreme.sum()} obs with turnover &gt; 50%, set to NaN\")\n        df.loc[extreme, 'turnover'] = np.nan\n    \n    return df",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#sec-calendar",
    "href": "19_divop.html#sec-calendar",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "18.4 Trading Calendar Construction",
    "text": "18.4 Trading Calendar Construction\nThe rolling regression approach for SUV and volatility requires a trading calendar that ensures each estimation window contains exactly the specified number of trading days. We construct this directly from observed trading dates.\n\ndef build_trading_calendar(df, config):\n    \"\"\"\n    Map each trading date to its estimation window [est_start, est_end].\n    \n    For date t, the estimation window runs from\n    t - gap - est_window to t - gap - 1 (in trading-day terms).\n    \"\"\"\n    trading_dates = sorted(df['date'].unique())\n    trading_dates = pd.Series(trading_dates)\n    \n    est_window = config['est_window']\n    gap = config['gap']\n    offset = est_window + gap\n    \n    records = []\n    for i in range(offset, len(trading_dates)):\n        records.append({\n            'date': trading_dates.iloc[i],\n            'est_start': trading_dates.iloc[i - gap - est_window],\n            'est_end': trading_dates.iloc[i - gap - 1]\n        })\n    \n    calendar = pd.DataFrame(records)\n    print(f\"Trading calendar: {len(calendar)} dates, \"\n          f\"{calendar['date'].min()} to {calendar['date'].max()}\")\n    return calendar",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#theoretical-motivation",
    "href": "19_divop.html#theoretical-motivation",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "19.1 Theoretical Motivation",
    "text": "19.1 Theoretical Motivation\nTrading volume has long been recognized as a natural proxy for divergence of investor opinion. In the rational expectations framework of Milgrom and Stokey (1982), trade occurs only when investors disagree about the value of a security (i.e., a “no-trade theorem” that implies, by contrapositive, that observed trading volume must reflect some form of heterogeneous beliefs). Harris and Raviv (1993) and Kandel and Pearson (1995) formalize this intuition, showing that trading volume is positively related to the dispersion of investors’ prior beliefs and to the degree to which public information is differentially interpreted.\nThe challenge in using raw trading volume as a DIVOP proxy is that volume is also driven by factors unrelated to disagreement, including portfolio rebalancing, liquidity needs, tax-loss selling, and index reconstitution effects. Garfinkel (2009) proposes two approaches to extract the disagreement component from raw volume. The first, Unexplained Volume (DTO), removes market-wide volume effects and secular trends. The second, Standardized Unexplained Volume (SUV), additionally controls for the information content of returns through a cross-sectional regression, isolating the “pure disagreement” component of trading activity.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#sec-dto",
    "href": "19_divop.html#sec-dto",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "19.2 Unexplained Volume (DTO)",
    "text": "19.2 Unexplained Volume (DTO)\n\n19.2.1 Construction Methodology\nThe construction of the Unexplained Volume measure proceeds in four steps.\nStep 1: Compute firm-level daily turnover. For each stock \\(i\\) on day \\(t\\):\n\\[\n\\text{Turn}_{i,t} = \\frac{\\text{AdjVolume}_{i,t}}{\\text{AdjSharesOut}_{i,t}}\n\\]\nStep 2: Compute market-wide turnover. We calculate aggregate turnover across all common stocks as a value-weighted average:\n\\[\n\\text{MktTurn}_{t} = \\frac{\\sum_{i} \\text{AdjVolume}_{i,t}}{\\sum_{i} \\text{AdjSharesOut}_{i,t}}\n\\]\nUnlike the U.S. methodology that computes market turnover across NYSE/AMEX stocks only and applies a scaling adjustment for NASDAQ securities (following A.-M. Anderson and Dyl 2005), we compute market turnover across all HOSE and HNX common stocks without any exchange-specific volume scaling. Both Vietnamese exchanges operate as order-driven markets (HOSE uses continuous order matching; HNX uses a combination of continuous matching and periodic call auctions) without the dealer-market double-counting issue that necessitates the NASDAQ volume adjustment in U.S. studies.\nStep 3: Compute market-adjusted turnover.\n\\[\n\\text{MATO}_{i,t} = \\text{Turn}_{i,t} - \\text{MktTurn}_{t}\n\\]\nStep 4: Detrend by rolling median. To remove secular trends in firm-specific trading activity:\n\\[\n\\text{DTO}_{i,t} = \\text{MATO}_{i,t} - \\text{Median}_{180}(\\text{MATO}_{i,t-7})\n\\]\nwhere \\(\\text{Median}_{180}(\\text{MATO}_{i,t-7})\\) is the median of market-adjusted turnover over the 180-trading-day window ending 7 days before date \\(t\\). The 7-day lag prevents the current day’s turnover from influencing its own detrending baseline.\n\ndef compute_market_turnover(df):\n    \"\"\"Compute daily market-wide turnover across all stocks.\"\"\"\n    mkt_turn = df.groupby('date').apply(\n        lambda x: x['adj_volume'].sum() / x['adj_shares_out'].sum()\n        if x['adj_shares_out'].sum() &gt; 0 else np.nan\n    ).reset_index()\n    mkt_turn.columns = ['date', 'market_turnover']\n    return mkt_turn\n\n\ndef compute_dto(df, config):\n    \"\"\"\n    Construct Unexplained Volume (DTO).\n    \n    Steps:\n    1. Subtract market turnover -&gt; MATO\n    2. Rolling 180-day median of MATO (lagged 7 days) -&gt; trend\n    3. DTO = MATO - trend\n    \"\"\"\n    detrend_window = config['detrend_window']\n    lag = config['lag']\n    \n    # Market turnover\n    mkt_turn = compute_market_turnover(df)\n    df = df.merge(mkt_turn, on='date', how='left')\n    \n    # Market-adjusted turnover\n    df['mato'] = df['turnover'] - df['market_turnover']\n    \n    # Rolling median with lag, computed per stock\n    df = df.sort_values(['ticker', 'date'])\n    \n    def _rolling_median_lagged(group):\n        mato = group['mato']\n        med = mato.rolling(\n            window=detrend_window,\n            min_periods=int(detrend_window * 0.5)\n        ).median()\n        return med.shift(lag)\n    \n    df['mato_trend'] = (\n        df.groupby('ticker', group_keys=False)\n          .apply(lambda g: _rolling_median_lagged(g))\n    )\n    \n    # DTO\n    df['dto'] = df['mato'] - df['mato_trend']\n    \n    print(\"DTO construction complete.\")\n    print(f\"  Non-missing: {df['dto'].notna().sum():,}\")\n    print(f\"  Mean: {df['dto'].mean():.6f}, Std: {df['dto'].std():.6f}\")\n    return df\n\n\n\n19.2.2 Vietnam-Specific Considerations for DTO\nSeveral features of the Vietnamese market require attention when constructing DTO:\n\nNo NASDAQ-type volume adjustment needed. Both HOSE and HNX are order-driven auction markets. The double-counting adjustment applied to NASDAQ securities in the U.S. literature is not necessary.\nThinly traded stocks. A substantial fraction of listed Vietnamese stocks, particularly on HNX, may have zero volume on many trading days. For stocks with intermittent trading, the rolling median may be biased toward zero, making DTO less informative. We require at least 80% non-zero volume days in each estimation window.\nPrice limit effects on volume. When a stock hits its daily price limit, unfilled orders accumulate and recorded volume may understate true clearing volume. The following day often shows a “catch-up” effect. Researchers should consider flagging limit-hit days.\nForeign investor trading decomposition. DataCore provides volume by investor type (foreign versus domestic). Researchers may wish to construct separate DTO measures for foreign and domestic volume, or use the foreign-to-domestic volume ratio as an additional dimension of disagreement.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#sec-suv",
    "href": "19_divop.html#sec-suv",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "19.3 Standardized Unexplained Volume (SUV)",
    "text": "19.3 Standardized Unexplained Volume (SUV)\n\n19.3.1 Construction Methodology\nThe Standardized Unexplained Volume measure, proposed by Garfinkel (2009), isolates the disagreement component of volume by explicitly controlling for the information content of returns. The insight is that trading volume has both a liquidity component and an informedness component correlated with the magnitude and sign of returns. By regressing turnover on signed returns and extracting the standardized residual, SUV captures volume attributable to disagreement after controlling for both liquidity trends and information-driven trading.\nFor each stock \\(i\\), on each trading date \\(t\\), we estimate using data from the estimation window \\([\\tau_1, \\tau_2]\\):\n\\[\n\\text{Turn}_{i,s} = \\alpha_i + \\beta_i^{+} \\cdot \\text{RetPos}_{i,s} + \\beta_i^{-} \\cdot \\text{RetNeg}_{i,s} + \\epsilon_{i,s}, \\quad s \\in [\\tau_1, \\tau_2]\n\\tag{19.1}\\]\nwhere \\(\\text{RetPos}_{i,s} = |r_{i,s}| \\cdot \\mathbf{1}(r_{i,s} &gt; 0)\\) and \\(\\text{RetNeg}_{i,s} = |r_{i,s}| \\cdot \\mathbf{1}(r_{i,s} &lt; 0)\\).\nThe Standardized Unexplained Volume on date \\(t\\) is:\n\\[\n\\text{SUV}_{i,t} = \\frac{\\text{Turn}_{i,t} - \\hat{\\text{Turn}}_{i,t}}{\\hat{\\sigma}_{\\epsilon,i}}\n\\tag{19.2}\\]\nwhere \\(\\hat{\\text{Turn}}_{i,t}\\) is the predicted turnover and \\(\\hat{\\sigma}_{\\epsilon,i}\\) is the RMSE from Equation 19.1.\nThe asymmetric specification with separate coefficients for positive and negative returns reflects that the volume-return relation differs by return sign. In the U.S., buying pressure tends to generate more volume than selling pressure due to short-sale frictions. In Vietnam, where short selling was unavailable until 2025, this asymmetry should be even more pronounced because all selling activity was constrained to existing shareholders.\n\ndef compute_suv(df, calendar, config):\n    \"\"\"\n    Compute Standardized Unexplained Volume via rolling regressions.\n    \n    For each stock-date, regress Turn on RetPos and RetNeg over the\n    estimation window, then compute SUV = (actual - predicted) / RMSE.\n    \"\"\"\n    est_window = config['est_window']\n    min_obs = int(est_window * config['min_volume_days'])\n    \n    # Prepare signed return components\n    df = df.copy()\n    df['ret_pos'] = np.where(df['ret'] &gt; 0, np.abs(df['ret']), 0.0)\n    df['ret_neg'] = np.where(\n        (df['ret'] &lt; 0) & df['ret'].notna(), np.abs(df['ret']), 0.0\n    )\n    \n    results = []\n    grouped = {t: g for t, g in df.groupby('ticker')}\n    \n    for _, cal_row in calendar.iterrows():\n        dt = cal_row['date']\n        est_s, est_e = cal_row['est_start'], cal_row['est_end']\n        \n        for ticker, tdata in grouped.items():\n            # Estimation window\n            est = tdata[\n                (tdata['date'] &gt;= est_s) & (tdata['date'] &lt;= est_e)\n            ].dropna(subset=['turnover', 'ret_pos', 'ret_neg'])\n            \n            if len(est) &lt; min_obs:\n                continue\n            \n            # Event date\n            evt = tdata[tdata['date'] == dt]\n            if evt.empty or evt['turnover'].isna().all():\n                continue\n            \n            # OLS: Turn = alpha + beta_pos * RetPos + beta_neg * RetNeg\n            X = est[['ret_pos', 'ret_neg']].values\n            y = est['turnover'].values\n            \n            reg = LinearRegression().fit(X, y)\n            y_hat = reg.predict(X)\n            rmse = np.sqrt(np.mean((y - y_hat) ** 2))\n            \n            if rmse &lt;= 0:\n                continue\n            \n            # Predict and standardize for event date\n            X_evt = evt[['ret_pos', 'ret_neg']].values\n            pred = reg.predict(X_evt)[0]\n            actual = evt['turnover'].values[0]\n            suv = (actual - pred) / rmse\n            \n            results.append({\n                'ticker': ticker, 'date': dt,\n                'suv': suv,\n                'predicted_turnover': pred,\n                'rmse_turn': rmse,\n                'n_est': len(est),\n                'alpha_turn': reg.intercept_,\n                'beta_pos': reg.coef_[0],\n                'beta_neg': reg.coef_[1],\n            })\n    \n    suv_df = pd.DataFrame(results)\n    print(f\"SUV: {len(suv_df):,} stock-date obs\")\n    print(f\"  Mean: {suv_df['suv'].mean():.4f}, \"\n          f\"Median: {suv_df['suv'].median():.4f}\")\n    return suv_df\n\n\n\n19.3.2 Interpreting the SUV Regression Coefficients\nThe estimated coefficients from Equation 19.1 are informative about market microstructure. Garfinkel (2009) reports \\(\\hat{\\beta}^{+} &gt; \\hat{\\beta}^{-}\\) for most U.S. stocks. In Vietnam, we expect this asymmetry to be even stronger because:\n\nNo short selling (pre-2025): All selling is by existing shareholders, limiting volume response to negative returns.\nT+2 settlement: Investors cannot immediately reinvest sale proceeds, further dampening sell-side volume.\nPrice limits: The \\(\\pm\\) 7% (HOSE) and \\(\\pm\\) 10% (HNX) daily limits truncate the return distribution, compressing the range of both regressors.\n\nResearchers should report summary statistics of \\((\\hat{\\alpha}, \\hat{\\beta}^{+}, \\hat{\\beta}^{-}, R^2)\\) across the cross-section and over time.\n\ndef suv_diagnostics(suv_df):\n    \"\"\"Report cross-sectional summary of SUV regression parameters.\"\"\"\n    print(\"\\n=== SUV Regression Diagnostics ===\")\n    \n    params = ['alpha_turn', 'beta_pos', 'beta_neg']\n    print(suv_df[params].describe(\n        percentiles=[.05, .25, .50, .75, .95]\n    ).T.to_string(float_format='{:.6f}'.format))\n    \n    # Asymmetry test\n    diff = suv_df['beta_pos'] - suv_df['beta_neg']\n    print(f\"\\nbeta_pos - beta_neg: mean = {diff.mean():.6f}, \"\n          f\"frac &gt; 0 = {(diff &gt; 0).mean():.3f}\")",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#sec-total-vol",
    "href": "19_divop.html#sec-total-vol",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "20.1 Total Return Volatility",
    "text": "20.1 Total Return Volatility\n\n20.1.1 Theoretical Motivation\nStock return volatility serves as a proxy for divergence of opinion through several channels. Shalen (1993) develops a model in which both volume and volatility are increasing in the dispersion of investor beliefs. Scheinkman and Xiong (2003) predict that higher volatility reflects the speculative trading component driven by overconfident investors who disagree about value. Empirically, Boehme, Danielsen, and Sorescu (2006) and Chatterjee, John, and Yan (2012) use idiosyncratic volatility as a DIVOP proxy and find it positively correlated with other disagreement measures and negatively associated with subsequent returns when short-sale constraints bind.\n\n\n20.1.2 Construction\nTotal return volatility is the standard deviation of daily returns over the rolling estimation window:\n\\[\n\\text{VOLATILITY}_{i,t} = \\sqrt{\\frac{1}{N_i - 1} \\sum_{s \\in [\\tau_1, \\tau_2]} (r_{i,s} - \\bar{r}_i)^2}\n\\tag{20.1}\\]\nwhere \\(N_i\\) is the number of non-missing return observations for stock \\(i\\) in the window \\([\\tau_1, \\tau_2]\\).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#sec-ivol",
    "href": "19_divop.html#sec-ivol",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "20.2 Idiosyncratic Volatility (IVOL)",
    "text": "20.2 Idiosyncratic Volatility (IVOL)\nIdiosyncratic volatility isolates firm-specific return variation by removing the systematic component explained by market movements. We compute IVOL from the residuals of a market model:\n\\[\nr_{i,s} = \\alpha_i + \\beta_i \\cdot r_{m,s} + \\epsilon_{i,s}, \\quad s \\in [\\tau_1, \\tau_2]\n\\tag{20.2}\\]\n\\[\n\\text{IVOL}_{i,t} = \\text{Std}(\\hat{\\epsilon}_{i,s})\n\\tag{20.3}\\]\nResearchers may extend this to a Fama and French (1993) three-factor or five-factor model using Vietnamese factor portfolios constructed elsewhere in this book. A richer factor model yields IVOL estimates that better isolate truly idiosyncratic disagreement, at the cost of requiring factor portfolio construction.\n\ndef compute_volatility(df, calendar, config):\n    \"\"\"\n    Compute total return volatility and idiosyncratic volatility\n    via rolling estimation windows.\n    \n    Total vol = std(returns) in window.\n    IVOL = std(residuals) from market model regression.\n    \"\"\"\n    est_window = config['est_window']\n    min_obs = int(est_window * config['min_volume_days'])\n    \n    # Value-weighted market return\n    def _vw_ret(g):\n        valid = g.dropna(subset=['ret'])\n        if valid.empty:\n            return np.nan\n        w = valid['adj_shares_out'] * valid['close']\n        return np.average(valid['ret'], weights=w)\n    \n    mkt_ret = df.groupby('date').apply(_vw_ret).reset_index()\n    mkt_ret.columns = ['date', 'mkt_ret']\n    df = df.merge(mkt_ret, on='date', how='left')\n    \n    results = []\n    grouped = {t: g for t, g in df.groupby('ticker')}\n    \n    for _, cal_row in calendar.iterrows():\n        dt = cal_row['date']\n        est_s, est_e = cal_row['est_start'], cal_row['est_end']\n        \n        for ticker, tdata in grouped.items():\n            est = tdata[\n                (tdata['date'] &gt;= est_s) & (tdata['date'] &lt;= est_e)\n            ].dropna(subset=['ret', 'mkt_ret'])\n            \n            if len(est) &lt; min_obs:\n                continue\n            \n            # Total volatility\n            total_vol = est['ret'].std()\n            \n            # Market model -&gt; IVOL\n            X = est[['mkt_ret']].values\n            y = est['ret'].values\n            reg = LinearRegression().fit(X, y)\n            resid = y - reg.predict(X)\n            ivol = np.std(resid, ddof=1)\n            \n            results.append({\n                'ticker': ticker, 'date': dt,\n                'total_volatility': total_vol,\n                'idio_volatility': ivol,\n                'market_beta': reg.coef_[0],\n                'market_alpha': reg.intercept_,\n                'r_squared_mm': reg.score(X, y),\n                'n_vol': len(est),\n            })\n    \n    vol_df = pd.DataFrame(results)\n    print(f\"Volatility: {len(vol_df):,} stock-date obs\")\n    print(f\"  Total vol (ann. mean): \"\n          f\"{vol_df['total_volatility'].mean() * np.sqrt(252):.4f}\")\n    print(f\"  IVOL (ann. mean): \"\n          f\"{vol_df['idio_volatility'].mean() * np.sqrt(252):.4f}\")\n    return vol_df\n\n\n20.2.1 Vietnam-Specific Considerations for Volatility\n\nPrice limits compress measured volatility. Daily limits of \\(\\pm\\) 7% (HOSE) and \\(\\pm\\) 10% (HNX) mechanically truncate the return distribution, leading to underestimation of true volatility. On limit-hit days, the true equilibrium return may exceed the observed return. Researchers should be aware that volatility-based DIVOP measures may be downward-biased for stocks that frequently hit limits.\nVN-Index concentration. The VN-Index is highly concentrated, the top 10 stocks often account for 50-60% of index weight. For small- and mid-cap stocks, an equal-weighted market return or a composite HOSE+HNX index may provide a better market factor in Equation 20.2.\nThin trading and non-synchronous returns. For thinly traded stocks, consecutive zero-return days can depress measured volatility. The Dimson (1979) adjustment (including lagged and lead market returns in the market model) may help correct for non-synchronous trading bias in the beta estimate, though its effect on IVOL is typically small.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#sec-baspread",
    "href": "19_divop.html#sec-baspread",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "21.1 Bid-Ask Spread (BASPREAD)",
    "text": "21.1 Bid-Ask Spread (BASPREAD)\n\n21.1.1 Theoretical Motivation\nThe bid-ask spread reflects the adverse selection costs faced by limit order providers. When investors hold heterogeneous beliefs, each trade is more likely to convey private information, raising the adverse selection component of the spread. Handa, Schwartz, and Tiwari (2003) show that in order-driven markets the spread widens when divergence of opinion increases because limit order providers face greater risk of being picked off by informed traders. Chung and Zhang (2014) demonstrate that closing bid-ask spreads from daily data provide a reliable approximation to intraday effective spreads.\n\n\n21.1.2 Construction\nWe compute the proportional bid-ask spread using end-of-day quote data:\n\\[\n\\text{BASPREAD}_{i,t} = \\frac{\\text{Ask}_{i,t} - \\text{Bid}_{i,t}}{\\text{Midpoint}_{i,t}}\n\\tag{21.1}\\]\nwhere \\(\\text{Midpoint}_{i,t} = (\\text{Ask}_{i,t} + \\text{Bid}_{i,t}) / 2\\). When end-of-day bid and ask are unavailable, we use the daily high-low range as a fallback. Following Chung and Zhang (2014), we delete observations where both Bid and Ask are zero, and where the spread exceeds 50% of the midpoint.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#sec-amihud",
    "href": "19_divop.html#sec-amihud",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "21.2 Amihud Illiquidity (ILLIQ)",
    "text": "21.2 Amihud Illiquidity (ILLIQ)\nThe Amihud (2002) ratio measures the price impact of order flow:\n\\[\n\\text{ILLIQ}_{i,t} = \\frac{|r_{i,t}|}{\\text{DolVol}_{i,t}}\n\\tag{21.2}\\]\nwhere \\(\\text{DolVol}_{i,t} = \\text{Volume}_{i,t} \\times \\text{Price}_{i,t}\\) (in billions VND for scaling). Higher ILLIQ reflects greater information asymmetry. We average daily ratios over monthly horizons and use the log transformation due to heavy right skew.\n\ndef compute_spread_and_illiq(df, config):\n    \"\"\"Compute bid-ask spread (BASPREAD) and Amihud illiquidity.\"\"\"\n    df = df.copy()\n    \n    # --- Bid-Ask Spread ---\n    df['midpoint_ba'] = (df['ask'] + df['bid']) / 2\n    df['baspread_ba'] = np.where(\n        (df['ask'] &gt; 0) & (df['bid'] &gt; 0) & (df['midpoint_ba'] &gt; 0),\n        (df['ask'] - df['bid']) / df['midpoint_ba'], np.nan\n    )\n    \n    # Fallback: high/low range\n    df['midpoint_hl'] = (df['high'] + df['low']) / 2\n    df['baspread_hl'] = np.where(\n        (df['high'] &gt; 0) & (df['low'] &gt; 0) & (df['midpoint_hl'] &gt; 0),\n        (df['high'] - df['low']) / df['midpoint_hl'], np.nan\n    )\n    \n    df['baspread'] = df['baspread_ba'].fillna(df['baspread_hl'])\n    df['midpoint'] = df['midpoint_ba'].fillna(df['midpoint_hl'])\n    \n    # Chung & Zhang (2009) filters\n    bad = (df['baspread'].isna()) | \\\n          (df['baspread'] &gt; config['max_spread_pct']) | \\\n          (df['baspread'] &lt; 0)\n    df.loc[bad, 'baspread'] = np.nan\n    \n    # --- Amihud Illiquidity ---\n    df['dollar_vol'] = df['volume'] * df['close'] / 1e9\n    df['amihud_daily'] = np.where(\n        df['dollar_vol'] &gt; 0,\n        np.abs(df['ret']) / df['dollar_vol'], np.nan\n    )\n    \n    print(f\"BASPREAD: {df['baspread'].notna().sum():,} valid obs, \"\n          f\"mean = {df['baspread'].mean():.6f}\")\n    print(f\"AMIHUD: {df['amihud_daily'].notna().sum():,} valid obs, \"\n          f\"mean = {df['amihud_daily'].mean():.6f}\")\n    return df\n\n\ndef compute_amihud_monthly(df):\n    \"\"\"Monthly Amihud = mean daily |ret|/dollar_vol (min 15 days).\"\"\"\n    df = df.copy()\n    df['ym'] = df['date'].dt.to_period('M')\n    agg = df.groupby(['ticker', 'ym']).agg(\n        illiq_mean=('amihud_daily', 'mean'),\n        n_days=('amihud_daily', 'count'),\n    ).reset_index()\n    agg = agg[agg['n_days'] &gt;= 15].copy()\n    agg['log_illiq'] = np.log(agg['illiq_mean'] + 1e-10)\n    return agg\n\n\n21.2.1 Vietnam-Specific Considerations for Spread and Liquidity\n\nTick size schedule. Vietnam uses variable tick sizes: 10 VND (prices &lt; 10,000), 50 VND (10,000–49,950), and 100 VND (≥ 50,000) on HOSE. These impose a floor on quoted spreads for low-priced stocks. Researchers should be cautious interpreting cross-price-decile spread variation as reflecting opinion divergence rather than tick-size mechanics.\nOrder-driven market structure. Both HOSE and HNX are pure order-driven markets where public limit orders provide liquidity. This makes the Chung and Zhang (2014) CRSP-based spread approximation appropriate.\nLot size requirements. HOSE requires 100-share standard lots for continuous trading. For high-priced stocks, the standard lot represents a large capital commitment, potentially inflating quoted spreads relative to effective trading costs.\nCall auction effects. Opening and closing sessions on HOSE use periodic call auctions, which can produce bid-ask quotes that differ substantially from continuous-trading spreads.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#theoretical-motivation-3",
    "href": "19_divop.html#theoretical-motivation-3",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "22.1 Theoretical Motivation",
    "text": "22.1 Theoretical Motivation\nAnalyst forecast dispersion, the cross-sectional standard deviation of individual analysts’ earnings forecasts, is the most direct measure of divergence of opinion. Unlike market-based proxies that capture disagreement indirectly, forecast dispersion directly measures disagreement among informed market participants. Abarbanell, Lanen, and Verrecchia (1995) establish the theoretical basis, and Diether, Malloy, and Scherbina (2002) demonstrate that stocks with higher analyst forecast dispersion earn lower subsequent returns, consistent with the Miller overpricing hypothesis.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#data-challenges-in-vietnam",
    "href": "19_divop.html#data-challenges-in-vietnam",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "22.2 Data Challenges in Vietnam",
    "text": "22.2 Data Challenges in Vietnam\nConstructing analyst forecast dispersion in Vietnam presents substantial challenges relative to the U.S.:\n\nCoverage breadth. While I/B/E/S covers over 4,000 U.S. companies, only 100–150 Vietnamese firms typically have coverage by at least 3 analysts, concentrated among VN30 constituents.\nData sources. Analyst forecasts are available from DataCore.vn, FiinPro, Bloomberg, and Refinitiv. The choice of source affects coverage and timeliness.\nForecast staleness. With limited coverage, forecasts may go unrevised for months. Following I/B/E/S methodology, we carry each forecast forward for a maximum of 105 days.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#construction-methodology-2",
    "href": "19_divop.html#construction-methodology-2",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "22.3 Construction Methodology",
    "text": "22.3 Construction Methodology\nThe construction proceeds as follows:\n\nClean individual forecasts. Remove observations where the announcement date precedes the review date. Keep only annual EPS forecasts. For each analyst-ticker-fiscal period, retain only the latest forecast per calendar month.\nHandle stopped and excluded estimates. Remove forecasts where the analyst has left the brokerage or the estimate has been excluded from consensus.\nCarry forward with staleness control. Each forecast is valid until the earlier of: (a) the next forecast by the same analyst, (b) 105 days after the announcement, or (c) the actual earnings announcement date.\nExpand to monthly frequency. For each ticker-month, identify all valid outstanding forecasts and compute dispersion.\nCompute scaled measures:\n\n\\[\n\\text{DISP1}_{i,m} = \\frac{\\text{Std}(\\hat{\\text{EPS}}_{i,m}^{(a)})}{|\\text{Mean}(\\hat{\\text{EPS}}_{i,m}^{(a)})|}\n\\qquad\n\\text{DISP2}_{i,m} = \\frac{\\text{Std}(\\hat{\\text{EPS}}_{i,m}^{(a)})}{\\bar{P}_{i,m}}\n\\]\n\ndef construct_analyst_dispersion(forecasts_df, price_df, config):\n    \"\"\"\n    Construct analyst forecast dispersion measures.\n    \n    Parameters\n    ----------\n    forecasts_df : pd.DataFrame\n        Individual analyst forecasts with: ticker, analyst_id, broker,\n        fpedats, anndats, revdats, value (EPS), anndats_act.\n    price_df : pd.DataFrame\n        Monthly price: ticker, month, mean_price.\n    config : dict\n        With min_analysts, forecast_carry_days.\n    \"\"\"\n    carry_days = config['forecast_carry_days']\n    min_analysts = config['min_analysts']\n    \n    df = forecasts_df.copy()\n    df = df[df['anndats'] &lt;= df['revdats']].copy()\n    df = df.dropna(subset=['fpedats', 'anndats', 'value'])\n    \n    # Latest forecast per analyst-month\n    df['ym'] = df['anndats'].dt.to_period('M')\n    df = df.sort_values(\n        ['ticker', 'fpedats', 'analyst_id', 'ym', 'anndats', 'revdats']\n    )\n    df = df.groupby(['ticker', 'fpedats', 'analyst_id', 'ym']).tail(1)\n    \n    # Carry-forward end date\n    df = df.sort_values(\n        ['ticker', 'analyst_id', 'fpedats', 'anndats'],\n        ascending=[True, True, True, False]\n    )\n    df['next_ann'] = df.groupby(\n        ['ticker', 'analyst_id', 'fpedats']\n    )['anndats'].shift(-1)\n    \n    def _carry_end(row):\n        candidates = [row['anndats'] + pd.Timedelta(days=carry_days)]\n        if pd.notna(row.get('next_ann')):\n            candidates.append(row['next_ann'])\n        if pd.notna(row.get('anndats_act')):\n            candidates.append(row['anndats_act'])\n        return min(candidates)\n    \n    df['carry_end'] = df.apply(_carry_end, axis=1)\n    \n    # Monthly expansion\n    months = pd.period_range(config['beg_date'], config['end_date'], freq='M')\n    records = []\n    for month in months:\n        me = month.to_timestamp(how='end')\n        valid = df[(df['anndats'] &lt;= me) & (df['carry_end'] &gt; me)].copy()\n        valid = valid[valid['fpedats'] &gt; me]\n        valid = valid.sort_values(['ticker', 'analyst_id', 'anndats'])\n        valid = valid.groupby(['ticker', 'analyst_id']).tail(1)\n        \n        disp = valid.groupby('ticker').agg(\n            n_analysts=('analyst_id', 'nunique'),\n            mean_fcst=('value', 'mean'),\n            std_fcst=('value', 'std'),\n        ).reset_index()\n        disp['month'] = month\n        records.append(disp)\n    \n    if not records:\n        return pd.DataFrame()\n    disp_df = pd.concat(records, ignore_index=True)\n    \n    # Scaled measures\n    disp_df['disp1'] = np.where(\n        disp_df['mean_fcst'].abs() &gt; 0,\n        disp_df['std_fcst'] / disp_df['mean_fcst'].abs(), np.nan\n    )\n    disp_df = disp_df.merge(price_df, on=['ticker', 'month'], how='left')\n    disp_df['disp2'] = np.where(\n        disp_df['mean_price'] &gt; 0,\n        disp_df['std_fcst'] / disp_df['mean_price'], np.nan\n    )\n    disp_df['disp_raw'] = disp_df['std_fcst']\n    \n    out = disp_df[disp_df['n_analysts'] &gt;= min_analysts].copy()\n    print(f\"DISP: {len(out):,} ticker-months (&gt;= {min_analysts} analysts)\")\n    print(f\"  Mean analysts: {out['n_analysts'].mean():.1f}\")\n    return out",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#scaling-considerations",
    "href": "19_divop.html#scaling-considerations",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "22.4 Scaling Considerations",
    "text": "22.4 Scaling Considerations\nFollowing Cheong and Thomas (2011), we note that each scaling choice has pitfalls. DISP1 (scaled by absolute mean forecast) can produce extreme values when the mean forecast approaches zero—common for Vietnamese firms near breakeven. DISP2 (scaled by price) introduces a mechanical negative correlation between price and scaled dispersion. We recommend reporting all three versions (DISP1, DISP2, and unscaled DISP_RAW with \\(\\ln(\\text{Price})\\) as an additional control), and winsorizing DISP1 at the 1st and 99th percentiles.\n\n\n\n\n\n\nWarningCaution on Analyst Dispersion in Thin-Coverage Markets\n\n\n\nWith typical coverage of 5–10 analysts per firm in Vietnam (versus 15–25 in the U.S.), forecast dispersion is estimated with substantially greater noise. A dispersion measure from 3 analysts has a very different sampling distribution than one from 20. Always include the number of analysts as a control and test robustness with varying minimum-analyst thresholds (3, 5, 7).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#summary-statistics",
    "href": "19_divop.html#summary-statistics",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "24.1 Summary Statistics",
    "text": "24.1 Summary Statistics\n\ndef descriptive_statistics(merged_df):\n    \"\"\"Comprehensive descriptive statistics for DIVOP proxies.\"\"\"\n    proxies = {\n        'dto': 'Unexplained Volume (DTO)',\n        'suv': 'Std Unexplained Volume (SUV)',\n        'total_volatility': 'Total Return Volatility',\n        'idio_volatility': 'Idiosyncratic Volatility',\n        'baspread': 'Bid-Ask Spread',\n        'amihud_daily': 'Amihud Illiquidity',\n        'disp1': 'Analyst Disp (mean-scaled)',\n        'disp2': 'Analyst Disp (price-scaled)',\n    }\n    avail = {k: v for k, v in proxies.items() if k in merged_df.columns}\n    rows = []\n    for col, label in avail.items():\n        s = merged_df[col].dropna()\n        rows.append({\n            'Proxy': label, 'N': f'{len(s):,}',\n            'Mean': f'{s.mean():.6f}', 'Std': f'{s.std():.6f}',\n            'P5': f'{s.quantile(.05):.6f}',\n            'Median': f'{s.median():.6f}',\n            'P95': f'{s.quantile(.95):.6f}',\n            'Skew': f'{s.skew():.2f}',\n            'Kurt': f'{s.kurtosis():.2f}',\n        })\n    stats = pd.DataFrame(rows).set_index('Proxy')\n    print(\"\\n\" + \"=\" * 90)\n    print(\"Descriptive Statistics of DIVOP Proxies\")\n    print(\"Vietnamese Equity Market, HOSE and HNX\")\n    print(\"=\" * 90)\n    print(stats.to_string())\n    return stats",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#divop-by-firm-characteristics",
    "href": "19_divop.html#divop-by-firm-characteristics",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "24.2 DIVOP by Firm Characteristics",
    "text": "24.2 DIVOP by Firm Characteristics\n\ndef divop_by_size(merged_df):\n    \"\"\"Mean DIVOP proxies by market-cap quintile.\"\"\"\n    df = merged_df.copy()\n    df['mkt_cap'] = df['close'] * df['shares_outstanding']\n    df['size_q'] = df.groupby('date')['mkt_cap'].transform(\n        lambda x: pd.qcut(x, 5,\n            labels=['Q1 Small','Q2','Q3','Q4','Q5 Large'],\n            duplicates='drop')\n    )\n    proxies = ['dto','suv','total_volatility','idio_volatility',\n               'baspread','amihud_daily']\n    avail = [p for p in proxies if p in df.columns]\n    tab = df.groupby('size_q')[avail].mean()\n    print(\"\\n=== Mean DIVOP by Size Quintile ===\")\n    print(tab.to_string(float_format='{:.6f}'.format))\n    return tab\n\ndef divop_by_exchange(merged_df):\n    \"\"\"Compare mean DIVOP across HOSE and HNX.\"\"\"\n    proxies = ['dto','suv','total_volatility','idio_volatility',\n               'baspread','amihud_daily']\n    avail = [p for p in proxies if p in merged_df.columns]\n    tab = merged_df.groupby('exchange')[avail].mean()\n    print(\"\\n=== Mean DIVOP by Exchange ===\")\n    print(tab.to_string(float_format='{:.6f}'.format))\n    return tab",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#time-series-evolution",
    "href": "19_divop.html#time-series-evolution",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "24.3 Time-Series Evolution",
    "text": "24.3 Time-Series Evolution\n\ndef plot_divop_timeseries(merged_df):\n    \"\"\"Plot monthly cross-sectional median DIVOP with crisis shading.\"\"\"\n    df = merged_df.copy()\n    df['ym'] = df['date'].dt.to_period('M')\n    proxies = ['dto','suv','total_volatility','baspread']\n    avail = [p for p in proxies if p in df.columns]\n    monthly = df.groupby('ym')[avail].median()\n    monthly.index = monthly.index.to_timestamp()\n    \n    fig, axes = plt.subplots(len(avail), 1,\n        figsize=(13, 3.5*len(avail)), sharex=True)\n    if len(avail) == 1: axes = [axes]\n    \n    labels = {'dto':'DTO','suv':'SUV',\n              'total_volatility':'Volatility','baspread':'Spread'}\n    colors = ['#1976D2','#388E3C','#F57C00','#D32F2F']\n    \n    for i, (proxy, ax) in enumerate(zip(avail, axes)):\n        ax.plot(monthly.index, monthly[proxy],\n                color=colors[i], linewidth=1.3)\n        ax.set_ylabel(labels.get(proxy, proxy), fontsize=10)\n        ax.grid(True, alpha=0.25)\n        for s, e, c in [('2008-01','2009-06','red'),\n                         ('2020-01','2020-12','orange'),\n                         ('2022-09','2023-06','purple')]:\n            ax.axvspan(pd.Timestamp(s), pd.Timestamp(e),\n                        alpha=0.1, color=c)\n    \n    axes[0].set_title(\n        'Time-Series of DIVOP Proxies\\n'\n        'Monthly Cross-Sectional Median, HOSE & HNX',\n        fontsize=13, fontweight='bold')\n    from matplotlib.patches import Patch\n    axes[-1].legend(handles=[\n        Patch(facecolor='red', alpha=.2, label='GFC 2008-09'),\n        Patch(facecolor='orange', alpha=.2, label='COVID-19'),\n        Patch(facecolor='purple', alpha=.2, label='Bond Crisis 2022-23'),\n    ], loc='upper right', fontsize=8)\n    plt.tight_layout()\n    plt.savefig('divop_timeseries.png', dpi=300, bbox_inches='tight')\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#application-1-divop-and-the-cross-section-of-returns",
    "href": "19_divop.html#application-1-divop-and-the-cross-section-of-returns",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "26.1 Application 1: DIVOP and the Cross-Section of Returns",
    "text": "26.1 Application 1: DIVOP and the Cross-Section of Returns\nThe fundamental test of the Miller hypothesis is whether stocks with higher divergence of opinion earn lower subsequent returns. We implement Fama-MacBeth cross-sectional regressions:\n\\[\nr_{i,t+1:t+h} = \\gamma_{0,t} + \\gamma_{1,t} \\cdot \\text{DIVOP}_{i,t} + \\gamma_{2,t}' \\mathbf{X}_{i,t} + \\varepsilon_{i,t}\n\\]\nwhere \\(\\mathbf{X}_{i,t}\\) includes controls for market beta, log market capitalization, and log book-to-market ratio. The Miller hypothesis predicts \\(\\bar{\\gamma}_1 &lt; 0\\).\n\ndef fama_macbeth_divop(merged_df, divop_proxy='suv',\n                        controls=None, horizon=21):\n    \"\"\"\n    Fama-MacBeth cross-sectional regressions.\n    Miller predicts gamma_1 &lt; 0; Varian predicts gamma_1 &gt; 0.\n    \"\"\"\n    if controls is None:\n        controls = ['market_beta', 'log_mktcap']\n    \n    df = merged_df.copy()\n    df = df.sort_values(['ticker', 'date'])\n    df['fwd_ret'] = df.groupby('ticker')['ret'].transform(\n        lambda x: x.shift(-1).rolling(horizon).sum().shift(-(horizon-1))\n    )\n    df['log_mktcap'] = np.log(\n        df['close'] * df['shares_outstanding'] + 1\n    )\n    \n    reg_vars = ['fwd_ret', divop_proxy] + \\\n               [c for c in controls if c in df.columns]\n    df_reg = df[['ticker','date'] + reg_vars].dropna()\n    \n    from numpy.linalg import lstsq\n    results = []\n    for date, cross in df_reg.groupby('date'):\n        if len(cross) &lt; 30: continue\n        y = cross['fwd_ret'].values\n        X_cols = [divop_proxy] + [c for c in controls if c in cross.columns]\n        X = np.column_stack([np.ones(len(cross)), cross[X_cols].values])\n        try:\n            coefs, _, _, _ = lstsq(X, y, rcond=None)\n            results.append({\n                'date': date, 'intercept': coefs[0],\n                f'gamma_{divop_proxy}': coefs[1], 'n': len(cross),\n            })\n        except Exception: continue\n    \n    fm = pd.DataFrame(results)\n    gc = f'gamma_{divop_proxy}'\n    mu = fm[gc].mean()\n    se = fm[gc].std() / np.sqrt(len(fm))\n    t = mu / se\n    \n    print(f\"\\n=== Fama-MacBeth: {divop_proxy} -&gt; \"\n          f\"{horizon}-day fwd returns ===\")\n    print(f\"  Mean gamma: {mu:.6f}, t-stat: {t:.3f}\")\n    if t &lt; -1.96:   print(\"  -&gt; Supports Miller (1977)\")\n    elif t &gt; 1.96:   print(\"  -&gt; Supports Varian (1985)\")\n    else:            print(\"  -&gt; Inconclusive at 5%\")\n    return fm",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#application-2-divop-and-earnings-announcements",
    "href": "19_divop.html#application-2-divop-and-earnings-announcements",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "26.2 Application 2: DIVOP and Earnings Announcements",
    "text": "26.2 Application 2: DIVOP and Earnings Announcements\nFollowing Berkman et al. (2009), we test whether high-DIVOP stocks experience negative abnormal returns around earnings announcements, as uncertainty resolution reduces the optimism premium.\n\ndef divop_earnings_event(merged_df, ea_dates_df,\n                          divop_proxy='suv', window=(-1, 3)):\n    \"\"\"\n    Sort stocks into DIVOP quintiles pre-EA, compute CAR in window.\n    Miller predicts: Q5 (high DIVOP) has lower CAR than Q1 (low DIVOP).\n    \"\"\"\n    df = merged_df.copy()\n    ea = ea_dates_df.copy()\n    \n    # Pre-EA DIVOP value (5 days before)\n    ea['pre_date'] = ea['ea_date'] - pd.Timedelta(days=5)\n    ea = ea.merge(\n        df[['ticker','date',divop_proxy]].rename(\n            columns={'date':'pre_date'}),\n        on=['ticker','pre_date'], how='inner'\n    )\n    ea['divop_q'] = pd.qcut(\n        ea[divop_proxy], 5,\n        labels=['Q1 Low','Q2','Q3','Q4','Q5 High'],\n        duplicates='drop'\n    )\n    \n    print(f\"\\n=== EA Event Study by {divop_proxy} quintile ===\")\n    print(f\"  Window: ({window[0]}, {window[1]}) days\")\n    print(f\"  Miller predicts: Q5 has lower CAR than Q1\")\n    return ea",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "19_divop.html#application-3-composite-divop-index-via-pca",
    "href": "19_divop.html#application-3-composite-divop-index-via-pca",
    "title": "16  Measuring Divergence of Investor Opinion",
    "section": "26.3 Application 3: Composite DIVOP Index via PCA",
    "text": "26.3 Application 3: Composite DIVOP Index via PCA\nWhen a single summary measure of disagreement is needed, PCA on the battery of standardized proxies extracts the common “disagreement factor.”\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndef composite_divop_pca(merged_df, proxies=None):\n    \"\"\"Extract first principal component from standardized DIVOP proxies.\"\"\"\n    if proxies is None:\n        proxies = ['dto','suv','total_volatility','idio_volatility',\n                   'baspread','amihud_daily']\n    avail = [p for p in proxies if p in merged_df.columns]\n    data = merged_df[['ticker','date'] + avail].dropna()\n    \n    scaler = StandardScaler()\n    X = scaler.fit_transform(data[avail])\n    \n    pca = PCA(n_components=3)\n    factors = pca.fit_transform(X)\n    data['divop_composite'] = factors[:, 0]\n    \n    # Ensure positive correlation with inputs\n    for col in avail:\n        if data['divop_composite'].corr(data[col]) &lt; 0:\n            data['divop_composite'] *= -1\n            break\n    \n    loadings = pd.DataFrame(\n        pca.components_.T, index=avail,\n        columns=['PC1','PC2','PC3']\n    )\n    \n    print(f\"\\n=== PCA Composite DIVOP ===\")\n    print(f\"Variance explained: \"\n          f\"{pca.explained_variance_ratio_[:3].round(3)}\")\n    print(f\"\\nLoadings:\\n{loadings.to_string(float_format='{:.4f}'.format)}\")\n    return data[['ticker','date','divop_composite']], loadings",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "25_pe_ratio.html",
    "href": "25_pe_ratio.html",
    "title": "17  P/E Ratio",
    "section": "",
    "text": "17.0.1 Technical Schema and Variable Engineering\nThe systematic analysis of equity valuations in frontier and emerging markets requires a robust integration of high-fidelity financial data, rigorous mathematical modeling, and an acute understanding of local regulatory frameworks. In the context of the Vietnamese equity market, the Price-Earnings (P/E) ratio serves as a primary instrument for assessing corporate performance and market sentiment. This chapter provides an exploration of P/E valuation methodologies, ranging from firm-specific trailing and forward metrics to cyclically adjusted and unlevered variations.\nThe primary identifier for equities is the symbol, typically a three-letter uppercase code, though special indices such as E1VFVN30 (ETF) or market indices like HNX-INDEX and UPCOM-INDEX follow unique naming conventions. Price data is categorized as OHLC type, encompassing ‘open’, ‘high’, ‘low’, and ‘close’. For valuation purposes, the ‘adjusted close’ is preferred to account for corporate actions such as stock splits and dividend payments.\nThe accounting variables provided in the Vietnam Fundamentals product follow a standardized classification that reconciles different reporting standards across sectors. This reconciliation is vital because Vietnamese firms may have differing interpretations of revenue and net profit under Vietnamese Accounting Standards (VAS).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "25_pe_ratio.html#firm-specific-pe-valuation-metrics",
    "href": "25_pe_ratio.html#firm-specific-pe-valuation-metrics",
    "title": "17  P/E Ratio",
    "section": "17.1 Firm-Specific P/E Valuation Metrics",
    "text": "17.1 Firm-Specific P/E Valuation Metrics\nAt the individual firm level, the P/E ratio is the most fundamental measure of how much an investor is willing to pay for each unit of current or future profit. However, the calculation of “E” (Earnings) can take several forms, each offering a different insight into the company’s value.\n\n17.1.1 Trailing P/E and the TTM Methodology\nThe Trailing P/E ratio is calculated using the reported earnings from the last four quarters, known as Trailing Twelve Months (TTM). In the Vietnamese market, this is often the default metric reported by exchanges and financial news outlets.\nThe mathematical representation is:\n\\[\nP/E_{Trailing} = \\frac{P_t}{\\sum_{i=0}^{3} EPS_{q-i}}\n\\]\nWhere \\(P_t\\) is the current market price and \\(EPS_{q-i}\\) represents the earnings per share for each of the four most recent quarters.\n\n\n17.1.2 Forward P/E and Analyst Forecast Reliability\nForward P/E shifts the focus from historical performance to future expectations by using consensus earnings forecasts for the next fiscal year.\n\\[\nP/E_{Forward} = \\frac{P_t}{EPS_{forecasted, t+1}}\n\\]\nIn Vietnam, these forecasts are typically generated by research departments within major securities firms such as SSI, VCSC, MBS, and FPTS. These analysts rely on financial statements, projections, models, and subjective evaluations of market sentiment to generate their estimates. However, the accuracy of these forecasts is a significant area of research. However, analysts in Vietnam are often influenced by anchoring and adjustment bias, where they base future predictions heavily on past records and then make insufficient adjustments.\nThe accuracy of analyst earnings forecasts in Vietnam is significantly impacted by corporate governance characteristics. Specifically, state ownership has been found to have a negative impact on forecast accuracy, while institutional ownership has a positive effect. This implies that for firms with high state-controlled stakes, which are common in Vietnam’s strategic industries, researchers should apply a higher discount or “fudge factor” to forward P/E estimates provided by consensus sources.\n\n\n17.1.3 Cyclically Adjusted Price-Earnings (CAPE) Ratio\nThe Shiller P/E, or CAPE ratio, is designed to smooth out the volatility of the business cycle by using a 10-year average of inflation-adjusted earnings. For a market like Vietnam, which has seen periods of extreme growth followed by stabilization, the CAPE ratio provides a more tempered view of valuation than trailing metrics.\nThe calculation requires adjusting historical earnings by the Consumer Price Index (CPI). Vietnam’s CPI data is updated monthly and is available from January 1996 through early 2026, with an average year-on-year growth rate of approximately 12.2%.\n\\[\nEPS_{real, t} = EPS_{nominal, t} \\times \\frac{CPI_{current}}{CPI_t}\n\\]\n\\[\nCAPE = \\frac{P_t}{\\frac{1}{10} \\sum_{i=0}^{9} EPS_{real, t-i}}\n\\]\nHistorically, Vietnam’s CPI has reached an all-time high of 28.3% YoY in August 2008 and a record low of -2.6% in July 2000. These extreme swings mean that nominal earnings in the mid-2000s are not comparable to earnings today without the Shiller adjustment. Researchers must be mindful of the different base years used by the National Statistics Office (GSO), such as 2024=100, 2019=100, and 2014=100.\n\n\n17.1.4 Unlevered P/E and the Leibowitz Framework\nThe concept of the “Unlevered P/E” ratio attempts to view a company’s valuation independent of its capital structure. This is particularly relevant in the Vietnamese market, where debt loads vary significantly between state-owned enterprises (SOEs) and private firms. Research by Leibowitz (2002) highlights that for the investment analyst, a company is already levered, and the task is to estimate its theoretical value by inferring the underlying structure of returns.\nAccording to Leibowitz, leverage always moves the P/E toward a lower value than what would be obtained from a standard Gordon growth formula. As a company adds debt, the market discount rate for equity increases to compensate for the additional risk, which in turn compresses the P/E multiple. For example, a debt-free company with a theoretical P/E of 30 might see its P/E drop to 23 with a 40% debt ratio, and down to 20 with a 50% debt ratio.\nThe Unlevered P/E (often proxied by Enterprise Value to EBIT or EBITDA) can be calculated using fundamental variables:\n\\[\nEV = (\\text{Shares Outstanding} \\times \\text{Price}) + \\text{Total Debt} - \\text{Cash}\n\\]\n\\[\nP/E_{\\text{Unlevered}} \\approx \\frac{EV}{EBIT \\times (1 - \\tau)}\n\\]\nWhere \\(\\tau\\) is the corporate income tax rate. The standard CIT rate in Vietnam is 20%, which has been stable since the mid-2010s but is subject to new revisions under the Law on CIT ratified in June 2025.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "25_pe_ratio.html#market-level-aggregation-techniques",
    "href": "25_pe_ratio.html#market-level-aggregation-techniques",
    "title": "17  P/E Ratio",
    "section": "17.2 Market-Level Aggregation Techniques",
    "text": "17.2 Market-Level Aggregation Techniques\nAggregating firm-specific metrics into a single market-level index P/E is a complex task that requires careful consideration of weighting and the inclusion of loss-making entities. In the Vietnamese market, two primary indices (i.e., the VN-Index and the VN30), provide the benchmarks for performance and valuation.\n\n17.2.1 The VN-Index: Capitalization-Weighted Aggregation\nThe VN-Index is a capitalization-weighted index of all companies listed on HOSE. It serves as a broad indicator of market health, where an increase in the index reflects a general rise in the stock prices of listed companies.\nThe index value is calculated as:\n\\[\n\\text{VN-Index} = \\frac{\\sum (Price_i \\times Shares_i)}{\\text{Base Factor}} \\times \\text{Adjustment Factor}\n\\]\nSimilarly, the index-level P/E is typically calculated as the sum of all market capitalizations divided by the sum of all net incomes (earnings) of the constituents.\n\\[\nP/E_{Market} = \\frac{\\sum MarketCap_i}{\\sum Earnings_i}\n\\]\nAs of early 2026, the total market capitalization for 701 tracked companies was approximately 8,629.8 trillion VND, with total earnings of 588.9 trillion VND, yielding a median P/E of approximately 14.7x.\n\n\n17.2.2 The VN30 and Free-Float Adjustments\nThe VN30 Index represents a basket of the 30 largest and most liquid stocks on HOSE, screened through layers of capitalization, free-float ratio, and transactional volume. Unlike the broad VN-Index, the VN30 applies a free-float adjustment to its capitalization weighting to ensure that only shares available for public trading affect the index movement.\nThe free-float ratio (\\(f\\)) is calculated as:\n\\(f = \\frac{N - N_l}{N}\\)\nWhere \\(N\\) is the total number of outstanding shares and \\(N_l\\) is the number of limited trading shares (held by founders, state, or strategic partners).\nWhen aggregating P/E for the VN30, researchers must account for this adjustment, as it more accurately reflects the valuation of the investable universe. In the Vietnamese market, a handful of large-cap stocks can significantly skew the broad index. For instance, Vingroup (VIC) and its related companies have at times represented nearly a quarter of the VN-Index’s total weighting, creating a “closet indexing” dilemma for fund managers. Removing these high-impact stocks can reveal a much different valuation profile; while the VN-Index might trade at 13x, the market excluding VIC-related stocks could be closer to 11x.\n\n\n17.2.3 Median vs. Mean Aggregation\nQuantitative researchers often prefer median P/E ratios over mean P/E ratios to mitigate the influence of extreme outliers (i.e., companies with either exceptionally high P/E ratios due to temporarily low earnings or those that are technically “expensive” because of high growth expectations). Table 17.1 shows different aggregation methods.\n\n\n\nTable 17.1: Median vs. Mean Aggregation\n\n\n\n\n\n\n\n\n\n\nAggregation Method\nMathematical Definition\nResilience to Outliers\n\n\nSimple Mean\n\\(\\frac{1}{n} \\sum (P/E)_i\\)\nLow (highly skewed by extreme values)\n\n\nWeighted Mean\n\\(\\frac{\\sum (Cap_i \\times (P/E)_i)}{\\sum Cap_i}\\)\nMedium (skewed by large-cap valuation)\n\n\nMedian\n\\(\\text{Median}((P/E)_1, \\dots, (P/E)_n)\\)\nHigh (most robust for market sentiment)\n\n\nTotal-to-Total\n\\(\\frac{\\sum MarketCap_i}{\\sum Earnings_i}\\)\nHigh (standard for index providers)\n\n\n\n\n\n\nThe Vietnamese market currently trades near its 3-year average P/E of 14.8x, suggesting that investors are relatively neutral on current valuations, expecting earnings to grow in line with historical rates.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "25_pe_ratio.html#implementation",
    "href": "25_pe_ratio.html#implementation",
    "title": "17  P/E Ratio",
    "section": "17.3 Implementation",
    "text": "17.3 Implementation\n\n17.3.1 Data Ingestion and Processing\n\n\n17.3.2 Shiller CAPE\nThe Shiller CAPE requires a more sophisticated approach, involving the merging of earnings data with monthly CPI series.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "25_pe_ratio.html#macroeconomic-and-regulatory-factors-in-valuation",
    "href": "25_pe_ratio.html#macroeconomic-and-regulatory-factors-in-valuation",
    "title": "17  P/E Ratio",
    "section": "17.4 Macroeconomic and Regulatory Factors in Valuation",
    "text": "17.4 Macroeconomic and Regulatory Factors in Valuation\nThe P/E ratio does not exist in a vacuum; it is deeply influenced by the macroeconomic environment and the regulatory framework within which companies operate.\n\n17.4.1 The Role of Corporate Income Tax (CIT)\nThe earnings component of the P/E ratio is a post-tax metric. In Vietnam, the standard CIT rate is 20%. However, there is a significant shift occurring with the ratification of the new Law on CIT in June 2025, taking effect on October 1, 2025. This law introduces:\n\nA 15% rate for enterprises with annual revenue not exceeding 3 billion VND.\nA 17% rate for enterprises with revenue between 3 and 50 billion VND.\nThe maintenance of higher rates (up to 50%) for the oil, gas, and mineral resource sectors.\n\nFurthermore, Vietnam has adopted the Global Minimum Tax (GMT) rules under Pillar Two, effective January 1, 2024, to protect its tax revenue from inbound and outbound investments. These changes mean that historical P/E ratios may not be directly comparable to future ratios for companies that previously benefited from generous tax incentives, as the effective tax rate is likely to rise for large multinational subsidiaries operating in Vietnam.\n\n\n17.4.2 Macroeconomic Determinants: Inflation and Interest Rates\nStock price indices, and by extension P/E ratios, are heavily influenced by fundamental macroeconomic factors such as inflation, exchange rates, and interest rates. The interest rate serves as the cost of capital for enterprises. When interest rates fall, the cost of borrowing drops, increasing corporate profits and potentially raising the P/E that investors are willing to pay.\nThe Vietnamese dong (VND) exchange rate also plays a crucial role. In early 2025, the VND depreciated by 1.33% YTD, reflecting volatility in the timeline for Federal Reserve rate cuts. Exchange rate depreciation can be inflationary by default, which may prompt the State Bank of Vietnam to tighten monetary policy, thereby putting downward pressure on equity valuations.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "25_pe_ratio.html#synthesis-of-valuation-dynamics",
    "href": "25_pe_ratio.html#synthesis-of-valuation-dynamics",
    "title": "17  P/E Ratio",
    "section": "17.5 Synthesis of Valuation Dynamics",
    "text": "17.5 Synthesis of Valuation Dynamics\nThe Vietnamese equity market is currently in a state of transition. While the market has seen robust growth—the VN-Index rose 38% in the year leading up to early 2026—investors remain neutral on overall valuations, as reflected by the market’s alignment with its 3-year average P/E of 14.8x. This neutrality suggests that investors expect earnings growth (forecast at 14% annually) to keep pace with price appreciation.\nThe reliability of these valuations, however, remains dependent on the quality of non-financial disclosure. While companies are increasingly required by law to provide non-financial information, the level of transparency remains at a medium level (approximately 58.5% of required disclosures). Researchers must therefore balance quantitative P/E analysis with qualitative assessments of corporate governance and sustainability strategies. Companies that effectively integrate ESG principles have been shown to experience fewer negative impacts on abnormal stock returns during uncertain periods, such as the COVID-19 pandemic.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "25_pe_ratio.html#conclusions",
    "href": "25_pe_ratio.html#conclusions",
    "title": "17  P/E Ratio",
    "section": "17.6 Conclusions",
    "text": "17.6 Conclusions\nThe exploration of P/E ratio valuation and aggregation within the Vietnamese equity market reveals a sophisticated landscape where traditional metrics must be adapted to local realities. The reliance on trailing metrics, while common, fails to account for the cyclicality and high inflation history of the Vietnamese economy, a gap that the Shiller CAPE ratio effectively bridges through CPI-adjusted normalization. Furthermore, the sensitivity of P/E to capital structure, as defined in the Leibowitz framework, highlights the necessity of unlevered valuation metrics in a market with diverse corporate funding models.\nThe aggregation of these metrics into market-level indices like the VN-Index and VN30 requires an awareness of concentration risks and the importance of free-float adjustments. The “Vingroup Effect” demonstrates how a single corporate ecosystem can skew index-wide valuations, necessitating a “closet indexing” awareness for fund managers and researchers.\nLooking forward, the evolution of the regulatory environment—specifically the CIT law of 2025 and the Global Minimum Tax—will introduce new variables into the valuation equation. The shift toward higher-quality non-financial disclosures and the increasing accuracy of analyst forecasts in institutionalized firms suggest that the Vietnamese equity market is steadily maturing, offering a more transparent and predictable environment for the application of advanced quantitative valuation techniques.\n\n\n\n\n\n\nLeibowitz, Martin L. 2002. “The Levered p/e Ratio.” Financial Analysts Journal 58 (6): 68–77.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html",
    "href": "30_institutional_ownership.html",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "",
    "text": "18.1 Institutional Ownership in Vietnam: A Distinct Landscape\nVietnam’s equity market presents a fundamentally different institutional ownership landscape from the mature markets of the US, Europe, or Japan. Since the Ho Chi Minh City Securities Trading Center (now HOSE) opened on July 28, 2000 with just two listed stocks, the market has grown to over 1,700 listed companies across three exchanges (HOSE, HNX, and UPCOM) with a combined market capitalization exceeding 200 billion USD. Yet the ownership structure remains distinctive in several critical ways:",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#institutional-ownership-in-vietnam-a-distinct-landscape",
    "href": "30_institutional_ownership.html#institutional-ownership-in-vietnam-a-distinct-landscape",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "",
    "text": "Retail dominance. Individual investors account for approximately 85% of trading value on Vietnamese exchanges, far exceeding the institutional share. This contrasts sharply with the US, where institutional investors dominate both ownership and trading (Bao Dinh and Tran 2024). The implications for market efficiency, price discovery, and volatility are profound.\nState ownership legacy. Vietnam’s equitization (privatization) program, initiated under Đổi Mới reforms in 1986, means that the state remains a significant or controlling shareholder in many listed companies. As of 2022, SOEs (firms with state ownership &gt; 50%) account for approximately 30% of total market capitalization despite representing less than 10% of listed firms (Huang, Liu, and Shu 2023). State ownership introduces unique agency problems, governance dynamics, and liquidity constraints.\nForeign Ownership Limits (FOLs). Vietnam imposes sector-specific caps on aggregate foreign ownership, typically 49% for most sectors, 30% for banking, and varying limits for aviation, media, and telecommunications. When a stock reaches its FOL, foreign investors can only buy from other foreign sellers, creating a segmented market with distinct pricing dynamics and a well-documented “FOL premium” (Vo 2015).\nDisclosure regime. Unlike the US quarterly 13F filing system, Vietnam’s ownership disclosure is event-driven and periodic. Major shareholders (≥5%) must disclose within 7 business days of crossing thresholds. Annual reports contain detailed shareholder registers. Semi-annual fund reports provide portfolio snapshots. This creates a patchwork of disclosure frequencies that require careful handling.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-datacore",
    "href": "30_institutional_ownership.html#sec-datacore",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.2 Data Infrastructure: DataCore.vn",
    "text": "18.2 Data Infrastructure: DataCore.vn\nDataCore.vn is a comprehensive Vietnamese financial data platform that provides academic-grade datasets for the Vietnamese market. Throughout this chapter, we assume all data is sourced exclusively from DataCore.vn, which provides:\n\n\n\nTable 18.1: DataCore.vn Data Tables Used in This Chapter\n\n\n\n\n\n\n\n\n\n\nDataCore.vn Dataset\nContent\nKey Variables\n\n\n\n\nStock Prices\nDaily/monthly OHLCV for HOSE, HNX, UPCOM\nticker, date, close, adjusted_close, volume, shares_outstanding\n\n\nOwnership Structure\nShareholder composition snapshots\nticker, date, shareholder_name, shares_held, ownership_pct, shareholder_type\n\n\nMajor Shareholders\nDetailed ≥5% holders\nticker, date, shareholder_name, shares_held, is_foreign, is_state, is_institution\n\n\nCorporate Actions\nDividends, stock splits, bonus shares, rights issues\nticker, ex_date, action_type, ratio, record_date\n\n\nCompany Profile\nSector, exchange, listing date, charter capital\nticker, exchange, industry_code, listing_date, fol_limit\n\n\nFinancial Statements\nQuarterly/annual financials\nticker, period, revenue, net_income, total_assets, equity\n\n\nForeign Ownership\nDaily foreign ownership tracking\nticker, date, foreign_shares, foreign_pct, fol_limit, foreign_room\n\n\nFund Holdings\nSemi-annual fund portfolio disclosures\nfund_name, report_date, ticker, shares_held, market_value\n\n\n\n\n\n\n\nclass DataCoreReader:\n    \"\"\"\n    Unified data reader for DataCore.vn datasets.\n    \n    Assumes data has been downloaded from DataCore.vn and stored locally.\n    Supports both Parquet (recommended for performance) and CSV formats.\n    \n    Parameters\n    ----------\n    data_dir : str or Path\n        Root directory containing DataCore.vn data files\n    file_format : str\n        'parquet' or 'csv' (default: 'parquet')\n    \"\"\"\n    \n    # Expected file names in the data directory\n    FILE_MAP = {\n        'prices': 'stock_prices',\n        'ownership': 'ownership_structure',\n        'major_shareholders': 'major_shareholders',\n        'corporate_actions': 'corporate_actions',\n        'company_profile': 'company_profile',\n        'financials': 'financial_statements',\n        'foreign_ownership': 'foreign_ownership_daily',\n        'fund_holdings': 'fund_holdings',\n    }\n    \n    def __init__(self, data_dir: Union[str, Path], file_format: str = 'parquet'):\n        self.data_dir = Path(data_dir)\n        self.fmt = file_format\n        self._cache = {}\n        \n        # Verify data directory exists\n        if not self.data_dir.exists():\n            raise FileNotFoundError(\n                f\"Data directory not found: {self.data_dir}\\n\"\n                f\"Please download data from DataCore.vn and place it in this directory.\"\n            )\n        \n        print(f\"DataCore.vn reader initialized: {self.data_dir}\")\n        available = [f.stem for f in self.data_dir.glob(f'*.{self.fmt}')]\n        print(f\"Available datasets: {available}\")\n    \n    def _read(self, key: str) -&gt; pd.DataFrame:\n        \"\"\"Read and cache a dataset.\"\"\"\n        if key in self._cache:\n            return self._cache[key]\n        \n        fname = self.FILE_MAP.get(key, key)\n        filepath = self.data_dir / f\"{fname}.{self.fmt}\"\n        \n        if not filepath.exists():\n            raise FileNotFoundError(\n                f\"Dataset not found: {filepath}\\n\"\n                f\"Expected file: {fname}.{self.fmt} in {self.data_dir}\"\n            )\n        \n        if self.fmt == 'parquet':\n            df = pd.read_parquet(filepath)\n        else:\n            df = pd.read_csv(filepath, parse_dates=True)\n        \n        # Auto-detect and parse date columns\n        for col in df.columns:\n            if 'date' in col.lower() or col.lower() in ['period', 'ex_date', 'record_date']:\n                try:\n                    df[col] = pd.to_datetime(df[col])\n                except (ValueError, TypeError):\n                    pass\n        \n        self._cache[key] = df\n        print(f\"Loaded {key}: {len(df):,} rows, {len(df.columns)} columns\")\n        return df\n    \n    @property\n    def prices(self) -&gt; pd.DataFrame:\n        return self._read('prices')\n    \n    @property\n    def ownership(self) -&gt; pd.DataFrame:\n        return self._read('ownership')\n    \n    @property\n    def major_shareholders(self) -&gt; pd.DataFrame:\n        return self._read('major_shareholders')\n    \n    @property\n    def corporate_actions(self) -&gt; pd.DataFrame:\n        return self._read('corporate_actions')\n    \n    @property\n    def company_profile(self) -&gt; pd.DataFrame:\n        return self._read('company_profile')\n    \n    @property\n    def financials(self) -&gt; pd.DataFrame:\n        return self._read('financials')\n    \n    @property\n    def foreign_ownership(self) -&gt; pd.DataFrame:\n        return self._read('foreign_ownership')\n    \n    @property\n    def fund_holdings(self) -&gt; pd.DataFrame:\n        return self._read('fund_holdings')\n    \n    def clear_cache(self):\n        \"\"\"Clear all cached datasets to free memory.\"\"\"\n        self._cache.clear()\n\n# Initialize reader — adjust path to your local DataCore.vn data\n# dc = DataCoreReader('/path/to/datacore_data', file_format='parquet')\n\nThis chapter proceeds as follows. Section 18.3 builds the complete data pipeline from raw DataCore.vn extracts to clean, analysis-ready datasets, with particular attention to corporate action adjustments. Section 18.4 defines Vietnam’s unique ownership taxonomy. Section 18.5 computes institutional ownership ratios, concentration, and breadth for the Vietnamese market. Section 18.6 develops specialized foreign ownership analytics including FOL utilization and room premium. Section 18.7 derives institutional trades from ownership disclosure snapshots. Section 18.8 computes fund-level flows and turnover. Section 18.9 analyzes state ownership dynamics. Section 18.10 introduces network analysis, ML classification, and event-study frameworks. Section 18.11 presents complete empirical applications, and Section 18.12 concludes.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-data-pipeline",
    "href": "30_institutional_ownership.html#sec-data-pipeline",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.3 Data Pipeline",
    "text": "18.3 Data Pipeline\n\n18.3.1 Stock Price Data and Corporate Action Adjustments\nVietnam’s equity market is notorious for frequent corporate actions, particularly stock dividends and bonus share issuances, that dramatically alter share counts. A company issuing a 30% stock dividend means every 100 shares become 130 shares, and the reference price adjusts downward proportionally. Failure to properly adjust historical shares and prices for these events is the single most common source of error in Vietnamese equity research.\n\n# ============================================================================\n# Step 1: Corporate Action Adjustment Factors\n# ============================================================================\n\ndef build_adjustment_factors(corporate_actions: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Build cumulative adjustment factors from the corporate actions history.\n    \n    In Vietnam, the most common share-altering corporate actions are:\n    1. Stock dividends (cổ tức bằng cổ phiếu): e.g., 30% → ratio = 0.30\n       Effect: shares × (1 + 0.30), price × (1 / 1.30)\n    2. Bonus shares (thưởng cổ phiếu): mechanically identical to stock dividends\n    3. Stock splits (chia tách): e.g., 2:1 → ratio = 2.0\n       Effect: shares × 2, price × 0.5\n    4. Rights issues (phát hành thêm): dilutive, but not all shareholders exercise\n       We approximate with the subscription ratio\n    5. Reverse splits (gộp cổ phiếu): rare in Vietnam\n       Effect: shares ÷ ratio, price × ratio\n    \n    We construct a FORWARD-LOOKING cumulative adjustment factor such that:\n       adjusted_shares = raw_shares × cum_adj_factor(from_date, to_date)\n       adjusted_price = raw_price / cum_adj_factor(from_date, to_date)\n    \n    This is analogous to CRSP's cfacshr in the US context.\n    \n    Parameters\n    ----------\n    corporate_actions : pd.DataFrame\n        DataCore.vn corporate actions with columns:\n        ticker, ex_date, action_type, ratio\n        \n        action_type values:\n        - 'stock_dividend': ratio = dividend rate (e.g., 0.30 for 30%)\n        - 'bonus_shares': ratio = bonus rate (e.g., 0.20 for 20%)\n        - 'stock_split': ratio = split factor (e.g., 2.0 for 2:1)\n        - 'reverse_split': ratio = merge factor (e.g., 5.0 for 5:1 merge)\n        - 'rights_issue': ratio = subscription rate (e.g., 0.10 for 10:1)\n        - 'cash_dividend': ratio = VND per share (no share adjustment needed)\n    \n    Returns\n    -------\n    pd.DataFrame\n        Adjustment factors: ticker, ex_date, point_factor, cum_factor\n    \"\"\"\n    # Filter to share-altering events only\n    share_events = ['stock_dividend', 'bonus_shares', 'stock_split', \n                    'reverse_split', 'rights_issue']\n    ca = corporate_actions[\n        corporate_actions['action_type'].isin(share_events)\n    ].copy()\n    \n    if len(ca) == 0:\n        print(\"No share-altering corporate actions found.\")\n        return pd.DataFrame(columns=['ticker', 'ex_date', 'point_factor', 'cum_factor'])\n    \n    # Compute point adjustment factor for each event\n    def compute_point_factor(row):\n        atype = row['action_type']\n        ratio = row['ratio']\n        \n        if atype in ['stock_dividend', 'bonus_shares']:\n            # 30% stock dividend: 100 shares → 130 shares\n            return 1 + ratio\n        elif atype == 'stock_split':\n            # 2:1 split: 100 shares → 200 shares\n            return ratio\n        elif atype == 'reverse_split':\n            # 5:1 reverse: 500 shares → 100 shares\n            return 1.0 / ratio\n        elif atype == 'rights_issue':\n            # Approximate: assume all rights exercised\n            # In practice, this overestimates the adjustment\n            return 1 + ratio\n        else:\n            return 1.0\n    \n    ca['point_factor'] = ca.apply(compute_point_factor, axis=1)\n    \n    # Sort chronologically within each ticker\n    ca = ca.sort_values(['ticker', 'ex_date']).reset_index(drop=True)\n    \n    # Cumulative factor: product of all point factors from listing to date\n    # This gives us a running \"total adjustment\" for each ticker\n    ca['cum_factor'] = ca.groupby('ticker')['point_factor'].cumprod()\n    \n    # Summary statistics\n    n_tickers = ca['ticker'].nunique()\n    n_events = len(ca)\n    avg_events = n_events / n_tickers if n_tickers &gt; 0 else 0\n    \n    print(f\"Corporate action adjustment factors built:\")\n    print(f\"  Tickers with adjustments: {n_tickers:,}\")\n    print(f\"  Total share-altering events: {n_events:,}\")\n    print(f\"  Average events per ticker: {avg_events:.1f}\")\n    print(f\"\\nEvent type distribution:\")\n    print(ca['action_type'].value_counts().to_string())\n    \n    return ca[['ticker', 'ex_date', 'action_type', 'ratio', \n               'point_factor', 'cum_factor']]\n\n\ndef adjust_shares(shares: float, ticker: str, from_date, to_date, \n                  adj_factors: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Adjust a share count from one date to another for corporate actions.\n    \n    Example: If a company had a 30% stock dividend with ex_date between\n    from_date and to_date, then 1000 shares at from_date = 1300 shares \n    at to_date.\n    \n    Parameters\n    ----------\n    shares : float\n        Number of shares at from_date\n    ticker : str\n        Stock ticker\n    from_date, to_date : pd.Timestamp\n        Period for adjustment\n    adj_factors : pd.DataFrame\n        Output of build_adjustment_factors()\n    \n    Returns\n    -------\n    float\n        Adjusted shares at to_date\n    \"\"\"\n    events = adj_factors[\n        (adj_factors['ticker'] == ticker) &\n        (adj_factors['ex_date'] &gt; pd.Timestamp(from_date)) &\n        (adj_factors['ex_date'] &lt;= pd.Timestamp(to_date))\n    ]\n    \n    if len(events) == 0:\n        return shares\n    \n    total_factor = events['point_factor'].prod()\n    return shares * total_factor\n\n\n# Example usage:\n# adj_factors = build_adjustment_factors(dc.corporate_actions)\n\n\n\n\n\n\n\nImportantThe Stock Dividend Problem in Vietnam\n\n\n\nVietnamese companies issue stock dividends with remarkable frequency, many growth companies do so 2-3 times per year. Consider Vinhomes (VHM) or FPT Corporation: their share counts may double or triple over a 5-year period purely from stock dividends. If you compare raw ownership shares from 2019 to 2024 without adjustment, you will obtain nonsensical ownership ratios. Every time-series analysis of Vietnamese ownership data must use adjusted shares. This is the Vietnamese equivalent of the CRSP cfacshr adjustment factor problem in US data, but more severe because the events are more frequent and larger in magnitude.\n\n\n\n# ============================================================================\n# Step 2: Process Stock Price Data\n# ============================================================================\n\ndef process_price_data(prices: pd.DataFrame, \n                       adj_factors: pd.DataFrame,\n                       company_profile: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Process DataCore.vn stock price data:\n    1. Align dates to month-end and quarter-end\n    2. Merge company metadata (exchange, sector, FOL limit)\n    3. Compute adjusted prices and shares outstanding\n    4. Compute market capitalization\n    5. Create quarter-end snapshots\n    \n    Parameters\n    ----------\n    prices : pd.DataFrame\n        Daily/monthly price data from DataCore.vn\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors\n    company_profile : pd.DataFrame\n        Company metadata including exchange, sector, FOL\n    \n    Returns\n    -------\n    pd.DataFrame\n        Quarter-end processed stock data\n    \"\"\"\n    df = prices.copy()\n    \n    # Standardize date\n    df['date'] = pd.to_datetime(df['date'])\n    df['month_end'] = df['date'] + pd.offsets.MonthEnd(0)\n    df['quarter_end'] = df['date'] + pd.offsets.QuarterEnd(0)\n    \n    # Merge company profile\n    profile_cols = ['ticker', 'exchange', 'industry_code', 'fol_limit', \n                    'listing_date', 'company_name']\n    profile_cols = [c for c in profile_cols if c in company_profile.columns]\n    df = df.merge(company_profile[profile_cols], on='ticker', how='left')\n    \n    # Build cumulative adjustment factor for each ticker-date\n    # For each observation, compute the total adjustment from listing to that date\n    df = df.sort_values(['ticker', 'date'])\n    \n    # Merge adjustment events\n    # For each ticker-date, find the cumulative factor as of that date\n    def get_cum_factor_at_date(group):\n        ticker = group.name\n        ticker_adj = adj_factors[adj_factors['ticker'] == ticker].copy()\n        \n        if len(ticker_adj) == 0:\n            group['cum_adj_factor'] = 1.0\n            return group\n        \n        # For each date, find cumulative factor (product of all events up to that date)\n        group = group.sort_values('date')\n        group['cum_adj_factor'] = 1.0\n        \n        for _, event in ticker_adj.iterrows():\n            mask = group['date'] &gt;= event['ex_date']\n            group.loc[mask, 'cum_adj_factor'] *= event['point_factor']\n        \n        return group\n    \n    df = df.groupby('ticker', group_keys=False).apply(get_cum_factor_at_date)\n    \n    # Adjusted price and shares\n    # adjusted_close should already be provided by DataCore.vn\n    # But we compute our own for consistency\n    if 'adjusted_close' not in df.columns:\n        df['adjusted_close'] = df['close'] / df['cum_adj_factor']\n    \n    # Adjusted shares outstanding\n    df['adjusted_shares'] = df['shares_outstanding'] * df['cum_adj_factor']\n    \n    # Market capitalization (in billion VND)\n    df['market_cap'] = df['close'] * df['shares_outstanding'] / 1e9\n    \n    # Monthly returns\n    df = df.sort_values(['ticker', 'date'])\n    df['ret'] = df.groupby('ticker')['adjusted_close'].pct_change()\n    \n    # Keep quarter-end observations\n    # For daily data: keep last trading day of each quarter\n    df_quarterly = (df.sort_values(['ticker', 'quarter_end', 'date'])\n                      .groupby(['ticker', 'quarter_end'])\n                      .last()\n                      .reset_index())\n    \n    print(f\"Processed price data:\")\n    print(f\"  Total records (daily): {len(df):,}\")\n    print(f\"  Quarter-end records: {len(df_quarterly):,}\")\n    print(f\"  Unique tickers: {df_quarterly['ticker'].nunique():,}\")\n    print(f\"  Date range: {df_quarterly['quarter_end'].min()} to \"\n          f\"{df_quarterly['quarter_end'].max()}\")\n    print(f\"\\nExchange distribution:\")\n    print(df_quarterly.groupby('exchange')['ticker'].nunique().to_string())\n    \n    return df_quarterly\n\n# prices_q = process_price_data(dc.prices, adj_factors, dc.company_profile)\n\n\n\n18.3.2 Ownership Structure Data\nVietnamese ownership data captures the composition of shareholders as disclosed in annual reports, semi-annual reports, and event-driven disclosures. The key distinction from US 13F data is that Vietnamese disclosures provide a complete ownership decomposition, not just institutional long positions, but the full breakdown into state, institutional, foreign, and individual ownership.\n\n# ============================================================================\n# Step 3: Process Ownership Structure Data\n# ============================================================================\n\nclass OwnershipType:\n    \"\"\"\n    Vietnam's ownership taxonomy.\n    \n    Unlike the US where 13F captures only institutional long positions,\n    Vietnamese disclosure provides a complete ownership decomposition.\n    We classify shareholders into five mutually exclusive categories.\n    \"\"\"\n    STATE = 'state'                    # Nhà nước (government entities, SOE parents)\n    FOREIGN_INST = 'foreign_inst'      # Tổ chức nước ngoài\n    DOMESTIC_INST = 'domestic_inst'    # Tổ chức trong nước (non-state)\n    INDIVIDUAL = 'individual'          # Cá nhân\n    TREASURY = 'treasury'              # Cổ phiếu quỹ\n    \n    ALL_TYPES = [STATE, FOREIGN_INST, DOMESTIC_INST, INDIVIDUAL, TREASURY]\n    INSTITUTIONAL = [STATE, FOREIGN_INST, DOMESTIC_INST]\n    FOREIGN = [FOREIGN_INST]  # Can be expanded if foreign individuals are tracked\n\n\ndef classify_shareholders(ownership: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Classify shareholders into Vietnam's ownership taxonomy.\n    \n    DataCore.vn may provide a `shareholder_type` field, but naming \n    conventions vary. This function standardizes the classification \n    using a combination of provided flags and name-based heuristics.\n    \n    The classification challenge in Vietnam (noted by @huang2023factors):\n    DataCore.vn may not always cleanly separate institution types, so we \n    use a cascading approach:\n    1. Use explicit flags (is_state, is_foreign, is_institution) if available\n    2. Apply name-based heuristics for Vietnamese entity names\n    3. Default to 'individual' for unclassified shareholders\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Raw ownership data from DataCore.vn\n    \n    Returns\n    -------\n    pd.DataFrame\n        Ownership data with standardized `owner_type` column\n    \"\"\"\n    df = ownership.copy()\n    \n    # --- Method 1: Use explicit flags if available ---\n    if all(col in df.columns for col in ['is_state', 'is_foreign', 'is_institution']):\n        conditions = [\n            (df['is_state'] == True),\n            (df['is_foreign'] == True) & (df['is_institution'] == True),\n            (df['is_foreign'] == True) & (df['is_institution'] != True),\n            (df['is_institution'] == True) & (df['is_state'] != True) & \n                (df['is_foreign'] != True),\n        ]\n        choices = [\n            OwnershipType.STATE,\n            OwnershipType.FOREIGN_INST,\n            OwnershipType.FOREIGN_INST,  # Foreign individuals often grouped\n            OwnershipType.DOMESTIC_INST,\n        ]\n        df['owner_type'] = np.select(conditions, choices, \n                                      default=OwnershipType.INDIVIDUAL)\n    \n    # --- Method 2: Name-based heuristics ---\n    elif 'shareholder_name' in df.columns:\n        name = df['shareholder_name'].str.lower().fillna('')\n        \n        # State entities: government ministries, SCIC, state corporations\n        state_keywords = [\n            'bộ tài chính', 'tổng công ty đầu tư', 'scic', \n            'ủy ban nhân dân', 'nhà nước', 'state capital',\n            'tổng công ty', 'vốn nhà nước', 'bộ công thương',\n            'bộ quốc phòng', 'bộ giao thông', 'vinashin',\n        ]\n        is_state = name.apply(\n            lambda x: any(kw in x for kw in state_keywords)\n        )\n        \n        # Foreign entities: common fund names, foreign company patterns\n        foreign_keywords = [\n            'fund', 'investment', 'capital', 'limited', 'ltd', 'inc',\n            'corporation', 'holdings', 'asset management', 'pte',\n            'gmbh', 'management', 'partners', 'advisors',\n            'dragon capital', 'vinacapital', 'templeton', \n            'blackrock', 'jpmorgan', 'samsung', 'mirae',\n        ]\n        # Also check for non-Vietnamese characters as a heuristic\n        is_foreign_name = name.apply(\n            lambda x: any(kw in x for kw in foreign_keywords)\n        )\n        \n        # Domestic institutions: Vietnamese bank, securities, insurance names\n        domestic_inst_keywords = [\n            'ngân hàng', 'chứng khoán', 'bảo hiểm', 'quỹ đầu tư',\n            'công ty quản lý', 'bảo việt', 'techcombank', 'vietcombank',\n            'bidv', 'vietinbank', 'vpbank', 'mb bank', 'ssi', 'hsc',\n            'vcsc', 'vndirect', 'fpt capital', 'manulife',\n        ]\n        is_domestic_inst = name.apply(\n            lambda x: any(kw in x for kw in domestic_inst_keywords)\n        )\n        \n        # Treasury shares\n        is_treasury = name.str.contains('cổ phiếu quỹ|treasury', case=False)\n        \n        # Apply classification cascade\n        df['owner_type'] = OwnershipType.INDIVIDUAL  # Default\n        df.loc[is_domestic_inst, 'owner_type'] = OwnershipType.DOMESTIC_INST\n        df.loc[is_foreign_name, 'owner_type'] = OwnershipType.FOREIGN_INST\n        df.loc[is_state, 'owner_type'] = OwnershipType.STATE\n        df.loc[is_treasury, 'owner_type'] = OwnershipType.TREASURY\n    \n    # --- Method 3: Use shareholder_type directly ---\n    elif 'shareholder_type' in df.columns:\n        type_map = {\n            'state': OwnershipType.STATE,\n            'foreign_institution': OwnershipType.FOREIGN_INST,\n            'foreign_individual': OwnershipType.FOREIGN_INST,\n            'domestic_institution': OwnershipType.DOMESTIC_INST,\n            'individual': OwnershipType.INDIVIDUAL,\n            'treasury': OwnershipType.TREASURY,\n        }\n        df['owner_type'] = df['shareholder_type'].str.lower().map(type_map)\n        df['owner_type'] = df['owner_type'].fillna(OwnershipType.INDIVIDUAL)\n    \n    else:\n        raise ValueError(\n            \"Cannot classify shareholders. Expected one of:\\n\"\n            \"  1. Columns: is_state, is_foreign, is_institution\\n\"\n            \"  2. Column: shareholder_name (for heuristic classification)\\n\"\n            \"  3. Column: shareholder_type (pre-classified)\"\n        )\n    \n    # Summary\n    print(\"Ownership classification results:\")\n    print(df['owner_type'].value_counts().to_string())\n    \n    return df\n\n# ownership_classified = classify_shareholders(dc.ownership)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-ownership-taxonomy",
    "href": "30_institutional_ownership.html#sec-ownership-taxonomy",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.4 Vietnam’s Ownership Taxonomy",
    "text": "18.4 Vietnam’s Ownership Taxonomy\n\n18.4.1 The Five Ownership Categories\nVietnam’s ownership structure is decomposed into five mutually exclusive categories that together sum to 100% of shares outstanding:\n\n\n\nTable 18.2: Vietnam’s Ownership Taxonomy\n\n\n\n\n\n\n\n\n\n\n\nCategory\nVietnamese Term\nDescription\nTypical Share (2020s)\n\n\n\n\nState\nSở hữu Nhà nước\nGovernment entities, SCIC, SOE parent companies\n~15-25% of market cap\n\n\nForeign Institutional\nTổ chức nước ngoài\nForeign funds, banks, corporations\n~15-20%\n\n\nDomestic Institutional\nTổ chức trong nước\nVietnamese funds, banks, insurance, securities firms\n~5-10%\n\n\nIndividual\nCá nhân\nRetail investors (both Vietnamese and foreign individuals)\n~55-65%\n\n\nTreasury\nCổ phiếu quỹ\nCompany’s own repurchased shares\n~0-2%\n\n\n\n\n\n\nThis taxonomy differs fundamentally from the US 13F framework in several ways:\n\nCompleteness: We observe 100% of ownership, not just institutional long positions above $100 million AUM.\nState as a category: State ownership is a first-class analytical category, not subsumed under “All Others” as in the LSEG type code system.\nIndividual visibility: We observe aggregate individual ownership directly, whereas in the US, individual ownership is merely the residual (100% − institutional ownership).\nNo short position ambiguity: Vietnam’s market has very limited short-selling infrastructure, so ownership data genuinely represents long positions.\n\n\n# ============================================================================\n# Step 4: Compute Ownership Decomposition\n# ============================================================================\n\ndef compute_ownership_decomposition(ownership: pd.DataFrame,\n                                     prices_q: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the full ownership decomposition for each stock at each \n    disclosure date.\n    \n    For each stock-date combination, aggregates shares held by each \n    ownership category and computes ownership ratios relative to \n    total shares outstanding.\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership data (output of classify_shareholders)\n    prices_q : pd.DataFrame\n        Quarter-end price data with shares_outstanding\n    \n    Returns\n    -------\n    pd.DataFrame\n        Stock-period level ownership decomposition with columns for\n        each ownership type's share count and percentage\n    \"\"\"\n    # Aggregate shares by ticker, date, and owner type\n    agg = (ownership.groupby(['ticker', 'date', 'owner_type'])['shares_held']\n                    .sum()\n                    .reset_index())\n    \n    # Pivot to wide format: one column per ownership type\n    wide = agg.pivot_table(\n        index=['ticker', 'date'],\n        columns='owner_type',\n        values='shares_held',\n        fill_value=0\n    ).reset_index()\n    \n    # Rename columns\n    type_cols = [c for c in wide.columns if c in OwnershipType.ALL_TYPES]\n    rename_map = {t: f'shares_{t}' for t in type_cols}\n    wide = wide.rename(columns=rename_map)\n    \n    # Total institutional shares\n    inst_cols = [f'shares_{t}' for t in OwnershipType.INSTITUTIONAL \n                 if f'shares_{t}' in wide.columns]\n    wide['shares_institutional'] = wide[inst_cols].sum(axis=1)\n    \n    # Total foreign shares (for FOL tracking)\n    foreign_cols = [f'shares_{t}' for t in OwnershipType.FOREIGN \n                    if f'shares_{t}' in wide.columns]\n    wide['shares_foreign_total'] = wide[foreign_cols].sum(axis=1)\n    \n    # Align with quarter-end dates for merging with price data\n    wide['quarter_end'] = wide['date'] + pd.offsets.QuarterEnd(0)\n    \n    # Merge with price data to get shares outstanding\n    merged = wide.merge(\n        prices_q[['ticker', 'quarter_end', 'shares_outstanding', \n                  'adjusted_shares', 'market_cap', 'exchange', \n                  'industry_code', 'fol_limit', 'close']],\n        on=['ticker', 'quarter_end'],\n        how='left'\n    )\n    \n    # Compute ownership ratios\n    tso = merged['shares_outstanding']\n    for col in merged.columns:\n        if col.startswith('shares_') and col != 'shares_outstanding':\n            ratio_col = col.replace('shares_', 'pct_')\n            merged[ratio_col] = merged[col] / tso\n            merged.loc[tso &lt;= 0, ratio_col] = np.nan\n    \n    # Derived measures\n    merged['pct_free_float'] = 1 - merged.get('pct_state', 0) - merged.get('pct_treasury', 0)\n    \n    # SOE flag: state ownership &gt; 50%\n    merged['is_soe'] = (merged.get('pct_state', 0) &gt; 0.50).astype(int)\n    \n    # FOL utilization\n    if 'fol_limit' in merged.columns and 'pct_foreign_total' in merged.columns:\n        merged['fol_utilization'] = merged['pct_foreign_total'] / merged['fol_limit']\n        merged['foreign_room'] = merged['fol_limit'] - merged['pct_foreign_total']\n        merged.loc[merged['fol_limit'] &lt;= 0, ['fol_utilization', 'foreign_room']] = np.nan\n    \n    # Number of institutional owners (breadth)\n    n_owners = (ownership[ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)]\n                .groupby(['ticker', 'date'])['shareholder_name']\n                .nunique()\n                .reset_index()\n                .rename(columns={'shareholder_name': 'n_inst_owners'}))\n    \n    n_foreign_owners = (ownership[ownership['owner_type'] == OwnershipType.FOREIGN_INST]\n                        .groupby(['ticker', 'date'])['shareholder_name']\n                        .nunique()\n                        .reset_index()\n                        .rename(columns={'shareholder_name': 'n_foreign_owners'}))\n    \n    merged = merged.merge(n_owners, on=['ticker', 'date'], how='left')\n    merged = merged.merge(n_foreign_owners, on=['ticker', 'date'], how='left')\n    merged[['n_inst_owners', 'n_foreign_owners']] = (\n        merged[['n_inst_owners', 'n_foreign_owners']].fillna(0)\n    )\n    \n    print(f\"Ownership decomposition computed:\")\n    print(f\"  Stock-period observations: {len(merged):,}\")\n    print(f\"  Unique tickers: {merged['ticker'].nunique():,}\")\n    print(f\"\\nMean ownership structure:\")\n    pct_cols = [c for c in merged.columns if c.startswith('pct_')]\n    print(merged[pct_cols].mean().round(4).to_string())\n    \n    return merged\n\n# ownership_decomp = compute_ownership_decomposition(\n#     ownership_classified, prices_q\n# )",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-ownership-metrics",
    "href": "30_institutional_ownership.html#sec-ownership-metrics",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.5 Institutional Ownership Measures",
    "text": "18.5 Institutional Ownership Measures\n\n18.5.1 Ownership Ratio\nThe Institutional Ownership Ratio (IOR) for stock \\(i\\) at time \\(t\\) in Vietnam is:\n\\[\nIOR_{i,t} = \\frac{S_{i,t}^{state} + S_{i,t}^{foreign\\_inst} + S_{i,t}^{domestic\\_inst}}{TSO_{i,t}}\n\\tag{18.1}\\]\nwhere \\(S_{i,t}^{type}\\) denotes adjusted shares held by each ownership category and \\(TSO_{i,t}\\) is total shares outstanding. Unlike the US where the IOR can exceed 100% due to long-only reporting and short selling, the Vietnamese IOR is bounded by construction in \\([0, 1]\\) because we observe the complete ownership decomposition.\nWe also compute category-specific ownership ratios:\n\\[\n\\begin{aligned}\nIOR_{i,t}^{foreign} &= \\frac{S_{i,t}^{foreign\\_inst}}{TSO_{i,t}},\\\\\nIOR_{i,t}^{state} &= \\frac{S_{i,t}^{state}}{TSO_{i,t}},\\\\\nIOR_{i,t}^{domestic} &= \\frac{S_{i,t}^{domestic\\_inst}}{TSO_{i,t}}\n\\end{aligned}\n\\tag{18.2}\\]\n\n\n18.5.2 Concentration: Herfindahl-Hirschman Index\nThe Institutional Ownership Concentration via the Herfindahl-Hirschman Index is:\n\\[\nIOC_{i,t}^{HHI} = \\sum_{j=1}^{N_{i,t}} \\left(\\frac{S_{i,j,t}}{\\sum_{k=1}^{N_{i,t}} S_{i,k,t}}\\right)^2\n\\tag{18.3}\\]\nIn Vietnam, the HHI is particularly informative because it captures the dominance of state shareholders. A company where the government holds 65% will have a mechanically high HHI even if the remaining 35% is diversely held.\nWe therefore compute separate HHI measures for different ownership categories:\n\\[\nHHI_{i,t}^{total} = \\sum_{j} w_{i,j,t}^2, \\quad\nHHI_{i,t}^{non-state} = \\sum_{j \\notin state} \\left(\\frac{S_{i,j,t}}{\\sum_{k \\notin state} S_{i,k,t}}\\right)^2\n\\tag{18.4}\\]\nThe non-state HHI is more comparable to the US institutional HHI, as it captures concentration among market-driven investors.\n\n\n18.5.3 Breadth of Ownership\nFollowing Chen, Hong, and Stein (2002), Institutional Breadth (\\(N_{i,t}\\)) is the number of institutional investors holding stock \\(i\\) in period \\(t\\). The Change in Breadth is:\n\\[\n\\Delta Breadth_{i,t} = \\frac{N_{i,t}^{cont} - N_{i,t-1}^{cont}}{TotalInstitutions_{t-1}}\n\\tag{18.5}\\]\nwhere \\(N_{i,t}^{cont}\\) counts only institutions that appear in the disclosure universe in both periods \\(t\\) and \\(t-1\\), following the Lehavy and Sloan (2008) algorithm. This adjustment is particularly important in Vietnam where:\n\nNew funds launch frequently (especially ETFs tracking VN30)\nForeign funds enter and exit the market\nDomestic securities firms consolidate or spin off asset management divisions\n\n\n# ============================================================================\n# Step 5: Compute All IO Metrics\n# ============================================================================\n\ndef compute_io_metrics_vietnam(ownership: pd.DataFrame,\n                                ownership_decomp: pd.DataFrame,\n                                adj_factors: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute security-level institutional ownership metrics adapted for Vietnam.\n    \n    Computes:\n    1. Ownership ratios by category (state, foreign, domestic inst, individual)\n    2. HHI concentration (total, non-state, foreign-only)\n    3. Number of institutional owners (total, foreign, domestic)\n    4. Change in breadth (Lehavy-Sloan adjusted)\n    5. FOL-related metrics (utilization, room, near-cap indicator)\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership data with individual shareholder records\n    ownership_decomp : pd.DataFrame\n        Aggregated ownership decomposition (output of compute_ownership_decomposition)\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors\n    \n    Returns\n    -------\n    pd.DataFrame\n        Stock-period level metrics\n    \"\"\"\n    # Start with the ownership decomposition\n    metrics = ownership_decomp.copy()\n    \n    # --- HHI Concentration ---\n    # Total HHI: across all institutional shareholders\n    inst_ownership = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)\n    ].copy()\n    \n    def compute_hhi_group(group):\n        \"\"\"Compute HHI for a group of shareholders.\"\"\"\n        total = group['shares_held'].sum()\n        if total &lt;= 0:\n            return np.nan\n        weights = group['shares_held'] / total\n        return (weights ** 2).sum()\n    \n    # Total institutional HHI\n    hhi_total = (inst_ownership.groupby(['ticker', 'date'])\n                               .apply(compute_hhi_group)\n                               .reset_index(name='hhi_institutional'))\n    metrics = metrics.merge(hhi_total, on=['ticker', 'date'], how='left')\n    \n    # Non-state HHI (exclude state shareholders)\n    non_state = ownership[\n        ownership['owner_type'].isin([OwnershipType.FOREIGN_INST, \n                                       OwnershipType.DOMESTIC_INST])\n    ]\n    hhi_nonstate = (non_state.groupby(['ticker', 'date'])\n                             .apply(compute_hhi_group)\n                             .reset_index(name='hhi_non_state'))\n    metrics = metrics.merge(hhi_nonstate, on=['ticker', 'date'], how='left')\n    \n    # Foreign-only HHI\n    foreign_only = ownership[ownership['owner_type'] == OwnershipType.FOREIGN_INST]\n    hhi_foreign = (foreign_only.groupby(['ticker', 'date'])\n                               .apply(compute_hhi_group)\n                               .reset_index(name='hhi_foreign'))\n    metrics = metrics.merge(hhi_foreign, on=['ticker', 'date'], how='left')\n    \n    # --- Change in Breadth (Lehavy-Sloan Algorithm) ---\n    metrics = metrics.sort_values(['ticker', 'date'])\n    \n    # Get list of all institutions filing in each period\n    inst_by_period = (inst_ownership.groupby('date')['shareholder_name']\n                                     .apply(set)\n                                     .to_dict())\n    \n    # For each stock-period: count continuing institutions\n    def compute_breadth_change(group):\n        group = group.sort_values('date').reset_index(drop=True)\n        group['dbreadth'] = np.nan\n        \n        for i in range(1, len(group)):\n            current_date = group.loc[i, 'date']\n            prev_date = group.loc[i-1, 'date']\n            \n            # Institutions in universe for both periods\n            current_universe = inst_by_period.get(current_date, set())\n            prev_universe = inst_by_period.get(prev_date, set())\n            continuing_universe = current_universe & prev_universe\n            \n            if len(prev_universe) == 0:\n                continue\n            \n            # Count continuing institutions holding this stock in each period\n            ticker = group.loc[i, 'ticker']\n            \n            current_holders = set(\n                inst_ownership[\n                    (inst_ownership['ticker'] == ticker) & \n                    (inst_ownership['date'] == current_date)\n                ]['shareholder_name']\n            )\n            prev_holders = set(\n                inst_ownership[\n                    (inst_ownership['ticker'] == ticker) & \n                    (inst_ownership['date'] == prev_date)\n                ]['shareholder_name']\n            )\n            \n            # Count only continuing institutions\n            n_current_cont = len(current_holders & continuing_universe)\n            n_prev_cont = len(prev_holders & continuing_universe)\n            \n            group.loc[i, 'dbreadth'] = (\n                (n_current_cont - n_prev_cont) / len(prev_universe)\n            )\n        \n        return group\n    \n    metrics = metrics.groupby('ticker', group_keys=False).apply(compute_breadth_change)\n    \n    # --- FOL Indicators ---\n    if 'fol_utilization' in metrics.columns:\n        metrics['near_fol_cap'] = (metrics['fol_utilization'] &gt; 0.90).astype(int)\n        metrics['at_fol_cap'] = (metrics['fol_utilization'] &gt; 0.98).astype(int)\n    \n    print(f\"IO metrics computed for Vietnam:\")\n    print(f\"  Observations: {len(metrics):,}\")\n    print(f\"\\nKey metric distributions:\")\n    summary_cols = ['pct_institutional', 'pct_state', 'pct_foreign_total',\n                    'hhi_institutional', 'n_inst_owners', 'dbreadth']\n    summary_cols = [c for c in summary_cols if c in metrics.columns]\n    print(metrics[summary_cols].describe().round(4).to_string())\n    \n    return metrics\n\n# io_metrics = compute_io_metrics_vietnam(\n#     ownership_classified, ownership_decomp, adj_factors\n# )\n\n\n\n18.5.4 Time Series Visualization\n\n\n\ndef plot_ownership_timeseries_vietnam(metrics: pd.DataFrame):\n    \"\"\"\n    Create publication-quality time series plots of Vietnamese \n    ownership structure evolution.\n    \"\"\"\n    fig, axes = plt.subplots(3, 1, figsize=(12, 14))\n    \n    # Aggregate across all stocks (market-cap weighted)\n    ts = metrics.groupby('quarter_end').apply(\n        lambda g: pd.Series({\n            'pct_state': np.average(g['pct_state'].fillna(0), \n                                     weights=g['market_cap'].fillna(1)),\n            'pct_foreign': np.average(g['pct_foreign_total'].fillna(0), \n                                       weights=g['market_cap'].fillna(1)),\n            'pct_domestic_inst': np.average(g['pct_domestic_inst'].fillna(0), \n                                             weights=g['market_cap'].fillna(1)),\n            'pct_individual': np.average(g['pct_individual'].fillna(0), \n                                          weights=g['market_cap'].fillna(1)),\n            'n_stocks': g['ticker'].nunique(),\n            'total_mktcap': g['market_cap'].sum(),\n            'median_n_inst': g['n_inst_owners'].median(),\n            'median_hhi': g['hhi_institutional'].median(),\n            'pct_soe': g['is_soe'].mean(),\n        })\n    ).reset_index()\n    \n    # ---- Panel A: Ownership Composition (Stacked Area) ----\n    ax = axes[0]\n    dates = ts['quarter_end']\n    ax.stackplot(dates,\n                 ts['pct_state'] * 100,\n                 ts['pct_foreign'] * 100,\n                 ts['pct_domestic_inst'] * 100,\n                 ts['pct_individual'] * 100,\n                 labels=['State', 'Foreign Institutional', \n                         'Domestic Institutional', 'Individual'],\n                 colors=[OWNER_COLORS['State'], OWNER_COLORS['Foreign Institutional'],\n                         OWNER_COLORS['Domestic Institutional'], OWNER_COLORS['Individual']],\n                 alpha=0.8)\n    ax.set_ylabel('Ownership Share (%)')\n    ax.set_title('Panel A: Ownership Composition of Vietnamese Listed Companies '\n                 '(Market-Cap Weighted)')\n    ax.legend(loc='upper right', frameon=True, framealpha=0.9)\n    ax.set_ylim(0, 100)\n    \n    # ---- Panel B: Institutional Ownership by Component ----\n    ax = axes[1]\n    ax.plot(dates, ts['pct_state'] * 100, label='State',\n            color=OWNER_COLORS['State'], linewidth=2)\n    ax.plot(dates, ts['pct_foreign'] * 100, label='Foreign Institutional',\n            color=OWNER_COLORS['Foreign Institutional'], linewidth=2)\n    ax.plot(dates, ts['pct_domestic_inst'] * 100, label='Domestic Institutional',\n            color=OWNER_COLORS['Domestic Institutional'], linewidth=2)\n    total_inst = (ts['pct_state'] + ts['pct_foreign'] + ts['pct_domestic_inst']) * 100\n    ax.plot(dates, total_inst, label='Total Institutional',\n            color=OWNER_COLORS['Total Institutional'], linewidth=2.5, linestyle='--')\n    ax.set_ylabel('Ownership Ratio (%)')\n    ax.set_title('Panel B: Institutional Ownership Components')\n    ax.legend(loc='upper left', frameon=True, framealpha=0.9)\n    \n    # ---- Panel C: Market Structure ----\n    ax = axes[2]\n    ax2 = ax.twinx()\n    ax.plot(dates, ts['n_stocks'], color='#1f77b4', linewidth=2, label='# Listed Stocks')\n    ax2.plot(dates, ts['total_mktcap'] / 1000, color='#d62728', linewidth=2, \n             label='Total Market Cap (Trillion VND)')\n    ax.set_ylabel('Number of Listed Stocks', color='#1f77b4')\n    ax2.set_ylabel('Market Cap (Trillion VND)', color='#d62728')\n    ax.set_title('Panel C: Vietnamese Stock Market Development')\n    \n    # Combine legends\n    lines1, labels1 = ax.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left', framealpha=0.9)\n    \n    plt.tight_layout()\n    plt.savefig('fig_ownership_timeseries_vn.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_ownership_timeseries_vietnam(io_metrics)\n\n\nFigure 18.1\n\n\n\n\n\n\ndef plot_io_by_exchange_size(metrics: pd.DataFrame):\n    \"\"\"Plot IO ratios by exchange and size quintile.\"\"\"\n    df = metrics[metrics['market_cap'].notna() & (metrics['market_cap'] &gt; 0)].copy()\n    \n    # Size quintiles within each quarter\n    df['size_quintile'] = df.groupby('quarter_end')['market_cap'].transform(\n        lambda x: pd.qcut(x, 5, labels=['Q1\\n(Small)', 'Q2', 'Q3', 'Q4', 'Q5\\n(Large)'],\n                          duplicates='drop')\n    )\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n    \n    metrics_to_plot = [\n        ('pct_institutional', 'Total Institutional'),\n        ('pct_foreign_total', 'Foreign Institutional'),\n        ('pct_state', 'State'),\n    ]\n    \n    for ax, (col, title) in zip(axes, metrics_to_plot):\n        for exchange, color in EXCHANGE_COLORS.items():\n            data = df[df['exchange'] == exchange]\n            if len(data) == 0:\n                continue\n            means = data.groupby('size_quintile')[col].mean() * 100\n            ax.bar(np.arange(len(means)) + list(EXCHANGE_COLORS.keys()).index(exchange) * 0.25,\n                   means, width=0.25, label=exchange, color=color, alpha=0.8)\n        \n        ax.set_title(title)\n        ax.set_xlabel('Size Quintile')\n        if ax == axes[0]:\n            ax.set_ylabel('Mean Ownership (%)')\n        ax.legend()\n        ax.set_xticks(np.arange(5) + 0.25)\n        ax.set_xticklabels(['Q1\\n(Small)', 'Q2', 'Q3', 'Q4', 'Q5\\n(Large)'])\n    \n    plt.tight_layout()\n    plt.savefig('fig_io_by_exchange_size.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_io_by_exchange_size(io_metrics)\n\n\nFigure 18.2\n\n\n\n\n\n\nTable 18.3: Summary Statistics of Ownership Structure in Vietnam by Size Quintile and Exchange (Pooled 2010-2024)\n\n\ndef tabulate_io_summary(metrics: pd.DataFrame, start_year: int = 2010) -&gt; pd.DataFrame:\n    \"\"\"\n    Create publication-quality summary table of Vietnamese ownership\n    structure by firm size.\n    \"\"\"\n    df = metrics[\n        (metrics['quarter_end'].dt.year &gt;= start_year) &\n        (metrics['market_cap'].notna()) & (metrics['market_cap'] &gt; 0)\n    ].copy()\n    \n    df['size_quintile'] = df.groupby('quarter_end')['market_cap'].transform(\n        lambda x: pd.qcut(x, 5, labels=['Q1 (Small)', 'Q2', 'Q3', 'Q4', 'Q5 (Large)'],\n                          duplicates='drop')\n    )\n    \n    table = df.groupby('size_quintile').agg(\n        N=('ticker', 'count'),\n        Mean_MktCap=('market_cap', 'mean'),\n        Mean_IO_Total=('pct_institutional', 'mean'),\n        Mean_State=('pct_state', 'mean'),\n        Mean_Foreign=('pct_foreign_total', 'mean'),\n        Mean_Domestic_Inst=('pct_domestic_inst', 'mean'),\n        Mean_Individual=('pct_individual', 'mean'),\n        Median_N_Owners=('n_inst_owners', 'median'),\n        Median_HHI=('hhi_institutional', 'median'),\n        Pct_SOE=('is_soe', 'mean'),\n        Mean_FOL_Util=('fol_utilization', 'mean'),\n    ).round(4)\n    \n    # Format\n    table['N'] = table['N'].apply(lambda x: f\"{x:,.0f}\")\n    table['Mean_MktCap'] = table['Mean_MktCap'].apply(lambda x: f\"{x:,.0f}B VND\")\n    for col in ['Mean_IO_Total', 'Mean_State', 'Mean_Foreign', \n                'Mean_Domestic_Inst', 'Mean_Individual', 'Pct_SOE', 'Mean_FOL_Util']:\n        table[col] = table[col].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"—\")\n    table['Median_N_Owners'] = table['Median_N_Owners'].apply(lambda x: f\"{x:.0f}\")\n    table['Median_HHI'] = table['Median_HHI'].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"—\")\n    \n    table.columns = ['N', 'Mean Mkt Cap', 'IO Total', 'State', 'Foreign', \n                      'Dom. Inst.', 'Individual', 'Med. # Owners', \n                      'Med. HHI', '% SOE', 'FOL Util.']\n    \n    return table\n\n# io_summary = tabulate_io_summary(io_metrics)\n# print(io_summary.to_string())",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-foreign-ownership",
    "href": "30_institutional_ownership.html#sec-foreign-ownership",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.6 Foreign Ownership Dynamics",
    "text": "18.6 Foreign Ownership Dynamics\n\n18.6.1 Foreign Ownership Limits and the FOL Premium\nVietnam’s Foreign Ownership Limits create a unique market segmentation. When a stock reaches its FOL, the only way for a new foreign investor to buy is if an existing foreign holder sells. This creates a de facto “foreign-only” market for FOL-constrained stocks, with documented price premiums (Vo 2015).\nThe FOL Utilization Ratio for stock \\(i\\) at time \\(t\\) is:\n\\[\nFOL\\_Util_{i,t} = \\frac{ForeignOwnership_{i,t}}{FOL\\_Limit_i}\n\\tag{18.6}\\]\nStocks are classified by FOL proximity (Table 18.4).\n\n\n\nTable 18.4: FOL Proximity Zones\n\n\n\n\n\n\n\n\n\n\nFOL Zone\nUtilization Range\nMarket Implication\n\n\n\n\nGreen\n&lt; 50%\nAmple foreign room; normal trading\n\n\nYellow\n50-80%\nModerate room; some foreign interest pressure\n\n\nOrange\n80-95%\nLimited room; foreign premium emerging\n\n\nRed\n95-100%\nNear cap; significant foreign premium\n\n\nCapped\n≈ 100%\nAt limit; foreign-only secondary market\n\n\n\n\n\n\n\n# ============================================================================\n# Step 6: Foreign Ownership Limit Analysis\n# ============================================================================\n\nclass FOLAnalyzer:\n    \"\"\"\n    Analyze Foreign Ownership Limit dynamics in the Vietnamese market.\n    \n    Key analyses:\n    1. FOL utilization tracking and classification\n    2. FOL premium estimation (price impact of being near cap)\n    3. Foreign room dynamics (opening/closing events)\n    4. Cross-sectional determinants of foreign ownership\n    \"\"\"\n    \n    FOL_ZONES = {\n        'Green': (0, 0.50),\n        'Yellow': (0.50, 0.80),\n        'Orange': (0.80, 0.95),\n        'Red': (0.95, 1.00),\n        'Capped': (1.00, 1.50),\n    }\n    \n    def __init__(self, io_metrics: pd.DataFrame,\n                 foreign_daily: Optional[pd.DataFrame] = None):\n        \"\"\"\n        Parameters\n        ----------\n        io_metrics : pd.DataFrame\n            Full ownership metrics from compute_io_metrics_vietnam()\n        foreign_daily : pd.DataFrame, optional\n            Daily foreign ownership tracking from DataCore.vn\n        \"\"\"\n        self.metrics = io_metrics.copy()\n        self.foreign_daily = foreign_daily\n    \n    def classify_fol_zones(self) -&gt; pd.DataFrame:\n        \"\"\"Classify stocks into FOL proximity zones.\"\"\"\n        df = self.metrics.copy()\n        \n        if 'fol_utilization' not in df.columns:\n            print(\"FOL utilization not available in metrics.\")\n            return df\n        \n        conditions = []\n        choices = []\n        for zone, (lo, hi) in self.FOL_ZONES.items():\n            conditions.append(\n                (df['fol_utilization'] &gt;= lo) & (df['fol_utilization'] &lt; hi)\n            )\n            choices.append(zone)\n        \n        df['fol_zone'] = np.select(conditions, choices, default='Unknown')\n        \n        # Summary\n        zone_dist = df.groupby('fol_zone')['ticker'].nunique()\n        print(\"FOL Zone Distribution (unique stocks):\")\n        print(zone_dist.to_string())\n        \n        return df\n    \n    def estimate_fol_premium(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Estimate the FOL premium using a cross-sectional approach.\n        \n        For each period, regress stock valuations (P/B or P/E) on FOL \n        utilization, controlling for fundamentals. The coefficient on \n        FOL utilization captures the premium investors pay for stocks \n        near their foreign ownership cap.\n        \n        Alternative: Compare returns of stocks transitioning between \n        FOL zones as a natural experiment.\n        \"\"\"\n        df = self.metrics.copy()\n        df = df[df['fol_utilization'].notna() & df['market_cap'].notna()].copy()\n        \n        # FOL zone dummies\n        df['near_cap'] = (df['fol_utilization'] &gt; 0.90).astype(int)\n        df['at_cap'] = (df['fol_utilization'] &gt; 0.98).astype(int)\n        \n        # Price-to-book as valuation measure\n        # (Assumes 'equity' is available from financial data)\n        if 'equity' in df.columns:\n            df['pb_ratio'] = df['market_cap'] * 1e9 / df['equity']\n        else:\n            # Use market cap as proxy for cross-sectional analysis\n            df['log_mktcap'] = np.log(df['market_cap'])\n        \n        # Fama-MacBeth style: run cross-sectional regressions each period\n        results = []\n        for quarter, group in df.groupby('quarter_end'):\n            group = group.dropna(subset=['fol_utilization', 'log_mktcap'])\n            if len(group) &lt; 50:\n                continue\n            \n            y = group['log_mktcap']\n            X = sm.add_constant(group[['fol_utilization', 'pct_state', \n                                        'n_inst_owners']])\n            try:\n                model = sm.OLS(y, X).fit()\n                results.append({\n                    'quarter': quarter,\n                    'beta_fol': model.params.get('fol_utilization', np.nan),\n                    'tstat_fol': model.tvalues.get('fol_utilization', np.nan),\n                    'r2': model.rsquared,\n                    'n': len(group),\n                })\n            except Exception:\n                continue\n        \n        if results:\n            results_df = pd.DataFrame(results)\n            print(\"FOL Premium (Fama-MacBeth Regression):\")\n            print(f\"  Mean β(FOL_util): {results_df['beta_fol'].mean():.4f}\")\n            print(f\"  t-statistic: {results_df['beta_fol'].mean() / \"\n                  f\"(results_df['beta_fol'].std() / np.sqrt(len(results_df))):.2f}\")\n            return results_df\n        \n        return pd.DataFrame()\n    \n    def analyze_foreign_room_events(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Analyze events where foreign room opens or closes.\n        \n        Room-opening events (FOL cap raised, foreign seller exits) can\n        trigger significant price movements as pent-up foreign demand \n        is released. Room-closing events (approaching cap) can create\n        selling pressure as foreign investors anticipate illiquidity.\n        \"\"\"\n        if self.foreign_daily is None:\n            print(\"Daily foreign ownership data required for event analysis.\")\n            return pd.DataFrame()\n        \n        df = self.foreign_daily.copy()\n        df = df.sort_values(['ticker', 'date'])\n        \n        # Compute daily change in foreign room\n        df['foreign_room_change'] = df.groupby('ticker')['foreign_room'].diff()\n        \n        # Identify room-opening events (room increases by &gt; 1 percentage point)\n        df['room_open_event'] = (df['foreign_room_change'] &gt; 0.01).astype(int)\n        \n        # Identify room-closing events (room decreases to &lt; 2%)\n        df['room_close_event'] = (\n            (df['foreign_room'] &lt; 0.02) & \n            (df.groupby('ticker')['foreign_room'].shift(1) &gt;= 0.02)\n        ).astype(int)\n        \n        events = df[\n            (df['room_open_event'] == 1) | (df['room_close_event'] == 1)\n        ].copy()\n        \n        print(f\"Foreign room events identified:\")\n        print(f\"  Room-opening events: {df['room_open_event'].sum():,}\")\n        print(f\"  Room-closing events: {df['room_close_event'].sum():,}\")\n        \n        return events\n\n# fol_analyzer = FOLAnalyzer(io_metrics, dc.foreign_ownership)\n# fol_classified = fol_analyzer.classify_fol_zones()\n# fol_premium = fol_analyzer.estimate_fol_premium()\n\n\n\n\ndef plot_fol_utilization(metrics: pd.DataFrame):\n    \"\"\"Plot FOL utilization distribution by sector.\"\"\"\n    df = metrics[metrics['fol_utilization'].notna()].copy()\n    \n    # Assign broad sectors\n    sector_map = {\n        'Banking': ['VCB', 'BID', 'CTG', 'TCB', 'VPB', 'MBB', 'ACB', 'HDB', 'STB', 'TPB'],\n        'Real Estate': ['VHM', 'VIC', 'NVL', 'KDH', 'DXG', 'HDG', 'VRE'],\n        'Technology': ['FPT', 'CMG', 'FOX'],\n        'Consumer': ['VNM', 'MSN', 'SAB', 'MWG', 'PNJ'],\n    }\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    for sector, tickers in sector_map.items():\n        data = df[df['ticker'].isin(tickers)]['fol_utilization']\n        if len(data) &gt; 0:\n            ax.hist(data * 100, bins=30, alpha=0.4, label=sector, density=True)\n    \n    ax.axvline(x=30, color='red', linestyle='--', alpha=0.7, label='Banking FOL (30%)')\n    ax.axvline(x=49, color='blue', linestyle='--', alpha=0.7, label='Standard FOL (49%)')\n    ax.set_xlabel('FOL Utilization (%)')\n    ax.set_ylabel('Density')\n    ax.set_title('Foreign Ownership Limit Utilization Distribution')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig('fig_fol_utilization.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_fol_utilization(io_metrics)\n\n\nFigure 18.3",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-trades",
    "href": "30_institutional_ownership.html#sec-trades",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.7 Institutional Trades",
    "text": "18.7 Institutional Trades\n\n18.7.1 Trade Inference in Vietnam\nIn the US, institutional trades are inferred from quarterly 13F holding snapshots. In Vietnam, the challenge is more acute because disclosure frequency varies:\n\nMajor shareholders (\\(\\ge\\) 5%): Must disclose within 7 business days of crossing ownership thresholds (5%, 10%, 15%, 20%, 25%, 50%, 65%, 75%)\nFund portfolio reports: Semi-annual disclosure required; some funds report quarterly\nAnnual reports: Provide complete shareholder register but only once per year\nDaily foreign ownership: HOSE/HNX publish aggregate daily foreign buy/sell data\n\nWe derive trades from the change in ownership between consecutive disclosure dates, applying the same logic as the US Ben-David et al. (2013) algorithm but adapted for Vietnam’s irregular disclosure intervals.\n\n# ============================================================================\n# Step 7: Derive Institutional Trades\n# ============================================================================\n\ndef derive_trades_vietnam(ownership: pd.DataFrame,\n                           adj_factors: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Derive institutional trades from changes in ownership disclosures.\n    \n    Adapted from Ben-David, Franzoni, and Moussawi (2012) for \n    Vietnam's irregular disclosure frequency.\n    \n    Key differences from US approach:\n    1. Disclosure intervals are irregular (not always quarterly)\n    2. We observe ALL institutional types, not just 13F filers\n    3. No $100M AUM threshold (we see all institutional holders)\n    4. Must adjust for corporate actions between disclosure dates\n    \n    Trade types:\n    +1: Initiating Buy (new position)\n    +2: Incremental Buy (increased existing position)\n    -1: Terminating Sale (fully exited position)\n    -2: Incremental Sale (reduced existing position)\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership with: ticker, date, shareholder_name, \n        shares_held, owner_type\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors\n    \n    Returns\n    -------\n    pd.DataFrame\n        Trade-level data: date, shareholder_name, ticker, trade, \n        buysale, owner_type\n    \"\"\"\n    # Focus on institutional shareholders only\n    inst = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)\n    ].copy()\n    \n    inst = inst.sort_values(['shareholder_name', 'ticker', 'date']).reset_index(drop=True)\n    \n    trades_list = []\n    \n    for (shareholder, ticker), group in inst.groupby(['shareholder_name', 'ticker']):\n        group = group.reset_index(drop=True)\n        \n        for i in range(len(group)):\n            current = group.iloc[i]\n            current_date = current['date']\n            current_shares = current['shares_held']\n            owner_type = current['owner_type']\n            \n            if i == 0:\n                # First observation: if institution appears, it's an initiating buy\n                # (we don't know if they held before our data starts)\n                # Skip the very first observation to avoid false initiating buys\n                continue\n            \n            prev = group.iloc[i - 1]\n            prev_date = prev['date']\n            prev_shares = prev['shares_held']\n            \n            # Adjust previous shares for corporate actions between dates\n            prev_shares_adj = adjust_shares(\n                prev_shares, ticker, prev_date, current_date, adj_factors\n            )\n            \n            # Compute trade (in adjusted shares)\n            trade = current_shares - prev_shares_adj\n            \n            # Classify trade type\n            if abs(trade) &lt; 1:  # De minimis threshold\n                continue\n            \n            if prev_shares_adj &lt;= 0 and current_shares &gt; 0:\n                buysale = 1  # Initiating buy\n            elif prev_shares_adj &gt; 0 and current_shares &lt;= 0:\n                buysale = -1  # Terminating sale\n            elif trade &gt; 0:\n                buysale = 2  # Incremental buy\n            else:\n                buysale = -2  # Incremental sale\n            \n            trades_list.append({\n                'date': current_date,\n                'shareholder_name': shareholder,\n                'ticker': ticker,\n                'trade': trade,\n                'prev_shares_adj': prev_shares_adj,\n                'current_shares': current_shares,\n                'buysale': buysale,\n                'owner_type': owner_type,\n                'days_between': (current_date - prev_date).days,\n            })\n    \n    trades = pd.DataFrame(trades_list)\n    \n    if len(trades) &gt; 0:\n        print(f\"Trades derived: {len(trades):,}\")\n        print(f\"\\nTrade type distribution:\")\n        labels = {1: 'Initiating Buy', 2: 'Incremental Buy',\n                  -1: 'Terminating Sale', -2: 'Incremental Sale'}\n        for bs, label in sorted(labels.items()):\n            n = (trades['buysale'] == bs).sum()\n            print(f\"  {label}: {n:,} ({n/len(trades):.1%})\")\n        \n        print(f\"\\nBy owner type:\")\n        print(trades.groupby('owner_type')['trade'].agg(['count', 'mean', 'median'])\n              .round(0).to_string())\n    \n    return trades\n\n# trades = derive_trades_vietnam(ownership_classified, adj_factors)\n\n\n\n\n\n\n\nWarningCorporate Action Adjustment in Trade Derivation\n\n\n\nWhen computing trades as \\(\\Delta Shares = Shares_t - Shares_{t-1}\\), the previous period’s shares must be adjusted for any corporate actions between \\(t-1\\) and \\(t\\). If VNM issued a 20% stock dividend between the two disclosure dates, then 1,000 shares at \\(t-1\\) should be compared to 1,200 adjusted shares, not 1,000 raw shares. Failing to make this adjustment would create a phantom “buy” of 200 shares that never actually occurred.\n\n\n\ndef derive_trades_vectorized_vietnam(ownership: pd.DataFrame,\n                                      adj_factors: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Vectorized version of Vietnamese trade derivation.\n    \n    Uses pandas groupby and vectorized operations instead of Python loops.\n    Approximately 20-50x faster for large datasets.\n    \n    Note: Corporate action adjustment is applied per-group, which still\n    requires some iteration but is much faster than row-by-row.\n    \"\"\"\n    inst = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL) &\n        (ownership['shares_held'] &gt; 0)\n    ].copy()\n    \n    inst = inst.sort_values(['shareholder_name', 'ticker', 'date']).reset_index(drop=True)\n    \n    # Lagged values\n    inst['prev_date'] = inst.groupby(['shareholder_name', 'ticker'])['date'].shift(1)\n    inst['prev_shares'] = inst.groupby(['shareholder_name', 'ticker'])['shares_held'].shift(1)\n    inst['is_first'] = inst['prev_date'].isna()\n    \n    # Remove first observations (no prior to compare)\n    inst = inst[~inst['is_first']].copy()\n    \n    # Adjust previous shares for corporate actions\n    # Vectorized: for each row, apply adjustment between prev_date and date\n    def adjust_row(row):\n        return adjust_shares(\n            row['prev_shares'], row['ticker'], \n            row['prev_date'], row['date'], adj_factors\n        )\n    \n    inst['prev_shares_adj'] = inst.apply(adjust_row, axis=1)\n    \n    # Compute trade\n    inst['trade'] = inst['shares_held'] - inst['prev_shares_adj']\n    inst['days_between'] = (inst['date'] - inst['prev_date']).dt.days\n    \n    # Classify trade type\n    inst['buysale'] = np.select(\n        [\n            (inst['prev_shares_adj'] &lt;= 0) & (inst['shares_held'] &gt; 0),\n            (inst['prev_shares_adj'] &gt; 0) & (inst['shares_held'] &lt;= 0),\n            inst['trade'] &gt; 0,\n            inst['trade'] &lt; 0,\n        ],\n        [1, -1, 2, -2],\n        default=0\n    )\n    \n    # Remove zero trades\n    trades = inst[inst['buysale'] != 0].copy()\n    \n    trades = trades[['date', 'shareholder_name', 'ticker', 'trade', \n                     'buysale', 'owner_type', 'days_between',\n                     'prev_shares_adj', 'shares_held']].copy()\n    trades = trades.rename(columns={'shares_held': 'current_shares'})\n    \n    print(f\"Vectorized trades: {len(trades):,}\")\n    return trades\n\n# trades = derive_trades_vectorized_vietnam(ownership_classified, adj_factors)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-flows-turnover",
    "href": "30_institutional_ownership.html#sec-flows-turnover",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.8 Fund-Level Flows and Turnover",
    "text": "18.8 Fund-Level Flows and Turnover\n\n18.8.1 Portfolio Assets and Returns from Fund Holdings\nUsing DataCore.vn’s fund holdings data, we compute fund-level portfolio analytics analogous to the US 13F approach:\n\\[\nAssets_{j,t} = \\sum_{i=1}^{N_{j,t}} S_{i,j,t} \\times P_{i,t}\n\\tag{18.7}\\]\n\\[\nR_{j,t \\to t+1}^{holdings} = \\frac{\\sum_{i} S_{i,j,t} \\times P_{i,t} \\times R_{i,t \\to t+1}}{\\sum_{i} S_{i,j,t} \\times P_{i,t}}\n\\tag{18.8}\\]\n\\[\nNetFlows_{j,t} = Assets_{j,t} - Assets_{j,t-1} \\times (1 + R_{j,t-1 \\to t}^{holdings})\n\\tag{18.9}\\]\n\n\n18.8.2 Turnover Measures\nFollowing Carhart (1997), adapted for Vietnam’s fund reporting:\n\\[\nTurnover_{j,t}^{Carhart} = \\frac{\\min(TotalBuys_{j,t}, TotalSales_{j,t})}{\\overline{Assets}_{j,t}}\n\\tag{18.10}\\]\n\n# ============================================================================\n# Step 8: Fund-Level Portfolio Analytics\n# ============================================================================\n\ndef compute_fund_analytics(fund_holdings: pd.DataFrame,\n                            prices_q: pd.DataFrame,\n                            adj_factors: pd.DataFrame) -&gt; Dict:\n    \"\"\"\n    Compute fund-level portfolio analytics from DataCore.vn fund holdings.\n    \n    Vietnamese fund disclosure is typically semi-annual (some quarterly),\n    which limits the frequency of these analytics compared to the US\n    quarterly approach.\n    \n    Returns\n    -------\n    dict with keys:\n        'fund_assets': pd.DataFrame of fund-level assets and returns\n        'fund_trades': pd.DataFrame of fund-level derived trades\n        'fund_aggregates': pd.DataFrame of flows and turnover\n    \"\"\"\n    fh = fund_holdings.copy()\n    fh = fh[fh['shares_held'] &gt; 0].copy()\n    \n    # Merge with prices\n    fh = fh.merge(\n        prices_q[['ticker', 'quarter_end', 'close', 'adjusted_close', 'ret']],\n        left_on=['ticker', 'report_date'],\n        right_on=['ticker', 'quarter_end'],\n        how='inner'\n    )\n    \n    # Portfolio value\n    fh['holding_value'] = fh['shares_held'] * fh['close']\n    \n    # --- Fund-Level Assets ---\n    fund_assets = fh.groupby(['fund_name', 'report_date']).agg(\n        total_assets=('holding_value', lambda x: x.sum() / 1e9),  # Billion VND\n        n_stocks=('ticker', 'nunique'),\n    ).reset_index()\n    \n    # Holdings return (value-weighted)\n    fh['weight'] = fh.groupby(['fund_name', 'report_date'])['holding_value'].transform(\n        lambda x: x / x.sum()\n    )\n    fund_hret = (fh.groupby(['fund_name', 'report_date'])\n                   .apply(lambda g: np.average(g['ret'].fillna(0), weights=g['weight']))\n                   .reset_index(name='holdings_return'))\n    \n    fund_assets = fund_assets.merge(fund_hret, on=['fund_name', 'report_date'])\n    \n    # --- Fund-Level Trades ---\n    # Derive trades from changes in holdings\n    fh_sorted = fh.sort_values(['fund_name', 'ticker', 'report_date'])\n    fh_sorted['prev_shares'] = fh_sorted.groupby(['fund_name', 'ticker'])['shares_held'].shift(1)\n    fh_sorted['prev_date'] = fh_sorted.groupby(['fund_name', 'ticker'])['report_date'].shift(1)\n    \n    # Adjust for corporate actions\n    fh_sorted['prev_shares_adj'] = fh_sorted.apply(\n        lambda r: adjust_shares(r['prev_shares'], r['ticker'], \n                                r['prev_date'], r['report_date'], adj_factors)\n        if pd.notna(r['prev_shares']) else np.nan,\n        axis=1\n    )\n    \n    fh_sorted['trade'] = fh_sorted['shares_held'] - fh_sorted['prev_shares_adj']\n    fh_sorted['trade_value'] = fh_sorted['trade'] * fh_sorted['close'] / 1e9  # Billion VND\n    \n    # Aggregate buys and sells per fund-period\n    fund_trades = fh_sorted[fh_sorted['trade'].notna()].copy()\n    fund_flows = fund_trades.groupby(['fund_name', 'report_date']).agg(\n        total_buys=('trade_value', lambda x: x[x &gt; 0].sum()),\n        total_sales=('trade_value', lambda x: -x[x &lt; 0].sum()),\n    ).reset_index()\n    \n    # --- Fund-Level Aggregates ---\n    fund_agg = fund_assets.merge(fund_flows, on=['fund_name', 'report_date'], how='left')\n    fund_agg[['total_buys', 'total_sales']] = fund_agg[['total_buys', 'total_sales']].fillna(0)\n    \n    fund_agg = fund_agg.sort_values(['fund_name', 'report_date'])\n    fund_agg['lag_assets'] = fund_agg.groupby('fund_name')['total_assets'].shift(1)\n    fund_agg['lag_hret'] = fund_agg.groupby('fund_name')['holdings_return'].shift(1)\n    \n    # Net flows\n    fund_agg['net_flows'] = (fund_agg['total_assets'] - \n                              fund_agg['lag_assets'] * (1 + fund_agg['holdings_return']))\n    \n    # Turnover (Carhart definition)\n    fund_agg['avg_assets'] = (fund_agg['total_assets'] + fund_agg['lag_assets']) / 2\n    fund_agg['turnover'] = (\n        fund_agg[['total_buys', 'total_sales']].min(axis=1) / fund_agg['avg_assets']\n    )\n    \n    # Annualize (approximate, since disclosure may be semi-annual)\n    fund_agg['periods_per_year'] = 365 / fund_agg.groupby('fund_name')['report_date'].diff().dt.days\n    fund_agg['turnover_annual'] = fund_agg['turnover'] * fund_agg['periods_per_year'].fillna(2)\n    \n    print(f\"Fund analytics computed:\")\n    print(f\"  Unique funds: {fund_agg['fund_name'].nunique():,}\")\n    print(f\"  Fund-period observations: {len(fund_agg):,}\")\n    print(f\"\\nTurnover statistics:\")\n    print(fund_agg[['turnover', 'turnover_annual']].describe().round(4))\n    \n    return {\n        'fund_assets': fund_assets,\n        'fund_trades': fund_trades,\n        'fund_aggregates': fund_agg,\n    }\n\n# fund_analytics = compute_fund_analytics(dc.fund_holdings, prices_q, adj_factors)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-state-ownership",
    "href": "30_institutional_ownership.html#sec-state-ownership",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.9 State Ownership Analysis",
    "text": "18.9 State Ownership Analysis\n\n18.9.1 Equitization and the Decline of State Ownership\nVietnam’s equitization (cổ phần hóa) program has been a defining feature of the market since the early 2000s. The program converts state-owned enterprises into joint-stock companies, typically with the state retaining a controlling or significant minority stake that is then gradually reduced through secondary offerings.\n\n# ============================================================================\n# Step 9: State Ownership Analysis\n# ============================================================================\n\ndef analyze_state_ownership(metrics: pd.DataFrame) -&gt; Dict:\n    \"\"\"\n    Comprehensive analysis of state ownership in Vietnam.\n    \n    Computes:\n    1. Aggregate state ownership trends\n    2. SOE population dynamics (entry/exit from SOE classification)\n    3. Equitization event detection (large drops in state ownership)\n    4. State ownership by sector and size\n    5. Governance implications (state as blockholder)\n    \"\"\"\n    df = metrics.copy()\n    \n    # --- 1. Aggregate Trends ---\n    ts = df.groupby('quarter_end').agg(\n        n_soe=('is_soe', 'sum'),\n        n_total=('ticker', 'nunique'),\n        pct_soe=('is_soe', 'mean'),\n        mean_state_pct=('pct_state', 'mean'),\n        median_state_pct=('pct_state', 'median'),\n        # Market cap share of SOEs\n        soe_mktcap=('market_cap', lambda x: x[df.loc[x.index, 'is_soe'] == 1].sum()),\n        total_mktcap=('market_cap', 'sum'),\n    ).reset_index()\n    ts['soe_mktcap_share'] = ts['soe_mktcap'] / ts['total_mktcap']\n    \n    # --- 2. Equitization Events ---\n    # Detect large drops in state ownership (&gt;10 percentage points)\n    df_sorted = df.sort_values(['ticker', 'quarter_end'])\n    df_sorted['state_change'] = df_sorted.groupby('ticker')['pct_state'].diff()\n    \n    equitization_events = df_sorted[\n        df_sorted['state_change'] &lt; -0.10  # &gt; 10pp drop\n    ][['ticker', 'quarter_end', 'pct_state', 'state_change', 'market_cap']].copy()\n    \n    # --- 3. By Sector ---\n    if 'industry_code' in df.columns:\n        by_sector = df.groupby('industry_code').agg(\n            mean_state=('pct_state', 'mean'),\n            pct_soe=('is_soe', 'mean'),\n            n_firms=('ticker', 'nunique'),\n        ).sort_values('mean_state', ascending=False)\n    else:\n        by_sector = None\n    \n    print(f\"State Ownership Analysis:\")\n    print(f\"  Current SOE count: {ts.iloc[-1]['n_soe']:.0f} / {ts.iloc[-1]['n_total']:.0f}\")\n    print(f\"  SOE market cap share: {ts.iloc[-1]['soe_mktcap_share']:.1%}\")\n    print(f\"  Mean state ownership: {ts.iloc[-1]['mean_state_pct']:.1%}\")\n    print(f\"\\nEquitization events detected: {len(equitization_events):,}\")\n    \n    return {\n        'trends': ts,\n        'equitization_events': equitization_events,\n        'by_sector': by_sector,\n    }\n\n# state_analysis = analyze_state_ownership(io_metrics)\n\n\n\n\ndef plot_state_ownership(state_analysis: Dict, metrics: pd.DataFrame):\n    \"\"\"Plot state ownership dynamics.\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n    ts = state_analysis['trends']\n    \n    # Panel A: SOE trends\n    ax = axes[0]\n    ax.plot(ts['quarter_end'], ts['pct_soe'] * 100, \n            label='% of Firms that are SOEs', linewidth=2, color='#d62728')\n    ax.plot(ts['quarter_end'], ts['soe_mktcap_share'] * 100,\n            label='SOE Market Cap Share (%)', linewidth=2, color='#1f77b4')\n    ax.plot(ts['quarter_end'], ts['mean_state_pct'] * 100,\n            label='Mean State Ownership (%)', linewidth=2, color='#2ca02c', linestyle='--')\n    ax.set_ylabel('Percentage')\n    ax.set_title('Panel A: State Ownership and SOE Prevalence Over Time')\n    ax.legend(frameon=True, framealpha=0.9)\n    \n    # Panel B: Distribution\n    ax = axes[1]\n    # Use most recent period\n    latest = metrics[metrics['quarter_end'] == metrics['quarter_end'].max()]\n    state_pct = latest['pct_state'].dropna() * 100\n    \n    ax.hist(state_pct, bins=50, color='#d62728', alpha=0.7, edgecolor='black')\n    ax.axvline(x=50, color='black', linestyle='--', alpha=0.7, label='50% (SOE threshold)')\n    ax.set_xlabel('State Ownership (%)')\n    ax.set_ylabel('Number of Companies')\n    ax.set_title('Panel B: Distribution of State Ownership (Most Recent Quarter)')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig('fig_state_ownership.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_state_ownership(state_analysis, io_metrics)\n\n\nFigure 18.4",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-modern-extensions",
    "href": "30_institutional_ownership.html#sec-modern-extensions",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.10 Modern Extensions",
    "text": "18.10 Modern Extensions\n\n18.10.1 Network Analysis of Co-Ownership\nInstitutional co-ownership networks capture how stocks are connected through shared investors. In Vietnam, these networks reveal the influence structure of major domestic conglomerates (e.g., Vingroup, Masan, FPT) and the overlap between foreign fund portfolios.\n\ndef construct_stock_coownership_network(ownership: pd.DataFrame,\n                                         period: str,\n                                         min_overlap: int = 3) -&gt; Dict:\n    \"\"\"\n    Construct a stock-level co-ownership network.\n    \n    Two stocks are connected if they share institutional investors.\n    Edge weight = number of shared institutional investors.\n    \n    This is particularly informative in Vietnam where:\n    - Foreign fund portfolios concentrate on the same blue-chips\n    - Conglomerate cross-holdings create explicit linkages\n    - State ownership creates implicit connections (SCIC holds multiple stocks)\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership data\n    period : str\n        Analysis date\n    min_overlap : int\n        Minimum shared investors to create an edge\n    \n    Returns\n    -------\n    dict with network statistics and adjacency data\n    \"\"\"\n    import networkx as nx\n    \n    date = pd.Timestamp(period)\n    \n    # Get institutional holders for this period\n    inst = ownership[\n        (ownership['date'] == date) &\n        (ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL))\n    ][['ticker', 'shareholder_name', 'owner_type']].copy()\n    \n    # Create bipartite mapping: institution → set of stocks held\n    inst_to_stocks = inst.groupby('shareholder_name')['ticker'].apply(set).to_dict()\n    \n    # Stock → set of institutions\n    stock_to_inst = inst.groupby('ticker')['shareholder_name'].apply(set).to_dict()\n    \n    # Build stock-level network\n    stocks = list(stock_to_inst.keys())\n    G = nx.Graph()\n    \n    for i in range(len(stocks)):\n        for j in range(i + 1, len(stocks)):\n            shared = stock_to_inst[stocks[i]] & stock_to_inst[stocks[j]]\n            if len(shared) &gt;= min_overlap:\n                G.add_edge(stocks[i], stocks[j], weight=len(shared),\n                           shared_investors=list(shared)[:5])  # Store sample\n    \n    # Add node attributes\n    for stock in stocks:\n        if stock in G.nodes:\n            G.nodes[stock]['n_inst_holders'] = len(stock_to_inst[stock])\n    \n    # Network statistics\n    stats = {\n        'n_nodes': G.number_of_nodes(),\n        'n_edges': G.number_of_edges(),\n        'density': nx.density(G) if G.number_of_nodes() &gt; 1 else 0,\n        'avg_clustering': nx.average_clustering(G, weight='weight') if G.number_of_nodes() &gt; 0 else 0,\n        'n_components': nx.number_connected_components(G),\n    }\n    \n    # Centrality measures\n    if G.number_of_nodes() &gt; 0:\n        degree_cent = nx.degree_centrality(G)\n        stats['most_connected'] = sorted(degree_cent.items(), \n                                          key=lambda x: x[1], reverse=True)[:10]\n        \n        if G.number_of_nodes() &gt; 2:\n            try:\n                eigen_cent = nx.eigenvector_centrality_numpy(G, weight='weight')\n                stats['most_central'] = sorted(eigen_cent.items(),\n                                                key=lambda x: x[1], reverse=True)[:10]\n            except Exception:\n                stats['most_central'] = []\n    \n    print(f\"Co-Ownership Network ({period}):\")\n    for k, v in stats.items():\n        if k not in ['most_connected', 'most_central']:\n            print(f\"  {k}: {v}\")\n    \n    if 'most_connected' in stats:\n        print(f\"\\nMost connected stocks:\")\n        for stock, cent in stats['most_connected'][:5]:\n            print(f\"  {stock}: {cent:.3f}\")\n    \n    return {'graph': G, 'stats': stats}\n\n# network = construct_stock_coownership_network(\n#     ownership_classified, '2024-06-30'\n# )\n\n\n\n18.10.2 ML-Enhanced Investor Classification\nVietnam’s investor classification challenge is distinct from the US. While the US has the Bushee typology based on portfolio turnover and concentration, Vietnam requires classification of both investor type (when not explicitly labeled) and investor behavior (active vs passive, short-term vs long-term).\n\ndef classify_investors_vietnam(ownership: pd.DataFrame,\n                                prices_q: pd.DataFrame,\n                                n_clusters: int = 4) -&gt; pd.DataFrame:\n    \"\"\"\n    ML-based classification of Vietnamese institutional investors.\n    \n    Features adapted for Vietnam's market:\n    1. Portfolio concentration (HHI of holdings)\n    2. Holding duration (average time in positions)\n    3. Size preference (average market cap of holdings)\n    4. Sector concentration\n    5. Foreign/domestic indicator\n    6. Trading frequency (inverse of average days between disclosures)\n    \n    Expected clusters for Vietnam:\n    - Passive State Holders: SOE parents, SCIC - low turnover, concentrated\n    - Active Foreign Funds: Dragon Capital, VinaCapital - moderate turnover\n    - Domestic Securities Firms: SSI, VNDirect - high turnover, diversified\n    - Long-Term Foreign: Pension funds, sovereign wealth - low turnover\n    \"\"\"\n    from sklearn.cluster import KMeans\n    from sklearn.preprocessing import StandardScaler\n    \n    inst = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)\n    ].copy()\n    \n    # Merge with price data\n    inst = inst.merge(\n        prices_q[['ticker', 'quarter_end', 'close', 'market_cap']],\n        left_on=['ticker', 'date'],\n        right_on=['ticker', 'quarter_end'],\n        how='left'\n    )\n    \n    inst['holding_value'] = inst['shares_held'] * inst['close'].fillna(0)\n    \n    # Compute features per investor-period\n    features = inst.groupby(['shareholder_name', 'date']).agg(\n        n_stocks=('ticker', 'nunique'),\n        total_value=('holding_value', 'sum'),\n        hhi_portfolio=('holding_value', \n                        lambda x: ((x/x.sum())**2).sum() if x.sum() &gt; 0 else np.nan),\n        avg_mktcap=('market_cap', 'mean'),\n        is_foreign=('owner_type', \n                     lambda x: (x == OwnershipType.FOREIGN_INST).any().astype(int)),\n        is_state=('owner_type', \n                   lambda x: (x == OwnershipType.STATE).any().astype(int)),\n    ).reset_index()\n    \n    # Average across all periods per investor\n    investor_features = features.groupby('shareholder_name').agg(\n        avg_n_stocks=('n_stocks', 'mean'),\n        avg_hhi=('hhi_portfolio', 'mean'),\n        avg_mktcap=('avg_mktcap', 'mean'),\n        avg_total_value=('total_value', 'mean'),\n        is_foreign=('is_foreign', 'max'),\n        is_state=('is_state', 'max'),\n        n_periods=('date', 'nunique'),\n    ).dropna()\n    \n    # Feature matrix\n    feature_cols = ['avg_n_stocks', 'avg_hhi', 'avg_mktcap', 'avg_total_value']\n    X = investor_features[feature_cols].copy()\n    \n    # Log-transform\n    for col in feature_cols:\n        X[col] = np.log1p(X[col].clip(lower=0))\n    \n    # Add binary features\n    X['is_foreign'] = investor_features['is_foreign']\n    X['is_state'] = investor_features['is_state']\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # K-means\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n    investor_features['cluster'] = kmeans.fit_predict(X_scaled)\n    \n    # Label clusters\n    cluster_profiles = investor_features.groupby('cluster').agg({\n        'avg_n_stocks': 'mean',\n        'avg_hhi': 'mean',\n        'avg_total_value': 'mean',\n        'is_foreign': 'mean',\n        'is_state': 'mean',\n        'shareholder_name': 'count',\n    }).rename(columns={'shareholder_name': 'n_investors'})\n    \n    print(\"Investor Clusters:\")\n    print(cluster_profiles.round(3).to_string())\n    \n    return investor_features\n\n# investor_classes = classify_investors_vietnam(ownership_classified, prices_q)\n\n\n\n18.10.3 Event Study: Ownership Disclosure Shocks\nVietnam’s threshold-based major shareholder disclosure creates natural events for studying the price impact of ownership changes.\n\ndef ownership_event_study(major_shareholders: pd.DataFrame,\n                           prices: pd.DataFrame,\n                           event_window: Tuple[int, int] = (-5, 20),\n                           estimation_window: int = 120) -&gt; pd.DataFrame:\n    \"\"\"\n    Event study of ownership disclosure announcements.\n    \n    Vietnam requires major shareholders (≥5%) to disclose within 7 \n    business days of crossing ownership thresholds. These disclosures \n    can be informationally significant, especially:\n    1. Foreign fund accumulation (signal of quality)\n    2. State divestiture (equitization signal)\n    3. Insider purchases (management confidence signal)\n    \n    Uses market model for expected returns:\n    E[R_i,t] = α_i + β_i × R_m,t\n    \n    Parameters\n    ----------\n    major_shareholders : pd.DataFrame\n        Disclosure events from DataCore.vn\n    prices : pd.DataFrame\n        Daily stock prices\n    event_window : tuple\n        (pre_event_days, post_event_days)\n    estimation_window : int\n        Days before event window for market model estimation\n    \"\"\"\n    events = major_shareholders.copy()\n    events = events.sort_values(['ticker', 'date'])\n    \n    # Identify significant ownership changes\n    events['ownership_change'] = events.groupby(\n        ['ticker', 'shareholder_name']\n    )['ownership_pct'].diff()\n    \n    significant_events = events[\n        events['ownership_change'].abs() &gt; 0.01  # &gt; 1 percentage point\n    ].copy()\n    \n    significant_events['event_type'] = np.where(\n        significant_events['ownership_change'] &gt; 0, 'accumulation', 'divestiture'\n    )\n    \n    # Merge with daily prices\n    prices_daily = prices[['ticker', 'date', 'ret']].copy()\n    prices_daily = prices_daily.sort_values(['ticker', 'date'])\n    \n    # VN-Index as market return (ticker code depends on data provider)\n    if 'VNINDEX' in prices_daily['ticker'].values:\n        market_ret = prices_daily[prices_daily['ticker'] == 'VNINDEX'][['date', 'ret']].copy()\n        market_ret = market_ret.rename(columns={'ret': 'mkt_ret'})\n    else:\n        # Use equal-weighted market return as proxy\n        market_ret = (prices_daily.groupby('date')['ret']\n                                  .mean()\n                                  .reset_index()\n                                  .rename(columns={'ret': 'mkt_ret'}))\n    \n    # For each event, compute abnormal returns\n    results = []\n    pre, post = event_window\n    \n    for _, event in significant_events.iterrows():\n        ticker = event['ticker']\n        event_date = event['date']\n        \n        # Get stock returns around the event\n        stock_ret = prices_daily[prices_daily['ticker'] == ticker].copy()\n        stock_ret = stock_ret.merge(market_ret, on='date', how='left')\n        stock_ret = stock_ret.sort_values('date').reset_index(drop=True)\n        \n        # Find event date index\n        event_idx = stock_ret[stock_ret['date'] &gt;= event_date].index\n        if len(event_idx) == 0:\n            continue\n        event_idx = event_idx[0]\n        \n        # Estimation window\n        est_start = max(0, event_idx - estimation_window + pre)\n        est_end = event_idx + pre\n        est_data = stock_ret.iloc[est_start:est_end].dropna(subset=['ret', 'mkt_ret'])\n        \n        if len(est_data) &lt; 30:\n            continue\n        \n        # Market model\n        X = sm.add_constant(est_data['mkt_ret'])\n        y = est_data['ret']\n        try:\n            model = sm.OLS(y, X).fit()\n        except Exception:\n            continue\n        \n        # Event window abnormal returns\n        ew_start = event_idx + pre\n        ew_end = min(event_idx + post + 1, len(stock_ret))\n        event_data = stock_ret.iloc[ew_start:ew_end].copy()\n        \n        if len(event_data) == 0:\n            continue\n        \n        event_data['expected_ret'] = (model.params['const'] + \n                                       model.params['mkt_ret'] * event_data['mkt_ret'])\n        event_data['abnormal_ret'] = event_data['ret'] - event_data['expected_ret']\n        event_data['car'] = event_data['abnormal_ret'].cumsum()\n        event_data['event_day'] = range(pre, pre + len(event_data))\n        event_data['ticker'] = ticker\n        event_data['event_date'] = event_date\n        event_data['event_type'] = event['event_type']\n        event_data['ownership_change'] = event['ownership_change']\n        event_data['shareholder_name'] = event['shareholder_name']\n        \n        results.append(event_data)\n    \n    if results:\n        all_results = pd.concat(results, ignore_index=True)\n        \n        # Average CARs by event type\n        avg_car = (all_results.groupby(['event_type', 'event_day'])['car']\n                              .agg(['mean', 'std', 'count'])\n                              .reset_index())\n        avg_car['t_stat'] = avg_car['mean'] / (avg_car['std'] / np.sqrt(avg_car['count']))\n        \n        print(f\"Event Study Results:\")\n        print(f\"  Total events: {significant_events['event_type'].value_counts().to_string()}\")\n        \n        # CAR at event day 0, +5, +10, +20\n        for et in ['accumulation', 'divestiture']:\n            print(f\"\\n  {et.title()} Events:\")\n            subset = avg_car[avg_car['event_type'] == et]\n            for day in [0, 5, 10, 20]:\n                row = subset[subset['event_day'] == day]\n                if len(row) &gt; 0:\n                    print(f\"    CAR({day:+d}): {row.iloc[0]['mean']:.4f} \"\n                          f\"(t={row.iloc[0]['t_stat']:.2f})\")\n        \n        return all_results\n    \n    return pd.DataFrame()\n\n# event_results = ownership_event_study(dc.major_shareholders, dc.prices)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-empirical-applications",
    "href": "30_institutional_ownership.html#sec-empirical-applications",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.11 Empirical Applications",
    "text": "18.11 Empirical Applications\n\n18.11.1 Application 1: Foreign Ownership and Stock Returns in Vietnam\nDoes foreign institutional ownership predict returns in Vietnam? Huang, Liu, and Shu (2023) find evidence consistent with the information advantage hypothesis.\n\ndef test_foreign_io_returns(metrics: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Test whether changes in foreign institutional ownership predict \n    future stock returns in Vietnam.\n    \n    Methodology:\n    1. Sort stocks into quintiles by change in foreign IO\n    2. Compute equal-weighted and VN-Index-adjusted returns\n    3. Report portfolio returns and long-short spread\n    \n    This adapts the Chen, Hong, and Stein (2002) breadth test \n    specifically for Vietnam's foreign ownership component.\n    \"\"\"\n    df = metrics.copy()\n    df = df.sort_values(['ticker', 'quarter_end'])\n    \n    # Change in foreign IO\n    df['delta_foreign'] = df.groupby('ticker')['pct_foreign_total'].diff()\n    \n    # Forward quarterly return\n    df['fwd_ret'] = df.groupby('ticker')['ret'].shift(-1)\n    \n    # Drop missing\n    df = df.dropna(subset=['delta_foreign', 'fwd_ret'])\n    \n    # Quintile portfolios each quarter\n    df['foreign_quintile'] = df.groupby('quarter_end')['delta_foreign'].transform(\n        lambda x: pd.qcut(x, 5, labels=[1, 2, 3, 4, 5], duplicates='drop')\n    )\n    \n    # Portfolio returns\n    port_ret = (df.groupby(['quarter_end', 'foreign_quintile'])['fwd_ret']\n                  .mean()\n                  .reset_index())\n    \n    port_wide = port_ret.pivot(index='quarter_end', columns='foreign_quintile', \n                                values='fwd_ret')\n    port_wide['LS'] = port_wide[5] - port_wide[1]\n    \n    # Test significance\n    results = {}\n    for q in [1, 2, 3, 4, 5, 'LS']:\n        data = port_wide[q].dropna()\n        mean_ret = data.mean()\n        t_stat = mean_ret / (data.std() / np.sqrt(len(data)))\n        results[q] = {\n            'Mean Return (%)': mean_ret * 100,\n            't-statistic': t_stat,\n            'N quarters': len(data),\n        }\n    \n    results_df = pd.DataFrame(results).T\n    results_df.index.name = 'ΔForeign IO Quintile'\n    \n    print(\"Foreign Ownership Change and Future Returns (Vietnam)\")\n    print(\"=\" * 60)\n    print(results_df.round(3).to_string())\n    \n    return results_df\n\n# foreign_return_results = test_foreign_io_returns(io_metrics)\n\n\n\n18.11.2 Application 2: State Divestiture and Value Creation\n\ndef analyze_equitization_value(metrics: pd.DataFrame, \n                                state_analysis: Dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Test whether reductions in state ownership are associated with \n    subsequent value creation (higher returns, improved governance).\n    \n    Hypothesis: State divestiture reduces agency costs, improves \n    operational efficiency, and attracts institutional investors,\n    leading to positive abnormal returns.\n    \n    Uses a difference-in-differences approach:\n    Treatment: Firms experiencing &gt;10pp drop in state ownership\n    Control: Matched firms with stable state ownership\n    \"\"\"\n    df = metrics.copy()\n    events = state_analysis['equitization_events']\n    \n    if len(events) == 0:\n        print(\"No equitization events detected.\")\n        return pd.DataFrame()\n    \n    # Get treated firms and their event quarters\n    treated = events[['ticker', 'quarter_end']].drop_duplicates()\n    treated['treated'] = 1\n    \n    # Merge with metrics\n    df = df.merge(treated, on=['ticker', 'quarter_end'], how='left')\n    df['treated'] = df['treated'].fillna(0)\n    \n    # Pre/post comparison for treated firms\n    treated_tickers = treated['ticker'].unique()\n    \n    results = []\n    for ticker in treated_tickers:\n        firm = df[df['ticker'] == ticker].sort_values('quarter_end')\n        event_row = firm[firm['treated'] == 1]\n        if len(event_row) == 0:\n            continue\n        \n        event_q = event_row.iloc[0]['quarter_end']\n        \n        # Pre-event (4 quarters before)\n        pre = firm[firm['quarter_end'] &lt; event_q].tail(4)\n        # Post-event (4 quarters after)\n        post = firm[firm['quarter_end'] &gt; event_q].head(4)\n        \n        if len(pre) &lt; 2 or len(post) &lt; 2:\n            continue\n        \n        results.append({\n            'ticker': ticker,\n            'event_quarter': event_q,\n            'state_pct_pre': pre['pct_state'].mean(),\n            'state_pct_post': post['pct_state'].mean(),\n            'foreign_pct_pre': pre['pct_foreign_total'].mean(),\n            'foreign_pct_post': post['pct_foreign_total'].mean(),\n            'n_inst_pre': pre['n_inst_owners'].mean(),\n            'n_inst_post': post['n_inst_owners'].mean(),\n            'ret_pre': pre['ret'].mean(),\n            'ret_post': post['ret'].mean(),\n        })\n    \n    if results:\n        results_df = pd.DataFrame(results)\n        \n        # Paired t-tests\n        print(\"Equitization Value Analysis\")\n        print(\"=\" * 60)\n        for metric in ['state_pct', 'foreign_pct', 'n_inst', 'ret']:\n            pre_col = f'{metric}_pre'\n            post_col = f'{metric}_post'\n            diff = results_df[post_col] - results_df[pre_col]\n            t_stat, p_val = stats.ttest_1samp(diff.dropna(), 0)\n            print(f\"  Δ{metric}: {diff.mean():.4f} (t={t_stat:.2f}, p={p_val:.3f})\")\n        \n        return results_df\n    \n    return pd.DataFrame()\n\n# equitization_results = analyze_equitization_value(io_metrics, state_analysis)\n\n\n\n18.11.3 Application 3: Institutional Herding in Vietnam\n\ndef compute_herding_vietnam(trades: pd.DataFrame,\n                             owner_types: Optional[List[str]] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the Lakonishok, Shleifer, and Vishny (1992) herding measure\n    adapted for the Vietnamese market.\n    \n    Can be computed separately for:\n    - All institutional investors\n    - Foreign institutions only\n    - Domestic institutions only\n    \n    The herding measure captures whether institutions systematically\n    trade in the same direction beyond what chance would predict.\n    \"\"\"\n    from scipy.stats import binom\n    \n    t = trades.copy()\n    \n    if owner_types:\n        t = t[t['owner_type'].isin(owner_types)]\n    \n    t['is_buy'] = (t['trade'] &gt; 0).astype(int)\n    \n    # For each stock-period\n    stock_trades = t.groupby(['ticker', 'date']).agg(\n        n_traders=('shareholder_name', 'nunique'),\n        n_buyers=('is_buy', 'sum'),\n    ).reset_index()\n    \n    # Minimum traders threshold\n    stock_trades = stock_trades[stock_trades['n_traders'] &gt;= 3]\n    stock_trades['p_buy'] = stock_trades['n_buyers'] / stock_trades['n_traders']\n    \n    # Expected proportion per period\n    E_p = stock_trades.groupby('date').apply(\n        lambda g: g['n_buyers'].sum() / g['n_traders'].sum()\n    ).reset_index(name='E_p')\n    \n    stock_trades = stock_trades.merge(E_p, on='date')\n    \n    # Adjustment factor\n    def expected_abs_dev(n, p):\n        k = np.arange(0, n + 1)\n        probs = binom.pmf(k, n, p)\n        return np.sum(probs * np.abs(k / n - p))\n    \n    stock_trades['adj_factor'] = stock_trades.apply(\n        lambda r: expected_abs_dev(int(r['n_traders']), r['E_p']), axis=1\n    )\n    \n    stock_trades['hm'] = (np.abs(stock_trades['p_buy'] - stock_trades['E_p']) - \n                           stock_trades['adj_factor'])\n    \n    stock_trades['buy_herd'] = np.where(\n        stock_trades['p_buy'] &gt; stock_trades['E_p'], stock_trades['hm'], np.nan\n    )\n    stock_trades['sell_herd'] = np.where(\n        stock_trades['p_buy'] &lt; stock_trades['E_p'], stock_trades['hm'], np.nan\n    )\n    \n    # Time series of herding\n    ts_herding = stock_trades.groupby('date').agg(\n        mean_hm=('hm', 'mean'),\n        mean_buy_herd=('buy_herd', 'mean'),\n        mean_sell_herd=('sell_herd', 'mean'),\n        pct_herding=('hm', lambda x: (x &gt; 0).mean()),\n        n_stocks=('ticker', 'nunique'),\n    ).reset_index()\n    \n    print(f\"Herding Analysis ({owner_types or 'All Institutions'}):\")\n    print(f\"  Mean HM: {stock_trades['hm'].mean():.4f}\")\n    print(f\"  Mean Buy Herding: {stock_trades['buy_herd'].mean():.4f}\")\n    print(f\"  Mean Sell Herding: {stock_trades['sell_herd'].mean():.4f}\")\n    print(f\"  % stocks with herding: {(stock_trades['hm'] &gt; 0).mean():.1%}\")\n    \n    return stock_trades, ts_herding\n\n# herding_all, herding_ts = compute_herding_vietnam(trades)\n# herding_foreign, _ = compute_herding_vietnam(\n#     trades, owner_types=[OwnershipType.FOREIGN_INST]\n# )",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "30_institutional_ownership.html#sec-conclusion",
    "href": "30_institutional_ownership.html#sec-conclusion",
    "title": "18  Institutional Ownership Analytics in Vietnam",
    "section": "18.12 Conclusion and Practical Recommendations",
    "text": "18.12 Conclusion and Practical Recommendations\n\n18.12.1 Summary of Measures\nTable 18.5 summarizes all institutional ownership measures developed in this chapter for the Vietnamese market.\n\n\n\nTable 18.5: Summary of All Ownership Measures for Vietnam\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nDefinition\nKey Adaptation for Vietnam\nPython Function\n\n\n\n\nIO Ratio\nInst. shares / TSO\nDecomposed into state, foreign, domestic\ncompute_ownership_decomposition()\n\n\nHHI Concentration\n\\(\\sum w_j^2\\)\nSeparate HHI for total, non-state, foreign\ncompute_io_metrics_vietnam()\n\n\nΔBreadth\nLehavy-Sloan adjusted\nApplied to irregular disclosure intervals\ncompute_io_metrics_vietnam()\n\n\nFOL Utilization\nForeign % / FOL limit\nVietnam-specific; no US equivalent\nFOLAnalyzer\n\n\nFOL Premium\nPrice impact of FOL proximity\nCross-sectional regression approach\nFOLAnalyzer.estimate_fol_premium()\n\n\nTrades\nΔShares (corp-action adjusted)\nCritical: adjust for stock dividends\nderive_trades_vectorized_vietnam()\n\n\nFund Turnover\nmin(B,S)/avg(A)\nSemi-annual frequency; annualized\ncompute_fund_analytics()\n\n\nSOE Status\nState ownership &gt; 50%\nTracks equitization program\nanalyze_state_ownership()\n\n\nLSV Herding\n\\(|p - E[p]| - E[|p - E[p]|]\\)\nSeparate foreign vs domestic herding\ncompute_herding_vietnam()\n\n\nCo-Ownership Network\nShared institutional holders\nReveals conglomerate linkages\nconstruct_stock_coownership_network()\n\n\n\n\n\n\n\n\n18.12.2 Data Quality Checklist for Vietnam\n\n\n\n\n\n\nTipVietnam Data Quality Checklist\n\n\n\n\nCorporate actions: Have you built and applied adjustment factors for ALL stock dividends, bonus shares, splits, and rights issues?\nShareholder classification: Have you verified the owner type classification (state vs foreign vs domestic institutional vs individual)?\nFOL limits: Are sector-specific FOL limits correctly assigned (30% for banks, 49% standard, unlimited for some sectors)?\nDisclosure dates: Are you using the actual disclosure date (not the record date or ex-date) for ownership snapshots?\nTreasury shares: Are treasury shares excluded from ownership ratio denominators?\nUPCOM coverage: Does your sample include or exclude UPCOM stocks (which have weaker disclosure requirements)?\nCross-listings: Are you handling NVDR (Non-Voting Depository Receipts) if applicable after market reforms?\nName consistency: Are shareholder names standardized across disclosure periods (Vietnamese names can have multiple romanization forms)?\nTrade adjustment: When deriving trades between periods, have you adjusted previous shares for ALL intervening corporate actions?\nFund mandate changes: For fund analytics, have you accounted for fund mergers, closures, and mandate changes that affect time-series continuity?\n\n\n\n\n\n18.12.3 Comparison with US Framework\n\n\n\nTable 18.6: US vs Vietnam Institutional Ownership Framework Comparison\n\n\n\n\n\n\n\n\n\n\nDimension\nUS (WRDS/13F)\nVietnam (DataCore.vn)\n\n\n\n\nDisclosure\nQuarterly 13F (mandatory)\nAnnual reports + event-driven\n\n\nCoverage\nInstitutions &gt; $100M AUM\nAll shareholders in annual reports\n\n\nOwnership observed\nLong positions only\nComplete decomposition\n\n\nIO can exceed 100%\nYes (short selling)\nNo (by construction)\n\n\nPermanent ID\nCRSP PERMNO\nTicker (with manual tracking of changes)\n\n\nAdjustment factors\nCRSP cfacshr\nMust build from corporate actions\n\n\nInvestor classification\nLSEG typecode / Bushee\nState/Foreign/Domestic/Individual\n\n\nShort selling\nNot in 13F; exists in market\nVery limited; not a concern\n\n\nUnique features\n—\nFOL, SOE ownership, stock dividend frequency\n\n\n\n\n\n\n\n\n\n\n\n\nBao Dinh, Ngoc, and Van Nguyen Hong Tran. 2024. “Institutional Ownership and Stock Liquidity: Evidence from an Emerging Market.” SAGE Open 14 (1): 21582440241239116.\n\n\nBen-David, ITZHAK, Francesco Franzoni, Augustin Landier, and Rabih Moussawi. 2013. “Do Hedge Funds Manipulate Stock Prices?” The Journal of Finance 68 (6): 2383–2434.\n\n\nCarhart, Mark M. 1997. “On Persistence in Mutual Fund Performance.” The Journal of Finance 52 (1): 57–82.\n\n\nChen, Joseph, Harrison Hong, and Jeremy C Stein. 2002. “Breadth of Ownership and Stock Returns.” Journal of Financial Economics 66 (2-3): 171–205.\n\n\nHuang, Xiangqian, Clark Liu, and Tao Shu. 2023. “Factors and Anomalies in the Vietnamese Stock Market.” Pacific-Basin Finance Journal 82: 102176.\n\n\nLehavy, Reuven, and Richard G Sloan. 2008. “Investor Recognition and Stock Returns.” Review of Accounting Studies 13 (2): 327–61.\n\n\nVo, Xuan Vinh. 2015. “Foreign Ownership and Stock Return Volatility–Evidence from Vietnam.” Journal of Multinational Financial Management 30: 101–9.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html",
    "href": "31_institutional_trade_flow.html",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "",
    "text": "19.1 Measuring Institutional Ownership and Trading\nInstitutional investors play a pivotal role in price discovery, corporate governance, and market liquidity. Understanding how institutions trade and how much they trade provides insights into both asset pricing dynamics and the real effects of institutional monitoring. The seminal work of Grinblatt, Titman, and Wermers (1995) on mutual fund momentum trading, Wermers (2000) on fund performance decomposition, and Yan (2008) on the relationship between turnover and future returns all rely on accurately measured institutional trades, flows, and turnover.\nIn the United States, this research is enabled by the mandatory quarterly 13F filing system administered by the Securities and Exchange Commission (SEC). Every institutional investment manager with at least $100 million in qualifying assets must disclose their equity holdings within 45 days of each calendar quarter end. The Thomson-Reuters (now Refinitiv) 13F database, accessible through WRDS, provides the canonical data infrastructure for this literature.\nVietnam’s equity market presents a fundamentally different institutional landscape. This chapter adapts the core methodology for the Vietnamese context, addressing five critical differences:\nThe measurement of institutional ownership and trading activity has been a central concern in empirical finance since Gompers, Ishii, and Metrick (2003) documented the rise of institutional investors. The approach relies on comparing holdings snapshots across consecutive reporting periods to infer trades. If manager \\(j\\) holds \\(h_{j,i,t}\\) shares of stock \\(i\\) at time \\(t\\), then the inferred trade is:\n\\[\n\\Delta h_{j,i,t} = h_{j,i,t} - h_{j,i,t-1}\n\\tag{19.1}\\]\nwhere \\(\\Delta h_{j,i,t} &gt; 0\\) indicates a buy and \\(\\Delta h_{j,i,t} &lt; 0\\) indicates a sale. This simple differencing approach requires that holdings are observed at regular intervals (e.g., quarterly), share counts are adjusted for corporate actions between reporting dates, and entry and exit from the dataset are handled appropriately.\nChen, Jegadeesh, and Wermers (2000) introduced the concept of ownership breadth (i.e., the number of institutions holding a stock) and showed that changes in breadth predict future returns. Sias (2004) decomposed institutional demand into a herding component and an information component. Yan (2008) linked fund turnover to information-based trading and documented that high-turnover funds outperform, challenging the view that turnover reflects noise trading.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#trade-classification",
    "href": "31_institutional_trade_flow.html#trade-classification",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "19.2 Trade Classification",
    "text": "19.2 Trade Classification\nTable 19.1 shows four categories of trades:\n\n\n\nTable 19.1: Trade Classification Taxonomy\n\n\n\n\n\nCode\nType\nDescription\n\n\n\n\n\\(+1\\)\nInitiating Buy\nManager enters a new position\n\n\n\\(+2\\)\nIncremental Buy\nManager increases an existing position\n\n\n\\(-1\\)\nTerminating Sale\nManager completely exits a position\n\n\n\\(-2\\)\nRegular Sale\nManager reduces an existing position\n\n\n\n\n\n\nThis classification is informative because initiating buys and terminating sales represent discrete portfolio decisions with different information content from marginal position adjustments (Alexander, Cici, and Gibson 2007).",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#turnover-measures",
    "href": "31_institutional_trade_flow.html#turnover-measures",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "19.3 Turnover Measures",
    "text": "19.3 Turnover Measures\nThree standard turnover definitions have been used in the literature:\nCarhart (1997) Turnover. The minimum of aggregate buys and sales, normalized by average assets:\n\\[\n\\text{Turnover}^{C}_{j,t} = \\frac{\\min\\left(\\sum_i B_{j,i,t},\\, \\sum_i S_{j,i,t}\\right)}\n{\\frac{1}{2}\\left(A_{j,t} + A_{j,t-1}\\right)}\n\\tag{19.2}\\]\nwhere \\(B_{j,i,t}\\) and \\(S_{j,i,t}\\) are the dollar values of buys and sales of stock \\(i\\) by manager \\(j\\) in quarter \\(t\\), and \\(A_{j,t}\\) is total portfolio assets (Carhart 1997).\nFlow-Adjusted Turnover. Adds back the absolute value of net flows to account for flow-driven trading:\n\\[\n\\text{Turnover}^{F}_{j,t} = \\frac{\\min\\left(\\sum_i B_{j,i,t},\\, \\sum_i S_{j,i,t}\\right) + |\\text{NetFlow}_{j,t}|}\n{A_{j,t-1}}\n\\tag{19.3}\\]\nSymmetric Turnover. Uses the sum of buys and sales minus the absolute net flow:\n\\[\n\\text{Turnover}^{S}_{j,t} = \\frac{\\sum_i B_{j,i,t} + \\sum_i S_{j,i,t} - |\\text{NetFlow}_{j,t}|}\n{A_{j,t-1}}\n\\tag{19.4}\\]\nThe relationship between these measures depends on the correlation between discretionary trading and flow-induced trading (Pástor and Stambaugh 2003).",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#institutional-ownership-in-emerging-markets",
    "href": "31_institutional_trade_flow.html#institutional-ownership-in-emerging-markets",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "19.4 Institutional Ownership in Emerging Markets",
    "text": "19.4 Institutional Ownership in Emerging Markets\nThe emerging markets literature has documented several stylized facts about institutional ownership that differ from developed market findings. Aggarwal et al. (2011) documented that foreign institutional ownership improves corporate governance in emerging markets. For Vietnam specifically, Phung and Mishra (2016) examined the relationship between ownership structure and firm performance, while Vo (2015) studied the impact of foreign ownership on stock market liquidity.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#net-flows-and-performance-attribution",
    "href": "31_institutional_trade_flow.html#net-flows-and-performance-attribution",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "19.5 Net Flows and Performance Attribution",
    "text": "19.5 Net Flows and Performance Attribution\nNet flows measure the dollar amount of new money entering or leaving a fund:\n\\[\n\\text{NetFlow}_{j,t} = A_{j,t} - A_{j,t-1}(1 + R_{j,t}^p)\n\\tag{19.5}\\]\nwhere \\(R_{j,t}^p\\) is the portfolio return. This decomposition, due to Sirri and Tufano (1998), separates changes in fund assets into investment returns and investor capital allocation decisions. Coval and Stafford (2007) showed that flow-driven trades create price pressure, with fire sales by funds experiencing redemptions generating significant negative abnormal returns.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-reader",
    "href": "31_institutional_trade_flow.html#sec-reader",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "20.1 Data Reader Class",
    "text": "20.1 Data Reader Class\nWe begin by defining a unified data reader that handles file loading, date parsing, and basic validation:\n\n@dataclass\nclass DataCoreReader:\n    \"\"\"\n    Unified reader for DataCore.vn datasets stored locally.\n    \n    Supports Parquet (recommended) and CSV formats. Implements\n    lazy loading with caching to minimize memory footprint.\n    \n    Parameters\n    ----------\n    data_dir : str or Path\n        Directory containing DataCore.vn data files.\n    file_format : str\n        File format: 'parquet' or 'csv'.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; dc = DataCoreReader('/data/datacore', file_format='parquet')\n    &gt;&gt;&gt; prices = dc.prices\n    &gt;&gt;&gt; ownership = dc.ownership\n    \"\"\"\n    data_dir: Path\n    file_format: str = 'parquet'\n    _cache: Dict[str, pd.DataFrame] = field(\n        default_factory=dict, repr=False\n    )\n    \n    FILE_MAP: Dict[str, str] = field(default_factory=lambda: {\n        'prices': 'stock_prices',\n        'ownership': 'ownership_structure',\n        'major_shareholders': 'major_shareholders',\n        'corporate_actions': 'corporate_actions',\n        'company_profile': 'company_profile',\n        'financials': 'financial_statements',\n        'foreign_ownership': 'foreign_ownership',\n        'fund_holdings': 'fund_holdings',\n    }, repr=False)\n    \n    def __post_init__(self):\n        self.data_dir = Path(self.data_dir)\n        if not self.data_dir.exists():\n            raise FileNotFoundError(\n                f\"Data directory not found: {self.data_dir}\"\n            )\n    \n    def _read(self, key: str) -&gt; pd.DataFrame:\n        \"\"\"Read and cache a dataset with automatic date parsing.\"\"\"\n        if key in self._cache:\n            return self._cache[key]\n        \n        fname = self.FILE_MAP.get(key, key)\n        filepath = self.data_dir / f\"{fname}.{self.file_format}\"\n        \n        if not filepath.exists():\n            raise FileNotFoundError(\n                f\"Dataset not found: {filepath}\\n\"\n                f\"Available: \"\n                f\"{list(self.data_dir.glob(f'*.{self.file_format}'))}\"\n            )\n        \n        if self.file_format == 'parquet':\n            df = pd.read_parquet(filepath)\n        else:\n            df = pd.read_csv(filepath, parse_dates=True)\n        \n        # Auto-detect and parse date columns\n        date_cols = [\n            'date', 'ex_date', 'record_date', 'period',\n            'report_date', 'listing_date'\n        ]\n        for col in df.columns:\n            if col.lower() in date_cols or 'date' in col.lower():\n                try:\n                    df[col] = pd.to_datetime(df[col])\n                except (ValueError, TypeError):\n                    pass\n        \n        self._cache[key] = df\n        print(f\"  Loaded {key}: {len(df):,} rows x {len(df.columns)} cols\")\n        return df\n    \n    @property\n    def prices(self) -&gt; pd.DataFrame:\n        return self._read('prices')\n    \n    @property\n    def ownership(self) -&gt; pd.DataFrame:\n        return self._read('ownership')\n    \n    @property\n    def major_shareholders(self) -&gt; pd.DataFrame:\n        return self._read('major_shareholders')\n    \n    @property\n    def corporate_actions(self) -&gt; pd.DataFrame:\n        return self._read('corporate_actions')\n    \n    @property\n    def company_profile(self) -&gt; pd.DataFrame:\n        return self._read('company_profile')\n    \n    @property\n    def foreign_ownership(self) -&gt; pd.DataFrame:\n        return self._read('foreign_ownership')\n    \n    @property\n    def fund_holdings(self) -&gt; pd.DataFrame:\n        return self._read('fund_holdings')\n    \n    def clear_cache(self):\n        n = len(self._cache)\n        self._cache.clear()\n        print(f\"  Cleared {n} cached datasets\")\n\n# Initialize:\n# dc = DataCoreReader('/path/to/datacore_data', file_format='parquet')",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-price-adj",
    "href": "31_institutional_trade_flow.html#sec-price-adj",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "21.1 Price Data Extraction and Adjustment",
    "text": "21.1 Price Data Extraction and Adjustment\nVietnamese stock data requires careful adjustment for frequent corporate actions. Unlike the US where CRSP provides a cumulative adjustment factor (cfacpr, cfacshr), in Vietnam we must construct adjustment factors from the corporate actions history.\n\n\n\n\n\n\nNoteVietnamese Corporate Actions\n\n\n\nVietnamese firms commonly execute the following corporate actions, each requiring share count and/or price adjustment:\n\nStock dividend (co tuc bang co phieu): e.g., 20% stock dividend means 100 shares become 120 shares\nBonus shares (co phieu thuong): free shares distributed from retained earnings\nRights issue (phat hanh quyen mua): right to buy new shares at a discount\nStock split/reverse split (chia/gop co phieu): rare but occasionally used\n\n\n\n\ndef build_adjustment_factors(\n    corporate_actions: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Construct cumulative share adjustment factors from corporate actions.\n    \n    This is the Vietnamese equivalent of CRSP's cfacshr factor. For each\n    ticker, we compute a cumulative product of adjustment ratios from\n    corporate actions, working forward in time.\n    \n    The adjustment factor at date t converts historical share counts to\n    be comparable with current (post-action) share counts:\n    \n        shares_adjusted_t = shares_raw_t * cfacshr_t\n    \n    Parameters\n    ----------\n    corporate_actions : pd.DataFrame\n        Corporate actions with columns: ticker, ex_date, action_type,\n        ratio. The ratio field represents:\n        - Stock dividend 20%: ratio = 1.20\n        - 2:1 stock split: ratio = 2.00\n        - Bonus shares 10%: ratio = 1.10\n    \n    Returns\n    -------\n    pd.DataFrame\n        Adjustment factors: ticker, ex_date, cfacshr (cumulative).\n    \"\"\"\n    share_actions = corporate_actions[\n        corporate_actions['action_type'].isin([\n            'stock_dividend', 'bonus_shares', 'stock_split',\n            'reverse_split', 'rights_issue'\n        ])\n    ].copy()\n    \n    if share_actions.empty:\n        return pd.DataFrame(columns=['ticker', 'ex_date', 'cfacshr'])\n    \n    share_actions = share_actions.sort_values(['ticker', 'ex_date'])\n    \n    share_actions['cfacshr'] = (\n        share_actions\n        .groupby('ticker')['ratio']\n        .cumprod()\n    )\n    \n    return share_actions[['ticker', 'ex_date', 'cfacshr']].reset_index(\n        drop=True\n    )\n\n\ndef get_cfacshr_at_date(\n    ticker: str,\n    date: pd.Timestamp,\n    adj_factors: pd.DataFrame,\n) -&gt; float:\n    \"\"\"\n    Look up the cumulative share adjustment factor for a given\n    ticker and date. Returns 1.0 if no corporate actions occurred.\n    \"\"\"\n    mask = (\n        (adj_factors['ticker'] == ticker) &\n        (adj_factors['ex_date'] &lt;= date)\n    )\n    subset = adj_factors.loc[mask]\n    \n    if subset.empty:\n        return 1.0\n    return subset.iloc[-1]['cfacshr']\n\n\ndef adjust_shares_between_dates(\n    shares: float,\n    ticker: str,\n    date_from: pd.Timestamp,\n    date_to: pd.Timestamp,\n    adj_factors: pd.DataFrame,\n) -&gt; float:\n    \"\"\"\n    Adjust a share count observed at date_from to be comparable\n    with shares observed at date_to, accounting for all intervening\n    corporate actions.\n    \n    Example\n    -------\n    &gt;&gt;&gt; # Investor held 1000 shares on 2023-01-01\n    &gt;&gt;&gt; # A 20% stock dividend occurred on 2023-03-15\n    &gt;&gt;&gt; adjust_shares_between_dates(\n    ...     1000, 'VNM',\n    ...     pd.Timestamp('2023-01-01'),\n    ...     pd.Timestamp('2023-06-30'), adj_factors\n    ... )\n    1200.0\n    \"\"\"\n    factor_from = get_cfacshr_at_date(ticker, date_from, adj_factors)\n    factor_to = get_cfacshr_at_date(ticker, date_to, adj_factors)\n    relative_factor = factor_to / factor_from\n    return shares * relative_factor",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-price-processing",
    "href": "31_institutional_trade_flow.html#sec-price-processing",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "21.2 Monthly and Quarterly Price Processing",
    "text": "21.2 Monthly and Quarterly Price Processing\n\ndef process_prices(\n    prices: pd.DataFrame,\n    adj_factors: pd.DataFrame,\n    begdate: str = '2010-01-01',\n    enddate: str = '2024-12-31',\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Process raw DataCore.vn price data into analysis-ready format.\n    \n    Block logic:\n    1. Filter to date range\n    2. Compute adjusted prices and shares outstanding\n    3. Compute quarterly compounded returns\n    4. Create forward quarterly returns (shifted one quarter)\n    \n    Parameters\n    ----------\n    prices : pd.DataFrame\n        Raw price data with: ticker, date, close, adjusted_close,\n        volume, shares_outstanding.\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors.\n    begdate, enddate : str\n        Sample period boundaries.\n    \n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        (price_quarterly, qret): quarter-end observations with\n        adjusted price, total shares, and forward quarterly return.\n    \"\"\"\n    price = prices[\n        (prices['date'] &gt;= begdate) & (prices['date'] &lt;= enddate)\n    ].copy()\n    \n    # Month-end and quarter-end dates\n    price['mdate'] = price['date'] + pd.offsets.MonthEnd(0)\n    price['qdate'] = price['date'] + pd.offsets.QuarterEnd(0)\n    \n    # Adjusted price\n    if 'adjusted_close' in price.columns:\n        price['p'] = price['adjusted_close']\n    else:\n        price['p'] = price['close']\n    \n    # Total shares outstanding\n    price['tso'] = price['shares_outstanding']\n    \n    # Market capitalization (millions VND)\n    price['mcap'] = price['p'] * price['tso'] / 1e6\n    \n    # Filter out zero shares\n    price = price[price['tso'] &gt; 0].copy()\n    \n    # Compute daily returns if not present\n    if 'ret' not in price.columns:\n        price = price.sort_values(['ticker', 'date'])\n        price['ret'] = price.groupby('ticker')['p'].pct_change()\n    \n    price['ret'] = price['ret'].fillna(0)\n    price['logret'] = np.log(1 + price['ret'])\n    \n    # ---- Quarterly compounded returns ----\n    qret = (\n        price\n        .groupby(['ticker', 'qdate'])['logret']\n        .sum()\n        .reset_index()\n    )\n    qret['qret'] = np.exp(qret['logret']) - 1\n    \n    # Shift qdate back one quarter: make qret a *forward* return\n    qret['qdate'] = qret['qdate'] + pd.offsets.QuarterEnd(-1)\n    qret = qret.drop(columns=['logret'])\n    \n    # ---- Quarter-end observations ----\n    price_q = price[price['qdate'] == price['mdate']].copy()\n    price_q = price_q[['qdate', 'ticker', 'p', 'tso', 'mcap']].copy()\n    \n    # Merge forward quarterly return\n    price_q = price_q.merge(qret, on=['ticker', 'qdate'], how='left')\n    \n    # Build cfacshr lookup at each quarter-end\n    price_q['cfacshr'] = price_q.apply(\n        lambda row: get_cfacshr_at_date(\n            row['ticker'], row['qdate'], adj_factors\n        ),\n        axis=1\n    )\n    \n    return price_q, qret\n\n\n\n\n\n\n\nTipPerformance Optimization\n\n\n\nThe get_cfacshr_at_date function uses a row-wise lookup which can be slow for large datasets. For production use with millions of rows, vectorize using pd.merge_asof():\nprice_q = pd.merge_asof(\n    price_q.sort_values('qdate'),\n    adj_factors.sort_values('ex_date'),\n    by='ticker',\n    left_on='qdate',\n    right_on='ex_date',\n    direction='backward'\n).fillna({'cfacshr': 1.0})\n\n\nThe output is a quarterly panel of stock-level observations (@tbl-institutional-price-vars)\n\n\n\nTable 21.1: Quarter-End Price Panel Variables\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nticker\nStock ticker (e.g., VNM, VCB, FPT)\n\n\nqdate\nQuarter-end date\n\n\np\nAdjusted closing price (VND)\n\n\ntso\nTotal shares outstanding\n\n\nmcap\nMarket capitalization (millions VND)\n\n\nqret\nForward quarterly compounded return\n\n\ncfacshr\nCumulative share adjustment factor",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-taxonomy",
    "href": "31_institutional_trade_flow.html#sec-taxonomy",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "22.1 Ownership Taxonomy",
    "text": "22.1 Ownership Taxonomy\nWe define a classification system for Vietnamese shareholders that maps to the categories available in DataCore.vn:\n\nclass OwnershipType:\n    \"\"\"\n    Vietnamese ownership type classification.\n    \n    Vietnam's ownership structure is fundamentally different from the US:\n    \n    - **State** (Nha nuoc): SCIC, ministries, state-owned parents\n    - **Foreign Institutional** (To chuc nuoc ngoai): foreign funds,\n      ETFs, pension funds, insurance, sovereign wealth funds\n    - **Domestic Institutional** (To chuc trong nuoc): Vietnamese\n      securities companies, fund managers, banks, insurance\n    - **Individual** (Ca nhan): retail investors (domestic + foreign)\n    - **Treasury** (Co phieu quy): company repurchases\n    \"\"\"\n    \n    STATE = 'State'\n    FOREIGN_INST = 'Foreign Institutional'\n    DOMESTIC_INST = 'Domestic Institutional'\n    INDIVIDUAL = 'Individual'\n    TREASURY = 'Treasury'\n    \n    INSTITUTIONAL = [FOREIGN_INST, DOMESTIC_INST]\n    ALL_INSTITUTIONAL = [STATE, FOREIGN_INST, DOMESTIC_INST]\n    ALL_TYPES = [STATE, FOREIGN_INST, DOMESTIC_INST, INDIVIDUAL, TREASURY]\n    \n    STATE_KEYWORDS = [\n        'scic', 'state capital', 'bo', 'ubnd', 'tong cong ty',\n        'nha nuoc', 'state', 'government', \"people's committee\",\n        'ministry', 'vietnam national', 'vnpt', 'evn', 'pvn',\n    ]\n    \n    FOREIGN_KEYWORDS = [\n        'fund', 'investment', 'capital', 'asset management',\n        'securities', 'gic', 'templeton', 'dragon capital',\n        'vinacapital', 'mekong capital', 'kb securities',\n        'mirae asset', 'samsung', 'jp morgan', 'goldman',\n        'blackrock', 'vanguard', 'aberdeen', 'hsbc',\n    ]\n    \n    @classmethod\n    def classify(cls, row: pd.Series) -&gt; str:\n        \"\"\"Classify based on explicit flags, then keyword fallback.\"\"\"\n        if pd.notna(row.get('is_state')) and row['is_state']:\n            return cls.STATE\n        if pd.notna(row.get('is_foreign')) and row['is_foreign']:\n            if pd.notna(row.get('is_institution')) and row['is_institution']:\n                return cls.FOREIGN_INST\n            return cls.INDIVIDUAL\n        if pd.notna(row.get('is_institution')) and row['is_institution']:\n            return cls.DOMESTIC_INST\n        \n        name = str(row.get('shareholder_name', '')).lower()\n        if any(kw in name for kw in cls.STATE_KEYWORDS):\n            return cls.STATE\n        if any(kw in name for kw in cls.FOREIGN_KEYWORDS):\n            return cls.FOREIGN_INST\n        \n        return cls.INDIVIDUAL",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-holdings-panel",
    "href": "31_institutional_trade_flow.html#sec-holdings-panel",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "22.2 Building the Holdings Panel",
    "text": "22.2 Building the Holdings Panel\nWe construct the holdings panel (i.e., the Vietnamese equivalent of merging the 13F Type 1 and Type 3 datasets). The key steps are:\n\nIdentify the first available vintage for each shareholder-stock-report date combination.\nCompute reporting gaps to flag first and last reports.\nClassify shareholders.\nAdjust shares for corporate actions.\n\n\ndef build_holdings_panel(\n    ownership: pd.DataFrame,\n    adj_factors: pd.DataFrame,\n    price_q: pd.DataFrame,\n    company_profile: pd.DataFrame,\n    begdate: str = '2010-01-01',\n    enddate: str = '2024-12-31',\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Construct the institutional holdings panel from DataCore.vn\n    ownership data.\n    \"\"\"\n    own = ownership.copy()\n    \n    # Align to quarter-end\n    own['rdate'] = own['date'] + pd.offsets.QuarterEnd(0)\n    own['fdate'] = own['date']\n    \n    own = own[\n        (own['rdate'] &gt;= begdate) & (own['rdate'] &lt;= enddate)\n    ].copy()\n    \n    # Keep earliest vintage per shareholder-ticker-rdate\n    own = own.sort_values(\n        ['shareholder_name', 'ticker', 'rdate', 'fdate']\n    )\n    fst_vint = (\n        own\n        .groupby(['shareholder_name', 'ticker', 'rdate'])\n        .first()\n        .reset_index()\n    )\n    \n    # ---- Reporting gaps for first/last flags ----\n    fst_vint = fst_vint.sort_values(\n        ['shareholder_name', 'ticker', 'rdate']\n    )\n    \n    grp = fst_vint.groupby(['shareholder_name', 'ticker'])\n    fst_vint['lag_rdate'] = grp['rdate'].shift(1)\n    \n    fst_vint['qtr_gap'] = fst_vint.apply(\n        lambda r: (\n            (r['rdate'].to_period('Q')\n             - r['lag_rdate'].to_period('Q')).n\n            if pd.notna(r['lag_rdate']) else np.nan\n        ),\n        axis=1\n    )\n    \n    fst_vint['first_report'] = (\n        fst_vint['qtr_gap'].isna() | (fst_vint['qtr_gap'] &gt;= 2)\n    )\n    \n    # Last report flag (forward gap)\n    fst_vint = fst_vint.sort_values(\n        ['shareholder_name', 'ticker', 'rdate'],\n        ascending=[True, True, False]\n    )\n    fst_vint['lead_rdate'] = grp['rdate'].shift(1)\n    \n    fst_vint['lead_gap'] = fst_vint.apply(\n        lambda r: (\n            (r['lead_rdate'].to_period('Q')\n             - r['rdate'].to_period('Q')).n\n            if pd.notna(r['lead_rdate']) else np.nan\n        ),\n        axis=1\n    )\n    \n    fst_vint['last_report'] = (\n        fst_vint['lead_gap'].isna() | (fst_vint['lead_gap'] &gt;= 2)\n    )\n    \n    fst_vint = fst_vint.drop(\n        columns=['lag_rdate', 'qtr_gap', 'lead_rdate', 'lead_gap'],\n        errors='ignore'\n    )\n    \n    # ---- Classify shareholders ----\n    fst_vint['owner_type'] = fst_vint.apply(\n        OwnershipType.classify, axis=1\n    )\n    \n    # ---- Adjust shares for corporate actions ----\n    fst_vint = fst_vint.merge(\n        price_q[['ticker', 'qdate', 'cfacshr']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    fst_vint['shares_adj'] = (\n        fst_vint['shares_held'] * fst_vint['cfacshr']\n    )\n    fst_vint = fst_vint[fst_vint['shares_adj'] &gt; 0].copy()\n    \n    fst_vint = fst_vint.drop_duplicates(\n        subset=['shareholder_name', 'ticker', 'rdate']\n    )\n    \n    # Merge company profile\n    if company_profile is not None:\n        fst_vint = fst_vint.merge(\n            company_profile[['ticker', 'exchange', 'fol_limit']]\n            .drop_duplicates(),\n            on='ticker',\n            how='left'\n        )\n    \n    cols = [\n        'shareholder_name', 'ticker', 'rdate', 'fdate',\n        'shares_held', 'shares_adj', 'owner_type',\n        'first_report', 'last_report'\n    ]\n    if 'exchange' in fst_vint.columns:\n        cols.extend(['exchange', 'fol_limit'])\n    \n    holdings = fst_vint[cols].copy()\n    \n    print(f\"Holdings panel: {len(holdings):,} observations\")\n    print(f\"  Shareholders: {holdings['shareholder_name'].nunique():,}\")\n    print(f\"  Stocks: {holdings['ticker'].nunique():,}\")\n    print(f\"  Quarters: {holdings['rdate'].nunique()}\")\n    \n    return holdings",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-io-ratio",
    "href": "31_institutional_trade_flow.html#sec-io-ratio",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "23.1 Institutional Ownership Ratio",
    "text": "23.1 Institutional Ownership Ratio\nThe institutional ownership ratio (IO) for stock \\(i\\) at time \\(t\\) is:\n\\[\nIO_{i,t} = \\frac{\\sum_{j \\in \\mathcal{J}} h_{j,i,t}}{TSO_{i,t}}\n\\tag{23.1}\\]\nwhere \\(\\mathcal{J}\\) is the set of institutional investors and \\(TSO_{i,t}\\) is total shares outstanding. In Vietnam, we compute separate ratios for each ownership type:\n\\[\nIO_{i,t}^{\\text{type}} = \\frac{\\sum_{j \\in \\mathcal{J}^{\\text{type}}} h_{j,i,t}}{TSO_{i,t}},\n\\quad \\text{type} \\in \\{\\text{State}, \\text{Foreign}, \\text{Domestic}, \\text{Individual}\\}\n\\tag{23.2}\\]\n\ndef compute_io_ratios(\n    holdings: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute IO ratios by type for each stock-quarter.\"\"\"\n    agg = (\n        holdings\n        .groupby(['ticker', 'rdate', 'owner_type'])['shares_adj']\n        .sum()\n        .reset_index()\n    )\n    \n    io_wide = agg.pivot_table(\n        index=['ticker', 'rdate'],\n        columns='owner_type',\n        values='shares_adj',\n        fill_value=0\n    ).reset_index()\n    \n    io_wide.columns = [\n        c if c in ['ticker', 'rdate']\n        else f'shares_{c.lower().replace(\" \", \"_\")}'\n        for c in io_wide.columns\n    ]\n    \n    io_wide = io_wide.merge(\n        price_q[['ticker', 'qdate', 'tso']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    share_cols = [c for c in io_wide.columns if c.startswith('shares_')]\n    for col in share_cols:\n        ratio_name = col.replace('shares_', 'io_')\n        io_wide[ratio_name] = io_wide[col] / io_wide['tso']\n    \n    inst_cols = [\n        c for c in io_wide.columns\n        if c.startswith('shares_')\n        and 'individual' not in c\n        and 'treasury' not in c\n    ]\n    io_wide['io_total_inst'] = (\n        io_wide[inst_cols].sum(axis=1) / io_wide['tso']\n    )\n    \n    return io_wide",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-hhi",
    "href": "31_institutional_trade_flow.html#sec-hhi",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "23.2 Ownership Concentration: Herfindahl-Hirschman Index",
    "text": "23.2 Ownership Concentration: Herfindahl-Hirschman Index\nThe HHI measures ownership concentration:\n\\[\nHHI_{i,t} = \\sum_{j=1}^{N_{i,t}} \\left(\\frac{h_{j,i,t}}{\\sum_{k=1}^{N_{i,t}} h_{k,i,t}}\\right)^2\n\\tag{23.3}\\]\nwhere \\(N_{i,t}\\) is the number of shareholders. HHI ranges from \\(1/N_{i,t}\\) (equal) to 1 (single shareholder). In Vietnam, ownership tends to be highly concentrated due to large state and founding-family blocks.\n\ndef compute_hhi(holdings: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute HHI for each stock-quarter, overall and institutional.\"\"\"\n    def _hhi(shares: pd.Series) -&gt; float:\n        total = shares.sum()\n        if total &lt;= 0:\n            return np.nan\n        weights = shares / total\n        return (weights ** 2).sum()\n    \n    hhi_overall = (\n        holdings.groupby(['ticker', 'rdate'])['shares_adj']\n        .apply(_hhi).reset_index()\n        .rename(columns={'shares_adj': 'hhi_overall'})\n    )\n    \n    inst = holdings[\n        holdings['owner_type'].isin(OwnershipType.ALL_INSTITUTIONAL)\n    ]\n    hhi_inst = (\n        inst.groupby(['ticker', 'rdate'])['shares_adj']\n        .apply(_hhi).reset_index()\n        .rename(columns={'shares_adj': 'hhi_institutional'})\n    )\n    \n    return hhi_overall.merge(hhi_inst, on=['ticker', 'rdate'], how='left')",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-breadth",
    "href": "31_institutional_trade_flow.html#sec-breadth",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "23.3 Ownership Breadth",
    "text": "23.3 Ownership Breadth\nFollowing Chen, Jegadeesh, and Wermers (2000), ownership breadth is the number of institutional holders:\n\\[\n\\text{Breadth}_{i,t} = \\#\\{j : h_{j,i,t} &gt; 0, \\, j \\in \\mathcal{J}\\}\n\\tag{23.4}\\]\nThe change in breadth predicts future returns:\n\\[\n\\Delta\\text{Breadth}_{i,t} = \\text{Breadth}_{i,t} - \\text{Breadth}_{i,t-1}\n\\tag{23.5}\\]\n\ndef compute_breadth(holdings: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute ownership breadth and changes by type.\"\"\"\n    breadth = (\n        holdings[\n            holdings['owner_type'].isin(OwnershipType.ALL_INSTITUTIONAL)\n        ]\n        .groupby(['ticker', 'rdate', 'owner_type'])['shareholder_name']\n        .nunique()\n        .reset_index()\n        .rename(columns={'shareholder_name': 'n_holders'})\n    )\n    \n    breadth_wide = breadth.pivot_table(\n        index=['ticker', 'rdate'],\n        columns='owner_type',\n        values='n_holders',\n        fill_value=0\n    ).reset_index()\n    \n    breadth_wide.columns = [\n        c if c in ['ticker', 'rdate']\n        else f'n_{c.lower().replace(\" \", \"_\")}'\n        for c in breadth_wide.columns\n    ]\n    \n    n_cols = [c for c in breadth_wide.columns if c.startswith('n_')]\n    breadth_wide['n_total_inst'] = breadth_wide[n_cols].sum(axis=1)\n    \n    breadth_wide = breadth_wide.sort_values(['ticker', 'rdate'])\n    for col in n_cols + ['n_total_inst']:\n        breadth_wide[f'd_{col}'] = (\n            breadth_wide.groupby('ticker')[col].diff()\n        )\n    \n    return breadth_wide\n\n(\\(\\text{BS} = -1\\)) is generated for the prior position, dated to the quarter after the last report.\nFor intermediate gaps (reports at \\(t-2\\) and \\(t\\) but not \\(t-1\\)), we split into:\n\nA terminating sale at \\(t-1\\) of \\(-h_{j,i,t-2}^{\\text{adj}}\\);\nAn initiating buy at \\(t\\) of \\(h_{j,i,t}\\).",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-trade-impl",
    "href": "31_institutional_trade_flow.html#sec-trade-impl",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "23.4 Implementation",
    "text": "23.4 Implementation\n\ndef compute_trades(\n    holdings: pd.DataFrame,\n    adj_factors: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute institutional trades from holdings panel.\n    \n    Uses vectorized conditional logic (NOT apply()) for performance.\n    \n    Algorithm:\n    1. Sort holdings by shareholder, ticker, quarter\n    2. Compute lagged holdings and reporting gaps\n    3. Apply modified trade logic based on first_report, gap\n    4. Handle terminating sales and intermediate gaps\n    5. Append all trade records\n    \"\"\"\n    t1 = holdings.sort_values(\n        ['shareholder_name', 'ticker', 'rdate']\n    ).copy()\n    \n    # Previous holding quarter and shares\n    grp = t1.groupby(['shareholder_name', 'ticker'])\n    t1['phrdate'] = grp['rdate'].shift(1)\n    t1['pshares_adj'] = grp['shares_adj'].shift(1)\n    \n    # Raw trade\n    t1['trade'] = t1['shares_adj'] - t1['pshares_adj']\n    \n    # Quarter gap\n    t1['qtrgap'] = t1.apply(\n        lambda r: (\n            (r['rdate'].to_period('Q')\n             - r['phrdate'].to_period('Q')).n\n            if pd.notna(r['phrdate']) else np.nan\n        ),\n        axis=1\n    )\n    \n    # Boundary detection keys\n    t1['l_key'] = (\n        t1['shareholder_name'] + '_' + t1['ticker']\n    ).shift(1)\n    t1['n_key'] = (\n        t1['shareholder_name'] + '_' + t1['ticker']\n    ).shift(-1)\n    t1['curr_key'] = t1['shareholder_name'] + '_' + t1['ticker']\n    \n    # ---- Vectorized trade classification ----\n    is_new = (t1['curr_key'] != t1['l_key'])\n    not_first = ~t1['first_report']\n    consec = (t1['qtrgap'] == 1)\n    gap = (t1['qtrgap'] != 1) & t1['qtrgap'].notna()\n    \n    cond1   = is_new\n    cond1_1 = is_new & not_first\n    cond2_1 = (~is_new) & not_first & consec\n    cond2_2 = (~is_new) & not_first & gap\n    \n    # Modified trade amounts\n    t1['modtrade'] = t1['trade']\n    t1.loc[cond1, 'modtrade'] = np.nan\n    t1.loc[cond1_1, 'modtrade'] = t1.loc[cond1_1, 'shares_adj']\n    t1.loc[cond2_1, 'modtrade'] = t1.loc[cond2_1, 'trade']\n    t1.loc[cond2_2, 'modtrade'] = t1.loc[cond2_2, 'shares_adj']\n    \n    # Buy/sale classification\n    t1['buysale'] = np.nan\n    t1.loc[cond1_1, 'buysale'] = 1\n    t1.loc[cond2_1, 'buysale'] = (\n        2 * np.sign(t1.loc[cond2_1, 'trade'])\n    )\n    t1.loc[cond2_2, 'buysale'] = 1.5  # placeholder for split\n    \n    # ---- Handle intermediate gaps (buysale == 1.5) ----\n    t2 = t1[t1['buysale'] == 1.5].copy()\n    t2['rdate'] = t2['phrdate'] + pd.offsets.QuarterEnd(1)\n    t2['buysale'] = -1\n    t2['modtrade'] = -t2['pshares_adj']\n    \n    t1.loc[t1['buysale'] == 1.5, 'buysale'] = 1\n    \n    # ---- Terminating sales ----\n    is_last_combo = (t1['curr_key'] != t1['n_key'])\n    not_last_rpt = ~t1['last_report']\n    \n    t3 = t1[is_last_combo & not_last_rpt].copy()\n    t3['rdate'] = t3['rdate'] + pd.offsets.QuarterEnd(1)\n    t3['modtrade'] = -t3['shares_adj']\n    t3['buysale'] = -1\n    \n    # ---- Combine ----\n    trades = pd.concat([t1, t2, t3], ignore_index=True)\n    trades = trades[\n        (trades['modtrade'] != 0) &\n        trades['modtrade'].notna() &\n        trades['buysale'].notna()\n    ].copy()\n    \n    trades = trades[[\n        'rdate', 'shareholder_name', 'ticker', 'modtrade',\n        'buysale', 'owner_type', 'first_report', 'last_report'\n    ]].rename(columns={'modtrade': 'trade'})\n    \n    print(f\"\\nTrade computation complete:\")\n    print(f\"  Total records: {len(trades):,}\")\n    print(f\"  Initiating buys:  {(trades['buysale'] == 1).sum():,}\")\n    print(f\"  Incremental buys: {(trades['buysale'] == 2).sum():,}\")\n    print(f\"  Terminating sales:{(trades['buysale'] == -1).sum():,}\")\n    print(f\"  Regular sales:    {(trades['buysale'] == -2).sum():,}\")\n    \n    return trades\n\n\n23.4.1 Trade Visualization\n\n\n\n\nCode\ndef plot_trade_distribution(trades: pd.DataFrame):\n    \"\"\"Plot time series of trade types by quarter.\"\"\"\n    bs_labels = {\n        1: 'Initiating Buy', 2: 'Incremental Buy',\n        -1: 'Terminating Sale', -2: 'Regular Sale'\n    }\n    trades = trades.copy()\n    trades['trade_type'] = trades['buysale'].map(bs_labels)\n    \n    counts = (\n        trades\n        .groupby([pd.Grouper(key='rdate', freq='QE'), 'trade_type'])\n        .size()\n        .unstack(fill_value=0)\n    )\n    \n    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n    \n    buy_cols = [c for c in counts.columns if 'Buy' in c]\n    counts[buy_cols].plot(\n        kind='bar', stacked=True, ax=axes[0],\n        color=['#1f77b4', '#aec7e8'], width=0.8\n    )\n    axes[0].set_title('Panel A: Institutional Purchases', fontweight='bold')\n    axes[0].set_ylabel('Number of Trades')\n    \n    sale_cols = [c for c in counts.columns if 'Sale' in c]\n    counts[sale_cols].plot(\n        kind='bar', stacked=True, ax=axes[1],\n        color=['#d62728', '#ff9896'], width=0.8\n    )\n    axes[1].set_title('Panel B: Institutional Sales', fontweight='bold')\n    axes[1].set_ylabel('Number of Trades')\n    \n    for ax in axes:\n        ax.tick_params(axis='x', rotation=45)\n        for i, label in enumerate(ax.get_xticklabels()):\n            if i % 4 != 0:\n                label.set_visible(False)\n    \n    plt.tight_layout()\n    plt.show()\n\n# plot_trade_distribution(trades)\n\n\n\nFigure 23.1\n\n\n\n\n\n\n\nCode\ndef plot_net_trading_by_type(trades: pd.DataFrame, price_q: pd.DataFrame):\n    \"\"\"Plot net trading volume by owner type over time.\"\"\"\n    _t = trades.merge(\n        price_q[['ticker', 'qdate', 'p']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    _t['trade_vnd'] = _t['trade'] * _t['p'] / 1e9\n    \n    net = (\n        _t\n        .groupby([pd.Grouper(key='rdate', freq='QE'), 'owner_type'])\n        ['trade_vnd'].sum()\n        .unstack(fill_value=0)\n    )\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    for col in net.columns:\n        ax.plot(net.index, net[col], label=col,\n                color=OWNER_COLORS.get(col, '#333'), linewidth=1.5)\n    ax.axhline(y=0, color='black', linewidth=0.5)\n    ax.set_title('Net Institutional Trading by Ownership Type',\n                 fontweight='bold')\n    ax.set_ylabel('Net Trading (Billions VND)')\n    ax.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n\n# plot_net_trading_by_type(trades, price_q)\n\n\n\nFigure 23.2",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-assets",
    "href": "31_institutional_trade_flow.html#sec-assets",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "24.1 Total Assets and Portfolio Returns",
    "text": "24.1 Total Assets and Portfolio Returns\nFor each manager \\(j\\) and quarter \\(t\\), portfolio assets are:\n\\[\nA_{j,t} = \\sum_{i=1}^{N_{j,t}} h_{j,i,t} \\cdot P_{i,t}\n\\tag{24.1}\\]\nThe portfolio return assuming buy-and-hold is:\n\\[\nR_{j,t}^{p} = \\frac{\\sum_{i=1}^{N_{j,t}} h_{j,i,t} \\cdot P_{i,t} \\cdot r_{i,t+1}}\n{\\sum_{i=1}^{N_{j,t}} h_{j,i,t} \\cdot P_{i,t}}\n\\tag{24.2}\\]\n\ndef compute_assets_and_returns(\n    holdings: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute total portfolio assets and buy-and-hold returns.\"\"\"\n    _assets = holdings[\n        ['shareholder_name', 'ticker', 'rdate', 'shares_adj']\n    ].merge(\n        price_q[['ticker', 'qdate', 'p', 'qret']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    _assets['hold_per_stock'] = _assets['shares_adj'] * _assets['p'] / 1e6\n    _assets['next_value'] = (\n        _assets['shares_adj'] * _assets['p'] * _assets['qret']\n    )\n    _assets['curr_value'] = _assets['shares_adj'] * _assets['p']\n    \n    assets = (\n        _assets\n        .groupby(['shareholder_name', 'rdate'])\n        .agg(\n            assets=('hold_per_stock', 'sum'),\n            total_next=('next_value', 'sum'),\n            total_curr=('curr_value', 'sum'),\n        )\n        .reset_index()\n    )\n    \n    assets['pret'] = assets['total_next'] / assets['total_curr']\n    assets = assets.drop(columns=['total_next', 'total_curr'])\n    return assets",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-aggregate-buysales",
    "href": "31_institutional_trade_flow.html#sec-aggregate-buysales",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "24.2 Aggregate Buys and Sales",
    "text": "24.2 Aggregate Buys and Sales\nTotal buys and sales for manager \\(j\\) in quarter \\(t\\):\n\\[\nB_{j,t} = \\sum_{i : \\Delta h &gt; 0} \\Delta h_{j,i,t} \\cdot P_{i,t}, \\qquad\nS_{j,t} = \\sum_{i : \\Delta h &lt; 0} |\\Delta h_{j,i,t}| \\cdot P_{i,t}\n\\tag{24.3}\\]\nThe trade gain is:\n\\[\nG_{j,t} = \\sum_{i=1}^{N_{j,t}} \\Delta h_{j,i,t} \\cdot P_{i,t} \\cdot r_{i,t+1}\n\\tag{24.4}\\]\n\ndef compute_buys_sales(\n    trades: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute aggregate buys, sales, trade gains per manager-quarter.\"\"\"\n    _flows = trades.merge(\n        price_q[['ticker', 'qdate', 'p', 'qret']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    _flows['tbuys'] = (\n        _flows['trade'] * (_flows['trade'] &gt; 0).astype(float)\n        * _flows['p'] / 1e6\n    )\n    _flows['tsales'] = (\n        (-1) * _flows['trade'] * (_flows['trade'] &lt; 0).astype(float)\n        * _flows['p'] / 1e6\n    )\n    _flows['tgain'] = (\n        _flows['trade'] * _flows['p'] * _flows['qret'] / 1e6\n    )\n    \n    flows = (\n        _flows\n        .groupby(['shareholder_name', 'rdate'])\n        .agg(\n            tbuys=('tbuys', 'sum'),\n            tsales=('tsales', 'sum'),\n            tgain=('tgain', 'sum'),\n        )\n        .reset_index()\n    )\n    return flows",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-netflows",
    "href": "31_institutional_trade_flow.html#sec-netflows",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "25.1 Net Flows",
    "text": "25.1 Net Flows\nNet flows separate capital allocation decisions from investment returns:\n\\[\n\\text{NetFlow}_{j,t} = A_{j,t} - A_{j,t-1}(1 + R_{j,t}^p)\n\\tag{25.1}\\]\n\n\n\n\n\n\nWarningInterpreting Net Flows in Vietnam\n\n\n\nFor state entities or corporate cross-holders, “net flows” do not necessarily reflect investment decisions. State ownership changes often result from government policy (equitization, divestment programs). Interpretation should account for institutional context.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-turnover-measures",
    "href": "31_institutional_trade_flow.html#sec-turnover-measures",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "25.2 Three Turnover Measures",
    "text": "25.2 Three Turnover Measures\n\ndef compute_aggregates(\n    holdings: pd.DataFrame,\n    assets: pd.DataFrame,\n    flows: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute net flows and three turnover measures.\n    \n    1. Carhart (1997): min(buys, sales) / avg(assets)\n    2. Flow-adjusted: [min(buys, sales) + |net flows|] / lag assets\n    3. Symmetric: [buys + sales - |net flows|] / lag assets\n    \"\"\"\n    report_flags = (\n        holdings\n        .groupby(['shareholder_name', 'rdate'])\n        .agg(first_report=('first_report', 'any'),\n             last_report=('last_report', 'any'))\n        .reset_index()\n    )\n    \n    agg = report_flags.merge(\n        assets, on=['shareholder_name', 'rdate'], how='inner'\n    )\n    agg = agg.merge(\n        flows, on=['shareholder_name', 'rdate'], how='left'\n    )\n    \n    agg = agg.sort_values(['shareholder_name', 'rdate'])\n    \n    agg['assets_comp'] = agg['assets'] * (1 + agg['pret'].fillna(0))\n    \n    grp = agg.groupby('shareholder_name')\n    agg['lassets_comp'] = grp['assets_comp'].shift(1)\n    agg['lassets'] = grp['assets'].shift(1)\n    \n    # Trade gain return\n    agg['tgainret'] = agg['tgain'] / (agg['tbuys'] + agg['tsales'])\n    \n    # Net flows\n    agg['netflows'] = agg['assets'] - agg['lassets_comp']\n    \n    # Turnover 1: Carhart (1997)\n    agg['turnover1'] = (\n        agg[['tbuys', 'tsales']].min(axis=1) /\n        agg[['assets', 'lassets']].mean(axis=1)\n    )\n    \n    # Turnover 2: Flow-adjusted\n    agg['turnover2'] = (\n        (agg[['tbuys', 'tsales']].min(axis=1)\n         + agg['netflows'].abs().fillna(0))\n        / agg['lassets']\n    )\n    \n    # Turnover 3: Symmetric\n    agg['turnover3'] = (\n        (agg['tbuys'].fillna(0) + agg['tsales'].fillna(0)\n         - agg['netflows'].abs().fillna(0))\n        / agg['lassets']\n    )\n    \n    # Missing for first report\n    first_mask = agg['first_report']\n    for col in ['netflows', 'tgainret',\n                'turnover1', 'turnover2', 'turnover3']:\n        agg.loc[first_mask, col] = np.nan\n    \n    agg = agg.drop(columns=['assets_comp', 'lassets_comp', 'lassets'])\n    \n    print(f\"\\nAggregates: {len(agg):,} manager-quarters\")\n    print(f\"  Turnover1 mean: {agg['turnover1'].mean():.4f}\")\n    print(f\"  Turnover2 mean: {agg['turnover2'].mean():.4f}\")\n    print(f\"  Turnover3 mean: {agg['turnover3'].mean():.4f}\")\n    \n    return agg\n\n\n25.2.1 Turnover Summary Statistics\n\n\n\nTable 25.1: Summary statistics for three turnover measures across institutional investor types in Vietnam. Turnover 1 follows Carhart (1997), Turnover 2 adds back absolute net flows, and Turnover 3 uses the symmetric definition.\n\n\n\nCode\ndef turnover_summary_table(\n    aggregates: pd.DataFrame,\n    holdings: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Publication-quality turnover summary statistics table.\"\"\"\n    owner_map = (\n        holdings.groupby('shareholder_name')['owner_type']\n        .first().reset_index()\n    )\n    agg = aggregates.merge(owner_map, on='shareholder_name', how='left')\n    \n    turnover_cols = ['turnover1', 'turnover2', 'turnover3']\n    results = []\n    \n    for otype in ['All'] + OwnershipType.ALL_TYPES:\n        subset = agg if otype == 'All' else agg[agg['owner_type'] == otype]\n        row = {'Owner Type': otype, 'N': len(subset)}\n        for col in turnover_cols:\n            s = subset[col].dropna()\n            row[f'{col}_mean'] = s.mean()\n            row[f'{col}_median'] = s.median()\n            row[f'{col}_std'] = s.std()\n        results.append(row)\n    \n    return pd.DataFrame(results).round(4)\n\n# turnover_summary_table(aggregates, holdings)\n\n\n\n\n\n\n\n\nCode\ndef plot_turnover_timeseries(\n    aggregates: pd.DataFrame, holdings: pd.DataFrame\n):\n    \"\"\"Plot turnover time series by ownership type.\"\"\"\n    owner_map = (\n        holdings.groupby('shareholder_name')['owner_type']\n        .first().reset_index()\n    )\n    agg = aggregates.merge(owner_map, on='shareholder_name', how='left')\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    for otype in OwnershipType.ALL_INSTITUTIONAL:\n        subset = agg[agg['owner_type'] == otype]\n        qtr_mean = (\n            subset\n            .groupby(pd.Grouper(key='rdate', freq='QE'))['turnover1']\n            .mean()\n        )\n        ax.plot(qtr_mean.index, qtr_mean.values, label=otype,\n                color=OWNER_COLORS.get(otype, '#333'), linewidth=1.5)\n    \n    ax.set_title('Quarterly Average Turnover (Carhart)',\n                 fontweight='bold')\n    ax.set_ylabel('Turnover Ratio')\n    ax.legend(loc='best')\n    ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n    plt.tight_layout()\n    plt.show()\n\n# plot_turnover_timeseries(aggregates, holdings)\n\n\n\nFigure 25.1",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-fol-util",
    "href": "31_institutional_trade_flow.html#sec-fol-util",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "26.1 FOL Utilization",
    "text": "26.1 FOL Utilization\n\\[\n\\text{FOL\\_Util}_{i,t} = \\frac{FO_{i,t}}{FOL_i}\n\\tag{26.1}\\]\nStocks with \\(\\text{FOL\\_Util}_{i,t} \\to 1\\) face mechanical foreign buying restrictions.\n\ndef compute_fol_analytics(\n    foreign_ownership: pd.DataFrame,\n    company_profile: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute FOL utilization and related metrics.\"\"\"\n    fo = foreign_ownership.copy()\n    fo = fo.merge(\n        company_profile[['ticker', 'fol_limit']].drop_duplicates(),\n        on='ticker', how='left'\n    )\n    \n    fo['fol_utilization'] = fo['foreign_pct'] / fo['fol_limit']\n    fo['foreign_room'] = fo['fol_limit'] - fo['foreign_pct']\n    fo['fol_binding'] = (fo['fol_utilization'] &gt;= 0.98)\n    fo['fol_category'] = pd.cut(\n        fo['fol_utilization'],\n        bins=[0, 0.25, 0.50, 0.75, 0.95, 1.0, float('inf')],\n        labels=['&lt;25%', '25-50%', '50-75%', '75-95%',\n                '95-100%', '&gt;100%']\n    )\n    return fo",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-room-premium",
    "href": "31_institutional_trade_flow.html#sec-room-premium",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "26.2 Room Premium Regression",
    "text": "26.2 Room Premium Regression\nWhen foreign ownership approaches the FOL, remaining “room” becomes scarce. We model:\n\\[\nr_{i,t+1} = \\alpha + \\beta_1 \\cdot \\text{FOL\\_Util}_{i,t} +\n\\beta_2 \\cdot \\text{FOL\\_Util}_{i,t}^2 + \\gamma \\cdot X_{i,t} + \\varepsilon_{i,t}\n\\tag{26.2}\\]\nThe quadratic term captures nonlinear acceleration of the premium as ownership approaches the limit.\n\ndef estimate_room_premium(\n    fol_analytics: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; dict:\n    \"\"\"Estimate foreign ownership room premium via panel regression.\"\"\"\n    fol_q = (\n        fol_analytics\n        .assign(qdate=lambda x: x['date'] + pd.offsets.QuarterEnd(0))\n        .groupby(['ticker', 'qdate'])\n        .agg(fol_utilization=('fol_utilization', 'last'),\n             foreign_room=('foreign_room', 'last'))\n        .reset_index()\n    )\n    \n    panel = fol_q.merge(\n        price_q[['ticker', 'qdate', 'mcap', 'qret']],\n        on=['ticker', 'qdate'], how='inner'\n    )\n    \n    panel['log_mcap'] = np.log(panel['mcap'] + 1)\n    panel['fol_util_sq'] = panel['fol_utilization'] ** 2\n    panel = panel.dropna(subset=['qret', 'fol_utilization', 'log_mcap'])\n    \n    X = panel[['fol_utilization', 'fol_util_sq', 'log_mcap']]\n    X = sm.add_constant(X)\n    y = panel['qret']\n    \n    model = sm.OLS(y, X).fit(\n        cov_type='cluster', cov_kwds={'groups': panel['ticker']}\n    )\n    return {'model': model, 'n_obs': len(panel)}\n\n# results = estimate_room_premium(fol_analytics, price_q)\n\n\n\n\n\nCode\ndef plot_fol_utilization(fol_analytics: pd.DataFrame):\n    \"\"\"Plot FOL utilization distribution.\"\"\"\n    latest = (\n        fol_analytics.sort_values(['ticker', 'date'])\n        .groupby('ticker').last().reset_index()\n    )\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    axes[0].hist(latest['fol_utilization'].dropna(), bins=50,\n                 color='#1f77b4', alpha=0.7, edgecolor='white')\n    axes[0].axvline(x=0.95, color='red', linestyle='--',\n                     label='95% threshold')\n    axes[0].set_title('Panel A: FOL Utilization Distribution',\n                       fontweight='bold')\n    axes[0].set_xlabel('FOL Utilization Ratio')\n    axes[0].set_ylabel('Number of Stocks')\n    axes[0].legend()\n    \n    for exch in ['HOSE', 'HNX', 'UPCOM']:\n        sub = latest[latest.get('exchange') == exch]\n        if len(sub) &gt; 0:\n            axes[1].hist(sub['fol_utilization'].dropna(), bins=30,\n                        alpha=0.5, label=exch,\n                        color=EXCHANGE_COLORS.get(exch, '#333'))\n    axes[1].set_title('Panel B: By Exchange', fontweight='bold')\n    axes[1].set_xlabel('FOL Utilization Ratio')\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n# plot_fol_utilization(fol_analytics)\n\n\n\nFigure 26.1",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-herding",
    "href": "31_institutional_trade_flow.html#sec-herding",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "28.1 Herding Measures",
    "text": "28.1 Herding Measures\nFollowing Sias (2004), the Lakonishok-Shleifer-Vishny herding measure is:\n\\[\nHM_{i,t} = \\left|\\frac{B_{i,t}}{B_{i,t} + S_{i,t}} - p_t\\right|\n- E\\left[\\left|\\frac{B_{i,t}}{B_{i,t} + S_{i,t}} - p_t\\right|\\right]\n\\tag{28.1}\\]\nwhere \\(B_{i,t}\\) is the number of managers buying stock \\(i\\) in quarter \\(t\\), \\(S_{i,t}\\) the number selling, and \\(p_t\\) the expected buyer proportion under independent trading.\n\ndef compute_lsv_herding(\n    trades: pd.DataFrame,\n    min_traders: int = 5,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute LSV herding measure for each stock-quarter.\"\"\"\n    tc = (\n        trades.groupby(['ticker', 'rdate'])\n        .apply(lambda g: pd.Series({\n            'n_buyers': (g['trade'] &gt; 0).sum(),\n            'n_sellers': (g['trade'] &lt; 0).sum(),\n            'n_traders': len(g),\n        }))\n        .reset_index()\n    )\n    \n    tc = tc[tc['n_traders'] &gt;= min_traders].copy()\n    tc['buy_prop'] = tc['n_buyers'] / tc['n_traders']\n    tc['p_t'] = tc.groupby('rdate')['buy_prop'].transform('mean')\n    tc['raw_hm'] = (tc['buy_prop'] - tc['p_t']).abs()\n    \n    def expected_abs_deviation(row):\n        n = int(row['n_traders'])\n        p = row['p_t']\n        if n == 0 or p == 0 or p == 1:\n            return 0\n        from scipy.stats import binom\n        k = np.arange(0, n + 1)\n        probs = binom.pmf(k, n, p)\n        return np.sum(np.abs(k / n - p) * probs)\n    \n    tc['expected_hm'] = tc.apply(expected_abs_deviation, axis=1)\n    tc['herding'] = tc['raw_hm'] - tc['expected_hm']\n    \n    tc['buy_herding'] = np.where(\n        tc['buy_prop'] &gt; tc['p_t'], tc['herding'], np.nan\n    )\n    tc['sell_herding'] = np.where(\n        tc['buy_prop'] &lt; tc['p_t'], tc['herding'], np.nan\n    )\n    \n    return tc[['ticker', 'rdate', 'n_buyers', 'n_sellers',\n               'n_traders', 'herding', 'buy_herding', 'sell_herding']]",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-persistence",
    "href": "31_institutional_trade_flow.html#sec-persistence",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "28.2 Demand Persistence",
    "text": "28.2 Demand Persistence\nSias (2004) showed institutional demand is persistent:\n\\[\n\\rho_t = \\text{Corr}\\left(\\Delta IO_{i,t},\\, \\Delta IO_{i,t-1}\\right)\n\\tag{28.2}\\]\n\ndef compute_demand_persistence(io_ratios: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Rolling cross-sectional correlation of IO changes.\"\"\"\n    io = io_ratios[['ticker', 'rdate', 'io_total_inst']].copy()\n    io = io.sort_values(['ticker', 'rdate'])\n    io['dio'] = io.groupby('ticker')['io_total_inst'].diff()\n    io['lag_dio'] = io.groupby('ticker')['dio'].shift(1)\n    \n    persistence = (\n        io.dropna(subset=['dio', 'lag_dio'])\n        .groupby('rdate')\n        .apply(lambda g: g['dio'].corr(g['lag_dio']))\n        .reset_index()\n        .rename(columns={0: 'persistence'})\n    )\n    persistence = persistence.sort_values('rdate')\n    persistence['persistence_ma'] = (\n        persistence['persistence'].rolling(window=20, min_periods=4).mean()\n    )\n    return persistence\n\n\n\n\n\nCode\ndef plot_demand_persistence(persistence: pd.DataFrame):\n    fig, ax = plt.subplots(figsize=(12, 5))\n    ax.bar(persistence['rdate'], persistence['persistence'],\n           width=80, alpha=0.3, color='#1f77b4', label='Quarterly')\n    ax.plot(persistence['rdate'], persistence['persistence_ma'],\n            color='#d62728', linewidth=2, label='Rolling Average')\n    ax.axhline(y=0, color='black', linewidth=0.5)\n    ax.set_title('Persistence of Institutional Demand', fontweight='bold')\n    ax.set_ylabel('Cross-Sectional Correlation')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\nFigure 28.1",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-info-content",
    "href": "31_institutional_trade_flow.html#sec-info-content",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "28.3 Information Content of Trades",
    "text": "28.3 Information Content of Trades\nFollowing Alexander, Cici, and Gibson (2007), the InfoTrade ratio measures the proportion of dollar trading from entry/exit decisions vs. position adjustments:\n\\[\n\\text{InfoTrade}_{i,t} = \\frac{\n\\sum_{j: BS \\in \\{+1,-1\\}} |\\Delta h_{j,i,t}| \\cdot P_{i,t}\n}{\n\\sum_j |\\Delta h_{j,i,t}| \\cdot P_{i,t}\n}\n\\tag{28.3}\\]\n\ndef compute_info_trade_ratio(\n    trades: pd.DataFrame, price_q: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Compute info trade ratio for each stock-quarter.\"\"\"\n    _t = trades.merge(\n        price_q[['ticker', 'qdate', 'p']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    _t['dollar_trade'] = _t['trade'].abs() * _t['p'] / 1e6\n    _t['is_discrete'] = _t['buysale'].isin([1, -1])\n    \n    info = _t.groupby(['ticker', 'rdate']).apply(\n        lambda g: pd.Series({\n            'discrete_vol': g.loc[g['is_discrete'], 'dollar_trade'].sum(),\n            'total_vol': g['dollar_trade'].sum(),\n        })\n    ).reset_index()\n    \n    info['info_trade_ratio'] = (\n        info['discrete_vol'] / info['total_vol']\n    ).clip(0, 1)\n    return info",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-app-returns",
    "href": "31_institutional_trade_flow.html#sec-app-returns",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "29.1 Application 1: Institutional Ownership Changes and Future Returns",
    "text": "29.1 Application 1: Institutional Ownership Changes and Future Returns\nWe test whether changes in institutional ownership predict future stock returns (Chen, Jegadeesh, and Wermers 2000) via Fama-MacBeth regressions:\n\\[\nr_{i,t+1} = \\alpha_t + \\beta_{1,t} \\cdot \\Delta IO_{i,t} + \\beta_{2,t} \\cdot\n\\Delta\\text{Breadth}_{i,t} + \\gamma_t \\cdot X_{i,t} + \\varepsilon_{i,t}\n\\tag{29.1}\\]\n\ndef fama_macbeth_io_returns(\n    io_ratios: pd.DataFrame,\n    breadth: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Run Fama-MacBeth regressions of future returns on IO changes.\"\"\"\n    panel = io_ratios[['ticker', 'rdate', 'io_total_inst']].merge(\n        breadth[['ticker', 'rdate', 'n_total_inst', 'd_n_total_inst']],\n        on=['ticker', 'rdate'], how='inner'\n    ).merge(\n        price_q[['ticker', 'qdate', 'mcap', 'qret']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    panel = panel.sort_values(['ticker', 'rdate'])\n    panel['dio'] = panel.groupby('ticker')['io_total_inst'].diff()\n    panel['log_mcap'] = np.log(panel['mcap'] + 1)\n    panel['mom'] = panel.groupby('ticker')['qret'].shift(1)\n    \n    reg_vars = ['qret', 'dio', 'd_n_total_inst', 'log_mcap', 'mom']\n    panel = panel.dropna(subset=reg_vars)\n    \n    quarters = sorted(panel['rdate'].unique())\n    results = []\n    \n    for q in quarters:\n        qdata = panel[panel['rdate'] == q]\n        if len(qdata) &lt; 30:\n            continue\n        X = sm.add_constant(\n            qdata[['dio', 'd_n_total_inst', 'log_mcap', 'mom']]\n        )\n        try:\n            model = sm.OLS(qdata['qret'], X).fit()\n            coefs = model.params.to_dict()\n            coefs['rdate'] = q\n            coefs['n_obs'] = len(qdata)\n            results.append(coefs)\n        except Exception:\n            continue\n    \n    fm = pd.DataFrame(results)\n    \n    # Time-series averages with Newey-West t-statistics\n    print(\"\\nFama-MacBeth Results:\")\n    print(\"=\" * 50)\n    for var in ['const', 'dio', 'd_n_total_inst', 'log_mcap', 'mom']:\n        coefs = fm[var].dropna()\n        mean_c = coefs.mean()\n        nw_se = sm.OLS(\n            coefs - mean_c, np.ones(len(coefs))\n        ).fit(cov_type='HAC', cov_kwds={'maxlags': 4}).bse[0]\n        t = mean_c / nw_se if nw_se &gt; 0 else np.nan\n        print(f\"  {var:20s}: coef={mean_c:8.4f}, t={t:6.2f}\")\n    \n    return fm",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-app-turnover",
    "href": "31_institutional_trade_flow.html#sec-app-turnover",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "29.2 Application 2: Turnover and Performance",
    "text": "29.2 Application 2: Turnover and Performance\nYan (2008) documented a positive turnover-performance relationship. We test in Vietnam:\n\\[\n\\alpha_{j,t} = a + b \\cdot \\text{Turnover}_{j,t-1} + c \\cdot\n\\log(A_{j,t-1}) + d \\cdot \\text{Flow}_{j,t} + \\varepsilon_{j,t}\n\\tag{29.2}\\]\n\ndef turnover_performance_regression(\n    aggregates: pd.DataFrame,\n) -&gt; dict:\n    \"\"\"Test turnover-performance relationship.\"\"\"\n    agg = aggregates.sort_values(['shareholder_name', 'rdate']).copy()\n    agg['lag_turnover1'] = (\n        agg.groupby('shareholder_name')['turnover1'].shift(1)\n    )\n    agg['log_assets'] = np.log(agg['assets'] + 1)\n    agg['flow_ratio'] = agg['netflows'] / agg['assets'].shift(1)\n    \n    panel = agg.dropna(\n        subset=['pret', 'lag_turnover1', 'log_assets', 'flow_ratio']\n    )\n    \n    for col in ['pret', 'lag_turnover1', 'flow_ratio']:\n        lo, hi = panel[col].quantile([0.01, 0.99])\n        panel[col] = panel[col].clip(lo, hi)\n    \n    X = sm.add_constant(\n        panel[['lag_turnover1', 'log_assets', 'flow_ratio']]\n    )\n    model = sm.OLS(panel['pret'], X).fit(\n        cov_type='cluster',\n        cov_kwds={'groups': panel['shareholder_name']}\n    )\n    return {'model': model, 'n': len(panel)}",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-app-foreign-domestic",
    "href": "31_institutional_trade_flow.html#sec-app-foreign-domestic",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "29.3 Application 3: Foreign vs. Domestic Trading",
    "text": "29.3 Application 3: Foreign vs. Domestic Trading\n\ndef compare_foreign_domestic(\n    trades: pd.DataFrame, price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compare trading patterns between foreign and domestic institutions.\"\"\"\n    _t = trades.merge(\n        price_q[['ticker', 'qdate', 'p']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    _t['dollar_trade'] = _t['trade'] * _t['p'] / 1e6\n    _t['is_buy'] = _t['trade'] &gt; 0\n    \n    return (\n        _t[_t['owner_type'].isin(OwnershipType.INSTITUTIONAL)]\n        .groupby('owner_type')\n        .agg(\n            n_trades=('trade', 'count'),\n            n_buys=('is_buy', 'sum'),\n            avg_dollar=('dollar_trade', lambda x: x.abs().mean()),\n            net_buying=('dollar_trade', 'sum'),\n            pct_initiating=('buysale', lambda x: (x.abs() == 1).mean()),\n        )\n        .reset_index()\n    )\n\n\n\n\n\nCode\ndef plot_cumulative_net_buying(\n    trades: pd.DataFrame, price_q: pd.DataFrame\n):\n    _t = trades.merge(\n        price_q[['ticker', 'qdate', 'p']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    _t['trade_vnd'] = _t['trade'] * _t['p'] / 1e9\n    \n    inst = _t[_t['owner_type'].isin(OwnershipType.INSTITUTIONAL)]\n    net = (\n        inst.groupby(\n            [pd.Grouper(key='rdate', freq='QE'), 'owner_type']\n        )['trade_vnd'].sum().unstack(fill_value=0)\n    )\n    cum = net.cumsum()\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    for col in cum.columns:\n        ax.plot(cum.index, cum[col], label=col,\n                color=OWNER_COLORS.get(col, '#333'), linewidth=2)\n    ax.axhline(y=0, color='black', linewidth=0.5)\n    ax.set_title('Cumulative Net Institutional Buying', fontweight='bold')\n    ax.set_ylabel('Billions VND')\n    ax.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n\n\n\nFigure 29.1",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-pitfalls",
    "href": "31_institutional_trade_flow.html#sec-pitfalls",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "30.1 Common Pitfalls",
    "text": "30.1 Common Pitfalls\n\n30.1.1 Corporate Action Misadjustment\n\n\n\n\n\n\nCautionExample: Phantom Trade from Unadjusted Stock Dividend\n\n\n\nVinamilk (VNM) issues a 20% stock dividend with ex-date March 15, 2023.\n\nQ4 2022: Fund X holds 1,000,000 shares of VNM\nQ1 2023: Fund X holds 1,200,000 shares of VNM\n\nWithout adjustment: Inferred buy of +200,000 shares (BS = +2) With adjustment: Prior holdings become 1,200,000 adjusted shares, trade = 0\nThis phantom trade inflates measured turnover and creates spurious buying signals.\n\n\n\n\n30.1.2 Disclosure Timing Mismatches\nVietnamese ownership disclosure dates may not align with calendar quarter ends. Our pipeline addresses this by aligning all disclosures to the nearest quarter-end.\n\n\n30.1.3 Name Changes and Entity Mergers\nVietnamese institutions frequently rename. Without a stable identifier, the same entity may appear as two different shareholders, creating phantom entries/exits. We recommend maintaining a master entity mapping table.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "31_institutional_trade_flow.html#sec-validation",
    "href": "31_institutional_trade_flow.html#sec-validation",
    "title": "19  Institutional Trades, Flows, and Turnover Ratios",
    "section": "30.2 Validation Checks",
    "text": "30.2 Validation Checks\n\ndef validate_pipeline_outputs(\n    results: Dict[str, pd.DataFrame],\n) -&gt; pd.DataFrame:\n    \"\"\"Run comprehensive validation on pipeline outputs.\"\"\"\n    checks = []\n    h = results['holdings']\n    t = results['trades']\n    a = results['aggregates']\n    \n    checks.append({\n        'Check': 'No negative adjusted shares',\n        'Result': 'PASS' if (h['shares_adj'] &lt; 0).sum() == 0 else 'FAIL',\n        'Detail': f'{(h[\"shares_adj\"] &lt; 0).sum()} negative obs'\n    })\n    \n    checks.append({\n        'Check': 'No duplicate holdings',\n        'Result': 'PASS' if h.duplicated(\n            subset=['shareholder_name', 'ticker', 'rdate']\n        ).sum() == 0 else 'FAIL',\n    })\n    \n    checks.append({\n        'Check': 'Valid buysale codes only',\n        'Result': 'PASS' if t['buysale'].isin([1, 2, -1, -2]).all()\n        else 'FAIL',\n    })\n    \n    checks.append({\n        'Check': 'No zero trades',\n        'Result': 'PASS' if (t['trade'] == 0).sum() == 0 else 'FAIL',\n    })\n    \n    t1 = a['turnover1'].dropna()\n    checks.append({\n        'Check': 'Turnover1 in [0, 10]',\n        'Result': 'PASS' if ((t1 &lt; 0) | (t1 &gt; 10)).sum() == 0\n        else 'WARNING',\n        'Detail': f'{((t1&lt;0)|(t1&gt;10)).sum()} extreme values'\n    })\n    \n    first_rpt = a[a['first_report']]\n    checks.append({\n        'Check': 'First report -&gt; missing netflows',\n        'Result': 'PASS' if first_rpt['netflows'].isna().all()\n        else 'FAIL',\n    })\n    \n    return pd.DataFrame(checks)\n\n# validate_pipeline_outputs(results)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "99_conclusion.html",
    "href": "99_conclusion.html",
    "title": "20  Conclusion",
    "section": "",
    "text": "20.1 What you should take away\nEmpirical finance in emerging and frontier markets is often judged less by the elegance of an estimator than by the credibility of its inputs and the transparency of its decisions. Vietnam makes this point vividly: trading venues and regulatory regimes have evolved quickly, firm coverage can be uneven across time, corporate actions need careful treatment, and accounting conventions require close attention to timing and comparability. Those features do not prevent high-quality research; they simply shift the center of gravity toward reproducible data engineering, auditable transformations, and clear identification of assumptions.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "99_conclusion.html#what-you-should-take-away",
    "href": "99_conclusion.html#what-you-should-take-away",
    "title": "20  Conclusion",
    "section": "",
    "text": "20.1.1 Reproducibility is an identification strategy\nIn textbook settings, identification focuses on variation and exogeneity. In real-world market data, identification also depends on whether your dataset is the same dataset when you rerun the work next month or next year. The practical discipline of versioned inputs, deterministic transformations, and documented filters reduces the scope for accidental \\(p\\)-hacking and silent sample drift (e.g., survivorship bias from symbol changes or late-arriving delistings). Reproducible workflows are not administrative overhead; they are a commitment device that makes results more trustworthy and easier to challenge constructively (Peng 2011; Sandve et al. 2013).\n\n\n20.1.2 Vietnam rewards “microstructure humility”\nThe chapters on returns, beta estimation, and factor construction emphasized that naïve carryover of developed-market defaults can be costly. Thin trading, price limits, lot-size rules, and regime changes mean that decisions like (i) return interval, (ii) stale-price handling, (iii) corporate-action adjustment, and (iv) portfolio formation frequency can materially change inference. This is not a Vietnam-only phenomenon, but it is more visible there, and therefore a useful laboratory for best practices in emerging markets.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "99_conclusion.html#a-reproducibility-checklist-you-can-actually-use",
    "href": "99_conclusion.html#a-reproducibility-checklist-you-can-actually-use",
    "title": "20  Conclusion",
    "section": "20.2 A reproducibility checklist you can actually use",
    "text": "20.2 A reproducibility checklist you can actually use\nThe list below is designed to be operational: each item can be verified in a repository review.\n\n\n\nTable 20.1: Reproducibility deliverables for research\n\n\n\n\n\n\n\n\n\n\nDeliverable\nWhat “done” looks like\nWhere it lives\n\n\n\n\nDeterministic transforms\nSame raw inputs yield identical normalized outputs\nR/transform_*.R (or python/transform_*.py)\n\n\nTest suite\nCoverage, identity, and corporate-action tests run in CI\ntests/ + CI config\n\n\nData dictionary\nTables/fields documented with units, timing, and keys\ndocs/dictionary.qmd\n\n\nResearch log\nAll key design choices recorded (filters, winsorization, periods)\nnotes/research_log.md\n\n\nArtifact registry\nEvery figure/table has a script and a checksum\nartifacts/manifest.json",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abarbanell, Jeffery S, William N Lanen, and Robert E Verrecchia. 1995.\n“Analysts’ Forecasts as Proxies for Investor Beliefs in Empirical\nResearch.” Journal of Accounting and Economics 20 (1):\n31–60.\n\n\nAggarwal, Reena, Isil Erel, Miguel Ferreira, and Pedro Matos. 2011.\n“Does Governance Travel Around the World? Evidence from\nInstitutional Investors.” Journal of Financial Economics\n100 (1): 154–81.\n\n\nAharony, Joseph, and Itzhak Swary. 1980. “Quarterly Dividend and\nEarnings Announcements and Stockholders’ Returns: An Empirical\nAnalysis.” The Journal of Finance 35 (1): 1–12.\n\n\nAlexander, Gordon J, Gjergji Cici, and Scott Gibson. 2007. “Does\nMotivation Matter When Assessing Trade Performance? An Analysis of\nMutual Funds.” The Review of Financial Studies 20 (1):\n125–50.\n\n\nAlexandridis, George, Antonios Antoniou, and Dimitris Petmezas. 2007.\n“Divergence of Opinion and Post-Acquisition Performance.”\nJournal of Business Finance & Accounting 34 (3-4): 439–60.\n\n\nAmihud, Yakov. 2002. “Illiquidity and Stock Returns: Cross-Section\nand Time-Series Effects.” Journal of Financial Markets 5\n(1): 31–56.\n\n\nAnderson, Anne-Marie, and Edward A Dyl. 2005. “Market Structure\nand Trading Volume.” Journal of Financial Research 28\n(1): 115–31.\n\n\nAnderson, Kirsten L, Jeffrey H Harris, and Eric C So. 2007.\n“Opinion Divergence and Post-Earnings Announcement Drift.”\nAvailable at SSRN 969736.\n\n\nAndrade, Gregor, Mark Mitchell, and Erik Stafford. 2001. “New\nEvidence and Perspectives on Mergers.” Journal of Economic\nPerspectives 15 (2): 103–20.\n\n\nBali, Turan G, Robert F Engle, and Scott Murray. 2016. Empirical asset pricing: The cross section of stock\nreturns. John Wiley & Sons. https://doi.org/10.1002/9781118445112.stat07954.\n\n\nBall, Ray, and Philip Brown. 2013. “An Empirical Evaluation of\nAccounting Income Numbers.” In Financial Accounting and\nEquity Markets, 27–46. Routledge.\n\n\nBao Dinh, Ngoc, and Van Nguyen Hong Tran. 2024. “Institutional\nOwnership and Stock Liquidity: Evidence from an Emerging Market.”\nSAGE Open 14 (1): 21582440241239116.\n\n\nBarber, Brad M, and John D Lyon. 1997. “Detecting Long-Run\nAbnormal Stock Returns: The Empirical Power and Specification of Test\nStatistics.” Journal of Financial Economics 43 (3):\n341–72.\n\n\nBarberis, Nicholas, Andrei Shleifer, and Robert Vishny. 1998. “A\nModel of Investor Sentiment.” Journal of Financial\nEconomics 49 (3): 307–43.\n\n\nBen-David, ITZHAK, Francesco Franzoni, Augustin Landier, and Rabih\nMoussawi. 2013. “Do Hedge Funds Manipulate Stock Prices?”\nThe Journal of Finance 68 (6): 2383–2434.\n\n\nBerkman, Henk, Valentin Dimitrov, Prem C Jain, Paul D Koch, and Sheri\nTice. 2009. “Sell on the News: Differences of Opinion, Short-Sales\nConstraints, and Returns Around Earnings Announcements.”\nJournal of Financial Economics 92 (3): 376–99.\n\n\nBernard, Victor L, and Jacob K Thomas. 1989.\n“Post-Earnings-Announcement Drift: Delayed Price Response or Risk\nPremium?” Journal of Accounting Research 27: 1–36.\n\n\nBhattacharya, Utpal, Hazem Daouk, Brian Jorgenson, and Carl-Heinrich\nKehr. 2000. “When an Event Is Not an Event: The Curious Case of an\nEmerging Market.” Journal of Financial Economics 55 (1):\n69–101.\n\n\nBinder, John. 1998. “The Event Study Methodology Since\n1969.” Review of Quantitative Finance and Accounting 11\n(2): 111–37.\n\n\nBoehme, Rodney D, Bartley R Danielsen, and Sorin M Sorescu. 2006.\n“Short-Sale Constraints, Differences of Opinion, and\nOvervaluation.” Journal of Financial and Quantitative\nAnalysis 41 (2): 455–87.\n\n\nBoehmer, Ekkehart, Jim Masumeci, and Annette B Poulsen. 1991.\n“Event-Study Methodology Under Conditions of Event-Induced\nVariance.” Journal of Financial Economics 30 (2):\n253–72.\n\n\nBrown, Stephen J, and Jerold B Warner. 1980. “Measuring Security\nPrice Performance.” Journal of Financial Economics 8\n(3): 205–58.\n\n\n———. 1985. “Using Daily Stock Returns: The Case of Event\nStudies.” Journal of Financial Economics 14 (1): 3–31.\n\n\nCampbell, John Y, Andrew W Lo, A Craig MacKinlay, and Robert F Whitelaw.\n1998. “The Econometrics of Financial Markets.”\nMacroeconomic Dynamics 2 (4): 559–62.\n\n\nCarhart, Mark M. 1997a. “On Persistence in Mutual Fund\nPerformance.” The Journal of Finance 52 (1): 57–82.\n\n\nCarhart, Mark M. 1997b. “On persistence in\nmutual fund performance.” The Journal of\nFinance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nChan, Kalok, Allaudeen Hameed, and Wilson Tong. 2000.\n“Profitability of Momentum Strategies in the International Equity\nMarkets.” Journal of Financial and Quantitative\nAnalysis, 153–72.\n\n\nChatterjee, Sris, Kose John, and An Yan. 2012. “Takeovers and\nDivergence of Investor Opinion.” The Review of Financial\nStudies 25 (1): 227–77.\n\n\nChen, Hsiu-Lang, Narasimhan Jegadeesh, and Russ Wermers. 2000.\n“The Value of Active Mutual Fund Management: An Examination of the\nStockholdings and Trades of Fund Managers.” Journal of\nFinancial and Quantitative Analysis 35 (3): 343–68.\n\n\nChen, Joseph, Harrison Hong, and Jeremy C Stein. 2002. “Breadth of\nOwnership and Stock Returns.” Journal of Financial\nEconomics 66 (2-3): 171–205.\n\n\nCheong, Foong Soon, and Jacob Thomas. 2011. “Why Do EPS Forecast\nError and Dispersion Not Vary with Scale? Implications for Analyst and\nManagerial Behavior.” Journal of Accounting Research 49\n(2): 359–401.\n\n\nChui, Andy CW, Sheridan Titman, and KC John Wei. 2010.\n“Individualism and Momentum Around the World.” The\nJournal of Finance 65 (1): 361–92.\n\n\nChung, Kee H, and Hao Zhang. 2014. “A Simple Approximation of\nIntraday Spreads Using Daily Data.” Journal of Financial\nMarkets 17: 94–120.\n\n\nCooper, Michael J, Roberto C Gutierrez Jr, and Allaudeen Hameed. 2004.\n“Market States and Momentum.” The Journal of\nFinance 59 (3): 1345–65.\n\n\nCorrado, Charles J. 1989. “A Nonparametric Test for Abnormal\nSecurity-Price Performance in Event Studies.” Journal of\nFinancial Economics 23 (2): 385–95.\n\n\nCoval, Joshua, and Erik Stafford. 2007. “Asset Fire Sales (and\nPurchases) in Equity Markets.” Journal of Financial\nEconomics 86 (2): 479–512.\n\n\nCowan, Arnold Richard. 1992. “Nonparametric Event Study\nTests.” Review of Quantitative Finance and Accounting 2\n(4): 343–58.\n\n\nDaniel, Kent, David Hirshleifer, and Avanidhar Subrahmanyam. 1998.\n“Investor Psychology and Security Market Under-and\nOverreactions.” The Journal of Finance 53 (6): 1839–85.\n\n\nDaniel, Kent, and Tobias J Moskowitz. 2016. “Momentum\nCrashes.” Journal of Financial Economics 122 (2):\n221–47.\n\n\nDiether, Karl B, Christopher J Malloy, and Anna Scherbina. 2002.\n“Differences of Opinion and the Cross Section of Stock\nReturns.” The Journal of Finance 57 (5): 2113–41.\n\n\nDimson, Elroy. 1979. “Risk Measurement When Shares Are Subject to\nInfrequent Trading.” Journal of Financial Economics 7\n(2): 197–226.\n\n\nDoukas, John A, Chansog Francis Kim, and Christos Pantzalis. 2006.\n“Divergence of Opinion and Equity Returns.” Journal of\nFinancial and Quantitative Analysis 41 (3): 573–606.\n\n\nDoukas, John A, Chansog Kim, and Christos Pantzalis. 2004.\n“Divergent Opinions and the Performance of Value Stocks.”\nFinancial Analysts Journal 60 (6): 55–64.\n\n\nFama, Eugene F. 1998. “Market Efficiency, Long-Term Returns, and\nBehavioral Finance.” Journal of Financial Economics 49\n(3): 283–306.\n\n\nFama, Eugene F, Lawrence Fisher, Michael C Jensen, and Richard Roll.\n1969. “The Adjustment of Stock Prices to New Information.”\nInternational Economic Review 10 (1): 1–21.\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock\nreturns.” The Journal of Finance 47\n(2): 427–65. https://doi.org/2329112.\n\n\n———. 1993a. “Common risk factors in the\nreturns on stocks and bonds.” Journal of\nFinancial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\n———. 1993b. “Common risk factors in the\nreturns on stocks and bonds.” Journal of\nFinancial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal\nof Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nFama, Eugene F, and Kenneth R French. 1996. “Multifactor\nExplanations of Asset Pricing Anomalies.” The Journal of\nFinance 51 (1): 55–84.\n\n\nFama, Eugene F., and James D. MacBeth. 1973. “Risk, return, and equilibrium: Empirical\ntests.” Journal of Political Economy\n81 (3): 607–36. https://doi.org/10.1086/260061.\n\n\nFlannery, Mark J, and Aris A Protopapadakis. 2002. “Macroeconomic\nFactors Do Influence Aggregate Stock Returns.” The Review of\nFinancial Studies 15 (3): 751–82.\n\n\nFrazzini, Andrea, and Lasse Heje Pedersen. 2014. “Betting against beta.” Journal of\nFinancial Economics 111 (1): 1–25. https://doi.org/10.1016/j.jfineco.2013.10.005.\n\n\nGarfinkel, Jon A. 2009. “Measuring Investors’ Opinion\nDivergence.” Journal of Accounting Research 47 (5):\n1317–48.\n\n\nGarfinkel, Jon A, and Jonathan Sokobin. 2006. “Volume, Opinion\nDivergence, and Returns: A Study of Post–Earnings Announcement\nDrift.” Journal of Accounting Research 44 (1): 85–112.\n\n\nGentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for\nthe Social Sciences: A Practitioner’s Guide.” Working Paper,\nUniversity of Chicago.\n\n\nGompers, Paul, Joy Ishii, and Andrew Metrick. 2003. “Corporate\nGovernance and Equity Prices.” The Quarterly Journal of\nEconomics 118 (1): 107–56.\n\n\nGriffin, John M, Patrick J Kelly, and Federico Nardari. 2010. “Do\nMarket Efficiency Measures Yield Correct Inferences? A Comparison of\nDeveloped and Emerging Markets.” The Review of Financial\nStudies 23 (8): 3225–77.\n\n\nGrinblatt, Mark, Sheridan Titman, and Russ Wermers. 1995.\n“Momentum Investment Strategies, Portfolio Performance, and\nHerding: A Study of Mutual Fund Behavior.” American Economic\nReview 85 (5): 1088–1105.\n\n\nGrundy, Bruce D, and J Spencer Martin Martin. 2001. “Understanding\nthe Nature of the Risks and the Source of the Rewards to Momentum\nInvesting.” The Review of Financial Studies 14 (1):\n29–78.\n\n\nHall, Peter. 1992. “On the Removal of Skewness by\nTransformation.” Journal of the Royal Statistical Society\nSeries B: Statistical Methodology 54 (1): 221–28.\n\n\nHanda, Puneet, Robert Schwartz, and Ashish Tiwari. 2003. “Quote\nSetting and Price Formation in an Order Driven Market.”\nJournal of Financial Markets 6 (4): 461–89.\n\n\nHarris, Milton, and Artur Raviv. 1993. “Differences of Opinion\nMake a Horse Race.” The Review of Financial Studies 6\n(3): 473–506.\n\n\nHofstede, Geert. 2001. “Culture’s Consequences: Comparing Values,\nBehaviors, Institutions and Organizations Across Nations.”\nInternational Educational and Professional.\n\n\nHong, Harrison, and Jeremy C Stein. 1999. “A Unified Theory of\nUnderreaction, Momentum Trading, and Overreaction in Asset\nMarkets.” The Journal of Finance 54 (6): 2143–84.\n\n\n———. 2003. “Differences of Opinion, Short-Sales Constraints, and\nMarket Crashes.” The Review of Financial Studies 16 (2):\n487–525.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2014. “Digesting anomalies: An investment\napproach.” Review of Financial\nStudies 28 (3): 650–705. https://doi.org/10.1093/rfs/hhu068.\n\n\n———. 2020. “Replicating\nanomalies.” Review of Financial\nStudies 33 (5): 2019–2133. https://doi.org/10.1093/rfs/hhy131.\n\n\nHouge, Todd, Tim Loughran, Gerry Suchanek, and Xuemin Yan. 2001.\n“Divergence of Opinion, Uncertainty, and the Quality of Initial\nPublic Offerings.” Financial Management, 5–23.\n\n\nHuang, Xiangqian, Clark Liu, and Tao Shu. 2023. “Factors and\nAnomalies in the Vietnamese Stock Market.” Pacific-Basin\nFinance Journal 82: 102176.\n\n\nJagannathan, Ravi, and Zhenyu Wang. 1996. “The conditional CAPM and the cross-section of expected\nreturns.” The Journal of Finance 51\n(1): 3–53. https://doi.org/10.2307/2329301.\n\n\nJegadeesh, Narasimhan. 1990. “Evidence of Predictable Behavior of\nSecurity Returns.” The Journal of Finance 45 (3):\n881–98.\n\n\nJegadeesh, Narasimhan, and Sheridan Titman. 1993. “Returns to\nBuying Winners and Selling Losers: Implications for Stock Market\nEfficiency.” The Journal of Finance 48 (1): 65–91.\n\n\nJensen, Michael C, and Richard S Ruback. 1983. “The Market for\nCorporate Control: The Scientific Evidence.” Journal of\nFinancial Economics 11 (1-4): 5–50.\n\n\nJohnson, Timothy C. 2002. “Rational Momentum Effects.”\nThe Journal of Finance 57 (2): 585–608.\n\n\nKandel, Eugene, and Neil D Pearson. 1995. “Differential\nInterpretation of Public Signals and Trade in Speculative\nMarkets.” Journal of Political Economy 103 (4): 831–72.\n\n\nKolari, James W, and Seppo Pynnönen. 2010. “Event Study Testing\nwith Cross-Sectional Correlation of Abnormal Returns.” The\nReview of Financial Studies 23 (11): 3996–4025.\n\n\nKorajczyk, Robert A, and Ronnie Sadka. 2004. “Are Momentum Profits\nRobust to Trading Costs?” The Journal of Finance 59 (3):\n1039–82.\n\n\nKothari, Sagar P, and Jerold B Warner. 2007. “Econometrics of\nEvent Studies.” In Handbook of Empirical Corporate\nFinance, 3–36. Elsevier.\n\n\nLehavy, Reuven, and Richard G Sloan. 2008. “Investor Recognition\nand Stock Returns.” Review of Accounting Studies 13 (2):\n327–61.\n\n\nLeibowitz, Martin L. 2002. “The Levered p/e Ratio.”\nFinancial Analysts Journal 58 (6): 68–77.\n\n\nLesmond, David A, Michael J Schill, and Chunsheng Zhou. 2004. “The\nIllusory Nature of Momentum Profits.” Journal of Financial\nEconomics 71 (2): 349–80.\n\n\nLintner, John. 1965. “Security prices, risk,\nand maximal gains from diversification.” The\nJournal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nLivnat, Joshua, and Richard R Mendenhall. 2006. “Comparing the\nPost–Earnings Announcement Drift for Surprises Calculated from Analyst\nand Time Series Forecasts.” Journal of Accounting\nResearch 44 (1): 177–205.\n\n\nMacKinlay, A Craig. 1997. “Event Studies in Economics and\nFinance.” Journal of Economic Literature 35 (1): 13–39.\n\n\nMarkowitz, Harry. 1952. “Portfolio\nselection.” The Journal of Finance 7\n(1): 77–91. https://doi.org/10.1111/j.1540-6261.1952.tb01525.x.\n\n\nMilgrom, Paul, and Nancy Stokey. 1982. “Information, Trade and\nCommon Knowledge.” Journal of Economic Theory 26 (1):\n17–27.\n\n\nMiller, Edward M. 1977. “Risk, Uncertainty, and Divergence of\nOpinion.” The Journal of Finance 32 (4): 1151–68.\n\n\nMitchell, Mark L, and Jeffry M Netter. 1993. “The Role of\nFinancial Economics in Securities Fraud Cases: Applications at the\nSecurities and Exchange Commission.” Bus. Law. 49: 545.\n\n\nMitchell, Mark L, and Erik Stafford. 2000. “Managerial Decisions\nand Long-Term Stock Price Performance.” The Journal of\nBusiness 73 (3): 287–329.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital\nasset market.” Econometrica 34 (4):\n768–83. https://doi.org/10.2307/1910098.\n\n\nNewey, Whitney K., and Kenneth D. West. 1987. “A simple, positive semi-definite, heteroskedasticity and\nautocorrelation consistent covariance Matrix.”\nEconometrica 55 (3): 703–8. http://www.jstor.org/stable/1913610.\n\n\nNguyen, Du D, and Minh C Pham. 2018. “Search-Based Sentiment and\nStock Market Reactions: An Empirical Evidence in Vietnam.”\nThe Journal of Asian Finance, Economics and Business 5 (4):\n45–56.\n\n\nPástor, L’uboš, and Robert F Stambaugh. 2003. “Liquidity Risk and\nExpected Stock Returns.” Journal of Political Economy\n111 (3): 642–85.\n\n\nPatell, James M. 1976. “Corporate Forecasts of Earnings Per Share\nand Stock Price Behavior: Empirical Test.” Journal of\nAccounting Research, 246–76.\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27.\n\n\nPhan, Thi Nha Truc, Philippe Bertrand, Hong Hai Phan, and Xuan Vinh Vo.\n2023. “The Role of Investor Behavior in Emerging Stock Markets:\nEvidence from Vietnam.” The Quarterly Review of Economics and\nFinance 87: 367–76.\n\n\nPhung, Duc Nam, and Anil V Mishra. 2016. “Ownership Structure and\nFirm Performance: Evidence from Vietnamese Listed Firms.”\nAustralian Economic Papers 55 (1): 63–98.\n\n\nRouwenhorst, K Geert. 1998. “International Momentum\nStrategies.” The Journal of Finance 53 (1): 267–84.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig.\n2013. “Ten Simple Rules for Reproducible Computational\nResearch.” PLoS Computational Biology 9 (10): e1003285.\n\n\nScheinkman, Jose A, and Wei Xiong. 2003. “Overconfidence and\nSpeculative Bubbles.” Journal of Political Economy 111\n(6): 1183–1220.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy\nFinance with r. CRC Press.\n\n\nScheuch, Christoph, Stefan Voigt, Patrick Weiss, and Christoph Frey.\n2024. Tidy Finance with Python. Chapman; Hall/CRC.\n\n\nScholes, Myron, and Joseph Williams. 1977. “Estimating Betas from\nNonsynchronous Data.” Journal of Financial Economics 5\n(3): 309–27.\n\n\nSchwert, G William. 1981. “Using Financial Data to Measure Effects\nof Regulation.” The Journal of Law and Economics 24 (1):\n121–58.\n\n\nShalen, Catherine T. 1993. “Volume, Volatility, and the Dispersion\nof Beliefs.” The Review of Financial Studies 6 (2):\n405–34.\n\n\nSharpe, William F. 1964. “Capital asset\nprices: A theory of market equilibrium under conditions of risk\n.” The Journal of Finance 19 (3):\n425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.\n\n\nSias, Richard W. 2004. “Institutional Herding.” The\nReview of Financial Studies 17 (1): 165–206.\n\n\nSirri, Erik R, and Peter Tufano. 1998. “Costly Search and Mutual\nFund Flows.” The Journal of Finance 53 (5): 1589–1622.\n\n\nVarian, Hal R. 1985. “Divergence of Opinion in Complete Markets: A\nNote.” The Journal of Finance 40 (1): 309–17.\n\n\nVilhuber, Lars. 2020. “Reproducibility and Replicability in\nEconomics.” Harvard Data Science Review 2 (4): 1–39.\n\n\nVo, Xuan Vinh. 2015. “Foreign Ownership and Stock Return\nVolatility–Evidence from Vietnam.” Journal of Multinational\nFinancial Management 30: 101–9.\n\n\n———. 2017. “Do Foreign Investors Improve Stock Price\nInformativeness in Emerging Equity Markets? Evidence from\nVietnam.” Research in International Business and Finance\n42: 986–91.\n\n\nVo, Xuan Vinh, and Dang Bao Anh Phan. 2017. “Further Evidence on\nthe Herd Behavior in Vietnam Stock Market.” Journal of\nBehavioral and Experimental Finance 13: 33–41.\n\n\nWarner, Jerold B, Ross L Watts, and Karen H Wruck. 1988. “Stock\nPrices and Top Management Changes.” Journal of Financial\nEconomics 20: 461–92.\n\n\nWermers, Russ. 2000. “Mutual Fund Performance: An Empirical\nDecomposition into Stock-Picking Talent, Style, Transactions Costs, and\nExpenses.” The Journal of Finance 55 (4): 1655–95.\n\n\nYan, Xuemin Sterling. 2008. “Liquidity, Investment Style, and the\nRelation Between Fund Size and Fund Performance.” Journal of\nFinancial and Quantitative Analysis 43 (3): 741–67.",
    "crumbs": [
      "References"
    ]
  }
]