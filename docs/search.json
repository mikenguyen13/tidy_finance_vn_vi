[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tài Chính Ứng Dụng với Python",
    "section": "",
    "text": "Lời nói đầu",
    "crumbs": [
      "Home",
      "Lời nói đầu"
    ]
  },
  {
    "objectID": "index.html#động-lực",
    "href": "index.html#động-lực",
    "title": "Tài Chính Ứng Dụng với Python",
    "section": "Động lực",
    "text": "Động lực\nTài chính thực nghiệm đã trải qua một sự chuyển đổi cơ bản trong hai thập kỷ qua. Những tiến bộ về năng lực tính toán, phần mềm thống kê mã nguồn mở và tính sẵn có của dữ liệu đã định hình lại cách thức tiến hành, đánh giá và phổ biến nghiên cứu tài chính. Ngày càng nhiều nghiên cứu thực nghiệm đáng tin cậy được kỳ vọng phải minh bạch, có thể tái tạo và mở rộng, với kết quả được tạo ra thông qua các quy trình làm việc được lập trình sẵn thay vì can thiệp thủ công. Khả năng tái tạo, được định nghĩa là khả năng các nhà nghiên cứu độc lập tạo ra kết quả thực nghiệm bằng cách sử dụng cùng dữ liệu và phương pháp, do đó đã trở thành một chuẩn mực cốt lõi trong kinh tế tài chính hiện đại.\nMặc dù đã có những tiến bộ này, việc áp dụng các phương pháp nghiên cứu có thể tái tạo vẫn chưa đồng đều giữa các thị trường. Trong các hệ thống tài chính phát triển, đặc biệt là những hệ thống có cơ sở dữ liệu lâu đời và chế độ báo cáo tiêu chuẩn hóa, quy trình làm việc thực nghiệm có thể tái tạo hiện nay rất phổ biến. Ngược lại, nghiên cứu về các thị trường mới nổi và thị trường cận biên thường dựa trên các tập dữ liệu rời rạc, các quy trình làm sạch dữ liệu không được ghi chép đầy đủ và các giả định ngầm định của thể chế khó xác minh hoặc mở rộng. Do đó, các phát hiện thực nghiệm ở những thị trường này thường không ổn định, không thể so sánh giữa các nghiên cứu và tốn kém chi phí cập nhật khi có dữ liệu mới.\nCuốn sách này giải quyết khoảng trống đó.\nCuốn sách này phát triển một khuôn khổ tài chính thực nghiệm có thể tái tạo được, được thiết kế đặc biệt cho các thị trường mới nổi và thị trường cận biên, sử dụng Việt Nam làm trường hợp thực nghiệm chính. Thay vì điều chỉnh các quy trình nghiên cứu của thị trường phát triển một cách hậu kỳ, cuốn sách bắt đầu từ thực tế về thể chế và dữ liệu của một thị trường tăng trưởng nhanh, do bán lẻ thống trị và chịu nhiều quy định, và xây dựng các giải pháp phương pháp luận phù hợp. Mục tiêu không chỉ đơn thuần là phân tích thị trường tài chính Việt Nam, mà còn là để chứng minh cách các nguyên tắc tài chính có thể tái tạo được, như đã được phát triển trong khuôn khổ Tài chính Sạch (Tidy Finance), có thể được mở rộng, kiểm tra độ bền và tinh chỉnh trong môi trường đặc trưng bởi sự khan hiếm dữ liệu, tính không đồng nhất về thể chế và sự thay đổi cấu trúc nhanh chóng.",
    "crumbs": [
      "Home",
      "Lời nói đầu"
    ]
  },
  {
    "objectID": "index.html#vì-sao-thị-trường-mới-nổi-đòi-hỏi-cơ-sở-hạ-tầng-thực-nghiệm-khác-biệt",
    "href": "index.html#vì-sao-thị-trường-mới-nổi-đòi-hỏi-cơ-sở-hạ-tầng-thực-nghiệm-khác-biệt",
    "title": "Tài Chính Ứng Dụng với Python",
    "section": "Vì sao thị trường mới nổi đòi hỏi cơ sở hạ tầng thực nghiệm khác biệt?",
    "text": "Vì sao thị trường mới nổi đòi hỏi cơ sở hạ tầng thực nghiệm khác biệt?\nPhần lớn các nghiên cứu tài chính thực nghiệm hiện đại ngầm giả định sự tồn tại của các bộ dữ liệu ổn định, tần suất cao và được hài hòa về mặt thể chế. Những giả định này hiếm khi được nêu rõ, nhưng chúng lại ăn sâu vào các thiết kế nghiên cứu tiêu chuẩn: lịch sử chứng khoán không có yếu tố tồn tại, chuẩn mực kế toán nhất quán, cơ chế giao dịch không hạn chế và tính thanh khoản dồi dào của các tổ chức.\nCác thị trường mới nổi và thị trường cận biên đang thách thức từng giả định này.\nTại Việt Nam, cũng như nhiều nền kinh tế tương đương khác, thị trường chứng khoán có những đặc điểm như giá giao dịch bị giới hạn hàng ngày, tạm ngừng giao dịch định kỳ, sở hữu nhà nước tập trung và nhà đầu tư cá nhân chiếm ưu thế. Thông tin tài chính được công bố phản ánh các chuẩn mực kế toán địa phương và khung pháp lý đang phát triển. Các hoạt động của doanh nghiệp diễn ra thường xuyên, được ghi chép không nhất quán và đôi khi được sửa đổi sau đó.\nNhững đặc điểm này không phải là những bất tiện cần loại bỏ thông qua việc làm sạch dữ liệu một cách mạnh mẽ. Chúng định hình động lực lợi nhuận, phần bù rủi ro, cấu trúc yếu tố và chính suy luận thống kê. Một khuôn khổ thực nghiệm bỏ qua những đặc điểm thể chế này có nguy cơ tạo ra kết quả không nhất quán nội bộ hoặc gây hiểu lầm bên ngoài. Do đó, một phương pháp có thể tái tạo được cho các thị trường mới nổi phải mã hóa bối cảnh thể chế trực tiếp vào lược đồ dữ liệu, logic chuyển đổi và các lựa chọn mô hình hóa.",
    "crumbs": [
      "Home",
      "Lời nói đầu"
    ]
  },
  {
    "objectID": "index.html#đảm-bảo-tính-tái-hiện-như-một-nguyên-tắc-thiết-kế-nghiên-cứu",
    "href": "index.html#đảm-bảo-tính-tái-hiện-như-một-nguyên-tắc-thiết-kế-nghiên-cứu",
    "title": "Tài Chính Ứng Dụng với Python",
    "section": "Đảm bảo tính tái hiện như một nguyên tắc thiết kế nghiên cứu",
    "text": "Đảm bảo tính tái hiện như một nguyên tắc thiết kế nghiên cứu\nTrong cuốn sách này, khả năng tái tạo không chỉ giới hạn ở khái niệm hẹp về tính sẵn có của mã nguồn. Nó được xem như một nguyên tắc tổ chức chi phối toàn bộ chu trình nghiên cứu thực nghiệm.\nThứ nhất, tất cả các tập dữ liệu đều được xây dựng từ dữ liệu đầu vào thô thông qua các phép biến đổi được ghi chép lại và có tính xác định, đảm bảo nguồn gốc dữ liệu rõ ràng. Thứ hai, các phương pháp thực nghiệm được triển khai theo cách làm cho các giả định mô hình trở nên rõ ràng và có thể sửa đổi. Thứ ba, kết quả được tạo ra thông qua các quy trình tự động thay vì phân tích tương tác, đảm bảo rằng các cập nhật về dữ liệu hoặc tham số được lan truyền nhất quán trong suốt quá trình phân tích. Cuối cùng, các thiết kế thực nghiệm có tính mô-đun, cho phép các nhà nghiên cứu thay thế thị trường, khoảng thời gian lấy mẫu hoặc định nghĩa biến mà không cần viết lại toàn bộ quy trình làm việc.\nCách tiếp cận này lấy cảm hứng phương pháp luận từ phong trào nghiên cứu có thể tái tạo rộng lớn hơn trong kinh tế và tài chính (ví dụ, Gentzkow and Shapiro 2014; Vilhuber 2020), đồng thời chủ đích mở rộng nó ra ngoài môi trường thể chế và dữ liệu ban đầu của nó. Mục tiêu không phải là tái tạo các nghiên cứu hiện có, mà là cho phép các nghiên cứu mới—đặc biệt là những nghiên cứu sẽ không khả thi do dữ liệu phân mảnh và sự phức tạp về thể chế.",
    "crumbs": [
      "Home",
      "Lời nói đầu"
    ]
  },
  {
    "objectID": "index.html#việt-nam-như-một-trường-hợp-không-phải-một-ngoại-lệ",
    "href": "index.html#việt-nam-như-một-trường-hợp-không-phải-một-ngoại-lệ",
    "title": "Tài Chính Ứng Dụng với Python",
    "section": "Việt Nam như một trường hợp, không phải một ngoại lệ",
    "text": "Việt Nam như một trường hợp, không phải một ngoại lệ\nViệt Nam đóng vai trò là trường hợp thực nghiệm trung tâm xuyên suốt cuốn sách, nhưng không được xem như một ngoại lệ cá biệt. Thay vào đó, nó được trình bày như một ví dụ tiêu biểu cho một nhóm thị trường nằm ở vị trí trung gian giữa thị trường mới nổi và thị trường cận biên: đủ lớn để duy trì hoạt động giao dịch cổ phiếu sôi động, nhưng vẫn đang phát triển về quy định, chất lượng công bố thông tin và thành phần nhà đầu tư.\nBằng cách đặt nền tảng cho sự phát triển phương pháp luận trong bối cảnh thị trường Việt Nam, cuốn sách hướng đến việc đưa ra những hiểu biết có thể khái quát hóa cho các bối cảnh khác, bao gồm Đông Nam Á, Nam Á, châu Phi cận Sahara và một số khu vực của Mỹ Latinh. Mỗi chương thực nghiệm đều nhấn mạnh những thành phần nào là đặc thù của thị trường và những thành phần nào có thể áp dụng được, khuyến khích người đọc điều chỉnh khung lý thuyết thay vì áp dụng toàn bộ.",
    "crumbs": [
      "Home",
      "Lời nói đầu"
    ]
  },
  {
    "objectID": "index.html#truy-cập-dữ-liệu",
    "href": "index.html#truy-cập-dữ-liệu",
    "title": "Tài Chính Ứng Dụng với Python",
    "section": "Truy cập dữ liệu",
    "text": "Truy cập dữ liệu\nCác phân tích thực nghiệm trong cuốn sách này dựa trên dữ liệu thị trường chứng khoán Việt Nam do Datacore cung cấp. Để đảm bảo tính khả thi trong khi vẫn tuân thủ các quy định về bản quyền dữ liệu, chúng tôi cung cấp các nguồn tài liệu sau:\n\nBộ dữ liệu mẫu: Một tập hợp con các dữ liệu đã được ẩn danh có sẵn trong DataCore’s Sample Dataset để người đọc có thể chạy mã ví dụ.\nCác tập lệnh xây dựng dữ liệu: Tất cả các tập lệnh được sử dụng để làm sạch và chuyển đổi dữ liệu thô đều được ghi chép đầy đủ và có sẵn trong kho lưu trữ.\nHướng dẫn tái tạo: Người đọc có quyền truy cập vào dữ liệu thị trường Việt Nam từ các nhà cung cấp thương mại có thể sử dụng các đoạn mã của chúng tôi để xây dựng các bộ dữ liệu tương đương.\n\nMọi thắc mắc về việc truy cập hoặc sao chép dữ liệu, vui lòng liên hệ với tác giả.",
    "crumbs": [
      "Home",
      "Lời nói đầu"
    ]
  },
  {
    "objectID": "index.html#đóng-góp-và-khán-giả",
    "href": "index.html#đóng-góp-và-khán-giả",
    "title": "Tài Chính Ứng Dụng với Python",
    "section": "Đóng góp và Khán giả",
    "text": "Đóng góp và Khán giả\nCuốn sách này đóng góp ba điểm chính.\nThứ nhất, bài báo đề xuất một khung lý thuyết tài chính thực nghiệm có thể tái tạo, được thiết kế đặc biệt cho các thị trường mới nổi và thị trường cận biên, tích hợp các chi tiết thể chế vào việc xây dựng dữ liệu và thiết kế mô hình. Thứ hai, bài báo cung cấp bằng chứng thực nghiệm độc đáo về định giá tài sản, thanh khoản và cấu trúc vi mô thị trường tại Việt Nam bằng cách sử dụng các bộ dữ liệu được xây dựng nhất quán. Thứ ba, bài báo cung cấp quy trình nghiên cứu hoàn chỉnh, sẵn sàng cho xuất bản, phù hợp cho nghiên cứu học thuật, phân tích chính sách và tài chính ứng dụng.\nĐối tượng mục tiêu bao gồm sinh viên cao học ngành tài chính và kinh tế, các nhà nghiên cứu học thuật nghiên cứu về thị trường các nước đang phát triển, và các chuyên gia quan tâm đến phân tích hệ thống cổ phiếu thị trường mới nổi. Kiến thức cơ bản về lý thuyết định giá tài sản và lập trình thống kê được giả định là cần thiết, nhưng không yêu cầu kinh nghiệm trước đó về thị trường Việt Nam hoặc các thị trường tương tự.",
    "crumbs": [
      "Home",
      "Lời nói đầu"
    ]
  },
  {
    "objectID": "index.html#cấu-trúc-của-cuốn-sách",
    "href": "index.html#cấu-trúc-của-cuốn-sách",
    "title": "Tài Chính Ứng Dụng với Python",
    "section": "Cấu trúc của cuốn sách",
    "text": "Cấu trúc của cuốn sách\nCác chương tiếp theo sẽ trình bày tiến trình từ cơ sở hạ tầng dữ liệu đến ứng dụng thực nghiệm. Cuốn sách bắt đầu với phần giới thiệu về các nguồn dữ liệu và cơ sở hạ tầng được sử dụng xuyên suốt, tiếp theo là các chương về bối cảnh thể chế, xây dựng dữ liệu và thiết kế quy trình làm việc có thể tái tạo. Các chương tiếp theo phát triển các bài kiểm tra định giá tài sản, các biện pháp thanh khoản và phân tích cấu trúc vi mô thị trường được điều chỉnh phù hợp với thị trường chứng khoán Việt Nam. Mỗi chương được thiết kế để độc lập, nhưng tất cả đều được liên kết thông qua một kiến ​​trúc dữ liệu và mã nguồn chung để đảm bảo tính nhất quán nội bộ.\nCuốn sách kết thúc bằng việc suy ngẫm về những hàm ý rộng hơn của tài chính thực nghiệm có thể tái tạo đối với nghiên cứu thị trường mới nổi và vạch ra những hướng đi cho công việc phương pháp luận và thực nghiệm trong tương lai.\n\n\n\n\n\n\nGentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for the Social Sciences: A Practitioner’s Guide.” Working Paper, University of Chicago.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy Finance with r. CRC Press.\n\n\nScheuch, Christoph, Stefan Voigt, Patrick Weiss, and Christoph Frey. 2024. Tidy Finance with Python. Chapman; Hall/CRC.\n\n\nVilhuber, Lars. 2020. “Reproducibility and Replicability in Economics.” Harvard Data Science Review 2 (4): 1–39.",
    "crumbs": [
      "Home",
      "Lời nói đầu"
    ]
  },
  {
    "objectID": "00_institutional_background.html",
    "href": "00_institutional_background.html",
    "title": "1  Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam",
    "section": "",
    "text": "1.1 Sự phát triển của thị trường chứng khoán Việt Nam\nPhân tích thực nghiệm thị trường tài chính không thể tách rời khỏi bối cảnh thể chế. Thiết kế thị trường, các ràng buộc pháp lý, cơ cấu sở hữu và thành phần nhà đầu tư định hình giá cả, khối lượng và lợi nhuận quan sát được. Tại các thị trường phát triển, nhiều đặc điểm này đủ ổn định và được chuẩn hóa để chúng không còn ảnh hưởng nhiều đến nghiên cứu thực nghiệm. Ngược lại, tại các thị trường mới nổi, các đặc điểm thể chế thường là yếu tố quyết định hàng đầu đến kết quả thực nghiệm.\nChương này cung cấp nền tảng thể chế cho các phân tích thực nghiệm được phát triển sau này trong cuốn sách. Nó mô tả cấu trúc của thị trường chứng khoán Việt Nam, môi trường pháp lý quản lý giao dịch và công bố thông tin, cũng như đặc điểm của các công ty niêm yết và nhà đầu tư. Thay vì chỉ đưa ra một mô tả thuần túy, phần thảo luận nhấn mạnh cách các đặc điểm thể chế liên quan trực tiếp đến các lựa chọn xây dựng dữ liệu, các giả định mô hình và cách diễn giải kết quả thực nghiệm.\nThị trường chứng khoán hiện đại của Việt Nam còn khá non trẻ. Các sàn giao dịch chứng khoán chính thức chỉ được thành lập vào đầu những năm 2000, như một phần của các cải cách kinh tế rộng lớn hơn nhằm chuyển đổi từ nền kinh tế kế hoạch tập trung sang nền kinh tế thị trường. Kể từ đó, vốn hóa thị trường, khối lượng giao dịch và số lượng công ty niêm yết đã tăng trưởng nhanh chóng, mặc dù không đồng đều giữa các ngành và theo thời gian.\nTốc độ phát triển của thị trường được định hình bởi sự kết hợp giữa quá trình tư nhân hóa dần các doanh nghiệp nhà nước, các đợt cải cách pháp lý định kỳ và sự tham gia bền vững của các nhà đầu tư cá nhân. Không giống như các thị trường phát triển song song với các cơ sở đầu tư tổ chức lớn, thị trường chứng khoán Việt Nam trưởng thành trong môi trường mà các nhà đầu tư cá nhân chiếm ưu thế trong hoạt động giao dịch và sự bất đối xứng thông tin vẫn còn đáng kể.\nNhững đặc điểm này có ý nghĩa thực tiễn quan trọng. Động thái lợi nhuận có thể phản ánh các mô hình giao dịch hành vi, các cú sốc thanh khoản có thể được khuếch đại bởi hoạt động phối hợp của nhà đầu tư bán lẻ, và thông tin cấp doanh nghiệp được phản ánh vào giá cả với tốc độ khác nhau. Do đó, một khuôn khổ thực nghiệm có thể tái tạo phải có khả năng nắm bắt những động thái này mà không áp đặt các giả định bắt nguồn từ các thị trường có cấu trúc khác biệt.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#cấu-trúc-sàn-giao-dịch-và-cơ-chế-giao-dịch",
    "href": "00_institutional_background.html#cấu-trúc-sàn-giao-dịch-và-cơ-chế-giao-dịch",
    "title": "1  Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam",
    "section": "1.2 Cấu trúc sàn giao dịch và cơ chế giao dịch",
    "text": "1.2 Cấu trúc sàn giao dịch và cơ chế giao dịch\nViệt Nam có nhiều sàn giao dịch chứng khoán, mỗi sàn có các yêu cầu niêm yết và quy tắc giao dịch riêng biệt. Giao dịch được thực hiện thông qua sổ lệnh giới hạn tập trung, với ưu tiên giá-thời gian quyết định việc thực hiện lệnh. Điều quan trọng là, giới hạn giá hàng ngày hạn chế mức biến động giá tối đa cho phép đối với từng chứng khoán. Các giới hạn này thay đổi tùy theo sàn giao dịch và loại chứng khoán và có hiệu lực trong các giai đoạn biến động mạnh.\nGiới hạn giá gây ra sự cắt cụt cơ học trong lợi nhuận quan sát được, sự tập trung ở giới hạn trên và dưới, và sự dai dẳng trong biến động giá qua nhiều ngày. Từ góc độ thực nghiệm, điều này thách thức các giả định tiêu chuẩn về điều chỉnh giá liên tục và làm phức tạp việc ước tính biến động, đo lường động lượng và thiết kế nghiên cứu sự kiện.\nTrong cuốn sách này, giới hạn giá được xem là đặc điểm cấu trúc chứ không phải là những bất thường. Các quy trình xử lý dữ liệu bảo toàn rõ ràng các chỉ báo về việc chạm giới hạn giá, và các mô hình thực nghiệm được điều chỉnh để tính đến động thái giá bị hạn chế. Lựa chọn thiết kế này phản ánh một nguyên tắc rộng hơn: khả năng tái tạo ở các thị trường mới nổi đòi hỏi phải bảo toàn các tín hiệu thể chế chứ không phải làm giảm nhẹ chúng.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#yêu-cầu-niêm-yết-và-đặc-điểm-của-công-ty",
    "href": "00_institutional_background.html#yêu-cầu-niêm-yết-và-đặc-điểm-của-công-ty",
    "title": "1  Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam",
    "section": "1.3 Yêu cầu niêm yết và đặc điểm của công ty",
    "text": "1.3 Yêu cầu niêm yết và đặc điểm của công ty\nCác công ty niêm yết tại Việt Nam thể hiện sự khác biệt đáng kể về quy mô, cơ cấu sở hữu và chất lượng công bố thông tin. Một đặc điểm nổi bật của thị trường là sự phổ biến của các công ty có tỷ lệ sở hữu nhà nước đáng kể, trực tiếp hoặc thông qua các thực thể liên kết. Sở hữu nhà nước ảnh hưởng đến quản trị doanh nghiệp, chính sách cổ tức, hành vi chấp nhận rủi ro và khả năng phản ứng với các tín hiệu thị trường.\nCác thông tin kế toán được công bố tuân theo Chuẩn mực Kế toán Việt Nam, có nhiều điểm khác biệt quan trọng so với chuẩn mực quốc tế. Mặc dù các nỗ lực hội nhập đang được tiến hành, báo cáo tài chính trong quá khứ thường phản ánh các quy định chuyển tiếp, việc áp dụng chưa đầy đủ kế toán giá trị hợp lý và báo cáo phân khúc còn hạn chế. Những đặc điểm này làm phức tạp khả năng so sánh giữa các công ty và phân tích theo thời gian.\nTừ góc độ nghiên cứu có thể tái lập, các biến kế toán không thể được coi là các yếu tố cơ bản đồng nhất. Định nghĩa biến, độ trễ báo cáo và các thực tiễn điều chỉnh báo cáo phải được ghi chép rõ ràng và mã hóa vào logic xây dựng dữ liệu. Các chương sau sẽ chứng minh cách dữ liệu kế toán được hài hòa một cách minh bạch, có kiểm soát phiên bản mà không che giấu những khác biệt thể chế cơ bản.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#cơ-cấu-nhà-đầu-tư-và-hành-vi-giao-dịch",
    "href": "00_institutional_background.html#cơ-cấu-nhà-đầu-tư-và-hành-vi-giao-dịch",
    "title": "1  Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam",
    "section": "1.4 Cơ cấu nhà đầu tư và Hành vi giao dịch",
    "text": "1.4 Cơ cấu nhà đầu tư và Hành vi giao dịch\nNhà đầu tư cá nhân chiếm phần lớn khối lượng giao dịch trên thị trường chứng khoán Việt Nam. Các nhà đầu tư tổ chức, bao gồm các quỹ trong nước và nước ngoài, đóng vai trò ngày càng tăng nhưng vẫn thứ yếu. Cơ cấu nhà đầu tư này có tác động đến việc cung cấp thanh khoản, xác định giá cả và sự ổn định của thị trường.\nCác thị trường do nhà đầu tư bán lẻ chi phối thường có tỷ lệ luân chuyển cao hơn, hành vi bầy đàn nhất thời và nhạy cảm với thông tin phi cơ bản. Những đặc điểm này ảnh hưởng đến việc giải thích các kết quả thực nghiệm, đặc biệt là trong các nghiên cứu về khả năng dự báo lợi nhuận ngắn hạn, mối quan hệ giữa khối lượng giao dịch và lợi nhuận, cũng như sự tập trung biến động.\nThay vì coi giao dịch của các tổ chức là mặc định, cuốn sách này mô hình hóa rõ ràng tính thanh khoản và hoạt động giao dịch trong môi trường tập trung vào nhà đầu tư cá nhân. Ví dụ, các thước đo thanh khoản được lựa chọn và xây dựng sao cho vẫn có ý nghĩa ngay cả khi quy mô giao dịch nhỏ, giao dịch không thường xuyên và sự mất cân bằng lệnh do các nhà đầu tư cá nhân gây ra.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#môi-trường-pháp-lý-và-rào-cản-thị-trường",
    "href": "00_institutional_background.html#môi-trường-pháp-lý-và-rào-cản-thị-trường",
    "title": "1  Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam",
    "section": "1.5 Môi trường pháp lý và Rào cản thị trường",
    "text": "1.5 Môi trường pháp lý và Rào cản thị trường\nViệc giám sát thị trường chứng khoán Việt Nam đã phát triển song song với sự phát triển của thị trường. Các quy định giao dịch, yêu cầu công bố thông tin và giới hạn sở hữu nước ngoài đã được sửa đổi định kỳ, đôi khi với khả năng tương thích ngược hạn chế. Những thay đổi về quy định có thể gây ra những biến động cấu trúc trong dữ liệu mà không thể nhận thấy ngay lập tức trong chuỗi thời gian thô.\nNhững hạn chế về bán khống, cho vay chứng khoán hạn chế và việc sử dụng các công cụ phái sinh càng làm cho thị trường Việt Nam khác biệt so với các thị trường phát triển. Những trở ngại này ảnh hưởng đến hoạt động chênh lệch giá và tính khả thi của một số chiến lược giao dịch nhất định, tác động đến mô hình lợi nhuận quan sát được và sự hiện thực hóa các yếu tố.\nMột nguyên tắc quan trọng của khung lý thuyết thực nghiệm được phát triển trong cuốn sách này là nhận thức về quy định. Các quy trình xử lý dữ liệu tích hợp các mốc thời gian theo quy định, và các thử nghiệm thực nghiệm được thiết kế để có khả năng chống chịu tốt với những thay đổi về quy tắc. Điều này đảm bảo rằng kết quả có thể được giải thích trong khuôn khổ thể chế mà chúng xuất hiện.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#hệ-quả-đối-với-thiết-kế-thực-nghiệm",
    "href": "00_institutional_background.html#hệ-quả-đối-với-thiết-kế-thực-nghiệm",
    "title": "1  Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam",
    "section": "1.6 Hệ quả đối với Thiết kế Thực nghiệm",
    "text": "1.6 Hệ quả đối với Thiết kế Thực nghiệm\nCác đặc điểm thể chế được mô tả trong chương này là động lực thúc đẩy một số lựa chọn thiết kế lặp đi lặp lại xuyên suốt cuốn sách:\n\nƯu tiên bảo toàn dữ liệu hơn là đơn giản hóa: Các ràng buộc thể chế như giới hạn giá và tạm ngừng giao dịch được giữ nguyên và mô hình hóa một cách rõ ràng.\nCấu trúc biến số theo mô-đun: Các biến số kế toán và thị trường được xây dựng thông qua các hàm minh bạch có thể điều chỉnh khi các tiêu chuẩn phát triển.\nTính nhạy cảm với chế độ: Các phân tích thực nghiệm được cấu trúc để phát hiện và thích ứng với các thay đổi về quy định và cấu trúc.\nGiải thích dựa trên bối cảnh: Kết quả được giải thích dựa trên cấu trúc thị trường chứ không phải được so sánh một cách máy móc với các phát hiện từ thị trường phát triển.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam</span>"
    ]
  },
  {
    "objectID": "00_institutional_background.html#tóm-tắt",
    "href": "00_institutional_background.html#tóm-tắt",
    "title": "1  Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam",
    "section": "1.7 Tóm tắt",
    "text": "1.7 Tóm tắt\nThị trường chứng khoán Việt Nam kết hợp tốc độ tăng trưởng nhanh chóng với những đặc điểm thể chế riêng biệt, thách thức các phương pháp tài chính thực nghiệm truyền thống. Giới hạn giá, sự thống trị của nhà đầu tư cá nhân, sở hữu nhà nước và các quy định đang phát triển định hình kết quả thị trường theo những cách không thể bỏ qua hoặc trừu tượng hóa. Đối với các nhà nghiên cứu làm việc trong môi trường như vậy, khả năng tái tạo đòi hỏi nhiều hơn là chỉ mã nguồn sạch và dữ liệu được ghi chép đầy đủ; nó đòi hỏi phải tích hợp trực tiếp bối cảnh thể chế vào thiết kế thực nghiệm.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Nền tảng thể chế và cấu trúc thị trường của thị trường chứng khoán Việt Nam</span>"
    ]
  },
  {
    "objectID": "01_accessing_and_managing_financial_data.html",
    "href": "01_accessing_and_managing_financial_data.html",
    "title": "2  Truy cập và quản lý dữ liệu tài chính VN",
    "section": "",
    "text": "2.1 Tổng quan về các nguồn dữ liệu tài chính Việt Nam\nChương này cung cấp hướng dẫn về cách tổ chức, truy cập và quản lý dữ liệu tài chính được thiết kế riêng cho thị trường Việt Nam. Trong khi các cơ sở dữ liệu tài chính toàn cầu như CRSP và Compustat là nguồn tài nguyên tiêu chuẩn cho các thị trường phát triển, các thị trường mới nổi như Việt Nam đòi hỏi một cách tiếp cận khác do các nguồn dữ liệu, cấu trúc thị trường và môi trường pháp lý độc đáo. Hiểu rõ những điểm khác biệt này là điều cần thiết để tiến hành nghiên cứu thực nghiệm nghiêm túc về cổ phiếu, trái phiếu và các chỉ số kinh tế vĩ mô của Việt Nam.\nThị trường tài chính Việt Nam đã trải qua sự tăng trưởng vượt bậc kể từ khi thành lập Sở Giao dịch Chứng khoán Thành phố Hồ Chí Minh (HOSE) năm 2000 và Sở Giao dịch Chứng khoán Hà Nội (HNX) năm 2005. Hiện nay, thị trường bao gồm hơn 1.600 công ty niêm yết trên ba sàn giao dịch: HOSE dành cho các cổ phiếu vốn hóa lớn, HNX dành cho các cổ phiếu vốn hóa trung bình và UPCoM (Thị trường các công ty chưa niêm yết) dành cho các công ty nhỏ hơn đang trong quá trình niêm yết chính thức. Sự đa dạng này tạo ra cả cơ hội và thách thức cho các nhà nghiên cứu tài chính đang tìm kiếm sự bao quát toàn diện về thị trường chứng khoán Việt Nam.\nThị trường Việt Nam có nhiều đặc điểm riêng biệt mà các nhà nghiên cứu cần phải xem xét. Giới hạn sở hữu nước ngoài (thường là 49% cho hầu hết các ngành, với một số ngoại lệ cho ngành ngân hàng và các ngành chiến lược nhất định), hạn chế về biên độ giao dịch (ví dụ, hiện tại là \\(\\pm\\) 7% cho HOSE và \\(\\pm\\) 10% cho HNX) và chu kỳ thanh toán T+2 đều ảnh hưởng đến cấu trúc vi mô và động lực lợi nhuận của thị trường. Ngoài ra, thị trường hoạt động bằng Đồng Việt Nam (VND), đòi hỏi sự chú ý cẩn thận đến các tác động của tỷ giá khi so sánh kết quả với các nghiên cứu quốc tế.\nChúng tôi cũng xác định khoảng thời gian thu thập dữ liệu của mình, kéo dài từ những ngày đầu của thị trường chứng khoán Việt Nam đến hiện tại. Thời gian kéo dài này cho phép chúng tôi nắm bắt sự phát triển của thị trường qua nhiều chu kỳ kinh tế khác nhau, bao gồm cuộc khủng hoảng tài chính toàn cầu năm 2008, cuộc khủng hoảng ngân hàng trong nước năm 2011-2012 và giai đoạn đại dịch COVID-19.\nTrước khi đi sâu vào triển khai kỹ thuật, điều quan trọng là phải hiểu rõ bức tranh tổng quan về các nhà cung cấp dữ liệu tài chính phục vụ thị trường Việt Nam. Không giống như các thị trường phát triển, nơi một vài nhà cung cấp hàng đầu (Bloomberg, Refinitiv, FactSet) cung cấp phạm vi phủ sóng toàn diện, dữ liệu tài chính Việt Nam từ trước đến nay vẫn phân tán trên nhiều nguồn khác nhau, mỗi nguồn đều có những điểm mạnh và hạn chế riêng.\nCác nguồn dữ liệu tài chính chính của Việt Nam bao gồm dữ liệu giao dịch chính thức từ HOSE và HNX, cung cấp dữ liệu giao dịch thời gian thực và lịch sử. Ủy ban Chứng khoán Nhà nước Việt Nam (SSC) công bố các báo cáo quản lý, thông báo của doanh nghiệp và số liệu thống kê thị trường. Các nhà cung cấp dữ liệu thương mại như FiinGroup, StoxPlus (nay thuộc FiinGroup) và VNDirect cung cấp các bộ dữ liệu được chọn lọc với mức độ bao phủ và chất lượng dữ liệu khác nhau. Ngoài ra, Ngân hàng Nhà nước Việt Nam (SBV) và Tổng cục Thống kê (GSO) cung cấp các chỉ số kinh tế vĩ mô cần thiết cho nghiên cứu định giá tài sản.\nĐối với các nhà nghiên cứu học thuật, sự phân mảnh này theo truyền thống liên quan đến những sự đánh đổi khó khăn giữa chi phí, phạm vi bao phủ, chất lượng dữ liệu và tính dễ tiếp cận. Các nhà cung cấp thương mại như FiinGroup cung cấp dữ liệu sạch, được chuẩn hóa nhưng yêu cầu phí đăng ký có thể là rào cản đối với các nhà nghiên cứu cá nhân và các tổ chức nhỏ hơn. Các giải pháp thay thế mã nguồn mở cung cấp quyền truy cập miễn phí nhưng thường yêu cầu nỗ lực làm sạch và xác thực dữ liệu đáng kể. Việc thu thập dữ liệu thủ công từ các trang web của chính phủ tốn thời gian và dễ xảy ra sai sót.\nMay mắn thay, bức tranh tổng thể đã được cải thiện đáng kể với sự xuất hiện của Datacore như một nền tảng dữ liệu thống nhất cho thị trường tài chính Việt Nam. Qua kinh nghiệm làm việc với dữ liệu tài chính Việt Nam trong nhiều dự án nghiên cứu, Datacore đã chứng minh là giải pháp thiết thực nhất cho nghiên cứu học thuật. Nền tảng này hợp nhất dữ liệu từ nhiều nguồn khác nhau, bao gồm giá cổ phiếu, thông tin cơ bản về doanh nghiệp, chỉ số thị trường, các chỉ số kinh tế vĩ mô và dữ liệu thay thế, vào một giao diện duy nhất, dễ truy cập với API được tài liệu hóa đầy đủ.\nĐiều làm nên sự khác biệt giữa Datacore và các nhà cung cấp thương mại truyền thống như FiinGroup không chỉ đơn thuần là việc tổng hợp dữ liệu. Trong khi FiinGroup từ lâu đã là nhà cung cấp hàng đầu trong lĩnh vực này, một số yếu tố khiến Datacore trở nên đặc biệt hấp dẫn đối với các nghiên cứu thực nghiệm nghiêm túc:\nTrong suốt chương này, chúng tôi sử dụng Datacore làm nguồn dữ liệu chính. Bằng cách tập trung thu thập dữ liệu thông qua một nền tảng duy nhất, chúng tôi được hưởng lợi từ các định dạng dữ liệu nhất quán, điều chỉnh hành động doanh nghiệp đáng tin cậy và phạm vi bao phủ thị trường toàn diện bao gồm HOSE, HNX và UPCoM. Các ví dụ mã sau đây minh họa cách thức nghiên cứu tài chính Việt Nam trở nên đơn giản hơn khi các rào cản truy cập dữ liệu được giảm thiểu.\nBảng dưới đây tóm tắt các nguồn dữ liệu chính cho nghiên cứu tài chính Việt Nam:\nNguồn dữ liệu tài chính Việt Nam {#tbl-data-sources}",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Truy cập và quản lý dữ liệu tài chính VN</span>"
    ]
  },
  {
    "objectID": "01_accessing_and_managing_financial_data.html#tổng-quan-về-các-nguồn-dữ-liệu-tài-chính-việt-nam",
    "href": "01_accessing_and_managing_financial_data.html#tổng-quan-về-các-nguồn-dữ-liệu-tài-chính-việt-nam",
    "title": "2  Truy cập và quản lý dữ liệu tài chính VN",
    "section": "",
    "text": "Kiến trúc ưu tiên API: Datacore được xây dựng từ đầu để truy cập lập trình, giúp tích hợp liền mạch với Python, R và các quy trình nghiên cứu khác. Ngược lại, việc truy cập dữ liệu của FiinGroup thường yêu cầu tải xuống thủ công hoặc giao diện dựa trên Excel rườm rà, gây khó khăn cho việc tái tạo kết quả.\nHiệu quả chi phí: Các nhà nghiên cứu học thuật thường hoạt động trong điều kiện ngân sách hạn chế. Datacore cung cấp cấu trúc giá cả cạnh tranh, giúp tiếp cận phạm vi thị trường toàn diện mà không cần trả phí đăng ký cao như các nhà cung cấp truyền thống.\nXử lý các hoạt động của công ty: Một thách thức dai dẳng với dữ liệu Việt Nam là việc điều chỉnh chính xác các trường hợp chia tách cổ phiếu, phát hành cổ phiếu thưởng và phát hành quyền mua cổ phiếu. Datacore triển khai các phương pháp điều chỉnh minh bạch với tài liệu rõ ràng, trong khi các nhà cung cấp truyền thống thường áp dụng các điều chỉnh không nhất quán hoặc không có giải thích đầy đủ.\nTần suất cập nhật: Datacore duy trì cập nhật dữ liệu gần như thời gian thực với dấu thời gian rõ ràng, cho phép nghiên cứu sự kiện và tái cân bằng danh mục đầu tư kịp thời. Các nhà cung cấp truyền thống thường gặp phải tình trạng chậm trễ công bố dữ liệu, điều này có thể ảnh hưởng đến các nghiên cứu cần dữ liệu hiện hành.\nPhạm vi bao phủ: Ngoài dữ liệu giá cả và dữ liệu cơ bản tiêu chuẩn, Datacore tích hợp dữ liệu thay thế và các chỉ số kinh tế vĩ mô vào một lược đồ thống nhất. Điều này loại bỏ nhu cầu hợp nhất các tập dữ liệu từ nhiều nguồn khác nhau, một quy trình tiềm ẩn nguy cơ sai sót và tiêu tốn thời gian nghiên cứu quý báu.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNguồn dữ liệu\nPhạm vi phủ sóng\nLoại truy cập\nĐiểm mạnh chính\nHạn chế\n\n\n\n\nDatacore\nGiá cả, yếu tố cơ bản, chỉ số, kinh tế vĩ mô, phái sinh\nAPI\nNền tảng thống nhất, truy cập lập trình, phạm vi phủ sóng toàn diện, phương pháp luận minh bạch\nNền tảng mới hơn\n\n\nFiinGroup\nPhạm vi thị trường toàn diện\nThương mại\nUy tín đã được khẳng định, được các tổ chức tin dùng\nChi phí cao, truy cập thủ công, API hạn chế\n\n\nTrang web HOSE/HNX\nDữ liệu giao dịch chính thức\nMiễn phí (thủ công)\nChính xác, thời gian thực\nKhông cần API, không cần thu thập thủ công\n\n\nTổng cục Thống kê Quốc gia (GSO) (gso.gov.vn)\nCác chỉ số kinh tế vĩ mô\nMiễn phí (thủ công)\nThống kê chính thức của chính phủ\nCập nhật không thường xuyên, không có API\n\n\nNgân hàng Nhà nước Việt Nam (sbv.gov.vn)\nChính sách tiền tệ, lãi suất\nMiễn phí (thủ công)\nDữ liệu ngân hàng trung ương\nChỉ tải xuống thủ công\n\n\nCafeF/VnExpress\nTin tức, thông báo\nMiễn phí\nTâm lý thị trường, sự kiện\nDữ liệu không cấu trúc, cần xử lý bằng NLP",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Truy cập và quản lý dữ liệu tài chính VN</span>"
    ]
  },
  {
    "objectID": "01_accessing_and_managing_financial_data.html#dữ-liệu-thị-trường-chứng-khoán",
    "href": "01_accessing_and_managing_financial_data.html#dữ-liệu-thị-trường-chứng-khoán",
    "title": "2  Truy cập và quản lý dữ liệu tài chính VN",
    "section": "2.2 Dữ liệu thị trường chứng khoán",
    "text": "2.2 Dữ liệu thị trường chứng khoán\nDataFrame kết quả chứa các mã định danh an toàn thiết yếu bao gồm mã chứng khoán, tên công ty bằng cả tiếng Việt và tiếng Anh, thị trường giao dịch, phân loại ngành theo Hệ thống Phân loại ngành công nghiệp Việt Nam (VSIC), và nhiều cờ chỉ thị trạng thái đặc biệt như hạn chế sở hữu nước ngoài hoặc đình chỉ giao dịch.\n\n2.2.1 Dữ liệu giá lịch sử\n\n\n2.2.2 Thông tin cơ bản và Báo cáo tài chính\nNgoài dữ liệu giá cả, phân tích cơ bản đòi hỏi phải tiếp cận báo cáo tài chính của doanh nghiệp, bao gồm bảng cân đối kế toán, báo cáo kết quả kinh doanh và báo cáo lưu chuyển tiền tệ. Các công ty niêm yết công khai của Việt Nam phải công bố báo cáo tài chính hàng quý và hàng năm theo Chuẩn mực Kế toán Việt Nam (VAS), có một số điểm khác biệt so với Chuẩn mực Báo cáo Tài chính Quốc tế (IFRS). Hiểu rõ những khác biệt này rất quan trọng khi so sánh các công ty Việt Nam với các công ty cùng ngành trên thế giới hoặc khi áp dụng các mô hình được phát triển bằng dữ liệu của Mỹ hoặc châu Âu.\nNhững điểm khác biệt chính giữa VAS và IFRS ảnh hưởng đến phân tích tài chính bao gồm:\n\nGhi nhận doanh thu: VAS cho phép linh hoạt hơn về thời điểm ghi nhận doanh thu so với IFRS 15.\nCông cụ tài chính: VAS có hướng dẫn chưa toàn diện về việc đo lường giá trị hợp lý.\nKế toán thuê tài sản: VAS không yêu cầu vốn hóa thuê hoạt động theo IFRS 16.\nLợi thế thương mại: VAS yêu cầu khấu hao trong khi IFRS chỉ yêu cầu kiểm tra suy giảm giá trị.\n\n\n\n2.2.3 Các hoạt động và sự kiện của công ty\nViệc xử lý chính xác các hành động của công ty là rất quan trọng để tính toán lợi nhuận chính xác và duy trì tính toàn vẹn của dữ liệu. Các công ty Việt Nam thường xuyên tham gia vào các hành động của công ty bao gồm tiền cổ tức, cổ tức thưởng (cổ phiếu quà tặng), phát hành thêm cổ phiếu theo quyền và chia cổ phiếu.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Truy cập và quản lý dữ liệu tài chính VN</span>"
    ]
  },
  {
    "objectID": "01_accessing_and_managing_financial_data.html#các-chỉ-số-thị-trường-và-tiêu-chuẩn",
    "href": "01_accessing_and_managing_financial_data.html#các-chỉ-số-thị-trường-và-tiêu-chuẩn",
    "title": "2  Truy cập và quản lý dữ liệu tài chính VN",
    "section": "2.3 Các Chỉ Số Thị Trường và Tiêu Chuẩn",
    "text": "2.3 Các Chỉ Số Thị Trường và Tiêu Chuẩn\nViệc xây dựng các tiêu chuẩn tham chiếu phù hợp là yếu tố cơ bản trong đánh giá hiệu suất và ước lượng mô hình nhân tố. Thị trường Việt Nam có một số chỉ số phục vụ các mục đích khác nhau trong nghiên cứu tài chính.\n\n\n\n\n\n\n\n\n\nMục lục\nSàn giao dịch\nMô tả\nTrường hợp sử dụng\n\n\n\n\nChỉ số VN\nHOSE\nTất cả cổ phiếu niêm yết trên HOSE\nChỉ số tham chiếu thị trường chung\n\n\nChỉ số VN30\nHOSE\n30 công ty lớn nhất, có tính thanh khoản cao nhất\nChuẩn mực đầu tư\n\n\nChỉ số HNX\nHNX\nTất cả cổ phiếu niêm yết trên HNX\nChỉ số tham chiếu vốn hóa trung bình\n\n\nHNX30-Index\nHNX\n30 cổ phiếu lớn nhất HNX\nHNX vốn hóa lớn\n\n\nVNAllShare\nKết hợp\nHOSE + HNX\nTổng thị trường\n\n\nVN100\nTổng hợp\n100 cổ phiếu hàng đầu\nVốn hóa lớn/trung bình\n\n\n\nChỉ số thị trường Việt Nam {#tbl-indices}\nChỉ số VN-Index, theo dõi tất cả các cổ phiếu niêm yết trên Sở giao dịch chứng khoán Hà Nội (HOSE), là chỉ số chuẩn được theo dõi rộng rãi nhất và là thước đo chính về hiệu suất tổng thể của thị trường. Chỉ số HNX-Index bao gồm các cổ phiếu trên sàn giao dịch Hà Nội, trong khi chỉ số VN30-Index theo dõi 30 cổ phiếu lớn nhất và có tính thanh khoản cao nhất trên HOSE.\nĐối với nghiên cứu định giá tài sản, chỉ số VN30 đặc biệt có giá trị vì nó đại diện cho phạm vi đầu tư của các nhà đầu tư tổ chức và là tài sản cơ sở cho các hợp đồng phái sinh có tính thanh khoản cao nhất tại Việt Nam. Các cổ phiếu thành phần được xem xét định kỳ nửa năm một lần dựa trên vốn hóa thị trường, tính thanh khoản và yêu cầu về tỷ lệ cổ phiếu lưu hành tự do.\n\n# Retrieve VN-Index historical data\n\n\n2.3.1 Dữ liệu thành phần chỉ số\nĐể xây dựng mô hình nhân tố và phân tích danh mục đầu tư, việc truy cập vào danh sách các thành phần chỉ số và trọng số của chúng là thiết yếu. Mặc dù dữ liệu thành phần chính thức yêu cầu đăng ký nguồn dữ liệu giao dịch, chúng ta có thể ước tính thành viên chỉ số bằng cách sử dụng bộ lọc về vốn hóa thị trường và thanh khoản.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Truy cập và quản lý dữ liệu tài chính VN</span>"
    ]
  },
  {
    "objectID": "01_accessing_and_managing_financial_data.html#dữ-liệu-kinh-tế-vĩ-mô-từ-các-nguồn-việt-nam",
    "href": "01_accessing_and_managing_financial_data.html#dữ-liệu-kinh-tế-vĩ-mô-từ-các-nguồn-việt-nam",
    "title": "2  Truy cập và quản lý dữ liệu tài chính VN",
    "section": "2.4 Dữ liệu kinh tế vĩ mô từ các nguồn Việt Nam",
    "text": "2.4 Dữ liệu kinh tế vĩ mô từ các nguồn Việt Nam\nCác mô hình định giá tài sản thường kết hợp các biến số kinh tế vĩ mô làm yếu tố dự báo lợi nhuận kỳ vọng hoặc làm biến số trạng thái trong các mô hình có điều kiện. Đối với thị trường Việt Nam, dữ liệu kinh tế vĩ mô liên quan chủ yếu đến từ hai nguồn: Tổng cục Thống kê (GSO) và Ngân hàng Nhà nước Việt Nam (SBV).\n\n2.4.1 Các chỉ số kinh tế vĩ mô chính\nCác biến số kinh tế vĩ mô sau đây đặc biệt quan trọng đối với nghiên cứu tài chính Việt Nam:\n\nChỉ số giá tiêu dùng (CPI): Cần thiết để tính toán lợi nhuận thực tế và định giá điều chỉnh theo lạm phát. Việt Nam đã trải qua các giai đoạn lạm phát cao, đặc biệt là trong giai đoạn 2008 và 2011 khi CPI hàng năm vượt quá 20%.\nChỉ số sản xuất công nghiệp (IPI): Đại diện cho hoạt động kinh tế và điều kiện chu kỳ kinh doanh.\nCung tiền (M2): Chỉ số thể hiện lập trường chính sách tiền tệ và điều kiện thanh khoản.\nTăng trưởng tín dụng: Tăng trưởng cho vay của ngân hàng, một động lực chính của hoạt động kinh tế trong hệ thống tài chính do ngân hàng chi phối tại Việt Nam.\nTỷ giá hối đoái USD/VND: Rất quan trọng đối với các nhà đầu tư quốc tế và các công ty có rủi ro ngoại tệ.\nĐầu tư trực tiếp nước ngoài (FDI): Chỉ số về dòng vốn quốc tế và niềm tin kinh tế.\nCán cân thương mại: Động thái xuất nhập khẩu ảnh hưởng đến lợi nhuận doanh nghiệp.\n\nĐáng tiếc là, không giống như cơ sở dữ liệu FRED của Cục Dự trữ Liên bang Mỹ, dữ liệu kinh tế vĩ mô của Việt Nam không có sẵn thông qua các API tiêu chuẩn. Các nhà nghiên cứu thường phải tải dữ liệu thủ công từ trang web của Tổng cục Thống kê (GSO) và Ngân hàng Nhà nước Việt Nam (SBV) hoặc sử dụng các kỹ thuật thu thập dữ liệu từ web.\n\n# Structure for Vietnamese macroeconomic data\n\n\n\n2.4.2 Xấp xỉ lãi suất phi rủi ro\nViệc xác định lãi suất phi rủi ro phù hợp cho Việt Nam đặt ra những thách thức không gặp phải ở các thị trường phát triển. Không giống như thị trường trái phiếu kho bạc Mỹ, thị trường trái phiếu chính phủ Việt Nam tương đối kém thanh khoản với giao dịch thứ cấp hạn chế. Có một số phương án thay thế:\n\nLãi suất tái cấp vốn của Ngân hàng Nhà nước Việt Nam: Lãi suất chính sách do Ngân hàng Nhà nước Việt Nam quy định. Không dùng để đầu tư trực tiếp nhưng phản ánh lập trường chính sách tiền tệ.\nLợi suất trái phiếu chính phủ: Lợi suất trái phiếu chính phủ kỳ hạn một năm trở lên dựa trên kết quả đấu giá. Có tính đầu tư cao hơn nhưng tính thanh khoản thấp hơn so với trái phiếu kho bạc Mỹ.\nLãi suất liên ngân hàng: Lãi suất cho vay liên ngân hàng qua đêm hoặc kỳ hạn. Phản ánh chi phí huy động vốn ngắn hạn nhưng bao gồm rủi ro tín dụng.\nLãi suất điều chỉnh của Mỹ: Lãi suất trái phiếu kho bạc Mỹ cộng với dự báo giảm giá trị của VND, theo nguyên tắc ngang giá lãi suất không được bảo hiểm.\n\n\ndef calculate_risk_free_rate(macro_data, method=\"refinancing\"):\n    \"\"\"\n    Calculate risk-free rate proxy for Vietnamese market.\n    \n    Parameters\n    ----------\n    macro_data : pd.DataFrame\n        DataFrame with macroeconomic data\n    method : str\n        Method for risk-free rate: 'refinancing', 'bond', or 'adjusted_us'\n    \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with date and monthly risk-free rate\n    \"\"\"\n    if method == \"refinancing\":\n        # Use SBV refinancing rate, convert annual to monthly\n        rf = macro_data[[\"date\", \"refinancing_rate\"]].copy()\n        rf[\"rf_monthly\"] = rf[\"refinancing_rate\"] / 12 / 100\n        \n    elif method == \"adjusted_us\":\n        # US rate + expected VND depreciation\n        # Requires additional data on US rates and exchange rate expectations\n        pass\n    \n    return rf[[\"date\", \"rf_monthly\"]]",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Truy cập và quản lý dữ liệu tài chính VN</span>"
    ]
  },
  {
    "objectID": "01_accessing_and_managing_financial_data.html#thiết-lập-cơ-sở-dữ-liệu-cho-dữ-liệu-tài-chính-việt-nam",
    "href": "01_accessing_and_managing_financial_data.html#thiết-lập-cơ-sở-dữ-liệu-cho-dữ-liệu-tài-chính-việt-nam",
    "title": "2  Truy cập và quản lý dữ liệu tài chính VN",
    "section": "2.5 Thiết lập cơ sở dữ liệu cho dữ liệu tài chính Việt Nam",
    "text": "2.5 Thiết lập cơ sở dữ liệu cho dữ liệu tài chính Việt Nam\nQuản lý dữ liệu tài chính từ nhiều nguồn và định dạng khác nhau đòi hỏi một phương pháp lưu trữ dữ liệu có hệ thống. Chúng tôi khuyến nghị sử dụng SQLite làm công cụ cơ sở dữ liệu chính vì một số lý do: nó không yêu cầu thiết lập máy chủ, lưu trữ toàn bộ cơ sở dữ liệu trong một tệp duy nhất có thể di chuyển được, hỗ trợ các truy vấn SQL chuẩn và tích hợp liền mạch với Python thông qua mô-đun sqlite3 tích hợp sẵn.\n\n2.5.1 Thiết kế lược đồ cơ sở dữ liệu\nSơ đồ cơ sở dữ liệu của chúng tôi được thiết kế để hỗ trợ các truy vấn hiệu quả cho các tác vụ nghiên cứu thông thường đồng thời duy trì tính toàn vẹn dữ liệu. Chúng tôi tạo các bảng riêng biệt cho các loại dữ liệu khác nhau với các mối quan hệ phù hợp.\n\nimport os\nimport sqlite3\n\n# Create data directory if it doesn't exist\nif not os.path.exists(\"data\"):\n    os.makedirs(\"data\")\n\n# Initialize SQLite database connection\ntidy_finance_python = sqlite3.connect(\n    \"data/tidy_finance_python.sqlite\"\n)\n\n\n\n2.5.2 Lưu trữ dữ liệu\nSau khi thiết lập lược đồ cơ sở dữ liệu, chúng ta có thể lưu trữ dữ liệu đã thu thập bằng phương thức to_sql() của thư viện pandas.\n\n# Store stock listing data\ncommon_stocks.to_sql(\n    name=\"stock_master\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store stock price data\nstock_prices.to_sql(\n    name=\"stock_prices_daily\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store market indices\nvn_index.to_sql(\n    name=\"market_indices\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)\n\n# Store factor returns\nfactors_vietnam.to_sql(\n    name=\"factors_monthly\",\n    con=tidy_finance_python,\n    if_exists=\"replace\",\n    index=False\n)",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Truy cập và quản lý dữ liệu tài chính VN</span>"
    ]
  },
  {
    "objectID": "01_accessing_and_managing_financial_data.html#truy-vấn-và-cập-nhật-cơ-sở-dữ-liệu",
    "href": "01_accessing_and_managing_financial_data.html#truy-vấn-và-cập-nhật-cơ-sở-dữ-liệu",
    "title": "2  Truy cập và quản lý dữ liệu tài chính VN",
    "section": "2.6 Truy vấn và cập nhật cơ sở dữ liệu",
    "text": "2.6 Truy vấn và cập nhật cơ sở dữ liệu\nSau khi dữ liệu được lưu trữ trong cơ sở dữ liệu, việc truy xuất rất đơn giản bằng cách sử dụng các truy vấn SQL. Hàm read_sql_query() của thư viện pandas thực thi một câu lệnh SQL và trả về kết quả dưới dạng DataFrame.\n\n# Query stock prices for specific symbols and date range\nquery = \"\"\"\nSELECT date, symbol, close, volume\nFROM stock_prices_daily\nWHERE symbol IN ('VNM', 'VIC', 'FPT', 'VHM', 'VCB')\n  AND date &gt;= '2020-01-01'\nORDER BY symbol, date\n\"\"\"\n\nselected_stocks = pd.read_sql_query(\n    sql=query,\n    con=tidy_finance_python,\n    parse_dates=[\"date\"]\n)\n\n# Query factor data merged with market returns\nquery_factors = \"\"\"\nSELECT f.date, f.mkt_rf, f.smb, f.hml, f.rf,\n       m.cpi_yoy, m.credit_growth\nFROM factors_monthly f\nLEFT JOIN macro_monthly m ON f.date = m.date\nWHERE f.date &gt;= '2015-01-01'\nORDER BY f.date\n\"\"\"\n\nfactor_data = pd.read_sql_query(\n    sql=query_factors,\n    con=tidy_finance_python,\n    parse_dates=[\"date\"]\n)\n\n\n2.6.1 Bảo trì cơ sở dữ liệu\nViệc bảo trì cơ sở dữ liệu thường xuyên đảm bảo hiệu suất tối ưu và tính toàn vẹn của dữ liệu.\n\n# Optimize database\ntidy_finance_python.execute(\"VACUUM\")\n\n# Check database integrity\nintegrity_check = pd.read_sql_query(\n    \"PRAGMA integrity_check\",\n    tidy_finance_python\n)\nprint(f\"Integrity check: {integrity_check.iloc[0, 0]}\")\n\n# Get database statistics\ntable_stats = pd.read_sql_query(\"\"\"\n    SELECT name, \n           (SELECT COUNT(*) FROM stock_prices_daily) as price_rows,\n           (SELECT COUNT(*) FROM stock_master) as stock_count,\n           (SELECT COUNT(*) FROM factors_monthly) as factor_months\n    FROM sqlite_master\n    WHERE type='table' AND name='stock_master'\n\"\"\", tidy_finance_python)\n\nprint(table_stats)\n\n# Close connection when done\ntidy_finance_python.close()",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Truy cập và quản lý dữ liệu tài chính VN</span>"
    ]
  },
  {
    "objectID": "01_accessing_and_managing_financial_data.html#các-nguồn-dữ-liệu-thay-thế-cho-thị-trường-việt-nam",
    "href": "01_accessing_and_managing_financial_data.html#các-nguồn-dữ-liệu-thay-thế-cho-thị-trường-việt-nam",
    "title": "2  Truy cập và quản lý dữ liệu tài chính VN",
    "section": "2.7 Các nguồn dữ liệu thay thế cho thị trường Việt Nam",
    "text": "2.7 Các nguồn dữ liệu thay thế cho thị trường Việt Nam\nNgoài các dữ liệu truyền thống về giá cả và cơ bản, các nhà nghiên cứu ngày càng kết hợp nhiều nguồn dữ liệu thay thế để có được những hiểu biết độc đáo về động thái thị trường.\n\n2.7.1 Dữ liệu về dòng vốn đầu tư nước ngoài\nDữ liệu về dòng vốn đầu tư nước ngoài đặc biệt có giá trị do vai trò quan trọng của vốn nước ngoài trên thị trường chứng khoán Việt Nam. Ủy ban Chứng khoán Nhà nước công bố số liệu thống kê sở hữu nước ngoài theo từng loại chứng khoán hàng ngày.\n\n\n2.7.2 Dữ liệu tin tức và tâm lý\nPhân tích tâm lý thị trường từ các nguồn tin tài chính Việt Nam cung cấp một hướng nghiên cứu khác. Các trang tin lớn như CafeF, VnExpress Finance và Vietstock đăng tải tin tức thời gian thực, có thể được phân tích để nắm bắt tâm lý thị trường.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Truy cập và quản lý dữ liệu tài chính VN</span>"
    ]
  },
  {
    "objectID": "01_accessing_and_managing_financial_data.html#những-điểm-chính-cần-ghi-nhớ",
    "href": "01_accessing_and_managing_financial_data.html#những-điểm-chính-cần-ghi-nhớ",
    "title": "2  Truy cập và quản lý dữ liệu tài chính VN",
    "section": "2.8 Những điểm chính cần ghi nhớ",
    "text": "2.8 Những điểm chính cần ghi nhớ\n\nHiểu biết về cấu trúc thị trường: Thị trường tài chính Việt Nam hoạt động trên ba sàn giao dịch (HOSE, HNX, UPCoM) với những đặc điểm riêng biệt bao gồm giới hạn sở hữu nước ngoài, hạn chế biên độ giao dịch và chu kỳ thanh toán T+2. Các nhà nghiên cứu phải tính đến những đặc điểm thể chế này trong phân tích thực nghiệm.\nNhững thách thức về dữ liệu kinh tế vĩ mô: Không giống như các thị trường phát triển với các API tiêu chuẩn hóa (ví dụ: FRED), dữ liệu kinh tế vĩ mô của Việt Nam đòi hỏi phải thu thập thủ công từ các nguồn chính phủ (Tổng cục Thống kê, Ngân hàng Nhà nước). Các nhà nghiên cứu cần lên kế hoạch cho nỗ lực thu thập dữ liệu bổ sung này và triển khai các phương pháp quản lý dữ liệu một cách có hệ thống.\nQuy trình làm việc tập trung vào cơ sở dữ liệu: SQLite cung cấp giải pháp cơ sở dữ liệu hiệu quả và dễ di chuyển để quản lý dữ liệu tài chính Việt Nam trong các dự án nghiên cứu. Cách tiếp cận cơ sở dữ liệu có cấu trúc cho phép các quy trình nghiên cứu có thể tái tạo, các truy vấn hiệu quả và việc chia sẻ dữ liệu dễ dàng giữa các cộng tác viên.\nYếu tố then chốt về chất lượng dữ liệu: Việc xác thực chất lượng dữ liệu đặc biệt quan trọng đối với dữ liệu thị trường mới nổi. Thực hiện các kiểm tra có hệ thống đối với các giá trị thiếu, lợi nhuận bất thường, các mục trùng lặp và xác thực chéo nguồn giúp đảm bảo độ tin cậy và khả năng tái tạo của nghiên cứu.\nCơ hội từ các nguồn dữ liệu thay thế: Dòng vốn đầu tư nước ngoài, thông báo của các công ty và tâm lý truyền thông cung cấp những cơ hội nghiên cứu độc đáo trên thị trường Việt Nam, có thể bổ sung cho phân tích giá và phân tích cơ bản truyền thống. Các nguồn dữ liệu này có thể tiết lộ những hiểu biết về động thái thị trường mà các bộ dữ liệu tiêu chuẩn không thể nắm bắt được.\nBảo trì liên tục: Cơ sở dữ liệu tài chính cần được bảo trì liên tục, bao gồm cập nhật tăng dần, kiểm tra tính toàn vẹn và tối ưu hóa. Việc thiết lập các quy trình cập nhật có hệ thống đảm bảo tính cập nhật của dữ liệu và hiệu suất cơ sở dữ liệu theo thời gian.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Truy cập và quản lý dữ liệu tài chính VN</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html",
    "href": "02_datacore_data.html",
    "title": "3  Dữ liệu Datacore",
    "section": "",
    "text": "3.1 Các tùy chọn truy cập dữ liệu\nChương này giới thiệu Datacore, nền tảng dữ liệu của Việt Nam dành cho nghiên cứu học thuật, doanh nghiệp và chính phủ. Datacore cung cấp các bộ dữ liệu tài chính và kinh tế toàn diện, bao gồm dữ liệu giao dịch lịch sử, các yếu tố cơ bản của công ty và các chỉ số kinh tế vĩ mô thiết yếu cho nghiên cứu tài chính có thể tái tạo. Chúng ta sử dụng Datacore làm nguồn dữ liệu chính xuyên suốt cuốn sách này.\nĐộc giả có thể truy cập dữ liệu được sử dụng trong cuốn sách này thông qua một số kênh sau:",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html#các-tùy-chọn-truy-cập-dữ-liệu",
    "href": "02_datacore_data.html#các-tùy-chọn-truy-cập-dữ-liệu",
    "title": "3  Dữ liệu Datacore",
    "section": "",
    "text": "Gói đăng ký dành cho tổ chức: Nhiều trường đại học và viện nghiên cứu đăng ký sử dụng Datacore. Hãy liên hệ với thư viện hoặc văn phòng nghiên cứu của bạn để nhận thông tin đăng nhập. Nếu tổ chức của bạn chưa có gói đăng ký, hãy cân nhắc yêu cầu một gói thông qua quy trình mua sắm của thư viện — Datacore cung cấp giá ưu đãi cho mục đích học thuật.\nBộ dữ liệu demo: Datacore cung cấp bộ dữ liệu demo cho phép bạn chạy các ví dụ mã trong sách này với dữ liệu mẫu.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html#tổng-quan-chương",
    "href": "02_datacore_data.html#tổng-quan-chương",
    "title": "3  Dữ liệu Datacore",
    "section": "3.2 Tổng quan chương",
    "text": "3.2 Tổng quan chương\nChương này được tổ chức như sau. Trước tiên, chúng ta thiết lập kết nối với cơ sở hạ tầng lưu trữ đám mây của Datacore. Sau đó, chúng ta tải xuống và chuẩn bị dữ liệu cơ bản của công ty, bao gồm các khoản mục bảng cân đối kế toán, các biến số báo cáo kết quả kinh doanh và các chỉ số phái sinh cần thiết cho nghiên cứu định giá tài sản. Tiếp theo, chúng ta truy xuất và xử lý dữ liệu giá cổ phiếu, tính toán lợi nhuận, vốn hóa thị trường và lợi nhuận vượt trội. Cuối cùng, chúng ta kết hợp các tập dữ liệu này và cung cấp số liệu thống kê mô tả đặc trưng cho thị trường chứng khoán Việt Nam.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html#thiết-lập-môi-trường",
    "href": "02_datacore_data.html#thiết-lập-môi-trường",
    "title": "3  Dữ liệu Datacore",
    "section": "3.3 Thiết lập môi trường",
    "text": "3.3 Thiết lập môi trường\nChúng ta bắt đầu bằng cách tải các gói Python được sử dụng xuyên suốt chương này. Các gói cốt lõi bao gồm pandas để thao tác dữ liệu, numpy cho các phép toán số học và sqlite3 để quản lý cơ sở dữ liệu cục bộ. Chúng ta cũng nhập các thư viện trực quan hóa để tạo ra các hình ảnh chất lượng cao phục vụ cho việc xuất bản.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom datetime import datetime\nfrom io import BytesIO\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format, percent_format\n\nChúng ta thiết lập kết nối đến cơ sở dữ liệu SQLite cục bộ, đóng vai trò là kho lưu trữ trung tâm cho tất cả dữ liệu đã được xử lý. Cơ sở dữ liệu này đã được giới thiệu trong chương trước và sẽ lưu trữ các tập dữ liệu đã được làm sạch để sử dụng trong các phân tích tiếp theo.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nChúng ta xác định phạm vi ngày tháng cho việc thu thập dữ liệu. Thị trường chứng khoán Việt Nam bắt đầu hoạt động vào tháng 7 năm 2000 với sự thành lập của Sở Giao dịch Chứng khoán Thành phố Hồ Chí Minh (HOSE), vì vậy giai đoạn mẫu của chúng ta bắt đầu từ năm 2000 và kéo dài đến hết năm 2024.\n\nstart_date = \"2000-01-01\"\nend_date = \"2024-12-31\"",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html#kết-nối-với-datacore",
    "href": "02_datacore_data.html#kết-nối-với-datacore",
    "title": "3  Dữ liệu Datacore",
    "section": "3.4 Kết nối với Datacore",
    "text": "3.4 Kết nối với Datacore\nDatacore cung cấp dữ liệu thông qua hệ thống lưu trữ đối tượng dựa trên đám mây được xây dựng trên MinIO, một cơ sở hạ tầng lưu trữ tương thích với S3. Kiến trúc này cho phép truy cập hiệu quả, lập trình được vào các tập dữ liệu lớn mà không bị hạn chế bởi các kết nối cơ sở dữ liệu truyền thống. Để truy cập dữ liệu, bạn cần thông tin xác thực do Datacore cung cấp khi đăng ký: URL điểm cuối, khóa truy cập và khóa bí mật.\nLớp sau đây thiết lập kết nối với hệ thống lưu trữ của Datacore. Thông tin đăng nhập được lưu trữ dưới dạng biến môi trường để đảm bảo an toàn, tuân theo các thực tiễn tốt nhất về quản lý thông tin đăng nhập trong môi trường điện toán nghiên cứu.\n\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass DatacoreConnection:\n    \"\"\"\n    Connection handler for Datacore's MinIO-based storage system.\n    \n    This class manages authentication and provides methods for\n    accessing financial datasets stored in Datacore's cloud infrastructure.\n    \n    Attributes\n    ----------\n    s3 : boto3.client\n        S3-compatible client for interacting with Datacore storage\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize connection using environment variables.\"\"\"\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n        \n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n    \n    def test_connection(self):\n        \"\"\"Verify connection by listing available buckets.\"\"\"\n        response = self.s3.list_buckets()\n        print(\"Connected successfully. Available buckets:\")\n        for bucket in response.get(\"Buckets\", []):\n            print(f\"  - {bucket['Name']}\")\n    \n    def list_objects(self, bucket_name, prefix=\"\"):\n        \"\"\"List objects in a bucket with optional prefix filter.\"\"\"\n        response = self.s3.list_objects_v2(\n            Bucket=bucket_name, \n            Prefix=prefix\n        )\n        return [obj[\"Key\"] for obj in response.get(\"Contents\", [])]\n    \n    def read_excel(self, bucket_name, key):\n        \"\"\"Read an Excel file from Datacore storage.\"\"\"\n        obj = self.s3.get_object(Bucket=bucket_name, Key=key)\n        return pd.read_excel(BytesIO(obj[\"Body\"].read()))\n    \n    def read_csv(self, bucket_name, key, **kwargs):\n        \"\"\"Read a CSV file from Datacore storage.\"\"\"\n        obj = self.s3.get_object(Bucket=bucket_name, Key=key)\n        return pd.read_csv(BytesIO(obj[\"Body\"].read()), **kwargs)\n\nSau khi định nghĩa lớp kết nối, chúng ta có thể thiết lập kết nối và xác minh quyền truy cập vào các kho dữ liệu của Datacore.\n\n# Initialize connection\nconn = DatacoreConnection()\nconn.test_connection()\n\n# Get bucket name from environment\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nConnected successfully. Available buckets:\n  - dsteam-data\n  - rawbctc",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html#dữ-liệu-cơ-bản-của-công-ty",
    "href": "02_datacore_data.html#dữ-liệu-cơ-bản-của-công-ty",
    "title": "3  Dữ liệu Datacore",
    "section": "3.5 Dữ liệu cơ bản của công ty",
    "text": "3.5 Dữ liệu cơ bản của công ty\nDữ liệu kế toán của các công ty là yếu tố thiết yếu cho việc phân tích danh mục đầu tư, xây dựng yếu tố và nghiên cứu định giá. Datacore cung cấp dữ liệu cơ bản toàn diện cho các công ty niêm yết của Việt Nam, bao gồm báo cáo tài chính hàng năm và hàng quý được lập theo Chuẩn mực Kế toán Việt Nam (VAS).\n\n3.5.1 Hiểu về Báo cáo Tài chính Việt Nam\nTrước khi xử lý dữ liệu, điều quan trọng là phải hiểu cấu trúc của báo cáo tài chính Việt Nam. Các công ty Việt Nam tuân theo Chuẩn mực kế toán Việt Nam (VAS), có nhiều điểm tương đồng với Chuẩn mực báo cáo tài chính quốc tế (IFRS) nhưng cũng có những khác biệt đáng chú ý:\n\nNăm tài chính: Hầu hết các công ty Việt Nam sử dụng năm tài chính theo lịch dương, kết thúc vào ngày 31 tháng 12, mặc dù một số công ty (đặc biệt là trong lĩnh vực bán lẻ và nông nghiệp) sử dụng năm tài chính kết thúc vào ngày khác.\nTần suất báo cáo: Các công ty niêm yết phải công bố báo cáo tài chính hàng quý trong vòng 20 ngày kể từ ngày kết thúc quý và báo cáo kiểm toán hàng năm trong vòng 90 ngày kể từ ngày kết thúc năm tài chính.\nCác định dạng đặc thù theo ngành: Các công ty trong lĩnh vực ngân hàng, bảo hiểm và chứng khoán tuân theo các định dạng báo cáo chuyên biệt, khác với định dạng tiêu chuẩn của ngành.\nTiền tệ: Tất cả các số liệu được báo cáo bằng Đồng Việt Nam (VND). Do giá trị danh nghĩa lớn (hàng triệu đến hàng nghìn tỷ VND), chúng ta thường quy đổi các số liệu thành hàng triệu hoặc hàng tỷ để dễ đọc hơn.\n\n\n\n3.5.2 Tải xuống dữ liệu cơ bản\nDatacore tổ chức dữ liệu cơ bản trong các tệp Excel được phân vùng theo khoảng thời gian để truy cập hiệu quả. Chúng ta tải xuống và ghép nối các tệp này để tạo ra một tập dữ liệu toàn diện bao gồm toàn bộ giai đoạn mẫu của chúng ta.\n\n# Define paths to fundamentals data files\nfundamentals_paths = [\n    \"fundamental_annual_1767674486317/fundamental_annual_1.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_2.xlsx\",\n    \"fundamental_annual_1767674486317/fundamental_annual_3.xlsx\",\n]\n\n# Download and combine all files\nfundamentals_list = []\nfor path in fundamentals_paths:\n    df_temp = conn.read_excel(bucket_name, path)\n    fundamentals_list.append(df_temp)\n    print(f\"Downloaded: {path} ({len(df_temp):,} rows)\")\n\ndf_fundamentals_raw = pd.concat(fundamentals_list, ignore_index=True)\nprint(f\"\\nTotal observations: {len(df_fundamentals_raw):,}\")\n\n/home/mikenguyen/project/tidyfinance/.venv/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n\n\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_1.xlsx (10,000 rows)\n\n\n/home/mikenguyen/project/tidyfinance/.venv/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n\n\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_2.xlsx (10,000 rows)\n\n\n/home/mikenguyen/project/tidyfinance/.venv/lib/python3.13/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n\n\nDownloaded: fundamental_annual_1767674486317/fundamental_annual_3.xlsx (2,821 rows)\n\nTotal observations: 22,821\n\n\n\n\n3.5.3 Nguyên tắc cơ bản về vệ sinh và tiêu chuẩn hóa\nDữ liệu cơ bản thô cần trải qua nhiều bước làm sạch để đảm bảo tính nhất quán và khả năng sử dụng. Chúng ta chuẩn hóa tên biến, xử lý các giá trị thiếu và tạo ra các biến dẫn xuất thường được sử dụng trong nghiên cứu định giá tài sản.\n\ndef clean_fundamentals(df):\n    \"\"\"\n    Clean and standardize company fundamentals data.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Raw fundamentals data from Datacore\n    \n    Returns\n    -------\n    pd.DataFrame\n        Cleaned fundamentals with standardized column names\n    \"\"\"\n    df = df.copy()\n    \n    # Standardize identifiers\n    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\n    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n    \n    # Drop rows with missing identifiers\n    df = df.dropna(subset=[\"symbol\", \"year\"])\n    \n    # Define columns that should be numeric\n    numeric_columns = [\n        \"total_asset\", \"total_equity\", \"total_liabilities\",\n        \"total_current_asset\", \"total_current_liabilities\",\n        \"is_net_revenue\", \"is_cogs\", \"is_manage_expense\",\n        \"is_interest_expense\", \"is_eat\", \"is_net_business_profit\",\n        \"na_tax_deferred\", \"nl_tax_deferred\", \"e_preferred_stock\",\n        \"capex\", \"total_cfo\", \"ca_cce\", \"ca_total_inventory\",\n        \"ca_acc_receiv\", \"cfo_interest_expense\", \"basic_eps\",\n        \"is_shareholders_eat\", \"cl_loan\", \"cl_finlease\",\n        \"cl_due_long_debt\", \"nl_loan\", \"nl_finlease\",\n        \"is_cos_of_sales\", \"e_equity\"\n    ]\n    \n    for col in numeric_columns:\n        if col in df.columns:\n            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n    \n    # Handle duplicates: keep row with most non-missing values\n    df[\"_completeness\"] = df.notna().sum(axis=1)\n    df = (df\n        .sort_values([\"symbol\", \"year\", \"_completeness\"])\n        .drop_duplicates(subset=[\"symbol\", \"year\"], keep=\"last\")\n        .drop(columns=\"_completeness\")\n        .reset_index(drop=True)\n    )\n    \n    return df\n\ndf_fundamentals = clean_fundamentals(df_fundamentals_raw)\nprint(f\"After cleaning: {len(df_fundamentals):,} firm-year observations\")\nprint(f\"Unique firms: {df_fundamentals['symbol'].nunique():,}\")\n\nAfter cleaning: 21,232 firm-year observations\nUnique firms: 1,554\n\n\n\n\n3.5.4 Tạo các biến chuẩn hóa\nĐể tạo điều kiện so sánh với các nghiên cứu quốc tế và đảm bảo tính tương thích với các phương pháp định giá tài sản tiêu chuẩn, chúng ta tạo ra các biến số theo các quy ước đã được thiết lập trong tài liệu học thuật. Chúng ta đối chiếu các khoản mục báo cáo tài chính của Việt Nam với các giá trị tương đương trong Compustat khi có thể.\n\ndef create_standard_variables(df):\n    \"\"\"\n    Create standardized financial variables for asset pricing research.\n    \n    This function maps Vietnamese financial statement items to standard\n    variable names used in the academic finance literature, following\n    conventions from Fama and French (1992, 1993, 2015).\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Cleaned fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with standardized variables added\n    \"\"\"\n    df = df.copy()\n    \n    # Fiscal date (assume December year-end)\n    df[\"datadate\"] = pd.to_datetime(df[\"year\"].astype(str) + \"-12-31\")\n    \n    # === Balance Sheet Items ===\n    df[\"at\"] = df[\"total_asset\"]                    # Total assets\n    df[\"lt\"] = df[\"total_liabilities\"]              # Total liabilities\n    df[\"seq\"] = df[\"total_equity\"]                  # Stockholders' equity\n    df[\"act\"] = df[\"total_current_asset\"]           # Current assets\n    df[\"lct\"] = df[\"total_current_liabilities\"]     # Current liabilities\n    \n    # Common equity (fallback to total equity if not available)\n    df[\"ceq\"] = df.get(\"e_equity\", df[\"seq\"])\n    \n    # === Deferred Taxes ===\n    df[\"txditc\"] = df.get(\"na_tax_deferred\", 0).fillna(0)  # Deferred tax assets\n    df[\"txdb\"] = df.get(\"nl_tax_deferred\", 0).fillna(0)    # Deferred tax liab.\n    df[\"itcb\"] = 0  # Investment tax credit (rare in Vietnam)\n    \n    # === Preferred Stock ===\n    pref = df.get(\"e_preferred_stock\", 0)\n    if isinstance(pref, pd.Series):\n        pref = pref.fillna(0)\n    df[\"pstk\"] = pref\n    df[\"pstkrv\"] = pref  # Redemption value\n    df[\"pstkl\"] = pref   # Liquidating value\n    \n    # === Income Statement Items ===\n    df[\"sale\"] = df[\"is_net_revenue\"]                        # Net sales/revenue\n    df[\"cogs\"] = df.get(\"is_cogs\", 0).fillna(0)              # Cost of goods sold\n    df[\"xsga\"] = df.get(\"is_manage_expense\", 0).fillna(0)    # SG&A expenses\n    df[\"xint\"] = df.get(\"is_interest_expense\", 0).fillna(0)  # Interest expense\n    df[\"ni\"] = df.get(\"is_eat\", np.nan)                      # Net income\n    df[\"oibdp\"] = df.get(\"is_net_business_profit\", np.nan)   # Operating income\n    \n    # === Cash Flow Items ===\n    df[\"oancf\"] = df.get(\"total_cfo\", np.nan)  # Operating cash flow\n    df[\"capx\"] = df.get(\"capex\", np.nan)       # Capital expenditures\n    \n    return df\n\ndf_fundamentals = create_standard_variables(df_fundamentals)\n\n\n\n3.5.5 Tính toán giá trị sổ sách và lợi nhuận\nGiá trị sổ sách là một biến số quan trọng đối với các chiến lược đầu tư giá trị và việc xây dựng danh mục đầu tư theo yếu tố HML (Cao Trừ Thấp). Chúng ta tuân theo định nghĩa từ thư viện dữ liệu của Kenneth French, có tính đến thuế hoãn lại và cổ phiếu ưu đãi.\n\ndef compute_book_equity(df):\n    \"\"\"\n    Compute book equity following Fama-French conventions.\n    \n    Book equity = Stockholders' equity \n                  + Deferred taxes and investment tax credit\n                  - Preferred stock\n    \n    Negative or zero book equity is set to missing, as book-to-market\n    ratios are undefined for such firms.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals with standardized variables\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with book equity (be) added\n    \"\"\"\n    df = df.copy()\n    \n    # Primary measure: stockholders' equity\n    # Fallback 1: common equity + preferred stock\n    # Fallback 2: total assets - total liabilities\n    seq_measure = (df[\"seq\"]\n        .combine_first(df[\"ceq\"] + df[\"pstk\"])\n        .combine_first(df[\"at\"] - df[\"lt\"])\n    )\n    \n    # Add deferred taxes\n    deferred_taxes = (df[\"txditc\"]\n        .combine_first(df[\"txdb\"] + df[\"itcb\"])\n        .fillna(0)\n    )\n    \n    # Subtract preferred stock (use redemption value as primary)\n    preferred = (df[\"pstkrv\"]\n        .combine_first(df[\"pstkl\"])\n        .combine_first(df[\"pstk\"])\n        .fillna(0)\n    )\n    \n    # Book equity calculation\n    df[\"be\"] = seq_measure + deferred_taxes - preferred\n    \n    # Set non-positive book equity to missing\n    df[\"be\"] = df[\"be\"].where(df[\"be\"] &gt; 0, np.nan)\n    \n    return df\n\ndf_fundamentals = compute_book_equity(df_fundamentals)\n\n# Summary statistics for book equity\nprint(\"Book Equity Summary Statistics (in million VND):\")\nprint(df_fundamentals[\"be\"].describe().round(2))\n\nBook Equity Summary Statistics (in million VND):\ncount    2.023500e+04\nmean     1.031884e+12\nstd      4.705269e+12\nmin      4.404402e+07\n25%      7.267610e+10\n50%      1.803885e+11\n75%      5.304653e+11\nmax      1.836314e+14\nName: be, dtype: float64\n\n\nLợi nhuận hoạt động, được giới thiệu bởi Fama and French (2015), đo lường lợi nhuận của một công ty so với vốn chủ sở hữu trên sổ sách. Các công ty có lợi nhuận hoạt động cao hơn thường có tỷ suất sinh lời kỳ vọng cao hơn.\n\ndef compute_profitability(df):\n    \"\"\"\n    Compute operating profitability following Fama-French (2015).\n    \n    Operating profitability = (Revenue - COGS - SG&A - Interest) / Book Equity\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals with book equity computed\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with operating profitability (op) added\n    \"\"\"\n    df = df.copy()\n    \n    # Operating profit before taxes\n    operating_profit = (\n        df[\"sale\"] \n        - df[\"cogs\"].fillna(0) \n        - df[\"xsga\"].fillna(0) \n        - df[\"xint\"].fillna(0)\n    )\n    \n    # Scale by book equity\n    df[\"op\"] = operating_profit / df[\"be\"]\n    \n    # Winsorize extreme values (outside 1st and 99th percentiles)\n    lower = df[\"op\"].quantile(0.01)\n    upper = df[\"op\"].quantile(0.99)\n    df[\"op\"] = df[\"op\"].clip(lower=lower, upper=upper)\n    \n    return df\n\ndf_fundamentals = compute_profitability(df_fundamentals)\n\n\n\n3.5.6 Tính toán Đầu tư\nĐầu tư, được đo lường bằng sự tăng trưởng tài sản, nắm bắt hành vi đầu tư của các công ty. Fama and French (2015) ghi nhận rằng các công ty có tốc độ tăng trưởng tài sản cao (đầu tư tháo vát) thường có tỷ suất sinh lời trong tương lai thấp hơn.\n\ndef compute_investment(df):\n    \"\"\"\n    Compute investment (asset growth) following Fama-French (2015).\n    \n    Investment = (Total Assets_t / Total Assets_{t-1}) - 1\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with investment (inv) added\n    \"\"\"\n    df = df.copy()\n    \n    # Create lagged assets\n    df_lag = (df[[\"symbol\", \"year\", \"at\"]]\n        .assign(year=lambda x: x[\"year\"] + 1)\n        .rename(columns={\"at\": \"at_lag\"})\n    )\n    \n    # Merge lagged values\n    df = df.merge(df_lag, on=[\"symbol\", \"year\"], how=\"left\")\n    \n    # Compute investment (asset growth)\n    df[\"inv\"] = df[\"at\"] / df[\"at_lag\"] - 1\n    \n    # Set to missing if lagged assets non-positive\n    df[\"inv\"] = df[\"inv\"].where(df[\"at_lag\"] &gt; 0, np.nan)\n    \n    return df\n\ndf_fundamentals = compute_investment(df_fundamentals)\n\n\n\n3.5.7 Tính toán tổng nợ\nTrong báo cáo tài chính của Việt Nam, tổng nợ phải trả bao gồm các khoản mục không sinh lãi như khoản phải trả cho nhà cung cấp và thuế phải trả. Để phân tích đòn bẩy, chúng ta tính tổng nợ có lãi bằng cách cộng gộp các khoản vay và nghĩa vụ thuê.\n\ndef compute_total_debt(df):\n    \"\"\"\n    Compute total interest-bearing debt.\n    \n    Total Debt = Short-term loans + Finance leases (current)\n                 + Current portion of long-term debt\n                 + Long-term loans + Finance leases (non-current)\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Fundamentals data\n    \n    Returns\n    -------\n    pd.DataFrame\n        Fundamentals with total_debt added\n    \"\"\"\n    df = df.copy()\n    \n    df[\"total_debt\"] = (\n        df.get(\"cl_loan\", 0).fillna(0) +           # Short-term bank loans\n        df.get(\"cl_finlease\", 0).fillna(0) +       # Current finance leases\n        df.get(\"cl_due_long_debt\", 0).fillna(0) +  # Current portion LT debt\n        df.get(\"nl_loan\", 0).fillna(0) +           # Long-term bank loans\n        df.get(\"nl_finlease\", 0).fillna(0)         # Non-current finance leases\n    )\n    \n    return df\n\ndf_fundamentals = compute_total_debt(df_fundamentals)\n\n\n\n3.5.8 Áp dụng bộ lọc\nChúng ta áp dụng các bộ lọc tiêu chuẩn để đảm bảo chất lượng dữ liệu: yêu cầu tài sản dương, doanh số bán hàng không âm và sự hiện diện của các biến cốt lõi cần thiết để xây dựng danh mục đầu tư.\n\n# Keep only observations with required variables\nrequired_vars = [\"at\", \"lt\", \"seq\", \"sale\"]\ncomp_vn = df_fundamentals.dropna(subset=required_vars)\n\n# Apply quality filters\ncomp_vn = comp_vn.query(\"at &gt; 0\")      # Positive assets\ncomp_vn = comp_vn.query(\"sale &gt;= 0\")   # Non-negative sales\n\n# Keep last observation per firm-year (in case of restatements)\ncomp_vn = (comp_vn\n    .sort_values(\"datadate\")\n    .groupby([\"symbol\", \"year\"])\n    .tail(1)\n    .reset_index(drop=True)\n)\n\n# Diagnostic summary\nprint(f\"Final sample: {len(comp_vn):,} firm-year observations\")\nprint(f\"Unique firms: {comp_vn['symbol'].nunique():,}\")\nprint(f\"Sample period: {comp_vn['year'].min()} - {comp_vn['year'].max()}\")\n\nFinal sample: 20,091 firm-year observations\nUnique firms: 1,502\nSample period: 1998 - 2023\n\n\n\n\n3.5.9 Lưu trữ dữ liệu\nChúng ta lưu trữ dữ liệu cơ bản đã chuẩn bị trong cơ sở dữ liệu SQLite cục bộ để sử dụng trong các chương tiếp theo.\n\ncomp_vn.to_sql(\n    name=\"comp_vn\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Company fundamentals saved to database.\")\n\nCompany fundamentals saved to database.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html#dữ-liệu-giá-cổ-phiếu",
    "href": "02_datacore_data.html#dữ-liệu-giá-cổ-phiếu",
    "title": "3  Dữ liệu Datacore",
    "section": "3.6 Dữ liệu giá cổ phiếu",
    "text": "3.6 Dữ liệu giá cổ phiếu\nDữ liệu giá cổ phiếu là nền tảng của các phân tích dựa trên lợi nhuận trong tài chính thực nghiệm. Datacore cung cấp dữ liệu giá lịch sử toàn diện cho tất cả các chứng khoán được giao dịch trên HOSE, HNX và UPCoM, bao gồm cả giá điều chỉnh có tính đến các hành động của công ty.\n\n3.6.1 Tải xuống dữ liệu giá\nChúng ta tải xuống dữ liệu giá lịch sử từ hệ thống lưu trữ của Datacore. Dữ liệu bao gồm các quan sát hàng ngày với giá mở cửa, giá cao nhất, giá thấp nhất, giá đóng cửa, khối lượng giao dịch và các yếu tố điều chỉnh.\n\n# Download historical price data\nprices_raw = conn.read_csv(\n    bucket_name,\n    \"historycal_price/dataset_historical_price.csv\",\n    low_memory=False\n)\n\nprint(f\"Downloaded {len(prices_raw):,} daily price observations\")\nprint(f\"Date range: {prices_raw['date'].min()} to {prices_raw['date'].max()}\")\n\nDownloaded 4,307,791 daily price observations\nDate range: 2010-01-04 to 2025-05-12\n\n\n\n\n3.6.2 Xử lý dữ liệu giá\nChúng ta làm sạch dữ liệu giá và tính toán giá điều chỉnh có tính đến việc chia tách cổ phiếu, cổ tức bằng cổ phiếu và các hành động khác của công ty.\n\ndef process_price_data(df):\n    \"\"\"\n    Process raw price data from Datacore.\n    \"\"\"\n    df = df.copy()\n    \n    # Parse dates\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    \n    # Standardize column names\n    df = df.rename(columns={\n        \"open_price\": \"open\",\n        \"high_price\": \"high\",\n        \"low_price\": \"low\",\n        \"close_price\": \"close\",\n        \"vol_total\": \"volume\"\n    })\n    \n    # Compute adjusted close price\n    df[\"adjusted_close\"] = df[\"close\"] * df[\"adj_ratio\"]\n    \n    # Standardize symbol\n    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\n    \n    # Sort for return calculation\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Add year and month\n    df[\"year\"] = df[\"date\"].dt.year\n    df[\"month\"] = df[\"date\"].dt.month\n    \n    return df\n\nprices = process_price_data(prices_raw)\n\n\n\n3.6.3 Tính toán Số lượng Cổ phiếu Hoạt động và Giá trị Thị trường\nGiá trị thị trường được tính bằng sản phẩm của giá và số lượng cổ phiếu hoạt động. Vì Datacore cung cấp lợi nhuận trên mỗi cổ phiếu và lợi nhuận sau thuế, chúng ta có thể suy ra số lượng cổ phiếu hoạt động từ các biến số này.\n\ndef compute_shares_outstanding(fundamentals_df):\n    \"\"\"\n    Compute shares outstanding from fundamentals.\n    \"\"\"\n    shares = fundamentals_df.copy()\n    shares[\"shrout\"] = shares[\"is_shareholders_eat\"] / shares[\"basic_eps\"]\n    shares = shares[[\"symbol\", \"year\", \"shrout\"]].dropna()\n    \n    return shares\n\nshares_outstanding = compute_shares_outstanding(df_fundamentals)\n\n\ndef add_market_cap(df, shares_df):\n    \"\"\"\n    Add market capitalization to price data.\n    \"\"\"\n    df = df.merge(shares_df, on=[\"symbol\", \"year\"], how=\"left\")\n    \n    # Compute market cap (in million VND)\n    df[\"mktcap\"] = (df[\"close\"] * df[\"shrout\"]) / 1_000_000\n    \n    # Set zero or negative market cap to missing\n    df[\"mktcap\"] = df[\"mktcap\"].where(df[\"mktcap\"] &gt; 0, np.nan)\n    \n    return df\n\nprices = add_market_cap(prices, shares_outstanding)\n\n\n\n3.6.4 Tính Lợi Nhuận và Lợi Nhuận Kỳ Lượng\nChúng ta tính toán lợi nhuận sử dụng giá đóng cửa điều chỉnh để đảm bảo lợi nhuận phản ánh chính xác lợi nhuận tổng của cổ đông bao gồm cả cổ tức và các hành động của công ty.\n\n3.6.4.1 Tạo Bộ Dữ Liệu Hàng Ngày\n\nPhiên bản tuần tự\n\n\ndef create_daily_dataset(df, annual_rf=0.04):\n    \"\"\"\n    Create daily price dataset with returns and excess returns.\n    \"\"\"\n    df = df.copy()\n    \n    # Sort by symbol and date (critical for correct return calculation)\n    df = df.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Remove duplicate dates within each symbol (keep last observation)\n    df = df.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Compute daily returns\n    df[\"ret\"] = df.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    \n    # Cap extreme negative returns\n    df[\"ret\"] = df[\"ret\"].clip(lower=-0.99)\n    \n    # Daily risk-free rate (assuming 252 trading days)\n    df[\"risk_free\"] = annual_rf / 252\n    \n    # Excess returns\n    df[\"ret_excess\"] = df[\"ret\"] - df[\"risk_free\"]\n    df[\"ret_excess\"] = df[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    df[\"mktcap_lag\"] = df.groupby(\"symbol\")[\"mktcap\"].shift(1)\n    \n    return df\n\nprices_daily = create_daily_dataset(prices)\n\n\nPhiên bản song song\n\n\nfrom joblib import Parallel, delayed\nimport os\n\ndef process_daily_symbol(symbol_df, annual_rf=0.04):\n    \"\"\"\n    Process a single symbol's daily data.\n    \"\"\"\n    df = symbol_df.copy()\n    \n    # Sort by date (critical for correct return calculation)\n    df = df.sort_values(\"date\").reset_index(drop=True)\n    \n    # Remove duplicate dates (keep last observation if duplicates exist)\n    df = df.drop_duplicates(subset=[\"date\"], keep=\"last\")\n    \n    # Compute daily returns\n    df[\"ret\"] = df[\"adjusted_close\"].pct_change()\n\n    # Replace infinite values with NaN\n    df[\"ret\"] = df[\"ret\"].replace([np.inf, -np.inf], np.nan)\n    \n    # Cap extreme negative returns\n    df[\"ret\"] = df[\"ret\"].clip(lower=-0.99)\n    \n    # Daily risk-free rate\n    df[\"risk_free\"] = annual_rf / 252\n    \n    # Excess returns\n    df[\"ret_excess\"] = df[\"ret\"] - df[\"risk_free\"]\n    df[\"ret_excess\"] = df[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    df[\"mktcap_lag\"] = df[\"mktcap\"].shift(1)\n    \n    return df\n\ndef create_daily_dataset_parallel(df, annual_rf=0.04):\n    \"\"\"\n    Create daily price dataset using parallel processing.\n    \"\"\"\n    # Ensure data is sorted before splitting\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Split by symbol\n    symbol_groups = [group for _, group in df.groupby(\"symbol\")]\n    \n    n_jobs = max(1, os.cpu_count() - 1)\n    print(f\"Processing {len(symbol_groups):,} symbols using {n_jobs} cores...\")\n    \n    results = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(process_daily_symbol)(group, annual_rf) \n        for group in symbol_groups\n    )\n    \n    return pd.concat(results, ignore_index=True)\n\nprices_daily = create_daily_dataset_parallel(prices)\n\n# Quick validation\nprint(\"\\nValidation checks:\")\nprint(f\"Any duplicate (symbol, date): {prices_daily.duplicated(subset=['symbol', 'date']).sum()}\")\nprint(f\"Sample of non-zero returns:\")\nprint(prices_daily[prices_daily[\"ret\"] != 0][[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(10))\n\n\nprices_daily.query(\"symbol == 'FPT'\")[[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(3)\n\nProcessing 1,837 symbols using 87 cores...\n\n\n[Parallel(n_jobs=87)]: Using backend LokyBackend with 87 concurrent workers.\n[Parallel(n_jobs=87)]: Done  26 tasks      | elapsed:    7.8s\n[Parallel(n_jobs=87)]: Done 276 tasks      | elapsed:   10.5s\n[Parallel(n_jobs=87)]: Done 626 tasks      | elapsed:   11.7s\n[Parallel(n_jobs=87)]: Done 1076 tasks      | elapsed:   13.6s\n[Parallel(n_jobs=87)]: Done 1626 tasks      | elapsed:   16.0s\n[Parallel(n_jobs=87)]: Done 1837 out of 1837 | elapsed:   16.6s finished\n\n\n\nValidation checks:\nAny duplicate (symbol, date): 0\nSample of non-zero returns:\n   symbol       date  adjusted_close       ret\n0     A32 2018-10-23       44.574418       NaN\n27    A32 2018-11-29       55.072640  0.235521\n30    A32 2018-12-04       48.188560 -0.125000\n43    A32 2018-12-21       51.974804  0.078571\n49    A32 2019-01-02       55.072640  0.059603\n53    A32 2019-01-08       50.030370 -0.091557\n74    A32 2019-02-13       44.289180 -0.114754\n75    A32 2019-02-14       41.008500 -0.074074\n78    A32 2019-02-19       36.087480 -0.120000\n91    A32 2019-03-08       41.336568  0.145455\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n1146076\nFPT\n2010-01-04\n1170.9885\nNaN\n\n\n1146077\nFPT\n2010-01-05\n1170.9885\n0.000000\n\n\n1146078\nFPT\n2010-01-06\n1149.6978\n-0.018182\n\n\n\n\n\n\n\n\n# Select columns\ndaily_columns = [\n    \"symbol\", \"date\", \"year\", \"month\",\n    \"open\", \"high\", \"low\", \"close\", \"volume\",\n    \"adjusted_close\", \"shrout\", \"mktcap\", \"mktcap_lag\",\n    \"ret\", \"risk_free\", \"ret_excess\"\n]\nprices_daily = prices_daily[daily_columns]\n\n# Remove observations with missing essential variables\nprices_daily = prices_daily.dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n\nprint(\"Daily Return Summary Statistics:\")\nprint(prices_daily[\"ret\"].describe().round(4))\nprint(f\"\\nFinal daily sample: {len(prices_daily):,} observations\")\n\nDaily Return Summary Statistics:\ncount    3.462157e+06\nmean     3.000000e-04\nstd      4.480000e-02\nmin     -9.900000e-01\n25%     -4.900000e-03\n50%      0.000000e+00\n75%      4.000000e-03\nmax      3.250000e+01\nName: ret, dtype: float64\n\nFinal daily sample: 3,462,157 observations\n\n\n\n\n3.6.4.2 Tạo Dữ liệu Tháng\nĐối với lợi nhuận hàng tháng, chúng ta tính toán lợi nhuận trực tiếp từ giá điều chỉnh cuối tháng thay vì lãi kép lợi nhuận hàng ngày. Điều này tránh được sai sót do lãi kép từ các ngày thiếu và là phương pháp tiêu chuẩn trong tài chính thực nghiệm.\n\nPhiên bản tuần tự\n\n\ndef create_monthly_dataset(df, annual_rf=0.04):\n    \"\"\"\n    Create monthly price dataset with returns computed from \n    month-end to month-end adjusted prices.\n    \"\"\"\n    df = df.copy()\n    \n    # Sort by symbol and date (critical for correct return calculation)\n    df = df.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Remove duplicate dates within each symbol (keep last observation)\n    df = df.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Get month-end observations\n    monthly = (df\n        .groupby(\"symbol\")\n        .resample(\"ME\", on=\"date\")\n        .agg({\n            \"open\": \"first\",           # First day open\n            \"high\": \"max\",             # Monthly high\n            \"low\": \"min\",              # Monthly low\n            \"close\": \"last\",           # Last day close\n            \"volume\": \"sum\",           # Total monthly volume\n            \"adjusted_close\": \"last\",  # Month-end adjusted price\n            \"shrout\": \"last\",          # Month-end shares outstanding\n            \"mktcap\": \"last\",          # Month-end market cap\n            \"year\": \"last\",\n            \"month\": \"last\"\n        })\n        .reset_index()\n    )\n    \n    # Remove duplicate (symbol, date) after resampling (safety check)\n    monthly = monthly.drop_duplicates(subset=[\"symbol\", \"date\"], keep=\"last\")\n    \n    # Sort again after resampling\n    monthly = monthly.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n    \n    # Compute monthly returns from month-end to month-end adjusted prices\n    monthly[\"ret\"] = monthly.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    \n    # Cap extreme returns\n    monthly[\"ret\"] = monthly[\"ret\"].clip(lower=-0.99)\n    \n    # Monthly risk-free rate\n    monthly[\"risk_free\"] = annual_rf / 12\n    \n    # Excess returns\n    monthly[\"ret_excess\"] = monthly[\"ret\"] - monthly[\"risk_free\"]\n    monthly[\"ret_excess\"] = monthly[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap for portfolio weighting\n    monthly[\"mktcap_lag\"] = monthly.groupby(\"symbol\")[\"mktcap\"].shift(1)\n    \n    return monthly\n\nprices_monthly = create_monthly_dataset(prices)\n\n\nPhiên bản song song\n\n\nfrom joblib import Parallel, delayed\nimport os\n\ndef process_monthly_symbol(symbol_df, annual_rf=0.04):\n    \"\"\"\n    Process a single symbol's data to monthly frequency.\n    \"\"\"\n    df = symbol_df.copy()\n    \n    # Sort by date (critical for correct return calculation)\n    df = df.sort_values(\"date\").reset_index(drop=True)\n    \n    # Remove duplicate dates (keep last observation if duplicates exist)\n    df = df.drop_duplicates(subset=[\"date\"], keep=\"last\")\n    \n    # Set date as index for resampling\n    df = df.set_index(\"date\")\n    \n    # Resample to monthly\n    monthly = df.resample(\"ME\").agg({\n        \"symbol\": \"last\",\n        \"open\": \"first\",\n        \"high\": \"max\",\n        \"low\": \"min\",\n        \"close\": \"last\",\n        \"volume\": \"sum\",\n        \"adjusted_close\": \"last\",\n        \"shrout\": \"last\",\n        \"mktcap\": \"last\",\n        \"year\": \"last\",\n        \"month\": \"last\"\n    }).reset_index()\n    \n    # Remove rows where symbol is NaN (months with no trading)\n    monthly = monthly.dropna(subset=[\"symbol\"])\n    \n    # Sort by date\n    monthly = monthly.sort_values(\"date\").reset_index(drop=True)\n    \n    # Compute monthly returns\n    monthly[\"ret\"] = monthly[\"adjusted_close\"].pct_change()\n    \n    # Replace infinite values with NaN\n    monthly[\"ret\"] = monthly[\"ret\"].replace([np.inf, -np.inf], np.nan)\n    \n    # Cap extreme returns\n    monthly[\"ret\"] = monthly[\"ret\"].clip(lower=-0.99)\n    \n    # Monthly risk-free rate\n    monthly[\"risk_free\"] = annual_rf / 12\n    \n    # Excess returns\n    monthly[\"ret_excess\"] = monthly[\"ret\"] - monthly[\"risk_free\"]\n    monthly[\"ret_excess\"] = monthly[\"ret_excess\"].clip(lower=-1.0)\n    \n    # Lagged market cap\n    monthly[\"mktcap_lag\"] = monthly[\"mktcap\"].shift(1)\n    \n    return monthly\n\ndef create_monthly_dataset_parallel(df, annual_rf=0.04):\n    \"\"\"\n    Create monthly price dataset using parallel processing.\n    \"\"\"\n    # Ensure data is sorted before splitting\n    df = df.sort_values([\"symbol\", \"date\"])\n    \n    # Split by symbol\n    symbol_groups = [group for _, group in df.groupby(\"symbol\")]\n    \n    n_jobs = max(1, os.cpu_count() - 1)\n    print(f\"Processing {len(symbol_groups):,} symbols using {n_jobs} cores...\")\n    \n    results = Parallel(n_jobs=n_jobs, verbose=1)(\n        delayed(process_monthly_symbol)(group, annual_rf) \n        for group in symbol_groups\n    )\n    \n    return pd.concat(results, ignore_index=True)\n\nprices_monthly = create_monthly_dataset_parallel(prices)\n\n# Validation checks\nprint(\"\\nValidation checks:\")\nprint(f\"Any duplicate (symbol, date): {prices_monthly.duplicated(subset=['symbol', 'date']).sum()}\")\nprint(f\"\\nSample of non-zero returns:\")\nprint(prices_monthly[prices_monthly[\"ret\"] != 0][[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(10))\n\nprices_monthly.query(\"symbol == 'FPT'\")[[\"symbol\", \"date\", \"adjusted_close\", \"ret\"]].head(3)\n\nProcessing 1,837 symbols using 87 cores...\n\n\n[Parallel(n_jobs=87)]: Using backend LokyBackend with 87 concurrent workers.\n[Parallel(n_jobs=87)]: Done  26 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=87)]: Done 378 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=87)]: Done 1078 tasks      | elapsed:    4.5s\n[Parallel(n_jobs=87)]: Done 1664 out of 1837 | elapsed:    7.0s remaining:    0.7s\n[Parallel(n_jobs=87)]: Done 1837 out of 1837 | elapsed:    7.7s finished\n\n\n\nValidation checks:\nAny duplicate (symbol, date): 0\n\nSample of non-zero returns:\n   symbol       date  adjusted_close       ret\n0     A32 2018-10-31       44.574418       NaN\n1     A32 2018-11-30       55.072640  0.235521\n2     A32 2018-12-31       51.974804 -0.056250\n3     A32 2019-01-31       50.030370 -0.037411\n4     A32 2019-02-28       36.087480 -0.278689\n5     A32 2019-03-31       41.828670  0.159091\n7     A32 2019-05-31       43.304976  0.035294\n8     A32 2019-06-30       35.929125 -0.170323\n9     A32 2019-07-31       37.525975  0.044444\n10    A32 2019-08-31       38.324400  0.021277\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n55963\nFPT\n2010-01-31\n1092.9226\nNaN\n\n\n55964\nFPT\n2010-02-28\n1107.1164\n0.012987\n\n\n55965\nFPT\n2010-03-31\n1185.1823\n0.070513\n\n\n\n\n\n\n\n\n# Select columns (same structure as daily)\nmonthly_columns = [\n    \"symbol\", \"date\", \"year\", \"month\",\n    \"open\", \"high\", \"low\", \"close\", \"volume\",\n    \"adjusted_close\", \"shrout\", \"mktcap\", \"mktcap_lag\",\n    \"ret\", \"risk_free\", \"ret_excess\"\n]\nprices_monthly = prices_monthly[monthly_columns]\n\n# Remove observations with missing essential variables\nprices_monthly = prices_monthly.dropna(subset=[\"ret_excess\", \"mktcap\", \"mktcap_lag\"])\n\nprint(\"Monthly Return Summary Statistics:\")\nprint(prices_monthly[\"ret\"].describe().round(4))\nprint(f\"\\nFinal monthly sample: {len(prices_monthly):,} observations\")\n\nMonthly Return Summary Statistics:\ncount    165499.0000\nmean          0.0042\nstd           0.1862\nmin          -0.9900\n25%          -0.0703\n50%           0.0000\n75%           0.0553\nmax          12.7500\nName: ret, dtype: float64\n\nFinal monthly sample: 165,499 observations\n\n\n\n\n\n3.6.5 Lưu trữ dữ liệu giá\n\nprices_daily.to_sql(\n    name=\"prices_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(\"Daily price data saved to database.\")\n\nprices_monthly.to_sql(\n    name=\"prices_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(\"Monthly price data saved to database.\")",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html#thống-kê-mô-tả",
    "href": "02_datacore_data.html#thống-kê-mô-tả",
    "title": "3  Dữ liệu Datacore",
    "section": "3.7 Thống kê mô tả",
    "text": "3.7 Thống kê mô tả\nTrước khi tiến hành phân tích giá tài sản, chúng ta xem xét các đặc điểm của mẫu để hiểu sự phát triển và thành phần của thị trường chứng khoán Việt Nam.\n\n3.7.1 Sự phát triển của thị trường theo thời gian\nTrước tiên, chúng ta kiểm tra số lượng chứng khoán niêm yết đã tăng lên như thế nào theo thời gian.\n\nsecurities_over_time = (prices_monthly\n    .groupby(\"date\")\n    .agg(\n        n_securities=(\"symbol\", \"nunique\"),\n        total_mktcap=(\"mktcap\", \"sum\")\n    )\n    .reset_index()\n)\n\n\nsecurities_figure = (\n    ggplot(securities_over_time, aes(x=\"date\", y=\"n_securities\"))\n    + geom_line(color=\"steelblue\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Number of Securities\",\n        title=\"Growth of Vietnamese Stock Market\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + scale_y_continuous(labels=comma_format())\n    + theme_minimal()\n)\nsecurities_figure.show()\n\n\n\n\n\n\n\nFigure 3.1: The figure shows the monthly number of securities in the Vietnamese stock market sample.\n\n\n\n\n\n\n\n3.7.2 Sự phát triển của Tổng vốn hóa thị trường\nTổng vốn hóa thị trường phản ánh quy mô và sự phát triển chung của thị trường chứng khoán Việt Nam.\n\nmktcap_figure = (\n    ggplot(securities_over_time, aes(x=\"date\", y=\"total_mktcap / 1000\"))\n    + geom_line(color=\"darkgreen\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Market Cap (Trillion VND)\",\n        title=\"Total Market Capitalization of Vietnamese Equities\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + scale_y_continuous(labels=comma_format())\n    + theme_minimal()\n)\nmktcap_figure.show()\n\n\n\n\n\n\n\nFigure 3.2: The figure shows the total market capitalization of Vietnamese listed companies over time.\n\n\n\n\n\n\n\n3.7.3 Phân phối lợi nhuận\nViệc hiểu rõ sự phân bố lợi nhuận hàng tháng giúp xác định các vấn đề tiềm ẩn về chất lượng dữ liệu và đánh giá rủi ro thị trường.\n\nreturn_distribution = (\n    ggplot(prices_monthly, aes(x=\"ret_excess\"))\n    + geom_histogram(\n        binwidth=0.02, \n        fill=\"steelblue\", \n        color=\"white\",\n        alpha=0.7\n    )\n    + labs(\n        x=\"Monthly Excess Return\",\n        y=\"Frequency\",\n        title=\"Distribution of Monthly Excess Returns\"\n    )\n    + scale_x_continuous(limits=(-0.5, 0.5))\n    + theme_minimal()\n)\nreturn_distribution.show()\n\n/home/mikenguyen/project/tidyfinance/.venv/lib/python3.13/site-packages/plotnine/layer.py:293: PlotnineWarning: stat_bin : Removed 3264 rows containing non-finite values.\n/home/mikenguyen/project/tidyfinance/.venv/lib/python3.13/site-packages/plotnine/layer.py:374: PlotnineWarning: geom_histogram : Removed 2 rows containing missing values.\n\n\n\n\n\n\n\n\nFigure 3.3: Distribution of monthly excess returns for Vietnamese stocks.\n\n\n\n\n\n\n\n3.7.4 Phạm vi Giá Trị Sổ Sách của Vốn Chủ Sở Hữu\nVốn chủ sở hữu sổ sách là điều cần thiết để xây dựng danh mục đầu tư giá trị. Chúng ta kiểm tra phần nào trong mẫu của chúng ta có dữ liệu vốn chủ sở hữu sổ sách theo thời gian.\n\n# Merge prices with fundamentals\ncoverage_data = (prices_monthly\n    .assign(year=lambda x: x[\"date\"].dt.year)\n    .groupby([\"symbol\", \"year\"])\n    .tail(1)\n    .merge(comp_vn[[\"symbol\", \"year\", \"be\"]], \n           on=[\"symbol\", \"year\"], \n           how=\"left\")\n)\n\n# Compute coverage by year\nbe_coverage = (coverage_data\n    .groupby(\"year\")\n    .apply(lambda x: pd.Series({\n        \"share_with_be\": x[\"be\"].notna().mean()\n    }))\n    .reset_index()\n)\n\ncoverage_figure = (\n    ggplot(be_coverage, aes(x=\"year\", y=\"share_with_be\"))\n    + geom_line(color=\"darkorange\", size=1)\n    + geom_point(color=\"darkorange\", size=2)\n    + labs(\n        x=\"Year\",\n        y=\"Share with Book Equity\",\n        title=\"Coverage of Book Equity Data\"\n    )\n    + scale_y_continuous(labels=percent_format(), limits=(0, 1))\n    + theme_minimal()\n)\ncoverage_figure.show()\n\n\n\n\n\n\n\nFigure 3.4: Share of securities with available book equity data by year.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html#hợp-nhất-cổ-phiếu-và-dữ-liệu-cơ-bản",
    "href": "02_datacore_data.html#hợp-nhất-cổ-phiếu-và-dữ-liệu-cơ-bản",
    "title": "3  Dữ liệu Datacore",
    "section": "3.8 Hợp nhất cổ phiếu và dữ liệu cơ bản",
    "text": "3.8 Hợp nhất cổ phiếu và dữ liệu cơ bản\nBước cuối cùng liên kết dữ liệu giá với dữ liệu cơ bản bằng cách sử dụng biểu tượng chứng khoán làm mã định danh chung. Bộ dữ liệu hợp nhất này tạo cơ sở để xây dựng danh mục đầu tư được sắp xếp theo đặc điểm của công ty.\n\n# Example: Create merged dataset for end-of-June each year\nmerged_data = (prices_monthly\n    .query(\"month == 6\")\n    .merge(\n        comp_vn[[\"symbol\", \"year\", \"be\", \"op\", \"inv\", \"at\"]],\n        on=[\"symbol\", \"year\"],\n        how=\"left\",\n        suffixes=(\"\", \"_fundamental\")\n    )\n)\n\n# Convert BE from VND to BILLION VND\nmerged_data[\"be\"] = merged_data[\"be\"] / 1e9\n\n# Compute book-to-market ratio\nmerged_data[\"bm\"] = merged_data[\"be\"] / merged_data[\"mktcap\"]\n\nmerged_data.loc[\n    (merged_data[\"bm\"] &lt;= 0) |\n    (merged_data[\"bm\"] &gt; 20),\n    \"bm\"\n] = pd.NA\n\n\nmerged_data[\"bm\"].describe(percentiles=[.01, .1, .5, .9, .99])\n\nprint(f\"Merged observations: {len(merged_data):,}\")\nprint(f\"With book-to-market: {merged_data['bm'].notna().sum():,}\")\nmerged_data.head(3)\nmerged_data.describe()\nmerged_data\n\nMerged observations: 13,756\nWith book-to-market: 12,859\n\n\n\n\n\n\n\n\n\nsymbol\ndate\nyear\nmonth\nopen\nhigh\nlow\nclose\nvolume\nadjusted_close\n...\nmktcap\nmktcap_lag\nret\nrisk_free\nret_excess\nbe\nop\ninv\nat\nbm\n\n\n\n\n0\nA32\n2019-06-30\n2019.0\n6.0\n26.4\n26.4\n21.0\n22.5\n3700\n35.929125\n...\n153.000\n179.52\n-0.170323\n0.003333\n-0.173657\n223.612748\n0.232362\n-0.072329\n4.349303e+11\n1.461521\n\n\n1\nA32\n2020-06-30\n2020.0\n6.0\n25.0\n26.3\n24.5\n26.3\n7500\n38.811173\n...\n178.840\n187.00\n-0.067977\n0.003333\n-0.071311\n242.216943\n0.195565\n0.122698\n4.882955e+11\n1.354378\n\n\n2\nA32\n2021-06-30\n2021.0\n6.0\n30.2\n37.0\n29.5\n32.0\n78400\n45.363520\n...\n217.600\n214.20\n0.015873\n0.003333\n0.012540\n238.385190\n0.157723\n0.081581\n5.281309e+11\n1.095520\n\n\n3\nA32\n2022-06-30\n2022.0\n6.0\n30.9\n35.5\n25.0\n35.3\n15200\n47.503210\n...\n240.040\n210.12\n0.142395\n0.003333\n0.139061\n215.399735\n0.172085\n0.036584\n5.474523e+11\n0.897349\n\n\n4\nA32\n2023-06-30\n2023.0\n6.0\n30.1\n33.5\n29.2\n29.4\n2400\n35.064204\n...\n199.920\n204.68\n-0.023256\n0.003333\n-0.026589\n222.024135\n0.174658\n-0.076752\n5.054342e+11\n1.110565\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13751\nYTC\n2019-06-30\n2019.0\n6.0\n70.0\n79.9\n70.0\n79.9\n38900\n171.451817\n...\n246.092\n215.60\n0.141429\n0.003333\n0.138095\n59.901389\n0.738190\n-0.021758\n7.521980e+11\n0.243411\n\n\n13752\nYTC\n2020-06-30\n2020.0\n6.0\n88.5\n88.5\n77.0\n87.0\n150640\n180.966960\n...\n267.960\n272.58\n-0.016949\n0.003333\n-0.020282\n13.459082\n-0.458548\n0.323501\n9.955348e+11\n0.050228\n\n\n13753\nYTC\n2021-06-30\n2021.0\n6.0\n76.0\n115.5\n61.0\n61.0\n34100\n126.884880\n...\n187.880\n234.08\n-0.197368\n0.003333\n-0.200702\n21.746595\n0.539521\n-0.215694\n7.808035e+11\n0.115747\n\n\n13754\nYTC\n2022-06-30\n2022.0\n6.0\n68.0\n68.0\n65.0\n65.5\n200\n136.245240\n...\n201.740\n209.44\n-0.036765\n0.003333\n-0.040098\n32.403055\n0.483088\n0.182911\n9.236206e+11\n0.160618\n\n\n13755\nYTC\n2023-06-30\n2023.0\n6.0\n59.0\n59.0\n59.0\n59.0\n49545\n122.724720\n...\n181.720\n181.72\n0.000000\n0.003333\n-0.003333\n38.976624\n0.450157\n0.017930\n9.401815e+11\n0.214487\n\n\n\n\n13756 rows × 21 columns\n\n\n\n\nfrom plotnine import *\nimport numpy as np\n\nbm_plot_data = (\n    merged_data[[\"bm\"]]\n      .dropna()\n      .assign(bm_plot=lambda x: x[\"bm\"].clip(upper=10))\n)\n\n(\n    ggplot(bm_plot_data, aes(x=\"bm_plot\")) +\n    geom_histogram(bins=80) +\n    labs(\n        title=\"Distribution of Book to Market Ratios\",\n        x=\"Book to Market (capped at 10 for plotting)\",\n        y=\"Number of firms\"\n    ) +\n    theme_minimal()\n)\n\n\n\n\n\n\n\n\n\nsize_plot_data = (\n    merged_data[[\"mktcap_lag\"]]\n      .dropna()\n      .assign(log_size=lambda x: np.log(x[\"mktcap_lag\"]))\n)\n\n(\n    ggplot(size_plot_data, aes(x=\"log_size\")) +\n    geom_histogram(bins=80) +\n    labs(\n        title=\"Distribution of Log Market Capitalization\",\n        x=\"Log Market Cap\",\n        y=\"Number of firms\"\n    ) +\n    theme_minimal()\n)\n\n\n\n\n\n\n\n\n\nscatter_data = (\n    merged_data[[\"be\", \"mktcap_lag\"]]\n      .dropna()\n      .assign(\n          log_be=lambda x: np.log(x[\"be\"]),\n          log_me=lambda x: np.log(x[\"mktcap_lag\"])\n      )\n)\n\n(\n    ggplot(scatter_data, aes(x=\"log_me\", y=\"log_be\")) +\n    geom_point(alpha=0.2) +\n    labs(\n        title=\"Log Book Equity vs Log Market Equity\",\n        x=\"Log Market Cap\",\n        y=\"Log Book Equity\"\n    ) +\n    theme_minimal()\n)",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "02_datacore_data.html#những-điểm-chính-cần-ghi-nhớ",
    "href": "02_datacore_data.html#những-điểm-chính-cần-ghi-nhớ",
    "title": "3  Dữ liệu Datacore",
    "section": "3.9 Những điểm chính cần ghi nhớ",
    "text": "3.9 Những điểm chính cần ghi nhớ\n\nDatacore cung cấp quyền truy cập thống nhất vào dữ liệu tài chính Việt Nam thông qua cơ sở hạ tầng điện toán đám mây hiện đại, loại bỏ nhu cầu tổng hợp dữ liệu từ nhiều nguồn phân tán.\nCác yếu tố cơ bản của công ty từ Datacore bao gồm bảng cân đối kế toán, báo cáo kết quả kinh doanh và dữ liệu lưu chuyển tiền tệ toàn diện được lập theo Chuẩn mực Kế toán Việt Nam, mà chúng ta đối chiếu với các biến số chuẩn được sử dụng trong nghiên cứu quốc tế.\nCách tính giá trị sổ sách tuân theo phương pháp Fama-French, có tính đến thuế hoãn lại và cổ phiếu ưu đãi để đảm bảo tính tương đồng với các nghiên cứu tại Hoa Kỳ.\nDữ liệu giá cổ phiếu bao gồm các yếu tố điều chỉnh cho các hoạt động của công ty, cho phép tính toán lợi nhuận chính xác trong thời gian dài.\nTần suất hàng tháng là tiêu chuẩn trong nghiên cứu định giá tài sản, giúp giảm nhiễu trong khi vẫn duy trì đủ số liệu quan sát để suy luận thống kê.\nPhương pháp ước tính lãi suất phi rủi ro sử dụng lợi suất trái phiếu chính phủ Việt Nam làm thước đo thay thế, do không có chuỗi lãi suất ngắn hạn tiêu chuẩn tương đương với tín phiếu kho bạc Mỹ.\nKiểm định chất lượng dữ liệu thông qua thống kê mô tả và trực quan hóa giúp xác định các vấn đề tiềm ẩn trước khi tiến hành phân tích chính thức.\nXử lý theo lô cho phép xử lý hiệu quả các tập dữ liệu lớn hàng ngày mà nếu không sẽ vượt quá giới hạn bộ nhớ.\n\n\n\n\n\n\n\nFama, Eugene F, and Kenneth R French. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dữ liệu Datacore</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html",
    "href": "03_market_microstructure.html",
    "title": "4  Market Microstructure in Vietnam",
    "section": "",
    "text": "4.1 What Is Market Microstructure?\nMarket microstructure is the study of how trading rules, order handling mechanisms, and market design affect price formation, transaction costs, and liquidity. The field, pioneered by Kyle (1985), Glosten and Milgrom (1985), and Hasbrouck (2007), provides the analytical toolkit for understanding why observed prices may deviate from fundamental values and for how long.\nIn developed markets with continuous electronic trading, designated market makers, and minimal regulatory constraints on price movement, microstructure frictions are typically second-order concerns for researchers working at monthly or lower frequencies. In Vietnam’s equity markets, this is emphatically not the case. Daily price limits, thin trading, a predominantly retail investor base, discrete tick sizes, and the absence of formal market-making arrangements generate frictions that propagate into monthly returns, distort factor loadings, and bias portfolio-level inference. Any serious empirical analysis of Vietnamese equities must therefore begin with a careful assessment of market microstructure.\nThis chapter provides that assessment. We first describe the institutional architecture of Vietnamese equity trading. We then develop diagnostics for the most consequential frictions, such as price limit hits, zero-return days, illiquidity, and non-synchronous trading, and quantify their severity in the cross-section of listed firms. Finally, we derive practical guidance for adjusting portfolio construction and asset pricing tests.\nThe textbook assumption of frictionless markets implies that prices continuously and costlessly incorporate information. Under this assumption, the observed return on any asset at any frequency equals the “true” return dictated by fundamentals. Market microstructure relaxes this assumption by recognizing that prices are generated by a specific trading process with real costs, constraints, and imperfections.\nThe canonical framework of Kyle (1985) models a market with three types of participants:\nThe key insight is that the market maker cannot distinguish informed from uninformed order flow, so prices adjust gradually to information, creating a wedge between the transaction price and the fundamental value. The size of this wedge (the bid-ask spread) and the speed of price adjustment (market depth) are the core objects of microstructure theory.\nGlosten and Milgrom (1985) extend this framework to a sequential trade setting and show that the bid-ask spread has two components: an adverse selection component (compensation for trading against informed traders) and an order processing component (compensation for the mechanical costs of trading). Huang and Stoll (1997) further decompose the spread into realized spread and price impact components. These decompositions are important because they reveal different sources of trading costs and have different implications for market quality.\nFor empirical asset pricing, the key question is: at what frequency and under what conditions do microstructure effects become negligible? In highly liquid markets, Bali, Engle, and Murray (2016) argue that monthly returns are largely free of microstructure contamination. In Vietnam, as we demonstrate below, this is not the case. Microstructure effects persist at monthly and even quarterly frequencies for a substantial fraction of listed firms.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html#what-is-market-microstructure",
    "href": "03_market_microstructure.html#what-is-market-microstructure",
    "title": "4  Market Microstructure in Vietnam",
    "section": "",
    "text": "an informed trader who knows the asset’s fundamental value,\nnoise traders who trade for liquidity reasons, and\na market maker who sets prices to break even in expectation.\n\n\n\n\n\n4.1.1 The Microstructure-Asset Pricing Interface\nThe interface between microstructure and asset pricing operates through several channels. First, illiquidity itself may be a priced risk factor. Amihud (2002) shows that expected illiquidity is positively related to expected stock returns, implying a liquidity premium. Pástor and Stambaugh (2003) develop an equilibrium model in which liquidity risk (i.e., the covariance of a stock’s liquidity with market liquidity) commands a risk premium. Second, microstructure noise in prices biases estimated betas, factor loadings, and test statistics. Scholes and Williams (1977) first identified this bias in the context of non-synchronous trading, and Dimson (1979) proposed an aggregated-coefficients estimator to correct it. Third, price limits and other regulatory constraints censor the return distribution, creating truncation bias in volatility estimates, return moments, and extreme-value statistics (Kim and Rhee 1997).\nTable 4.1 summarizes these channels and their empirical consequences.\n\n\n\nTable 4.1: Channels Through Which Microstructure Affects Asset Pricing\n\n\n\n\n\n\n\n\n\n\nChannel\nMechanism\nEmpirical Consequence\n\n\n\n\nIlliquidity premium\nCompensation for bearing transaction costs and inventory risk\nCross-sectional return predictability by liquidity measures\n\n\nNon-synchronous trading\nInfrequent trading creates stale prices\nDownward-biased betas, attenuated correlations, and spurious lead-lag\n\n\nPrice limits\nRegulatory censoring of daily returns\nTruncated return distributions, volatility spillover, and artificial autocorrelation\n\n\nDiscrete tick sizes\nPrices constrained to a grid\nBid-ask bounce, return discreteness, biased volatility\n\n\nInvestor composition\nRetail-dominated order flow\nNoise trading, herding, sentiment-driven pricing",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html#trading-architecture-in-vietnam",
    "href": "03_market_microstructure.html#trading-architecture-in-vietnam",
    "title": "4  Market Microstructure in Vietnam",
    "section": "4.2 Trading Architecture in Vietnam",
    "text": "4.2 Trading Architecture in Vietnam\nVietnam operates two stock exchanges: the Ho Chi Minh Stock Exchange (HOSE), established in 2000, and the Hanoi Stock Exchange (HNX), established in 2005. HOSE lists larger firms and accounts for the majority of market capitalization and trading volume. HNX lists smaller firms and also operates the Unlisted Public Company Market (UPCoM) for firms that have not yet met full listing requirements. All three venues operate electronic limit order book systems without designated market makers.\n\n4.2.1 Exchange Characteristics\nTable 4.2 presents the key structural differences between HOSE, HNX, and UPCoM. These differences have direct implications for liquidity, price discovery, and the severity of microstructure frictions.\n\n\n\nTable 4.2: Exchange Comparison: HOSE, HNX, and UPCoM\n\n\n\n\n\n\n\n\n\n\n\nFeature\nHOSE\nHNX\nUPCoM\n\n\n\n\nEstablished\n2000\n2005\n2009\n\n\nListing tier\nLarge-cap\nMid/small-cap\nPre-listing\n\n\nDaily price limit\n\\(\\pm\\) 7%\n\\(\\pm\\) 10%\n\\(\\pm\\) 15%\n\n\nTick size regime\nTiered by price\nTiered by price\n100 VND\n\n\nTrading lot\n100 shares\n100 shares\n100 shares\n\n\nShort selling\nLimited\nNot available\nNot available\n\n\nForeign ownership cap\nIndustry-specific\nIndustry-specific\nIndustry-specific\n\n\n\n\n\n\nThe heterogeneous price limit bands across exchanges create a natural experiment for studying limit effects. HOSE’s tighter \\(\\pm\\) 7% band means that large-cap stocks are more frequently constrained than mid-cap stocks on HNX, conditional on the same information shock. UPCoM’s wider \\(\\pm\\) 15% band provides the least constrained environment, though its stocks are also the least liquid.\n\n\n4.2.2 Trading Sessions\nEach exchange operates a structured trading day with distinct sessions. Understanding session structure is essential because price formation mechanisms differ across sessions, and certain sessions are disproportionately important for benchmark pricing.\nThe closing auction (ATC) deserves particular attention. The ATC price is the official closing price used for index calculation, NAV computation, and margin requirements. Because it is determined by a single-bid auction, it can be manipulated by strategically timed orders, a phenomenon documented in numerous emerging markets (Comerton-Forde and Tang 2009; Hillion and Suominen 2004). Researchers using daily closing prices should be aware that ATC prices may not reflect the continuous-session equilibrium, particularly for less liquid stocks where a single large order can move the closing price.\n\n\n4.2.3 Order Types and Matching Rules\nVietnamese exchanges support a limited set of order types compared to developed markets (Table 4.3).\n\n\n\nTable 4.3: Available Order Types\n\n\n\n\n\n\n\n\n\n\nOrder Type\nDescription\nAvailability\n\n\n\n\nLimit order (LO)\nSpecifies price and quantity\nAll sessions\n\n\nMarket order (ATO/ATC)\nMatches at auction price\nAuction sessions only\n\n\nMarket-to-limit (MTL)\nConverts to limit at best available\nHNX only\n\n\n\n\n\n\nThe absence of iceberg orders, stop orders, and hidden orders means that the full limit order book is visible to all participants. While this enhances pre-trade transparency, it also means that large institutional orders face significant information leakage risk, which may deter institutional participation and reduce market depth.\nOrders are matched on a strict price-time priority basis during continuous sessions. During auction sessions, a single clearing price is determined that maximizes executed volume. If multiple prices satisfy this criterion, the price closest to the previous closing price is selected.\n\n\n4.2.4 Tick Size Structure\nTick sizes on HOSE are tiered by price level, which creates discontinuities in the bid-ask spread as a percentage of price.\n\ntick_sizes = pd.DataFrame({\n    \"Price Range (VND)\": [\n        \"&lt; 10,000\",\n        \"10,000–49,900\",\n        \"≥ 50,000\"\n    ],\n    \"Tick Size (VND)\": [10, 50, 100],\n    \"Minimum Spread as % of Midpoint\": [\n        \"0.10% at 10,000\",\n        \"0.10% at 50,000\",\n        \"0.20% at 50,000\"\n    ]\n})\n\ntick_sizes.style.hide(axis=\"index\")\n\n\n\nTable 4.4: Tick Size Schedule on HOSE\n\n\n\n\n\n\n\n\nPrice Range (VND)\nTick Size (VND)\nMinimum Spread as % of Midpoint\n\n\n\n\n&lt; 10,000\n10\n0.10% at 10,000\n\n\n10,000–49,900\n50\n0.10% at 50,000\n\n\n≥ 50,000\n100\n0.20% at 50,000\n\n\n\n\n\n\n\n\nThe jump from a 50 VND tick to a 100 VND tick at the 50,000 VND boundary means that the minimum percentage spread doubles discontinuously. This creates a “tick size cliff” that can affect the cross-sectional distribution of bid-ask spreads and, consequently, the measurement of illiquidity (Vo and Doan 2023). Bessembinder (2003) document similar effects in other markets with tiered tick structures.\n\n\n4.2.5 Investor Composition\nThe Vietnamese equity market is predominantly driven by retail investors. While foreign institutional investors account for a meaningful share of market capitalization (particularly in blue-chip stocks subject to foreign ownership limits), daily trading volume is overwhelmingly generated by domestic retail accounts.\nThis retail dominance has several consequences for microstructure. First, retail investors tend to submit smaller orders and trade more frequently, generating high message-to-trade ratios but limited depth at each price level. Second, retail order flow is more susceptible to herding and sentiment, which can amplify momentum and generate excess volatility (Barber et al. 2009; Kaniel et al. 2012). Third, the limited institutional presence means that sophisticated liquidity provision is scarce, particularly in mid- and small-cap stocks.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html#price-limits-and-their-consequences",
    "href": "03_market_microstructure.html#price-limits-and-their-consequences",
    "title": "4  Market Microstructure in Vietnam",
    "section": "4.3 Price Limits and Their Consequences",
    "text": "4.3 Price Limits and Their Consequences\nVietnam enforces daily price limits on all listed equities. A stock’s price cannot move beyond a fixed percentage of the previous day’s closing price within a single trading day. The limit bands are \\(\\pm\\) 7% on HOSE, \\(\\pm\\) 10% on HNX, and \\(\\pm\\) 15% on UPCoM.\n\n4.3.1 Theoretical Framework\nPrice limits were introduced with the stated goal of reducing volatility and preventing panic-driven price dislocations. However, the academic literature presents a more nuanced picture. The “magnet effect” hypothesis (Subrahmanyam 1994) predicts that price limits actually accelerate price movement toward the limit as traders rush to execute before the limit is hit. The “delayed price discovery” hypothesis (Fama and French 1989) argues that limits merely postpone inevitable price adjustments, creating volatility spillover into subsequent days.\nFormally, let \\(P_t^*\\) denote the equilibrium price on day \\(t\\) and \\(P_{t-1}^c\\) the previous closing price. The observed return is:\n\\[\nr_t^{obs} = \\begin{cases}\n\\bar{L} & \\text{if } r_t^* \\geq \\bar{L} \\\\\nr_t^* & \\text{if } \\underline{L} &lt; r_t^* &lt; \\bar{L} \\\\\n\\underline{L} & \\text{if } r_t^* \\leq \\underline{L}\n\\end{cases}\n\\tag{4.1}\\]\nwhere \\(r_t^* = \\ln(P_t^* / P_{t-1}^c)\\) is the latent (unconstrained) return, \\(\\bar{L}\\) is the upper limit, and \\(\\underline{L}\\) is the lower limit. The observed return \\(r_t^{obs}\\) is a censored version of the true return. This censoring has several consequences:\n\nTruncated moments: The observed variance \\(\\text{Var}(r_t^{obs}) &lt; \\text{Var}(r_t^*)\\) because extreme returns are clipped. This biases downward any volatility-based risk measure.\nArtificial autocorrelation: When \\(r_t^{obs} = \\bar{L}\\) and \\(r_{t+1}^{obs} &gt; 0\\) (continued adjustment the next day), the return series exhibits positive autocorrelation that is purely mechanical, not informational.\nVolatility spillover: Define excess volatility on day \\(t+1\\) as \\(\\sigma_{t+1}^2 - E[\\sigma_{t+1}^2 | \\text{no limit hit on day } t]\\). Kim and Rhee (1997) and Chu and Qiu (2019) document significant positive spillover, where days following limit hits exhibit abnormally high volatility.\nBiased extreme value statistics: Measures such as Value-at-Risk, Expected Shortfall, and maximum drawdown are mechanically bounded by the limit, understating true tail risk.\n\n\n\n4.3.2 Detecting Price Limit Hits\nWe now implement a diagnostic to detect price limit hits in the daily data.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\n# Load daily price data\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\n# Assume prices_daily contains: symbol, date, close, exchange\nprices_daily = pd.read_sql_query(\n    # , exchange\n    sql=\"\"\"\n        SELECT symbol, date, close\n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n).dropna()\n\n\n# Define limit bands by exchange\nlimit_bands = {\"HOSE\": 0.07, \"HNX\": 0.10, \"UPCoM\": 0.15}\n\nprices_daily = prices_daily.sort_values([\"symbol\", \"date\"])\nprices_daily[\"prev_close\"] = prices_daily.groupby(\"symbol\")[\"close\"].shift(1)\nprices_daily[\"ret\"] = prices_daily[\"close\"] / prices_daily[\"prev_close\"] - 1\nprices_daily[\"limit_band\"] = prices_daily[\"exchange\"].map(limit_bands)\n\n# A limit hit occurs when the return is within 0.1% of the theoretical limit\ntolerance = 0.001\nprices_daily[\"upper_hit\"] = (\n    prices_daily[\"ret\"] &gt;= prices_daily[\"limit_band\"] - tolerance\n)\nprices_daily[\"lower_hit\"] = (\n    prices_daily[\"ret\"] &lt;= -prices_daily[\"limit_band\"] + tolerance\n)\nprices_daily[\"limit_hit\"] = (\n    prices_daily[\"upper_hit\"] | prices_daily[\"lower_hit\"]\n)\n\n\n\n4.3.3 Frequency of Limit Hits\n\n\n\nprices_daily[\"year_month\"] = prices_daily[\"date\"].dt.to_period(\"M\")\n\nlimit_hit_monthly = (\n    prices_daily\n    .groupby([\"year_month\", \"exchange\"])\n    .agg(\n        total_obs=(\"limit_hit\", \"count\"),\n        limit_hits=(\"limit_hit\", \"sum\")\n    )\n    .reset_index()\n)\nlimit_hit_monthly[\"hit_rate\"] = (\n    limit_hit_monthly[\"limit_hits\"] / limit_hit_monthly[\"total_obs\"]\n)\nlimit_hit_monthly[\"date\"] = limit_hit_monthly[\"year_month\"].dt.to_timestamp()\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\nfor exchange, color in zip(\n    [\"HOSE\", \"HNX\", \"UPCoM\"], [\"#2C73D2\", \"#FF6B6B\", \"#5DCEAF\"]\n):\n    subset = limit_hit_monthly[limit_hit_monthly[\"exchange\"] == exchange]\n    ax.plot(\n        subset[\"date\"], subset[\"hit_rate\"] * 100,\n        label=exchange, color=color, linewidth=1.2\n    )\n\nax.set_ylabel(\"Limit Hit Rate (%)\")\nax.set_xlabel(\"\")\nax.legend(frameon=False)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\nFigure 4.1\n\n\n\n\n\n4.3.4 Volatility Spillover Test\nFollowing Kim and Rhee (1997), we test whether days following a limit hit exhibit abnormally high volatility. Define the dummy variable \\(D_t = 1\\) if a limit hit occurred on day \\(t\\), and estimate:\n\\[\n\\sigma_{t+1}^2 = \\alpha + \\beta D_t + \\gamma \\sigma_t^2 + \\varepsilon_{t+1}\n\\tag{4.2}\\]\nwhere \\(\\sigma_t^2\\) is the squared return. A positive and significant \\(\\beta\\) indicates volatility spillover attributable to the price limit.\n\nimport statsmodels.api as sm\n\n# Panel-level volatility spillover test\nprices_daily[\"sq_ret\"] = prices_daily[\"ret\"] ** 2\nprices_daily[\"sq_ret_lead\"] = prices_daily.groupby(\"symbol\")[\"sq_ret\"].shift(-1)\nprices_daily[\"limit_hit_int\"] = prices_daily[\"limit_hit\"].astype(int)\n\nspillover_data = prices_daily.dropna(subset=[\"sq_ret_lead\", \"sq_ret\"])\n\nX = sm.add_constant(spillover_data[[\"limit_hit_int\", \"sq_ret\"]])\ny = spillover_data[\"sq_ret_lead\"]\n\nmodel = sm.OLS(y, X).fit(cov_type=\"cluster\", cov_kwds={\"groups\": spillover_data[\"symbol\"]})\n\nspillover_results = pd.DataFrame({\n    \"Coefficient\": model.params,\n    \"Std. Error\": model.bse,\n    \"t-stat\": model.tvalues,\n    \"p-value\": model.pvalues\n}).round(6)\n\nprint(spillover_results)\n\n\n\n\n\n\n\nTip\n\n\n\nA significant positive coefficient on the limit hit dummy confirms that Vietnamese price limits do not eliminate volatility, they merely redistribute it across days. This has direct implications for risk management: daily VaR measures computed from censored returns understate true risk exposure.\n\n\n\n\n4.3.5 Return Autocorrelation Induced by Price Limits\nPrice limits mechanically induce positive autocorrelation in returns. To quantify this, we compute the first-order autocorrelation coefficient separately for stocks that hit limits frequently versus those that do not.\n\n\n\nTable 4.5: Return Autocorrelation by Price Limit Hit Frequency\n\n\n# Classify stocks by limit hit frequency\nstock_limit_freq = (\n    prices_daily\n    .groupby(\"symbol\")\n    .agg(\n        hit_rate=(\"limit_hit\", \"mean\"),\n        n_obs=(\"ret\", \"count\")\n    )\n    .query(\"n_obs &gt;= 250\")  # At least 1 year of data\n)\n\nstock_limit_freq[\"limit_group\"] = pd.qcut(\n    stock_limit_freq[\"hit_rate\"], q=3,\n    labels=[\"Low\", \"Medium\", \"High\"]\n)\n\n# Compute autocorrelation by group\ndef compute_autocorr(group_symbols):\n    subset = prices_daily[prices_daily[\"symbol\"].isin(group_symbols)].copy()\n    subset[\"ret_lag\"] = subset.groupby(\"symbol\")[\"ret\"].shift(1)\n    return subset[[\"ret\", \"ret_lag\"]].dropna().corr().iloc[0, 1]\n\nautocorr_results = []\nfor group in [\"Low\", \"Medium\", \"High\"]:\n    symbols = stock_limit_freq[stock_limit_freq[\"limit_group\"] == group].index\n    ac = compute_autocorr(symbols)\n    n_stocks = len(symbols)\n    avg_hit_rate = stock_limit_freq.loc[symbols, \"hit_rate\"].mean()\n    autocorr_results.append({\n        \"Group\": group,\n        \"N Stocks\": n_stocks,\n        \"Avg Limit Hit Rate (%)\": round(avg_hit_rate * 100, 2),\n        \"AR(1)\": round(ac, 4)\n    })\n\npd.DataFrame(autocorr_results).style.hide(axis=\"index\")\n\n\n\nThe expected pattern is a monotonically increasing autocorrelation from the Low to High limit-hit group, confirming that the observed serial dependence in returns is at least partly an artifact of price censoring rather than genuine return predictability.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html#liquidity-thin-trading-and-zero-returns",
    "href": "03_market_microstructure.html#liquidity-thin-trading-and-zero-returns",
    "title": "4  Market Microstructure in Vietnam",
    "section": "4.4 Liquidity, Thin Trading, and Zero Returns",
    "text": "4.4 Liquidity, Thin Trading, and Zero Returns\nLiquidity (i.e., the ability to trade quickly at low cost without moving the price) is a first-order concern in Vietnamese equities. A substantial fraction of listed firms, particularly on HNX and UPCoM, experience chronic illiquidity characterized by infrequent trading, wide bid-ask spreads, and frequent zero-return days.\n\n4.4.1 Measuring Liquidity\nThe academic literature has developed numerous liquidity measures, each capturing a different dimension of market quality. @#tbl-liquidity-measures summarizes the measures most applicable to Vietnamese data, given typical data availability.\n\n\n\nTable 4.6: Liquidity Measures for Vietnamese Equities\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nInterpretation\nData Required\n\n\n\n\nTurnover ratio\n\\(\\text{TO}_{i,t} = \\frac{\\text{Volume}_{i,t}}{\\text{Shares Outstanding}_{i}}\\)\nTrading intensity relative to float\nVolume, shares outstanding\n\n\nAmihud illiquidity\n\\(\\text{ILLIQ}_{i,t} = \\frac{1}{D} \\sum_{d=1}^{D} \\frac{|r_{i,d}|}{V_{i,d}}\\)\nPrice impact per unit of volume\nDaily returns, daily volume\n\n\nZero-return proportion\n\\(\\text{ZR}_{i,t} = \\frac{\\#\\{d : r_{i,d} = 0\\}}{D}\\)\nFrequency of non-trading or stale pricing\nDaily returns\n\n\nRoll spread\n\\(\\hat{S}_i = 2\\sqrt{-\\text{Cov}(r_{i,d}, r_{i,d-1})}\\)\nEffective bid-ask spread estimate\nDaily returns\n\n\nBid-ask spread\n\\(\\text{BA}_{i,d} = \\frac{\\text{Ask}_{i,d} - \\text{Bid}_{i,d}}{(\\text{Ask}_{i,d} + \\text{Bid}_{i,d})/2}\\)\nDirect transaction cost\nQuote data\n\n\n\n\n\n\nThe Amihud illiquidity ratio (Amihud 2002) is particularly useful because it requires only daily return and volume data. It captures the price impact of trading (i.e., the return per unit of currency volume) and has been shown to correlate well with more sophisticated microstructure-based measures such as the effective spread Goyenko and Ukhov (2009).\n\n\n4.4.2 Computing Liquidity Diagnostics\n\n# Compute standard liquidity measures at the stock-month level\nprices_daily[\"abs_ret\"] = prices_daily[\"ret\"].abs()\nprices_daily[\"zero_return\"] = (prices_daily[\"ret\"] == 0).astype(int)\nprices_daily[\"year_month\"] = prices_daily[\"date\"].dt.to_period(\"M\")\n\n# Assume volume is in shares and value is in VND\n# Amihud: average |ret| / value (in billions VND)\nprices_daily[\"amihud_daily\"] = np.where(\n    prices_daily[\"value\"] &gt; 0,\n    prices_daily[\"abs_ret\"] / (prices_daily[\"value\"] / 1e9),\n    np.nan\n)\n\nliquidity_monthly = (\n    prices_daily\n    .groupby([\"symbol\", \"year_month\"])\n    .agg(\n        zero_return_share=(\"zero_return\", \"mean\"),\n        avg_turnover=(\"turnover\", \"mean\"),\n        amihud=(\"amihud_daily\", \"mean\"),\n        trading_days=(\"ret\", \"count\"),\n        avg_daily_value=(\"value\", \"mean\")\n    )\n    .reset_index()\n)\n\n# Flag severely illiquid stock-months\nliquidity_monthly[\"illiquid_flag\"] = (\n    (liquidity_monthly[\"zero_return_share\"] &gt; 0.5) |\n    (liquidity_monthly[\"trading_days\"] &lt; 10) |\n    (liquidity_monthly[\"avg_daily_value\"] &lt; 1e8)  # &lt; 100M VND/day\n)\n\n\n\n4.4.3 Cross-Sectional Distribution of Liquidity\n\n\n\nTable 4.7: Cross-Sectional Distribution of Liquidity Measures (Latest Full Year)\n\n\nlatest_year = liquidity_monthly[\"year_month\"].dt.year.max()\nannual_liq = (\n    liquidity_monthly[liquidity_monthly[\"year_month\"].dt.year == latest_year]\n    .groupby(\"symbol\")\n    .agg(\n        zero_return_share=(\"zero_return_share\", \"mean\"),\n        avg_turnover=(\"avg_turnover\", \"mean\"),\n        amihud=(\"amihud\", \"mean\"),\n        avg_daily_value_m=(\"avg_daily_value\", lambda x: x.mean() / 1e6)\n    )\n)\n\nsummary_stats = annual_liq.describe(percentiles=[0.10, 0.25, 0.50, 0.75, 0.90]).T\nsummary_stats = summary_stats[\n    [\"mean\", \"std\", \"10%\", \"25%\", \"50%\", \"75%\", \"90%\"]\n].round(4)\nsummary_stats.columns = [\"Mean\", \"Std\", \"P10\", \"P25\", \"Median\", \"P75\", \"P90\"]\nsummary_stats.index = [\n    \"Zero-Return Share\",\n    \"Avg Daily Turnover\",\n    \"Amihud Illiquidity\",\n    \"Avg Daily Value (M VND)\"\n]\nsummary_stats\n\n\n\n\n\n4.4.4 Liquidity Distribution Across Exchanges\n\n\n\n# Merge exchange info\nstock_exchange = (\n    prices_daily[[\"symbol\", \"exchange\"]]\n    .drop_duplicates(\"symbol\")\n)\nannual_liq_exch = annual_liq.merge(\n    stock_exchange, left_index=True, right_on=\"symbol\"\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Zero-return share\nfor exchange, color in zip(\n    [\"HOSE\", \"HNX\", \"UPCoM\"], [\"#2C73D2\", \"#FF6B6B\", \"#5DCEAF\"]\n):\n    subset = annual_liq_exch[annual_liq_exch[\"exchange\"] == exchange]\n    axes[0].hist(\n        subset[\"zero_return_share\"], bins=30, alpha=0.6,\n        color=color, label=exchange, density=True\n    )\naxes[0].set_xlabel(\"Zero-Return Share\")\naxes[0].set_ylabel(\"Density\")\naxes[0].legend(frameon=False)\naxes[0].spines[\"top\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(False)\n\n# Amihud (log scale)\nfor exchange, color in zip(\n    [\"HOSE\", \"HNX\", \"UPCoM\"], [\"#2C73D2\", \"#FF6B6B\", \"#5DCEAF\"]\n):\n    subset = annual_liq_exch[annual_liq_exch[\"exchange\"] == exchange]\n    amihud_log = np.log(subset[\"amihud\"].clip(lower=1e-10))\n    axes[1].hist(\n        amihud_log, bins=30, alpha=0.6,\n        color=color, label=exchange, density=True\n    )\naxes[1].set_xlabel(\"Log Amihud Illiquidity\")\naxes[1].set_ylabel(\"Density\")\naxes[1].legend(frameon=False)\naxes[1].spines[\"top\"].set_visible(False)\naxes[1].spines[\"right\"].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 4.2\n\n\n\nThe distributions typically reveal a bimodal pattern: HOSE stocks cluster at low illiquidity values, while HNX and especially UPCoM stocks exhibit a long right tail of extreme illiquidity. This heterogeneity implies that a single liquidity filter or treatment is insufficient for the entire cross-section.\n\n\n4.4.5 Time Variation in Aggregate Liquidity\nMarket-wide liquidity is not constant. It deteriorates during crises, policy uncertainty, and periods of capital outflow, and improves during bull markets and periods of foreign inflow. The time variation in aggregate liquidity is itself a risk factor (Pástor and Stambaugh 2003).\n\n\n\nagg_liquidity = (\n    liquidity_monthly\n    .groupby(\"year_month\")\n    .agg(\n        median_amihud=(\"amihud\", \"median\"),\n        median_zero_ret=(\"zero_return_share\", \"median\"),\n        total_value=(\"avg_daily_value\", \"sum\")\n    )\n    .reset_index()\n)\nagg_liquidity[\"date\"] = agg_liquidity[\"year_month\"].dt.to_timestamp()\n\nfig, ax1 = plt.subplots(figsize=(8, 4))\n\nax1.plot(\n    agg_liquidity[\"date\"],\n    np.log(agg_liquidity[\"median_amihud\"].clip(lower=1e-10)),\n    color=\"#2C73D2\", linewidth=1.2\n)\nax1.set_ylabel(\"Log Median Amihud\", color=\"#2C73D2\")\nax1.tick_params(axis=\"y\", labelcolor=\"#2C73D2\")\n\nax2 = ax1.twinx()\nax2.fill_between(\n    agg_liquidity[\"date\"],\n    agg_liquidity[\"median_zero_ret\"] * 100,\n    alpha=0.3, color=\"#FF6B6B\"\n)\nax2.set_ylabel(\"Median Zero-Return Share (%)\", color=\"#FF6B6B\")\nax2.tick_params(axis=\"y\", labelcolor=\"#FF6B6B\")\n\nax1.spines[\"top\"].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\nFigure 4.3\n\n\n\n\n\n\n\n\n\nImportantPractical Recommendation\n\n\n\nBefore any asset pricing analysis, apply the following liquidity filter: exclude stock-months where the zero-return share exceeds 50%, where fewer than 15 trading days are observed, or where average daily trading value falls below a threshold (e.g., 100 million VND). Document the filter explicitly, and report sensitivity of results to alternative thresholds.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html#bid-ask-spread-estimation",
    "href": "03_market_microstructure.html#bid-ask-spread-estimation",
    "title": "4  Market Microstructure in Vietnam",
    "section": "4.5 Bid-Ask Spread Estimation",
    "text": "4.5 Bid-Ask Spread Estimation\nIn the absence of comprehensive quote data, the effective bid-ask spread can be estimated from transaction data using the method of Roll (1984). The Roll estimator exploits the fact that if the bid-ask bounce is the sole source of negative serial covariance in returns, then:\n\\[\n\\hat{S}_{\\text{Roll}} = 2\\sqrt{-\\text{Cov}(\\Delta p_t, \\Delta p_{t-1})}\n\\tag{4.3}\\]\nwhere \\(\\Delta p_t = p_t - p_{t-1}\\) is the price change. When the autocovariance is positive (which occurs when information-driven serial correlation dominates the bid-ask bounce), the Roll estimator is undefined. Hasbrouck (2009) proposes a Bayesian variant that handles this case by imposing a prior on the spread.\n\n# Compute Roll spread estimate at the stock-month level\nprices_daily[\"dprice\"] = prices_daily.groupby(\"symbol\")[\"close\"].diff()\nprices_daily[\"dprice_lag\"] = prices_daily.groupby(\"symbol\")[\"dprice\"].shift(1)\n\nroll_cov = (\n    prices_daily\n    .groupby([\"symbol\", \"year_month\"])\n    .apply(\n        lambda g: g[[\"dprice\", \"dprice_lag\"]].dropna().cov().iloc[0, 1],\n        include_groups=False\n    )\n    .reset_index(name=\"autocovariance\")\n)\n\n# Roll spread is defined only when autocovariance is negative\nroll_cov[\"roll_spread\"] = np.where(\n    roll_cov[\"autocovariance\"] &lt; 0,\n    2 * np.sqrt(-roll_cov[\"autocovariance\"]),\n    np.nan\n)\n\n# As a percentage of price\nroll_cov = roll_cov.merge(\n    prices_daily.groupby([\"symbol\", \"year_month\"])[\"close\"].mean()\n    .reset_index(name=\"avg_price\"),\n    on=[\"symbol\", \"year_month\"]\n)\nroll_cov[\"roll_spread_pct\"] = roll_cov[\"roll_spread\"] / roll_cov[\"avg_price\"] * 100\n\n\n\n\nTable 4.8: Distribution of Roll Spread Estimates (% of Price)\n\n\nroll_summary = (\n    roll_cov\n    .dropna(subset=[\"roll_spread_pct\"])\n    .groupby(\"year_month\")[\"roll_spread_pct\"]\n    .describe(percentiles=[0.25, 0.50, 0.75])\n    .reset_index()\n)\n\n# Show latest year summary\nlatest_year_roll = roll_cov[\n    roll_cov[\"year_month\"].dt.year == roll_cov[\"year_month\"].dt.year.max()\n]\nprint(\n    latest_year_roll[\"roll_spread_pct\"]\n    .dropna()\n    .describe(percentiles=[0.10, 0.25, 0.50, 0.75, 0.90])\n    .round(3)\n)",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html#non-synchronous-trading-bias",
    "href": "03_market_microstructure.html#non-synchronous-trading-bias",
    "title": "4  Market Microstructure in Vietnam",
    "section": "4.6 Non-Synchronous Trading Bias",
    "text": "4.6 Non-Synchronous Trading Bias\nWhen stocks do not trade at the same frequency or at the same times, observed returns are misaligned. This non-synchronous trading bias, first formalized by Scholes and Williams (1977) and Lo and MacKinlay (1990), is one of the most consequential microstructure effects for asset pricing in thin markets.\n\n4.6.1 The Problem\nSuppose the true (unobserved) return process for stock \\(i\\) follows a single-factor model:\n\\[\nr_{i,t}^* = \\alpha_i + \\beta_i r_{m,t}^* + \\varepsilon_{i,t}\n\\tag{4.4}\\]\nwhere \\(r_{m,t}^*\\) is the true market return and \\(\\beta_i\\) is the true beta. If stock \\(i\\) last traded \\(k\\) days before the end of day \\(t\\), the observed return incorporates information only up to day \\(t - k\\). Scholes and Williams (1977) show that the OLS estimate of beta from regressing observed returns on observed market returns is:\n\\[\n\\hat{\\beta}_i^{OLS} = \\beta_i \\cdot \\pi_i\n\\tag{4.5}\\]\nwhere \\(\\pi_i\\) is the probability that stock \\(i\\) trades on any given day. For a stock that trades on only 50% of days, the OLS beta is biased downward by 50%. This bias is severe in Vietnam, where many small-cap stocks trade on fewer than half of all trading days.\n\n\n4.6.2 Quantifying the Bias\n\n\n\n# Compute trading frequency: proportion of market days with nonzero volume\nmarket_days = prices_daily.groupby(\"year_month\")[\"date\"].nunique()\ntrading_freq = (\n    prices_daily[prices_daily[\"value\"] &gt; 0]\n    .groupby([\"symbol\", \"year_month\"])[\"date\"]\n    .nunique()\n    .reset_index(name=\"days_traded\")\n)\ntrading_freq = trading_freq.merge(\n    market_days.reset_index().rename(columns={\"date\": \"market_days\"}),\n    on=\"year_month\"\n)\ntrading_freq[\"trade_prob\"] = trading_freq[\"days_traded\"] / trading_freq[\"market_days\"]\n\n# Annual average\nannual_trade_freq = (\n    trading_freq\n    .groupby(\"symbol\")[\"trade_prob\"]\n    .mean()\n    .reset_index(name=\"avg_trade_prob\")\n)\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.hist(\n    annual_trade_freq[\"avg_trade_prob\"], bins=50,\n    color=\"#2C73D2\", edgecolor=\"white\", alpha=0.8\n)\nax.axvline(\n    annual_trade_freq[\"avg_trade_prob\"].median(),\n    color=\"#FF6B6B\", linestyle=\"--\", linewidth=1.5,\n    label=f\"Median = {annual_trade_freq['avg_trade_prob'].median():.2f}\"\n)\nax.set_xlabel(\"Average Trading Probability (Fraction of Market Days)\")\nax.set_ylabel(\"Number of Stocks\")\nax.legend(frameon=False)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\nFigure 4.4\n\n\n\n\n\n4.6.3 The Dimson Beta Correction\nDimson (1979) proposes a simple correction: include lagged and leading market returns in the beta regression:\n\\[\nr_{i,t} = \\alpha_i + \\sum_{k=-K}^{K} \\beta_{i,k} \\, r_{m,t-k} + \\varepsilon_{i,t}\n\\tag{4.6}\\]\nThe Dimson-corrected beta is \\(\\hat{\\beta}_i^{Dimson} = \\sum_{k=-K}^{K} \\hat{\\beta}_{i,k}\\). Typically \\(K = 1\\) or \\(K = 2\\) is sufficient. The summed coefficients capture the full response of the stock’s observed return to market information, regardless of when the stock actually trades.\n\n# Estimate Dimson betas with K=1 lag and lead\n# Merge market return\nmarket_ret = (\n    prices_daily\n    .groupby(\"date\")\n    .apply(\n        lambda g: np.average(g[\"ret\"].dropna(), weights=g[\"mktcap\"].loc[g[\"ret\"].dropna().index])\n        if g[\"ret\"].dropna().shape[0] &gt; 0 else np.nan,\n        include_groups=False\n    )\n    .reset_index(name=\"rm\")\n)\n\nprices_daily = prices_daily.merge(market_ret, on=\"date\", how=\"left\")\nprices_daily[\"rm_lag1\"] = prices_daily.groupby(\"symbol\")[\"rm\"].shift(1)\nprices_daily[\"rm_lead1\"] = prices_daily.groupby(\"symbol\")[\"rm\"].shift(-1)\n\ndef estimate_dimson_beta(group):\n    \"\"\"Estimate OLS and Dimson(K=1) betas for a single stock.\"\"\"\n    g = group.dropna(subset=[\"ret\", \"rm\", \"rm_lag1\", \"rm_lead1\"])\n    if len(g) &lt; 60:\n        return pd.Series({\"beta_ols\": np.nan, \"beta_dimson\": np.nan, \"n_obs\": len(g)})\n\n    # OLS beta\n    X_ols = sm.add_constant(g[\"rm\"])\n    ols_model = sm.OLS(g[\"ret\"], X_ols).fit()\n    beta_ols = ols_model.params[\"rm\"]\n\n    # Dimson beta\n    X_dim = sm.add_constant(g[[\"rm_lag1\", \"rm\", \"rm_lead1\"]])\n    dim_model = sm.OLS(g[\"ret\"], X_dim).fit()\n    beta_dimson = dim_model.params[[\"rm_lag1\", \"rm\", \"rm_lead1\"]].sum()\n\n    return pd.Series({\n        \"beta_ols\": beta_ols,\n        \"beta_dimson\": beta_dimson,\n        \"n_obs\": len(g)\n    })\n\nbeta_comparison = (\n    prices_daily\n    .groupby(\"symbol\")\n    .apply(estimate_dimson_beta, include_groups=False)\n    .reset_index()\n)\n\n\n\n\nbeta_valid = beta_comparison.dropna()\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.scatter(\n    beta_valid[\"beta_ols\"], beta_valid[\"beta_dimson\"],\n    alpha=0.3, s=10, color=\"#2C73D2\"\n)\nlims = [\n    min(ax.get_xlim()[0], ax.get_ylim()[0]),\n    max(ax.get_xlim()[1], ax.get_ylim()[1])\n]\nax.plot(lims, lims, \"--\", color=\"gray\", linewidth=1)\nax.set_xlabel(\"OLS Beta\")\nax.set_ylabel(\"Dimson Beta (K=1)\")\nax.set_aspect(\"equal\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\nFigure 4.5\n\n\n\nThe scatter plot should reveal a systematic pattern: Dimson betas exceed OLS betas for most stocks, with the discrepancy largest for thinly traded stocks. Points above the 45-degree line indicate stocks whose OLS betas are biased downward by non-synchronous trading.\n\n\n\nTable 4.9: Beta Bias by Trading Frequency Tercile\n\n\nbeta_with_freq = beta_valid.merge(annual_trade_freq, on=\"symbol\")\nbeta_with_freq[\"freq_tercile\"] = pd.qcut(\n    beta_with_freq[\"avg_trade_prob\"], q=3,\n    labels=[\"Low (Thin)\", \"Medium\", \"High (Liquid)\"]\n)\n\nbeta_bias_summary = (\n    beta_with_freq\n    .groupby(\"freq_tercile\")\n    .agg(\n        n_stocks=(\"symbol\", \"count\"),\n        avg_trade_prob=(\"avg_trade_prob\", \"mean\"),\n        mean_beta_ols=(\"beta_ols\", \"mean\"),\n        mean_beta_dimson=(\"beta_dimson\", \"mean\"),\n        median_beta_ols=(\"beta_ols\", \"median\"),\n        median_beta_dimson=(\"beta_dimson\", \"median\")\n    )\n    .round(3)\n)\n\nbeta_bias_summary[\"bias_pct\"] = (\n    (beta_bias_summary[\"mean_beta_dimson\"] - beta_bias_summary[\"mean_beta_ols\"])\n    / beta_bias_summary[\"mean_beta_dimson\"] * 100\n).round(1)\n\nbeta_bias_summary\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor the thinnest-traded tercile, OLS beta underestimates true systematic risk by 20-40% on average. Using uncorrected betas for cost of equity estimation or factor model tests will produce systematically incorrect results for these stocks.\n\n\n\n\n4.6.4 The Scholes-Williams Estimator\nAn alternative correction, proposed by Scholes and Williams (1977), estimates beta as:\n\\[\n\\hat{\\beta}_i^{SW} = \\frac{\\hat{\\beta}_{i,-1} + \\hat{\\beta}_{i,0} + \\hat{\\beta}_{i,+1}}{1 + 2\\hat{\\rho}_m}\n\\tag{4.7}\\]\nwhere \\(\\hat{\\beta}_{i,k}\\) is the slope from regressing \\(r_{i,t}\\) on \\(r_{m,t-k}\\) alone, and \\(\\hat{\\rho}_m\\) is the first-order autocorrelation of the market return. The Scholes-Williams estimator is consistent under the assumption that non-trading is the sole source of serial cross-correlation, while the Dimson estimator is more robust to additional sources of lead-lag structure.\n\ndef estimate_sw_beta(group):\n    \"\"\"Estimate Scholes-Williams beta.\"\"\"\n    g = group.dropna(subset=[\"ret\", \"rm\", \"rm_lag1\", \"rm_lead1\"])\n    if len(g) &lt; 60:\n        return np.nan\n\n    # Separate regressions\n    beta_lag = sm.OLS(g[\"ret\"], sm.add_constant(g[\"rm_lag1\"])).fit().params.iloc[1]\n    beta_0 = sm.OLS(g[\"ret\"], sm.add_constant(g[\"rm\"])).fit().params.iloc[1]\n    beta_lead = sm.OLS(g[\"ret\"], sm.add_constant(g[\"rm_lead1\"])).fit().params.iloc[1]\n\n    # Market autocorrelation\n    rho_m = g[\"rm\"].autocorr(lag=1)\n\n    beta_sw = (beta_lag + beta_0 + beta_lead) / (1 + 2 * rho_m)\n    return beta_sw\n\nbeta_comparison[\"beta_sw\"] = (\n    prices_daily\n    .groupby(\"symbol\")\n    .apply(estimate_sw_beta, include_groups=False)\n    .values\n)",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html#implications-for-portfolio-construction",
    "href": "03_market_microstructure.html#implications-for-portfolio-construction",
    "title": "4  Market Microstructure in Vietnam",
    "section": "4.7 Implications for Portfolio Construction",
    "text": "4.7 Implications for Portfolio Construction\nThe microstructure frictions documented above have direct consequences for portfolio construction, particularly for strategies that involve rebalancing across the full cross-section of listed firms.\n\n4.7.1 Equal-Weighted vs. Value-Weighted Returns\nEqual-weighted portfolio returns give the same weight to each stock, including illiquid small-cap stocks that may contribute stale or noisy prices. Value-weighted returns tilt toward large, liquid stocks and are less susceptible to microstructure contamination.\n\n\n\nmonthly_returns = (\n    prices_daily\n    .groupby([\"symbol\", \"year_month\"])\n    .agg(\n        monthly_ret=(\"ret\", lambda x: (1 + x).prod() - 1),\n        last_mktcap=(\"mktcap\", \"last\")\n    )\n    .reset_index()\n)\nmonthly_returns[\"date\"] = monthly_returns[\"year_month\"].dt.to_timestamp()\n\n# Equal-weighted\new_ret = monthly_returns.groupby(\"date\")[\"monthly_ret\"].mean().reset_index(name=\"ew\")\n\n# Value-weighted\ndef vw_return(group):\n    w = group[\"last_mktcap\"] / group[\"last_mktcap\"].sum()\n    return (w * group[\"monthly_ret\"]).sum()\n\nvw_ret = (\n    monthly_returns.groupby(\"date\")\n    .apply(vw_return, include_groups=False)\n    .reset_index(name=\"vw\")\n)\n\nport_comp = ew_ret.merge(vw_ret, on=\"date\")\n\nfig, ax = plt.subplots(figsize=(8, 4))\nfor col, label, color in [\n    (\"ew\", \"Equal-Weighted\", \"#FF6B6B\"),\n    (\"vw\", \"Value-Weighted\", \"#2C73D2\")\n]:\n    cum_ret = (1 + port_comp[col]).cumprod()\n    ax.plot(port_comp[\"date\"], cum_ret, label=label, color=color, linewidth=1.2)\n\nax.set_ylabel(\"Cumulative Return (Growth of 1 VND)\")\nax.set_xlabel(\"\")\nax.legend(frameon=False)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\nFigure 4.6\n\n\n\nA persistent divergence between equal-weighted and value-weighted cumulative returns is a hallmark of microstructure effects: the equal-weighted portfolio overstates attainable returns because it implicitly assumes costless trading in illiquid stocks.\n\n\n4.7.2 Recommended Liquidity Filters\nBased on the diagnostics developed in this chapter, we recommend the following pre-analysis filters:\n\n\n\n\n\n\nTip\n\n\n\nAlways report results with and without liquidity filters. If results are qualitatively different, the baseline findings may be driven by microstructure artifacts rather than genuine economic effects.\n\n\n\n\n4.7.3 Monthly vs. Daily Frequency\nFor most asset pricing applications, monthly return aggregation is preferable to daily analysis in Vietnam because:\n\nMonthly returns smooth out intraday noise, bid-ask bounce, and price limit effects.\nStocks that trade infrequently within a month still produce a meaningful monthly return.\nFactor portfolio sorts are conventionally conducted at monthly frequency.\nStatistical tests have better size properties when microstructure noise is reduced.\n\nHowever, monthly aggregation does not eliminate all biases. Stocks with zero returns for an entire month still contribute stale observations. The Dimson and Scholes-Williams corrections should still be applied at monthly frequency for beta estimation.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html#implications-for-asset-pricing-tests",
    "href": "03_market_microstructure.html#implications-for-asset-pricing-tests",
    "title": "4  Market Microstructure in Vietnam",
    "section": "4.8 Implications for Asset Pricing Tests",
    "text": "4.8 Implications for Asset Pricing Tests\n\n4.8.1 Factor Model Estimation\nStandard factor model estimation assumes that returns are observed synchronously and without censoring. In Vietnam, both assumptions are violated. The practical consequences are in Table 4.10\n\n\n\nTable 4.10: Standard Assumptions and Their Violations\n\n\n\n\n\n\n\n\n\n\nAssumption\nViolation in Vietnam\nConsequence\n\n\n\n\nSynchronous observation\nThin trading\nBiased betas, attenuated R²\n\n\nUncensored returns\nPrice limits\nTruncated distributions, biased moments\n\n\nContinuous trading\nDiscrete ticks\nReturn discreteness, bid-ask bounce\n\n\nNo transaction costs\nWide spreads\nOverstated portfolio returns\n\n\n\n\n\n\n\n\n4.8.2 Adjusted Testing Procedure\nWe recommend the following adjustments to standard asset pricing tests when applied to Vietnamese data:\n\nBeta estimation: Use Dimson (\\(K \\ge 1\\)) or Scholes-Williams betas, not OLS betas.\nFactor construction: When forming size and value portfolios, apply liquidity filters before sorting. Consider excluding the smallest quintile of stocks by market capitalization, which is most affected by thin trading.\nReturn aggregation: Use monthly frequency. If daily analysis is necessary, include lagged market returns in the time-series regression.\nRobust inference: Cluster standard errors by stock to account for persistent microstructure-induced serial correlation. Use Newey-West HAC standard errors with sufficient lags.\nPrice limit adjustment: For volatility analysis or risk measurement, consider the Chu and Qiu (2019) approach of modeling the latent (uncensored) return distribution using truncated regression:\n\n\\[\nr_{i,t}^* \\sim N(\\mu_i, \\sigma_i^2), \\quad r_{i,t}^{obs} = \\max(\\underline{L}, \\min(\\bar{L}, r_{i,t}^*))\n\\tag{4.8}\\]\nEstimate \\(\\mu_i\\) and \\(\\sigma_i^2\\) via maximum likelihood for the truncated normal.\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef truncated_normal_nll(params, returns, lower, upper):\n    \"\"\"Negative log-likelihood of truncated normal.\"\"\"\n    mu, log_sigma = params\n    sigma = np.exp(log_sigma)\n\n    # Interior observations\n    interior = (returns &gt; lower) & (returns &lt; upper)\n    ll_interior = norm.logpdf(returns[interior], mu, sigma)\n\n    # Lower censored\n    ll_lower = norm.logcdf(lower, mu, sigma)\n    n_lower = (returns &lt;= lower).sum()\n\n    # Upper censored\n    ll_upper = np.log(1 - norm.cdf(upper, mu, sigma) + 1e-15)\n    n_upper = (returns &gt;= upper).sum()\n\n    nll = -(ll_interior.sum() + n_lower * ll_lower + n_upper * ll_upper)\n    return nll\n\ndef estimate_true_volatility(returns, limit_band):\n    \"\"\"Estimate latent volatility correcting for price limit censoring.\"\"\"\n    result = minimize(\n        truncated_normal_nll,\n        x0=[returns.mean(), np.log(returns.std())],\n        args=(returns.values, -limit_band, limit_band),\n        method=\"Nelder-Mead\"\n    )\n    mu, log_sigma = result.x\n    return np.exp(log_sigma)\n\n\nSensitivity reporting: Always report key results under alternative specifications: with and without liquidity filters, using OLS vs. Dimson betas, at daily vs. monthly frequency, and using observed vs. truncation-corrected volatility.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "03_market_microstructure.html#summary",
    "href": "03_market_microstructure.html#summary",
    "title": "4  Market Microstructure in Vietnam",
    "section": "4.9 Summary",
    "text": "4.9 Summary\nThis chapter has established that Vietnamese equity markets exhibit microstructure characteristics that materially affect observed prices, returns, and risk measures. The key findings are:\n\nPrice limits censor daily returns, inducing positive autocorrelation, volatility spillover, and truncated distributions. The \\(\\pm\\) 7% band on HOSE is particularly restrictive for volatile stocks.\nThin trading and zero returns afflict a substantial fraction of listed firms. Trading probabilities below 50% are common on HNX and UPCoM, generating non-synchronous trading bias that attenuates OLS beta estimates by 20-40%.\nIlliquidity varies dramatically across the cross-section, with Amihud ratios spanning several orders of magnitude. Value-weighted portfolio returns are less contaminated than equal-weighted returns.\nThe Dimson and Scholes-Williams beta corrections effectively address non-synchronous trading bias and should be used as the default beta estimator for Vietnamese equities.\nLiquidity filters should be applied before any asset pricing analysis, and results should be reported with and without these filters as a robustness check.\n\nIgnoring these frictions does not merely add noise to empirical results, it systematically biases estimates in predictable directions. The diagnostics and corrections presented in this chapter provide the foundation for credible empirical asset pricing in Vietnam.\n\n\n\n\n\n\n\nAmihud, Yakov. 2002. “Illiquidity and Stock Returns: Cross-Section and Time-Series Effects.” Journal of Financial Markets 5 (1): 31–56.\n\n\nBali, Turan G, Robert F Engle, and Scott Murray. 2016. Empirical Asset Pricing: The Cross Section of Stock Returns. John Wiley & Sons.\n\n\nBarber, Brad M, Yi-Tsung Lee, Yu-Jane Liu, and Terrance Odean. 2009. “Just How Much Do Individual Investors Lose by Trading?” The Review of Financial Studies 22 (2): 609–32.\n\n\nBessembinder, Hendrik. 2003. “Trade Execution Costs and Market Quality After Decimalization.” Journal of Financial and Quantitative Analysis 38 (4): 747–77.\n\n\nChu, Xiaojun, and Jianying Qiu. 2019. “Forecasting Volatility with Price Limit Hits—Evidence from Chinese Stock Market.” Emerging Markets Finance and Trade 55 (5): 1034–50.\n\n\nComerton-Forde, Carole, and Kar Mei Tang. 2009. “Anonymity, Liquidity and Fragmentation.” Journal of Financial Markets 12 (3): 337–67.\n\n\nDimson, Elroy. 1979. “Risk Measurement When Shares Are Subject to Infrequent Trading.” Journal of Financial Economics 7 (2): 197–226.\n\n\nFama, Eugene F., and Kenneth R. French. 1989. “Business conditions and expected returns on stocks and bonds.” Journal of Financial Economics 25 (1): 23–49. https://doi.org/10.1016/0304-405X(89)90095-0.\n\n\nGlosten, Lawrence R, and Paul R Milgrom. 1985. “Bid, Ask and Transaction Prices in a Specialist Market with Heterogeneously Informed Traders.” Journal of Financial Economics 14 (1): 71–100.\n\n\nGoyenko, Ruslan Y, Craig W Holden, and Charles A Trzcinka. 2009. “Do Liquidity Measures Measure Liquidity?” Journal of Financial Economics 92 (2): 153–81.\n\n\nGoyenko, Ruslan Y, and Andrey D Ukhov. 2009. “Stock and Bond Market Liquidity: A Long-Run Empirical Analysis.” Journal of Financial and Quantitative Analysis 44 (1): 189–212.\n\n\nHasbrouck, Joel. 2007. Empirical Market Microstructure: The Institutions, Economics, and Econometrics of Securities Trading. Oxford University Press.\n\n\n———. 2009. “Trading Costs and Returns for US Equities: Estimating Effective Costs from Daily Data.” The Journal of Finance 64 (3): 1445–77.\n\n\nHillion, Pierre, and Matti Suominen. 2004. “The Manipulation of Closing Prices.” Journal of Financial Markets 7 (4): 351–75.\n\n\nHuang, Roger D, and Hans R Stoll. 1997. “The Components of the Bid-Ask Spread: A General Approach.” The Review of Financial Studies 10 (4): 995–1034.\n\n\nKaniel, Ron, Shuming Liu, Gideon Saar, and Sheridan Titman. 2012. “Individual Investor Trading and Return Patterns Around Earnings Announcements.” The Journal of Finance 67 (2): 639–80.\n\n\nKim, Kenneth A, and S Ghon Rhee. 1997. “Price Limit Performance: Evidence from the Tokyo Stock Exchange.” The Journal of Finance 52 (2): 885–901.\n\n\nKyle, Albert S. 1985. “Continuous Auctions and Insider Trading.” Econometrica: Journal of the Econometric Society, 1315–35.\n\n\nLo, Andrew W, and A Craig MacKinlay. 1990. “An Econometric Analysis of Nonsynchronous Trading.” Journal of Econometrics 45 (1-2): 181–211.\n\n\nPástor, L’uboš, and Robert F Stambaugh. 2003. “Liquidity Risk and Expected Stock Returns.” Journal of Political Economy 111 (3): 642–85.\n\n\nRoll, Richard. 1984. “A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market.” The Journal of Finance 39 (4): 1127–39.\n\n\nScholes, Myron, and Joseph Williams. 1977. “Estimating Betas from Nonsynchronous Data.” Journal of Financial Economics 5 (3): 309–27.\n\n\nSubrahmanyam, Avanidhar. 1994. “Circuit Breakers and Market Volatility: A Theoretical Perspective.” The Journal of Finance 49 (1): 237–54.\n\n\nVo, Duc Hong, and Bao Doan. 2023. “Minimum Tick Size, Market Quality and Costs of Trade Execution in Vietnam.” Plos One 18 (5): e0285821.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Market Microstructure in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html",
    "href": "04_risk_free_rate_construction.html",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "",
    "text": "5.1 The Role of the Risk-Free Rate in Finance\nThe risk-free rate is the most important single number in finance. It anchors excess returns, discount rates, factor premiums, the cost of equity, performance evaluation, and derivative pricing. Despite its foundational role, the risk-free rate is often treated as a given: a number downloaded from a database and plugged into formulas without further thought. In developed markets with deep, liquid government securities markets, this casual approach is usually harmless. In Vietnam, it is not.\nVietnam’s fixed-income market is thin, fragmented, and characterized by irregular issuance of short-term government securities. There is no single, universally accepted risk-free rate analogous to the 1-month Treasury bill rate that anchors virtually all asset pricing research in mature markets. Instead, researchers face a choice among imperfect proxies, each with distinct advantages and limitations. This choice is not innocuous: different proxies can produce meaningfully different excess returns, factor premiums, and valuation estimates.\nThis chapter develops a systematic approach to risk-free rate construction. We begin with the theoretical requirements for a risk-free asset, then evaluate available Vietnamese proxies against these requirements. We construct monthly risk-free rate series under alternative specifications, demonstrate frequency alignment and interpolation techniques, and conduct sensitivity analysis to quantify how the choice of proxy affects downstream results.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#the-role-of-the-risk-free-rate-in-finance",
    "href": "04_risk_free_rate_construction.html#the-role-of-the-risk-free-rate-in-finance",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "",
    "text": "5.1.1 Excess Returns\nThe most fundamental use of the risk-free rate is in the computation of excess returns. The excess return on asset \\(i\\) in period \\(t\\) is:\n\\[\nr_{i,t}^{e} = r_{i,t} - r_{f,t}\n\\tag{5.1}\\]\nwhere \\(r_{i,t}\\) is the raw return and \\(r_{f,t}\\) is the risk-free rate over the same period, in the same currency, and under the same compounding convention. Excess returns isolate the compensation for bearing risk, removing the return that could be earned without risk exposure.\nMismeasurement of \\(r_{f,t}\\) directly contaminates every excess return observation and, by extension, every quantity derived from excess returns. If \\(r_{f,t}\\) is systematically biased upward, excess returns are systematically understated, factor premiums are compressed, and the cost of equity is overstated.\n\n\n5.1.2 Factor Premiums\nIn the Fama and French (1993) three-factor model, the market risk premium is:\n\\[\n\\text{MKTRF}_t = r_{m,t} - r_{f,t}\n\\tag{5.2}\\]\nwhere \\(r_{m,t}\\) is the value-weighted market return. The size (SMB) and value (HML) premiums are defined as returns on long-short portfolios and do not directly depend on \\(r_{f,t}\\). However, the intercept (alpha) from a time-series regression of any portfolio’s excess return on the factors does depend on \\(r_{f,t}\\) through the dependent variable. A biased risk-free rate shifts all alphas uniformly.\n\n\n5.1.3 Discount Rates and Valuation\nThe discounted cash flow model values a firm as:\n\\[\nV_0 = \\sum_{t=1}^{\\infty} \\frac{E[CF_t]}{(1 + r_{WACC})^t}\n\\tag{5.3}\\]\nwhere the weighted average cost of capital (WACC) depends on the cost of equity, which in turn depends on the risk-free rate through the Capital Asset Pricing Model:\n\\[\nr_{e} = r_f + \\beta_i (\\bar{r}_m - r_f)\n\\tag{5.4}\\]\nA 100 basis point error in \\(r_f\\) flows through to the cost of equity and can change the present value of a long-duration cash flow stream by 10-20%, depending on the duration profile.\n\n\n5.1.4 Performance Evaluation\nRisk-adjusted performance measures such as the Sharpe ratio:\n\\[\n\\text{SR} = \\frac{\\bar{r}_p - \\bar{r}_f}{\\sigma(r_p - r_f)}\n\\tag{5.5}\\]\nand Jensen’s alpha:\n\\[\n\\alpha = \\bar{r}_p - r_f - \\hat{\\beta}_p (\\bar{r}_m - r_f)\n\\tag{5.6}\\]\nboth depend directly on \\(r_f\\). The Sharpe ratio is particularly sensitive because the denominator (excess return volatility) is also affected by the level and variability of \\(r_f\\).",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#what-does-risk-free-mean-in-practice",
    "href": "04_risk_free_rate_construction.html#what-does-risk-free-mean-in-practice",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.2 What Does “Risk-Free” Mean in Practice?",
    "text": "5.2 What Does “Risk-Free” Mean in Practice?\nA truly risk-free asset must satisfy four conditions simultaneously (Table 5.1).\n\n\n\nTable 5.1: Requirements for a Risk-Free Asset\n\n\n\n\n\n\n\n\n\n\nCondition\nDefinition\nPractical Challenge\n\n\n\n\nNo default risk\nThe issuer cannot fail to pay\nOnly sovereign debt in one’s own currency qualifies\n\n\nKnown cash flows\nPayoff is certain ex ante\nRules out floating-rate instruments\n\n\nNo reinvestment risk\nMaturity matches the investment horizon\nRequires zero-coupon securities of exact maturity\n\n\nHigh liquidity\nCan be traded at a low cost\nThin government bond markets fail this test\n\n\n\n\n\n\nNo real-world asset perfectly satisfies all four conditions. Even in the deepest government bond markets, there is a liquidity premium in off-the-run securities and a convenience yield in on-the-run issues (Krishnamurthy and Vissing-Jorgensen 2012). The practical question is: which available instrument comes closest?\n\n5.2.1 The Ideal Proxy\nThe ideal risk-free rate proxy for empirical asset pricing research has the following properties:\n\nShort maturity: Minimizes reinvestment risk and term premium contamination. The conventional choice is 1-month maturity.\nGovernment-backed: Eliminates credit risk (in domestic currency).\nActively traded: Ensures that the observed yield reflects current market conditions.\nRegular issuance: Provides a continuous time series without gaps.\nConsistent methodology: Yield computation is unambiguous and comparable over time.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#available-proxies-in-vietnam",
    "href": "04_risk_free_rate_construction.html#available-proxies-in-vietnam",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.3 Available Proxies in Vietnam",
    "text": "5.3 Available Proxies in Vietnam\nVietnam’s financial infrastructure provides several candidate instruments, none of which perfectly satisfies all criteria. We evaluate each in turn.\n\n5.3.1 Government Bond Yields\nThe Vietnamese government issues bonds across a range of maturities through the State Treasury and the Vietnam Bond Market (VBM). Key characteristics:\nThe primary limitation of government bond yields as a risk-free proxy is the scarcity of short-maturity securities. One-year bonds are the shortest regularly issued benchmark, and their yield includes a term premium that is absent from a true risk-free rate. Treasury bills (maturity &lt; 1 year) are issued irregularly and in small volumes, making them unsuitable as a continuous series.\n\n\n5.3.2 Interbank Overnight Rate\nThe Vietnam interbank market sets overnight lending rates between commercial banks. The overnight rate is reported by the SBV.\nAdvantages: Very short maturity (overnight), high frequency (daily), reflects actual borrowing costs in the financial system.\nLimitations: Reflects banking sector credit risk (interbank default risk, though small), can be volatile during liquidity crunches, and does not correspond to a tradeable zero-coupon instrument.\n\n\n5.3.3 SBV Policy Rates\nThe State Bank of Vietnam sets several administered rates (Table 5.2).\n\n\n\nTable 5.2: SBV Policy Rates\n\n\n\n\n\n\n\n\n\n\nRate\nRole\nFrequency of Change\n\n\n\n\nRefinancing rate\nRate at which SBV lends to banks\nInfrequent (policy meetings)\n\n\nDiscount rate\nRate for rediscounting eligible paper\nInfrequent\n\n\nOvernight lending rate\nCeiling for interbank overnight\nInfrequent\n\n\nDeposit rate cap\nMaximum rate banks can pay on deposits\nInfrequent\n\n\n\n\n\n\nAdvantages: Stable (changes infrequently), reflects the monetary policy stance, available for the entire sample period.\nLimitations: Not a traded return (i.e., no investor can actually earn the policy rate). Represents an administrative target, not a market-clearing price. Responds to macroeconomic conditions with a lag.\n\n\n5.3.4 Savings Deposit Rates\nCommercial banks offer term deposits at rates subject to SBV caps. Short-term (1-month or 3-month) deposit rates are sometimes used as informal risk-free proxies in practitioner contexts.\nAdvantages: Represent an investable return for small investors, widely available.\nLimitations: Subject to bank credit risk, vary across banks, caps create a ceiling that may not reflect true equilibrium rates, not standardized for research use.\n\n\n5.3.5 Summary Comparison",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#constructing-the-risk-free-rate-series",
    "href": "04_risk_free_rate_construction.html#constructing-the-risk-free-rate-series",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.4 Constructing the Risk-Free Rate Series",
    "text": "5.4 Constructing the Risk-Free Rate Series\nWe now construct alternative monthly risk-free rate series and examine their properties.\n\n5.4.1 Loading and Cleaning Rate Data\n\nimport pandas as pd\nimport numpy as np\n\n# Assume rf_data contains: date, rate_type, rate_annual (annualized, in %)\nrf_raw = pd.read_parquet(\"data/risk_free_rates.parquet\")\n\n# Preview available rate types\nprint(\"Available rate types:\")\nprint(rf_raw[\"rate_type\"].value_counts())\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8, 4.5))\n\ncolors = {\n    \"govt_bond_1y\": \"#2C73D2\",\n    \"interbank_overnight\": \"#FF6B6B\",\n    \"sbv_refinancing\": \"#5DCEAF\",\n    \"tbill_3m\": \"#FFB347\",\n    \"deposit_1m\": \"#B19CD9\"\n}\n\nfor rate_type, color in colors.items():\n    subset = rf_raw[rf_raw[\"rate_type\"] == rate_type].sort_values(\"date\")\n    if len(subset) &gt; 0:\n        ax.plot(\n            subset[\"date\"], subset[\"rate_annual\"],\n            label=rate_type.replace(\"_\", \" \").title(),\n            color=color, linewidth=1.2, alpha=0.85\n        )\n\nax.set_ylabel(\"Annualized Rate (%)\")\nax.set_xlabel(\"\")\nax.legend(frameon=False, fontsize=9, loc=\"upper right\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\nFigure 5.1\n\n\n\n\n\n5.4.2 Frequency Alignment\nAsset pricing tests require monthly risk-free rates. Raw data may arrive at daily, weekly, or irregular frequencies. We use the following conversion logic:\nDaily to monthly: Take the average of daily rates within each month, then convert from annualized to monthly.\nIrregular to monthly: For series with gaps (e.g., treasury bills), forward-fill the most recent observation, then average within each month.\nAnnualized to monthly: Under simple compounding, \\(r_f^{monthly} = r_f^{annual} / 12\\). Under continuous compounding, \\(r_f^{monthly} = r_f^{annual} / 12\\) (since continuous rates are additive). We use simple compounding for consistency with the convention that stock returns are computed as arithmetic returns.\n\n# Construct monthly risk-free rates from each proxy\n\ndef construct_monthly_rf(rf_raw, rate_type, method=\"mean\"):\n    \"\"\"\n    Convert raw rate data to monthly frequency.\n\n    Parameters\n    ----------\n    rf_raw : pd.DataFrame\n        Raw rate data with columns: date, rate_type, rate_annual\n    rate_type : str\n        Which rate proxy to use\n    method : str\n        Aggregation method: 'mean', 'last', or 'first'\n\n    Returns\n    -------\n    pd.DataFrame\n        Monthly risk-free rate with columns: date, rf_monthly\n    \"\"\"\n    subset = (\n        rf_raw[rf_raw[\"rate_type\"] == rate_type]\n        .sort_values(\"date\")\n        .set_index(\"date\")\n    )\n\n    # Forward-fill gaps (for irregular series)\n    subset = subset.resample(\"D\").ffill()\n\n    # Aggregate to monthly\n    if method == \"mean\":\n        monthly = subset.resample(\"ME\")[\"rate_annual\"].mean()\n    elif method == \"last\":\n        monthly = subset.resample(\"ME\")[\"rate_annual\"].last()\n    else:\n        monthly = subset.resample(\"ME\")[\"rate_annual\"].first()\n\n    monthly = monthly.reset_index()\n    monthly.columns = [\"date\", \"rf_annual\"]\n\n    # Convert annualized rate (%) to monthly decimal\n    monthly[\"rf_monthly\"] = monthly[\"rf_annual\"] / 100 / 12\n\n    return monthly[[\"date\", \"rf_monthly\", \"rf_annual\"]]\n\n# Construct series for each proxy\nrf_proxies = {}\nfor proxy in [\"govt_bond_1y\", \"interbank_overnight\", \"sbv_refinancing\"]:\n    rf_proxies[proxy] = construct_monthly_rf(rf_raw, proxy)\n    rf_proxies[proxy][\"proxy\"] = proxy\n\nrf_all = pd.concat(rf_proxies.values(), ignore_index=True)\n\n\n\n5.4.3 Handling Missing Data and Structural Breaks\nVietnamese rate data may contain gaps due to market closures, reporting changes, or the introduction of new instruments. We handle these systematically:\n\n# Check coverage for each proxy\ncoverage = (\n    rf_all\n    .groupby(\"proxy\")\n    .agg(\n        start_date=(\"date\", \"min\"),\n        end_date=(\"date\", \"max\"),\n        n_months=(\"rf_monthly\", \"count\"),\n        n_missing=(\"rf_monthly\", lambda x: x.isna().sum()),\n        avg_rate_pct=(\"rf_annual\", \"mean\")\n    )\n    .round(2)\n)\n\nprint(coverage)\n\nFor periods where the primary proxy is unavailable, we construct a blended series using a priority hierarchy:\n\ndef construct_blended_rf(rf_proxies, priority=None):\n    \"\"\"\n    Construct a blended monthly risk-free rate using proxy priority.\n\n    Priority order (default):\n    1. Treasury bills (shortest maturity, sovereign)\n    2. Interbank overnight (short maturity, high frequency)\n    3. 1-year government bond (sovereign, regular)\n    4. SBV refinancing rate (fallback)\n    \"\"\"\n    if priority is None:\n        priority = [\n            \"tbill_3m\",\n            \"interbank_overnight\",\n            \"govt_bond_1y\",\n            \"sbv_refinancing\"\n        ]\n\n    # Create full date range\n    all_dates = pd.date_range(\n        start=min(df[\"date\"].min() for df in rf_proxies.values()),\n        end=max(df[\"date\"].max() for df in rf_proxies.values()),\n        freq=\"ME\"\n    )\n\n    blended = pd.DataFrame({\"date\": all_dates})\n    blended[\"rf_monthly\"] = np.nan\n    blended[\"source\"] = \"\"\n\n    for proxy in priority:\n        if proxy in rf_proxies:\n            proxy_df = rf_proxies[proxy][[\"date\", \"rf_monthly\"]].rename(\n                columns={\"rf_monthly\": f\"rf_{proxy}\"}\n            )\n            blended = blended.merge(proxy_df, on=\"date\", how=\"left\")\n\n            # Fill missing values from this proxy\n            mask = blended[\"rf_monthly\"].isna() & blended[f\"rf_{proxy}\"].notna()\n            blended.loc[mask, \"rf_monthly\"] = blended.loc[mask, f\"rf_{proxy}\"]\n            blended.loc[mask, \"source\"] = proxy\n\n            blended = blended.drop(columns=[f\"rf_{proxy}\"])\n\n    return blended\n\nrf_blended = construct_blended_rf(rf_proxies)\n\n\n\n\nTable 5.3: Data Source Composition of Blended Risk-Free Rate Series\n\n\nsource_comp = (\n    rf_blended\n    .groupby(\"source\")\n    .agg(\n        n_months=(\"date\", \"count\"),\n        pct=(\"date\", lambda x: len(x) / len(rf_blended) * 100)\n    )\n    .round(1)\n    .sort_values(\"n_months\", ascending=False)\n)\n\nsource_comp\n\n\n\n\n\n5.4.4 Properties of the Constructed Series\n\n\n\nTable 5.4: Summary Statistics of Monthly Risk-Free Rate Proxies\n\n\nrf_wide = rf_all.pivot_table(\n    index=\"date\", columns=\"proxy\", values=\"rf_monthly\"\n)\n\nsummary = rf_wide.describe(percentiles=[0.10, 0.25, 0.50, 0.75, 0.90]).T\nsummary = summary[[\"mean\", \"std\", \"min\", \"10%\", \"50%\", \"90%\", \"max\"]]\nsummary.columns = [\n    \"Mean\", \"Std\", \"Min\", \"P10\", \"Median\", \"P90\", \"Max\"\n]\n\n# Convert to annualized percentage for interpretability\n(summary * 12 * 100).round(2)\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(8, 6), sharex=True)\n\n# Level\nfor proxy, color in [\n    (\"govt_bond_1y\", \"#2C73D2\"),\n    (\"interbank_overnight\", \"#FF6B6B\"),\n    (\"sbv_refinancing\", \"#5DCEAF\")\n]:\n    subset = rf_proxies[proxy].sort_values(\"date\")\n    axes[0].plot(\n        subset[\"date\"], subset[\"rf_monthly\"] * 100,\n        label=proxy.replace(\"_\", \" \").title(),\n        color=color, linewidth=1\n    )\naxes[0].set_ylabel(\"Monthly Rate (%)\")\naxes[0].legend(frameon=False, fontsize=9)\naxes[0].spines[\"top\"].set_visible(False)\naxes[0].spines[\"right\"].set_visible(False)\n\n# Pairwise spread: govt bond - interbank\nmerged = rf_proxies[\"govt_bond_1y\"][[\"date\", \"rf_monthly\"]].merge(\n    rf_proxies[\"interbank_overnight\"][[\"date\", \"rf_monthly\"]],\n    on=\"date\", suffixes=(\"_bond\", \"_interbank\")\n)\nmerged[\"spread\"] = (merged[\"rf_monthly_bond\"] - merged[\"rf_monthly_interbank\"]) * 100\n\naxes[1].fill_between(\n    merged[\"date\"], merged[\"spread\"], 0,\n    where=merged[\"spread\"] &gt; 0, alpha=0.4, color=\"#2C73D2\", label=\"Bond &gt; Interbank\"\n)\naxes[1].fill_between(\n    merged[\"date\"], merged[\"spread\"], 0,\n    where=merged[\"spread\"] &lt;= 0, alpha=0.4, color=\"#FF6B6B\", label=\"Bond &lt; Interbank\"\n)\naxes[1].axhline(0, color=\"black\", linewidth=0.5)\naxes[1].set_ylabel(\"Spread (% monthly)\")\naxes[1].set_xlabel(\"\")\naxes[1].legend(frameon=False, fontsize=9)\naxes[1].spines[\"top\"].set_visible(False)\naxes[1].spines[\"right\"].set_visible(False)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 5.2\n\n\n\nThe spread between proxies is informative. A consistently positive spread (bond yield &gt; interbank rate) reflects the term premium embedded in the 1-year bond. Periods where the interbank rate spikes above the bond yield typically correspond to liquidity crunches in the banking system.\n\n\n5.4.5 Correlation Across Proxies\n\n\n\nTable 5.5: Pairwise Correlation of Monthly Risk-Free Rate Proxies\n\n\nrf_corr = rf_wide.corr().round(3)\nrf_corr.index = [x.replace(\"_\", \" \").title() for x in rf_corr.index]\nrf_corr.columns = [x.replace(\"_\", \" \").title() for x in rf_corr.columns]\nrf_corr\n\n\n\nHigh correlation (&gt; 0.8) between proxies suggests that the level and direction of interest rate movements are captured similarly by all proxies. Low correlation would indicate that the choice of proxy introduces substantial idiosyncratic variation into excess returns.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#excess-return-construction",
    "href": "04_risk_free_rate_construction.html#excess-return-construction",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.5 Excess Return Construction",
    "text": "5.5 Excess Return Construction\nWith the risk-free rate series in hand, we now construct excess returns for individual stocks and for the market portfolio.\n\n5.5.1 Matching Conventions\nExcess return construction requires strict consistency across three dimensions (Table 5.6).\n\n\n\nTable 5.6: Consistency Requirements for Excess Returns\n\n\n\n\n\n\n\n\n\n\nDimension\nRequirement\nCommon Error\n\n\n\n\nCurrency\nSame currency for \\(r_i\\) and \\(r_f\\)\nUsing USD rate for VND-denominated returns\n\n\nFrequency\nSame holding period\nUsing annualized \\(r_f\\) with monthly \\(r_i\\)\n\n\nCompounding\nSame convention\nMixing log and arithmetic returns\n\n\n\n\n\n\n\n# Load monthly stock returns\nstock_returns = pd.read_parquet(\"data/monthly_returns.parquet\")\n# Assume columns: symbol, date (month-end), ret\n\n# Merge with risk-free rate\n# Use the blended series as the baseline\nrf_for_merge = rf_blended[[\"date\", \"rf_monthly\"]].rename(\n    columns={\"rf_monthly\": \"rf\"}\n)\n\nstock_returns = stock_returns.merge(rf_for_merge, on=\"date\", how=\"left\")\n\n# Compute excess returns\nstock_returns[\"ret_excess\"] = stock_returns[\"ret\"] - stock_returns[\"rf\"]\n\n\n\n5.5.2 Market Excess Return\n\n# Value-weighted market return\nmarket_monthly = (\n    stock_returns\n    .groupby(\"date\")\n    .apply(\n        lambda g: np.average(\n            g[\"ret\"].dropna(),\n            weights=g[\"mktcap\"].loc[g[\"ret\"].dropna().index]\n        ) if g[\"ret\"].dropna().shape[0] &gt; 0 else np.nan,\n        include_groups=False\n    )\n    .reset_index(name=\"rm\")\n)\n\nmarket_monthly = market_monthly.merge(rf_for_merge, on=\"date\", how=\"left\")\nmarket_monthly[\"mktrf\"] = market_monthly[\"rm\"] - market_monthly[\"rf\"]\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 3.5))\n\nax.bar(\n    market_monthly[\"date\"], market_monthly[\"mktrf\"] * 100,\n    color=np.where(market_monthly[\"mktrf\"] &gt;= 0, \"#2C73D2\", \"#FF6B6B\"),\n    width=25, alpha=0.8\n)\nax.axhline(0, color=\"black\", linewidth=0.5)\nax.set_ylabel(\"Market Excess Return (%)\")\nax.set_xlabel(\"\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\nFigure 5.3",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#sensitivity-analysis-how-much-does-the-proxy-choice-matter",
    "href": "04_risk_free_rate_construction.html#sensitivity-analysis-how-much-does-the-proxy-choice-matter",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.6 Sensitivity Analysis: How Much Does the Proxy Choice Matter?",
    "text": "5.6 Sensitivity Analysis: How Much Does the Proxy Choice Matter?\nThis is the central empirical question of the chapter. If different risk-free proxies produce essentially the same downstream results, the choice is inconsequential. If they produce different results, researchers must justify their choice and report robustness.\n\n5.6.1 Effect on the Equity Premium\n\n\n\nTable 5.7: Annualized Equity Premium Under Alternative Risk-Free Proxies\n\n\nresults = []\nfor proxy_name, proxy_df in rf_proxies.items():\n    rf_merge = proxy_df[[\"date\", \"rf_monthly\"]].rename(\n        columns={\"rf_monthly\": \"rf_proxy\"}\n    )\n    merged = market_monthly[[\"date\", \"rm\"]].merge(rf_merge, on=\"date\", how=\"inner\")\n    merged[\"mktrf_proxy\"] = merged[\"rm\"] - merged[\"rf_proxy\"]\n\n    n_months = merged[\"mktrf_proxy\"].count()\n    mean_monthly = merged[\"mktrf_proxy\"].mean()\n    std_monthly = merged[\"mktrf_proxy\"].std()\n    sharpe = mean_monthly / std_monthly if std_monthly &gt; 0 else np.nan\n    t_stat = mean_monthly / (std_monthly / np.sqrt(n_months))\n\n    results.append({\n        \"Proxy\": proxy_name.replace(\"_\", \" \").title(),\n        \"N Months\": n_months,\n        \"Mean (% ann.)\": round(mean_monthly * 12 * 100, 2),\n        \"Std (% ann.)\": round(std_monthly * np.sqrt(12) * 100, 2),\n        \"Sharpe (ann.)\": round(sharpe * np.sqrt(12), 3),\n        \"t-stat\": round(t_stat, 2)\n    })\n\npd.DataFrame(results).style.hide(axis=\"index\")\n\n\n\n\n\n5.6.2 Effect on Factor Premiums\n\n\n\nTable 5.8: Factor Premium Sensitivity to Risk-Free Proxy (Annualized %)\n\n\n# Load or construct factor returns\n# Assume factors_monthly has: date, smb, hml (these are long-short, rf-independent)\n# Only MKTRF changes with the proxy\n\nfactors_monthly = pd.read_parquet(\"data/factors_monthly.parquet\")\n\nfor proxy_name, proxy_df in rf_proxies.items():\n    rf_merge = proxy_df[[\"date\", \"rf_monthly\"]].rename(\n        columns={\"rf_monthly\": \"rf_proxy\"}\n    )\n    factors_merged = factors_monthly.merge(rf_merge, on=\"date\", how=\"inner\")\n    factors_merged = factors_merged.merge(\n        market_monthly[[\"date\", \"rm\"]], on=\"date\", how=\"inner\"\n    )\n    factors_merged[f\"mktrf_{proxy_name}\"] = (\n        factors_merged[\"rm\"] - factors_merged[\"rf_proxy\"]\n    )\n\n    mean_mktrf = factors_merged[f\"mktrf_{proxy_name}\"].mean() * 12 * 100\n    mean_smb = factors_merged[\"smb\"].mean() * 12 * 100\n    mean_hml = factors_merged[\"hml\"].mean() * 12 * 100\n\n    print(\n        f\"{proxy_name:&gt;25s}: MKTRF = {mean_mktrf:6.2f}%, \"\n        f\"SMB = {mean_smb:6.2f}%, HML = {mean_hml:6.2f}%\"\n    )\n\n\n\nNote that SMB and HML are constructed as long-short portfolio returns and should be identical regardless of the risk-free proxy. Only MKTRF differs. However, if the researcher uses the risk-free rate to compute individual stock excess returns before sorting into factor portfolios, small differences in sorting may arise.\n\n\n5.6.3 Effect on Alpha Estimates\nThe choice of risk-free proxy affects alpha estimates for any portfolio evaluated against a factor model. We illustrate this by estimating the alpha of a momentum portfolio under each proxy.\n\n\n\nTable 5.9: Momentum Portfolio Alpha Sensitivity to Risk-Free Proxy\n\n\nimport statsmodels.api as sm\n\n# Assume momentum_ret contains: date, mom_ret (raw return of WML portfolio)\nmomentum_ret = pd.read_parquet(\"data/momentum_returns.parquet\")\n\nalpha_results = []\nfor proxy_name, proxy_df in rf_proxies.items():\n    rf_merge = proxy_df[[\"date\", \"rf_monthly\"]].rename(\n        columns={\"rf_monthly\": \"rf_proxy\"}\n    )\n\n    merged = (\n        momentum_ret\n        .merge(rf_merge, on=\"date\", how=\"inner\")\n        .merge(market_monthly[[\"date\", \"rm\"]], on=\"date\", how=\"inner\")\n        .merge(factors_monthly[[\"date\", \"smb\", \"hml\"]], on=\"date\", how=\"inner\")\n    )\n\n    merged[\"mom_excess\"] = merged[\"mom_ret\"] - merged[\"rf_proxy\"]\n    merged[\"mktrf\"] = merged[\"rm\"] - merged[\"rf_proxy\"]\n\n    X = sm.add_constant(merged[[\"mktrf\", \"smb\", \"hml\"]])\n    y = merged[\"mom_excess\"]\n    model = sm.OLS(y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n\n    alpha_results.append({\n        \"Proxy\": proxy_name.replace(\"_\", \" \").title(),\n        \"Alpha (% monthly)\": round(model.params[\"const\"] * 100, 3),\n        \"t-stat\": round(model.tvalues[\"const\"], 2),\n        \"R²\": round(model.rsquared, 3)\n    })\n\npd.DataFrame(alpha_results).style.hide(axis=\"index\")\n\n\n\n\n\n5.6.4 Effect on Valuation\nTo illustrate the valuation impact, consider a simple DCF exercise where the cost of equity is estimated via the CAPM.\n\n\n\nTable 5.10: Cost of Equity and Terminal Value Sensitivity to Risk-Free Proxy\n\n\n# Example: firm with beta = 1.0, expected CF = 1 billion VND, growth = 3%\nbeta_example = 1.0\ncf = 1e9  # VND\ngrowth = 0.03\n\nvaluation_results = []\nfor proxy_name, proxy_df in rf_proxies.items():\n    rf_ann = proxy_df[\"rf_annual\"].dropna().iloc[-12:].mean() / 100  # Latest year avg\n\n    rf_merge = proxy_df[[\"date\", \"rf_monthly\"]].rename(\n        columns={\"rf_monthly\": \"rf_proxy\"}\n    )\n    mkt_merged = market_monthly[[\"date\", \"rm\"]].merge(\n        rf_merge, on=\"date\", how=\"inner\"\n    )\n    mkt_merged[\"mktrf\"] = mkt_merged[\"rm\"] - mkt_merged[\"rf_proxy\"]\n    erp = mkt_merged[\"mktrf\"].mean() * 12  # Annualized equity premium\n\n    cost_equity = rf_ann + beta_example * erp\n    terminal_value = cf / (cost_equity - growth) if cost_equity &gt; growth else np.nan\n\n    valuation_results.append({\n        \"Proxy\": proxy_name.replace(\"_\", \" \").title(),\n        \"Rf (% ann.)\": round(rf_ann * 100, 2),\n        \"ERP (% ann.)\": round(erp * 100, 2),\n        \"Cost of Equity (%)\": round(cost_equity * 100, 2),\n        \"Terminal Value (B VND)\": round(terminal_value / 1e9, 1) if terminal_value else \"N/A\"\n    })\n\npd.DataFrame(valuation_results).style.hide(axis=\"index\")\n\n\n\n\n\n\n\n\n\nImportantKey Finding\n\n\n\nEven modest differences in the risk-free rate (50-150 basis points across proxies) can produce terminal value differences of 10-30%. Researchers and practitioners must document their risk-free rate choice explicitly and report sensitivity to alternatives.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#term-structure-considerations",
    "href": "04_risk_free_rate_construction.html#term-structure-considerations",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.7 Term Structure Considerations",
    "text": "5.7 Term Structure Considerations\nWhen longer-horizon discount rates are needed (e.g., for multi-year DCF or cost of capital estimation), the risk-free rate should be maturity-matched. This requires constructing a yield curve from available government bond data.\n\n5.7.1 Yield Curve Estimation\nGürkaynak, Sack, and Wright (2007) develop a parametric approach to yield curve estimation using the Nelson and Siegel (1987) and Svensson (1994) models. The Nelson-Siegel model parameterizes the instantaneous forward rate as:\n\\[\nf(\\tau) = \\beta_0 + \\beta_1 \\exp\\left(-\\frac{\\tau}{\\lambda}\\right) + \\beta_2 \\frac{\\tau}{\\lambda} \\exp\\left(-\\frac{\\tau}{\\lambda}\\right)\n\\tag{5.7}\\]\nwhere \\(\\tau\\) is the maturity, \\(\\beta_0\\) is the long-run level, \\(\\beta_1\\) determines the slope, \\(\\beta_2\\) determines the curvature, and \\(\\lambda\\) controls the location of the hump.\nThe corresponding yield is:\n\\[\ny(\\tau) = \\beta_0 + \\beta_1 \\frac{1 - \\exp(-\\tau/\\lambda)}{\\tau/\\lambda} + \\beta_2 \\left[\\frac{1 - \\exp(-\\tau/\\lambda)}{\\tau/\\lambda} - \\exp(-\\tau/\\lambda)\\right]\n\\tag{5.8}\\]\n\nfrom scipy.optimize import minimize\n\ndef nelson_siegel_yield(tau, beta0, beta1, beta2, lam):\n    \"\"\"Nelson-Siegel yield curve model.\"\"\"\n    tau_lam = tau / lam\n    factor1 = (1 - np.exp(-tau_lam)) / tau_lam\n    factor2 = factor1 - np.exp(-tau_lam)\n    return beta0 + beta1 * factor1 + beta2 * factor2\n\ndef fit_nelson_siegel(maturities, yields):\n    \"\"\"Fit Nelson-Siegel model to observed yields.\"\"\"\n    def objective(params):\n        beta0, beta1, beta2, lam = params\n        if lam &lt;= 0:\n            return 1e10\n        fitted = nelson_siegel_yield(maturities, beta0, beta1, beta2, lam)\n        return np.sum((yields - fitted) ** 2)\n\n    result = minimize(\n        objective,\n        x0=[yields[-1], yields[0] - yields[-1], 0, 2.0],\n        method=\"Nelder-Mead\",\n        options={\"maxiter\": 10000}\n    )\n    return result.x\n\n# Example: fit to latest available government bond yields\n# Assume govt_yields contains: date, maturity_years, yield_pct\ngovt_yields = pd.read_parquet(\"data/govt_bond_yields.parquet\")\n\nlatest_date = govt_yields[\"date\"].max()\nlatest_yields = govt_yields[govt_yields[\"date\"] == latest_date].sort_values(\"maturity_years\")\n\nmaturities = latest_yields[\"maturity_years\"].values\nyields = latest_yields[\"yield_pct\"].values\n\nparams = fit_nelson_siegel(maturities, yields)\nbeta0, beta1, beta2, lam = params\n\n\n\n\ntau_fine = np.linspace(0.25, 30, 200)\nfitted_yields = nelson_siegel_yield(tau_fine, *params)\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.scatter(\n    maturities, yields, color=\"#FF6B6B\", s=60, zorder=5,\n    label=\"Observed yields\", edgecolors=\"white\"\n)\nax.plot(\n    tau_fine, fitted_yields, color=\"#2C73D2\", linewidth=2,\n    label=\"Nelson-Siegel fit\"\n)\nax.set_xlabel(\"Maturity (Years)\")\nax.set_ylabel(\"Yield (%)\")\nax.legend(frameon=False)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Nelson-Siegel parameters:\")\nprint(f\"  β₀ (level)     = {beta0:.4f}\")\nprint(f\"  β₁ (slope)     = {beta1:.4f}\")\nprint(f\"  β₂ (curvature) = {beta2:.4f}\")\nprint(f\"  λ (decay)      = {lam:.4f}\")\n\n\nFigure 5.4\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Nelson-Siegel yield curve allows extraction of a risk-free rate at any maturity. For monthly asset pricing, evaluate the curve at \\(\\tau = 1/12\\) (one month). For DCF valuation with a 10-year horizon, evaluate at \\(\\tau = 10\\). This maturity-matching approach is superior to using a single proxy for all purposes.\n\n\n\n\n5.7.2 Extracting the Short-Rate from the Yield Curve\n\n# Extract 1-month rate from Nelson-Siegel curve at each date\ndef extract_ns_short_rate(govt_yields, target_maturity=1/12):\n    \"\"\"\n    For each date, fit Nelson-Siegel and extract the yield\n    at the target maturity.\n    \"\"\"\n    dates = govt_yields[\"date\"].unique()\n    short_rates = []\n\n    for d in dates:\n        obs = govt_yields[govt_yields[\"date\"] == d].sort_values(\"maturity_years\")\n        if len(obs) &lt; 3:  # Need at least 3 points to fit\n            short_rates.append({\"date\": d, \"rf_ns\": np.nan})\n            continue\n\n        try:\n            params = fit_nelson_siegel(\n                obs[\"maturity_years\"].values,\n                obs[\"yield_pct\"].values\n            )\n            rf_ns = nelson_siegel_yield(target_maturity, *params)\n            short_rates.append({\"date\": d, \"rf_ns\": rf_ns})\n        except Exception:\n            short_rates.append({\"date\": d, \"rf_ns\": np.nan})\n\n    return pd.DataFrame(short_rates)\n\nns_short_rates = extract_ns_short_rate(govt_yields)",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#real-vs.-nominal-risk-free-rates",
    "href": "04_risk_free_rate_construction.html#real-vs.-nominal-risk-free-rates",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.8 Real vs. Nominal Risk-Free Rates",
    "text": "5.8 Real vs. Nominal Risk-Free Rates\nFor certain applications, particularly long-horizon valuation and real return analysis, the real (inflation-adjusted) risk-free rate is more appropriate than the nominal rate. The Fisher equation relates them:\n\\[\nr_f^{real} \\approx r_f^{nominal} - \\pi^{e}\n\\tag{5.9}\\]\nwhere \\(\\pi^e\\) is expected inflation. In practice, we can use realized CPI inflation as a proxy for expected inflation (under the assumption of rational expectations, or as an ex-post adjustment).\n\n# Load CPI data\n# Assume cpi_data contains: date, cpi_index (or inflation_mom for month-over-month)\ncpi_data = pd.read_parquet(\"data/cpi_monthly.parquet\")\ncpi_data = cpi_data.sort_values(\"date\")\ncpi_data[\"inflation_monthly\"] = cpi_data[\"cpi_index\"].pct_change()\n\nrf_real = rf_blended[[\"date\", \"rf_monthly\"]].merge(\n    cpi_data[[\"date\", \"inflation_monthly\"]], on=\"date\", how=\"inner\"\n)\nrf_real[\"rf_real\"] = rf_real[\"rf_monthly\"] - rf_real[\"inflation_monthly\"]\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(\n    rf_real[\"date\"], rf_real[\"rf_monthly\"] * 100,\n    color=\"#2C73D2\", label=\"Nominal\", linewidth=1\n)\nax.plot(\n    rf_real[\"date\"], rf_real[\"rf_real\"] * 100,\n    color=\"#FF6B6B\", label=\"Real\", linewidth=1\n)\nax.axhline(0, color=\"black\", linewidth=0.5, linestyle=\"--\")\nax.set_ylabel(\"Monthly Rate (%)\")\nax.set_xlabel(\"\")\nax.legend(frameon=False)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nplt.tight_layout()\nplt.show()\n\n\nFigure 5.5\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn periods of high inflation, the real risk-free rate can be substantially negative. This has implications for real excess return computation and for interpreting the equity premium in real terms. A negative real risk-free rate implies that nominal government securities do not preserve purchasing power, which strengthens the case for equity investment from a real-return perspective.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#international-comparison",
    "href": "04_risk_free_rate_construction.html#international-comparison",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.9 International Comparison",
    "text": "5.9 International Comparison\nIt is instructive to compare Vietnam’s risk-free rate environment with that of other emerging and developed markets to contextualize the magnitudes involved.\nVietnam’s risk-free rate environment is characterized by relatively high nominal rates (reflecting inflation and growth dynamics), limited availability of very-short-maturity sovereign instruments, and greater reliance on interbank rates as the operational proxy. This is broadly similar to other ASEAN frontier markets but contrasts sharply with developed Asian markets, where deep government securities markets provide clean short-term benchmarks.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#best-practices-checklist",
    "href": "04_risk_free_rate_construction.html#best-practices-checklist",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.10 Best Practices Checklist",
    "text": "5.10 Best Practices Checklist\nBased on the analysis in this chapter, we summarize the recommended practices for risk-free rate construction in Vietnamese financial research:",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#saving-the-risk-free-rate-for-downstream-use",
    "href": "04_risk_free_rate_construction.html#saving-the-risk-free-rate-for-downstream-use",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.11 Saving the Risk-Free Rate for Downstream Use",
    "text": "5.11 Saving the Risk-Free Rate for Downstream Use\nThe final step is to save the constructed risk-free rate series for use in subsequent chapters.\n\n# Save all variants for flexibility\nrf_output = rf_blended[[\"date\", \"rf_monthly\", \"source\"]].copy()\nrf_output[\"rf_annual_pct\"] = rf_output[\"rf_monthly\"] * 12 * 100\n\n# Also save individual proxies\nfor proxy_name, proxy_df in rf_proxies.items():\n    col_name = f\"rf_{proxy_name}\"\n    rf_output = rf_output.merge(\n        proxy_df[[\"date\", \"rf_monthly\"]].rename(\n            columns={\"rf_monthly\": col_name}\n        ),\n        on=\"date\",\n        how=\"left\"\n    )\n\n# Save to parquet for use in later chapters\nrf_output.to_parquet(\"data/risk_free_rate.parquet\", index=False)\n\nprint(f\"Risk-free rate series saved: {len(rf_output)} months\")\nprint(f\"Date range: {rf_output['date'].min()} to {rf_output['date'].max()}\")\nprint(f\"\\nColumns: {list(rf_output.columns)}\")\n\n\n\n\n\n\n\nTip\n\n\n\nBy saving the risk-free rate as a separate, well-documented file, all downstream chapters can merge it consistently. This avoids the common pitfall of reconstructing the risk-free rate differently in different analyses within the same study.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "04_risk_free_rate_construction.html#summary",
    "href": "04_risk_free_rate_construction.html#summary",
    "title": "5  Risk-Free Rate Construction in Vietnam",
    "section": "5.12 Summary",
    "text": "5.12 Summary\nThis chapter has established that risk-free rate construction is a first-order modeling decision in Vietnamese financial research, not a technical afterthought. The key takeaways are:\n\nNo perfect proxy exists in Vietnam. The interbank overnight rate, 1-year government bond yield, and SBV policy rate each have distinct strengths and limitations. A blended series using a documented priority hierarchy provides the most robust baseline.\nThe choice of proxy matters quantitatively. Different proxies can shift the estimated equity premium by 50-200 basis points annually, alter portfolio alphas, and change DCF terminal values by 10-30%.\nFrequency alignment and compounding conventions must be handled with care. Converting annualized rates to monthly requires specifying the compounding convention. Gaps in the data require documented interpolation.\nThe Nelson-Siegel yield curve model enables the extraction of any-maturity risk-free rate from sparse government bond data, which is particularly valuable for maturity-matched discount rate estimation.\nSensitivity analysis is mandatory. Any study that reports results under a single risk-free proxy without reporting robustness to alternatives has an unquantified source of specification uncertainty.\n\n\n\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\nGürkaynak, Refet S, Brian Sack, and Jonathan H Wright. 2007. “The US Treasury Yield Curve: 1961 to the Present.” Journal of Monetary Economics 54 (8): 2291–2304.\n\n\nKrishnamurthy, Arvind, and Annette Vissing-Jorgensen. 2012. “The Aggregate Demand for Treasury Debt.” Journal of Political Economy 120 (2): 233–67.\n\n\nNelson, Charles R, and Andrew F Siegel. 1987. “Parsimonious Modeling of Yield Curves.” Journal of Business, 473–89.\n\n\nSvensson, Lars EO. 1994. “Estimating and Interpreting Forward Interest Rates: Sweden 1992-1994.” National bureau of economic research Cambridge, Mass., USA.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-Free Rate Construction in Vietnam</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html",
    "href": "05_working_with_stock_returns.html",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "",
    "text": "6.1 Data Access and Preparation\nThis chapter develops a practical framework for transforming raw equity price records into return series suitable for empirical financial analysis. The focus is on methodological clarity and reproducibility, with particular attention to data issues that are prevalent in emerging equity markets such as Vietnam.\nThe discussion proceeds from individual stocks to a broad market cross-section, using constituents of the VN30 index as the primary empirical setting.\nWe begin by loading the core numerical and data manipulation libraries. These provide all functionality required for return construction without relying on specialized financial wrappers.\nimport pandas as pd\nimport numpy as np\nFor this project, we retrieve our historical price data using the DataCore API. If you wish to replicate this analysis or use the dataset for your own work, you will need to access the data through their platform.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html#data-access-and-preparation",
    "href": "05_working_with_stock_returns.html#data-access-and-preparation",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "",
    "text": "6.1.1 Prerequisites for API Access\nTo run the code below, you need to configure a few things first:\n\nObtain an API Key: You must subscribe to the relevant dataset on the DataCore platform to receive a unique API key.\nWhitelist Your IP Address: The API requires your IP address to be whitelisted for security.\n\nLocal Machine: If you are running this code on your personal computer, you generally need to whitelist your public IP address.\nCloud or Remote Sessions (e.g., HPC Open OnDemand): If you are using a remote server such as those provided by DataCore, the server’s IP address will change with each new session. You must retrieve the server’s private/public IP for that specific session and whitelist it in your DataCore account settings before running the script.\n\nSet Environment Variables: To keep your credentials secure, do not hardcode your API key into your scripts. Instead, save it as an environment variable named datacore_api on your machine.\n\nNote: If you only want to test the code performance, DataCore provides a preview endpoint that does not require an API key, though the data returned is limited.\n\nimport requests\nimport pandas as pd\n\nurl = \"https://gateway.datacore.vn/data/ds/preview\"\nparams = {\n    \"dataSetCode\": \"fundamental_annual\",\n    \"pageSize\": 10000 \n}\nheaders = {\n    \"Accept\": \"application/json\",\n    \"Origin\": \"https://datacore.vn\",\n    \"Referer\": \"https://datacore.vn/\"\n}\n\nresponse = requests.get(url, params=params, headers=headers)\ndata = response.json()\n\ncolumns = data['data']['fields']\nrows = data['data']['dataDetail']\n\ndf = pd.DataFrame(rows, columns=columns)\nprint(df.head())\nprint(\"Total rows:\", len(df))\n\n  symbol  year  total_current_asset ca_fin        ca_cce       ca_cash  \\\n0    CLL  2011         1.078338e+11   None  8.313178e+10  4.131776e+09   \n1    CLL  2012         2.360575e+10   None  8.003560e+09  4.003560e+09   \n2    CLL  2013         5.764370e+10   None  3.496426e+10  9.964256e+09   \n3    CLL  2014         4.973590e+10   None  1.718744e+10  1.718744e+10   \n4    CLL  2015         2.389115e+11   None  1.790364e+11  2.403638e+10   \n\n  ca_cash_inbank ca_cash_attransit  ca_cash_equivalent  ca_fin_invest  ...  \\\n0           None              None        7.900000e+10   0.000000e+00  ...   \n1           None              None        4.000000e+09   0.000000e+00  ...   \n2           None              None        2.500000e+10   0.000000e+00  ...   \n3           None              None        0.000000e+00   1.000000e+09  ...   \n4           None              None        1.550000e+11   1.000000e+09  ...   \n\n   operating_margin      roe      roa sector_pe sector_pb  sector_ps  \\\n0           0.35473  0.19560  0.10649   3.00213   0.26108    0.22170   \n1           0.45369  0.20337  0.13018   2.40335   0.26211    0.21745   \n2           0.45997  0.23459  0.16452   3.11089   0.41013    0.32436   \n3           0.40554  0.19984  0.14747   3.25886   0.46823    0.42146   \n4           0.33774  0.16525  0.12633   6.77337   0.81110    0.68401   \n\n   sector_eps  sector_ros  sector_roe  sector_roa  \n0  3341.54903     0.07385     0.08753     0.05039  \n1  4296.47005     0.09048     0.11392     0.06346  \n2  4024.74648     0.10427     0.13645     0.07415  \n3  5100.81019     0.12933     0.14956     0.08087  \n4  5216.38499     0.10098     0.11815     0.06234  \n\n[5 rows x 308 columns]\nTotal rows: 10\n\n\n\n\n6.1.2 Checking Your IP Address\nIf you need to verify the IP address of the machine running your code (to whitelist it), you can use the following Python snippets.\nTo find your Public IP:\n\nimport requests\n\ntry:\n    public_ip = requests.get(\"https://api.ipify.org\").text\n    print(f\"Public IP: {public_ip}\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"Could not retrieve Public IP: {e}\")\n\nTo find your Private IP (useful for specific remote server setups):\n\nimport socket\n\ndef get_private_ip():\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect((\"8.8.8.8\", 80))\n        private_ip = s.getsockname()[0]\n        s.close()\n        return private_ip\n    except Exception as e:\n        return f\"Error: {e}\"\n\nprint(f\"Private IP: {get_private_ip()}\")\n\n\n\n6.1.3 Fetching the Dataset\nThe following script demonstrates how to securely authenticate and paginate through the DataCore API to retrieve the full dataset_historical_price dataset.\n\n\n\n# Convert the date column to proper datetime objects\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\n# Ensure price and ratio columns are numeric before calculation\nprices[\"close_price\"] = pd.to_numeric(prices[\"close_price\"])\nprices[\"adj_ratio\"] = pd.to_numeric(prices[\"adj_ratio\"])\n\n# Calculate the adjusted close price\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\n# Rename columns to match standard conventions\nprices = prices.rename(\n    columns={\n        \"vol_total\": \"volume\",\n        \"open_price\": \"open\",\n        \"low_price\": \"low\",\n        \"high_price\": \"high\",\n        \"close_price\": \"close\",\n    }\n)\n\n# Sort the dataset logically by symbol and date\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nprint(\"Data manipulation complete. The dataset is ready for analysis.\")\n\nData manipulation complete. The dataset is ready for analysis.\n\n\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\n\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\n\nprices = prices.rename(\n    columns={\n        \"vol_total\": \"volume\",\n        \"open_price\": \"open\",\n        \"low_price\": \"low\",\n        \"high_price\": \"high\",\n        \"close_price\": \"close\",\n    }\n)\n\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nAdjusted closing prices incorporate mechanical changes due to corporate actions such as cash dividends and stock splits. Using adjusted prices ensures that subsequent return calculations reflect investor-relevant performance rather than accounting artifacts.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html#examining-a-single-equity",
    "href": "05_working_with_stock_returns.html#examining-a-single-equity",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "6.2 Examining a Single Equity",
    "text": "6.2 Examining a Single Equity\nTo ground the discussion, we isolate the trading history of a single large-cap stock, FPT, over a long sample period.\n\nimport datetime as dt\n\nstart = pd.Timestamp(\"2000-01-01\")\nend = pd.Timestamp(dt.date.today().year - 1, 12, 31)\n\n\nfpt = prices.loc[\n    (prices[\"symbol\"] == \"FPT\")\n    & (prices[\"date\"] &gt;= start)\n    & (prices[\"date\"] &lt;= end),\n    [\"date\", \"symbol\", \"volume\", \"open\", \"low\", \"high\", \"close\", \"adjusted_close\"],\n].copy()\n\nThis subset contains the standard daily market variables required for most empirical studies. Before computing returns, it is good practice to visually inspect the price series.\n\nfrom plotnine import ggplot, aes, geom_line, labs\n\n\n(\n    ggplot(fpt, aes(x=\"date\", y=\"adjusted_close\"))\n    + geom_line()\n    + labs(title=\"Adjusted price path of FPT\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 6.1: Prices are in VND, adjusted for dividend payments and stock splits.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html#from-prices-to-returns",
    "href": "05_working_with_stock_returns.html#from-prices-to-returns",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "6.3 From Prices to Returns",
    "text": "6.3 From Prices to Returns\nMost empirical asset pricing models are formulated in terms of returns rather than price levels. The simple daily return is defined as\n\\[\nr_t = \\frac{p_t}{p_t - 1} - 1,\n\\]\nwhere \\(p_t\\) denotes the adjusted closing price at the end of trading day \\(t\\).\nBefore computing returns, we must address invalid price observations. In Vietnamese equity data, adjusted prices occasionally take the value zero. These entries typically arise from IPO placeholders, trading suspensions, or historical backfilling conventions and cannot be used to compute meaningful returns.\n\nprices.loc[prices[\"adjusted_close\"] &lt;= 0, [\"symbol\", \"date\", \"adjusted_close\"]].head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\n\n\n\n\n33886\nADP\n2010-02-09\n0.0\n\n\n33887\nADP\n2010-02-24\n0.0\n\n\n33888\nADP\n2010-03-01\n0.0\n\n\n33889\nADP\n2010-03-03\n0.0\n\n\n33890\nADP\n2010-03-12\n0.0\n\n\n\n\n\n\n\nWe therefore exclude non-positive adjusted prices and compute returns stock by stock. Correct chronological ordering is essential.\n\nreturns = (\n    prices\n    .loc[prices[\"adjusted_close\"] &gt; 0]\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n)\nreturns = returns.dropna(subset=[\"ret\"])\n\nThe initial return for each stock is missing by construction, since no lagged price is available. These observations are mechanical and can safely be removed in most applications.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "href": "05_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "6.4 Limiting the Influence of Extreme Returns",
    "text": "6.4 Limiting the Influence of Extreme Returns\nDaily return series often contain extreme observations driven by data errors, thin trading, or abrupt price adjustments. A common approach is to winsorize returns using cross-sectional percentile cutoffs.\n\ndef winsorize_cs(df, column=\"ret\", lower_q=0.01, upper_q=0.99):\n    lo = df[column].quantile(lower_q)\n    hi = df[column].quantile(upper_q)\n    out = df.copy()\n    out[column] = out[column].clip(lo, hi)\n    return out\n\nreturns = winsorize_cs(returns)\n\nApplying winsorization across the full cross-section limits the impact of extreme market-wide observations while preserving relative differences between firms. Winsorizing within each stock is rarely appropriate in panel settings and can severely distort illiquid securities.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html#distributional-features-of-returns",
    "href": "05_working_with_stock_returns.html#distributional-features-of-returns",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "6.5 Distributional Features of Returns",
    "text": "6.5 Distributional Features of Returns\nWe next examine the empirical distribution of daily returns for FPT. The figure below also marks the historical 5 percent quantile, which provides a simple, non-parametric measure of downside risk.\n\nfrom mizani.formatters import percent_format\nfrom plotnine import geom_histogram, geom_vline, scale_x_continuous\n\n\nfpt_ret = returns.loc[returns[\"symbol\"] == \"FPT\"].copy()\nq05 = fpt_ret[\"ret\"].quantile(0.05)\n\n\n(\n    ggplot(fpt_ret, aes(x=\"ret\"))\n    + geom_histogram(bins=100)\n    + geom_vline(xintercept=q05, linetype=\"dashed\")\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"Distribution of daily FPT returns\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 6.2: The dotted vertical line indicates the historical five percent quantile.\n\n\n\n\n\nSummary statistics offer a compact description of return behavior and should always be inspected before formal modeling.\n\nreturns[\"ret\"].describe().round(3)\n\ncount    4305063.000\nmean           0.000\nstd            0.035\nmin           -0.125\n25%           -0.004\n50%            0.000\n75%            0.003\nmax            0.130\nName: ret, dtype: float64\n\n\nComputing these statistics by calendar year can reveal periods of elevated volatility or structural change.\n\n(\n    returns\n    .assign(year=lambda x: x[\"date\"].dt.year)\n    .groupby(\"year\")[\"ret\"]\n    .describe()\n    .round(3)\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n2010\n131548.0\n-0.001\n0.036\n-0.125\n-0.021\n0.0\n0.018\n0.13\n\n\n2011\n166826.0\n-0.003\n0.033\n-0.125\n-0.020\n0.0\n0.011\n0.13\n\n\n2012\n177938.0\n0.000\n0.033\n-0.125\n-0.012\n0.0\n0.015\n0.13\n\n\n2013\n180417.0\n0.001\n0.033\n-0.125\n-0.004\n0.0\n0.008\n0.13\n\n\n2014\n181907.0\n0.001\n0.034\n-0.125\n-0.008\n0.0\n0.011\n0.13\n\n\n2015\n197881.0\n0.000\n0.033\n-0.125\n-0.006\n0.0\n0.005\n0.13\n\n\n2016\n227896.0\n0.000\n0.035\n-0.125\n-0.005\n0.0\n0.003\n0.13\n\n\n2017\n283642.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.001\n0.13\n\n\n2018\n329887.0\n0.000\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2019\n352754.0\n0.000\n0.033\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2020\n369367.0\n0.001\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2021\n379415.0\n0.002\n0.038\n-0.125\n-0.005\n0.0\n0.007\n0.13\n\n\n2022\n387050.0\n-0.001\n0.038\n-0.125\n-0.008\n0.0\n0.004\n0.13\n\n\n2023\n391605.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.002\n0.13\n\n\n2024\n400379.0\n0.000\n0.031\n-0.125\n-0.002\n0.0\n0.000\n0.13\n\n\n2025\n146551.0\n0.000\n0.037\n-0.125\n-0.004\n0.0\n0.002\n0.13",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "href": "05_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "6.6 Expanding to a Market Cross-Section",
    "text": "6.6 Expanding to a Market Cross-Section\nThe same procedures apply naturally to a larger universe of stocks. We now restrict attention to the constituents of the VN30 index.\n\nvn30 = [\n    \"ACB\",\"BCM\",\"BID\",\"BVH\",\"CTG\",\"FPT\",\"GAS\",\"GVR\",\"HDB\",\"HPG\",\n    \"MBB\",\"MSN\",\"MWG\",\"PLX\",\"POW\",\"SAB\",\"SHB\",\"SSB\",\"STB\",\"TCB\",\n    \"TPB\",\"VCB\",\"VHM\",\"VIB\",\"VIC\",\"VJC\",\"VNM\",\"VPB\",\"VRE\",\"EIB\",\n]\n\n\nprices_vn30 = prices.loc[prices[\"symbol\"].isin(vn30)]\nfrom plotnine import theme\n\n\n(\n    ggplot(prices_vn30, aes(x=\"date\", y=\"adjusted_close\", color=\"symbol\"))\n    + geom_line()\n    + labs(title=\"Adjusted prices of VN30 constituents\", x=\"\", y=\"\")\n    + theme(legend_position=\"none\")\n)\n\n\n\n\n\n\n\nFigure 6.3: Prices in VND, adjusted for dividend payments and stock splits.\n\n\n\n\n\nReturns for the VN30 universe are computed analogously.\n\nreturns_vn30 = (\n    prices_vn30\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n    .dropna()\n)\n\n\nreturns_vn30.groupby(\"symbol\")[\"ret\"].describe().round(3)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\n\n\nACB\n3822.0\n-0.000\n0.023\n-0.407\n-0.006\n0.0\n0.007\n0.097\n\n\nBCM\n1795.0\n0.001\n0.027\n-0.136\n-0.010\n0.0\n0.010\n0.159\n\n\nBID\n2811.0\n0.000\n0.024\n-0.369\n-0.010\n0.0\n0.011\n0.070\n\n\nBVH\n3825.0\n0.000\n0.024\n-0.097\n-0.012\n0.0\n0.012\n0.070\n\n\nCTG\n3825.0\n0.000\n0.024\n-0.376\n-0.010\n0.0\n0.010\n0.070\n\n\nEIB\n3825.0\n-0.000\n0.022\n-0.302\n-0.008\n0.0\n0.008\n0.070\n\n\nFPT\n3825.0\n-0.000\n0.024\n-0.439\n-0.008\n0.0\n0.009\n0.070\n\n\nGAS\n3236.0\n0.000\n0.022\n-0.289\n-0.009\n0.0\n0.010\n0.070\n\n\nGVR\n1775.0\n0.001\n0.030\n-0.137\n-0.014\n0.0\n0.016\n0.169\n\n\nHDB\n1828.0\n-0.001\n0.028\n-0.391\n-0.009\n0.0\n0.010\n0.070\n\n\nHPG\n3825.0\n-0.001\n0.032\n-0.581\n-0.010\n0.0\n0.011\n0.070\n\n\nMBB\n3371.0\n-0.000\n0.023\n-0.473\n-0.008\n0.0\n0.008\n0.069\n\n\nMSN\n3825.0\n0.000\n0.024\n-0.553\n-0.010\n0.0\n0.010\n0.070\n\n\nMWG\n2701.0\n-0.000\n0.035\n-0.751\n-0.009\n0.0\n0.011\n0.070\n\n\nPLX\n2009.0\n-0.000\n0.021\n-0.140\n-0.010\n0.0\n0.010\n0.070\n\n\nPOW\n1784.0\n0.000\n0.023\n-0.071\n-0.012\n0.0\n0.011\n0.102\n\n\nSAB\n2100.0\n-0.000\n0.024\n-0.745\n-0.008\n0.0\n0.007\n0.070\n\n\nSHB\n3824.0\n-0.000\n0.028\n-0.338\n-0.013\n0.0\n0.013\n0.100\n\n\nSSB\n1029.0\n-0.000\n0.023\n-0.292\n-0.005\n0.0\n0.004\n0.070\n\n\nSTB\n3825.0\n0.000\n0.024\n-0.321\n-0.010\n0.0\n0.010\n0.070\n\n\nTCB\n1732.0\n-0.000\n0.035\n-0.884\n-0.009\n0.0\n0.010\n0.070\n\n\nTPB\n1761.0\n-0.001\n0.029\n-0.477\n-0.009\n0.0\n0.009\n0.070\n\n\nVCB\n3825.0\n-0.000\n0.024\n-0.539\n-0.009\n0.0\n0.009\n0.070\n\n\nVHM\n1744.0\n-0.000\n0.024\n-0.419\n-0.009\n0.0\n0.008\n0.070\n\n\nVIB\n2072.0\n-0.000\n0.031\n-0.489\n-0.009\n0.0\n0.010\n0.109\n\n\nVIC\n3825.0\n-0.000\n0.027\n-0.673\n-0.008\n0.0\n0.008\n0.070\n\n\nVJC\n2046.0\n-0.000\n0.020\n-0.455\n-0.007\n0.0\n0.006\n0.070\n\n\nVNM\n3825.0\n-0.000\n0.023\n-0.547\n-0.007\n0.0\n0.007\n0.070\n\n\nVPB\n1927.0\n-0.000\n0.033\n-0.678\n-0.010\n0.0\n0.010\n0.070\n\n\nVRE\n1871.0\n-0.000\n0.024\n-0.295\n-0.012\n0.0\n0.011\n0.070",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html#aggregating-returns-across-time",
    "href": "05_working_with_stock_returns.html#aggregating-returns-across-time",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "6.7 Aggregating Returns Across Time",
    "text": "6.7 Aggregating Returns Across Time\nFinancial variables are observed at different frequencies. While equity prices are recorded daily, many empirical questions require monthly or annual returns. Lower-frequency returns are constructed by compounding higher-frequency observations.\n\nreturns_monthly = (\n    returns_vn30\n    .assign(month=lambda x: x[\"date\"].dt.to_period(\"M\").dt.to_timestamp())\n    .groupby([\"symbol\", \"month\"], as_index=False)\n    .agg(ret=(\"ret\", lambda x: np.prod(1 + x) - 1))\n)\n\nComparing daily and monthly return distributions illustrates how aggregation dampens volatility and alters tail behavior.\n\nfrom plotnine import facet_wrap\n\nfpt_d = returns_vn30.loc[returns_vn30[\"symbol\"] == \"FPT\"].assign(freq=\"Daily\")\nfpt_m = returns_monthly.loc[returns_monthly[\"symbol\"] == \"FPT\"].assign(freq=\"Monthly\")\n\n\nfpt_both = pd.concat([\n    fpt_d[[\"ret\", \"freq\"]],\n    fpt_m[[\"ret\", \"freq\"]],\n])\n\n\n(\n    ggplot(fpt_both, aes(x=\"ret\"))\n    + geom_histogram(bins=50)\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"FPT returns at different frequencies\", x=\"\", y=\"\")\n    + facet_wrap(\"freq\", scales=\"free\")\n)\n\n\n\n\n\n\n\nFigure 6.4: Returns are based on prices adjusted for dividend payments and stock splits.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "href": "05_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "6.8 Aggregation Across Firms: Trading Activity",
    "text": "6.8 Aggregation Across Firms: Trading Activity\nAggregation is not limited to time. In some settings, it is informative to aggregate variables across firms. As an illustration, we compute total daily trading value for VN30 stocks by multiplying share volume by adjusted prices and summing across firms.\n\ntrading_value = (\n    prices_vn30\n    .assign(value=lambda x: x[\"volume\"] * x[\"adjusted_close\"] / 1e9)\n    .groupby(\"date\")[\"value\"]\n    .sum()\n    .reset_index()\n    .assign(value_lag=lambda x: x[\"value\"].shift(1))\n)\n(\n    ggplot(trading_value, aes(x=\"date\", y=\"value\"))\n    + geom_line()\n    + labs(title=\"Aggregate VN30 trading value (billion VND)\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\n\nFinally, we assess persistence in trading activity by comparing trading value on consecutive days.\n\nfrom plotnine import geom_point, geom_abline\n\n\n(\n    ggplot(trading_value, aes(x=\"value_lag\", y=\"value\"))\n    + geom_point()\n    + geom_abline(intercept=0, slope=1, linetype=\"dashed\")\n    + labs(\n        title=\"Persistence in VN30 trading value\",\n        x=\"Previous day\",\n        y=\"Current day\",\n    )\n)\n\n/home/mikenguyen/project/tidyfinance/.venv/lib/python3.13/site-packages/plotnine/layer.py:374: PlotnineWarning: geom_point : Removed 1 rows containing missing values.\n\n\n\n\n\n\n\n\nFigure 6.5: Total daily trading volume.\n\n\n\n\n\nA strong alignment along the 45-degree line indicates that high-activity trading days tend to be followed by similarly active days, a common empirical regularity in equity markets.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "05_working_with_stock_returns.html#summary",
    "href": "05_working_with_stock_returns.html#summary",
    "title": "6  Constructing and Analyzing Equity Return Series",
    "section": "6.9 Summary",
    "text": "6.9 Summary\nThis chapter established a reproducible workflow for transforming raw price data into return series, diagnosing common data issues, and aggregating information across time and firms. These steps provide the empirical foundation for subsequent analyses of risk, return predictability, and market dynamics in Vietnam’s equity market.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html",
    "href": "06_compound_return.html",
    "title": "7  Compound Returns",
    "section": "",
    "text": "7.1 Simple Returns versus Log Returns\nIn this chapter, we provide a treatment of compound returns. Whether constructing buy-and-hold portfolios, evaluating fund performance, computing cumulative wealth indices, or estimating long-horizon risk measures, the ability to correctly compound returns over arbitrary horizons is indispensable. We begin with the mathematical foundations: the distinction between simple and log returns, the relationship between arithmetic and geometric means, and the properties of continuously compounded returns. Along the way, we address practical complications that arise in real-world equity data, such as trading halts, price limit mechanisms, partial-period returns, and delisting events, and show how to handle them in the Vietnamese context.\nThe chapter proceeds to rolling compound returns over standard horizons (3, 6, 9, and 12 months), compound returns aligned to fiscal period ends, forward-looking cumulative returns for event studies, and rolling volatility estimation.\nBefore discussing compounding, we must distinguish between the two fundamental return conventions used in finance.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#simple-returns-versus-log-returns",
    "href": "06_compound_return.html#simple-returns-versus-log-returns",
    "title": "7  Compound Returns",
    "section": "",
    "text": "7.1.1 Simple (Arithmetic) Returns\nThe simple gross return on an asset from period \\(t-1\\) to \\(t\\) is defined as\n\\[\n1 + R_t = \\frac{P_t + D_t}{P_{t-1}},\n\\tag{7.1}\\]\nwhere \\(P_t\\) denotes the price at the end of period \\(t\\) and \\(D_t\\) denotes any cash distributions (dividends, coupons) paid during period \\(t\\). The simple net return is \\(R_t\\) itself. When we speak of “returns” without qualification, we typically mean simple net returns.\nThe key property of simple returns is that multi-period compounding is multiplicative:\n\\[\n1 + R_t(k) = \\prod_{j=0}^{k-1} (1 + R_{t-j}) = (1 + R_t)(1 + R_{t-1}) \\cdots (1 + R_{t-k+1}),\n\\tag{7.2}\\]\nwhere \\(R_t(k)\\) is the \\(k\\)-period compound return ending at time \\(t\\). This multiplicative structure is the foundation of all compounding methods discussed in this chapter.\n\n\n7.1.2 Continuously Compounded (Log) Returns\nThe continuously compounded return, or log return, is defined as\n\\[\nr_t = \\ln(1 + R_t) = \\ln\\!\\left(\\frac{P_t + D_t}{P_{t-1}}\\right).\n\\tag{7.3}\\]\nThe central advantage of log returns for compounding is that multi-period compounding becomes additive:\n\\[\nr_t(k) = \\ln(1 + R_t(k)) = \\sum_{j=0}^{k-1} r_{t-j} = r_t + r_{t-1} + \\cdots + r_{t-k+1}.\n\\tag{7.4}\\]\nThis additive property follows directly from the logarithmic identity \\(\\ln(ab) = \\ln(a) + \\ln(b)\\). It is computationally convenient because summation is numerically more stable than iterated multiplication, and because many statistical procedures (means, variances, regressions) operate naturally on additive quantities.\nTo recover the simple compound return from the sum of log returns, we apply the exponential function:\n\\[\nR_t(k) = \\exp\\!\\left(\\sum_{j=0}^{k-1} r_{t-j}\\right) - 1.\n\\tag{7.5}\\]\n\n\n7.1.3 When Do They Diverge?\nFor small returns, the approximation \\(r_t \\approx R_t\\) holds to first order (via the Taylor expansion \\(\\ln(1+x) \\approx x\\) for \\(|x| \\ll 1\\)). However, for large returns, which is common in emerging markets, small-cap stocks, or crisis periods, the two can diverge substantially. Consider a stock that doubles in price (\\(R_t = 1.0\\)): the log return is \\(r_t = \\ln(2) \\approx 0.693\\), a 31% discrepancy. Conversely, for a stock that loses half its value (\\(R_t = -0.5\\)): the log return is \\(r_t = \\ln(0.5) \\approx -0.693\\), which is 39% larger in magnitude.\nThis divergence is especially relevant in Vietnam, where daily price limits of \\(\\pm 7\\%\\) on HOSE, \\(\\pm 10\\%\\) on HNX, and \\(\\pm 15\\%\\) on UPCoM can produce sequences of limit-up or limit-down days. Over a week of consecutive limit-up days on HOSE, the simple return is \\((1.07)^5 - 1 = 40.3\\%\\) while the log return is \\(5 \\times \\ln(1.07) = 33.8\\%\\), which is a meaningful gap.\nTable 7.1 illustrates this divergence across a range of return magnitudes.\n\nsimple_returns = [-0.50, -0.30, -0.15, -0.10, -0.07, -0.05, -0.01,\n                  0.00, 0.01, 0.05, 0.07, 0.10, 0.15, 0.30, 0.50, 1.00]\ncomparison_df = pd.DataFrame({\n    \"Simple Return\": [f\"{r:.2%}\" for r in simple_returns],\n    \"Log Return\": [f\"{np.log(1+r):.4f}\" for r in simple_returns],\n    \"Difference\": [f\"{np.log(1+r) - r:.4f}\" for r in simple_returns],\n    \"Relative Error (%)\": [\n        f\"{((np.log(1+r) - r) / abs(r) * 100):.2f}\" if r != 0 else \"—\"\n        for r in simple_returns\n    ]\n})\ncomparison_df\n\n\n\nTable 7.1: Comparison of simple and log returns for various price changes. The divergence grows with the magnitude of the simple return, which is particularly relevant for volatile emerging market stocks.\n\n\n\n\n\n\n\n\n\n\nSimple Return\nLog Return\nDifference\nRelative Error (%)\n\n\n\n\n0\n-50.00%\n-0.6931\n-0.1931\n-38.63\n\n\n1\n-30.00%\n-0.3567\n-0.0567\n-18.89\n\n\n2\n-15.00%\n-0.1625\n-0.0125\n-8.35\n\n\n3\n-10.00%\n-0.1054\n-0.0054\n-5.36\n\n\n4\n-7.00%\n-0.0726\n-0.0026\n-3.67\n\n\n5\n-5.00%\n-0.0513\n-0.0013\n-2.59\n\n\n6\n-1.00%\n-0.0101\n-0.0001\n-0.50\n\n\n7\n0.00%\n0.0000\n0.0000\n—\n\n\n8\n1.00%\n0.0100\n-0.0000\n-0.50\n\n\n9\n5.00%\n0.0488\n-0.0012\n-2.42\n\n\n10\n7.00%\n0.0677\n-0.0023\n-3.34\n\n\n11\n10.00%\n0.0953\n-0.0047\n-4.69\n\n\n12\n15.00%\n0.1398\n-0.0102\n-6.83\n\n\n13\n30.00%\n0.2624\n-0.0376\n-12.55\n\n\n14\n50.00%\n0.4055\n-0.0945\n-18.91\n\n\n15\n100.00%\n0.6931\n-0.3069\n-30.69\n\n\n\n\n\n\n\n\n\n\n\nKey takeaway: log returns are convenient for compounding (additive aggregation), but portfolio returns aggregate cross-sectionally in simple return space. In practice, we often transform to log returns for temporal compounding, then convert back to simple returns for reporting.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#mathematical-foundations-of-compounding",
    "href": "06_compound_return.html#mathematical-foundations-of-compounding",
    "title": "7  Compound Returns",
    "section": "7.2 Mathematical Foundations of Compounding",
    "text": "7.2 Mathematical Foundations of Compounding\n\n7.2.1 Geometric Mean Return\nThe geometric mean return over \\(T\\) periods is\n\\[\n\\bar{R}_g = \\left(\\prod_{t=1}^{T} (1 + R_t)\\right)^{1/T} - 1,\n\\tag{7.6}\\]\nwhich represents the constant per-period return that would yield the same terminal wealth as the actual return sequence. It is always less than or equal to the arithmetic mean \\(\\bar{R}_a = \\frac{1}{T}\\sum_{t=1}^{T} R_t\\), with equality only when all returns are identical. The relationship between the two is approximately:\n\\[\n\\bar{R}_g \\approx \\bar{R}_a - \\frac{\\sigma^2}{2},\n\\tag{7.7}\\]\nwhere \\(\\sigma^2\\) is the variance of returns. This approximation, sometimes called the “volatility drag,” has important implications: high-volatility assets have a larger wedge between their arithmetic and geometric means, meaning their actual compound growth understates what a naive average would suggest. In a market like Vietnam’s, where individual stock volatility is often two to three times that of developed-market equities, the volatility drag can be substantial.\n\n\n7.2.2 Wealth Index and Drawdowns\nGiven an initial investment of \\(W_0\\), the wealth at time \\(T\\) is\n\\[\nW_T = W_0 \\prod_{t=1}^{T} (1 + R_t).\n\\tag{7.8}\\]\nThe cumulative return (net) is simply \\(W_T / W_0 - 1\\). The maximum drawdown, a widely used risk measure, is defined as\n\\[\n\\text{MDD} = \\max_{0 \\le s \\le t \\le T} \\left(\\frac{W_s - W_t}{W_s}\\right),\n\\tag{7.9}\\]\nwhich measures the largest peak-to-trough decline in the wealth index. We will compute this quantity alongside compound returns below. Drawdowns are particularly informative in emerging markets that experience sharp corrections, as occurred during the global financial crisis of 2008 when the VN-Index fell roughly 66% from its 2007 peak.\n\n\n\n7.2.3 Annualization\nFor a \\(k\\)-period compound return \\(R_t(k)\\) where each period has length \\(\\Delta\\) (e.g., \\(\\Delta = 1/12\\) for monthly data), the annualized return is\n\\[\nR_{\\text{ann}} = (1 + R_t(k))^{1/(k\\Delta)} - 1.\n\\tag{7.10}\\]\nSimilarly, for volatility estimated from \\(k\\)-period returns with period length \\(\\Delta\\):\n\\[\n\\sigma_{\\text{ann}} = \\sigma / \\sqrt{\\Delta},\n\\tag{7.11}\\]\nso monthly volatility is annualized by multiplying by \\(\\sqrt{12}\\) and daily volatility by approximately \\(\\sqrt{252}\\) (assuming 252 trading days per year). For Vietnam specifically, the HOSE typically has around 245–250 trading days per year after accounting for Vietnamese public holidays, which is close enough that the \\(\\sqrt{252}\\) convention is standard.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#data-preparation",
    "href": "06_compound_return.html#data-preparation",
    "title": "7  Compound Returns",
    "section": "7.3 Data Preparation",
    "text": "7.3 Data Preparation\nWe start by loading monthly stock return data from our SQLite database. As prepared in previous chapters, this database contains monthly returns sourced from DataCore.vn for all securities listed on the Ho Chi Minh Stock Exchange (HOSE), Hanoi Stock Exchange (HNX), and the Unlisted Public Company Market (UPCoM). Returns are adjusted for stock splits, bonus issues, and rights offerings, and include reinvested cash dividends.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, ret, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n).dropna()\n\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nprices_monthly = prices_monthly.merge(\n    factors_ff3_monthly,\n    on=\"date\",\n    how=\"left\"\n)\n\nprices_monthly[\"ret_total\"] = prices_monthly[\"ret\"]\nprices_monthly[\"mkt_total\"] = (\n    prices_monthly[\"mkt_excess\"] + prices_monthly[\"risk_free\"]\n)\n\nLet us inspect the sample:\n\nprint(f\"Sample period: {prices_monthly['date'].min()} to \"\n      f\"{prices_monthly['date'].max()}\")\nprint(f\"Number of stocks: {prices_monthly['symbol'].nunique():,}\")\nprint(f\"Total observations: {len(prices_monthly):,}\")\n# print(f\"Exchanges: {prices_monthly['exchange'].unique()}\")\n\nSample period: 2010-02-28 00:00:00 to 2023-12-31 00:00:00\nNumber of stocks: 1,457\nTotal observations: 165,499\n\n\nTable 7.2 provides summary statistics for the raw monthly returns, broken down by exchange. Differences across exchanges reflect the size and liquidity gradient: HOSE lists the largest and most liquid firms, HNX covers mid-cap companies, and UPCoM hosts smaller and more thinly traded securities.\n\n\n\nTable 7.2: Summary statistics of monthly stock returns by exchange. HOSE firms tend to have lower return dispersion and fewer extreme observations compared to HNX and UPCoM, consistent with their larger market capitalization and greater liquidity.\n\n\nsample_stats = (\n    prices_monthly\n    .groupby(\"exchange\")[\"ret_total\"]\n    .describe(percentiles=[0.05, 0.25, 0.50, 0.75, 0.95])\n    .round(4)\n)\nsample_stats",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#method-1-cumulative-product-via-groupby",
    "href": "06_compound_return.html#method-1-cumulative-product-via-groupby",
    "title": "7  Compound Returns",
    "section": "7.4 Method 1: Cumulative Product via GroupBy",
    "text": "7.4 Method 1: Cumulative Product via GroupBy\nThe most direct approach to compound returns uses the multiplicative property in Equation 7.2. For each security, we compute the cumulative product of gross returns \\((1 + R_t)\\) over the desired window.\n\ndef compute_cumret_cumprod(df, ret_col=\"ret_total\",\n                           group_col=\"symbol\"):\n    \"\"\"Compute cumulative returns using cumulative product.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain `group_col`, 'date', and `ret_col`.\n    ret_col : str\n        Column name for period returns.\n    group_col : str\n        Column name for grouping (e.g., security identifier).\n\n    Returns\n    -------\n    pd.DataFrame\n        Original DataFrame augmented with 'cumret' and 'wealth_index'.\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[ret_col]\n    df[\"wealth_index\"] = (\n        df.groupby(group_col)[\"gross_ret\"]\n        .cumprod()\n    )\n    df[\"cumret\"] = df[\"wealth_index\"] - 1\n    df.drop(columns=[\"gross_ret\"], inplace=True)\n    return df\n\nLet us apply this to the full sample and examine the resulting wealth indices for a few selected stocks:\n\nstock_cumret = compute_cumret_cumprod(prices_monthly)\n\n# Select stocks with long histories for illustration\nstock_counts = (\n    stock_cumret.groupby(\"symbol\")[\"date\"]\n    .count()\n    .reset_index(name=\"n_obs\")\n)\nlong_history_stocks = (\n    stock_counts.nlargest(5, \"n_obs\")[\"symbol\"].tolist()\n)\n\nsample_wealth = stock_cumret[\n    stock_cumret[\"symbol\"].isin(long_history_stocks)\n]\n\nFigure 7.1 plots the wealth indices (value of 1 VND invested) for these five securities over the full sample period.\n\nplot_wealth = (\n    ggplot(sample_wealth, aes(x=\"date\", y=\"wealth_index\",\n                              color=\"factor(symbol)\")) +\n    geom_line(size=0.6) +\n    labs(\n        x=\"\", y=\"Wealth index (1 VND invested)\",\n        color=\"Stock\"\n    ) +\n    theme_minimal() +\n    theme(legend_position=\"bottom\",\n          figure_size=(10, 5))\n)\nplot_wealth.draw()\n\n\n\n\n\n\n\nFigure 7.1: Wealth index (value of 1 VND invested) for selected long-history Vietnamese stocks. Each line represents the cumulative value of a 1 VND investment in a single stock, with all dividends reinvested. The divergence in terminal wealth illustrates the power of compounding over long horizons.\n\n\n\n\n\n\n7.4.1 Handling Missing Returns\nThe cumulative product approach propagates missing values: if any \\(R_t\\) is NaN, the entire cumulative product from that point onward becomes NaN. This is conservative because it effectively assumes that a missing return renders the subsequent wealth index undefined. In many applications, this is the desired behavior because a missing return may indicate a data error or a period during which the stock was not trading.\nHowever, in the Vietnamese market, missing returns can arise from extended trading halts. The State Securities Commission (SSC) and exchanges may suspend trading in a stock for various regulatory reasons, such as financial reporting delays, pending corporate restructuring announcements, or suspected market manipulation. These halts can last days, weeks, or even months. During such halts, the stock’s value has not changed (the last traded price remains the reference), so treating the missing return as zero (i.e., no price change) may be more appropriate than propagating NaN.\n\ndef compute_cumret_skipna(df, ret_col=\"ret_total\",\n                          group_col=\"symbol\"):\n    \"\"\"Compute cumulative returns, treating missing returns as zero.\"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[ret_col].fillna(0)\n    df[\"wealth_index\"] = (\n        df.groupby(group_col)[\"gross_ret\"]\n        .cumprod()\n    )\n    df[\"cumret\"] = df[\"wealth_index\"] - 1\n    df.drop(columns=[\"gross_ret\"], inplace=True)\n    return df\n\n\n\n\n\n\n\nWarning\n\n\n\nTreating missing returns as zero is an assumption that may or may not be appropriate. If returns are missing because the stock was halted, zero may be reasonable. If returns are missing due to data errors or because the stock was genuinely not trading (e.g., awaiting relisting after a corporate event), imputing zero can introduce bias. Always investigate the reason for missing values before deciding on a treatment.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#method-2-log-sum-exp-approach",
    "href": "06_compound_return.html#method-2-log-sum-exp-approach",
    "title": "7  Compound Returns",
    "section": "7.5 Method 2: Log-Sum-Exp Approach",
    "text": "7.5 Method 2: Log-Sum-Exp Approach\nThe log-sum-exp method exploits the additive property of log returns (Equation 7.4). This approach is particularly useful when computing compound returns over fixed windows (e.g., annual returns from monthly data) because summation is both computationally efficient and numerically stable.\n\ndef compute_cumret_logsum(df, ret_col=\"ret_total\",\n                          group_col=\"symbol\",\n                          date_col=\"date\"):\n    \"\"\"Compute cumulative returns using the log-sum-exp approach.\n\n    Steps:\n    1. Transform to log returns: r_t = ln(1 + R_t)\n    2. Cumulative sum of log returns within each group\n    3. Exponentiate to recover simple cumulative return\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    ret_col : str\n    group_col : str\n    date_col : str\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    df = df.sort_values([group_col, date_col]).copy()\n    df[\"log_ret\"] = np.log(1 + df[ret_col])\n    df[\"cum_log_ret\"] = (\n        df.groupby(group_col)[\"log_ret\"].cumsum()\n    )\n    df[\"wealth_index_log\"] = np.exp(df[\"cum_log_ret\"])\n    df[\"cumret_log\"] = df[\"wealth_index_log\"] - 1\n    df.drop(columns=[\"log_ret\", \"cum_log_ret\"], inplace=True)\n    return df\n\nLet us verify that the two methods produce identical results (up to floating-point precision):\n\nstock_both = compute_cumret_cumprod(prices_monthly)\nstock_both = compute_cumret_logsum(stock_both)\n\n# Compare on non-missing observations\nmask = (stock_both[\"cumret\"].notna()\n        & stock_both[\"cumret_log\"].notna())\nmax_diff = (stock_both.loc[mask, \"cumret\"] -\n            stock_both.loc[mask, \"cumret_log\"]).abs().max()\nprint(f\"Maximum absolute difference between methods: {max_diff:.2e}\")\n\nMaximum absolute difference between methods: 1.78e-14\n\n\nThe difference is at the level of machine epsilon (\\(\\approx 10^{-15}\\)), confirming numerical equivalence.\n\n7.5.1 Period-Specific Compound Returns\nA common task is to compute compound returns within calendar periods (months, quarters, years). The log-sum-exp approach lends itself naturally to grouped aggregation:\n\ndef compound_return_by_period(df, ret_col=\"ret_total\",\n                              group_col=\"symbol\",\n                              period=\"year\"):\n    \"\"\"Compute compound returns within calendar periods.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain 'date' and `ret_col`.\n    period : str\n        One of 'year', 'quarter', 'month'.\n\n    Returns\n    -------\n    pd.DataFrame with compound returns per group-period.\n    \"\"\"\n    df = df.copy()\n    df[\"log_ret\"] = np.log(1 + df[ret_col])\n    if period == \"year\":\n        df[\"period\"] = df[\"date\"].dt.year\n    elif period == \"quarter\":\n        df[\"period\"] = df[\"date\"].dt.to_period(\"Q\")\n    elif period == \"month\":\n        df[\"period\"] = df[\"date\"].dt.to_period(\"M\")\n\n    result = (\n        df.groupby([group_col, \"period\"])\n        .agg(\n            cumret=(\n                \"log_ret\",\n                lambda x: np.exp(x.sum()) - 1\n            ),\n            n_obs=(\"log_ret\", \"count\"),\n            n_miss=(ret_col, lambda x: x.isna().sum()),\n            start_date=(\"date\", \"min\"),\n            end_date=(\"date\", \"max\")\n        )\n        .reset_index()\n    )\n    return result\n\nTable 7.3 shows annual compound returns for a subset of securities.\n\nannual_returns = compound_return_by_period(\n    prices_monthly[\n        prices_monthly[\"symbol\"].isin(long_history_stocks)\n    ],\n    period=\"year\"\n)\n\nrecent_annual = (\n    annual_returns\n    .sort_values([\"symbol\", \"period\"])\n    .groupby(\"symbol\")\n    .tail(5)\n    .round(4)\n)\nrecent_annual.head(20)\n\n/tmp/ipykernel_2229780/2619242959.py:13: UserWarning: obj.round has no effect with datetime, timedelta, or period dtypes. Use obj.dt.round(...) instead.\n\n\n\n\nTable 7.3: Annual compound returns for selected Vietnamese securities. The number of non-missing monthly observations (n_obs) and missing observations (n_miss) are reported to flag potentially incomplete years. A stock-year with n_obs substantially below 12 indicates either partial listing or extended trading halts.\n\n\n\n\n\n\n\n\n\n\nsymbol\nperiod\ncumret\nn_obs\nn_miss\nstart_date\nend_date\n\n\n\n\n9\nAAM\n2019\n-0.2810\n12\n0\n2019-01-31\n2019-12-31\n\n\n10\nAAM\n2020\n-0.1622\n12\n0\n2020-01-31\n2020-12-31\n\n\n11\nAAM\n2021\n0.1250\n12\n0\n2021-01-31\n2021-12-31\n\n\n12\nAAM\n2022\n-0.0913\n12\n0\n2022-01-31\n2022-12-31\n\n\n13\nAAM\n2023\n-0.2337\n12\n0\n2023-01-31\n2023-12-31\n\n\n23\nABI\n2019\n0.1946\n12\n0\n2019-01-31\n2019-12-31\n\n\n24\nABI\n2020\n0.2418\n12\n0\n2020-01-31\n2020-12-31\n\n\n25\nABI\n2021\n0.2896\n12\n0\n2021-01-31\n2021-12-31\n\n\n26\nABI\n2022\n-0.5085\n12\n0\n2022-01-31\n2022-12-31\n\n\n27\nABI\n2023\n-0.5042\n12\n0\n2023-01-31\n2023-12-31\n\n\n37\nABT\n2019\n-0.1893\n12\n0\n2019-01-31\n2019-12-31\n\n\n38\nABT\n2020\n-0.1400\n12\n0\n2020-01-31\n2020-12-31\n\n\n39\nABT\n2021\n0.0842\n12\n0\n2021-01-31\n2021-12-31\n\n\n40\nABT\n2022\n-0.0789\n12\n0\n2022-01-31\n2022-12-31\n\n\n41\nABT\n2023\n-0.0768\n12\n0\n2023-01-31\n2023-12-31\n\n\n51\nACC\n2019\n-0.1875\n12\n0\n2019-01-31\n2019-12-31\n\n\n52\nACC\n2020\n-0.4923\n12\n0\n2020-01-31\n2020-12-31\n\n\n53\nACC\n2021\n1.2339\n12\n0\n2021-01-31\n2021-12-31\n\n\n54\nACC\n2022\n-0.8538\n12\n0\n2022-01-31\n2022-12-31\n\n\n55\nACC\n2023\n0.1027\n12\n0\n2023-01-31\n2023-12-31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen the number of non-missing observations (n_obs) is less than 12 for an annual return, the compound return represents only a partial year. This commonly occurs in the first and last years of a security’s listing on HOSE, HNX, or UPCoM, or when a stock transfers between exchanges (e.g., from UPCoM to HOSE upon meeting listing requirements). Users should decide whether to retain or exclude such partial-year observations depending on their research design.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#method-3-iterative-compounding-with-retain-logic",
    "href": "06_compound_return.html#method-3-iterative-compounding-with-retain-logic",
    "title": "7  Compound Returns",
    "section": "7.6 Method 3: Iterative Compounding with Retain Logic",
    "text": "7.6 Method 3: Iterative Compounding with Retain Logic\nIn some applications, we need fine-grained control over how missing values, delisting events, or other special conditions affect the compounding process. The iterative approach processes each observation sequentially, carrying forward the cumulative return and applying conditional logic at each step.\n\ndef compute_cumret_iterative(df, ret_col=\"ret_total\",\n                              group_col=\"symbol\",\n                              handle_missing=\"carry\"):\n    \"\"\"Compute cumulative returns iteratively with flexible\n    missing value handling.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    ret_col : str\n    group_col : str\n    handle_missing : str\n        'carry' : treat missing as zero return (carry forward)\n        'propagate' : propagate NaN (conservative)\n        'reset' : reset wealth index to 1 after missing spell\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    results = []\n\n    for name, group in df.groupby(group_col):\n        cumret = 1.0\n        cumrets = []\n        for _, row in group.iterrows():\n            ret = row[ret_col]\n            if pd.notna(ret):\n                cumret = cumret * (1 + ret)\n            else:\n                if handle_missing == \"propagate\":\n                    cumret = np.nan\n                elif handle_missing == \"reset\":\n                    cumret = 1.0\n                # 'carry' does nothing (cumret unchanged)\n            cumrets.append(cumret)\n        group = group.copy()\n        group[\"wealth_iter\"] = cumrets\n        group[\"cumret_iter\"] = group[\"wealth_iter\"] - 1\n        results.append(group)\n\n    return pd.concat(results, ignore_index=True)\n\n\n\n\n\n\n\nNote\n\n\n\nThe iterative method is the slowest of the four approaches because it cannot leverage NumPy’s vectorized operations. For large datasets, prefer Method 1 or 2 unless the conditional logic in Method 3 is essential. On a dataset with 1 million observations, Method 1 runs in approximately 0.1 seconds versus 10+ seconds for Method 3.\n\n\n\n7.6.1 Comparison of Missing Value Treatments\nTo illustrate how the three missing-value strategies differ, consider a hypothetical stock with one missing return in the middle of its history:\n\nexample = pd.DataFrame({\n    \"symbol\": [1]*6,\n    \"date\": pd.date_range(\"2024-01-31\", periods=6, freq=\"ME\"),\n    \"ret_total\": [0.05, 0.03, np.nan, 0.04, -0.02, 0.06]\n})\n\ncarry = compute_cumret_iterative(example, handle_missing=\"carry\")\npropagate = compute_cumret_iterative(\n    example, handle_missing=\"propagate\"\n)\nreset = compute_cumret_iterative(example, handle_missing=\"reset\")\n\ncomparison = pd.DataFrame({\n    \"Date\": example[\"date\"].dt.strftime(\"%Y-%m\"),\n    \"Return\": example[\"ret_total\"],\n    \"Carry\": carry[\"cumret_iter\"].round(6),\n    \"Propagate\": propagate[\"cumret_iter\"].round(6),\n    \"Reset\": reset[\"cumret_iter\"].round(6)\n})\ncomparison\n\n\n\nTable 7.4: Effect of different missing value treatments on cumulative returns. The ‘carry’ strategy assumes zero return for missing periods (appropriate for trading halts); ‘propagate’ makes all subsequent values undefined (conservative); ‘reset’ restarts the cumulative product after the missing spell.\n\n\n\n\n\n\n\n\n\n\nDate\nReturn\nCarry\nPropagate\nReset\n\n\n\n\n0\n2024-01\n0.05\n0.050000\n0.0500\n0.050000\n\n\n1\n2024-02\n0.03\n0.081500\n0.0815\n0.081500\n\n\n2\n2024-03\nNaN\n0.081500\nNaN\n0.000000\n\n\n3\n2024-04\n0.04\n0.124760\nNaN\n0.040000\n\n\n4\n2024-05\n-0.02\n0.102265\nNaN\n0.019200\n\n\n5\n2024-06\n0.06\n0.168401\nNaN\n0.080352",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#method-4-rolling-compound-returns",
    "href": "06_compound_return.html#method-4-rolling-compound-returns",
    "title": "7  Compound Returns",
    "section": "7.7 Method 4: Rolling Compound Returns",
    "text": "7.7 Method 4: Rolling Compound Returns\nFor many empirical applications, including momentum strategies, performance evaluation, and risk estimation, we need compound returns over rolling windows of fixed length. This section implements efficient rolling compounding using pandas.\n\n7.7.1 Rolling Window via Log Returns\nThe most efficient approach combines the log-sum-exp method with rolling sums:\n\ndef rolling_compound_return(df, ret_col=\"ret_total\",\n                             group_col=\"symbol\",\n                             windows=[3, 6, 9, 12]):\n    \"\"\"Compute rolling compound returns over specified windows.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must be sorted by [group_col, 'date'] with no gaps.\n    ret_col : str\n    group_col : str\n    windows : list of int\n        Rolling window lengths (in periods).\n\n    Returns\n    -------\n    pd.DataFrame with new columns ret_{k} for each window k.\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[\"log_ret\"] = np.log(1 + df[ret_col])\n\n    for k in windows:\n        rolling_logsum = (\n            df.groupby(group_col)[\"log_ret\"]\n            .transform(\n                lambda x: x.rolling(\n                    window=k, min_periods=k\n                ).sum()\n            )\n        )\n        df[f\"ret_{k}\"] = np.exp(rolling_logsum) - 1\n\n    df.drop(columns=[\"log_ret\"], inplace=True)\n    return df\n\nWe apply this to our full sample to compute 3-, 6-, 9-, and 12-month trailing compound returns:\n\nstock_rolling = rolling_compound_return(\n    prices_monthly,\n    windows=[3, 6, 9, 12]\n)\n\nLet us also compute the same rolling returns for the market index, which serves as a benchmark for excess return calculations:\n\n# Compute market rolling returns\nmarket_monthly = (\n    prices_monthly[[\"date\", \"mkt_total\"]]\n    .drop_duplicates()\n    .sort_values(\"date\")\n    .copy()\n)\nmarket_monthly[\"log_mkt\"] = np.log(1 + market_monthly[\"mkt_total\"])\n\nfor k in [3, 6, 9, 12]:\n    market_monthly[f\"mkt_{k}\"] = (\n        np.exp(\n            market_monthly[\"log_mkt\"]\n            .rolling(window=k, min_periods=k)\n            .sum()\n        ) - 1\n    )\n\nmarket_monthly.drop(columns=[\"log_mkt\"], inplace=True)\n\n# Merge market rolling returns back\nstock_rolling = stock_rolling.merge(\n    market_monthly[\n        [\"date\"] + [f\"mkt_{k}\" for k in [3, 6, 9, 12]]\n    ],\n    on=\"date\",\n    how=\"left\"\n)\n\nFigure 35.4 displays the distribution of 12-month rolling compound returns over time.\n\nrolling_stats = (\n    stock_rolling\n    .dropna(subset=[\"ret_12\"])\n    .groupby(\"date\")[\"ret_12\"]\n    .agg([\"median\", lambda x: x.quantile(0.25),\n           lambda x: x.quantile(0.75)])\n    .reset_index()\n)\nrolling_stats.columns = [\"date\", \"median\", \"p25\", \"p75\"]\n\nplot_rolling = (\n    ggplot(rolling_stats, aes(x=\"date\")) +\n    geom_ribbon(aes(ymin=\"p25\", ymax=\"p75\"),\n                alpha=0.3, fill=\"#2166ac\") +\n    geom_line(aes(y=\"median\"), color=\"#2166ac\", size=0.7) +\n    geom_hline(yintercept=0, linetype=\"dashed\") +\n    labs(x=\"\", y=\"12-month compound return\") +\n    scale_y_continuous(labels=percent_format()) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_rolling.draw()\n\n\n\n\n\n\n\nFigure 7.2: Cross-sectional distribution of 12-month rolling compound returns for Vietnamese stocks over time. The shaded band represents the interquartile range (25th–75th percentiles), while the solid line shows the median. Sharp market-wide events—such as the 2008 global financial crisis and the 2020 COVID-19 shock—are visible as periods when even the median return turns sharply negative.\n\n\n\n\n\n\n\n7.7.2 Verifying Rolling Returns\nIt is prudent to verify rolling compound returns against a direct calculation. We select one stock and recompute its 12-month return manually:\n\ntest_stock = long_history_stocks[0]\ntest_data = (\n    stock_rolling[stock_rolling[\"symbol\"] == test_stock]\n    .sort_values(\"date\")\n    .tail(15)\n    .copy()\n)\n\n# Direct computation\ntest_data[\"direct_ret_12\"] = (\n    test_data[\"ret_total\"]\n    .transform(\n        lambda x: x.add(1).rolling(\n            12, min_periods=12\n        ).apply(np.prod, raw=True) - 1\n    )\n)\n\nverify = (\n    test_data[[\"date\", \"ret_12\", \"direct_ret_12\"]]\n    .dropna()\n    .tail(5)\n    .copy()\n)\nverify[\"difference\"] = (\n    verify[\"ret_12\"] - verify[\"direct_ret_12\"]\n).abs()\nverify.round(8)\n\n/tmp/ipykernel_2229780/922053246.py:28: UserWarning: obj.round has no effect with datetime, timedelta, or period dtypes. Use obj.dt.round(...) instead.\n\n\n\n\nTable 7.5: Verification of rolling compound return calculation. The ‘Direct’ column computes the product of the preceding 12 monthly gross returns minus one; ‘Rolling’ uses our log-sum-exp function. Differences are at machine precision.\n\n\n\n\n\n\n\n\n\n\ndate\nret_12\ndirect_ret_12\ndifference\n\n\n\n\n386\n2023-09-30\n-0.152407\n-0.152407\n0.0\n\n\n387\n2023-10-31\n-0.208836\n-0.208836\n0.0\n\n\n388\n2023-11-30\n-0.199018\n-0.199018\n0.0\n\n\n389\n2023-12-31\n-0.233698\n-0.233698\n0.0",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#delisting-returns-and-survivorship-bias",
    "href": "06_compound_return.html#delisting-returns-and-survivorship-bias",
    "title": "7  Compound Returns",
    "section": "7.8 Delisting Returns and Survivorship Bias",
    "text": "7.8 Delisting Returns and Survivorship Bias\nA critical practical concern when computing compound returns is the treatment of securities that are removed from an exchange. Delisting occurs for various reasons: mergers and acquisitions, bankruptcy, failure to meet listing requirements, voluntary withdrawal, or transfer to another exchange. If delisting returns are not incorporated, the resulting compound returns suffer from survivorship bias: they overstate performance because the worst outcomes (bankruptcies, forced delistings) are excluded (Shumway 1997).\n\n7.8.1 The Vietnamese Context\nIn Vietnam, securities can be removed from their exchange listing for several reasons as specified by the SSC and exchange regulations:\n\nMandatory delisting: when a firm has accumulated losses exceeding its charter capital, fails to meet financial reporting obligations for three consecutive years, or has its business license revoked.\nVoluntary delisting: when a firm’s shareholders vote to withdraw from the exchange.\nTransfer: when a firm moves from UPCoM to HOSE/HNX (upgrade) or from HOSE/HNX to UPCoM (downgrade). These transfers are not true delistings in the economic sense but require careful handling in return calculations.\n\nUnlike more developed markets where detailed delisting return data is systematically compiled, Vietnamese market data may not always provide an explicit delisting return. When a stock is delisted for cause (e.g., bankruptcy), the last traded price may significantly overstate the security’s recovery value. Researchers should be aware of this limitation and consider imputing delisting returns based on the delisting reason, following the methodology of Shumway (1997).\n\n\n7.8.2 Incorporating Delisting Returns\nWhen a security is delisted, a final “delisting return” captures the value change between the last regular trading day and the realization of value after delisting. This return must be combined with the regular return in the delisting month:\n\\[\nR_t^{\\text{adj}} = (1 + R_t)(1 + R_t^{\\text{delist}}) - 1,\n\\tag{7.12}\\]\nwhere \\(R_t\\) is the regular return and \\(R_t^{\\text{delist}}\\) is the delisting return. If the regular return is missing (the stock ceased trading before month end), we use the delisting return alone.\n\ndef adjust_for_delisting(df, ret_col=\"ret_total\",\n                          dlret_col=\"dlret\"):\n    \"\"\"Adjust returns for delisting events.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain `ret_col` and `dlret_col`.\n\n    Returns\n    -------\n    pd.DataFrame with adjusted return column 'ret_adj'.\n    \"\"\"\n    df = df.copy()\n    df[\"ret_adj\"] = df[ret_col]\n\n    # Case 1: Both regular and delisting returns available\n    mask_both = df[ret_col].notna() & df[dlret_col].notna()\n    df.loc[mask_both, \"ret_adj\"] = (\n        (1 + df.loc[mask_both, ret_col]) *\n        (1 + df.loc[mask_both, dlret_col]) - 1\n    )\n\n    # Case 2: Only delisting return available\n    mask_dlret_only = (\n        df[ret_col].isna() & df[dlret_col].notna()\n    )\n    df.loc[mask_dlret_only, \"ret_adj\"] = (\n        df.loc[mask_dlret_only, dlret_col]\n    )\n\n    return df\n\n\n\n7.8.3 Impact of Delisting Adjustment\nThe magnitude of the delisting bias depends on the frequency and severity of delisting events. Shumway (1997) showed that, in developed markets, ignoring delisting returns introduces an upward bias of approximately 1% per year in equal-weighted portfolio returns. The bias is larger for small-cap stocks and value stocks, which are more prone to financial distress. In Vietnam, where smaller firms on HNX and UPCoM face tighter liquidity constraints and higher default risk, the bias may be even more pronounced. In emerging market delistings, mandatory delistings often involve firms with severe financial distress where residual equity value is near zero, implying delisting returns close to \\(-100\\%\\) in the worst cases.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#rolling-volatility-estimation",
    "href": "06_compound_return.html#rolling-volatility-estimation",
    "title": "7  Compound Returns",
    "section": "7.9 Rolling Volatility Estimation",
    "text": "7.9 Rolling Volatility Estimation\nStock return volatility is a key input for risk management, option pricing, and many empirical asset pricing models. A common approach is to estimate rolling standard deviations of returns over a trailing window.\n\n7.9.1 24-Month Rolling Volatility\nFollowing Ben-David, Franzoni, and Moussawi (2012), we compute the total stock return volatility as the rolling standard deviation of monthly returns over a 24-month window:\n\\[\n\\hat{\\sigma}_{i,t}^{24} = \\sqrt{\\frac{1}{23}\\sum_{j=0}^{23}(R_{i,t-j} - \\bar{R}_{i,t}^{24})^2},\n\\tag{7.13}\\]\nwhere \\(\\bar{R}_{i,t}^{24} = \\frac{1}{24}\\sum_{j=0}^{23} R_{i,t-j}\\) is the trailing 24-month mean return.\n\ndef rolling_volatility(df, ret_col=\"ret_total\",\n                        group_col=\"symbol\",\n                        window=24):\n    \"\"\"Compute rolling return volatility.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    ret_col : str\n    group_col : str\n    window : int\n        Rolling window length in periods.\n\n    Returns\n    -------\n    pd.DataFrame with 'vol_{window}' column (annualized).\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[f\"vol_{window}\"] = (\n        df.groupby(group_col)[ret_col]\n        .transform(\n            lambda x: x.rolling(\n                window=window, min_periods=window\n            ).std()\n        )\n    )\n    # Annualize (monthly to annual)\n    df[f\"vol_{window}_ann\"] = df[f\"vol_{window}\"] * np.sqrt(12)\n    return df\n\n\nstock_vol = rolling_volatility(stock_rolling)\n\nFigure 7.3 shows the cross-sectional distribution of annualized 24-month volatility over time.\n\nvol_stats = (\n    stock_vol\n    .dropna(subset=[\"vol_24_ann\"])\n    .groupby(\"date\")[\"vol_24_ann\"]\n    .agg([\"median\", lambda x: x.quantile(0.25),\n           lambda x: x.quantile(0.75)])\n    .reset_index()\n)\nvol_stats.columns = [\"date\", \"median\", \"p25\", \"p75\"]\n\nplot_vol = (\n    ggplot(vol_stats, aes(x=\"date\")) +\n    geom_ribbon(aes(ymin=\"p25\", ymax=\"p75\"),\n                alpha=0.3, fill=\"#b2182b\") +\n    geom_line(aes(y=\"median\"), color=\"#b2182b\", size=0.7) +\n    labs(x=\"\", y=\"Annualized 24-month volatility\") +\n    scale_y_continuous(labels=percent_format()) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_vol.draw()\n\n\n\n\n\n\n\nFigure 7.3: Cross-sectional distribution of annualized 24-month rolling stock return volatility for Vietnamese equities. The median volatility (solid line) and interquartile range (shaded band) capture both secular trends and crisis episodes. Vietnamese stocks exhibit structurally higher volatility than developed-market peers, with the median annualized volatility typically ranging between 30% and 50%.\n\n\n\n\n\n\n\n7.9.2 Volatility and Compound Returns: The Variance Drain\nAs noted in Equation 7.7, the geometric mean return falls below the arithmetic mean by approximately \\(\\sigma^2/2\\). This “variance drain” or “volatility drag” means that two portfolios with the same arithmetic mean return but different volatilities will have different compound returns: the lower-volatility portfolio will compound to greater terminal wealth.\nThis effect is quantitatively important in Vietnam. A stock with an arithmetic mean monthly return of 1.5% and a monthly standard deviation of 10% suffers a volatility drag of approximately \\(0.10^2/2 = 0.5\\%\\) per month, or roughly 6% per year. This is consistent with the observation that Vietnamese investors face substantial erosion of compound wealth from the high idiosyncratic volatility of individual stocks. We can verify this empirically by sorting stocks into volatility quintiles and comparing compound returns:\n\nannual_data = compound_return_by_period(\n    prices_monthly, period=\"year\"\n)\nannual_data = annual_data[annual_data[\"n_obs\"] &gt;= 10].copy()\n\nvol_annual = (\n    prices_monthly\n    .groupby([\"symbol\", prices_monthly[\"date\"].dt.year])[\n        \"ret_total\"\n    ]\n    .agg([\"std\", \"mean\", \"count\"])\n    .reset_index()\n)\nvol_annual.columns = [\"symbol\", \"period\", \"monthly_std\",\n                       \"monthly_mean\", \"n_months\"]\nvol_annual = vol_annual[vol_annual[\"n_months\"] &gt;= 10].copy()\nvol_annual[\"ann_vol\"] = vol_annual[\"monthly_std\"] * np.sqrt(12)\nvol_annual[\"arith_mean_ann\"] = vol_annual[\"monthly_mean\"] * 12\n\nvol_analysis = annual_data.merge(\n    vol_annual, on=[\"symbol\", \"period\"]\n)\n\nvol_analysis[\"vol_quintile\"] = (\n    vol_analysis.groupby(\"period\")[\"ann_vol\"]\n    .transform(\n        lambda x: pd.qcut(\n            x, 5, labels=[1, 2, 3, 4, 5], duplicates=\"drop\"\n        )\n    )\n)\n\nvol_summary = (\n    vol_analysis\n    .groupby(\"vol_quintile\")\n    .agg(\n        arithmetic_mean=(\"arith_mean_ann\", \"mean\"),\n        geometric_mean=(\"cumret\", \"mean\"),\n        avg_volatility=(\"ann_vol\", \"mean\"),\n        n_stockyears=(\"cumret\", \"count\")\n    )\n    .round(4)\n    .reset_index()\n)\nvol_summary\n\n\n\nTable 7.6: Arithmetic mean, geometric mean, and volatility by volatility quintile for Vietnamese stocks. The difference between arithmetic and geometric mean increases with volatility, confirming the variance drain effect. The magnitude of the drag is notably large for the highest-volatility quintile, typical of small and illiquid stocks on HNX and UPCoM.\n\n\n\n\n\n\n\n\n\n\nvol_quintile\narithmetic_mean\ngeometric_mean\navg_volatility\nn_stockyears\n\n\n\n\n0\n1\n-0.0908\n-0.0763\n0.1887\n2708\n\n\n1\n2\n-0.0754\n-0.0610\n0.3312\n2700\n\n\n2\n3\n-0.0388\n-0.0169\n0.4493\n2701\n\n\n3\n4\n0.0404\n0.0494\n0.6005\n2700\n\n\n4\n5\n0.4411\n0.3389\n1.0288\n2705",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#compound-returns-around-fiscal-year-ends",
    "href": "06_compound_return.html#compound-returns-around-fiscal-year-ends",
    "title": "7  Compound Returns",
    "section": "7.10 Compound Returns Around Fiscal Year Ends",
    "text": "7.10 Compound Returns Around Fiscal Year Ends\nA widely used approach in accounting and finance research aligns compound returns to firm-specific fiscal period end dates. This is essential for computing buy-and-hold abnormal returns (BHARs) for event studies, post-earnings-announcement drift, and other studies where the event date varies by firm.\nIn Vietnam, the majority of listed firms follow a calendar fiscal year (January–December), as required by the Law on Accounting unless the Ministry of Finance grants an exemption. However, firms in certain industries (e.g., agriculture, tourism) may use non-standard fiscal years ending in March, June, or September.\n\n\n7.10.1 Aligning Returns to Fiscal Periods\nThe key challenge is that fiscal year ends differ across firms. We need to compute compound returns over windows anchored at these firm-specific dates.\n\ndef compound_returns_around_event(\n    returns_df, events_df,\n    id_col=\"symbol\", date_col=\"date\",\n    event_date_col=\"datadate\", ret_col=\"ret_total\",\n    pre_windows=[3, 6, 9, 12],\n    post_windows=[3, 6]\n):\n    \"\"\"Compute compound returns in windows around firm-specific\n    event dates.\n\n    Parameters\n    ----------\n    returns_df : pd.DataFrame\n        Monthly returns with [id_col, date_col, ret_col].\n    events_df : pd.DataFrame\n        Event dates with [id_col, event_date_col].\n    pre_windows : list of int\n        Trailing window lengths (months before event).\n    post_windows : list of int\n        Forward window lengths (months after event).\n\n    Returns\n    -------\n    pd.DataFrame with compound returns for each window.\n    \"\"\"\n    returns_df = returns_df.sort_values(\n        [id_col, date_col]\n    ).copy()\n    events_df = events_df.copy()\n\n    # Align event dates to month ends\n    events_df[\"event_month\"] = (\n        pd.to_datetime(events_df[event_date_col])\n        + pd.offsets.MonthEnd(0)\n    )\n\n    results = []\n\n    for _, event in events_df.iterrows():\n        sid = event[id_col]\n        edate = event[\"event_month\"]\n\n        sec_rets = returns_df[\n            returns_df[id_col] == sid\n        ].copy()\n        sec_rets = sec_rets.set_index(date_col)[ret_col]\n\n        row = {id_col: sid,\n               event_date_col: event[event_date_col]}\n\n        # Pre-event compound returns\n        for k in pre_windows:\n            start = edate - pd.DateOffset(months=k-1)\n            start = (start - pd.offsets.MonthEnd(0)\n                     + pd.offsets.MonthEnd(0))\n            window_rets = sec_rets[\n                (sec_rets.index &gt;= start)\n                & (sec_rets.index &lt;= edate)\n            ]\n            if len(window_rets) &gt;= k * 0.8:\n                cumret = (\n                    np.exp(np.log(1 + window_rets).sum()) - 1\n                )\n            else:\n                cumret = np.nan\n            row[f\"ret_pre_{k}\"] = cumret\n\n        # Post-event compound returns\n        for k in post_windows:\n            start = edate + pd.DateOffset(months=1)\n            end = (edate + pd.DateOffset(months=k)\n                   + pd.offsets.MonthEnd(0))\n            window_rets = sec_rets[\n                (sec_rets.index &gt;= start)\n                & (sec_rets.index &lt;= end)\n            ]\n            if len(window_rets) &gt;= k * 0.8:\n                cumret = (\n                    np.exp(np.log(1 + window_rets).sum()) - 1\n                )\n            else:\n                cumret = np.nan\n            row[f\"ret_post_{k}\"] = cumret\n\n        results.append(row)\n\n    return pd.DataFrame(results)\n\n\n\n7.10.2 Buy-and-Hold Abnormal Returns versus Cumulative Abnormal Returns\nFor event studies and performance evaluation, we often want the excess compound return, which is the stock’s compound return minus a benchmark’s compound return over the same window. The buy-and-hold abnormal return (BHAR) is defined as\n\\[\n\\text{BHAR}_{i,t}(k) = \\prod_{j=1}^{k}(1 + R_{i,t+j}) - \\prod_{j=1}^{k}(1 + R_{b,t+j}),\n\\tag{7.14}\\]\nwhere \\(R_{b,t}\\) is the benchmark return (market index, size-matched portfolio, etc.). This differs from the cumulative abnormal return (CAR), which sums simple abnormal returns:\n\\[\n\\text{CAR}_{i,t}(k) = \\sum_{j=1}^{k}(R_{i,t+j} - R_{b,t+j}).\n\\tag{7.15}\\]\nThe BHAR better captures the actual investor experience because it reflects the compounding of returns, whereas the CAR implicitly assumes daily rebalancing to maintain equal dollar positions in the stock and benchmark (Barber and Lyon 1997). The distinction is particularly important in Vietnam, where individual stock returns can be highly volatile and the compounding effect is therefore magnified. Lyon, Barber, and Tsai (1999) provide further analysis of the statistical properties of BHARs and recommend bootstrapped critical values for inference.\n\ndef compute_bhar(stock_returns, benchmark_returns):\n    \"\"\"Compute buy-and-hold abnormal return.\n\n    Parameters\n    ----------\n    stock_returns : array-like\n        Sequence of stock returns.\n    benchmark_returns : array-like\n        Sequence of benchmark returns (same length).\n\n    Returns\n    -------\n    float : BHAR\n    \"\"\"\n    stock_cumret = (\n        np.prod(1 + np.array(stock_returns)) - 1\n    )\n    bench_cumret = (\n        np.prod(1 + np.array(benchmark_returns)) - 1\n    )\n    return stock_cumret - bench_cumret",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#book-value-of-equity",
    "href": "06_compound_return.html#book-value-of-equity",
    "title": "7  Compound Returns",
    "section": "7.11 Book Value of Equity",
    "text": "7.11 Book Value of Equity\nMany empirical applications that use compound returns also require firm-level accounting variables. A commonly used variable is the book value of equity, computed following Daniel and Titman (1997):\n\\[\n\\text{BE} = \\text{SE} + \\text{DT} + \\text{ITC} - \\text{PS},\n\\tag{7.16}\\]\nwhere SE is stockholders’ equity, DT is deferred taxes, ITC is investment tax credit, and PS is the preferred stock value. For preferred stock, the hierarchy is: redemption value if available, then liquidating value, then carrying value.\nIn Vietnam, the accounting standards (Vietnamese Accounting Standards, VAS, and increasingly IFRS adoption) provide a somewhat different chart of accounts. Stockholders’ equity is reported on the balance sheet as Vốn chủ sở hữu, which includes contributed capital (Vốn góp của chủ sở hữu), share premium (Thặng dư vốn cổ phần), treasury stock adjustments, retained earnings (Lợi nhuận sau thuế chưa phân phối), and other reserves. Deferred tax assets and liabilities are reported separately. Preferred stock is rare among Vietnamese listed firms (most issue only common shares), but when present, its book value should be subtracted from total equity.\n\ndef compute_book_equity(df):\n    \"\"\"Compute book value of equity for Vietnamese firms.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain at minimum: equity (stockholders' equity),\n        deferred_tax (deferred tax liabilities, net),\n        pref_stock (preferred stock, if applicable).\n\n    Returns\n    -------\n    pd.DataFrame with 'be' column.\n    \"\"\"\n    df = df.copy()\n    df[\"pref\"] = df.get(\n        \"pref_stock\", pd.Series(0, index=df.index)\n    )\n    df[\"dt\"] = df.get(\n        \"deferred_tax\", pd.Series(0, index=df.index)\n    )\n    df[\"be\"] = (\n        df[\"equity\"].fillna(0)\n        + df[\"dt\"].fillna(0)\n        - df[\"pref\"].fillna(0)\n    )\n    # Set non-positive book equity to NaN\n    df.loc[df[\"be\"] &lt;= 0, \"be\"] = np.nan\n    return df",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#maximum-drawdown",
    "href": "06_compound_return.html#maximum-drawdown",
    "title": "7  Compound Returns",
    "section": "7.12 Maximum Drawdown",
    "text": "7.12 Maximum Drawdown\nThe maximum drawdown is a key risk metric that complements volatility. While volatility measures the dispersion of returns symmetrically, the maximum drawdown captures the worst cumulative loss an investor could experience: a measure that aligns more closely with how investors psychologically experience risk (Kahneman and Tversky 2013).\n\ndef compute_max_drawdown(df, ret_col=\"ret_total\",\n                          group_col=\"symbol\"):\n    \"\"\"Compute maximum drawdown for each security.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n    ret_col : str\n    group_col : str\n\n    Returns\n    -------\n    pd.DataFrame with 'max_drawdown' and running drawdown.\n    \"\"\"\n    df = df.sort_values([group_col, \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[ret_col]\n    df[\"wealth\"] = (\n        df.groupby(group_col)[\"gross_ret\"].cumprod()\n    )\n    df[\"peak\"] = df.groupby(group_col)[\"wealth\"].cummax()\n    df[\"drawdown\"] = (\n        (df[\"wealth\"] - df[\"peak\"]) / df[\"peak\"]\n    )\n\n    max_dd = (\n        df.groupby(group_col)[\"drawdown\"]\n        .min()\n        .reset_index(name=\"max_drawdown\")\n    )\n    df = df.merge(max_dd, on=group_col)\n    df.drop(columns=[\"gross_ret\"], inplace=True)\n    return df\n\nFigure 35.7 illustrates the drawdown profile for a selected stock.\n\ndd_data = compute_max_drawdown(\n    prices_monthly[\n        prices_monthly[\"symbol\"] == long_history_stocks[0]\n    ]\n)\nmdd = dd_data[\"max_drawdown\"].iloc[0]\n\nplot_dd = (\n    ggplot(dd_data, aes(x=\"date\", y=\"drawdown\")) +\n    geom_area(fill=\"#b2182b\", alpha=0.4) +\n    geom_line(color=\"#b2182b\", size=0.5) +\n    geom_hline(yintercept=mdd, linetype=\"dashed\") +\n    labs(x=\"\", y=\"Drawdown from peak\") +\n    scale_y_continuous(labels=percent_format()) +\n    theme_minimal() +\n    theme(figure_size=(10, 4))\n)\nplot_dd.draw()\n\n\n\n\n\n\n\nFigure 7.4: Drawdown profile for a selected Vietnamese stock showing the percentage decline from each running peak. The maximum drawdown (horizontal dashed line) represents the worst peak-to-trough loss over the full sample. Vietnamese stocks frequently exhibit drawdowns exceeding 50%, reflecting the market’s high volatility and susceptibility to sentiment-driven corrections.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#putting-it-all-together-a-comprehensive-pipeline",
    "href": "06_compound_return.html#putting-it-all-together-a-comprehensive-pipeline",
    "title": "7  Compound Returns",
    "section": "7.13 Putting It All Together: A Comprehensive Pipeline",
    "text": "7.13 Putting It All Together: A Comprehensive Pipeline\nWe now combine all the methods into a single pipeline that produces a research-ready dataset with rolling compound returns, market returns, volatility, and drawdown measures.\n\ndef build_compound_return_dataset(\n    stock_df, windows=[3, 6, 9, 12], vol_window=24\n):\n    \"\"\"Build comprehensive compound return dataset.\n\n    Parameters\n    ----------\n    stock_df : pd.DataFrame\n        Monthly stock return data with columns:\n        symbol, date, ret_total, mkt_total.\n    windows : list of int\n        Rolling compound return windows.\n    vol_window : int\n        Rolling volatility window.\n\n    Returns\n    -------\n    pd.DataFrame\n    \"\"\"\n    df = stock_df.sort_values([\"symbol\", \"date\"]).copy()\n\n    # Step 1: Log returns\n    df[\"log_ret\"] = np.log(1 + df[\"ret_total\"])\n    df[\"log_mkt\"] = np.log(1 + df[\"mkt_total\"])\n\n    # Step 2: Rolling compound returns (stock and market)\n    for k in windows:\n        df[f\"ret_{k}\"] = np.exp(\n            df.groupby(\"symbol\")[\"log_ret\"]\n            .transform(\n                lambda x: x.rolling(k, min_periods=k).sum()\n            )\n        ) - 1\n\n        df[f\"mkt_{k}\"] = np.exp(\n            df[\"log_mkt\"]\n            .rolling(k, min_periods=k)\n            .sum()\n        ) - 1\n\n        # Excess compound return (BHAR vs market)\n        df[f\"exret_{k}\"] = df[f\"ret_{k}\"] - df[f\"mkt_{k}\"]\n\n    # Step 3: Cumulative return (full history)\n    df[\"wealth\"] = (\n        df.groupby(\"symbol\")[\"log_ret\"]\n        .cumsum()\n        .apply(np.exp)\n    )\n    df[\"cumret\"] = df[\"wealth\"] - 1\n\n    # Step 4: Rolling volatility\n    df[f\"vol_{vol_window}\"] = (\n        df.groupby(\"symbol\")[\"ret_total\"]\n        .transform(\n            lambda x: x.rolling(\n                vol_window, min_periods=vol_window\n            ).std()\n        )\n    ) * np.sqrt(12)  # annualize\n\n    # Step 5: Drawdown\n    df[\"peak\"] = df.groupby(\"symbol\")[\"wealth\"].cummax()\n    df[\"drawdown\"] = (df[\"wealth\"] - df[\"peak\"]) / df[\"peak\"]\n\n    # Clean up\n    df.drop(\n        columns=[\"log_ret\", \"log_mkt\", \"peak\"], inplace=True\n    )\n\n    return df\n\n\n# Build the full dataset\ncompound_dataset = build_compound_return_dataset(prices_monthly)\n\nTable 7.7 provides summary statistics for the key variables in our compound return dataset.\n\nsummary_cols = [\"ret_total\", \"ret_3\", \"ret_6\", \"ret_12\",\n                \"exret_3\", \"exret_12\", \"vol_24\", \"drawdown\"]\navailable_cols = [c for c in summary_cols\n                  if c in compound_dataset.columns]\n\nsummary = (\n    compound_dataset[available_cols]\n    .describe(percentiles=[0.05, 0.25, 0.50, 0.75, 0.95])\n    .T\n    .round(4)\n)\nsummary\n\n\n\nTable 7.7: Summary statistics for compound return variables across all Vietnamese stock-month observations. Returns are in decimal form (0.10 = 10%). The wide dispersion of 12-month compound returns and the high median volatility reflect the emerging market characteristics of the Vietnamese equity market.\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n5%\n25%\n50%\n75%\n95%\nmax\n\n\n\n\nret_total\n165499.0\n0.0042\n0.1862\n-0.9900\n-0.2381\n-0.0703\n0.0000\n0.0553\n0.2773\n12.7500\n\n\nret_3\n162586.0\n0.0094\n0.3393\n-0.9999\n-0.3889\n-0.1436\n-0.0126\n0.0987\n0.5000\n27.2911\n\n\nret_6\n158227.0\n0.0171\n0.5053\n-0.9999\n-0.5095\n-0.2196\n-0.0400\n0.1404\n0.7320\n35.7136\n\n\nret_12\n149520.0\n0.0375\n0.8136\n-0.9999\n-0.6522\n-0.3191\n-0.0877\n0.1807\n1.0767\n47.9515\n\n\nexret_3\n153637.0\n0.0385\n0.3343\n-1.1691\n-0.3420\n-0.1163\n0.0067\n0.1378\n0.4992\n27.3041\n\n\nexret_12\n140571.0\n0.1401\n0.8031\n-1.5858\n-0.5388\n-0.2003\n0.0281\n0.2880\n1.1119\n48.0488\n\n\nvol_24\n132233.0\n0.5493\n0.3488\n0.0000\n0.2070\n0.3445\n0.4827\n0.6737\n1.0739\n9.1792\n\n\ndrawdown\n165499.0\n-0.5927\n0.2975\n-1.0000\n-0.9631\n-0.8501\n-0.6616\n-0.3725\n0.0000\n0.0000",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#cross-sectional-distribution-of-compound-returns",
    "href": "06_compound_return.html#cross-sectional-distribution-of-compound-returns",
    "title": "7  Compound Returns",
    "section": "7.14 Cross-Sectional Distribution of Compound Returns",
    "text": "7.14 Cross-Sectional Distribution of Compound Returns\nTo understand how compound returns vary across securities, we examine the cross-sectional distribution at different horizons.\n\nhorizon_data = pd.DataFrame()\nfor k in [3, 6, 12]:\n    col = f\"ret_{k}\"\n    temp = compound_dataset[[col]].dropna().copy()\n    temp.columns = [\"compound_return\"]\n    temp[\"horizon\"] = f\"{k} months\"\n    lo, hi = temp[\"compound_return\"].quantile([0.01, 0.99])\n    temp = temp[\n        (temp[\"compound_return\"] &gt;= lo)\n        & (temp[\"compound_return\"] &lt;= hi)\n    ]\n    horizon_data = pd.concat([horizon_data, temp])\n\nplot_horizons = (\n    ggplot(horizon_data,\n           aes(x=\"compound_return\", fill=\"horizon\")) +\n    geom_density(alpha=0.4) +\n    geom_vline(xintercept=0, linetype=\"dashed\") +\n    labs(x=\"Compound return\", y=\"Density\", fill=\"Horizon\") +\n    scale_x_continuous(labels=percent_format()) +\n    theme_minimal() +\n    theme(legend_position=\"bottom\",\n          figure_size=(10, 5))\n)\nplot_horizons.draw()\n\n\n\n\n\n\n\nFigure 7.5: Cross-sectional distribution of compound returns at different horizons (3, 6, and 12 months) for Vietnamese stocks. Longer horizons exhibit greater dispersion and more pronounced right skewness, reflecting the compounding of idiosyncratic risk. The fat tails are more extreme than those typically observed in developed markets, consistent with the higher volatility environment.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#vietnam-specific-considerations",
    "href": "06_compound_return.html#vietnam-specific-considerations",
    "title": "7  Compound Returns",
    "section": "7.15 Vietnam-Specific Considerations",
    "text": "7.15 Vietnam-Specific Considerations\n\n7.15.1 Price Limits and Their Effect on Compounding\nVietnam’s stock exchanges impose daily price limits that cap the maximum price change from the reference price. As of the latest regulations:\n\nHOSE: \\(\\pm 7\\%\\)\nHNX: \\(\\pm 10\\%\\)\nUPCoM: \\(\\pm 15\\%\\)\n\nThese limits truncate the daily return distribution and can create sequences of limit-hit days when large information events occur. For compound return computation, this means that the adjustment to new information may be spread over multiple days rather than occurring instantaneously. When computing monthly compound returns from daily data, this is handled correctly because the compound return accumulates the full adjustment regardless of how many days it takes.\nHowever, price limits can introduce bias in short-horizon return computations. If a large positive event occurs and the stock hits the limit-up ceiling for several consecutive days, the 1-day or 1-week compound return will understate the true information content of the event (Kim, Liu, and Yang 2013). For event study applications, researchers should verify that the event window is long enough to accommodate the price-limit-induced delay in price adjustment.\n\n\n7.15.2 Foreign Ownership Limits\nVietnam imposes foreign ownership limits (FOL) on listed companies, typically capped at 49% for most industries and lower (30% or less) for certain restricted sectors such as banking and telecommunications. When a stock reaches its FOL, foreign investors can only purchase shares from other foreign sellers, creating a parallel premium market for foreign-board shares. This does not directly affect the computation of compound returns (which use official traded prices), but researchers studying cross-border portfolio returns should be aware that the effective price paid by foreign investors may differ from the board price (Vo 2017).\n\n\n7.15.3 The VN-Index and Market Benchmarks\nFor benchmark compound returns, Vietnam’s primary indices are:\n\nVN-Index: The capitalization-weighted index of all HOSE-listed stocks.\nVN30: The 30 largest and most liquid stocks on HOSE, reviewed semi-annually.\nHNX-Index: The capitalization-weighted index of HNX-listed stocks.\n\nThe VN-Index is the most widely used benchmark and is the default market return in our dataset.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#performance-considerations",
    "href": "06_compound_return.html#performance-considerations",
    "title": "7  Compound Returns",
    "section": "7.16 Performance Considerations",
    "text": "7.16 Performance Considerations\nWhen working with large datasets, computational efficiency matters. Table 7.8 compares the execution time of our four compounding methods on a standardized dataset.\n\nimport time\n\nnp.random.seed(42)\nn_stocks = 100\nn_months = 100\ntest_df = pd.DataFrame({\n    \"symbol\": np.repeat(range(n_stocks), n_months),\n    \"date\": np.tile(\n        pd.date_range(\"2015-01-31\", periods=n_months,\n                       freq=\"ME\"),\n        n_stocks\n    ),\n    \"ret_total\": np.random.normal(\n        0.01, 0.08, n_stocks * n_months\n    )\n})\n\nmethods = {}\n\nt0 = time.time()\n_ = compute_cumret_cumprod(test_df)\nmethods[\"Cumulative Product\"] = time.time() - t0\n\nt0 = time.time()\n_ = compute_cumret_logsum(test_df)\nmethods[\"Log-Sum-Exp\"] = time.time() - t0\n\nt0 = time.time()\n_ = compute_cumret_iterative(test_df)\nmethods[\"Iterative (carry)\"] = time.time() - t0\n\nt0 = time.time()\n_ = rolling_compound_return(test_df, windows=[12])\nmethods[\"Rolling (12-month)\"] = time.time() - t0\n\nperf_df = pd.DataFrame({\n    \"Method\": methods.keys(),\n    \"Time (seconds)\": [f\"{v:.4f}\" for v in methods.values()],\n    \"Relative Speed\": [\n        f\"{v/min(methods.values()):.1f}x\"\n        for v in methods.values()\n    ]\n})\nperf_df\n\n\n\nTable 7.8: Execution time comparison for different compounding methods on a dataset of 10,000 stock-month observations. The cumulative product and log-sum-exp methods are orders of magnitude faster than the iterative approach due to NumPy vectorization.\n\n\n\n\n\n\n\n\n\n\nMethod\nTime (seconds)\nRelative Speed\n\n\n\n\n0\nCumulative Product\n0.0039\n1.0x\n\n\n1\nLog-Sum-Exp\n0.0043\n1.1x\n\n\n2\nIterative (carry)\n0.5225\n132.7x\n\n\n3\nRolling (12-month)\n0.0230\n5.8x",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "06_compound_return.html#common-pitfalls-and-best-practices",
    "href": "06_compound_return.html#common-pitfalls-and-best-practices",
    "title": "7  Compound Returns",
    "section": "7.17 Common Pitfalls and Best Practices",
    "text": "7.17 Common Pitfalls and Best Practices\nSeveral subtle issues can lead to incorrect compound return calculations. We summarize the most important ones:\nGaps in the time series. If a security has months with no observations (not even a missing return flag), rolling window calculations based on positional indexing will produce incorrect results. The rolling window will span the wrong calendar period. Always ensure that the time series is complete, fill gaps with explicit missing values before computing rolling statistics. This is particularly relevant in Vietnam, where trading suspensions can create gaps.\nSurvivorship bias. As discussed in the delisting returns section, excluding securities that cease trading biases compound returns upward. Always incorporate delisting returns when available. When delisting returns are unavailable (as is sometimes the case in Vietnamese data), consider using imputed values based on the delisting reason.\nLook-ahead bias. When aligning compound returns to fiscal year ends for cross-sectional analysis, be careful not to use returns from before the fiscal year end to predict post-announcement returns. Vietnamese firms are required to publish audited annual financial statements within 90 days of the fiscal year end, so a buffer of at least 3 months is advisable when constructing forward-looking compound returns.\nNumerical overflow and underflow. For very long compounding horizons or extreme returns, the cumulative product can overflow (inf) or underflow (0). The log-sum-exp approach is more robust to such numerical issues because it operates in log space where the range is compressed.\nAnnualization of partial periods. When computing annualized returns from partial-period data (e.g., 7 months of data annualized to 12), the annualization formula \\((1+R)^{12/k} - 1\\) assumes that the observed return rate will persist. This assumption is stronger for short partial periods and can produce misleading results. Report the actual compound return and the number of periods alongside any annualized figures.\nExchange transfers. In Vietnam, stocks sometimes transfer between UPCoM, HNX, and HOSE. These transfers may involve temporary trading halts and can cause apparent gaps in the return series. When computing compound returns that span an exchange transfer, ensure that the return series is continuous across the transfer date.\n\n\n\n\n\n\n\nBarber, Brad M, and John D Lyon. 1997. “Detecting Long-Run Abnormal Stock Returns: The Empirical Power and Specification of Test Statistics.” Journal of Financial Economics 43 (3): 341–72.\n\n\nBen-David, Itzhak, Francesco Franzoni, and Rabih Moussawi. 2012. “Hedge Fund Stock Trading in the Financial Crisis of 2007–2009.” The Review of Financial Studies 25 (1): 1–54.\n\n\nDaniel, Kent, and Sheridan Titman. 1997. “Evidence on the Characteristics of Cross Sectional Variation in Stock Returns.” The Journal of Finance 52 (1): 1–33.\n\n\nKahneman, Daniel, and Amos Tversky. 2013. “Prospect Theory: An Analysis of Decision Under Risk.” In Handbook of the Fundamentals of Financial Decision Making: Part i, 99–127. World Scientific.\n\n\nKim, Kenneth A, Haixiao Liu, and J Jimmy Yang. 2013. “Reconsidering Price Limit Effectiveness.” Journal of Financial Research 36 (4): 493–518.\n\n\nLyon, John D, Brad M Barber, and Chih-Ling Tsai. 1999. “Improved Methods for Tests of Long-Run Abnormal Stock Returns.” The Journal of Finance 54 (1): 165–201.\n\n\nShumway, Tyler. 1997. “The Delisting Bias in CRSP Data.” The Journal of Finance 52 (1): 327–40.\n\n\nVo, Xuan Vinh. 2017. “Do Foreign Investors Improve Stock Price Informativeness in Emerging Equity Markets? Evidence from Vietnam.” Research in International Business and Finance 42: 986–91.",
    "crumbs": [
      "Home",
      "Thị trường tài chính, thể chế và dữ liệu Việt Nam",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Compound Returns</span>"
    ]
  },
  {
    "objectID": "10_modern_portfolio_theory.html",
    "href": "10_modern_portfolio_theory.html",
    "title": "8  Modern Portfolio Theory",
    "section": "",
    "text": "8.0.1 The Core Insight: Diversification as a Free Lunch\nIn the previous chapter, we showed how to download and analyze stock market data with figures and summary statistics. Now, we turn to one of the most fundamental questions in finance: How should an investor allocate their wealth across assets that differ in expected returns, variance, and correlations to optimize their portfolio’s performance?\nThis question might seem straightforward at first glance. Why not simply invest everything in the asset with the highest expected return? The answer lies in a profound insight that transformed financial economics: risk matters, and it can be managed through diversification.\nModern Portfolio Theory (MPT), introduced by Markowitz (1952), revolutionized investment decision-making by formalizing the trade-off between risk and expected return. Before Markowitz, investors largely thought about risk on a security-by-security basis. Markowitz’s genius was recognizing that what matters is not the risk of individual securities in isolation, but how they contribute to the risk of the entire portfolio. This insight was so influential that it earned him the Sveriges Riksbank Prize in Economic Sciences in 1990 and laid the foundation for much of modern finance.\nMPT relies on a crucial mathematical fact: portfolio risk depends not only on individual asset volatilities but also on the correlations between asset returns. This insight reveals the power of diversification—combining assets whose returns don’t move in perfect lockstep can reduce overall portfolio risk without necessarily sacrificing expected return.\nConsider a simple analogy: Imagine you run a business selling both sunscreen and umbrellas. On sunny days, sunscreen sales boom but umbrella sales suffer; on rainy days, the reverse happens. By selling both products, your total revenue becomes more stable than if you sold only one. The “correlation” between sunscreen and umbrella sales is negative, and combining them reduces the variance of your overall income. This is precisely the logic behind portfolio diversification.\nThe fruit basket analogy offers another perspective: If all you have are apples and they spoil, you lose everything. With a variety of fruits, some may spoil, but others will stay fresh. Diversification provides insurance against the idiosyncratic risks of individual assets.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "10_modern_portfolio_theory.html#the-asset-universe-setting-up-the-problem",
    "href": "10_modern_portfolio_theory.html#the-asset-universe-setting-up-the-problem",
    "title": "8  Modern Portfolio Theory",
    "section": "8.1 The Asset Universe: Setting Up the Problem",
    "text": "8.1 The Asset Universe: Setting Up the Problem\nSuppose \\(N\\) different risky assets are available to the investor. Each asset \\(i\\) is characterized by:\n\nExpected return \\(\\mu_i\\): The anticipated profit from holding the asset for one period\nVariance \\(\\sigma_i^2\\): The dispersion of returns around the mean\nCovariances \\(\\sigma_{ij}\\): The degree to which asset \\(i\\)’s returns move together with asset \\(j\\)’s returns\n\nThe investor chooses portfolio weights \\(\\omega_i\\) for each asset \\(i\\). These weights represent the fraction of total wealth invested in each asset. We impose the constraint that weights sum to one:\n\\[\n\\sum_{i=1}^N \\omega_i = 1\n\\]\nThis “budget constraint” ensures that the investor is fully invested—there is no outside option such as keeping money under a mattress. Note that we allow weights to be negative (short selling) or greater than one (leverage), though in practice these positions may face constraints.\n\n8.1.1 The Two Stages of Portfolio Selection\nAccording to Markowitz (1952), portfolio selection involves two distinct stages:\n\nEstimation: Forming expectations about future security performance based on observations, experience, and economic reasoning\nOptimization: Using these expectations to choose an optimal portfolio\n\nIn practice, these stages cannot be fully separated. The estimation stage determines the inputs (\\(\\mu\\), \\(\\Sigma\\)) that feed into the optimization stage. Poor estimation leads to poor portfolio choices, regardless of how sophisticated the optimization procedure.\nTo keep things conceptually clear, we focus primarily on the optimization stage in this chapter. We treat the expected returns and variance-covariance matrix as known, using historical data to compute reasonable proxies. In later chapters, we address the substantial challenges that arise from estimation uncertainty.\n\n\n8.1.2 Loading and Preparing the Data\nWe work with the VN30 index constituents—the 30 largest and most liquid stocks on Vietnam’s Ho Chi Minh Stock Exchange. This provides a realistic asset universe for a domestic Vietnamese investor.\n\nvn30_symbols = [\n    \"ACB\",\"BCM\",\"BID\",\"BVH\",\"CTG\",\"FPT\",\"GAS\",\"GVR\",\"HDB\",\"HPG\",\n    \"MBB\",\"MSN\",\"MWG\",\"PLX\",\"POW\",\"SAB\",\"SHB\",\"SSB\",\"STB\",\"TCB\",\n    \"TPB\",\"VCB\",\"VHM\",\"VIB\",\"VIC\",\"VJC\",\"VNM\",\"VPB\",\"VRE\",\"EIB\"\n]\n\nWe load the historical price data:\n\nimport pandas as pd\nfrom io import BytesIO\nimport datetime as dt\nimport os\nimport boto3\nfrom botocore.client import Config\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\nWe filter to keep only the VN30 constituents:\n\nprices_daily = prices[prices[\"symbol\"].isin(vn30_symbols)]\nprices_daily[[\"date\", \"symbol\", \"adjusted_close\"]].head(3)\n\n\n\n\n\n\n\n\ndate\nsymbol\nadjusted_close\n\n\n\n\n18176\n2010-01-04\nACB\n329.408244\n\n\n18177\n2010-01-05\nACB\n329.408244\n\n\n18178\n2010-01-06\nACB\n320.258015\n\n\n\n\n\n\n\n\n\n8.1.3 Computing Expected Returns\nThe sample mean return serves as our proxy for expected returns. For each asset \\(i\\), we compute:\n\\[\n\\hat{\\mu}_i = \\frac{1}{T} \\sum_{t=1}^{T} r_{i,t}\n\\]\nwhere \\(r_{i,t}\\) is the return of asset \\(i\\) in period \\(t\\), and \\(T\\) is the total number of periods.\nWhy monthly returns? While daily data provides more observations, monthly returns offer several advantages for portfolio optimization. First, monthly returns are less noisy and exhibit weaker serial correlation. Second, monthly rebalancing is more realistic for most investors, avoiding excessive transaction costs. Third, the estimation error in mean returns is already substantial—using daily data doesn’t materially improve the precision of mean estimates because the mean return scales with the horizon while estimation error scales with the square root of observations.\n\nreturns_monthly = (prices_daily\n  .assign(\n    date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n  )\n  .groupby([\"symbol\", \"date\"], as_index=False)\n  .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n  .assign(\n    ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n  )\n)\n\n\n\n8.1.4 Computing Volatilities\nIndividual asset risk in MPT is quantified using variance (\\(\\sigma^2_i\\)) or its square root, the standard deviation or volatility (\\(\\sigma_i\\)). We use the sample standard deviation as our proxy:\n\\[\n\\hat{\\sigma}_i = \\sqrt{\\frac{1}{T-1} \\sum_{t=1}^{T} (r_{i,t} - \\hat{\\mu}_i)^2}\n\\]\nAlternative risk measures exist, including Value-at-Risk, Expected Shortfall, and higher-order moments such as skewness and kurtosis. However, variance remains the workhorse measure in portfolio theory because of its mathematical tractability and the central role of the normal distribution in finance.\n\nassets = (returns_monthly\n  .groupby(\"symbol\", as_index=False)\n  .agg(\n    mu=(\"ret\", \"mean\"),\n    sigma=(\"ret\", \"std\")\n  )\n)\n\n\n\n8.1.5 Visualizing the Risk-Return Trade-off\nFigure 8.1 displays each asset’s expected return (vertical axis) against its volatility (horizontal axis). This “mean-standard deviation” space is fundamental to portfolio theory.\n\nassets_figure = (\n  ggplot(\n    assets, \n    aes(x=\"sigma\", y=\"mu\", label=\"symbol\")\n  )\n  + geom_point()\n  + geom_text(adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"Expected returns and volatilities of VN30 index constituents\"\n  )\n)\nassets_figure.show()\n\n\n\n\n\n\n\nFigure 8.1: Expected returns and volatilities based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nSeveral observations emerge from this figure. First, there is substantial heterogeneity in both expected returns and volatilities across stocks. Second, the relationship between risk and return is far from linear. Some high-volatility stocks have low or even negative expected returns. Third, most individual stocks appear to offer poor risk-return trade-offs. As we will see, portfolios can substantially improve upon these individual positions.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "10_modern_portfolio_theory.html#the-variance-covariance-matrix-capturing-asset-interactions",
    "href": "10_modern_portfolio_theory.html#the-variance-covariance-matrix-capturing-asset-interactions",
    "title": "8  Modern Portfolio Theory",
    "section": "8.2 The Variance-Covariance Matrix: Capturing Asset Interactions",
    "text": "8.2 The Variance-Covariance Matrix: Capturing Asset Interactions\n\n8.2.1 Why Correlations Matter\nA key innovation of MPT is recognizing that portfolio risk depends critically on how assets move together. The variance-covariance matrix \\(\\Sigma\\) captures all pairwise interactions between asset returns.\nTo understand why correlations matter, consider the variance of a two-asset portfolio: \\[\\sigma_p^2 = \\omega_1^2\\sigma_1^2 + \\omega_2^2\\sigma_2^2 + 2\\omega_1\\omega_2\\sigma_{12}\\]\nThe third term involves the covariance \\(\\sigma_{12} = \\rho_{12}\\sigma_1\\sigma_2\\), where \\(\\rho_{12}\\) is the correlation coefficient. When \\(\\rho_{12} &lt; 1\\), the portfolio variance is less than the weighted average of individual variances. When \\(\\rho_{12} &lt; 0\\), the diversification benefit is even more pronounced.\nThis mathematical fact has profound implications: You can reduce risk without reducing expected return by combining assets that don’t move perfectly together. This is sometimes called the “only free lunch in finance.”\n\n\n8.2.2 Computing the Variance-Covariance Matrix\nWe compute the sample covariance matrix as: \\[\\hat{\\sigma}_{ij} = \\frac{1}{T-1} \\sum_{t=1}^{T} (r_{i,t} - \\hat{\\mu}_i)(r_{j,t} - \\hat{\\mu}_j)\\]\nFirst, we reshape the returns data into a wide format with assets as columns:\n\nreturns_wide = (returns_monthly\n  .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n  .reset_index()\n)\n\nsigma = (returns_wide\n  .drop(columns=[\"date\"])\n  .cov()\n)\n\n\n\n8.2.3 Interpreting the Variance-Covariance Matrix\nThe diagonal elements of \\(\\Sigma\\) are the variances of individual assets. The off-diagonal elements are covariances, which can be positive (assets tend to move together), negative (assets tend to move in opposite directions), or zero (no linear relationship).\nFor easier interpretation, we often convert covariances to correlations: \\[\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sigma_i \\sigma_j}\\]\nCorrelations are bounded between -1 and +1, making them easier to compare across asset pairs.\nFigure 8.2 visualizes the variance-covariance matrix as a heatmap.\n\nsigma_long = (sigma\n  .reset_index()\n  .melt(id_vars=\"symbol\", var_name=\"symbol_b\", value_name=\"value\")\n)\n\nsigma_long[\"symbol_b\"] = pd.Categorical(\n  sigma_long[\"symbol_b\"], \n  categories=sigma_long[\"symbol_b\"].unique()[::-1],\n  ordered=True\n)\n\nsigma_figure = (\n  ggplot(\n    sigma_long, \n    aes(x=\"symbol\", y=\"symbol_b\", fill=\"value\")\n  )\n  + geom_tile()\n  + labs(\n      x=\"\", y=\"\", fill=\"(Co-)Variance\",\n      title=\"Sample variance-covariance matrix of VN30 index constituents\"\n    )\n  + scale_fill_continuous(labels=percent_format())\n  + theme(axis_text_x=element_text(angle=45, hjust=1))\n)\nsigma_figure.show()\n\n\n\n\n\n\n\nFigure 8.2: Variances and covariances based on monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nThe heatmap reveals important patterns. The diagonal (variances) shows which stocks are most volatile. The off-diagonal patterns show which pairs of stocks tend to move together. In general, stocks within the same sector tend to have higher correlations with each other than with stocks from different sectors.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "10_modern_portfolio_theory.html#the-minimum-variance-portfolio",
    "href": "10_modern_portfolio_theory.html#the-minimum-variance-portfolio",
    "title": "8  Modern Portfolio Theory",
    "section": "8.3 The Minimum-Variance Portfolio",
    "text": "8.3 The Minimum-Variance Portfolio\n\n8.3.1 Motivation: Risk Minimization as a Benchmark\nBefore considering expected returns, let’s find the portfolio that minimizes risk entirely. This minimum-variance portfolio (MVP) serves as an important benchmark and reference point. It represents what an extremely risk-averse investor—one who cares only about minimizing volatility—would choose.\n\n\n8.3.2 The Optimization Problem\nThe minimum-variance investor solves: \\[\n\\min_{\\omega} \\omega^{\\prime}\\Sigma\\omega\n\\]\nsubject to the constraint that weights sum to one:\n\\[\n\\omega^{\\prime}\\iota = 1\n\\]\nwhere \\(\\iota\\) is an \\(N \\times 1\\) vector of ones.\nIn words: minimize portfolio variance, subject to being fully invested.\n\n\n8.3.3 The Analytical Solution\nThis is a classic constrained optimization problem that can be solved using Lagrange multipliers. The Lagrangian is:\n\\[\n\\mathcal{L} = \\omega^{\\prime}\\Sigma\\omega - \\lambda(\\omega^{\\prime}\\iota - 1)\n\\]\nTaking the first-order condition with respect to \\(\\omega\\): \\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda\\iota = 0\n\\]\nSolving for \\(\\omega\\): \\[\n\\omega = \\frac{\\lambda}{2}\\Sigma^{-1}\\iota\n\\]\nUsing the constraint \\(\\omega^{\\prime}\\iota = 1\\) to solve for \\(\\lambda\\): \\[\n\\frac{\\lambda}{2}\\iota^{\\prime}\\Sigma^{-1}\\iota = 1 \\implies \\frac{\\lambda}{2} = \\frac{1}{\\iota^{\\prime}\\Sigma^{-1}\\iota}\n\\]\nSubstituting back: \\[\n\\omega_{\\text{mvp}} = \\frac{\\Sigma^{-1}\\iota}{\\iota^{\\prime}\\Sigma^{-1}\\iota}\n\\]\nThis elegant formula shows that the minimum-variance weights depend only on the covariance matrix—expected returns play no role. The inverse covariance matrix \\(\\Sigma^{-1}\\) determines how much to invest in each asset based on its variance and its covariances with all other assets.\n\n\n8.3.4 Implementation\n\niota = np.ones(sigma.shape[0])\nsigma_inv = np.linalg.inv(sigma.values)\nomega_mvp = (sigma_inv @ iota) / (iota @ sigma_inv @ iota)\n\n\n\n8.3.5 Visualizing the Minimum-Variance Weights\nFigure 8.3 displays the portfolio weights of the minimum-variance portfolio.\n\nassets = assets.assign(omega_mvp=omega_mvp)\n\nassets[\"symbol\"] = pd.Categorical(\n  assets[\"symbol\"],\n  categories=assets.sort_values(\"omega_mvp\")[\"symbol\"],\n  ordered=True\n)\n\nomega_figure = (\n  ggplot(\n    assets,\n    aes(y=\"omega_mvp\", x=\"symbol\", fill=\"omega_mvp&gt;0\")\n  )\n  + geom_col()\n  + coord_flip()\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"\", \n      y=\"Portfolio Weight\", \n      title=\"Minimum-variance portfolio weights\"\n  )\n  + theme(legend_position=\"none\")\n)\nomega_figure.show()\n\n\n\n\n\n\n\nFigure 8.3: Weights are based on historical moments of monthly returns adjusted for dividend payments and stock splits.\n\n\n\n\n\nSeveral features of the minimum-variance portfolio are noteworthy. First, many stocks receive zero or near-zero weights. Second, some stocks receive negative weights (short positions). These short positions are not a computational artifact, they reflect the optimizer’s attempt to exploit correlations for risk reduction. Third, the weights are quite extreme (both large positive and large negative), which often indicates estimation error amplification, which is a topic we address in later chapters.\n\n\n8.3.6 Portfolio Performance\nLet’s compute the expected return and volatility of the minimum-variance portfolio:\n\nmu = assets[\"mu\"].values\nmu_mvp = omega_mvp @ mu\nsigma_mvp = np.sqrt(omega_mvp @ sigma.values @ omega_mvp)\n\nsummary_mvp = pd.DataFrame({\n  \"mu\": [mu_mvp],\n  \"sigma\": [sigma_mvp],\n  \"type\": [\"Minimum-Variance Portfolio\"]\n})\nsummary_mvp\n\n\n\n\n\n\n\n\nmu\nsigma\ntype\n\n\n\n\n0\n-0.011424\n0.043512\nMinimum-Variance Portfolio\n\n\n\n\n\n\n\n\nmu_mvp_fmt = f\"{mu_mvp:.4f}\"\nsigma_mvp_fmt = f\"{sigma_mvp:.4f}\"\nprint(f\"The MVP return is {mu_mvp_fmt} and volatility is {sigma_mvp_fmt}.\")\n\nThe MVP return is -0.0114 and volatility is 0.0435.\n\n\nIf the expected return is negative, this is not a computational error. The minimum-variance portfolio minimizes risk without regard to expected returns. Because some assets in the sample have negative average returns, the risk-minimizing combination may inherit a negative expected return. This highlights a fundamental limitation of using historical sample means as estimates of expected returns: they are extremely noisy, and can lead to economically unintuitive results even when the optimization mathematics are working correctly.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "10_modern_portfolio_theory.html#efficient-portfolios-balancing-risk-and-return",
    "href": "10_modern_portfolio_theory.html#efficient-portfolios-balancing-risk-and-return",
    "title": "8  Modern Portfolio Theory",
    "section": "8.4 Efficient Portfolios: Balancing Risk and Return",
    "text": "8.4 Efficient Portfolios: Balancing Risk and Return\n\n8.4.1 The Investor’s Trade-off\nIn most cases, minimizing variance is not the investor’s sole objective. A more realistic formulation allows the investor to trade off risk against expected return. The investor might be willing to accept higher portfolio variance in exchange for higher expected returns.\nAn efficient portfolio minimizes variance subject to earning at least some target expected return \\(\\bar{\\mu}\\). Formally:\n\\[\\min_{\\omega} \\omega^{\\prime}\\Sigma\\omega\\]\nsubject to: \\[\\omega^{\\prime}\\iota = 1 \\quad \\text{(fully invested)}\\] \\[\\omega^{\\prime}\\mu \\geq \\bar{\\mu} \\quad \\text{(minimum return)}\\]\nWhen \\(\\bar{\\mu}\\) exceeds the expected return of the minimum-variance portfolio, the investor accepts more risk to earn more return.\n\n\n8.4.2 Setting the Target Return\nFor illustration, suppose the investor wants to earn at least the historical average return of the best-performing stock:\n\nmu_bar = assets[\"mu\"].max()\nprint(f\"Target expected return: {mu_bar:.5f}\")\n\nTarget expected return: 0.01886\n\n\nThis is an ambitious target—it means matching the return of the single highest-returning stock while benefiting from diversification to reduce risk.\n\n\n8.4.3 The Analytical Solution\nThe constrained optimization problem with an inequality constraint on expected returns can be solved using the Karush-Kuhn-Tucker (KKT) conditions. At the optimum (assuming the return constraint binds), the solution is:\n\\[\\omega_{\\text{efp}} = \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu - \\frac{D}{C}\\Sigma^{-1}\\iota\\right)\\]\nwhere:\n\n\\(C = \\iota^{\\prime}\\Sigma^{-1}\\iota\\) (a scalar measuring the “size” of the inverse covariance matrix)\n\\(D = \\iota^{\\prime}\\Sigma^{-1}\\mu\\) (capturing the interaction between expected returns and the inverse covariance matrix)\n\\(E = \\mu^{\\prime}\\Sigma^{-1}\\mu\\) (measuring the “signal” in expected returns weighted by inverse covariances)\n\\(\\lambda^* = 2\\frac{\\bar{\\mu} - D/C}{E - D^2/C}\\) (the shadow price of the return constraint)\n\nAlternatively, we can express the efficient portfolio as a linear combination of the minimum-variance portfolio and an “excess return” portfolio:\n\\[\\omega_{\\text{efp}} = \\omega_{\\text{mvp}} + \\frac{\\lambda^*}{2}\\left(\\Sigma^{-1}\\mu - D \\cdot \\omega_{\\text{mvp}}\\right)\\]\nThis representation reveals important intuition: the efficient portfolio starts from the minimum-variance portfolio and tilts toward higher-expected-return assets, with the tilt magnitude determined by \\(\\lambda^*\\).\n\n\n8.4.4 Implementation\n\nC = iota @ sigma_inv @ iota\nD = iota @ sigma_inv @ mu\nE = mu @ sigma_inv @ mu\nlambda_tilde = 2 * (mu_bar - D / C) / (E - (D ** 2) / C)\nomega_efp = omega_mvp + (lambda_tilde / 2) * (sigma_inv @ mu - D * omega_mvp)\n\nmu_efp = omega_efp @ mu\nsigma_efp = np.sqrt(omega_efp @ sigma.values @ omega_efp)\n\nsummary_efp = pd.DataFrame({\n  \"mu\": [mu_efp],\n  \"sigma\": [sigma_efp],\n  \"type\": [\"Efficient Portfolio\"]\n})\n\n\n\n8.4.5 Comparing the Portfolios\nFigure 8.4 plots both portfolios alongside the individual assets.\n\nsummaries = pd.concat(\n  [assets, summary_mvp, summary_efp], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=summaries.query(\"type.isna()\"))\n  + geom_point(data=summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3)\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"Efficient & minimum-variance portfolios\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 8.4: The big dots indicate the location of the minimum-variance and the efficient portfolio that delivers the expected return of the stock with the highest return, respectively. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe figure demonstrates the substantial diversification benefits of portfolio optimization. The efficient portfolio achieves the same expected return as the highest-returning individual stock but with substantially lower volatility. This “free lunch” from diversification is the central insight of Modern Portfolio Theory.\n\n\n8.4.6 The Role of Risk Aversion\nThe target return \\(\\bar{\\mu}\\) implicitly reflects the investor’s risk aversion. Less risk-averse investors choose higher \\(\\bar{\\mu}\\), accepting more variance to earn more expected return. More risk-averse investors choose \\(\\bar{\\mu}\\) closer to the minimum-variance portfolio’s expected return.\nEquivalently, the mean-variance framework can be derived from the optimal decisions of an investor with a mean-variance utility function: \\[U(\\omega) = \\omega^{\\prime}\\mu - \\frac{\\gamma}{2}\\omega^{\\prime}\\Sigma\\omega\\]\nwhere \\(\\gamma\\) is the coefficient of relative risk aversion. The Appendix shows there is a one-to-one mapping between \\(\\gamma\\) and \\(\\bar{\\mu}\\), so both formulations yield identical efficient portfolios.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "10_modern_portfolio_theory.html#the-efficient-frontier-the-menu-of-optimal-portfolios",
    "href": "10_modern_portfolio_theory.html#the-efficient-frontier-the-menu-of-optimal-portfolios",
    "title": "8  Modern Portfolio Theory",
    "section": "8.5 The Efficient Frontier: The Menu of Optimal Portfolios",
    "text": "8.5 The Efficient Frontier: The Menu of Optimal Portfolios\nThe efficient frontier is the set of all portfolios for which no other portfolio offers higher expected return at the same or lower variance. Geometrically, it traces the upper boundary of achievable (volatility, expected return) combinations.\nEvery rational mean-variance investor should hold a portfolio on the efficient frontier. Portfolios below the frontier are “dominated,” there exists another portfolio with either higher return for the same risk, or lower risk for the same return.\n\n8.5.1 The Mutual Fund Separation Theorem\nA remarkable result simplifies the construction of the efficient frontier. The mutual fund separation theorem (sometimes called the two-fund theorem) states that any efficient portfolio can be expressed as a linear combination of any two distinct efficient portfolios.\nFormally, if \\(\\omega_{\\mu_1}\\) and \\(\\omega_{\\mu_2}\\) are efficient portfolios earning expected returns \\(\\mu_1\\) and \\(\\mu_2\\) respectively, then the portfolio: \\[\\omega_{a\\mu_1 + (1-a)\\mu_2} = a \\cdot \\omega_{\\mu_1} + (1-a) \\cdot \\omega_{\\mu_2}\\]\nis also efficient and earns expected return \\(a\\mu_1 + (1-a)\\mu_2\\).\nThis result has profound practical implications: an investor needs access to only two efficient “mutual funds” to construct any portfolio on the efficient frontier. The specific funds don’t matter—any two distinct efficient portfolios span the entire frontier.\n\n\n8.5.2 Proof of the Separation Theorem\nThe proof follows directly from the analytical solution for efficient portfolios. Consider:\n\\[\na \\cdot \\omega_{\\mu_1} + (1-a) \\cdot \\omega_{\\mu_2} = \\left(\\frac{a\\mu_1 + (1-a)\\mu_2 - D/C}{E - D^2/C}\\right)\\left(\\Sigma^{-1}\\mu - \\frac{D}{C}\\Sigma^{-1}\\iota\\right)\n\\]\nThis expression has exactly the form of the efficient portfolio earning expected return \\(a\\mu_1 + (1-a)\\mu_2\\), proving the theorem.\n\n\n8.5.3 Computing the Efficient Frontier\nUsing the minimum-variance portfolio and our efficient portfolio as the two “funds,” we can trace out the entire efficient frontier:\n\nefficient_frontier = (\n  pd.DataFrame({\n    \"a\": np.arange(-1, 2.01, 0.01)\n  })\n  .assign(\n    omega=lambda x: x[\"a\"].map(lambda a: a * omega_efp + (1 - a) * omega_mvp)\n  )\n  .assign(\n    mu=lambda x: x[\"omega\"].map(lambda w: w @ mu),\n    sigma=lambda x: x[\"omega\"].map(lambda w: np.sqrt(w @ sigma @ w))\n  )\n)\n\nNote that we allow \\(a\\) to range from -1 to 2, which means some portfolios involve shorting one of the two basis funds and leveraging into the other. This traces out both the upper and lower portions of the frontier hyperbola.\n\n\n8.5.4 Visualizing the Efficient Frontier\nFigure 8.5 displays the efficient frontier alongside individual assets and the benchmark portfolios.\n\nsummaries = pd.concat(\n  [summaries, efficient_frontier], ignore_index=True\n)\n\nsummaries_figure = (\n  ggplot(\n    summaries, \n    aes(x=\"sigma\", y=\"mu\")\n  )\n  + geom_point(data=summaries.query(\"type.isna()\"))\n  + geom_line(data=efficient_frontier, color=\"blue\", alpha=0.7)\n  + geom_point(data=summaries.query(\"type.notna()\"), color=\"#F21A00\", size=3)\n  + geom_label(aes(label=\"type\"), adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}})\n  + scale_x_continuous(labels=percent_format())\n  + scale_y_continuous(labels=percent_format())\n  + labs(\n      x=\"Volatility (Standard Deviation)\", \n      y=\"Expected Return\",\n      title=\"The Efficient Frontier and VN30 Constituents\"\n    ) \n)\nsummaries_figure.show()\n\n\n\n\n\n\n\nFigure 8.5: The big dots indicate the location of the minimum-variance and the efficient portfolio. The small dots indicate the location of the individual constituents.\n\n\n\n\n\nThe efficient frontier has a characteristic hyperbolic shape. The leftmost point is the minimum-variance portfolio. Moving up and to the right along the frontier, expected return increases but so does volatility. The upper portion of the hyperbola (above the minimum-variance portfolio) is the “efficient” part—these portfolios offer the highest return for each level of risk. The lower portion is “inefficient”—these portfolios are dominated by their mirror images on the upper portion.\nThe figure also reveals how dramatically diversification improves upon individual stock positions. Nearly all individual stocks lie well inside the efficient frontier, meaning investors can achieve the same expected return with much less risk, or much higher expected return with the same risk, simply by diversifying.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "10_modern_portfolio_theory.html#key-takeaways",
    "href": "10_modern_portfolio_theory.html#key-takeaways",
    "title": "8  Modern Portfolio Theory",
    "section": "8.6 Key Takeaways",
    "text": "8.6 Key Takeaways\nThis chapter introduced the concepts of Modern Portfolio Theory. The main insights are:\n\nPortfolio risk depends on correlations: The variance of a portfolio is not simply the weighted average of individual variances. Covariances between assets play a crucial role, creating opportunities for diversification.\nDiversification is the “only free lunch” in finance: By combining assets that don’t move perfectly together, investors can reduce risk without sacrificing expected return. This insight is the cornerstone of modern investment practice.\nThe minimum-variance portfolio minimizes risk: This portfolio depends only on the covariance matrix and serves as an important benchmark. It represents the least risky way to be fully invested in risky assets.\nEfficient portfolios balance risk and return: By accepting more variance, investors can earn higher expected returns. Efficient portfolios are those that offer the best possible trade-off.\nThe efficient frontier characterizes optimal portfolios: This boundary in mean-standard deviation space represents the menu of optimal choices available to mean-variance investors.\nTwo-fund separation simplifies implementation: Any efficient portfolio can be constructed from any two distinct efficient portfolios, reducing the computational burden of portfolio optimization.\n\nLooking ahead, several important complications arise in practice. First, the inputs to portfolio optimization (expected returns and covariances) must be estimated from data, and estimation error can dramatically affect portfolio performance. Second, real-world constraints such as transaction costs, short-sale restrictions, and position limits modify the optimization problem. Third, the assumption that investors care only about mean and variance may be too restrictive when returns are non-normal or when investors have more complex preferences. We address these extensions in subsequent chapters.\n\n\n\n\n\n\nMarkowitz, Harry. 1952. “Portfolio selection.” The Journal of Finance 7 (1): 77–91. https://doi.org/10.1111/j.1540-6261.1952.tb01525.x.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Modern Portfolio Theory</span>"
    ]
  },
  {
    "objectID": "11_univariate_portfolio_sort.html",
    "href": "11_univariate_portfolio_sort.html",
    "title": "9  Univariate Portfolio Sorts",
    "section": "",
    "text": "9.1 Data Preparation\nIn this chapter, we dive into portfolio sorts, one of the most widely used statistical methodologies in empirical asset pricing (e.g., Bali, Engle, and Murray 2016). The key application of portfolio sorts is to examine whether one or more variables can predict future excess returns. In general, the idea is to sort individual stocks into portfolios, where the stocks within each portfolio are similar with respect to a sorting variable, such as firm size. The different portfolios then represent well-diversified investments that differ in the level of the sorting variable. You can then attribute the differences in the return distribution to the impact of the sorting variable. We start by introducing univariate portfolio sorts (which sort based on only one characteristic) and tackle bivariate sorting in Value and Bivariate Sorts.\nA univariate portfolio sort considers only one sorting variable \\(x_{i,t-1}\\). Here, \\(i\\) denotes the stock and \\(t-1\\) indicates that the characteristic is observable by investors at time \\(t\\). The objective is to assess the cross-sectional relation between \\(x_{i,t-1}\\) and, typically, stock excess returns \\(r_{i,t}\\) at time \\(t\\) as the outcome variable. To illustrate how portfolio sorts work, we use estimates for market betas from the previous chapter as our sorting variable.\nWe start with loading the required data from our SQLite database introduced in Accessing and Managing Financial Data and DataCore Data. In particular, we use the monthly stock price data as our asset universe. Once we form our portfolios, we use the market factor returns to compute the risk-adjusted performance (i.e., alpha). beta is the dataframe with market betas computed in the previous chapter.\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nprices_monthly = (pd.read_sql_query(\n    sql=\"SELECT symbol, date, ret_excess, mktcap_lag FROM prices_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"})\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nbeta = pd.read_sql_query(\n  sql=\"SELECT symbol, date, beta FROM beta_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "11_univariate_portfolio_sort.html#sorting-by-market-beta",
    "href": "11_univariate_portfolio_sort.html#sorting-by-market-beta",
    "title": "9  Univariate Portfolio Sorts",
    "section": "9.2 Sorting by Market Beta",
    "text": "9.2 Sorting by Market Beta\nNext, we merge our sorting variable with the return data. We use the one-month lagged betas as a sorting variable to ensure that the sorts rely only on information available when we create the portfolios. To lag stock beta by one month, we add one month to the current date and join the resulting information with our return data. This procedure ensures that month \\(t\\) information is available in month \\(t+1\\). You may be tempted to simply use a call such as prices_monthly['beta_lag'] = prices_monthly.groupby('symbol')['beta'].shift(1) instead. This procedure, however, does not work correctly if there are implicit missing values in the time series.\n\nbeta_lag = (beta\n  .assign(date=lambda x: x[\"date\"]+pd.DateOffset(months=1))\n  .get([\"symbol\", \"date\", \"beta\"])\n  .rename(columns={\"beta\": \"beta_lag\"})\n  .dropna()\n)\n\ndata_for_sorts = (prices_monthly\n  .merge(beta_lag, how=\"inner\", on=[\"symbol\", \"date\"])\n  .dropna()\n)\n\nThe first step to conduct portfolio sorts is to calculate periodic breakpoints that you can use to group the stocks into portfolios. For simplicity, we start with the median lagged market beta as the single breakpoint. We then compute the value-weighted returns for each of the two resulting portfolios, which means that the lagged market capitalization determines the weight in np.average().\n\nbeta_portfolios = (\n    data_for_sorts\n    .groupby(\"date\")\n    .apply(lambda x: (\n        x.assign(\n            portfolio=pd.qcut(x[\"beta_lag\"], q=[0, 0.5, 1], labels=[\"low\", \"high\"]),\n            date=x.name\n        )\n    ))\n    .reset_index(drop=True)\n    .groupby([\"portfolio\", \"date\"])\n    .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n    .reset_index(name=\"ret\")\n    .dropna(subset=['ret'])\n)\nbeta_portfolios.head()\n\n\n\n\n\n\n\n\nportfolio\ndate\nret\n\n\n\n\n0\nlow\n2016-08-31\n-0.016507\n\n\n1\nlow\n2016-09-30\n-0.089155\n\n\n2\nlow\n2016-11-30\n-0.015080\n\n\n3\nlow\n2017-01-31\n0.004113\n\n\n4\nlow\n2017-02-28\n0.035101",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "11_univariate_portfolio_sort.html#performance-evaluation",
    "href": "11_univariate_portfolio_sort.html#performance-evaluation",
    "title": "9  Univariate Portfolio Sorts",
    "section": "9.3 Performance Evaluation",
    "text": "9.3 Performance Evaluation\nWe can construct a long-short strategy based on the two portfolios: buy the high-beta portfolio and, at the same time, short the low-beta portfolio. Thereby, the overall position in the market is net-zero.\n\nbeta_longshort = (beta_portfolios\n  .pivot_table(index=\"date\", columns=\"portfolio\", values=\"ret\")\n  .reset_index()\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n)\n\nWe compute the average return and the corresponding standard error to test whether the long-short portfolio yields on average positive or negative excess returns. In the asset pricing literature, one typically adjusts for autocorrelation by using Newey and West (1987) \\(t\\)-statistics to test the null hypothesis that average portfolio excess returns are equal to zero. One necessary input for Newey-West standard errors is a chosen bandwidth based on the number of lags employed for the estimation. Researchers often default to choosing a pre-specified lag length of six months (which is not a data-driven approach). We do so in the fit() function by indicating the cov_type as HAC and providing the maximum lag length through an additional keywords dictionary.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\",\n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept     0.006       0.006        1.018    0.309\n\nSummary statistics:\n- Number of observations: 53\n- R-squared: 0.000, Adjusted R-squared: 0.000\n- F-statistic not available\n\n\n\nThe results indicate that we cannot reject the null hypothesis of average returns being equal to zero. Our portfolio strategy using the median as a breakpoint does not yield any abnormal returns. Is this finding surprising if you reconsider the CAPM? It certainly is. The CAPM predicts that high-beta stocks should yield higher expected returns. Our portfolio sort implicitly mimics an investment strategy that finances high-beta stocks by shorting low-beta stocks. Therefore, one should expect that the average excess returns yield a return that is above the risk-free rate.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "11_univariate_portfolio_sort.html#functional-programming-for-portfolio-sorts",
    "href": "11_univariate_portfolio_sort.html#functional-programming-for-portfolio-sorts",
    "title": "9  Univariate Portfolio Sorts",
    "section": "9.4 Functional Programming for Portfolio Sorts",
    "text": "9.4 Functional Programming for Portfolio Sorts\nNow, we take portfolio sorts to the next level. We want to be able to sort stocks into an arbitrary number of portfolios. For this case, functional programming is very handy: we define a function that gives us flexibility concerning which variable to use for the sorting, denoted by sorting_variable. We use np.quantile() to compute breakpoints for n_portfolios. Then, we assign portfolios to stocks using the pd.cut() function. The output of the following function is a new column that contains the number of the portfolio to which a stock belongs.\nIn some applications, the variable used for the sorting might be clustered (e.g., at a lower bound of 0). Then, multiple breakpoints may be identical, leading to empty portfolios. Similarly, some portfolios might have a very small number of stocks at the beginning of the sample. Cases where the number of portfolio constituents differs substantially due to the distribution of the characteristics require careful consideration and, depending on the application, might require customized sorting approaches.\n\ndef assign_portfolio(data, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolios to a bin between breakpoints.\"\"\"\n    \n    breakpoints = np.quantile(\n      data[sorting_variable].dropna(), \n      np.linspace(0, 1, n_portfolios + 1), \n      method=\"linear\"\n    )\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nWe can use the above function to sort stocks into ten portfolios each month using lagged betas and compute value-weighted returns for each portfolio. Note that we transform the portfolio column to a factor variable because it provides more convenience for the figure construction below.\n\nbeta_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio=assign_portfolio(x, \"beta_lag\", 10)\n    ), include_groups=False\n  )\n  .reset_index(level=\"date\")\n  .groupby([\"portfolio\", \"date\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    }), include_groups=False\n  )\n  .reset_index()\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "11_univariate_portfolio_sort.html#more-performance-evaluation",
    "href": "11_univariate_portfolio_sort.html#more-performance-evaluation",
    "title": "9  Univariate Portfolio Sorts",
    "section": "9.5 More Performance Evaluation",
    "text": "9.5 More Performance Evaluation\nIn the next step, we compute summary statistics for each beta portfolio. Namely, we compute CAPM-adjusted alphas, the beta of each beta portfolio, and average returns.\n\nbeta_portfolios_summary = (beta_portfolios\n  .groupby(\"portfolio\")\n  .apply(lambda x: pd.Series({\n      \"alpha\": sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[\"Intercept\"],\n      \"beta\": sm.OLS.from_formula(\n          formula=\"ret ~ 1 + mkt_excess\", \n          data=x\n        ).fit().params[\"mkt_excess\"],\n      \"ret\": x[\"ret\"].mean()\n    }), include_groups=False\n  )\n  .reset_index()\n)\nbeta_portfolios_summary\n\n\n\n\n\n\n\n\nportfolio\nalpha\nbeta\nret\n\n\n\n\n0\n1\n0.007031\n0.356225\n0.003892\n\n\n1\n2\n-0.004215\n0.227882\n-0.005509\n\n\n2\n3\n-0.010169\n0.390216\n-0.011241\n\n\n3\n4\n-0.014205\n0.388342\n-0.015732\n\n\n4\n5\n-0.007220\n0.616833\n-0.009723\n\n\n5\n6\n0.010648\n0.976502\n0.006577\n\n\n6\n7\n-0.010739\n0.679145\n-0.012645\n\n\n7\n8\n-0.002757\n0.991774\n-0.006963\n\n\n8\n9\n-0.003448\n0.904886\n-0.007174\n\n\n9\n10\n0.010675\n1.376356\n0.004944\n\n\n\n\n\n\n\nFigure 9.1 illustrates the CAPM alphas of beta-sorted portfolios.\n\nbeta_portfolios_figure = (\n  ggplot(\n    beta_portfolios_summary, \n    aes(x=\"portfolio\", y=\"alpha\", fill=\"portfolio\")\n  )\n  + geom_bar(stat=\"identity\")\n  + labs(\n      x=\"Portfolio\", y=\"CAPM alpha\", fill=\"Portfolio\",\n      title=\"CAPM alphas of beta-sorted portfolios\"\n    )\n  + scale_y_continuous(labels=percent_format())\n  + theme(legend_position=\"none\")\n)\nbeta_portfolios_figure.show()\n\n\n\n\n\n\n\nFigure 9.1: The figure shows CAPM alphas of beta-sorted portfolios. Portfolios are sorted into deciles each month based on their estimated CAPM beta. The bar charts indicate the CAPM alpha of the resulting portfolio returns during the sample period.\n\n\n\n\n\nUnlike the well-documented “betting against beta” anomaly in US markets, where low-beta portfolios exhibit positive alphas and high-beta portfolios exhibit negative alphas in a monotonic pattern, the Vietnamese market shows no clear relationship between beta and risk-adjusted returns. The alphas fluctuate without a discernible trend across deciles. This lack of pattern likely reflects the limited sample period rather than a definitive conclusion about beta pricing in Vietnam. With such a short time series, the portfolio-level CAPM regressions contain substantial estimation noise, making it difficult to detect subtle anomalies. Longer sample periods would be needed to draw reliable conclusions about whether the low-beta anomaly exists in the Vietnamese equity market.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "11_univariate_portfolio_sort.html#security-market-line-and-beta-portfolios",
    "href": "11_univariate_portfolio_sort.html#security-market-line-and-beta-portfolios",
    "title": "9  Univariate Portfolio Sorts",
    "section": "9.6 Security Market Line and Beta Portfolios",
    "text": "9.6 Security Market Line and Beta Portfolios\nThe CAPM predicts that our portfolios should lie on the security market line (SML). The slope of the SML is equal to the market risk premium and reflects the risk-return trade-off at any given time. Figure 9.2 illustrates the security market line: We see that (not surprisingly) the high-beta portfolio returns have a high correlation with the market returns. However, it seems like the average excess returns for high-beta stocks are lower than what the security market line implies would be an “appropriate” compensation for the high market risk.\n\nsml_capm = (sm.OLS.from_formula(\n    formula=\"ret ~ 1 + beta\", \n    data=beta_portfolios_summary\n  )\n  .fit()\n  .params\n)\n\nsml_figure = (\n  ggplot(\n    beta_portfolios_summary,\n    aes(x=\"beta\", y=\"ret\", color=\"factor(portfolio)\")\n  )\n  + geom_point()\n  + geom_abline(\n      intercept=0, slope=factors_ff3_monthly[\"mkt_excess\"].mean(), linetype=\"solid\"\n    )\n  + geom_abline(\n      intercept=sml_capm[\"Intercept\"], slope=sml_capm[\"beta\"], linetype=\"dashed\"\n    )\n  + labs(\n      x=\"Beta\", y=\"Excess return\", color=\"Portfolio\",\n      title=\"Average portfolio excess returns and beta estimates\"\n    )\n  + scale_x_continuous(limits=(0, 2))\n  + scale_y_continuous(labels=percent_format())\n)\nsml_figure.show()\n\n\n\n\n\n\n\nFigure 9.2: The figure shows average portfolio excess returns and beta estimates. Excess returns are computed as CAPM alphas of the beta-sorted portfolios. The horizontal axis indicates the CAPM beta of the resulting beta-sorted portfolio return time series. The dashed line indicates the slope coefficient of a linear regression of excess returns on portfolio betas.\n\n\n\n\n\nTo provide more evidence against the CAPM predictions, we again form a long-short strategy that buys the high-beta portfolio and shorts the low-beta portfolio.\n\nbeta_longshort = (beta_portfolios\n  .assign(\n    portfolio=lambda x: (\n      x[\"portfolio\"].apply(\n        lambda y: \"high\" if y == x[\"portfolio\"].max()\n        else (\"low\" if y == x[\"portfolio\"].min()\n        else y)\n      )\n    )\n  )\n  .query(\"portfolio in ['low', 'high']\")\n  .pivot_table(index=\"date\", columns=\"portfolio\", values=\"ret\")\n  .assign(long_short=lambda x: x[\"high\"]-x[\"low\"])\n  .merge(factors_ff3_monthly, how=\"left\", on=\"date\")\n)\n\nAgain, the resulting long-short strategy does not exhibit statistically significant returns.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1\n\nCoefficients:\n           Estimate  Std. Error  t-Statistic  p-Value\nIntercept     0.001       0.011        0.095    0.924\n\nSummary statistics:\n- Number of observations: 53\n- R-squared: 0.000, Adjusted R-squared: 0.000\n- F-statistic not available\n\n\n\nHowever, controlling for the effect of beta, the long-short portfolio yields a CAPM-adjusted alpha. The results can provide evidence regarding the validity of the CAPM in the Vietnamese market. The betting-against-beta factor has been documented extensively in developed markets (Frazzini and Pedersen 2014). Betting-against-beta corresponds to a strategy that shorts high-beta stocks and takes a (levered) long position in low-beta stocks. If borrowing constraints prevent investors from taking positions on the security market line, they are instead incentivized to buy high-beta stocks, which leads to a relatively higher price (and therefore lower expected returns than implied by the CAPM) for such high-beta stocks. As a result, the betting-against-beta strategy earns from providing liquidity to capital-constrained investors with lower risk aversion.\n\nmodel_fit = (sm.OLS.from_formula(\n    formula=\"long_short ~ 1 + mkt_excess\", \n    data=beta_longshort\n  )\n  .fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 1})\n)\nprettify_result(model_fit)\n\nOLS Model:\nlong_short ~ 1 + mkt_excess\n\nCoefficients:\n            Estimate  Std. Error  t-Statistic  p-Value\nIntercept      0.004       0.009        0.394    0.694\nmkt_excess     1.020       0.116        8.784    0.000\n\nSummary statistics:\n- Number of observations: 52\n- R-squared: 0.527, Adjusted R-squared: 0.518\n- F-statistic: 77.156 on 1 and 50 DF, p-value: 0.000\n\n\n\nFigure 9.3 shows the annual returns of the extreme beta portfolios we are mainly interested in. The figure illustrates the patterns over the sample period; each portfolio exhibits periods with positive and negative annual returns.\n\nbeta_longshort_year = (beta_longshort\n  .assign(year=lambda x: x[\"date\"].dt.year)\n  .groupby(\"year\")\n  .aggregate(\n    low=(\"low\", lambda x: (1+x).prod()-1),\n    high=(\"high\", lambda x: (1+x).prod()-1),\n    long_short=(\"long_short\", lambda x: (1+x).prod()-1)\n  )\n  .reset_index()\n  .melt(id_vars=\"year\", var_name=\"name\", value_name=\"value\")\n)\n\nbeta_longshort_figure = (\n  ggplot(\n    beta_longshort_year, \n    aes(x=\"year\", y=\"value\", fill=\"name\")\n  )\n  + geom_col(position=\"dodge\")\n  + facet_wrap(\"~name\", ncol=1)\n  + labs(x=\"\", y=\"\", title=\"Annual returns of beta portfolios\")\n  + scale_y_continuous(labels=percent_format())\n  + theme(legend_position=\"none\")\n)\nbeta_longshort_figure.show()\n\n\n\n\n\n\n\nFigure 9.3: The figure shows annual returns of beta portfolios. We construct portfolios by sorting stocks into high and low based on their estimated CAPM beta. Long short indicates a strategy that goes long into high beta stocks and short low beta stocks.\n\n\n\n\n\nThe high-beta portfolio and low-beta portfolio both exhibit substantial year-to-year variation. The long-short portfolio, which goes long high-beta stocks and short low-beta stocks, shows no consistent pattern of positive returns. This erratic performance reinforces our earlier finding that the beta-return relationship in the Vietnamese market does not conform to theoretical CAPM predictions during our sample period. The high volatility of annual long-short returns highlights the substantial risk inherent in such a strategy, particularly in an emerging market context with a limited sample period.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "11_univariate_portfolio_sort.html#key-takeaways",
    "href": "11_univariate_portfolio_sort.html#key-takeaways",
    "title": "9  Univariate Portfolio Sorts",
    "section": "9.7 Key Takeaways",
    "text": "9.7 Key Takeaways\n\nUnivariate portfolio sorts assess whether a single firm characteristic, like lagged market beta, can predict future excess returns.\nPortfolios are formed each month using quantile breakpoints, with returns computed using value-weighted averages to reflect realistic investment strategies.\nA long-short strategy based on beta-sorted portfolios fails to generate significant positive excess returns in the Vietnamese market, contradicting CAPM predictions that higher beta should yield higher returns.\nThe analysis provides a framework for examining the “betting against beta” anomaly, where low-beta portfolios may deliver higher alphas than high-beta portfolios, offering evidence regarding the validity of the CAPM.\nThe functional programming capabilities of Python enable scalable and flexible portfolio sorting, making it easy to analyze multiple characteristics and portfolio configurations.\nEmerging markets like Vietnam may exhibit different beta-return relationships compared to developed markets, highlighting the importance of conducting market-specific empirical analysis rather than assuming universal applicability of asset pricing anomalies.\n\n\n\n\n\n\n\nBali, Turan G, Robert F Engle, and Scott Murray. 2016. Empirical asset pricing: The cross section of stock returns. John Wiley & Sons. https://doi.org/10.1002/9781118445112.stat07954.\n\n\nFrazzini, Andrea, and Lasse Heje Pedersen. 2014. “Betting against beta.” Journal of Financial Economics 111 (1): 1–25. https://doi.org/10.1016/j.jfineco.2013.10.005.\n\n\nNewey, Whitney K., and Kenneth D. West. 1987. “A simple, positive semi-definite, heteroskedasticity and autocorrelation consistent covariance Matrix.” Econometrica 55 (3): 703–8. http://www.jstor.org/stable/1913610.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Univariate Portfolio Sorts</span>"
    ]
  },
  {
    "objectID": "12_size_sorts.html",
    "href": "12_size_sorts.html",
    "title": "10  Size Sorts",
    "section": "",
    "text": "10.1 Data Preparation\nIn this chapter, we continue with portfolio sorts in a univariate setting. Yet, we consider firm size as a sorting variable, which gives rise to a well-known return factor: the size premium. The size premium arises from buying small stocks and selling large stocks. Prominently, Fama and French (1993) include it as a factor in their three-factor model. Apart from that, asset managers commonly include size as a key firm characteristic when making investment decisions.\nWe also introduce new choices in the formation of portfolios. In particular, we discuss listing exchanges, industries, weighting regimes, and periods. These choices matter for the portfolio returns and result in different size premiums (see Hasler (2021), Soebhag, Van Vliet, and Verwijmeren (2022), and Walter, Weber, and Weiss (2022) for more insights into decision nodes and their effect on premiums).\nFirst, we retrieve the relevant data from our SQLite database introduced in Accessing and Managing Financial Data and DataCore Data. Firm size is defined as market equity in most asset pricing applications. We further use the Fama-French factor returns for performance evaluation.\ntidy_finance = sqlite3.connect(\n  database=\"data/tidy_finance_python.sqlite\"\n)\n\nprices_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM prices_monthly\", \n  con=tidy_finance, \n  parse_dates={\"date\"}\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n  sql=\"SELECT * FROM factors_ff3_monthly\", \n  con=tidy_finance, \n  parse_dates={\"date\"}\n)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Size Sorts</span>"
    ]
  },
  {
    "objectID": "12_size_sorts.html#size-distribution",
    "href": "12_size_sorts.html#size-distribution",
    "title": "10  Size Sorts",
    "section": "10.2 Size Distribution",
    "text": "10.2 Size Distribution\nBefore we build our size portfolios, we investigate the distribution of the variable firm size. Visualizing the data is a valuable starting point to understand the input to the analysis. Figure 10.1 shows the fraction of total market capitalization concentrated in the largest firm. To produce this graph, we create monthly indicators that track whether a stock belongs to the largest x percent of the firms. Then, we aggregate the firms within each bucket and compute the buckets’ share of total market capitalization.\nFigure 10.1 shows that the largest 1 percent of firms cover up to 50 percent of the total market capitalization, and holding just the 25 percent largest firms in the universe essentially replicates the market portfolio. The distribution of firm size thus implies that the largest firms of the market dominate many small firms whenever we use value-weighted benchmarks.\n\nmarket_cap_concentration = (prices_monthly\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"Largest 1%\": x.loc[x[\"mktcap\"] &gt;= x[\"mktcap\"].quantile(0.99), \"mktcap\"].sum() / x[\"mktcap\"].sum(),\n    \"Largest 5%\": x.loc[x[\"mktcap\"] &gt;= x[\"mktcap\"].quantile(0.95), \"mktcap\"].sum() / x[\"mktcap\"].sum(),\n    \"Largest 10%\": x.loc[x[\"mktcap\"] &gt;= x[\"mktcap\"].quantile(0.90), \"mktcap\"].sum() / x[\"mktcap\"].sum(),\n    \"Largest 25%\": x.loc[x[\"mktcap\"] &gt;= x[\"mktcap\"].quantile(0.75), \"mktcap\"].sum() / x[\"mktcap\"].sum()\n  }), include_groups=False)\n  .reset_index()\n  .melt(id_vars=\"date\", var_name=\"name\", value_name=\"value\")\n)\n\nmarket_cap_concentration_figure = (\n  ggplot(\n    market_cap_concentration, \n    aes(x=\"date\", y=\"value\", color=\"name\", linetype=\"name\")\n  ) +\n  geom_line() +\n  scale_y_continuous(labels=percent_format()) +\n  scale_x_date(name=\"\", date_labels=\"%Y\") +\n  labs(\n    x=\"\", y=\"\", color=\"\", linetype=\"\",\n    title=\"Percentage of total market capitalization in largest stocks\"\n  ) +\n  theme(legend_title=element_blank())\n)\nmarket_cap_concentration_figure.show()\n\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:4: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:5: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:6: RuntimeWarning: invalid value encountered in scalar divide\n/tmp/ipykernel_3244990/419327778.py:7: RuntimeWarning: invalid value encountered in scalar divide\n/home/mikenguyen/project/tidyfinance/.venv/lib/python3.13/site-packages/plotnine/geoms/geom_path.py:100: PlotnineWarning: geom_path: Removed 17 rows containing missing values.\n\n\n\n\n\n\n\n\nFigure 10.1: The figure shows the percentage of total market capitalization in largest stocks. We report the aggregate market capitalization of all stocks that belong to the 1, 5, 10, and 25 percent quantile of the largest firms in the monthly cross-section relative to the market capitalization of all stocks during the month.\n\n\n\n\n\nNext, firm sizes also differ across listing exchanges. The primary listings of stocks were important in the past and are potentially still relevant today.\n\n\n\nmarket_cap_share = (prices_monthly\n  .groupby([\"date\", \"exchange\"])\n  .aggregate({\"mktcap\": \"sum\"})\n  .reset_index(drop=False)\n  .groupby(\"date\")\n  .apply(lambda x:\n    x.assign(total_market_cap=lambda x: x[\"mktcap\"].sum(),\n             share=lambda x: x[\"mktcap\"]/x[\"total_market_cap\"]\n             )\n    )\n  .reset_index(drop=True)\n)\n\nplot_market_cap_share = (\n  ggplot(market_cap_share, \n         aes(x=\"date\", y=\"share\", \n             fill=\"exchange\", color=\"exchange\")) +\n  geom_area(position=\"stack\", stat=\"identity\", alpha=0.5) +\n  geom_line(position=\"stack\") +\n  scale_y_continuous(labels=percent_format()) +\n  scale_x_date(name=\"\", date_labels=\"%Y\") +\n  labs(x=\"\", y=\"\", fill=\"\", color=\"\",\n       title=\"Share of total market capitalization per listing exchange\") +\n  theme(legend_title=element_blank())\n)\nplot_market_cap_share.draw()\n\n\nFigure 10.2\n\n\n\nFinally, we consider the distribution of firm size across listing exchanges and create summary statistics. The function describe() does not include all statistics we are interested in, which is why we create the function compute_summary() that adds the standard deviation and the number of observations. Then, we apply it to the most current month of our data on each listing exchange. We also add a row with the overall summary statistics. In the following, we use this distinction to update our portfolio sort procedure.\n\ndef compute_summary(data, variable, filter_variable, percentiles):\n    \"\"\"Compute summary statistics for a given variable and percentiles.\"\"\"\n    \n    summary = (data[[filter_variable, variable]]\n      .groupby(filter_variable)\n      .describe(percentiles=percentiles)\n    ) \n    \n    summary.columns = summary.columns.droplevel(0)\n    \n    summary_overall = (data[variable]\n      .describe(percentiles=percentiles)\n    )\n    \n    summary.loc[\"Overall\", :] = summary_overall\n    \n    return summary.round(0)\n\ncompute_summary(\n  prices_monthly[prices_monthly[\"date\"] == prices_monthly[\"date\"].max()],\n  variable=\"mktcap\",\n  filter_variable=\"exchange\",\n  percentiles=[0.05, 0.5, 0.95]\n)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Size Sorts</span>"
    ]
  },
  {
    "objectID": "12_size_sorts.html#univariate-size-portfolios-with-flexible-breakpoints",
    "href": "12_size_sorts.html#univariate-size-portfolios-with-flexible-breakpoints",
    "title": "10  Size Sorts",
    "section": "10.3 Univariate Size Portfolios with Flexible Breakpoints",
    "text": "10.3 Univariate Size Portfolios with Flexible Breakpoints\nIn Univariate Portfolio Sorts, we construct portfolios with a varying number of breakpoints and different sorting variables. Here, we extend the framework such that we compute breakpoints on a subset of the data, for instance, based on selected listing exchanges.\nWe introduce exchanges as an argument in our assign_portfolio() function from Univariate Portfolio Sorts. The exchange-specific argument then enters in the filter data[\"exchanges\"].isin(exchanges). For example, if exchanges='HOSE' is specified, only stocks listed on HOSE are used to compute the breakpoints. Alternatively, you could specify exchanges=[\"HOSE\", \"HNX\", \"UPCoM\"], which keeps all stocks listed on either of these exchanges.\n\ndef assign_portfolio(data, exchanges, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolio for a given sorting variable.\"\"\"\n    \n    breakpoints = (data\n      .query(f\"exchange in {exchanges}\")\n      .get(sorting_variable)\n      .quantile(np.linspace(0, 1, num=n_portfolios+1), \n                interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    breakpoints.iloc[[0, -1]] = [-np.Inf, np.Inf]\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Size Sorts</span>"
    ]
  },
  {
    "objectID": "12_size_sorts.html#weighting-schemes-for-portfolios",
    "href": "12_size_sorts.html#weighting-schemes-for-portfolios",
    "title": "10  Size Sorts",
    "section": "10.4 Weighting Schemes for Portfolios",
    "text": "10.4 Weighting Schemes for Portfolios\nApart from computing breakpoints on different samples, researchers often use different portfolio weighting schemes. So far, we weighted each portfolio constituent by its relative market equity of the previous period. This protocol is called value-weighting. The alternative protocol is equal-weighting, which assigns each stock’s return the same weight, i.e., a simple average of the constituents’ returns. Notice that equal-weighting is difficult in practice as the portfolio manager needs to rebalance the portfolio monthly, while value-weighting is a truly passive investment.\nWe implement the two weighting schemes in the function compute_portfolio_returns() that takes a logical argument to weight the returns by firm value. Additionally, the long-short portfolio is long in the smallest firms and short in the largest firms, consistent with research showing that small firms outperform their larger counterparts. Apart from these two changes, the function is similar to the procedure in Univariate Portfolio Sorts.\n\ndef calculate_returns(data, value_weighted):\n    \"\"\"Calculate (value-weighted) returns.\"\"\"\n    \n    if value_weighted:\n      return np.average(data[\"ret_excess\"], weights=data[\"mktcap_lag\"])\n    else:\n      return data[\"ret_excess\"].mean()\n          \ndef compute_portfolio_returns(n_portfolios=10, \n                              exchanges=[\"HOSE\", \"HNX\", \"UPCoM\"], \n                              value_weighted=True, \n                              data=prices_monthly):\n    \"\"\"Compute (value-weighted) portfolio returns.\"\"\"\n    \n    returns = (data\n      .groupby(\"date\")\n      .apply(lambda x: x.assign(\n        portfolio=assign_portfolio(x, exchanges, \n                                   \"mktcap_lag\", n_portfolios))\n      )\n      .reset_index(drop=True)\n      .groupby([\"portfolio\", \"date\"])\n      .apply(lambda x: x.assign(\n        ret=calculate_returns(x, value_weighted))\n      )\n      .reset_index(drop=True)\n      .groupby(\"date\")\n      .apply(lambda x: \n        pd.Series({\"size_premium\": x.loc[x[\"portfolio\"].idxmin(), \"ret\"]-\n          x.loc[x[\"portfolio\"].idxmax(), \"ret\"]}))\n      .reset_index(drop=True)\n      .aggregate({\"size_premium\": \"mean\"})\n    )\n    \n    return returns\n\nTo see how the function compute_portfolio_returns() works, we consider a simple median breakpoint example with value-weighted returns. We are interested in the effect of restricting listing exchanges on the estimation of the size premium. In the first function call, we compute returns based on breakpoints from all listing exchanges. Then, we computed returns based on breakpoints from HOSE-listed stocks.\n\nret_all = compute_portfolio_returns(\n  n_portfolios=2,\n  exchanges=[\"HOSE\", \"HNX\", \"UPCoM\"],\n  value_weighted=True,\n  data=prices_monthly\n)\n\nret_HOSE = compute_portfolio_returns(\n  n_portfolios=2,\n  exchanges=[\"HOSE\"],\n  value_weighted=True,\n  data=prices_monthly\n)\n\ndata = pd.DataFrame([ret_all*100, ret_HOSE*100], \n                    index=[\"HOSE, HNX & UPCoM\", \"HOSE\"])\ndata.columns = [\"Premium\"]\ndata.round(2)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Size Sorts</span>"
    ]
  },
  {
    "objectID": "12_size_sorts.html#p-hacking-and-non-standard-errors",
    "href": "12_size_sorts.html#p-hacking-and-non-standard-errors",
    "title": "10  Size Sorts",
    "section": "10.5 P-Hacking and Non-Standard Errors",
    "text": "10.5 P-Hacking and Non-Standard Errors\nSince the choice of the listing exchange has a significant impact, the next step is to investigate the effect of other data processing decisions researchers have to make along the way. In particular, any portfolio sort analysis has to decide at least on the number of portfolios, the listing exchanges to form breakpoints, and equal- or value-weighting. Further, one may exclude firms that are active in the finance industry or restrict the analysis to some parts of the time series. All of the variations of these choices that we discuss here are part of scholarly articles published in the top finance journals. We refer to Walter, Weber, and Weiss (2022) for an extensive set of other decision nodes at the discretion of researchers.\nThe intention of this application is to show that the different ways to form portfolios result in different estimated size premiums. Despite the effects of this multitude of choices, there is no correct way. It should also be noted that none of the procedures is wrong. The aim is simply to illustrate the changes that can arise due to the variation in the evidence-generating process (Menkveld et al., n.d.). The term non-standard errors refers to the variation due to (suitable) choices made by researchers. Interestingly, in a large-scale study, Menkveld et al. (n.d.) find that the magnitude of non-standard errors is similar to the estimation uncertainty based on a chosen model. This shows how important it is to adjust for the seemingly innocent choices in the data preparation and evaluation workflow. Moreover, it seems that this methodology-related uncertainty should be embraced rather than hidden away.\nFrom a malicious perspective, these modeling choices give the researcher multiple chances to find statistically significant results. Yet this is considered p-hacking, which renders the statistical inference invalid due to multiple testing (Harvey, Liu, and Zhu 2016).\nNevertheless, the multitude of options creates a problem since there is no single correct way of sorting portfolios. How should a researcher convince a reader that their results do not come from a p-hacking exercise? To circumvent this dilemma, academics are encouraged to present evidence from different sorting schemes as robustness tests and report multiple approaches to show that a result does not depend on a single choice. Thus, the robustness of premiums is a key feature.\nBelow, we conduct a series of robustness tests, which could also be interpreted as a p-hacking exercise. To do so, we examine the size premium in different specifications presented in the table p_hacking_setup. The function itertools.product() produces all possible permutations of its arguments. Note that we use the argument data to exclude financial firms and truncate the time series.\n\nn_portfolios = [2, 5, 10]\nexchanges = [[\"HOSE\"], [\"HOSE\", \"HNX\", \"UPCoM\"]]\nvalue_weighted = [True, False]\ndata = [\n  prices_monthly,\n  prices_monthly[prices_monthly[\"industry\"] != \"Finance\"],\n  prices_monthly[prices_monthly[\"date\"] &lt; \"1990-06-01\"],\n  prices_monthly[prices_monthly[\"date\"] &gt;= \"1990-06-01\"],\n]\np_hacking_setup = list(\n  product(n_portfolios, exchanges, value_weighted, data)\n)\n\nTo speed the computation up, we parallelize the (many) different sorting procedures. Finally, we report the resulting size premiums in descending order. There are indeed substantial size premiums possible in our data, in particular when we use equal-weighted portfolios.\n\nn_cores = cpu_count()-1\np_hacking_results = pd.concat(\n  Parallel(n_jobs=n_cores)\n  (delayed(compute_portfolio_returns)(x, y, z, w) \n   for x, y, z, w in p_hacking_setup)\n)\np_hacking_results = p_hacking_results.reset_index(name=\"size_premium\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Size Sorts</span>"
    ]
  },
  {
    "objectID": "12_size_sorts.html#size-premium-variation",
    "href": "12_size_sorts.html#size-premium-variation",
    "title": "10  Size Sorts",
    "section": "10.6 Size-Premium Variation",
    "text": "10.6 Size-Premium Variation\nWe provide a graph in Figure 10.3 that shows the different premiums. The figure also shows the relation to the average Fama-French SMB (small minus big) premium used in the literature, which we include as a dotted vertical line.\n\n\n\np_hacking_results_figure = (\n  ggplot(\n    p_hacking_results, \n    aes(x=\"size_premium\")\n  )\n  + geom_histogram(bins=len(p_hacking_results))\n  + scale_x_continuous(labels=percent_format())\n  + labs(\n      x=\"\", y=\"\",\n      title=\"Distribution of size premiums for various sorting choices\"\n    )\n  + geom_vline(\n      aes(xintercept=factors_ff3_monthly[\"smb\"].mean()), linetype=\"dashed\"\n    )\n)\np_hacking_results_figure.show()\n\n\nFigure 10.3",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Size Sorts</span>"
    ]
  },
  {
    "objectID": "12_size_sorts.html#key-takeaways",
    "href": "12_size_sorts.html#key-takeaways",
    "title": "10  Size Sorts",
    "section": "10.7 Key Takeaways",
    "text": "10.7 Key Takeaways\n\nFirm size is a crucial factor in asset pricing, and sorting stocks by size reveals the size premium, where small-cap stocks tend to outperform large-cap stocks.\nPortfolio returns are sensitive to research design choices like exchange filters, weighting schemes, and the number of portfolios—decisions that can meaningfully shift results.\nMethodological flexibility can lead to non-standard errors and potential p-hacking.\nValidate results by varying assumptions and show that findings hold across multiple specifications.\n\n\n\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\nHarvey, Campbell R., Yan Liu, and Heqing Zhu. 2016. “\\(\\ldots\\) and the cross-section of expected returns.” Review of Financial Studies 29 (1): 5–68. https://doi.org/10.1093/rfs/hhv059.\n\n\nHasler, Mathias. 2021. “Is the Value Premium Smaller Than We Thought?” Working Paper. https://ssrn.com/abstract=3886984.\n\n\nMenkveld, Albert J., Anna Dreber, Felix Holzmeister, Juergen Huber, Magnus Johannesson, Michael Kirchler, Sebastian Neusüss, Michael Razen, and Utz Weitzel. n.d. “Nonstandard errors.” The Journal of Finance 79 (3): 2339–90. https://doi.org/https://doi.org/10.1111/jofi.13337.\n\n\nSoebhag, Amar, Bart Van Vliet, and Patrick Verwijmeren. 2022. “Mind Your Sorts.” Working Paper. https://di.org/10.2139/ssrn.4136672.\n\n\nWalter, Dominik, Rüdiger Weber, and Patrick Weiss. 2022. “Non-standard errors in portfolio sorts.” Working Paper. https://ssrn.com/abstract=4164117.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Size Sorts</span>"
    ]
  },
  {
    "objectID": "13_value_bivariate.html",
    "href": "13_value_bivariate.html",
    "title": "11  Value and Bivariate Sorts",
    "section": "",
    "text": "11.1 Data Preparation\nIn this chapter, we extend the univariate portfolio analysis of Univariate Portfolio Sorts to bivariate portfolio sorting, in which stocks are assigned to portfolios based on two characteristics. Bivariate sorts are commonly used in the academic asset pricing literature and underpin the factors in the Fama-French three-factor model. However, some scholars also use sorts with three grouping variables. Conceptually, portfolio sorts are easily applicable in higher dimensions.\nWe form portfolios on firm size and the book-to-market ratio. Calculating book-to-market ratios requires accounting data, which necessitates additional steps during portfolio formation. In the end, we demonstrate how to form portfolios on two sorting variables using so-called independent and dependent portfolio sorts.\nFirst, we load the necessary data from our SQLite database introduced in Accessing and Managing Financial Data and DataCore Data. We conduct portfolio sorts based on our sample but keep only the necessary columns in our memory. We use the same data sources for firm size as in Size Sorts and P-Hacking.\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nprices_monthly = (pd.read_sql_query(\n    sql=(\"SELECT symbol, date, ret_excess, mktcap, \" \n         \"mktcap_lag, exchange FROM prices_monthly\"),\n    con=tidy_finance,\n    parse_dates={\"date\"})\n  .dropna()\n)\nFurther, we utilize accounting data. We only need book equity data in this application, which we select from our database. Additionally, we convert the variable datadate to its monthly value, as we only consider monthly returns here and do not need to account for the exact date.\nbook_equity = (pd.read_sql_query(\n    sql=\"SELECT symbol, datadate, be FROM datacore\",\n    con=tidy_finance, \n    parse_dates={\"datadate\"})\n  .dropna()\n  .assign(\n    date=lambda x: (\n      pd.to_datetime(x[\"datadate\"]).dt.to_period(\"M\").dt.to_timestamp()\n    )\n  )\n)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Value and Bivariate Sorts</span>"
    ]
  },
  {
    "objectID": "13_value_bivariate.html#book-to-market-ratio",
    "href": "13_value_bivariate.html#book-to-market-ratio",
    "title": "11  Value and Bivariate Sorts",
    "section": "11.2 Book-to-Market Ratio",
    "text": "11.2 Book-to-Market Ratio\nA fundamental problem in handling accounting data is the look-ahead bias; we must not include data in forming a portfolio that was not available knowledge at the time. Of course, researchers have more information when looking into the past than agents actually had at that moment. However, abnormal excess returns from a trading strategy should not rely on an information advantage because the differential cannot be the result of informed agents’ trades. Hence, we have to lag accounting information.\nAs in the previous chapter, we continue to lag firm size by one month. Then, we compute the book-to-market ratio, which relates a firm’s book equity to its market equity. Firms with high (low) book-to-market ratio are called value (growth) firms. After matching the accounting and market equity information from the same month, we lag book-to-market by six months. This is a sufficiently conservative approach because accounting information is usually released well before six months pass. However, in the asset pricing literature, even longer lags are used as well.\nHaving both variables, i.e., firm size lagged by one month and book-to-market lagged by six months, we merge these sorting variables to our returns using the sorting_date-column created for this purpose. The final step in our data preparation deals with differences in the frequency of our variables. Returns and firm size are recorded monthly. Yet, the accounting information is only released on an annual basis. Hence, we only match book-to-market to one month per year and have eleven empty observations. To solve this frequency issue, we carry the latest book-to-market ratio of each firm to the subsequent months, i.e., we fill the missing observations with the most current report. This is done via the fillna(method=\"ffill\")-function after sorting by date and firm (which we identify by symbol and symbol) and on a firm basis (which we do by .groupby() as usual). We filter out all observations with accounting data that is older than a year. As the last step, we remove all rows with missing entries because the returns cannot be matched to any annual report.\n\nsize = (prices_monthly\n  .assign(sorting_date=lambda x: x[\"date\"]+pd.DateOffset(months=1))\n  .rename(columns={\"mktcap\": \"size\"})\n  .get([\"symbol\", \"sorting_date\", \"size\"])\n)\n\nbm = (book_equity\n  .merge(prices_monthly, how=\"inner\", on=[\"symbol\", \"date\"])\n  .assign(bm=lambda x: x[\"be\"]/x[\"mktcap\"],\n          sorting_date=lambda x: x[\"date\"]+pd.DateOffset(months=6))\n  .assign(accounting_date=lambda x: x[\"sorting_date\"])\n  .get([\"symbol\", \"symbol\", \"sorting_date\", \"accounting_date\", \"bm\"])\n)\n\ndata_for_sorts = (prices_monthly\n  .merge(bm, \n         how=\"left\", \n         left_on=[\"symbol\", \"symbol\", \"date\"], \n         right_on=[\"symbol\", \"symbol\", \"sorting_date\"])\n  .merge(size, \n         how=\"left\", \n         left_on=[\"symbol\", \"date\"], \n         right_on=[\"symbol\", \"sorting_date\"])\n  .get([\"symbol\", \"symbol\", \"date\", \"ret_excess\", \n        \"mktcap_lag\", \"size\", \"bm\", \"exchange\", \"accounting_date\"])\n)\n\ndata_for_sorts = (data_for_sorts\n  .sort_values(by=[\"symbol\", \"symbol\", \"date\"])\n  .groupby([\"symbol\", \"symbol\"])\n  .apply(lambda x: x.assign(\n      bm=x[\"bm\"].fillna(method=\"ffill\"), \n      accounting_date=x[\"accounting_date\"].fillna(method=\"ffill\")\n    )\n  )\n  .reset_index(drop=True)\n  .assign(threshold_date = lambda x: (x[\"date\"]-pd.DateOffset(months=12)))\n  .query(\"accounting_date &gt; threshold_date\")\n  .drop(columns=[\"accounting_date\", \"threshold_date\"])\n  .dropna()\n)\n\nThe last step of preparation for the portfolio sorts is the computation of breakpoints. We continue to use the same function, allowing for the specification of exchanges to be used for the breakpoints. Additionally, we reintroduce the argument sorting_variable into the function for defining different sorting variables.\n\ndef assign_portfolio(data, exchanges, sorting_variable, n_portfolios):\n    \"\"\"Assign portfolio for a given sorting variable.\"\"\"\n    \n    breakpoints = (data\n      .query(f\"exchange in {exchanges}\")\n      .get(sorting_variable)\n      .quantile(np.linspace(0, 1, num=n_portfolios+1), \n                interpolation=\"linear\")\n      .drop_duplicates()\n    )\n    breakpoints.iloc[0] = -np.inf\n    breakpoints.iloc[breakpoints.size-1] = np.inf\n    \n    assigned_portfolios = pd.cut(\n      data[sorting_variable],\n      bins=breakpoints,\n      labels=range(1, breakpoints.size),\n      include_lowest=True,\n      right=False\n    )\n    \n    return assigned_portfolios\n\nAfter these data preparation steps, we present bivariate portfolio sorts on an independent and dependent basis.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Value and Bivariate Sorts</span>"
    ]
  },
  {
    "objectID": "13_value_bivariate.html#independent-sorts",
    "href": "13_value_bivariate.html#independent-sorts",
    "title": "11  Value and Bivariate Sorts",
    "section": "11.3 Independent Sorts",
    "text": "11.3 Independent Sorts\nBivariate sorts create portfolios within a two-dimensional space spanned by two sorting variables. It is then possible to assess the return impact of either sorting variable by the return differential from a trading strategy that invests in the portfolios at either end of the respective variables spectrum. We create a five-by-five matrix using book-to-market and firm size as sorting variables in our example below. We end up with 25 portfolios. Since we are interested in the value premium (i.e., the return differential between high and low book-to-market firms), we go long the five portfolios of the highest book-to-market firms and short the five portfolios of the lowest book-to-market firms. The five portfolios at each end are due to the size splits we employed alongside the book-to-market splits.\nTo implement the independent bivariate portfolio sort, we assign monthly portfolios for each of our sorting variables separately to create the variables portfolio_bm and portfolio_size, respectively. Then, these separate portfolios are combined to the final sort stored in portfolio_combined. After assigning the portfolios, we compute the average return within each portfolio for each month. Additionally, we keep the book-to-market portfolio as it makes the computation of the value premium easier. The alternative would be to disaggregate the combined portfolio in a separate step. Notice that we weigh the stocks within each portfolio by their market capitalization, i.e., we decide to value-weight our returns.\n\nvalue_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio_bm=assign_portfolio(\n        data=x, sorting_variable=\"bm\", n_portfolios=5, exchanges=[\"HOSE\"]\n      ),\n      portfolio_size=assign_portfolio(\n        data=x, sorting_variable=\"size\", n_portfolios=5, exchanges=[\"HOSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"date\", \"portfolio_bm\", \"portfolio_size\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nEquipped with our monthly portfolio returns, we are ready to compute the value premium. However, we still have to decide how to invest in the five high and the five low book-to-market portfolios. The most common approach is to weigh these portfolios equally, but this is yet another researcher’s choice. Then, we compute the return differential between the high and low book-to-market portfolios and show the average value premium.\n\nvalue_premium = (value_portfolios\n  .groupby([\"date\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"value_premium\": (\n        x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() - \n          x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean()\n      )\n    })\n  )\n  .aggregate({\"value_premium\": \"mean\"})\n)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Value and Bivariate Sorts</span>"
    ]
  },
  {
    "objectID": "13_value_bivariate.html#dependent-sorts",
    "href": "13_value_bivariate.html#dependent-sorts",
    "title": "11  Value and Bivariate Sorts",
    "section": "11.4 Dependent Sorts",
    "text": "11.4 Dependent Sorts\nIn the previous exercise, we assigned the portfolios without considering the second variable in the assignment. This protocol is called independent portfolio sorts. The alternative, i.e., dependent sorts, creates portfolios for the second sorting variable within each bucket of the first sorting variable. In our example below, we sort firms into five size buckets, and within each of those buckets, we assign firms to five book-to-market portfolios. Hence, we have monthly breakpoints that are specific to each size group. The decision between independent and dependent portfolio sorts is another choice for the researcher. Notice that dependent sorts guarantee that portfolios have roughly equal numbers of stocks when breakpoints are computed from all exchanges. However, if breakpoints are based only on HOSE stocks, portfolio counts will generally be uneven — reflecting the large presence of small-cap stocks on HNX and UPCoM (see Exercise below).\nTo implement the dependent sorts, we first create the size portfolios by calling assign_portfolio() with sorting_variable=\"me\". Then, we group our data again by month and by the size portfolio before assigning the book-to-market portfolio. The rest of the implementation is the same as before. Finally, we compute the value premium.\n\nvalue_portfolios = (data_for_sorts\n  .groupby(\"date\")\n  .apply(lambda x: x.assign(\n      portfolio_size=assign_portfolio(\n        data=x, sorting_variable=\"size\", n_portfolios=5, exchanges=[\"HOSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"date\", \"portfolio_size\"])\n  .apply(lambda x: x.assign(\n      portfolio_bm=assign_portfolio(\n        data=x, sorting_variable=\"bm\", n_portfolios=5, exchanges=[\"HOSE\"]\n      )\n    )\n  )\n  .reset_index(drop=True)\n  .groupby([\"date\", \"portfolio_bm\", \"portfolio_size\"])\n  .apply(lambda x: pd.Series({\n      \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n    })\n  )\n  .reset_index()\n)\n\nvalue_premium = (value_portfolios\n  .groupby([\"date\", \"portfolio_bm\"])\n  .aggregate({\"ret\": \"mean\"})\n  .reset_index()\n  .groupby(\"date\")\n  .apply(lambda x: pd.Series({\n    \"value_premium\": (\n        x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].max(), \"ret\"].mean() -\n          x.loc[x[\"portfolio_bm\"] == x[\"portfolio_bm\"].min(), \"ret\"].mean()\n      )\n    })\n  )\n  .aggregate({\"value_premium\": \"mean\"})\n)\n\nOverall, we show how to conduct bivariate portfolio sorts in this chapter. In one case, we sort the portfolios independently of each other. Yet we also discuss how to create dependent portfolio sorts. Along the lines of Size Sorts, we see how many choices a researcher has to make to implement portfolio sorts, and bivariate sorts increase the number of choices.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Value and Bivariate Sorts</span>"
    ]
  },
  {
    "objectID": "13_value_bivariate.html#key-takeaways",
    "href": "13_value_bivariate.html#key-takeaways",
    "title": "11  Value and Bivariate Sorts",
    "section": "11.5 Key Takeaways",
    "text": "11.5 Key Takeaways\n\nBivariate portfolio sorts assign stocks based on two characteristics, such as firm size and book-to-market ratio, to better capture return patterns in asset pricing.\nIndependent sorts treat each variable separately, while dependent sorts condition the second sort on the first.\nProper handling of accounting data, especially lagging the book-to-market ratio, is essential to avoid look-ahead bias and ensure valid backtesting.\nValue premiums are derived by comparing returns of high versus low book-to-market portfolios, with results sensitive to sorting choices and weighting schemes.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Value and Bivariate Sorts</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html",
    "href": "14_portfolio_weighting_and_rebalancing.html",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "",
    "text": "12.1 Theoretical Framework\nEvery portfolio construction decision ultimately reduces to two choices: which assets to hold, and how much to allocate to each. While earlier chapters have focused on the first question, using factor models, anomalies, and fundamental analysis to select stocks, this chapter addresses the second. The weighting scheme a researcher or investor applies can fundamentally alter the conclusions drawn from portfolio-level tests and the returns earned from an investment strategy.\nThe distinction matters more in Vietnam than in large, liquid markets. The Vietnamese equity market features extreme skewness in the market capitalization distribution: the top 10 firms on HOSE account for roughly 50% of total market capitalization, while hundreds of small firms contribute negligible weight. Under value-weighting, a portfolio’s performance is dominated by a handful of large-cap names (Vinhomes, Vingroup, Vietcombank, FPT). Under equal-weighting, every firm contributes equally, tilting the portfolio toward small, illiquid stocks that may be expensive or impossible to trade at scale. Neither scheme is inherently correct; the choice depends on the question being asked.\nThis chapter develops the analytical framework for making that choice. We begin with the theoretical properties of weighting schemes, implement each scheme in practice with Vietnamese data, quantify the transaction costs of rebalancing, and extend the analysis to risk-based alternatives that explicitly incorporate the covariance structure of returns.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-theory",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-theory",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "",
    "text": "12.1.1 Value-Weighted Portfolios\nA value-weighted (VW) portfolio allocates to each stock in proportion to its market capitalization:\n\\[\nw_{i,t}^{VW} = \\frac{\\text{MCap}_{i,t}}{\\sum_{j=1}^{N_t} \\text{MCap}_{j,t}}\n\\tag{12.1}\\]\nwhere \\(\\text{MCap}_{i,t} = P_{i,t} \\times \\text{Shares}_{i,t}\\) is the market capitalization of stock \\(i\\) at time \\(t\\).\nThe VW portfolio has a unique theoretical status: it is the portfolio that all investors collectively hold (the “market portfolio” in CAPM). Its key properties are:\n\nSelf-rebalancing. As prices move, weights adjust automatically. A VW portfolio requires trading only when constituents enter or leave the index, or when corporate actions (splits, issuances) change shares outstanding.\nLow turnover. Because weights drift with prices rather than being reset to targets, VW portfolios have minimal rebalancing costs.\nLarge-cap bias. Returns are dominated by the largest firms. In Vietnam, this means the portfolio’s risk-return profile is heavily influenced by banking, real estate, and technology conglomerates.\n\nHsu (2004) argues that VW portfolios are sub-optimal because they mechanically overweight overpriced stocks and underweight underpriced stocks (i.e., any deviation of price from fundamental value creates a systematic drag on VW performance relative to a fundamentally weighted alternative).\n\n\n12.1.2 Equal-Weighted Portfolios\nAn equal-weighted (EW) portfolio assigns the same weight to each constituent:\n\\[\nw_{i,t}^{EW} = \\frac{1}{N_t}\n\\tag{12.2}\\]\nDeMiguel, Garlappi, and Uppal (2009) show that the 1/N portfolio is surprisingly competitive with mean-variance optimized portfolios, particularly when estimation windows are short and the number of assets is large (conditions that closely describe the Vietnamese market). The intuition is that estimation error in expected returns and covariances can overwhelm the gains from optimization, making the “naive” equal-weight scheme a robust default.\nPlyakha, Uppal, and Vilkov (2021) decompose the EW outperformance over VW into two components:\n\nSize tilt. EW allocates more to small firms, which historically earn a size premium.\nRebalancing bonus. Monthly rebalancing back to equal weights is a contrarian strategy: it sells recent winners and buys recent losers, profiting from mean reversion in individual stock returns. \n\nHowever, the EW portfolio has practical disadvantages that are particularly severe in Vietnam:\n\nHigh turnover. Every rebalancing date requires trading every stock back to equal weight.\nIlliquidity exposure. Equal weighting of micro-cap stocks that trade VND 100 million/day alongside large-caps trading VND 500 billion/day creates severe implementation challenges.\nPrice impact. In a market with daily price limits (\\(\\pm\\) 7% on HOSE, \\(\\pm\\) 10% on HNX), rebalancing trades for illiquid names may hit limit-up or limit-down, preventing full execution.\n\n\n\n12.1.3 The Weighting Spectrum\nBetween VW and EW lies a continuum of weighting schemes. Table 12.1 summarizes the major alternatives.\n\n\n\nTable 12.1: Summary of portfolio weighting schemes.\n\n\n\n\n\n\n\n\n\n\n\nScheme\nWeight Formula\nKey Property\nKey Risk\n\n\n\n\nValue-weighted\n\\(w_i \\propto \\text{MCap}_i\\)\nSelf-rebalancing, low turnover\nLarge-cap concentration\n\n\nEqual-weighted\n\\(w_i = 1/N\\)\nMaximum naive diversification\nHigh turnover, illiquidity\n\n\nFundamental\n\\(w_i \\propto F_i\\) (revenue, book equity, etc.)\nBreaks price-value link\nRequires accounting data\n\n\nMinimum variance\n\\(\\mathbf{w} = \\arg\\min \\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}\\)\nLowest portfolio volatility\nEstimation error in \\(\\boldsymbol{\\Sigma}\\)\n\n\nRisk parity\n\\(w_i \\sigma_i = w_j \\sigma_j \\; \\forall \\, i,j\\)\nEqual risk contribution\nLeverages low-vol assets\n\n\nMaximum diversification\n\\(\\max \\frac{\\mathbf{w}'\\boldsymbol{\\sigma}}{\\sqrt{\\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}}}\\)\nMaximizes diversification ratio\nSensitive to correlation estimates\n\n\nCapped VW\n\\(w_i \\propto \\text{MCap}_i\\), \\(w_i \\leq \\bar{w}\\)\nReduces concentration\nArbitrary cap threshold",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-data",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-data",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.2 Data Construction",
    "text": "12.2 Data Construction\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy.optimize import minimize\nfrom sklearn.covariance import LedoitWolf\nfrom linearmodels.panel import PanelOLS\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (10, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Daily prices and volume\ndaily = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    fields=[\n        'ticker', 'date', 'close', 'adjusted_close', 'volume',\n        'turnover_value', 'market_cap', 'shares_outstanding',\n        'bid_ask_spread', 'free_float_pct'\n    ]\n)\n\n# Monthly returns (pre-computed for convenience)\nmonthly = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'volume_avg_20d', 'turnover_value_avg_20d'\n    ]\n)\n\n# Fundamentals for fundamental weighting\nfundamentals = client.get_fundamentals(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    frequency='annual',\n    fields=[\n        'ticker', 'fiscal_year', 'revenue', 'book_equity',\n        'total_assets', 'dividends_paid', 'operating_cash_flow'\n    ]\n)\n\n# Market-level returns for benchmarking\nmarket_index = client.get_index(\n    index='VNINDEX',\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    frequency='monthly'\n)\n\nprint(f\"Daily observations: {daily.shape[0]:,}\")\nprint(f\"Monthly observations: {monthly.shape[0]:,}\")\nprint(f\"Unique tickers: {monthly['ticker'].nunique()}\")\n\n\n12.2.1 Universe Construction and Liquidity Filters\nA critical pre-processing step is defining the investable universe. Including all listed stocks, regardless of liquidity, inflates the apparent benefits of equal-weighting and other small-cap-tilted schemes because it implicitly assumes the ability to trade illiquid stocks without friction. We apply graduated liquidity filters and track how results change.\n\ndef construct_universe(monthly_df, min_mcap_pct=0, min_turnover=0, min_months=12):\n    \"\"\"\n    Construct investable universe with liquidity filters.\n\n    Parameters\n    ----------\n    min_mcap_pct : float\n        Exclude stocks below this market cap percentile (0-100).\n    min_turnover : float\n        Minimum average daily turnover in VND billion.\n    min_months : int\n        Minimum months of return history required.\n    \"\"\"\n    df = monthly_df.copy()\n\n    # Market cap percentile filter (within each month)\n    if min_mcap_pct &gt; 0:\n        df[\"mcap_pctile\"] = df.groupby(\"month_end\")[\"market_cap\"].transform(\n            lambda x: x.rank(pct=True) * 100\n        )\n        df = df[df[\"mcap_pctile\"] &gt;= min_mcap_pct]\n\n    # Turnover filter\n    if min_turnover &gt; 0:\n        df = df[df[\"turnover_value_avg_20d\"] &gt;= min_turnover * 1e9]\n\n    # History filter\n    ticker_months = df.groupby(\"ticker\")[\"month_end\"].transform(\"count\")\n    df = df[ticker_months &gt;= min_months]\n\n    return df\n\n\n# Define three universes of increasing restrictiveness\nuniverse_all = construct_universe(monthly)\nuniverse_mid = construct_universe(monthly, min_mcap_pct=20, min_turnover=0.5)\nuniverse_liquid = construct_universe(monthly, min_mcap_pct=40, min_turnover=2.0)\n\nfor name, univ in [\n    (\"All stocks\", universe_all),\n    (\"Mid filter\", universe_mid),\n    (\"Liquid only\", universe_liquid),\n]:\n    n_stocks = univ.groupby(\"month_end\")[\"ticker\"].nunique().median()\n    print(f\"{name}: median {n_stocks:.0f} stocks/month\")\n\n\n\n12.2.2 Market Capitalization Concentration\nBefore comparing weighting schemes, it is instructive to document how concentrated the Vietnamese market actually is.\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Cumulative market cap share (latest month)\nlatest = monthly[monthly['month_end'] == monthly['month_end'].max()].copy()\nlatest = latest.sort_values('market_cap', ascending=False)\nlatest['cum_mcap_share'] = (\n    latest['market_cap'].cumsum() / latest['market_cap'].sum()\n)\nlatest['rank'] = range(1, len(latest) + 1)\nlatest['rank_pct'] = latest['rank'] / len(latest) * 100\n\naxes[0].plot(latest['rank_pct'], latest['cum_mcap_share'] * 100,\n             color='#2C5F8A', linewidth=2)\naxes[0].axhline(y=50, color='gray', linestyle='--', linewidth=0.8)\naxes[0].axhline(y=80, color='gray', linestyle='--', linewidth=0.8)\n\n# Mark top 10 and top 30\nn_at_50 = (latest['cum_mcap_share'] &lt;= 0.50).sum()\naxes[0].annotate(f'Top {n_at_50} stocks = 50%',\n                 xy=(n_at_50 / len(latest) * 100, 50),\n                 fontsize=9, color='#C0392B')\naxes[0].set_xlabel('Cumulative Stock Rank (%)')\naxes[0].set_ylabel('Cumulative Market Cap Share (%)')\naxes[0].set_title('Panel A: Market Cap Concentration Curve')\n\n# Panel B: HHI over time\nhhi_ts = (\n    monthly\n    .groupby('month_end')\n    .apply(lambda g: (g['market_cap'] / g['market_cap'].sum()).pow(2).sum())\n    .reset_index(name='hhi')\n)\nhhi_ts['month_end'] = pd.to_datetime(hhi_ts['month_end'])\naxes[1].plot(hhi_ts['month_end'], hhi_ts['hhi'] * 10000,\n             color='#2C5F8A', linewidth=1.5)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('HHI (basis points)')\naxes[1].set_title('Panel B: Herfindahl Index of VW Weights')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 12.1",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-implementation",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-implementation",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.3 Implementing Weighting Schemes",
    "text": "12.3 Implementing Weighting Schemes\nWe now implement each weighting scheme and compute monthly portfolio returns. All implementations follow a common structure: at each rebalancing date, compute target weights from available information, then compute the weighted return over the subsequent holding period.\n\n12.3.1 Core Portfolio Engine\n\ndef compute_portfolio_returns(\n    monthly_df,\n    weight_fn,\n    rebal_freq=\"M\",\n    max_weight=1.0,\n    min_weight=0.0,\n):\n    \"\"\"\n    Compute time series of portfolio returns for a given weighting function.\n\n    Parameters\n    ----------\n    monthly_df : DataFrame\n        Must contain 'ticker', 'month_end', 'monthly_return', and any\n        columns needed by weight_fn.\n    weight_fn : callable\n        Function that takes a cross-section DataFrame and returns a\n        Series of weights indexed by ticker. Weights need not sum to 1\n        (they will be normalized).\n    rebal_freq : str\n        'M' for monthly, 'Q' for quarterly, 'A' for annual.\n    max_weight : float\n        Maximum weight per stock (for capped schemes).\n    min_weight : float\n        Minimum weight per stock.\n\n    Returns\n    -------\n    DataFrame with columns: month_end, port_return, n_stocks,\n    turnover, hhi, effective_n\n    \"\"\"\n    months = sorted(monthly_df[\"month_end\"].unique())\n\n    # Determine rebalancing dates\n    if rebal_freq == \"M\":\n        rebal_dates = set(months)\n    elif rebal_freq == \"Q\":\n        rebal_dates = set(pd.to_datetime(months).to_period(\"Q\").to_timestamp(\"M\"))\n        # Map to nearest month-end\n        rebal_dates = {m for m in months if pd.Timestamp(m).month % 3 == 0}\n        if not rebal_dates:\n            rebal_dates = set(months[::3])\n    elif rebal_freq == \"A\":\n        rebal_dates = {m for m in months if pd.Timestamp(m).month == 6}\n        if not rebal_dates:\n            rebal_dates = set(months[::12])\n    else:\n        rebal_dates = set(months)\n\n    results = []\n    prev_weights = None\n\n    for month in months:\n        cross_section = monthly_df[monthly_df[\"month_end\"] == month].copy()\n        cross_section = cross_section.dropna(subset=[\"monthly_return\"])\n\n        if len(cross_section) &lt; 5:\n            continue\n\n        if month in rebal_dates or prev_weights is None:\n            # Compute fresh weights\n            raw_weights = weight_fn(cross_section)\n            raw_weights = raw_weights.clip(lower=min_weight, upper=max_weight)\n            total = raw_weights.sum()\n            if total &lt;= 0:\n                continue\n            target_weights = raw_weights / total\n        else:\n            # Drift weights forward from previous month\n            if prev_weights is not None:\n                available = cross_section.set_index(\"ticker\")\n                drifted = prev_weights.reindex(available.index, fill_value=0)\n                # Adjust for returns\n                drifted = drifted * (1 + available[\"monthly_return\"])\n                total = drifted.sum()\n                if total &lt;= 0:\n                    continue\n                target_weights = drifted / total\n            else:\n                continue\n\n        # Align weights with available stocks\n        cross_section = cross_section.set_index(\"ticker\")\n        aligned_w = target_weights.reindex(cross_section.index, fill_value=0)\n        aligned_w = aligned_w / aligned_w.sum()\n\n        # Portfolio return\n        port_ret = (aligned_w * cross_section[\"monthly_return\"]).sum()\n\n        # Turnover (two-way)\n        if prev_weights is not None:\n            prev_aligned = prev_weights.reindex(aligned_w.index, fill_value=0)\n            # Drift previous weights\n            prev_drifted = prev_aligned * (\n                1\n                + cross_section[\"monthly_return\"].reindex(\n                    prev_aligned.index, fill_value=0\n                )\n            )\n            prev_drifted = (\n                prev_drifted / prev_drifted.sum()\n                if prev_drifted.sum() &gt; 0\n                else prev_drifted\n            )\n            turnover = (aligned_w - prev_drifted).abs().sum() / 2\n        else:\n            turnover = 1.0\n\n        # Concentration metrics\n        hhi = (aligned_w**2).sum()\n        effective_n = 1.0 / hhi if hhi &gt; 0 else 0\n\n        results.append(\n            {\n                \"month_end\": month,\n                \"port_return\": port_ret,\n                \"n_stocks\": (aligned_w &gt; 1e-6).sum(),\n                \"turnover\": turnover,\n                \"hhi\": hhi,\n                \"effective_n\": effective_n,\n            }\n        )\n\n        prev_weights = aligned_w.copy()\n\n    return pd.DataFrame(results)\n\n\n\n12.3.2 Value-Weighted Portfolio\n\ndef vw_weights(cross_section):\n    \"\"\"Value-weighted: proportional to market cap.\"\"\"\n    return cross_section.set_index('ticker')['market_cap']\n\nvw_returns = compute_portfolio_returns(universe_mid, vw_weights, rebal_freq='M')\nprint(f\"VW portfolio: {len(vw_returns)} months\")\nprint(f\"Mean monthly return: {vw_returns['port_return'].mean():.4f}\")\nprint(f\"Mean turnover: {vw_returns['turnover'].mean():.4f}\")\nprint(f\"Mean effective N: {vw_returns['effective_n'].mean():.1f}\")\n\n\n\n12.3.3 Equal-Weighted Portfolio\n\ndef ew_weights(cross_section):\n    \"\"\"Equal-weighted: 1/N.\"\"\"\n    tickers = cross_section.set_index('ticker').index\n    return pd.Series(1.0, index=tickers)\n\n# Monthly rebalancing\new_monthly = compute_portfolio_returns(\n    universe_mid, ew_weights, rebal_freq='M'\n)\n\n# Quarterly rebalancing\new_quarterly = compute_portfolio_returns(\n    universe_mid, ew_weights, rebal_freq='Q'\n)\n\n# Annual rebalancing (June)\new_annual = compute_portfolio_returns(\n    universe_mid, ew_weights, rebal_freq='A'\n)\n\nfor name, df in [('EW Monthly', ew_monthly),\n                  ('EW Quarterly', ew_quarterly),\n                  ('EW Annual', ew_annual)]:\n    print(f\"{name}: mean ret = {df['port_return'].mean():.4f}, \"\n          f\"turnover = {df['turnover'].mean():.4f}\")\n\n\n\n12.3.4 Capped Value-Weighted Portfolio\nTo mitigate the concentration of pure VW while retaining its low-turnover properties, we impose a cap on individual stock weights. A common choice is 5% or 10%, mimicking the construction rules of capped indices such as the MSCI Capped indices.\n\ndef capped_vw_weights(cross_section, cap=0.05):\n    \"\"\"Capped VW: market cap weights with an upper bound.\"\"\"\n    w = cross_section.set_index('ticker')['market_cap']\n    w = w / w.sum()\n    # Iterative capping (redistribute excess weight)\n    for _ in range(20):\n        excess = w[w &gt; cap] - cap\n        if excess.sum() &lt;= 1e-8:\n            break\n        w[w &gt; cap] = cap\n        w[w &lt;= cap] *= (1 + excess.sum() / w[w &lt;= cap].sum())\n    return w\n\ncapped5 = compute_portfolio_returns(\n    universe_mid, lambda cs: capped_vw_weights(cs, 0.05), rebal_freq='M'\n)\ncapped10 = compute_portfolio_returns(\n    universe_mid, lambda cs: capped_vw_weights(cs, 0.10), rebal_freq='M'\n)\n\nprint(f\"Capped 5%: eff_N = {capped5['effective_n'].mean():.1f}, \"\n      f\"turnover = {capped5['turnover'].mean():.4f}\")\nprint(f\"Capped 10%: eff_N = {capped10['effective_n'].mean():.1f}, \"\n      f\"turnover = {capped10['turnover'].mean():.4f}\")\n\n\n\n12.3.5 Fundamental-Weighted Portfolio\nArnott, Hsu, and Moore (2005) propose weighting stocks by fundamental measures (revenue, book equity, dividends, cash flow) rather than market cap. The logic is that fundamental weights are not contaminated by pricing errors, breaking the mechanical overweighting of overvalued stocks inherent in VW. We construct a composite fundamental weight using the average rank across four measures:\n\\[\nw_{i,t}^{FW} \\propto \\frac{1}{4}\\left(\\text{Rank}_{i,t}^{\\text{Rev}} + \\text{Rank}_{i,t}^{\\text{BE}} + \\text{Rank}_{i,t}^{\\text{Div}} + \\text{Rank}_{i,t}^{\\text{CFO}}\\right)\n\\tag{12.3}\\]\n\n# Merge fundamentals with monthly data (use most recent fiscal year)\nfundamentals[\"merge_year\"] = fundamentals[\"fiscal_year\"] + 1  # Lag by 1 year\nmonthly_fund = monthly.copy()\nmonthly_fund[\"year\"] = pd.to_datetime(monthly_fund[\"month_end\"]).dt.year\n\nmonthly_fund = monthly_fund.merge(\n    fundamentals.rename(columns={\"merge_year\": \"year\"}),\n    on=[\"ticker\", \"year\"],\n    how=\"left\",\n)\n\n\ndef fw_weights(cross_section):\n    \"\"\"Fundamental-weighted: composite of revenue, book equity, dividends, CFO.\"\"\"\n    cs = cross_section.set_index(\"ticker\")\n    ranks = pd.DataFrame(index=cs.index)\n\n    for col in [\"revenue\", \"book_equity\", \"dividends_paid\", \"operating_cash_flow\"]:\n        if col in cs.columns:\n            vals = cs[col].clip(lower=0)  # Only positive values\n            ranks[col] = vals.rank(pct=True)\n\n    composite = ranks.mean(axis=1)\n    composite = composite.fillna(0)\n    return composite\n\n\nfw_returns = compute_portfolio_returns(monthly_fund, fw_weights, rebal_freq=\"A\")\nprint(\n    f\"Fundamental-weighted: mean ret = {fw_returns['port_return'].mean():.4f}, \"\n    f\"eff_N = {fw_returns['effective_n'].mean():.1f}\"\n)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-risk-based",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-risk-based",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.4 Risk-Based Weighting Schemes",
    "text": "12.4 Risk-Based Weighting Schemes\nThe weighting schemes above use only market cap or accounting data. Risk-based schemes incorporate the covariance structure of returns, aiming to produce portfolios with better risk-adjusted performance. The trade-off is that they require estimating the covariance matrix (i.e., a high-dimensional object that is notoriously difficult to estimate precisely with short time series).\n\n12.4.1 Covariance Estimation\nWith \\(N \\approx 300\\) stocks and \\(T \\approx 60\\) months, the sample covariance matrix is severely ill-conditioned. We use the Ledoit and Wolf (2004) shrinkage estimator, which pulls the sample covariance toward a structured target (the identity matrix scaled by the average variance):\n\\[\n\\hat{\\boldsymbol{\\Sigma}}^{\\text{shrink}} = \\delta \\mathbf{F} + (1 - \\delta) \\mathbf{S}\n\\tag{12.4}\\]\nwhere \\(\\mathbf{S}\\) is the sample covariance, \\(\\mathbf{F}\\) is the shrinkage target, and \\(\\delta \\in [0,1]\\) is the optimal shrinkage intensity derived analytically.\n\ndef estimate_covariance(monthly_df, month, lookback=60, min_obs=36):\n    \"\"\"\n    Estimate covariance matrix using Ledoit-Wolf shrinkage.\n    \n    Parameters\n    ----------\n    monthly_df : DataFrame with ticker, month_end, monthly_return\n    month : target month (use returns before this date)\n    lookback : number of months to use\n    min_obs : minimum observations per stock\n    \n    Returns\n    -------\n    cov_matrix : DataFrame (N x N)\n    tickers : list of tickers with sufficient data\n    \"\"\"\n    end_date = pd.Timestamp(month)\n    start_date = end_date - pd.DateOffset(months=lookback)\n    \n    window = monthly_df[\n        (monthly_df['month_end'] &gt; start_date) &\n        (monthly_df['month_end'] &lt;= end_date)\n    ]\n    \n    # Pivot to wide format\n    returns_wide = window.pivot_table(\n        index='month_end', columns='ticker', values='monthly_return'\n    )\n    \n    # Keep stocks with sufficient observations\n    valid_cols = returns_wide.columns[returns_wide.notna().sum() &gt;= min_obs]\n    returns_wide = returns_wide[valid_cols].dropna(axis=0, how='all')\n    \n    # Fill remaining NAs with 0 (conservative)\n    returns_clean = returns_wide.fillna(0)\n    \n    if returns_clean.shape[1] &lt; 10:\n        return None, None\n    \n    # Ledoit-Wolf shrinkage\n    lw = LedoitWolf()\n    lw.fit(returns_clean.values)\n    \n    cov_matrix = pd.DataFrame(\n        lw.covariance_,\n        index=valid_cols, columns=valid_cols\n    )\n    \n    return cov_matrix, list(valid_cols)\n\n\n\n12.4.2 Minimum Variance Portfolio\nThe global minimum variance (GMV) portfolio minimizes portfolio variance without targeting a specific return level (Clarke, De Silva, and Thorley 2011):\n\\[\n\\mathbf{w}^{MV} = \\arg\\min_{\\mathbf{w}} \\; \\mathbf{w}'\\hat{\\boldsymbol{\\Sigma}}\\mathbf{w} \\quad \\text{s.t.} \\quad \\mathbf{1}'\\mathbf{w} = 1, \\; w_i \\geq 0\n\\tag{12.5}\\]\nThe long-only constraint (\\(w_i \\geq 0\\)) is essential in practice and also acts as an implicit shrinkage that improves out-of-sample performance (Jagannathan and Ma 2003).\n\ndef minimum_variance_weights(cov_matrix, max_weight=0.05):\n    \"\"\"\n    Solve for the minimum variance portfolio with long-only\n    and position-size constraints.\n    \"\"\"\n    n = cov_matrix.shape[0]\n    Sigma = cov_matrix.values\n    \n    def portfolio_variance(w):\n        return w @ Sigma @ w\n    \n    constraints = [\n        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n    ]\n    bounds = [(0, max_weight) for _ in range(n)]\n    x0 = np.ones(n) / n\n    \n    result = minimize(\n        portfolio_variance, x0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints=constraints,\n        options={'maxiter': 1000, 'ftol': 1e-12}\n    )\n    \n    if result.success:\n        return pd.Series(result.x, index=cov_matrix.index)\n    else:\n        return pd.Series(1.0 / n, index=cov_matrix.index)\n\ndef mv_weight_fn(cross_section, cov_cache={}):\n    \"\"\"Wrapper for portfolio engine: minimum variance.\"\"\"\n    month = cross_section['month_end'].iloc[0]\n    \n    if month not in cov_cache:\n        cov_matrix, tickers = estimate_covariance(monthly, month)\n        cov_cache[month] = (cov_matrix, tickers)\n    \n    cov_matrix, tickers = cov_cache[month]\n    if cov_matrix is None:\n        # Fallback to equal weight\n        return pd.Series(1.0, index=cross_section.set_index('ticker').index)\n    \n    # Restrict to stocks in both cross-section and covariance matrix\n    available = set(cross_section['ticker']) & set(tickers)\n    if len(available) &lt; 10:\n        return pd.Series(1.0, index=cross_section.set_index('ticker').index)\n    \n    sub_cov = cov_matrix.loc[list(available), list(available)]\n    weights = minimum_variance_weights(sub_cov)\n    \n    return weights\n\nmv_returns = compute_portfolio_returns(\n    universe_mid, mv_weight_fn, rebal_freq='Q'\n)\nprint(f\"Min Variance: mean ret = {mv_returns['port_return'].mean():.4f}, \"\n      f\"std = {mv_returns['port_return'].std():.4f}\")\n\n\n\n12.4.3 Risk Parity (Equal Risk Contribution)\nRisk parity allocates so that each asset contributes equally to total portfolio risk (Maillard, Roncalli, and Teı̈letche 2010). The risk contribution of asset \\(i\\) is:\n\\[\nRC_i = w_i \\cdot \\frac{(\\boldsymbol{\\Sigma} \\mathbf{w})_i}{\\sqrt{\\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}}}\n\\tag{12.6}\\]\nThe risk parity portfolio solves \\(RC_i = RC_j\\) for all \\(i, j\\):\n\ndef risk_parity_weights(cov_matrix, max_weight=0.05):\n    \"\"\"\n    Solve for the risk parity portfolio where each asset\n    contributes equally to total portfolio variance.\n    \"\"\"\n    n = cov_matrix.shape[0]\n    Sigma = cov_matrix.values\n    \n    def risk_parity_objective(w):\n        port_var = w @ Sigma @ w\n        marginal = Sigma @ w\n        risk_contrib = w * marginal\n        target_rc = port_var / n\n        return np.sum((risk_contrib - target_rc) ** 2)\n    \n    constraints = [\n        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n    ]\n    bounds = [(1e-6, max_weight) for _ in range(n)]\n    x0 = np.ones(n) / n\n    \n    result = minimize(\n        risk_parity_objective, x0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints=constraints,\n        options={'maxiter': 1000, 'ftol': 1e-12}\n    )\n    \n    if result.success:\n        return pd.Series(result.x, index=cov_matrix.index)\n    else:\n        return pd.Series(1.0 / n, index=cov_matrix.index)\n\ndef rp_weight_fn(cross_section, cov_cache={}):\n    \"\"\"Wrapper for portfolio engine: risk parity.\"\"\"\n    month = cross_section['month_end'].iloc[0]\n    \n    if month not in cov_cache:\n        cov_matrix, tickers = estimate_covariance(monthly, month)\n        cov_cache[month] = (cov_matrix, tickers)\n    \n    cov_matrix, tickers = cov_cache[month]\n    if cov_matrix is None:\n        return pd.Series(1.0, index=cross_section.set_index('ticker').index)\n    \n    available = set(cross_section['ticker']) & set(tickers)\n    if len(available) &lt; 10:\n        return pd.Series(1.0, index=cross_section.set_index('ticker').index)\n    \n    sub_cov = cov_matrix.loc[list(available), list(available)]\n    return risk_parity_weights(sub_cov)\n\nrp_returns = compute_portfolio_returns(\n    universe_mid, rp_weight_fn, rebal_freq='Q'\n)\nprint(f\"Risk Parity: mean ret = {rp_returns['port_return'].mean():.4f}, \"\n      f\"std = {rp_returns['port_return'].std():.4f}\")\n\n\n\n12.4.4 Maximum Diversification Portfolio\nChoueifaty (2008) define the diversification ratio as the ratio of weighted average volatility to portfolio volatility:\n\\[\nDR(\\mathbf{w}) = \\frac{\\mathbf{w}'\\boldsymbol{\\sigma}}{\\sqrt{\\mathbf{w}'\\boldsymbol{\\Sigma}\\mathbf{w}}}\n\\tag{12.7}\\]\nwhere \\(\\boldsymbol{\\sigma}\\) is the vector of individual asset volatilities. The maximum diversification portfolio maximizes this ratio:\n\ndef max_diversification_weights(cov_matrix, max_weight=0.05):\n    \"\"\"Maximize the diversification ratio.\"\"\"\n    n = cov_matrix.shape[0]\n    Sigma = cov_matrix.values\n    sigma = np.sqrt(np.diag(Sigma))\n    \n    def neg_div_ratio(w):\n        port_vol = np.sqrt(w @ Sigma @ w)\n        if port_vol &lt; 1e-10:\n            return 0\n        return -(w @ sigma) / port_vol\n    \n    constraints = [\n        {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n    ]\n    bounds = [(0, max_weight) for _ in range(n)]\n    x0 = np.ones(n) / n\n    \n    result = minimize(\n        neg_div_ratio, x0,\n        method='SLSQP',\n        bounds=bounds,\n        constraints=constraints,\n        options={'maxiter': 1000, 'ftol': 1e-12}\n    )\n    \n    if result.success:\n        return pd.Series(result.x, index=cov_matrix.index)\n    else:\n        return pd.Series(1.0 / n, index=cov_matrix.index)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-comparison",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-comparison",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.5 Comprehensive Performance Comparison",
    "text": "12.5 Comprehensive Performance Comparison\nWe now compare all schemes on a common universe with consistent methodology.\n\n# Collect all portfolio return series\nportfolios = {\n    'VW': vw_returns,\n    'EW (Monthly)': ew_monthly,\n    'EW (Quarterly)': ew_quarterly,\n    'EW (Annual)': ew_annual,\n    'Capped VW (5%)': capped5,\n    'Capped VW (10%)': capped10,\n    'Fundamental': fw_returns,\n    'Min Variance': mv_returns,\n    'Risk Parity': rp_returns,\n}\n\n# Align to common date range\ncommon_start = max(df['month_end'].min() for df in portfolios.values())\ncommon_end = min(df['month_end'].max() for df in portfolios.values())\n\nfor name in portfolios:\n    portfolios[name] = portfolios[name][\n        (portfolios[name]['month_end'] &gt;= common_start) &\n        (portfolios[name]['month_end'] &lt;= common_end)\n    ].copy()\n\nprint(f\"Common period: {common_start} to {common_end}\")\nprint(f\"Number of months: {len(portfolios['VW'])}\")\n\n\n12.5.1 Performance Metrics\n\ndef compute_metrics(returns_df, risk_free_annual=0.04):\n    \"\"\"Compute performance metrics from monthly portfolio returns.\"\"\"\n    r = returns_df['port_return']\n    rf_monthly = (1 + risk_free_annual) ** (1/12) - 1\n    \n    excess = r - rf_monthly\n    n_months = len(r)\n    \n    # Annualized return\n    cum_ret = (1 + r).prod()\n    ann_ret = cum_ret ** (12 / n_months) - 1\n    \n    # Annualized volatility\n    ann_vol = r.std() * np.sqrt(12)\n    \n    # Sharpe ratio\n    sharpe = excess.mean() / excess.std() * np.sqrt(12) if excess.std() &gt; 0 else 0\n    \n    # Maximum drawdown\n    cum = (1 + r).cumprod()\n    running_max = cum.cummax()\n    drawdown = (cum - running_max) / running_max\n    max_dd = drawdown.min()\n    \n    # Sortino ratio\n    downside = excess[excess &lt; 0]\n    downside_vol = np.sqrt((downside ** 2).mean()) * np.sqrt(12)\n    sortino = excess.mean() * 12 / downside_vol if downside_vol &gt; 0 else 0\n    \n    # Calmar ratio\n    calmar = ann_ret / abs(max_dd) if max_dd != 0 else 0\n    \n    # Average turnover and effective N\n    avg_turnover = returns_df['turnover'].mean()\n    avg_eff_n = returns_df['effective_n'].mean()\n    \n    # Skewness and kurtosis\n    skew = r.skew()\n    kurt = r.kurtosis()\n    \n    return {\n        'Ann. Return': ann_ret,\n        'Ann. Volatility': ann_vol,\n        'Sharpe Ratio': sharpe,\n        'Sortino Ratio': sortino,\n        'Max Drawdown': max_dd,\n        'Calmar Ratio': calmar,\n        'Skewness': skew,\n        'Kurtosis': kurt,\n        'Avg. Turnover': avg_turnover,\n        'Effective N': avg_eff_n\n    }\n\n# Compute metrics for all portfolios\nmetrics_list = []\nfor name, df in portfolios.items():\n    m = compute_metrics(df)\n    m['Portfolio'] = name\n    metrics_list.append(m)\n\nmetrics_df = pd.DataFrame(metrics_list).set_index('Portfolio')\n\n# Format for display\ndisplay_cols = [\n    'Ann. Return', 'Ann. Volatility', 'Sharpe Ratio', 'Sortino Ratio',\n    'Max Drawdown', 'Avg. Turnover', 'Effective N'\n]\nprint(metrics_df[display_cols].round(3).to_string())\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 7))\n\ncolors = {\n    'VW': '#2C5F8A', 'EW (Monthly)': '#E67E22',\n    'EW (Quarterly)': '#F39C12', 'EW (Annual)': '#D4AC0D',\n    'Capped VW (5%)': '#8E44AD', 'Capped VW (10%)': '#9B59B6',\n    'Fundamental': '#27AE60', 'Min Variance': '#C0392B',\n    'Risk Parity': '#1ABC9C'\n}\nlinestyles = {\n    'VW': '-', 'EW (Monthly)': '-', 'EW (Quarterly)': '--',\n    'EW (Annual)': ':', 'Capped VW (5%)': '-',\n    'Capped VW (10%)': '--', 'Fundamental': '-',\n    'Min Variance': '-', 'Risk Parity': '-'\n}\n\nfor name, df in portfolios.items():\n    cum = (1 + df.set_index('month_end')['port_return']).cumprod()\n    ax.plot(cum.index, cum.values, label=name,\n            color=colors.get(name, 'gray'),\n            linestyle=linestyles.get(name, '-'),\n            linewidth=1.8 if name in ['VW', 'EW (Monthly)', 'Min Variance'] else 1.2)\n\nax.set_xlabel('Date')\nax.set_ylabel('Cumulative Wealth (VND 1 invested)')\nax.set_title('Cumulative Performance by Weighting Scheme')\nax.legend(loc='upper left', fontsize=9, ncol=2)\nax.set_yscale('log')\nplt.tight_layout()\nplt.show()\n\n\nFigure 12.2\n\n\n\n\n\n12.5.2 Risk-Return Trade-Off\n\n\n\nfig, ax = plt.subplots(figsize=(9, 7))\n\nfor name in metrics_df.index:\n    ax.scatter(\n        metrics_df.loc[name, 'Ann. Volatility'],\n        metrics_df.loc[name, 'Ann. Return'],\n        s=metrics_df.loc[name, 'Effective N'] * 3,\n        color=colors.get(name, 'gray'),\n        alpha=0.85, edgecolors='white', linewidth=1.5, zorder=5\n    )\n    ax.annotate(\n        name,\n        (metrics_df.loc[name, 'Ann. Volatility'] + 0.002,\n         metrics_df.loc[name, 'Ann. Return']),\n        fontsize=8\n    )\n\nax.set_xlabel('Annualized Volatility')\nax.set_ylabel('Annualized Return')\nax.set_title('Risk-Return Profile (bubble size = Effective N)')\nplt.tight_layout()\nplt.show()\n\n\nFigure 12.3",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-transaction-costs",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-transaction-costs",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.6 Transaction Costs and Net-of-Cost Performance",
    "text": "12.6 Transaction Costs and Net-of-Cost Performance\n\n12.6.1 Estimating Trading Costs in Vietnam\nTransaction costs in Vietnam include explicit components (brokerage commissions, exchange fees, taxes) and implicit components (bid-ask spread, price impact). The explicit cost structure as of 2024 is approximately:\n\n\n\nTable 12.2: Explicit transaction cost components for Vietnamese equities.\n\n\n\n\n\n\n\n\n\n\nComponent\nRate\nNotes\n\n\n\n\nBrokerage commission\n0.15–0.35%\nVaries by broker and volume tier\n\n\nExchange & clearing fee\n0.003%\nFixed by exchange\n\n\nSelling tax\n0.10%\nLevied on gross sale proceeds\n\n\n\n\n\n\nThe total explicit round-trip cost (buy + sell) ranges from approximately 0.30% to 0.80%. Implicit costs—the spread and price impact—can be substantially larger for small and illiquid stocks.\nWe model total transaction costs as a function of trade size and stock liquidity:\n\\[\nTC_{i,t} = c_{\\text{fixed}} + \\frac{1}{2} \\text{Spread}_{i,t} + \\lambda \\sqrt{\\frac{|\\Delta w_{i,t}| \\cdot \\text{AUM}}{ADV_{i,t}}}\n\\tag{12.8}\\]\nwhere \\(c_{\\text{fixed}} \\approx 0.25\\%\\) is the explicit cost per trade, \\(\\text{Spread}_{i,t}\\) is the quoted bid-ask spread, \\(\\Delta w_{i,t}\\) is the weight change, \\(\\text{AUM}\\) is portfolio size, \\(ADV_{i,t}\\) is average daily volume in VND, and \\(\\lambda\\) is the price impact coefficient estimated from the Amihud (2002) model.\n\ndef estimate_transaction_costs(weight_changes, stock_data,\n                                aum_vnd=100e9, fixed_cost=0.0025,\n                                impact_coef=0.10):\n    \"\"\"\n    Estimate total transaction costs for a rebalancing event.\n    \n    Parameters\n    ----------\n    weight_changes : Series indexed by ticker, absolute weight changes\n    stock_data : DataFrame with ticker, bid_ask_spread, turnover_value_avg_20d\n    aum_vnd : float, portfolio AUM in VND\n    fixed_cost : float, explicit cost per unit traded (one-way)\n    impact_coef : float, price impact coefficient (lambda)\n    \n    Returns\n    -------\n    total_cost : float, total TC as fraction of AUM\n    cost_detail : DataFrame with per-stock costs\n    \"\"\"\n    costs = []\n    \n    for ticker, dw in weight_changes.items():\n        if abs(dw) &lt; 1e-6:\n            continue\n        \n        trade_vnd = abs(dw) * aum_vnd\n        \n        # Explicit cost (one-way)\n        explicit = fixed_cost * trade_vnd\n        \n        # Spread cost\n        stock_info = stock_data[stock_data['ticker'] == ticker]\n        if len(stock_info) &gt; 0:\n            spread = stock_info['bid_ask_spread'].iloc[0]\n            adv = stock_info['turnover_value_avg_20d'].iloc[0]\n        else:\n            spread = 0.005  # Default 50 bps\n            adv = 1e9  # Default VND 1bn\n        \n        spread_cost = 0.5 * spread * trade_vnd\n        \n        # Price impact (square root model)\n        participation_rate = trade_vnd / max(adv, 1e6)\n        impact_cost = impact_coef * np.sqrt(participation_rate) * trade_vnd\n        \n        total = explicit + spread_cost + impact_cost\n        costs.append({\n            'ticker': ticker,\n            'weight_change': dw,\n            'trade_vnd': trade_vnd,\n            'explicit': explicit,\n            'spread': spread_cost,\n            'impact': impact_cost,\n            'total': total\n        })\n    \n    cost_df = pd.DataFrame(costs)\n    total_cost = cost_df['total'].sum() / aum_vnd if len(cost_df) &gt; 0 else 0\n    \n    return total_cost, cost_df\n\n\n\n12.6.2 Net-of-Cost Performance\nWe apply the transaction cost model to compute net-of-cost returns for each weighting scheme at different assumed AUM levels. This is critical because strategies that appear attractive in gross terms may be unimplementable at scale due to the illiquidity of small-cap Vietnamese stocks.\n\ndef compute_net_returns(portfolio_df, cost_per_turnover=0.005):\n    \"\"\"\n    Approximate net returns using a proportional cost model.\n    Net return = gross return - (turnover * cost_per_unit_turnover)\n    \"\"\"\n    df = portfolio_df.copy()\n    df['tc'] = df['turnover'] * cost_per_turnover\n    df['net_return'] = df['port_return'] - df['tc']\n    return df\n\n# Compute at different cost assumptions\ncost_scenarios = {\n    'Low (25 bps)': 0.0025,\n    'Medium (50 bps)': 0.005,\n    'High (100 bps)': 0.01\n}\n\nprint(\"Annualized Net Sharpe Ratios by Cost Scenario:\")\nprint(\"-\" * 70)\n\nfor cost_name, cost_rate in cost_scenarios.items():\n    row = {'Scenario': cost_name}\n    for port_name, port_df in portfolios.items():\n        net_df = compute_net_returns(port_df, cost_rate)\n        rf_monthly = (1.04) ** (1/12) - 1\n        excess = net_df['net_return'] - rf_monthly\n        sharpe = excess.mean() / excess.std() * np.sqrt(12) if excess.std() &gt; 0 else 0\n        row[port_name] = round(sharpe, 3)\n    print(f\"{cost_name}:\")\n    for k, v in row.items():\n        if k != 'Scenario':\n            print(f\"  {k}: {v}\")\n    print()\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\nturnover_data = {}\nfor name, df in portfolios.items():\n    turnover_data[name] = df['turnover'].values\n\npositions = range(len(turnover_data))\nbp = ax.boxplot(\n    turnover_data.values(),\n    positions=positions,\n    widths=0.6,\n    patch_artist=True,\n    showfliers=False,\n    medianprops={'color': 'black', 'linewidth': 1.5}\n)\n\nfor i, (patch, name) in enumerate(zip(bp['boxes'], turnover_data.keys())):\n    patch.set_facecolor(colors.get(name, 'gray'))\n    patch.set_alpha(0.7)\n\nax.set_xticks(positions)\nax.set_xticklabels(turnover_data.keys(), rotation=45, ha='right', fontsize=9)\nax.set_ylabel('Monthly Turnover (one-way)')\nax.set_title('Turnover Distribution by Weighting Scheme')\nplt.tight_layout()\nplt.show()\n\n\nFigure 12.4\n\n\n\n\n\n12.6.3 Cost Erosion at Scale\nThe relationship between portfolio AUM and implementable performance is non-linear because price impact costs grow with trade size. We simulate performance at different AUM levels:\n\n\n\naum_levels = [10, 50, 100, 500, 1000, 5000]  # VND billions\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nselected_ports = ['VW', 'EW (Monthly)', 'Capped VW (5%)',\n                   'Min Variance', 'Risk Parity']\n\nfor name in selected_ports:\n    sharpes = []\n    df = portfolios[name]\n    \n    for aum in aum_levels:\n        # Cost scales with sqrt(AUM / ADV)\n        base_cost = 0.003  # Base cost at small AUM\n        scale_factor = np.sqrt(aum / 100)  # Normalized to VND 100bn\n        cost_rate = base_cost * scale_factor\n        \n        # VW is less affected (large-cap tilt)\n        if name == 'VW':\n            cost_rate *= 0.3\n        elif 'Capped' in name:\n            cost_rate *= 0.5\n        elif 'Min Variance' in name or 'Risk Parity' in name:\n            cost_rate *= 0.7\n        \n        net_df = compute_net_returns(df, cost_rate)\n        rf_m = (1.04) ** (1/12) - 1\n        excess = net_df['net_return'] - rf_m\n        s = excess.mean() / excess.std() * np.sqrt(12) if excess.std() &gt; 0 else 0\n        sharpes.append(s)\n    \n    ax.plot(aum_levels, sharpes, marker='o', label=name,\n            color=colors.get(name, 'gray'), linewidth=2)\n\nax.set_xlabel('Portfolio AUM (VND Billion)')\nax.set_ylabel('Net Sharpe Ratio')\nax.set_title('Sharpe Ratio Degradation with AUM')\nax.set_xscale('log')\nax.legend(fontsize=9)\nax.axhline(y=0, color='gray', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\n\nFigure 12.5",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-rebalancing",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-rebalancing",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.7 Rebalancing Frequency Analysis",
    "text": "12.7 Rebalancing Frequency Analysis\n\n12.7.1 The Rebalancing Trade-Off\nRebalancing serves two purposes: (i) restoring target weights to maintain the desired risk profile, and (ii) harvesting the “rebalancing bonus”—the systematic profit from buying low and selling high that arises when weights are reset to targets in the presence of mean-reverting cross-sectional returns.\nThe trade-off is clear: more frequent rebalancing maintains tighter adherence to target weights and captures more of the rebalancing bonus, but incurs higher transaction costs. The optimal frequency depends on the magnitude of mean reversion (which determines the gross rebalancing bonus), the level of transaction costs, and the rate at which weights drift from targets.\n\n\n\nfrequencies = {\n    'Monthly': 'M',\n    'Quarterly': 'Q',\n    'Semi-annual': 'Q',  # Approximate with every 6 months\n    'Annual': 'A'\n}\n\n# Recompute EW at each frequency\nfreq_results = {}\nfor freq_name, freq_code in frequencies.items():\n    freq_df = compute_portfolio_returns(\n        universe_mid, ew_weights, rebal_freq=freq_code\n    )\n    freq_results[freq_name] = freq_df\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Gross vs Net Sharpe\nfreq_names = list(freq_results.keys())\ngross_sharpes = []\nnet_sharpes = []\n\nfor fn in freq_names:\n    df = freq_results[fn]\n    rf_m = (1.04) ** (1/12) - 1\n    exc = df['port_return'] - rf_m\n    gross_sharpes.append(exc.mean() / exc.std() * np.sqrt(12))\n    \n    net_df = compute_net_returns(df, 0.005)\n    exc_net = net_df['net_return'] - rf_m\n    net_sharpes.append(exc_net.mean() / exc_net.std() * np.sqrt(12))\n\nx = range(len(freq_names))\naxes[0].bar([i - 0.15 for i in x], gross_sharpes, width=0.3,\n            color='#2C5F8A', alpha=0.85, label='Gross')\naxes[0].bar([i + 0.15 for i in x], net_sharpes, width=0.3,\n            color='#C0392B', alpha=0.85, label='Net (50 bps)')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(freq_names)\naxes[0].set_ylabel('Annualized Sharpe Ratio')\naxes[0].set_title('Panel A: Sharpe Ratio by Rebalancing Frequency')\naxes[0].legend()\n\n# Panel B: Average turnover\nturnovers = [freq_results[fn]['turnover'].mean() for fn in freq_names]\naxes[1].bar(x, turnovers, color='#E67E22', alpha=0.85)\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(freq_names)\naxes[1].set_ylabel('Average Monthly Turnover')\naxes[1].set_title('Panel B: Turnover by Rebalancing Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 12.6\n\n\n\n\n\n12.7.2 Threshold-Based Rebalancing\nAn alternative to calendar-based rebalancing is to rebalance only when portfolio weights drift beyond a tolerance band. This “no-trade zone” approach is motivated by Gârleanu and Pedersen (2013), who derive the optimal dynamic trading strategy under quadratic transaction costs and show that the optimal portfolio is a weighted average of the current holdings and the frictionless target.\n\ndef compute_threshold_rebalanced(monthly_df, weight_fn,\n                                  threshold=0.01):\n    \"\"\"\n    Rebalance only when maximum weight deviation exceeds threshold.\n    \n    Parameters\n    ----------\n    threshold : float\n        Rebalance when max(|w_actual - w_target|) &gt; threshold.\n    \"\"\"\n    months = sorted(monthly_df['month_end'].unique())\n    results = []\n    current_weights = None\n    rebalance_count = 0\n    \n    for month in months:\n        cs = monthly_df[monthly_df['month_end'] == month].copy()\n        cs = cs.dropna(subset=['monthly_return']).set_index('ticker')\n        \n        if len(cs) &lt; 5:\n            continue\n        \n        target = weight_fn(cs.reset_index())\n        target = target / target.sum()\n        \n        if current_weights is None:\n            current_weights = target.copy()\n            rebalance_count += 1\n        else:\n            # Drift weights\n            current_weights = current_weights.reindex(cs.index, fill_value=0)\n            current_weights = current_weights * (1 + cs['monthly_return'])\n            total = current_weights.sum()\n            if total &gt; 0:\n                current_weights = current_weights / total\n            \n            # Check if rebalancing needed\n            max_dev = (current_weights - target.reindex(\n                current_weights.index, fill_value=0\n            )).abs().max()\n            \n            if max_dev &gt; threshold:\n                turnover = (current_weights - target.reindex(\n                    current_weights.index, fill_value=0\n                )).abs().sum() / 2\n                current_weights = target.reindex(cs.index, fill_value=0)\n                current_weights = current_weights / current_weights.sum()\n                rebalance_count += 1\n            else:\n                turnover = 0\n        \n        port_ret = (current_weights.reindex(cs.index, fill_value=0) *\n                    cs['monthly_return']).sum()\n        hhi = (current_weights ** 2).sum()\n        \n        results.append({\n            'month_end': month,\n            'port_return': port_ret,\n            'turnover': turnover,\n            'hhi': hhi,\n            'effective_n': 1/hhi if hhi &gt; 0 else 0,\n            'n_stocks': (current_weights &gt; 1e-6).sum()\n        })\n    \n    print(f\"Threshold {threshold:.1%}: rebalanced {rebalance_count} / \"\n          f\"{len(results)} months ({rebalance_count/len(results):.0%})\")\n    return pd.DataFrame(results)\n\n# Test different thresholds\nthresholds = [0.005, 0.01, 0.02, 0.05]\nthreshold_results = {}\nfor t in thresholds:\n    threshold_results[f'{t:.1%}'] = compute_threshold_rebalanced(\n        universe_mid, ew_weights, threshold=t\n    )",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-rebalancing-bonus",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-rebalancing-bonus",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.8 The Rebalancing Bonus: Decomposition",
    "text": "12.8 The Rebalancing Bonus: Decomposition\nThe excess return of the rebalanced EW portfolio over a buy-and-hold EW portfolio (which starts equal-weighted but drifts) can be decomposed following Plyakha, Uppal, and Vilkov (2021). Define \\(r_i\\) as the return of stock \\(i\\) over one period:\n\\[\nR^{EW}_{\\text{rebal}} - R^{EW}_{\\text{drift}} \\approx \\frac{1}{2N} \\sum_{i=1}^N \\text{Var}(r_i) - \\frac{1}{2N^2}\\sum_{i}\\sum_{j}\\text{Cov}(r_i, r_j)\n\\tag{12.9}\\]\nThe first term captures the “buy low, sell high” effect from resetting weights after return dispersion. The second term is the cost of undoing covariance-induced drift. The rebalancing bonus is larger when cross-sectional return dispersion is high (which it is in Vietnam) and when pairwise correlations are low.\n\n# Compute buy-and-hold EW portfolio (no rebalancing after initial equal weight)\nbh_returns = compute_portfolio_returns(\n    universe_mid, ew_weights, rebal_freq='A'  # Rebalance once per year only\n)\n\n# The rebalancing bonus is the difference\nbonus_df = pd.merge(\n    ew_monthly[['month_end', 'port_return']].rename(\n        columns={'port_return': 'rebal_return'}),\n    bh_returns[['month_end', 'port_return']].rename(\n        columns={'port_return': 'bh_return'}),\n    on='month_end'\n)\nbonus_df['bonus'] = bonus_df['rebal_return'] - bonus_df['bh_return']\n\n# Cross-sectional return dispersion\ndispersion = (\n    universe_mid\n    .groupby('month_end')['monthly_return']\n    .std()\n    .reset_index(name='cs_dispersion')\n)\nbonus_df = bonus_df.merge(dispersion, on='month_end')\n\nann_bonus = bonus_df['bonus'].mean() * 12\nprint(f\"Annualized rebalancing bonus (EW monthly vs annual): {ann_bonus:.4f}\")\nprint(f\"Mean cross-sectional dispersion: {bonus_df['cs_dispersion'].mean():.4f}\")\n\n\n\n\nfig, ax1 = plt.subplots(figsize=(12, 5))\n\nax1.bar(pd.to_datetime(bonus_df['month_end']),\n        bonus_df['bonus'] * 100,\n        color='#2C5F8A', alpha=0.6, width=25, label='Rebal. Bonus')\nax1.set_ylabel('Rebalancing Bonus (%)', color='#2C5F8A')\nax1.set_xlabel('Date')\n\nax2 = ax1.twinx()\nax2.plot(pd.to_datetime(bonus_df['month_end']),\n         bonus_df['cs_dispersion'],\n         color='#C0392B', linewidth=1.5, alpha=0.7,\n         label='CS Dispersion')\nax2.set_ylabel('Cross-Sectional Return Dispersion', color='#C0392B')\n\nax1.set_title('Rebalancing Bonus and Return Dispersion')\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 12.7",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-factor-exposure",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-factor-exposure",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.9 Factor Exposure Analysis",
    "text": "12.9 Factor Exposure Analysis\nDifferent weighting schemes induce different factor exposures, which may explain their return differences. We regress each portfolio’s excess returns on the Vietnamese Fama-French factors:\n\\[\nR_{p,t} - R_{f,t} = \\alpha_p + \\beta_p^{MKT}(R_{m,t} - R_{f,t}) + \\beta_p^{SMB} \\text{SMB}_t + \\beta_p^{HML} \\text{HML}_t + \\varepsilon_{p,t}\n\\tag{12.10}\\]\n\n# Retrieve Vietnamese factor returns from DataCore\nfactors = client.get_factor_returns(\n    market='vietnam',\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    factors=['mkt_excess', 'smb', 'hml', 'wml']\n)\n\n# Run factor regressions for each portfolio\nfactor_results = {}\nfor name, df in portfolios.items():\n    merged = pd.merge(\n        df[['month_end', 'port_return']],\n        factors,\n        on='month_end'\n    )\n    rf_m = (1.04) ** (1/12) - 1\n    merged['excess'] = merged['port_return'] - rf_m\n    \n    model = sm.OLS(\n        merged['excess'],\n        sm.add_constant(merged[['mkt_excess', 'smb', 'hml', 'wml']])\n    ).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n    \n    factor_results[name] = {\n        'Alpha (ann.)': model.params['const'] * 12,\n        'Alpha t-stat': model.tvalues['const'],\n        'MKT': model.params['mkt_excess'],\n        'SMB': model.params['smb'],\n        'HML': model.params['hml'],\n        'WML': model.params['wml'],\n        'R²': model.rsquared\n    }\n\nfactor_df = pd.DataFrame(factor_results).T\nprint(factor_df.round(3).to_string())\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nplot_data = factor_df[['MKT', 'SMB', 'HML', 'WML']].copy()\nim = ax.imshow(plot_data.values, cmap='RdBu_r', aspect='auto',\n               vmin=-1.5, vmax=1.5)\n\nax.set_xticks(range(len(plot_data.columns)))\nax.set_xticklabels(plot_data.columns, fontsize=10)\nax.set_yticks(range(len(plot_data.index)))\nax.set_yticklabels(plot_data.index, fontsize=9)\n\n# Add text annotations\nfor i in range(len(plot_data.index)):\n    for j in range(len(plot_data.columns)):\n        val = plot_data.values[i, j]\n        color = 'white' if abs(val) &gt; 0.8 else 'black'\n        ax.text(j, i, f'{val:.2f}', ha='center', va='center',\n                color=color, fontsize=9)\n\nplt.colorbar(im, ax=ax, label='Factor Loading')\nax.set_title('Factor Exposures by Weighting Scheme')\nplt.tight_layout()\nplt.show()\n\n\nFigure 12.8",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-practical",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-practical",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.10 Practical Guidance for Vietnam",
    "text": "12.10 Practical Guidance for Vietnam\nThe preceding analysis yields several practical recommendations for researchers and investors working with Vietnamese equities:\nFor academic factor research: VW portfolios remain the default for asset pricing tests because they represent the investable opportunity set and avoid inflating alpha estimates with small-cap illiquidity premia. When EW portfolios are used (e.g., to give equal influence to each stock in cross-sectional sorts), researchers should report both VW and EW results and discuss the sensitivity. Fama and French (2008) follow this practice systematically.\nFor fund management: The choice depends on AUM and mandate. At AUM below VND 500 billion, capped VW or fundamental weighting offers a practical compromise between diversification and implementability. At larger AUM, pure VW or sector-capped VW is more realistic. Risk parity and minimum variance are suitable for low-volatility mandates but require robust covariance estimation and quarterly rebalancing.\nFor index construction: Vietnamese index providers (VN30, VNINDEX) use variants of capped VW. The analysis suggests that the cap level significantly affects the index’s diversification properties and tracking error relative to the uncapped VW market. A 10% cap balances concentration reduction against turnover.\nFor transaction cost management: In all schemes, the marginal benefit of rebalancing declines faster than the marginal cost as frequency increases beyond quarterly. Calendar-based quarterly rebalancing or threshold-based rebalancing (with a 1–2% tolerance band) provides the best cost-benefit trade-off in the Vietnamese market.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "14_portfolio_weighting_and_rebalancing.html#sec-weight-summary",
    "href": "14_portfolio_weighting_and_rebalancing.html#sec-weight-summary",
    "title": "12  Portfolio Weighting and Rebalancing",
    "section": "12.11 Summary",
    "text": "12.11 Summary\n\n\n\nTable 12.3: Summary comparison of weighting schemes for Vietnamese equities.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimension\nVW\nEW\nCapped VW\nFundamental\nMin Var\nRisk Parity\n\n\n\n\nTurnover\nVery low\nHigh\nLow\nLow\nModerate\nModerate\n\n\nConcentration\nHigh\nNone\nModerate\nModerate\nVariable\nLow\n\n\nSize tilt\nLarge\nSmall\nModerate\nLarge-mid\nLow-vol\nMixed\n\n\nData required\nPrices\nNone\nPrices\nAccounting\nReturns cov.\nReturns cov.\n\n\nScale sensitivity\nLow\nHigh\nLow\nLow\nModerate\nModerate\n\n\nRebal. frequency\nPassive\nMonthly\nMonthly/Quarterly\nAnnual\nQuarterly\nQuarterly\n\n\nBest use case\nBenchmarks, large AUM\nCross-sectional tests\nIndex tracking\nLong-term investing\nLow-vol mandates\nBalanced risk\n\n\n\n\n\n\nThe choice of weighting scheme is not merely a technical detail—it reflects a substantive economic decision about the relative importance of diversification, investability, and cost control. In the Vietnamese market, where the capitalization distribution is highly skewed and small-cap liquidity is thin, this choice has larger consequences than in developed markets. Researchers who report results under only one weighting scheme risk conclusions that are specific to that scheme rather than reflective of a genuine economic relationship.\n\n\n\n\n\n\n\nAmihud, Yakov. 2002. “Illiquidity and Stock Returns: Cross-Section and Time-Series Effects.” Journal of Financial Markets 5 (1): 31–56.\n\n\nArnott, Robert D, Jason Hsu, and Philip Moore. 2005. “Fundamental Indexation.” Financial Analysts Journal 61 (2): 83–99.\n\n\nChoueifaty, Yves. 2008. “Towards Maximum Diversification.” Available at SSRN 4063676.\n\n\nClarke, Roger, Harindra De Silva, and Steven Thorley. 2011. “Minimum-Variance Portfolio Composition.” Journal of Portfolio Management 37 (2): 31.\n\n\nDeMiguel, Victor, Lorenzo Garlappi, and Raman Uppal. 2009. “Optimal Versus Naive Diversification: How Inefficient Is the 1/n Portfolio Strategy?” The Review of Financial Studies 22 (5): 1915–53.\n\n\nFama, Eugene F, and Kenneth R French. 2008. “Dissecting Anomalies.” The Journal of Finance 63 (4): 1653–78.\n\n\nGârleanu, Nicolae, and Lasse Heje Pedersen. 2013. “Dynamic Trading with Predictable Returns and Transaction Costs.” The Journal of Finance 68 (6): 2309–40.\n\n\nHsu, Jason C. 2004. “Cap-Weighted Portfolios Are Sub-Optimal Portfolios.” Journal of Investment Management 4 (3).\n\n\nJagannathan, Ravi, and Tongshu Ma. 2003. “Risk Reduction in Large Portfolios: Why Imposing the Wrong Constraints Helps.” The Journal of Finance 58 (4): 1651–83.\n\n\nLedoit, Olivier, and Michael Wolf. 2004. “A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices.” Journal of Multivariate Analysis 88 (2): 365–411.\n\n\nMaillard, Sébastien, Thierry Roncalli, and Jérôme Teı̈letche. 2010. “The Properties of Equally Weighted Risk Contribution Portfolios.” Journal of Portfolio Management 36 (4): 60.\n\n\nPlyakha, Yuliya, Raman Uppal, and Grigory Vilkov. 2021. “Equal or Value Weighting? Implications for Asset-Pricing Tests.” In Financial Risk Management and Modeling, 295–347. Springer.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Portfolio Weighting and Rebalancing</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html",
    "href": "15_missing_data_and_survivorship.html",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "",
    "text": "13.1 Taxonomy of Data Problems\nEvery empirical study in finance implicitly assumes that the data it analyzes are representative of the population it claims to study. When this assumption fails, because delisted firms are excluded, because databases begin coverage only after firms have survived, or because trading gaps create missing return observations, the resulting estimates are biased. In the U.S. context, Shumway (1997) showed that ignoring delisting returns biases average returns upward by approximately 1% per year for NYSE stocks and substantially more for Nasdaq stocks, with severe consequences for anomaly-based strategies that overweight small, distressed firms.\nThe Vietnamese market presents a distinct and, in many ways, more acute set of data integrity challenges. The market is young. HOSE opened in July 2000 with only two listed stocks, and the number of listings grew rapidly through the mid-2000s equitization wave. This means that any sample beginning before roughly 2007 suffers from severe new-listing bias: the early cross-section is tiny and unrepresentative. Delistings are common and often involuntary, driven by losses exceeding charter capital, failure to file financial statements, or SSC enforcement actions rather than by mergers or going-private transactions as in the U.S. These involuntary delistings are systematically associated with negative terminal returns. And the prevalence of zero-trading days among small-cap stocks creates return gaps that look like missing data but actually reflect illiquidity.\nThis chapter provides the tools to diagnose and, where possible, correct these problems.\nMissing data in financial research is not monolithic. The consequences depend critically on the mechanism generating the missingness. Rubin (1976) and Little and Rubin (2019) classify missing data into three types:\nIn the Vietnamese context, we encounter all three types, often simultaneously (Table 13.1).",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-taxonomy",
    "href": "15_missing_data_and_survivorship.html#sec-missing-taxonomy",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "",
    "text": "Missing Completely at Random (MCAR). The probability of a missing observation does not depend on any observed or unobserved variable. Example: a data vendor’s server crashes on a random Tuesday, losing that day’s records. MCAR is the most benign case, complete-case analysis (dropping missing observations) produces unbiased but less efficient estimates.\nMissing at Random (MAR). The probability of missingness depends on observed variables but not on the missing value itself, conditional on observables. Example: small firms are more likely to have missing analyst coverage, but conditional on firm size, whether coverage is missing is unrelated to the firm’s true expected return. MAR allows unbiased estimation through methods that condition on the observed predictors of missingness.\nMissing Not at Random (MNAR). The probability of missingness depends on the missing value itself. Example: firms with the worst performance are most likely to delist and disappear from the database. MNAR is a pathological case and, unfortunately, the most common in financial data. Survivorship bias and delisting bias are both instances of MNAR because the event that removes the observation (delisting) is correlated with the variable of interest (returns).\n\n\n\n\n\nTable 13.1: Taxonomy of missing data in Vietnamese equity databases\n\n\n\n\n\n\n\n\n\n\nData Problem\nMissingness Type\nMechanism in Vietnam\n\n\n\n\nZero-trading days\nMAR/MNAR\nSmall/illiquid stocks; correlated with returns\n\n\nPrice limit hits\nMNAR\nTrue return truncated at limit; observed return censored\n\n\nDelisting\nMNAR\nWorst-performing firms exit; returns disappear\n\n\nLate listing coverage\nSelection bias\nDatabase begins after firm survives initial period\n\n\nExchange transfers\nAdministrative\nHOSE→HNX or UPCoM transfers break ticker continuity\n\n\nSuspended trading\nMNAR\nSuspension precedes negative events; returns missing",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-data",
    "href": "15_missing_data_and_survivorship.html#sec-missing-data",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.2 Data Construction",
    "text": "13.2 Data Construction\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\nfrom datetime import datetime, timedelta\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Complete listing history: includes all firms ever listed, not just current\nlisting_history = client.get_listing_history(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    include_delisted=True,\n    fields=[\n        'ticker', 'company_name', 'exchange', 'listing_date',\n        'delisting_date', 'delisting_reason', 'is_active',\n        'transfer_from', 'transfer_to', 'transfer_date',\n        'ipo_date', 'equitization_date', 'sector'\n    ]\n)\n\n# Daily returns: includes delisted firms' full history\ndaily_returns = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    start_date='2000-07-28',   # HOSE opening date\n    end_date='2024-12-31',\n    include_delisted=True,      # Critical flag\n    fields=[\n        'ticker', 'date', 'close', 'adjusted_close', 'volume',\n        'turnover_value', 'market_cap', 'shares_outstanding',\n        'price_limit_hit'       # +1 = limit up, -1 = limit down, 0 = neither\n    ]\n)\n\n# Monthly returns (pre-computed, survivorship-bias-free)\nmonthly_returns = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    start_date='2000-07-28',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'volume_avg_20d', 'n_trading_days', 'n_zero_volume_days'\n    ]\n)\n\nprint(f\"Listing history: {listing_history.shape[0]:,} firms\")\nprint(f\"  Active: {listing_history['is_active'].sum():,}\")\nprint(f\"  Delisted: {(~listing_history['is_active']).sum():,}\")\nprint(f\"Daily observations: {daily_returns.shape[0]:,}\")\nprint(f\"Monthly observations: {monthly_returns.shape[0]:,}\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-listing-dynamics",
    "href": "15_missing_data_and_survivorship.html#sec-missing-listing-dynamics",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.3 Listing Dynamics in Vietnam",
    "text": "13.3 Listing Dynamics in Vietnam\n\n13.3.1 The Growth of the Vietnamese Market\nThe Vietnamese stock market’s short history creates a distinctive pattern: the investable universe has grown from near-zero to over 1,500 listed firms in approximately two decades. This rapid growth means that the composition of the market at any point in time is heavily influenced by the vintage of listings, and that studies using early data face extreme small-sample problems.\n\n\n\nlisting_history['listing_date'] = pd.to_datetime(listing_history['listing_date'])\nlisting_history['delisting_date'] = pd.to_datetime(listing_history['delisting_date'])\n\n# Count active listings at each month-end\nmonths = pd.date_range('2000-07-01', '2024-12-31', freq='M')\nactive_counts = []\n\nfor month in months:\n    for exchange in ['HOSE', 'HNX', 'UPCoM']:\n        active = listing_history[\n            (listing_history['exchange'] == exchange) &\n            (listing_history['listing_date'] &lt;= month) &\n            ((listing_history['delisting_date'].isna()) |\n             (listing_history['delisting_date'] &gt; month))\n        ]\n        active_counts.append({\n            'month': month,\n            'exchange': exchange,\n            'n_active': len(active)\n        })\n\nactive_df = pd.DataFrame(active_counts)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Active listings over time\nfor exchange, color in [('HOSE', '#2C5F8A'), ('HNX', '#E67E22'),\n                         ('UPCoM', '#27AE60')]:\n    subset = active_df[active_df['exchange'] == exchange]\n    axes[0].plot(subset['month'], subset['n_active'],\n                 color=color, linewidth=2, label=exchange)\n\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('Number of Active Listings')\naxes[0].set_title('Panel A: Active Listings by Exchange')\naxes[0].legend()\n\n# Panel B: Annual listings and delistings\nlisting_history['listing_year'] = listing_history['listing_date'].dt.year\nlisting_history['delisting_year'] = listing_history['delisting_date'].dt.year\n\nannual_listings = (\n    listing_history\n    .groupby('listing_year')\n    .size()\n    .reindex(range(2000, 2025), fill_value=0)\n)\nannual_delistings = (\n    listing_history\n    .dropna(subset=['delisting_year'])\n    .groupby('delisting_year')\n    .size()\n    .reindex(range(2000, 2025), fill_value=0)\n)\n\nx = np.arange(2000, 2025)\naxes[1].bar(x - 0.2, annual_listings.values, width=0.4,\n            color='#27AE60', alpha=0.85, label='New Listings')\naxes[1].bar(x + 0.2, annual_delistings.values, width=0.4,\n            color='#C0392B', alpha=0.85, label='Delistings')\naxes[1].set_xlabel('Year')\naxes[1].set_ylabel('Number of Firms')\naxes[1].set_title('Panel B: Annual Listings and Delistings')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 13.1\n\n\n\n\n\n13.3.2 Delisting Reasons\nVietnamese delistings are not homogeneous. The SSC mandates delisting for specific regulatory violations, but firms may also voluntarily delist, merge, or transfer between exchanges. The reason for delisting matters because it determines the likely terminal return.\n\ndelisted = listing_history[listing_history['delisting_date'].notna()].copy()\n\n# Standardize delisting reasons into categories\nreason_map = {\n    'losses_exceed_charter': 'Involuntary - Financial Distress',\n    'bankruptcy': 'Involuntary - Financial Distress',\n    'failure_to_file': 'Involuntary - Regulatory',\n    'audit_qualification': 'Involuntary - Regulatory',\n    'ssc_enforcement': 'Involuntary - Regulatory',\n    'merger': 'Voluntary - M&A',\n    'going_private': 'Voluntary - Going Private',\n    'transfer_exchange': 'Transfer',\n    'voluntary': 'Voluntary - Other',\n    'other': 'Other/Unknown'\n}\ndelisted['reason_category'] = (\n    delisted['delisting_reason']\n    .map(reason_map)\n    .fillna('Other/Unknown')\n)\n\n# Tabulate\nreason_counts = (\n    delisted['reason_category']\n    .value_counts()\n    .to_frame('Count')\n)\nreason_counts['Percentage'] = (\n    reason_counts['Count'] / reason_counts['Count'].sum() * 100\n)\n\nprint(\"Delisting Reasons:\")\nprint(reason_counts.round(1).to_string())\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Pie chart\ncolors_pie = ['#C0392B', '#E74C3C', '#8E44AD', '#27AE60',\n              '#2C5F8A', '#F1C40F', '#BDC3C7']\naxes[0].pie(reason_counts['Count'], labels=reason_counts.index,\n            colors=colors_pie[:len(reason_counts)],\n            autopct='%1.0f%%', startangle=90, textprops={'fontsize': 8})\naxes[0].set_title('Panel A: Delisting Reasons')\n\n# Panel B: Delisting reasons over time\ndelisted['year'] = delisted['delisting_date'].dt.year\nreason_by_year = pd.crosstab(delisted['year'], delisted['reason_category'])\nreason_by_year = reason_by_year.reindex(range(2000, 2025), fill_value=0)\n\nreason_by_year.plot(kind='bar', stacked=True, ax=axes[1],\n                     colormap='Set2', edgecolor='white', width=0.8)\naxes[1].set_xlabel('Year')\naxes[1].set_ylabel('Number of Delistings')\naxes[1].set_title('Panel B: Delisting Reasons Over Time')\naxes[1].legend(fontsize=7, loc='upper left')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 13.2\n\n\n\n\n\n13.3.3 Firm Characteristics at Delisting\nDo delisted firms differ systematically from survivors? If so, excluding them biases the observed distribution of firm characteristics.\n\n\n\n# Get fundamentals in the last available year before delisting\nlast_year_delisted = (\n    delisted[['ticker', 'delisting_date']]\n    .assign(last_fy=lambda x: x['delisting_date'].dt.year - 1)\n)\n\nfundamentals = client.get_fundamentals(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    start_date='2005-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'fiscal_year', 'total_assets', 'net_income',\n        'total_equity', 'revenue', 'market_cap'\n    ]\n)\n\n# Characteristics of delisted firms (last year before delisting)\ndelist_chars = (\n    last_year_delisted\n    .merge(fundamentals.rename(columns={'fiscal_year': 'last_fy'}),\n           on=['ticker', 'last_fy'], how='inner')\n)\ndelist_chars['roa'] = delist_chars['net_income'] / delist_chars['total_assets']\ndelist_chars['leverage'] = (\n    (delist_chars['total_assets'] - delist_chars['total_equity'])\n    / delist_chars['total_assets']\n)\ndelist_chars['log_assets'] = np.log(delist_chars['total_assets'])\ndelist_chars['group'] = 'Delisted'\n\n# Characteristics of all active firms (pooled)\nall_chars = fundamentals.copy()\nall_chars['roa'] = all_chars['net_income'] / all_chars['total_assets']\nall_chars['leverage'] = (\n    (all_chars['total_assets'] - all_chars['total_equity'])\n    / all_chars['total_assets']\n)\nall_chars['log_assets'] = np.log(all_chars['total_assets'])\nall_chars['group'] = 'All Active'\n\n# Compare distributions\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nvariables = [\n    ('log_assets', 'Log Total Assets', axes[0, 0]),\n    ('roa', 'Return on Assets', axes[0, 1]),\n    ('leverage', 'Leverage Ratio', axes[1, 0]),\n]\n\nfor col, label, ax in variables:\n    for grp, color in [('All Active', '#2C5F8A'), ('Delisted', '#C0392B')]:\n        if grp == 'Delisted':\n            data = delist_chars[col].dropna()\n        else:\n            data = all_chars[col].dropna()\n        data = data[np.isfinite(data)]\n        ax.hist(data, bins=50, density=True, alpha=0.5,\n                color=color, label=grp, edgecolor='white')\n    ax.set_xlabel(label)\n    ax.set_ylabel('Density')\n    ax.legend()\n\n# Panel D: Market cap distribution\nfor grp, color in [('All Active', '#2C5F8A'), ('Delisted', '#C0392B')]:\n    if grp == 'Delisted':\n        data = np.log(delist_chars['market_cap'].dropna())\n    else:\n        data = np.log(all_chars['market_cap'].dropna())\n    data = data[np.isfinite(data)]\n    axes[1, 1].hist(data, bins=50, density=True, alpha=0.5,\n                     color=color, label=grp, edgecolor='white')\naxes[1, 1].set_xlabel('Log Market Cap')\naxes[1, 1].set_ylabel('Density')\naxes[1, 1].legend()\n\nplt.suptitle('Characteristics of Delisted vs Active Firms', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Formal comparison\nprint(\"\\nMean Comparison (Delisted vs All Active):\")\nfor col in ['log_assets', 'roa', 'leverage']:\n    d = delist_chars[col].dropna()\n    a = all_chars[col].dropna()\n    d = d[np.isfinite(d)]\n    a = a[np.isfinite(a)]\n    t, p = stats.ttest_ind(d, a, equal_var=False)\n    print(f\"  {col:&lt;15}: Delisted = {d.mean():.3f}, \"\n          f\"Active = {a.mean():.3f}, t = {t:.2f}, p = {p:.4f}\")\n\n\nFigure 13.3",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-survivorship",
    "href": "15_missing_data_and_survivorship.html#sec-missing-survivorship",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.4 Survivorship Bias",
    "text": "13.4 Survivorship Bias\n\n13.4.1 Definition and Magnitude\nSurvivorship bias arises when a study uses only firms that are currently listed (or listed at the end of the sample), excluding firms that delisted during the sample period. Because delisted firms disproportionately experienced negative returns before delisting, their exclusion inflates average returns, understates risk, and distorts cross-sectional patterns.\nWe quantify the magnitude of survivorship bias by comparing portfolio returns computed from the survivorship-bias-free sample (all firms, including those that subsequently delisted) against a survivors-only sample (firms that remained listed through the end of the sample).\n\n# Define survivors: firms active as of 2024-12-31\nsurvivors = set(\n    listing_history[listing_history['is_active']]['ticker']\n)\n\n# Full sample: all firms, including delisted\nfull_sample = monthly_returns.copy()\n\n# Survivors only: restrict to firms still listed at end of sample\nsurvivors_only = monthly_returns[\n    monthly_returns['ticker'].isin(survivors)\n].copy()\n\n# Compute EW monthly portfolio returns\ndef compute_ew_portfolio(df):\n    return (\n        df\n        .groupby('month_end')['monthly_return']\n        .mean()\n        .to_frame('portfolio_return')\n    )\n\ndef compute_vw_portfolio(df):\n    return (\n        df\n        .groupby('month_end')\n        .apply(lambda g: np.average(g['monthly_return'],\n                                     weights=g['market_cap'])\n               if g['market_cap'].sum() &gt; 0 else np.nan)\n        .to_frame('portfolio_return')\n    )\n\new_full = compute_ew_portfolio(full_sample)\new_survivors = compute_ew_portfolio(survivors_only)\nvw_full = compute_vw_portfolio(full_sample)\nvw_survivors = compute_vw_portfolio(survivors_only)\n\n# Merge and compute bias\nbias_ew = pd.merge(\n    ew_full.rename(columns={'portfolio_return': 'full'}),\n    ew_survivors.rename(columns={'portfolio_return': 'survivors'}),\n    left_index=True, right_index=True\n)\nbias_ew['bias'] = bias_ew['survivors'] - bias_ew['full']\n\nbias_vw = pd.merge(\n    vw_full.rename(columns={'portfolio_return': 'full'}),\n    vw_survivors.rename(columns={'portfolio_return': 'survivors'}),\n    left_index=True, right_index=True\n)\nbias_vw['bias'] = bias_vw['survivors'] - bias_vw['full']\n\nprint(\"Survivorship Bias (Annualized):\")\nprint(f\"  EW: {bias_ew['bias'].mean() * 12:.4f} \"\n      f\"({bias_ew['bias'].mean() * 1200:.1f} bps/year)\")\nprint(f\"  VW: {bias_vw['bias'].mean() * 12:.4f} \"\n      f\"({bias_vw['bias'].mean() * 1200:.1f} bps/year)\")\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor i, (bias_df, title) in enumerate(\n    [(bias_ew, 'Panel A: Equal-Weighted'),\n     (bias_vw, 'Panel B: Value-Weighted')]\n):\n    cum_full = (1 + bias_df['full']).cumprod()\n    cum_surv = (1 + bias_df['survivors']).cumprod()\n\n    axes[i].plot(cum_full.index, cum_full,\n                 color='#2C5F8A', linewidth=2, label='Full Sample')\n    axes[i].plot(cum_surv.index, cum_surv,\n                 color='#C0392B', linewidth=2, label='Survivors Only')\n    axes[i].set_ylabel('Cumulative Wealth')\n    axes[i].set_xlabel('Date')\n    axes[i].set_title(title)\n    axes[i].legend()\n    axes[i].set_yscale('log')\n\n    ann_bias = bias_df['bias'].mean() * 12\n    axes[i].text(0.05, 0.95,\n                 f'Annual Bias: {ann_bias*100:.1f}%',\n                 transform=axes[i].transAxes, fontsize=11,\n                 verticalalignment='top',\n                 bbox=dict(facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 13.4\n\n\n\n\n\n13.4.2 Time-Varying Survivorship Bias\nThe magnitude of survivorship bias is not constant. It peaks during and after market downturns, when delisting activity is highest.\n\n\n\nbias_ew['rolling_bias_12m'] = bias_ew['bias'].rolling(12).mean() * 12\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), height_ratios=[2, 1])\n\n# Panel A: Rolling bias\naxes[0].fill_between(\n    bias_ew.index, 0, bias_ew['rolling_bias_12m'] * 100,\n    where=bias_ew['rolling_bias_12m'] &gt; 0,\n    color='#C0392B', alpha=0.4\n)\naxes[0].plot(bias_ew.index, bias_ew['rolling_bias_12m'] * 100,\n             color='#C0392B', linewidth=1.5)\naxes[0].axhline(y=0, color='gray', linewidth=0.5)\naxes[0].set_ylabel('Annualized Bias (%)')\naxes[0].set_title('Panel A: Rolling 12-Month Survivorship Bias (EW)')\n\n# Panel B: Number of delistings per quarter\ndelistings_quarterly = (\n    delisted\n    .set_index('delisting_date')\n    .resample('Q')\n    .size()\n)\naxes[1].bar(delistings_quarterly.index, delistings_quarterly.values,\n            width=80, color='#2C5F8A', alpha=0.7)\naxes[1].set_ylabel('Delistings per Quarter')\naxes[1].set_xlabel('Date')\naxes[1].set_title('Panel B: Quarterly Delisting Activity')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 13.5\n\n\n\n\n\n13.4.3 Survivorship Bias in Cross-Sectional Anomalies\nThe bias is not uniform across strategies. Anomalies that overweight small, distressed, or low-quality firms, precisely the firms most likely to delist, are most severely affected. We test this for the size, value, and momentum anomalies.\n\ndef compute_long_short(df, sort_var, n_quantiles=5):\n    \"\"\"\n    Compute long-short portfolio returns from quintile sorts.\n    Long = top quintile, Short = bottom quintile.\n    \"\"\"\n    results = []\n    for month, group in df.groupby('month_end'):\n        group = group.dropna(subset=[sort_var, 'monthly_return'])\n        if len(group) &lt; 20:\n            continue\n        group['quantile'] = pd.qcut(\n            group[sort_var], n_quantiles, labels=False, duplicates='drop'\n        )\n        long_ret = group[group['quantile'] == n_quantiles - 1]['monthly_return'].mean()\n        short_ret = group[group['quantile'] == 0]['monthly_return'].mean()\n        results.append({\n            'month_end': month,\n            'long': long_ret,\n            'short': short_ret,\n            'long_short': long_ret - short_ret\n        })\n    return pd.DataFrame(results)\n\n# Prepare sort variables\nmonthly_with_chars = monthly_returns.merge(\n    fundamentals[['ticker', 'fiscal_year', 'total_assets',\n                   'net_income', 'total_equity']],\n    left_on=['ticker', monthly_returns['month_end'].dt.year],\n    right_on=['ticker', 'fiscal_year'],\n    how='left'\n)\nmonthly_with_chars['log_mcap'] = np.log(monthly_with_chars['market_cap'])\nmonthly_with_chars['bm'] = (\n    monthly_with_chars['total_equity'] / monthly_with_chars['market_cap']\n)\nmonthly_with_chars['past_12m'] = (\n    monthly_with_chars\n    .groupby('ticker')['monthly_return']\n    .transform(lambda x: x.rolling(12).sum())\n)\n\n# Compute anomalies on full sample and survivors only\nanomaly_bias = {}\nfor anomaly, sort_var, ascending in [\n    ('Size (SMB)', 'log_mcap', True),\n    ('Value (HML)', 'bm', True),\n    ('Momentum (WML)', 'past_12m', True)\n]:\n    full_ls = compute_long_short(\n        monthly_with_chars, sort_var\n    )\n    surv_data = monthly_with_chars[\n        monthly_with_chars['ticker'].isin(survivors)\n    ]\n    surv_ls = compute_long_short(surv_data, sort_var)\n\n    # Merge\n    merged = pd.merge(\n        full_ls[['month_end', 'long_short']].rename(\n            columns={'long_short': 'full'}),\n        surv_ls[['month_end', 'long_short']].rename(\n            columns={'long_short': 'survivors'}),\n        on='month_end'\n    )\n    merged['bias'] = merged['survivors'] - merged['full']\n\n    ann_full = merged['full'].mean() * 12\n    ann_surv = merged['survivors'].mean() * 12\n    ann_bias = merged['bias'].mean() * 12\n\n    anomaly_bias[anomaly] = {\n        'Full Sample (ann.)': ann_full,\n        'Survivors Only (ann.)': ann_surv,\n        'Bias (ann.)': ann_bias,\n        'Bias (% of premium)': ann_bias / ann_full * 100 if ann_full != 0 else np.nan\n    }\n\nanomaly_bias_df = pd.DataFrame(anomaly_bias).T\nprint(\"Survivorship Bias by Anomaly:\")\nprint(anomaly_bias_df.round(4).to_string())",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-delisting",
    "href": "15_missing_data_and_survivorship.html#sec-missing-delisting",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.5 Delisting Bias and Return Imputation",
    "text": "13.5 Delisting Bias and Return Imputation\n\n13.5.1 The Shumway Correction\nShumway (1997) showed that CRSP’s treatment of delisting returns, often recording them as missing or zero, creates a systematic upward bias in average returns. The same problem exists in Vietnamese databases, where the last observed price may precede the actual delisting by days or weeks, and the true terminal return (from last traded price to the value shareholders actually receive) is unrecorded.\nWe implement a delisting return imputation procedure adapted for Vietnam:\nStep 1. For each delisted firm, identify the last trading day with a valid closing price.\nStep 2. Classify the delisting reason to determine the appropriate imputation (Table 13.2).\n\n\n\nTable 13.2: Delisting return imputation rules.\n\n\n\n\n\n\n\n\n\n\nDelisting Reason\nImputed Return\nRationale\n\n\n\n\nM&A / Acquisition\nActual tender offer premium (if available)\nAcquisition at premium\n\n\nGoing private\n0% (or actual buyout price)\nNegotiated exit\n\n\nFinancial distress\n−30% to −100%\nSubstantial loss of value\n\n\nRegulatory violation\n−50%\nPartial loss; some recovery possible\n\n\nExchange transfer\n0% (link to new ticker)\nNo economic event\n\n\n\n\n\n\nStep 3. Apply the imputed return to the month of delisting to complete the return series.\n\ndef impute_delisting_returns(listing_df, daily_df, monthly_df):\n    \"\"\"\n    Impute terminal returns for delisted firms.\n\n    Returns a DataFrame of imputed delisting returns to be\n    appended to the monthly return panel.\n    \"\"\"\n    delisted_firms = listing_df[listing_df['delisting_date'].notna()].copy()\n    imputed = []\n\n    for _, firm in delisted_firms.iterrows():\n        ticker = firm['ticker']\n        delist_date = firm['delisting_date']\n        reason = firm.get('reason_category', firm.get('delisting_reason', ''))\n\n        # Find last trading day\n        firm_daily = daily_df[daily_df['ticker'] == ticker].sort_values('date')\n        if len(firm_daily) == 0:\n            continue\n\n        last_trade = firm_daily.iloc[-1]\n        last_price = last_trade['adjusted_close']\n        last_date = last_trade['date']\n\n        # Check if last trade is already close to delisting date\n        gap_days = (pd.Timestamp(delist_date) - pd.Timestamp(last_date)).days\n        if gap_days &lt; 0:\n            continue  # Data issue\n\n        # Determine imputation based on reason\n        if 'M&A' in str(reason) or 'merger' in str(reason).lower():\n            imputed_return = 0.0  # Conservative; ideally use tender price\n        elif 'Going Private' in str(reason) or 'voluntary' in str(reason).lower():\n            imputed_return = 0.0\n        elif 'Transfer' in str(reason):\n            imputed_return = 0.0  # Not a real delisting\n        elif 'Financial Distress' in str(reason) or 'bankruptcy' in str(reason).lower():\n            imputed_return = -0.50  # Conservative estimate\n        elif 'Regulatory' in str(reason):\n            imputed_return = -0.30\n        else:\n            imputed_return = -0.30  # Default for unknown reasons\n\n        # Assign to the delisting month\n        delist_month = pd.Timestamp(delist_date).to_period('M').to_timestamp()\n\n        imputed.append({\n            'ticker': ticker,\n            'month_end': delist_month,\n            'monthly_return': imputed_return,\n            'market_cap': last_trade.get('market_cap', np.nan),\n            'source': 'imputed_delisting',\n            'delisting_reason': reason,\n            'gap_days': gap_days\n        })\n\n    return pd.DataFrame(imputed)\n\n# Apply imputation\nimputed_returns = impute_delisting_returns(\n    delisted.assign(reason_category=delisted['reason_category']),\n    daily_returns, monthly_returns\n)\n\nprint(f\"Imputed delisting returns: {len(imputed_returns)}\")\nprint(f\"\\nImputed return distribution:\")\nprint(imputed_returns['monthly_return'].value_counts().sort_index())\n\n\n\n13.5.2 Impact of Delisting Return Imputation\n\n# Augmented sample: monthly returns + imputed delisting returns\naugmented = pd.concat([\n    monthly_returns[['ticker', 'month_end', 'monthly_return', 'market_cap']],\n    imputed_returns[['ticker', 'month_end', 'monthly_return', 'market_cap']]\n], ignore_index=True)\n\n# Compare original vs augmented EW portfolios\new_original = compute_ew_portfolio(monthly_returns)\new_augmented = compute_ew_portfolio(augmented)\n\ncomparison = pd.merge(\n    ew_original.rename(columns={'portfolio_return': 'original'}),\n    ew_augmented.rename(columns={'portfolio_return': 'augmented'}),\n    left_index=True, right_index=True\n)\ncomparison['imputation_effect'] = (\n    comparison['augmented'] - comparison['original']\n)\n\nann_original = comparison['original'].mean() * 12\nann_augmented = comparison['augmented'].mean() * 12\nann_effect = comparison['imputation_effect'].mean() * 12\n\nprint(\"Delisting Return Imputation Impact:\")\nprint(f\"  EW without imputation: {ann_original:.4f} ({ann_original*100:.2f}%/yr)\")\nprint(f\"  EW with imputation:    {ann_augmented:.4f} ({ann_augmented*100:.2f}%/yr)\")\nprint(f\"  Difference:            {ann_effect:.4f} ({ann_effect*100:.2f}%/yr)\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-zero-trading",
    "href": "15_missing_data_and_survivorship.html#sec-missing-zero-trading",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.6 Zero-Trading Days and Illiquidity Gaps",
    "text": "13.6 Zero-Trading Days and Illiquidity Gaps\n\n13.6.1 Prevalence of Zero-Trading Days\nA distinctive feature of Vietnamese equity data is the high frequency of zero-volume days (i.e., days on which a listed stock records no trades). These are not true “missing” data in the database sense (the stock is listed and a closing price is recorded, often equal to the previous close), but they represent economically missing information: the observed price is stale and does not reflect current market conditions.\n\n\n\n# Compute zero-volume fraction per firm-year\ndaily_returns['year'] = pd.to_datetime(daily_returns['date']).dt.year\ndaily_returns['zero_volume'] = (daily_returns['volume'] == 0).astype(int)\n\nzero_vol_fy = (\n    daily_returns\n    .groupby(['ticker', 'year'])\n    .agg(\n        n_days=('zero_volume', 'count'),\n        n_zero=('zero_volume', 'sum'),\n        avg_mcap=('market_cap', 'mean')\n    )\n    .reset_index()\n)\nzero_vol_fy['zero_frac'] = zero_vol_fy['n_zero'] / zero_vol_fy['n_days']\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Distribution over time (boxplot by year)\nyears_to_plot = range(2008, 2025)\ndata_by_year = [\n    zero_vol_fy[zero_vol_fy['year'] == y]['zero_frac'].dropna().values\n    for y in years_to_plot\n]\nbp = axes[0].boxplot(data_by_year, positions=range(len(years_to_plot)),\n                      widths=0.6, showfliers=False, patch_artist=True,\n                      medianprops={'color': 'black'})\nfor patch in bp['boxes']:\n    patch.set_facecolor('#2C5F8A')\n    patch.set_alpha(0.6)\naxes[0].set_xticks(range(len(years_to_plot)))\naxes[0].set_xticklabels(years_to_plot, rotation=45, fontsize=8)\naxes[0].set_ylabel('Zero-Volume Fraction')\naxes[0].set_title('Panel A: Zero-Volume Days by Year')\n\n# Panel B: By market cap decile\nzero_vol_fy['mcap_decile'] = pd.qcut(\n    zero_vol_fy['avg_mcap'].rank(method='first'),\n    10, labels=[f'D{i}' for i in range(1, 11)]\n)\ndecile_zero = (\n    zero_vol_fy\n    .groupby('mcap_decile')['zero_frac']\n    .agg(['mean', 'median'])\n)\naxes[1].bar(range(10), decile_zero['mean'],\n            color='#2C5F8A', alpha=0.85, edgecolor='white')\naxes[1].set_xticks(range(10))\naxes[1].set_xticklabels(decile_zero.index)\naxes[1].set_xlabel('Market Cap Decile (D1 = smallest)')\naxes[1].set_ylabel('Mean Zero-Volume Fraction')\naxes[1].set_title('Panel B: Zero-Volume Days by Size')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 13.6\n\n\n\n\n\n13.6.2 Return Measurement During Zero-Trading Periods\nWhen a stock does not trade, the standard approach, using the last available closing price, produces a stale price that understates true volatility and biases returns toward zero. Several approaches exist to handle this:\nApproach 1: Drop zero-volume observations. Simple but discards information and introduces selection bias (if non-trading is correlated with returns).\nApproach 2: Multi-day compounding. Accumulate the return over the entire non-trading gap and assign it to the first day of resumption. This preserves the total return but concentrates it in a single observation.\nApproach 3: Distribute uniformly. Spread the accumulated return evenly across zero-volume days. This is economically unrealistic, but it reduces the impact of single-day outliers.\nApproach 4: Treat as missing and model. Treat zero-volume days as genuinely missing returns and use the Lesmond (2005) zero-return measure as a liquidity proxy.\n\ndef correct_zero_volume_returns(daily_df, method='compound'):\n    \"\"\"\n    Correct returns during zero-volume periods.\n\n    Parameters\n    ----------\n    method : str\n        'compound': assign accumulated return to first non-zero day\n        'distribute': spread return evenly across gap\n        'drop': remove zero-volume observations\n    \"\"\"\n    df = daily_df.copy()\n    df = df.sort_values(['ticker', 'date'])\n    df['daily_return'] = (\n        df.groupby('ticker')['adjusted_close']\n        .pct_change()\n    )\n\n    if method == 'drop':\n        return df[df['volume'] &gt; 0]\n\n    elif method == 'compound':\n        # For each zero-volume streak, accumulate return and\n        # assign to the next trading day\n        results = []\n        for ticker, group in df.groupby('ticker'):\n            group = group.sort_values('date').reset_index(drop=True)\n            accumulated = 0\n            gap_length = 0\n\n            for idx, row in group.iterrows():\n                if row['volume'] == 0:\n                    accumulated += row['daily_return'] if pd.notna(row['daily_return']) else 0\n                    gap_length += 1\n                else:\n                    if gap_length &gt; 0:\n                        # Add accumulated return to this day's return\n                        total_return = (1 + accumulated) * (1 + (row['daily_return'] or 0)) - 1\n                        group.loc[idx, 'daily_return'] = total_return\n                        accumulated = 0\n                        gap_length = 0\n                    results.append(group.loc[idx])\n\n            # If series ends with zero-volume days, include last non-zero\n            if gap_length &gt; 0 and len(results) &gt; 0:\n                last_valid = results[-1].copy()\n                last_valid['daily_return'] = (\n                    (1 + last_valid['daily_return']) * (1 + accumulated) - 1\n                )\n                results[-1] = last_valid\n\n        return pd.DataFrame(results)\n\n    elif method == 'distribute':\n        results = []\n        for ticker, group in df.groupby('ticker'):\n            group = group.sort_values('date').reset_index(drop=True)\n            i = 0\n            while i &lt; len(group):\n                if group.loc[i, 'volume'] &gt; 0:\n                    results.append(group.loc[i])\n                    i += 1\n                else:\n                    # Find end of zero-volume streak\n                    j = i\n                    while j &lt; len(group) and group.loc[j, 'volume'] == 0:\n                        j += 1\n                    # Total return over gap\n                    if j &lt; len(group):\n                        total_ret = (\n                            group.loc[j, 'adjusted_close']\n                            / group.loc[i - 1, 'adjusted_close'] - 1\n                            if i &gt; 0 else 0\n                        )\n                        n_days = j - i + 1\n                        daily_r = (1 + total_ret) ** (1 / n_days) - 1\n                        for k in range(i, j + 1):\n                            row = group.loc[k].copy()\n                            row['daily_return'] = daily_r\n                            results.append(row)\n                    i = j + 1\n\n        return pd.DataFrame(results)\n\n# Apply corrections and compare\nfor method in ['drop', 'compound', 'distribute']:\n    corrected = correct_zero_volume_returns(\n        daily_returns.head(500000), method=method\n    )\n    mean_ret = corrected['daily_return'].mean() * 252\n    vol = corrected['daily_return'].std() * np.sqrt(252)\n    print(f\"{method:&lt;12}: Ann. Return = {mean_ret:.4f}, \"\n          f\"Ann. Vol = {vol:.4f}, N = {len(corrected):,}\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-lookahead",
    "href": "15_missing_data_and_survivorship.html#sec-missing-lookahead",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.7 Look-Ahead Bias",
    "text": "13.7 Look-Ahead Bias\n\n13.7.1 Definition\nLook-ahead bias occurs when a study uses information that was not available at the time the investment decision would have been made. In the Vietnamese context, the most common sources are:\n\nConditioning on survival. Selecting firms based on their end-of-sample listing status implicitly uses future information (whether the firm will delist).\nUsing revised financial data. Vietnamese firms often restate financial statements after audit. Using the restated figures rather than the originally reported figures introduces look-ahead bias.\nBackfill bias. When a database adds a new firm, it may backfill historical data, creating the illusion that the firm was available for selection before its actual listing date.\nPoint-in-time accounting data. Using annual financial data as of the fiscal year-end rather than the date the financial statements were publicly filed assumes the data were available immediately.\n\n\n\n13.7.2 Point-in-Time Adjustment\nWe implement a point-in-time adjustment for accounting data that respects the actual reporting lag:\n\ndef point_in_time_merge(monthly_df, fundamentals_df, filings_df,\n                         lag_months=0):\n    \"\"\"\n    Merge accounting data with monthly returns respecting\n    the actual filing date (point-in-time).\n\n    Parameters\n    ----------\n    monthly_df : DataFrame with ticker, month_end\n    fundamentals_df : DataFrame with ticker, fiscal_year, and accounting vars\n    filings_df : DataFrame with ticker, fiscal_year, filing_date\n    lag_months : int, additional safety lag beyond filing date\n    \"\"\"\n    # Merge fundamentals with filing dates\n    fund_with_date = fundamentals_df.merge(\n        filings_df[['ticker', 'fiscal_year', 'filing_date']],\n        on=['ticker', 'fiscal_year'], how='left'\n    )\n\n    # If filing date is missing, assume available 4 months after FY end\n    fund_with_date['filing_date'] = pd.to_datetime(\n        fund_with_date['filing_date']\n    )\n    fund_with_date['fy_end'] = pd.to_datetime(\n        fund_with_date['fiscal_year'].astype(str) + '-12-31'\n    )\n    fund_with_date['available_date'] = fund_with_date['filing_date'].fillna(\n        fund_with_date['fy_end'] + pd.DateOffset(months=4)\n    )\n\n    # Add safety lag\n    if lag_months &gt; 0:\n        fund_with_date['available_date'] += pd.DateOffset(months=lag_months)\n\n    # For each firm-month, find the most recent accounting data\n    # that was available (filing_date &lt;= month_end)\n    results = []\n    for _, row in monthly_df.iterrows():\n        ticker = row['ticker']\n        month = row['month_end']\n\n        available = fund_with_date[\n            (fund_with_date['ticker'] == ticker) &\n            (fund_with_date['available_date'] &lt;= month)\n        ]\n\n        if len(available) &gt; 0:\n            latest = available.sort_values('fiscal_year').iloc[-1]\n            result = row.to_dict()\n            for col in ['total_assets', 'net_income', 'total_equity',\n                        'revenue']:\n                if col in latest:\n                    result[col] = latest[col]\n            result['data_fiscal_year'] = latest['fiscal_year']\n            result['data_lag_months'] = (\n                (pd.Timestamp(month) - pd.Timestamp(latest['available_date']))\n                .days / 30.44\n            )\n            results.append(result)\n\n    return pd.DataFrame(results)\n\n# Example: compare point-in-time vs naive merge\nfilings = client.get_filings(\n    exchanges=['HOSE', 'HNX'],\n    report_types=['annual'],\n    fields=['ticker', 'fiscal_year', 'filing_date']\n)\n\nprint(\"Point-in-time merge vs naive merge:\")\nprint(\"  Naive: use fiscal year directly (introduces look-ahead bias)\")\nprint(\"  PIT: use only data available as of the portfolio formation date\")\n\n\n\n13.7.3 Quantifying Look-Ahead Bias in Value Strategies\nValue strategies sort stocks on book-to-market ratios computed from accounting data. Using end-of-fiscal-year data without respecting reporting lags inflates the value premium because it implicitly uses information that was not yet publicly available.\n\n# Naive approach: use fiscal year data immediately\nmonthly_naive = monthly_returns.merge(\n    fundamentals[['ticker', 'fiscal_year', 'total_equity']],\n    left_on=['ticker', monthly_returns['month_end'].dt.year],\n    right_on=['ticker', 'fiscal_year'],\n    how='left'\n)\nmonthly_naive['bm_naive'] = (\n    monthly_naive['total_equity'] / monthly_naive['market_cap']\n)\n\n# Point-in-time approach (using 4-month lag as conservative default)\nmonthly_pit = monthly_returns.copy()\nmonthly_pit['bm_pit'] = np.nan  # Would be filled by point_in_time_merge\n\n# For demonstration: approximate PIT by using t-1 fiscal year data\n# (ensures data were available at formation date)\nfund_lagged = fundamentals.copy()\nfund_lagged['merge_year'] = fund_lagged['fiscal_year'] + 1\nmonthly_pit = monthly_pit.merge(\n    fund_lagged[['ticker', 'merge_year', 'total_equity']].rename(\n        columns={'merge_year': 'year'}),\n    left_on=['ticker', monthly_pit['month_end'].dt.year],\n    right_on=['ticker', 'year'],\n    how='left'\n)\nmonthly_pit['bm_pit'] = (\n    monthly_pit['total_equity'] / monthly_pit['market_cap']\n)\n\n# Compute HML for both approaches\nhml_naive = compute_long_short(monthly_naive, 'bm_naive')\nhml_pit = compute_long_short(monthly_pit, 'bm_pit')\n\nann_naive = hml_naive['long_short'].mean() * 12\nann_pit = hml_pit['long_short'].mean() * 12\n\nprint(\"Value Premium (HML):\")\nprint(f\"  Naive (look-ahead):     {ann_naive:.4f} ({ann_naive*100:.2f}%/yr)\")\nprint(f\"  Point-in-time:          {ann_pit:.4f} ({ann_pit*100:.2f}%/yr)\")\nprint(f\"  Look-ahead inflation:   {(ann_naive - ann_pit)*100:.2f}%/yr\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-transfers",
    "href": "15_missing_data_and_survivorship.html#sec-missing-transfers",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.8 Exchange Transfers and Ticker Discontinuities",
    "text": "13.8 Exchange Transfers and Ticker Discontinuities\n\n13.8.1 The Transfer Problem\nVietnamese firms frequently transfer between exchanges (e.g., from HNX to HOSE upon meeting HOSE’s listing requirements, or from HOSE to UPCoM/HNX following regulatory issues). These transfers can break the continuity of return series if the database treats each exchange listing as a separate entity.\n\ntransfers = listing_history[\n    listing_history['transfer_from'].notna()\n].copy()\n\nprint(f\"Total exchange transfers: {len(transfers)}\")\nprint(f\"\\nTransfer patterns:\")\ntransfer_pattern = transfers.groupby(\n    ['transfer_from', 'transfer_to']\n).size().sort_values(ascending=False)\nprint(transfer_pattern.head(10))\n\n\ndef link_transfer_returns(monthly_df, transfers_df):\n    \"\"\"\n    Link return series across exchange transfers to create\n    continuous firm-level return histories.\n    \"\"\"\n    # Build mapping: old_ticker -&gt; new_ticker -&gt; transfer_date\n    transfer_map = {}\n    for _, row in transfers_df.iterrows():\n        old_ticker = row.get('transfer_from_ticker', row['ticker'])\n        new_ticker = row['ticker']\n        transfer_date = row['transfer_date']\n\n        if old_ticker and new_ticker and old_ticker != new_ticker:\n            transfer_map[old_ticker] = {\n                'new_ticker': new_ticker,\n                'date': transfer_date\n            }\n\n    # Create unified ticker mapping\n    df = monthly_df.copy()\n    df['unified_ticker'] = df['ticker']\n\n    for old_t, info in transfer_map.items():\n        mask = (\n            (df['ticker'] == old_t) &\n            (df['month_end'] &lt; info['date'])\n        )\n        df.loc[mask, 'unified_ticker'] = info['new_ticker']\n\n    n_linked = sum(1 for t in transfer_map if t in df['ticker'].values)\n    print(f\"Linked {n_linked} transfer pairs\")\n\n    return df\n\nlinked = link_transfer_returns(monthly_returns, transfers)",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-sensitivity",
    "href": "15_missing_data_and_survivorship.html#sec-missing-sensitivity",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.9 Sensitivity Analysis Framework",
    "text": "13.9 Sensitivity Analysis Framework\n\n13.9.1 How Fragile Are Your Results?\nRather than choosing a single approach to handle missing data, a robust study tests how sensitive its conclusions are to different assumptions. We implement a systematic sensitivity framework that re-runs a given analysis under multiple data treatment assumptions.\n\ndef sensitivity_analysis(monthly_df, listing_df, sort_variable,\n                          compute_fn):\n    \"\"\"\n    Run an analysis under multiple data treatment assumptions.\n\n    Parameters\n    ----------\n    monthly_df : Full monthly return panel (including delisted)\n    listing_df : Listing history with delisting info\n    sort_variable : Column name for portfolio sorting\n    compute_fn : Function that takes a DataFrame and returns a\n                 scalar (e.g., annualized long-short return)\n\n    Returns\n    -------\n    DataFrame with results under each assumption\n    \"\"\"\n    survivors = set(listing_df[listing_df['is_active']]['ticker'])\n    results = {}\n\n    # 1. Survivors only (maximum bias)\n    surv_only = monthly_df[monthly_df['ticker'].isin(survivors)]\n    results['Survivors Only'] = compute_fn(surv_only, sort_variable)\n\n    # 2. Full sample, no delisting imputation\n    results['Full Sample (no imputation)'] = compute_fn(\n        monthly_df, sort_variable\n    )\n\n    # 3. Full sample + conservative delisting imputation (-50%)\n    augmented_50 = monthly_df.copy()\n    # Append imputed returns at -50%\n    for _, firm in listing_df[listing_df['delisting_date'].notna()].iterrows():\n        if 'Financial Distress' in str(firm.get('reason_category', '')):\n            augmented_50 = pd.concat([augmented_50, pd.DataFrame([{\n                'ticker': firm['ticker'],\n                'month_end': firm['delisting_date'],\n                'monthly_return': -0.50,\n                'market_cap': np.nan,\n                sort_variable: np.nan\n            }])], ignore_index=True)\n    results['Full + Impute -50%'] = compute_fn(\n        augmented_50, sort_variable\n    )\n\n    # 4. Full sample + aggressive delisting imputation (-100%)\n    augmented_100 = monthly_df.copy()\n    for _, firm in listing_df[listing_df['delisting_date'].notna()].iterrows():\n        if 'Financial Distress' in str(firm.get('reason_category', '')):\n            augmented_100 = pd.concat([augmented_100, pd.DataFrame([{\n                'ticker': firm['ticker'],\n                'month_end': firm['delisting_date'],\n                'monthly_return': -1.00,\n                'market_cap': np.nan,\n                sort_variable: np.nan\n            }])], ignore_index=True)\n    results['Full + Impute -100%'] = compute_fn(\n        augmented_100, sort_variable\n    )\n\n    # 5. Exclude bottom market cap quintile (liquidity filter)\n    liquid = monthly_df.copy()\n    liquid['mcap_quintile'] = (\n        liquid.groupby('month_end')['market_cap']\n        .transform(lambda x: pd.qcut(x, 5, labels=False, duplicates='drop'))\n    )\n    liquid = liquid[liquid['mcap_quintile'] &gt; 0]\n    results['Exclude Bottom Quintile'] = compute_fn(\n        liquid, sort_variable\n    )\n\n    return pd.DataFrame.from_dict(results, orient='index',\n                                   columns=['Result'])\n\n# Example: sensitivity of size premium\ndef compute_size_premium(df, sort_var):\n    ls = compute_long_short(df, sort_var, n_quantiles=5)\n    return ls['long_short'].mean() * 12 if len(ls) &gt; 0 else np.nan\n\n# Would need sort variable in the data; illustrative call:\n# sensitivity_results = sensitivity_analysis(\n#     monthly_with_chars, listing_history, 'log_mcap',\n#     compute_size_premium\n# )\n\n\n\n\n# Illustrative: create synthetic sensitivity results for plotting\nassumptions = [\n    'Survivors Only',\n    'Full Sample\\n(no imputation)',\n    'Full +\\nImpute -30%',\n    'Full +\\nImpute -50%',\n    'Full +\\nImpute -100%',\n    'Exclude Bottom\\nMcap Quintile'\n]\n\n# Hypothetical results (would be computed from actual data)\nsize_premium = [0.08, 0.06, 0.055, 0.05, 0.04, 0.07]\nvalue_premium = [0.07, 0.065, 0.063, 0.06, 0.055, 0.068]\nmomentum_premium = [0.10, 0.08, 0.075, 0.07, 0.06, 0.09]\n\nfig, ax = plt.subplots(figsize=(14, 6))\nx = np.arange(len(assumptions))\nwidth = 0.25\n\nax.bar(x - width, [s * 100 for s in size_premium], width,\n       color='#2C5F8A', alpha=0.85, label='Size (SMB)')\nax.bar(x, [v * 100 for v in value_premium], width,\n       color='#27AE60', alpha=0.85, label='Value (HML)')\nax.bar(x + width, [m * 100 for m in momentum_premium], width,\n       color='#E67E22', alpha=0.85, label='Momentum (WML)')\n\nax.set_xticks(x)\nax.set_xticklabels(assumptions, fontsize=9)\nax.set_ylabel('Annualized Premium (%)')\nax.set_title('Anomaly Premium Sensitivity to Data Treatment')\nax.legend()\nax.axhline(y=0, color='gray', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 13.7",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-recommendations",
    "href": "15_missing_data_and_survivorship.html#sec-missing-recommendations",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.10 Practical Recommendations",
    "text": "13.10 Practical Recommendations\nBased on the analysis in this chapter, we offer the following recommendations for researchers working with Vietnamese equity data:\n1. Always use survivorship-bias-free databases. When querying DataCore.vn (or any database), explicitly request include_delisted=True. Never condition on end-of-sample listing status when constructing investment universes.\n2. Impute delisting returns. For involuntary delistings (financial distress, regulatory enforcement), impute a terminal return of −30% to −50% in the delisting month. Report results across a range of imputation assumptions as a robustness check. For voluntary delistings and exchange transfers, impute 0%.\n3. Respect point-in-time data availability. Use accounting data only after its public filing date, not as of the fiscal year-end. In Vietnam, the standard lag is 90 days for annual reports; use a conservative 4–6 month lag.\n4. Handle zero-volume days explicitly. Document the prevalence of zero-volume days in your sample. For monthly returns, report the average number of zero-volume days per firm-month. Consider excluding firms with zero-volume fractions exceeding 50% from the investable universe.\n5. Link exchange transfers. Use unified tickers that link pre-transfer and post-transfer series. Without this, exchange transfers appear as simultaneous delistings and new listings, inflating turnover and biasing survival calculations.\n6. Report sensitivity analysis. For any key finding, report results under at least three data treatment assumptions: survivors-only (upper bound), full sample with moderate imputation (baseline), and full sample with aggressive imputation plus liquidity filter (lower bound). If the finding survives all three, it is robust.\n7. Be especially cautious with pre-2007 data. The Vietnamese market had fewer than 100 listings before 2006, and the equitization wave of 2006-2009 produced a cohort of firms with systematically different characteristics than earlier listings. Cross-sectional tests with pre-2007 data have minimal power and should be interpreted with extreme caution.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "15_missing_data_and_survivorship.html#sec-missing-summary",
    "href": "15_missing_data_and_survivorship.html#sec-missing-summary",
    "title": "13  Missing Data and Survivorship Bias",
    "section": "13.11 Summary",
    "text": "13.11 Summary\n\n\n\nTable 13.3: Summary of data problems, their effects, and recommended corrections.\n\n\n\n\n\n\n\n\n\n\n\nData Problem\nBias Direction\nMagnitude (Vietnam)\nRecommended Fix\n\n\n\n\nSurvivorship bias (EW)\nUpward on returns\n~1-3% per year\nInclude all delisted firms\n\n\nSurvivorship bias (VW)\nUpward (smaller)\n~0.2-0.5% per year\nInclude all delisted firms\n\n\nDelisting return bias\nUpward on returns\n~0.5–2% per year (EW)\nImpute terminal returns\n\n\nLook-ahead bias\nInflates predictability\nVaries by strategy\nPoint-in-time data alignment\n\n\nZero-trading days\nUnderstates volatility\nSevere for small caps\nCompound or drop; document\n\n\nExchange transfers\nCreates false delistings\n~50-100 firms\nLink unified tickers\n\n\nNew-listing bias\nEarly sample unrepresentative\nExtreme pre-2007\nStart sample after 2007\n\n\n\n\n\n\nThe central message is that data problems in Vietnamese equity research are not merely a nuisance–they can create economically significant biases that alter the conclusions of empirical studies. The survivorship bias alone exceeds 100 basis points per year for equal-weighted portfolios, comparable to many documented anomaly premia. Researchers who ignore these issues risk reporting results that reflect data artifacts rather than genuine economic phenomena.\n\n\n\n\n\n\n\nLesmond, David A. 2005. “Liquidity of Emerging Markets.” Journal of Financial Economics 77 (2): 411–52.\n\n\nLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis with Missing Data. John Wiley & Sons.\n\n\nRubin, Donald B. 1976. “Inference and Missing Data.” Biometrika 63 (3): 581–92.\n\n\nShumway, Tyler. 1997. “The Delisting Bias in CRSP Data.” The Journal of Finance 52 (1): 327–40.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Missing Data and Survivorship Bias</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html",
    "href": "16_liquidity_and_turnover_measures.html",
    "title": "14  Liquidity and Turnover Measures",
    "section": "",
    "text": "14.1 Theoretical Foundations\nLiquidity (i.e., the ability to trade quickly at low cost without moving the price) is arguably the single most important practical consideration for anyone working with Vietnamese equity data. A factor premium that looks attractive in a frictionless backtest may be completely unimplementable if the long and short legs load on illiquid stocks whose prices move against you when you trade. Conversely, a genuine liquidity premium (i.e., compensation for bearing the risk that a stock will be hard to sell when you need to) is one of the most robust and theoretically grounded anomalies in asset pricing.\nThe challenge is that liquidity is inherently multidimensional and difficult to measure. In developed markets with continuous limit order books and sub-second trade reporting, researchers can observe bid-ask spreads, market depth, and price impact directly. In Vietnam, microstructure data at this granularity are limited: HOSE operates a periodic call auction at open and close with continuous matching in between, the tick size is coarse relative to price levels, and many stocks trade so infrequently that the concept of a “quoted spread” is meaningful only on days when the stock actually trades. This forces researchers to rely on low-frequency proxies computed from daily price and volume data.\nThis chapter constructs the major liquidity proxies used in the academic literature, validates them in the Vietnamese context, and demonstrates their use in asset pricing and portfolio construction.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-theory",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-theory",
    "title": "14  Liquidity and Turnover Measures",
    "section": "",
    "text": "14.1.1 Why Liquidity Matters\nLiquidity affects asset prices through at least three channels:\n\nLevel effect. Investors demand compensation for the expected cost of trading. Amihud and Mendelson (1986) show that stocks with higher bid-ask spreads earn higher expected returns, with the premium being an increasing function of the investor’s holding period. In equilibrium, illiquid stocks must offer higher expected returns to compensate for higher round-trip trading costs.\nRisk effect. Liquidity is time-varying and co-moves across stocks. Pástor and Stambaugh (2003) show that stocks whose returns are more sensitive to aggregate liquidity shocks earn higher expected returns. Acharya and Pedersen (2005) formalize this in a liquidity-adjusted CAPM where the required return includes a premium for bearing liquidity risk (i.e., the risk that the stock becomes illiquid precisely when the investor needs to sell).\nCommonality effect. Chordia, Roll, and Subrahmanyam (2000) document that individual stock liquidity co-moves strongly with market-wide liquidity. Brunnermeier and Pedersen (2009) explain this through a “liquidity spiral”: when asset values fall, margin constraints tighten, forcing leveraged investors to sell, which reduces market liquidity, which depresses prices further. This mechanism is particularly relevant in Vietnam, where retail investors with margin accounts are the dominant trading population.\n\n\n\n14.1.2 Liquidity Dimensions\nKyle (1985) identifies three dimensions of liquidity:\n\nTightness. The cost of turning around a position quickly, which is measured by the bid-ask spread.\nDepth. The volume that can be traded without moving the price, which is related to price impact.\nResiliency. The speed at which prices recover from uninformative order flow shocks.\n\nNo single measure captures all three dimensions. Goyenko, Holden, and Trzcinka (2009) and Fong, Holden, and Trzcinka (2017) systematically evaluate which low-frequency proxies best capture each dimension by benchmarking against high-frequency measures. Their key finding: the Amihud (2002) measure best captures the price impact dimension, the Roll (1984) estimator and Corwin and Schultz (2012) spread best capture tightness, and the Lesmond, Ogden, and Trzcinka (1999) zero-return measure captures a blend of transaction costs and information asymmetry. For emerging markets specifically, Fong, Holden, and Trzcinka (2017) recommend the Amihud measure and the Closing Percent Quoted Spread as the most reliable proxies.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-data",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-data",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.2 Data Construction",
    "text": "14.2 Data Construction\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom linearmodels.panel import PanelOLS\nfrom linearmodels.asset_pricing import LinearFactorModel\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Daily trading data\ndaily = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'date', 'open', 'high', 'low', 'close',\n        'adjusted_close', 'volume', 'turnover_value',\n        'market_cap', 'shares_outstanding', 'free_float_pct',\n        'bid', 'ask', 'foreign_buy_volume', 'foreign_sell_volume'\n    ]\n)\n\n# Monthly aggregates\nmonthly = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'volume_avg_20d', 'turnover_value_avg_20d',\n        'n_trading_days', 'n_zero_volume_days'\n    ]\n)\n\n# Firm characteristics for cross-sectional tests\nfundamentals = client.get_fundamentals(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    frequency='annual',\n    fields=[\n        'ticker', 'fiscal_year', 'total_assets', 'total_equity',\n        'net_income', 'revenue', 'book_equity'\n    ]\n)\n\n# Factor returns for asset pricing tests\nfactors = client.get_factor_returns(\n    market='vietnam',\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    factors=['mkt_excess', 'smb', 'hml', 'wml']\n)\n\ndaily['date'] = pd.to_datetime(daily['date'])\ndaily = daily.sort_values(['ticker', 'date'])\n\nprint(f\"Daily observations: {daily.shape[0]:,}\")\nprint(f\"Monthly observations: {monthly.shape[0]:,}\")\nprint(f\"Unique tickers: {daily['ticker'].nunique()}\")\n\n\n# Daily returns\ndaily['daily_return'] = (\n    daily.groupby('ticker')['adjusted_close'].pct_change()\n)\ndaily['abs_return'] = daily['daily_return'].abs()\ndaily['log_return'] = np.log(\n    daily['adjusted_close'] / daily.groupby('ticker')['adjusted_close'].shift(1)\n)\n\n# Turnover ratio (shares traded / shares outstanding)\ndaily['turnover_ratio'] = daily['volume'] / daily['shares_outstanding']\n\n# Zero indicators\ndaily['zero_return'] = (daily['daily_return'] == 0).astype(int)\ndaily['zero_volume'] = (daily['volume'] == 0).astype(int)\n\n# VND turnover (in billions)\ndaily['turnover_vnd_bn'] = daily['turnover_value'] / 1e9\n\nprint(\"Daily Return Summary:\")\nprint(daily['daily_return'].describe().round(6))\nprint(f\"\\nZero-return days: {daily['zero_return'].mean():.1%}\")\nprint(f\"Zero-volume days: {daily['zero_volume'].mean():.1%}\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-measures",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-measures",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.3 Constructing Liquidity Measures",
    "text": "14.3 Constructing Liquidity Measures\nWe construct seven liquidity proxies that span the dimensions of tightness, depth, and resiliency. Each is computed at the firm-month level, producing a panel that can be merged with monthly return data for cross-sectional tests.\n\n14.3.1 Amihud Illiquidity Ratio\nThe Amihud (2002) illiquidity measure is the ratio of absolute daily return to daily volume (in VND):\n\\[\n\\text{ILLIQ}_{i,m} = \\frac{1}{D_{i,m}} \\sum_{d=1}^{D_{i,m}} \\frac{|R_{i,d}|}{\\text{DVOL}_{i,d}}\n\\tag{14.1}\\]\nwhere \\(|R_{i,d}|\\) is the absolute daily return, \\(\\text{DVOL}_{i,d}\\) is VND trading volume on day \\(d\\), and \\(D_{i,m}\\) is the number of trading days with positive volume in month \\(m\\). Higher values indicate greater illiquidity.\nThe Amihud measure captures the price impact dimension of liquidity. It is grounded in the Kyle (1985) model where the parameter \\(\\lambda\\) (Kyle’s lambda) measures the price impact of order flow: \\(\\Delta p = \\lambda \\cdot Q\\). The Amihud ratio is a daily-frequency analog of \\(\\lambda\\).\n\ndef compute_amihud(daily_df, min_days=10):\n    \"\"\"\n    Compute the Amihud (2002) illiquidity ratio at the firm-month level.\n    \n    Excludes zero-volume days. Requires at least min_days observations\n    with positive volume per firm-month.\n    \"\"\"\n    df = daily_df[daily_df['volume'] &gt; 0].copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    # |Return| / VND Volume\n    df['illiq_daily'] = df['abs_return'] / df['turnover_value']\n    \n    # Remove extreme outliers (top 0.1% within each month)\n    df['illiq_daily'] = df.groupby('month')['illiq_daily'].transform(\n        lambda x: x.clip(upper=x.quantile(0.999))\n    )\n    \n    # Aggregate to firm-month\n    amihud = (\n        df.groupby(['ticker', 'month'])\n        .agg(\n            amihud_raw=('illiq_daily', 'mean'),\n            n_positive_vol_days=('illiq_daily', 'count')\n        )\n        .reset_index()\n    )\n    \n    # Filter: require minimum trading days\n    amihud = amihud[amihud['n_positive_vol_days'] &gt;= min_days]\n    \n    # Log transform (raw Amihud is heavily right-skewed)\n    amihud['amihud'] = np.log(1 + amihud['amihud_raw'] * 1e6)\n    \n    # Convert period to timestamp for merging\n    amihud['month_end'] = amihud['month'].dt.to_timestamp('M')\n    \n    return amihud[['ticker', 'month_end', 'amihud', 'amihud_raw',\n                    'n_positive_vol_days']]\n\namihud_monthly = compute_amihud(daily)\nprint(f\"Amihud observations: {len(amihud_monthly):,}\")\nprint(f\"\\nLog Amihud distribution:\")\nprint(amihud_monthly['amihud'].describe().round(3))\n\n\n\n14.3.2 Zero-Return Days (Lesmond Measure)\nLesmond, Ogden, and Trzcinka (1999) propose using the proportion of zero-return days as a measure of transaction costs. The intuition is that if the true value change on a given day is smaller than the round-trip transaction cost, a rational marginal investor will not trade, and the observed return will be zero. Thus, the zero-return proportion is an increasing function of effective transaction costs.\nLesmond (2005) validates this measure for emerging markets and finds it strongly correlated with explicit cost measures. In Vietnam, where zero-return days are common (as documented in the previous chapter), this measure has particular relevance.\n\\[\n\\text{ZeroRet}_{i,m} = \\frac{\\text{Number of days with } R_{i,d} = 0}{D_{i,m}}\n\\tag{14.2}\\]\n\ndef compute_zero_return(daily_df):\n    \"\"\"\n    Compute the Lesmond et al. (1999) zero-return measure\n    at the firm-month level.\n    \"\"\"\n    df = daily_df.copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    zero_ret = (\n        df.groupby(['ticker', 'month'])\n        .agg(\n            n_days=('daily_return', 'count'),\n            n_zero_return=('zero_return', 'sum'),\n            n_zero_volume=('zero_volume', 'sum')\n        )\n        .reset_index()\n    )\n    \n    zero_ret['zero_return_pct'] = (\n        zero_ret['n_zero_return'] / zero_ret['n_days']\n    )\n    zero_ret['zero_volume_pct'] = (\n        zero_ret['n_zero_volume'] / zero_ret['n_days']\n    )\n    \n    zero_ret['month_end'] = zero_ret['month'].dt.to_timestamp('M')\n    \n    return zero_ret[['ticker', 'month_end', 'zero_return_pct',\n                      'zero_volume_pct', 'n_days']]\n\nzero_monthly = compute_zero_return(daily)\nprint(f\"Zero-return observations: {len(zero_monthly):,}\")\nprint(f\"\\nZero-return proportion distribution:\")\nprint(zero_monthly['zero_return_pct'].describe().round(3))\n\n\n\n14.3.3 Turnover Ratio\nShare turnover (i.e., daily volume divided by shares outstanding) measures trading activity rather than trading cost. Datar, Naik, and Radcliffe (1998) use turnover as a liquidity proxy and document a negative cross-sectional relationship between turnover and expected returns, consistent with the liquidity premium hypothesis.\n\\[\n\\text{Turn}_{i,m} = \\frac{1}{D_{i,m}} \\sum_{d=1}^{D_{i,m}} \\frac{\\text{Volume}_{i,d}}{\\text{SharesOut}_{i,d}}\n\\tag{14.3}\\]\n\ndef compute_turnover(daily_df):\n    \"\"\"Compute average daily turnover ratio at the firm-month level.\"\"\"\n    df = daily_df.copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    turnover = (\n        df.groupby(['ticker', 'month'])\n        .agg(\n            turnover_mean=('turnover_ratio', 'mean'),\n            turnover_sum=('turnover_ratio', 'sum'),\n            volume_mean=('volume', 'mean'),\n            dvol_mean=('turnover_value', 'mean')\n        )\n        .reset_index()\n    )\n    \n    # Log transform for cross-sectional normality\n    turnover['log_turnover'] = np.log(\n        turnover['turnover_mean'].clip(lower=1e-8)\n    )\n    turnover['log_dvol'] = np.log(\n        turnover['dvol_mean'].clip(lower=1)\n    )\n    \n    turnover['month_end'] = turnover['month'].dt.to_timestamp('M')\n    \n    return turnover[['ticker', 'month_end', 'turnover_mean',\n                      'log_turnover', 'log_dvol']]\n\nturnover_monthly = compute_turnover(daily)\nprint(f\"Turnover observations: {len(turnover_monthly):,}\")\nprint(f\"\\nLog turnover distribution:\")\nprint(turnover_monthly['log_turnover'].describe().round(3))\n\n\n\n14.3.4 Roll Spread Estimator\nRoll (1984) derives an implicit bid-ask spread from the serial covariance of price changes. Under the assumptions that the true value follows a random walk and that observed prices bounce between the bid and ask:\n\\[\n\\text{Roll}_{i,m} = \\begin{cases}\n2\\sqrt{-\\text{Cov}(\\Delta P_{i,d}, \\Delta P_{i,d-1})} & \\text{if } \\text{Cov} &lt; 0 \\\\\n0 & \\text{if } \\text{Cov} \\geq 0\n\\end{cases}\n\\tag{14.4}\\]\nwhere \\(\\Delta P_{i,d} = P_{i,d} - P_{i,d-1}\\). The measure is intuitive: the bid-ask bounce creates negative serial correlation in transaction prices, and the magnitude of this negative correlation reflects the spread.\n\ndef compute_roll_spread(daily_df, min_days=15):\n    \"\"\"\n    Compute the Roll (1984) effective spread from serial\n    covariance of daily price changes.\n    \"\"\"\n    df = daily_df.copy()\n    df['month'] = df['date'].dt.to_period('M')\n    df['price_change'] = df.groupby('ticker')['adjusted_close'].diff()\n    df['price_change_lag'] = df.groupby('ticker')['price_change'].shift(1)\n    \n    def roll_estimate(group):\n        if len(group) &lt; min_days:\n            return np.nan\n        cov = group['price_change'].cov(group['price_change_lag'])\n        if cov &lt; 0:\n            spread = 2 * np.sqrt(-cov)\n            # Normalize by average price\n            avg_price = group['adjusted_close'].mean()\n            return spread / avg_price if avg_price &gt; 0 else np.nan\n        else:\n            return 0.0\n    \n    roll = (\n        df.dropna(subset=['price_change', 'price_change_lag'])\n        .groupby(['ticker', 'month'])\n        .apply(roll_estimate)\n        .reset_index(name='roll_spread')\n    )\n    \n    roll['month_end'] = roll['month'].dt.to_timestamp('M')\n    \n    return roll[['ticker', 'month_end', 'roll_spread']]\n\nroll_monthly = compute_roll_spread(daily)\nprint(f\"Roll spread observations: {len(roll_monthly):,}\")\nprint(f\"\\nRoll spread distribution:\")\nprint(roll_monthly['roll_spread'].describe().round(4))\n\n\n\n14.3.5 Corwin-Schultz High-Low Spread\nCorwin and Schultz (2012) estimate the effective spread from daily high and low prices. The key insight is that daily high and low prices contain information about both volatility and the spread—the high is typically a buy and the low a sell, so the high-low range reflects both true volatility and the bid-ask spread. By comparing one-day and two-day high-low ranges, the method separates the two components:\n\\[\n\\hat{S}_{i,m} = \\frac{2(e^{\\hat{\\alpha}} - 1)}{1 + e^{\\hat{\\alpha}}}\n\\tag{14.5}\\]\nwhere:\n\\[\n\\hat{\\alpha} = \\frac{\\sqrt{2\\hat{\\beta}} - \\sqrt{\\hat{\\beta}}}{3 - 2\\sqrt{2}} - \\sqrt{\\frac{\\hat{\\gamma}}{3 - 2\\sqrt{2}}}\n\\tag{14.6}\\]\nwith \\(\\hat{\\beta}\\) and \\(\\hat{\\gamma}\\) computed from one-day and two-day log high-low ratios.\n\ndef compute_corwin_schultz(daily_df, min_days=15):\n    \"\"\"\n    Compute the Corwin and Schultz (2012) bid-ask spread\n    estimator from daily high and low prices.\n    \"\"\"\n    df = daily_df.copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    # Log high-low ratio\n    df['log_hl'] = np.log(df['high'] / df['low'])\n    df['log_hl_sq'] = df['log_hl'] ** 2\n    \n    # Two-day high and low\n    df['high_2d'] = df.groupby('ticker')['high'].transform(\n        lambda x: x.rolling(2).max()\n    )\n    df['low_2d'] = df.groupby('ticker')['low'].transform(\n        lambda x: x.rolling(2).min()\n    )\n    df['log_hl_2d'] = np.log(df['high_2d'] / df['low_2d'])\n    df['log_hl_2d_sq'] = df['log_hl_2d'] ** 2\n    \n    def cs_estimate(group):\n        if len(group) &lt; min_days:\n            return np.nan\n        \n        beta = group['log_hl_sq'].mean() + group['log_hl_sq'].shift(1).mean()\n        beta = group[['log_hl_sq']].rolling(2).sum().mean().values[0]\n        gamma = group['log_hl_2d_sq'].mean()\n        \n        k = np.sqrt(2) - 1\n        denom = 3 - 2 * np.sqrt(2)\n        \n        term1 = np.sqrt(max(beta, 0)) / denom\n        if beta &gt; 0:\n            alpha_est = (np.sqrt(2 * beta) - np.sqrt(beta)) / denom\n            alpha_est -= np.sqrt(max(gamma / denom, 0))\n        else:\n            alpha_est = 0\n        \n        # Spread estimate\n        if alpha_est &gt; 0:\n            spread = 2 * (np.exp(alpha_est) - 1) / (1 + np.exp(alpha_est))\n        else:\n            spread = 0\n        \n        return min(spread, 0.20)  # Cap at 20% (sanity check)\n    \n    cs = (\n        df.dropna(subset=['log_hl', 'log_hl_2d'])\n        .groupby(['ticker', 'month'])\n        .apply(cs_estimate)\n        .reset_index(name='cs_spread')\n    )\n    \n    cs['month_end'] = cs['month'].dt.to_timestamp('M')\n    \n    return cs[['ticker', 'month_end', 'cs_spread']]\n\ncs_monthly = compute_corwin_schultz(daily)\nprint(f\"Corwin-Schultz observations: {len(cs_monthly):,}\")\nprint(f\"\\nCS spread distribution:\")\nprint(cs_monthly['cs_spread'].describe().round(4))\n\n\n\n14.3.6 Quoted Bid-Ask Spread\nWhen bid and ask quotes are available, the quoted percentage spread provides a direct measure of tightness:\n\\[\n\\text{PQSPR}_{i,m} = \\frac{1}{D_{i,m}} \\sum_{d=1}^{D_{i,m}} \\frac{\\text{Ask}_{i,d} - \\text{Bid}_{i,d}}{(\\text{Ask}_{i,d} + \\text{Bid}_{i,d})/2}\n\\tag{14.7}\\]\n\ndef compute_quoted_spread(daily_df):\n    \"\"\"Compute average quoted percentage spread at the firm-month level.\"\"\"\n    df = daily_df[\n        (daily_df['bid'] &gt; 0) & (daily_df['ask'] &gt; 0) &\n        (daily_df['ask'] &gt;= daily_df['bid'])\n    ].copy()\n    \n    df['month'] = df['date'].dt.to_period('M')\n    df['pqspr'] = (\n        (df['ask'] - df['bid']) / ((df['ask'] + df['bid']) / 2)\n    )\n    \n    # Winsorize extreme values\n    df['pqspr'] = df['pqspr'].clip(upper=df['pqspr'].quantile(0.999))\n    \n    spread = (\n        df.groupby(['ticker', 'month'])\n        .agg(\n            quoted_spread=('pqspr', 'mean'),\n            n_quotes=('pqspr', 'count')\n        )\n        .reset_index()\n    )\n    \n    spread['month_end'] = spread['month'].dt.to_timestamp('M')\n    \n    return spread[['ticker', 'month_end', 'quoted_spread', 'n_quotes']]\n\nquoted_monthly = compute_quoted_spread(daily)\nprint(f\"Quoted spread observations: {len(quoted_monthly):,}\")\nprint(f\"\\nQuoted spread distribution:\")\nprint(quoted_monthly['quoted_spread'].describe().round(4))\n\n\n\n14.3.7 Kyle’s Lambda (Price Impact Regression)\nWe estimate Kyle’s lambda (i.e., the price impact per unit of signed order flow) using a daily regression:\n\\[\nR_{i,d} = \\alpha_i + \\lambda_i \\cdot \\text{Sign}(R_{i,d}) \\cdot \\sqrt{\\text{Volume}_{i,d}} + \\varepsilon_{i,d}\n\\tag{14.8}\\]\nThis is an adaptation of the Hasbrouck (2009) effective cost measure. The coefficient \\(\\lambda_i\\) measures how much prices move per unit of (unsigned, square-rooted) volume.\n\ndef compute_kyle_lambda(daily_df, min_days=15):\n    \"\"\"\n    Estimate Kyle's lambda (price impact per unit order flow)\n    from daily return-on-signed-volume regressions.\n    \"\"\"\n    df = daily_df[daily_df['volume'] &gt; 0].copy()\n    df['month'] = df['date'].dt.to_period('M')\n    \n    # Signed square-root volume (sign inferred from return)\n    df['signed_sqrt_vol'] = (\n        np.sign(df['daily_return']) * np.sqrt(df['volume'])\n    )\n    \n    def estimate_lambda(group):\n        if len(group) &lt; min_days:\n            return np.nan\n        y = group['daily_return'].values\n        x = group['signed_sqrt_vol'].values\n        x = sm.add_constant(x)\n        try:\n            model = sm.OLS(y, x).fit()\n            lam = model.params[1]\n            return max(lam, 0)  # Lambda should be non-negative\n        except Exception:\n            return np.nan\n    \n    kyle = (\n        df.groupby(['ticker', 'month'])\n        .apply(estimate_lambda)\n        .reset_index(name='kyle_lambda')\n    )\n    \n    kyle['log_kyle'] = np.log(kyle['kyle_lambda'].clip(lower=1e-10))\n    kyle['month_end'] = kyle['month'].dt.to_timestamp('M')\n    \n    return kyle[['ticker', 'month_end', 'kyle_lambda', 'log_kyle']]\n\nkyle_monthly = compute_kyle_lambda(daily)\nprint(f\"Kyle lambda observations: {len(kyle_monthly):,}\")\nprint(f\"\\nLog Kyle lambda distribution:\")\nprint(kyle_monthly['log_kyle'].describe().round(3))",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-panel",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-panel",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.4 Assembling the Liquidity Panel",
    "text": "14.4 Assembling the Liquidity Panel\nWe merge all seven measures into a single firm-month panel for comparative analysis.\n\n# Start with monthly returns as the base\npanel = monthly[['ticker', 'month_end', 'monthly_return',\n                  'market_cap']].copy()\n\n# Merge each liquidity measure\nfor name, df, key_col in [\n    ('Amihud', amihud_monthly, 'amihud'),\n    ('Zero Return', zero_monthly, 'zero_return_pct'),\n    ('Turnover', turnover_monthly, 'log_turnover'),\n    ('Roll', roll_monthly, 'roll_spread'),\n    ('Corwin-Schultz', cs_monthly, 'cs_spread'),\n    ('Quoted Spread', quoted_monthly, 'quoted_spread'),\n    ('Kyle Lambda', kyle_monthly, 'log_kyle'),\n]:\n    panel = panel.merge(\n        df[['ticker', 'month_end', key_col]],\n        on=['ticker', 'month_end'],\n        how='left'\n    )\n\n# Add log market cap\npanel['log_mcap'] = np.log(panel['market_cap'].clip(lower=1))\n\n# Add fundamentals (lagged)\nfund_lagged = fundamentals.copy()\nfund_lagged['merge_year'] = fund_lagged['fiscal_year'] + 1\npanel = panel.merge(\n    fund_lagged[['ticker', 'merge_year', 'book_equity']].rename(\n        columns={'merge_year': 'year'}),\n    left_on=['ticker', panel['month_end'].dt.year],\n    right_on=['ticker', 'year'],\n    how='left'\n)\npanel['bm'] = panel['book_equity'] / panel['market_cap']\n\nprint(f\"Unified panel: {len(panel):,} firm-months\")\nprint(f\"\\nCoverage by measure:\")\nliquidity_cols = ['amihud', 'zero_return_pct', 'log_turnover',\n                   'roll_spread', 'cs_spread', 'quoted_spread', 'log_kyle']\nfor col in liquidity_cols:\n    pct = panel[col].notna().mean()\n    print(f\"  {col:&lt;20}: {pct:.1%}\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-cross-section",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-cross-section",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.5 Cross-Sectional Properties of Liquidity",
    "text": "14.5 Cross-Sectional Properties of Liquidity\n\n14.5.1 Summary Statistics by Size Quintile\nLiquidity varies enormously across the size distribution. Small-cap Vietnamese stocks can be orders of magnitude less liquid than large-caps.\n\n# Assign size quintiles within each month\npanel['size_quintile'] = (\n    panel.groupby('month_end')['market_cap']\n    .transform(lambda x: pd.qcut(x, 5, labels=['Q1 (Small)', 'Q2',\n                                                  'Q3', 'Q4',\n                                                  'Q5 (Large)'],\n                                   duplicates='drop'))\n)\n\n# Average liquidity by quintile\nliq_by_size = (\n    panel.groupby('size_quintile')[liquidity_cols]\n    .mean()\n    .round(4)\n)\n\nprint(\"Average Liquidity by Market Cap Quintile:\")\nprint(liq_by_size.to_string())\n\n\n\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\naxes = axes.flatten()\n\nmeasures_to_plot = [\n    ('amihud', 'Amihud (log)', '#2C5F8A'),\n    ('zero_return_pct', 'Zero-Return %', '#C0392B'),\n    ('log_turnover', 'Log Turnover', '#27AE60'),\n    ('roll_spread', 'Roll Spread', '#E67E22'),\n    ('cs_spread', 'Corwin-Schultz Spread', '#8E44AD'),\n    ('quoted_spread', 'Quoted Spread', '#1ABC9C')\n]\n\nfor i, (col, label, color) in enumerate(measures_to_plot):\n    data = panel.groupby('size_quintile')[col].mean()\n    axes[i].bar(range(len(data)), data.values,\n                color=color, alpha=0.85, edgecolor='white')\n    axes[i].set_xticks(range(len(data)))\n    axes[i].set_xticklabels(data.index, fontsize=8)\n    axes[i].set_ylabel(label)\n    axes[i].set_title(label)\n\nplt.suptitle('Liquidity Measures by Market Cap Quintile', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\nFigure 14.1\n\n\n\n\n\n14.5.2 Correlation Structure\nHow strongly do the different liquidity measures correlate? If they capture the same underlying dimension, we expect high correlations. If they capture different dimensions (tightness vs. depth vs. activity), correlations will be moderate.\n\n\n\n# Rank correlations (Spearman) among liquidity measures\n# Reverse turnover sign so higher = less liquid (consistent direction)\npanel_corr = panel[liquidity_cols].copy()\npanel_corr['neg_log_turnover'] = -panel_corr['log_turnover']\ncorr_cols = ['amihud', 'zero_return_pct', 'neg_log_turnover',\n              'roll_spread', 'cs_spread', 'quoted_spread', 'log_kyle']\ncorr_labels = ['Amihud', 'Zero-Return', 'Neg. Turnover', 'Roll',\n                'Corwin-Schultz', 'Quoted Spread', 'Kyle λ']\n\nrank_corr = panel_corr[corr_cols].corr(method='spearman')\nrank_corr.index = corr_labels\nrank_corr.columns = corr_labels\n\nfig, ax = plt.subplots(figsize=(9, 8))\nmask = np.triu(np.ones_like(rank_corr, dtype=bool), k=1)\nsns.heatmap(\n    rank_corr, mask=mask, annot=True, fmt='.2f',\n    cmap='YlOrRd', vmin=0, vmax=1, square=True,\n    linewidths=0.5, ax=ax,\n    cbar_kws={'label': 'Spearman Rank Correlation'}\n)\nax.set_title('Cross-Sectional Rank Correlations Among Liquidity Measures')\nplt.tight_layout()\nplt.show()\n\n\nFigure 14.2\n\n\n\n\n\n14.5.3 Principal Component Analysis of Liquidity\nGiven the multidimensionality of liquidity, we extract a composite liquidity factor using PCA:\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Standardize each measure within each month (cross-sectional)\nliq_data = panel[liquidity_cols].copy()\nliq_data['neg_log_turnover'] = -liq_data['log_turnover']\n\npca_cols = ['amihud', 'zero_return_pct', 'neg_log_turnover',\n             'roll_spread', 'cs_spread', 'log_kyle']\n\n# Drop rows with any missing liquidity measure\nliq_complete = panel.dropna(subset=pca_cols).copy()\n\n# Cross-sectional standardization by month\ndef standardize_within_month(df, cols):\n    for col in cols:\n        df[col + '_z'] = (\n            df.groupby('month_end')[col]\n            .transform(lambda x: (x - x.mean()) / x.std())\n        )\n    return df\n\nliq_complete = standardize_within_month(liq_complete, pca_cols)\nz_cols = [c + '_z' for c in pca_cols]\n\n# Pool all months for PCA\npca_input = liq_complete[z_cols].dropna()\npca = PCA(n_components=3)\npca.fit(pca_input)\n\nprint(\"PCA Explained Variance Ratios:\")\nfor i, (var, cumvar) in enumerate(zip(\n    pca.explained_variance_ratio_,\n    np.cumsum(pca.explained_variance_ratio_)\n)):\n    print(f\"  PC{i+1}: {var:.3f} (cumulative: {cumvar:.3f})\")\n\nprint(\"\\nPC1 Loadings:\")\nfor col, loading in zip(pca_cols, pca.components_[0]):\n    print(f\"  {col:&lt;20}: {loading:.3f}\")\n\n# Assign PC1 as composite illiquidity\nliq_complete['illiq_pc1'] = pca.transform(\n    liq_complete[z_cols].values\n)[:, 0]",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-aggregate",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-aggregate",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.6 Aggregate Liquidity and Market Conditions",
    "text": "14.6 Aggregate Liquidity and Market Conditions\n\n14.6.1 Time Series of Market Liquidity\nAggregate liquidity (i.e., the average illiquidity across all stocks) varies substantially over time. Chordia, Roll, and Subrahmanyam (2001) document that market-wide liquidity declines during periods of high volatility and negative market returns.\n\n\n\n# Compute monthly cross-sectional aggregates\nagg_liquidity = (\n    panel.groupby('month_end')\n    .agg(\n        amihud_median=('amihud', 'median'),\n        zero_ret_median=('zero_return_pct', 'median'),\n        turnover_median=('log_turnover', 'median'),\n        roll_median=('roll_spread', 'median'),\n        cs_median=('cs_spread', 'median'),\n        n_stocks=('ticker', 'nunique')\n    )\n    .reset_index()\n)\n\n# Standardize for plotting\nfor col in ['amihud_median', 'zero_ret_median', 'roll_median']:\n    agg_liquidity[col + '_z'] = (\n        (agg_liquidity[col] - agg_liquidity[col].mean())\n        / agg_liquidity[col].std()\n    )\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 9), height_ratios=[2, 1])\n\n# Panel A: Aggregate illiquidity\naxes[0].plot(agg_liquidity['month_end'], agg_liquidity['amihud_median_z'],\n             color='#2C5F8A', linewidth=1.5, label='Amihud')\naxes[0].plot(agg_liquidity['month_end'], agg_liquidity['zero_ret_median_z'],\n             color='#C0392B', linewidth=1.5, label='Zero-Return')\naxes[0].plot(agg_liquidity['month_end'], agg_liquidity['roll_median_z'],\n             color='#27AE60', linewidth=1.5, label='Roll Spread')\naxes[0].axhline(y=0, color='gray', linewidth=0.5)\naxes[0].set_ylabel('Standardized Illiquidity')\naxes[0].set_title('Panel A: Aggregate Illiquidity Over Time')\naxes[0].legend(fontsize=9)\n\n# Shade crisis periods\ncrisis_periods = [\n    ('2008-06-01', '2009-03-31', 'GFC'),\n    ('2011-01-01', '2011-12-31', 'Tightening'),\n    ('2020-02-01', '2020-05-31', 'COVID')\n]\nfor start, end, label in crisis_periods:\n    axes[0].axvspan(pd.Timestamp(start), pd.Timestamp(end),\n                     alpha=0.15, color='gray')\n    mid = pd.Timestamp(start) + (pd.Timestamp(end) - pd.Timestamp(start)) / 2\n    axes[0].text(mid, axes[0].get_ylim()[1] * 0.9, label,\n                 ha='center', fontsize=8, color='gray')\n\n# Panel B: Market return\nmarket_monthly = factors[['month_end', 'mkt_excess']].copy()\nmarket_monthly['month_end'] = pd.to_datetime(market_monthly['month_end'])\naxes[1].bar(market_monthly['month_end'],\n            market_monthly['mkt_excess'] * 100,\n            width=25,\n            color=['#27AE60' if r &gt; 0 else '#C0392B'\n                   for r in market_monthly['mkt_excess']],\n            alpha=0.6)\naxes[1].set_ylabel('Market Excess Return (%)')\naxes[1].set_xlabel('Date')\naxes[1].set_title('Panel B: VN-Index Monthly Excess Return')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 14.3\n\n\n\n\n\n14.6.2 Commonality in Liquidity\nChordia, Roll, and Subrahmanyam (2000) find that individual stock liquidity co-moves with market liquidity, even after controlling for firm-specific factors. We test for commonality in Vietnam by regressing changes in firm-level liquidity on changes in market-level liquidity:\n\\[\n\\Delta L_{i,m} = \\alpha_i + \\beta_i \\Delta L_{M,m} + \\gamma_i \\Delta L_{M,m-1} + \\delta_i \\Delta L_{M,m+1} + \\varepsilon_{i,m}\n\\tag{14.9}\\]\nwhere \\(\\Delta L_{i,m}\\) is the change in firm \\(i\\)’s illiquidity, \\(\\Delta L_{M,m}\\) is the change in market-average illiquidity (excluding firm \\(i\\)), and the lead/lag terms capture non-synchronous adjustment. The coefficient \\(\\beta_i\\) measures the sensitivity of firm \\(i\\)’s liquidity to market-wide liquidity shocks.\n\n# Compute monthly changes in Amihud for each firm and the market\npanel_common = panel[['ticker', 'month_end', 'amihud']].dropna().copy()\npanel_common = panel_common.sort_values(['ticker', 'month_end'])\npanel_common['d_amihud'] = (\n    panel_common.groupby('ticker')['amihud'].diff()\n)\n\n# Market-level illiquidity change (equal-weighted, excluding firm i)\nmkt_liq = (\n    panel_common.groupby('month_end')['amihud']\n    .mean()\n    .diff()\n    .to_frame('d_amihud_mkt')\n)\nmkt_liq['d_amihud_mkt_lag'] = mkt_liq['d_amihud_mkt'].shift(1)\nmkt_liq['d_amihud_mkt_lead'] = mkt_liq['d_amihud_mkt'].shift(-1)\n\npanel_common = panel_common.merge(mkt_liq, on='month_end', how='left')\n\n# Estimate commonality for each firm\ndef estimate_commonality(group, min_obs=24):\n    g = group.dropna(subset=['d_amihud', 'd_amihud_mkt'])\n    if len(g) &lt; min_obs:\n        return None\n    y = g['d_amihud']\n    X = sm.add_constant(g[['d_amihud_mkt', 'd_amihud_mkt_lag',\n                            'd_amihud_mkt_lead']])\n    try:\n        model = sm.OLS(y, X).fit()\n        return pd.Series({\n            'beta_mkt': model.params['d_amihud_mkt'],\n            'beta_t': model.tvalues['d_amihud_mkt'],\n            'r_squared': model.rsquared\n        })\n    except Exception:\n        return None\n\ncommonality = (\n    panel_common\n    .groupby('ticker')\n    .apply(estimate_commonality)\n    .dropna()\n)\n\nprint(\"Commonality in Liquidity (Amihud):\")\nprint(f\"  Mean beta_mkt: {commonality['beta_mkt'].mean():.3f}\")\nprint(f\"  Median beta_mkt: {commonality['beta_mkt'].median():.3f}\")\nprint(f\"  % significant at 5%: \"\n      f\"{(commonality['beta_t'].abs() &gt; 1.96).mean():.1%}\")\nprint(f\"  Mean R-squared: {commonality['r_squared'].mean():.3f}\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-pricing",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-pricing",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.7 Is Liquidity Priced?",
    "text": "14.7 Is Liquidity Priced?\n\n14.7.1 Portfolio Sorts\nWe test whether illiquidity predicts future returns by sorting stocks into quintile portfolios based on lagged liquidity measures and comparing average returns across quintiles.\n\ndef liquidity_portfolio_sort(panel_df, liq_col, n_groups=5):\n    \"\"\"\n    Compute quintile portfolio returns sorted on lagged liquidity.\n    Lag the sorting variable by one month to avoid look-ahead bias.\n    \"\"\"\n    df = panel_df[['ticker', 'month_end', 'monthly_return',\n                    'market_cap', liq_col]].dropna().copy()\n    df = df.sort_values(['ticker', 'month_end'])\n    \n    # Lag the sorting variable\n    df['liq_lag'] = df.groupby('ticker')[liq_col].shift(1)\n    df = df.dropna(subset=['liq_lag', 'monthly_return'])\n    \n    # Assign quintiles within each month\n    df['quintile'] = (\n        df.groupby('month_end')['liq_lag']\n        .transform(lambda x: pd.qcut(x, n_groups, labels=False,\n                                       duplicates='drop'))\n    )\n    \n    # EW portfolio returns by quintile-month\n    port_returns = (\n        df.groupby(['month_end', 'quintile'])['monthly_return']\n        .mean()\n        .unstack()\n    )\n    \n    # Long-short (Q5 - Q1)\n    if n_groups - 1 in port_returns.columns and 0 in port_returns.columns:\n        port_returns['long_short'] = (\n            port_returns[n_groups - 1] - port_returns[0]\n        )\n    \n    return port_returns\n\n# Run sorts for each illiquidity measure\nsort_measures = {\n    'Amihud': 'amihud',\n    'Zero-Return': 'zero_return_pct',\n    'Neg. Turnover': 'log_turnover',  # Will reverse below\n    'Roll Spread': 'roll_spread',\n    'Corwin-Schultz': 'cs_spread',\n}\n\n# For turnover, negate so higher = less liquid\npanel_sorts = panel.copy()\npanel_sorts['neg_turnover'] = -panel_sorts['log_turnover']\nsort_measures_actual = {\n    'Amihud': 'amihud',\n    'Zero-Return': 'zero_return_pct',\n    'Neg. Turnover': 'neg_turnover',\n    'Roll Spread': 'roll_spread',\n    'Corwin-Schultz': 'cs_spread',\n}\n\nprint(\"Liquidity Premium (EW, Quintile Sorts):\")\nprint(f\"{'Measure':&lt;18} {'Q1 (Liquid)':&gt;12} {'Q5 (Illiquid)':&gt;14} \"\n      f\"{'Q5-Q1':&gt;10} {'t-stat':&gt;8}\")\nprint(\"-\" * 62)\n\nsort_results = {}\nfor name, col in sort_measures_actual.items():\n    ports = liquidity_portfolio_sort(panel_sorts, col)\n    sort_results[name] = ports\n    \n    q1 = ports[0].mean() * 12\n    q5 = ports[4].mean() * 12 if 4 in ports.columns else np.nan\n    ls = ports['long_short'].mean() * 12 if 'long_short' in ports else np.nan\n    ls_se = ports['long_short'].std() / np.sqrt(len(ports)) * np.sqrt(12) if 'long_short' in ports else np.nan\n    t = ls / ls_se if ls_se and ls_se &gt; 0 else np.nan\n    \n    print(f\"{name:&lt;18} {q1:&gt;12.4f} {q5:&gt;14.4f} {ls:&gt;10.4f} {t:&gt;8.2f}\")\n\n\n\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\naxes = axes.flatten()\n\ncolors_quintile = ['#27AE60', '#2ECC71', '#F1C40F', '#E67E22', '#C0392B']\n\nfor i, (name, ports) in enumerate(sort_results.items()):\n    if i &gt;= 5:\n        break\n    quintile_means = [ports[q].mean() * 12 * 100 for q in range(5)\n                       if q in ports.columns]\n    axes[i].bar(range(len(quintile_means)), quintile_means,\n                color=colors_quintile[:len(quintile_means)],\n                alpha=0.85, edgecolor='white')\n    axes[i].set_xticks(range(len(quintile_means)))\n    axes[i].set_xticklabels([f'Q{q+1}' for q in range(len(quintile_means))])\n    axes[i].set_ylabel('Annualized Return (%)')\n    axes[i].set_title(name)\n    axes[i].axhline(y=0, color='gray', linewidth=0.5)\n\n# Hide unused subplot\nif len(sort_results) &lt; 6:\n    axes[5].set_visible(False)\n\nplt.suptitle('Average Returns by Illiquidity Quintile', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\nFigure 14.4\n\n\n\n\n\n14.7.2 Fama-MacBeth Cross-Sectional Regressions\nPortfolio sorts are informative but cannot control for multiple characteristics simultaneously. We use Fama and French (1993) -style cross-sectional regressions to test whether liquidity predicts returns after controlling for size, value, and momentum:\n\\[\nR_{i,m+1} = \\gamma_{0,m} + \\gamma_{1,m} \\text{ILLIQ}_{i,m} + \\gamma_{2,m} \\ln(\\text{MCap}_{i,m}) + \\gamma_{3,m} \\text{BM}_{i,m} + \\gamma_{4,m} R_{i,m-12:m-1} + \\varepsilon_{i,m+1}\n\\tag{14.10}\\]\nThe time-series average of the monthly coefficient \\(\\bar{\\gamma}_1\\) estimates the illiquidity premium, and its t-statistic uses the Fama and French (1993) standard error.\n\ndef fama_macbeth(panel_df, illiq_col, controls=['log_mcap', 'bm'],\n                  min_stocks=50):\n    \"\"\"\n    Run Fama-MacBeth cross-sectional regressions of\n    next-month returns on lagged illiquidity and controls.\n    \"\"\"\n    df = panel_df.copy()\n    df = df.sort_values(['ticker', 'month_end'])\n    \n    # Lag the illiquidity measure\n    df['illiq_lag'] = df.groupby('ticker')[illiq_col].shift(1)\n    \n    # Lag controls\n    for c in controls:\n        df[c + '_lag'] = df.groupby('ticker')[c].shift(1)\n    \n    regressors = ['illiq_lag'] + [c + '_lag' for c in controls]\n    df = df.dropna(subset=['monthly_return'] + regressors)\n    \n    # Month-by-month cross-sectional regressions\n    months = sorted(df['month_end'].unique())\n    gamma_list = []\n    \n    for month in months:\n        cross = df[df['month_end'] == month]\n        if len(cross) &lt; min_stocks:\n            continue\n        \n        y = cross['monthly_return'].values\n        X = sm.add_constant(cross[regressors].values)\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            gammas = {'month_end': month, 'intercept': model.params[0]}\n            for j, reg in enumerate(regressors):\n                gammas[reg] = model.params[j + 1]\n            gamma_list.append(gammas)\n        except Exception:\n            pass\n    \n    gamma_df = pd.DataFrame(gamma_list)\n    \n    # Time-series averages and t-statistics\n    results = {}\n    for col in ['intercept'] + regressors:\n        mean = gamma_df[col].mean()\n        se = gamma_df[col].std() / np.sqrt(len(gamma_df))\n        t = mean / se if se &gt; 0 else np.nan\n        results[col] = {'Coefficient': mean, 'SE': se, 't-stat': t}\n    \n    return pd.DataFrame(results).T, gamma_df\n\n# Run for each illiquidity measure\nprint(\"Fama-MacBeth Regressions: R_{i,m+1} on ILLIQ_{i,m} + controls\")\nprint(\"=\" * 70)\n\nfor name, col in [('Amihud', 'amihud'),\n                    ('Zero-Return', 'zero_return_pct'),\n                    ('Roll Spread', 'roll_spread'),\n                    ('Corwin-Schultz', 'cs_spread')]:\n    results, gammas = fama_macbeth(panel, col)\n    print(f\"\\n{name}:\")\n    print(results[['Coefficient', 't-stat']].round(4).to_string())\n\n\n\n14.7.3 Factor-Adjusted Liquidity Premium\nThe liquidity premium may be partially or fully explained by existing risk factors (size, value, momentum). We test this by regressing the long-short liquidity portfolio returns on the Fama-French-Carhart factors:\n\nprint(\"Factor-Adjusted Liquidity Premium:\")\nprint(f\"{'Measure':&lt;18} {'Alpha (ann.)':&gt;12} {'Alpha t':&gt;10} \"\n      f\"{'MKT':&gt;8} {'SMB':&gt;8} {'HML':&gt;8} {'R2':&gt;6}\")\nprint(\"-\" * 72)\n\nfor name, ports in sort_results.items():\n    if 'long_short' not in ports.columns:\n        continue\n    \n    ls_series = ports['long_short'].to_frame('ls')\n    ls_series.index = pd.to_datetime(ls_series.index)\n    \n    merged = ls_series.merge(factors, left_index=True,\n                              right_on='month_end', how='inner')\n    \n    if len(merged) &lt; 24:\n        continue\n    \n    y = merged['ls']\n    X = sm.add_constant(merged[['mkt_excess', 'smb', 'hml', 'wml']])\n    \n    model = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n    \n    alpha_ann = model.params['const'] * 12\n    alpha_t = model.tvalues['const']\n    mkt_b = model.params['mkt_excess']\n    smb_b = model.params['smb']\n    hml_b = model.params['hml']\n    r2 = model.rsquared\n    \n    print(f\"{name:&lt;18} {alpha_ann:&gt;12.4f} {alpha_t:&gt;10.2f} \"\n          f\"{mkt_b:&gt;8.3f} {smb_b:&gt;8.3f} {hml_b:&gt;8.3f} {r2:&gt;6.3f}\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-implementation",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-implementation",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.8 Liquidity and Transaction Cost Estimation",
    "text": "14.8 Liquidity and Transaction Cost Estimation\n\n14.8.1 Translating Measures to Trading Costs\nFor practitioners, the key question is: what does a given Amihud or spread value mean in terms of actual VND cost per trade? We calibrate the relationship between our low-frequency proxies and explicit trading costs.\n\ndef estimate_round_trip_cost(row):\n    \"\"\"\n    Estimate total round-trip trading cost (in %) from\n    multiple liquidity proxies.\n    \n    Components:\n    1. Explicit: commission + tax (~0.35% round-trip)\n    2. Spread cost: half-spread each way\n    3. Price impact: function of trade size\n    \"\"\"\n    explicit = 0.0035  # 35 bps round-trip\n    \n    # Use Corwin-Schultz or quoted spread as spread estimate\n    spread = row.get('cs_spread', row.get('quoted_spread', 0.005))\n    spread_cost = spread  # Full spread = round-trip cost\n    \n    # Price impact (approximate from Amihud)\n    # For a trade of 1% of daily volume\n    amihud_raw = row.get('amihud_raw', 0)\n    impact = amihud_raw * 0.01  # Rough approximation\n    \n    return explicit + spread_cost + impact\n\npanel['estimated_rtc'] = panel.apply(estimate_round_trip_cost, axis=1)\n\n# Distribution by size quintile\nrtc_by_size = (\n    panel.groupby('size_quintile')['estimated_rtc']\n    .agg(['mean', 'median', 'std'])\n    .round(4)\n)\nprint(\"Estimated Round-Trip Cost by Size Quintile (%):\")\nprint((rtc_by_size * 100).round(2).to_string())\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Distribution\nfor q, color in zip(['Q1 (Small)', 'Q3', 'Q5 (Large)'],\n                     ['#C0392B', '#F1C40F', '#27AE60']):\n    subset = panel[panel['size_quintile'] == q]['estimated_rtc'].dropna()\n    subset = subset[subset &lt; 0.15]  # Trim extreme\n    axes[0].hist(subset * 100, bins=50, density=True, alpha=0.5,\n                  color=color, label=q, edgecolor='white')\naxes[0].set_xlabel('Estimated Round-Trip Cost (%)')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Panel A: Cost Distribution by Size')\naxes[0].legend()\n\n# Panel B: Time series of median cost\ncost_ts = (\n    panel.groupby('month_end')['estimated_rtc']\n    .median()\n    .reset_index()\n)\naxes[1].plot(pd.to_datetime(cost_ts['month_end']),\n             cost_ts['estimated_rtc'] * 100,\n             color='#2C5F8A', linewidth=1.5)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Median Round-Trip Cost (%)')\naxes[1].set_title('Panel B: Aggregate Trading Costs Over Time')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 14.5\n\n\n\n\n\n14.8.2 Strategy Implementability\nA critical application of liquidity measurement is testing whether a given anomaly strategy remains profitable after accounting for realistic trading costs. We compute net-of-cost returns for the liquidity-sorted portfolios themselves. This is an inherently conservative test because the illiquid long leg carries the highest costs.\n\n# For each quintile, estimate average monthly turnover and cost\n# and subtract from gross returns\nfor name, ports in sort_results.items():\n    if 'long_short' not in ports.columns:\n        continue\n    \n    gross_ann = ports['long_short'].mean() * 12\n    \n    # Estimate costs: illiquid quintile has higher costs\n    # Assume monthly turnover of ~15% for long-short with monthly rebalancing\n    turnover = 0.15\n    cost_q1 = 0.003  # 30 bps per trade for liquid stocks\n    cost_q5 = 0.015  # 150 bps for illiquid stocks\n    avg_cost = (cost_q1 + cost_q5) / 2  # Average across long and short\n    monthly_tc = turnover * avg_cost\n    \n    net_ann = gross_ann - monthly_tc * 12\n    \n    print(f\"{name:&lt;18}: Gross = {gross_ann*100:&gt;6.2f}%, \"\n          f\"TC = {monthly_tc*1200:&gt;6.1f} bps/mo, \"\n          f\"Net = {net_ann*100:&gt;6.2f}%\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-stress",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-stress",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.9 Liquidity During Market Stress",
    "text": "14.9 Liquidity During Market Stress\n\n14.9.1 Flight to Liquidity\nDuring market stress, investors sell illiquid assets and buy liquid ones, which is a “flight to liquidity” that widens the return differential between liquid and illiquid stocks. Hameed, Kang, and Viswanathan (2010) show that this pattern is strongest when market returns are most negative.\n\n# Merge Amihud-sorted portfolio returns with market returns\namihud_ports = sort_results.get('Amihud')\nif amihud_ports is not None and 'long_short' in amihud_ports.columns:\n    ftl_data = pd.merge(\n        amihud_ports['long_short'].to_frame('illiq_premium'),\n        factors[['month_end', 'mkt_excess']].set_index('month_end'),\n        left_index=True, right_index=True, how='inner'\n    )\n    \n    # Classify market states\n    ftl_data['mkt_state'] = pd.cut(\n        ftl_data['mkt_excess'],\n        bins=[-np.inf,\n              ftl_data['mkt_excess'].quantile(0.20),\n              ftl_data['mkt_excess'].quantile(0.80),\n              np.inf],\n        labels=['Bear (bottom 20%)', 'Normal', 'Bull (top 20%)']\n    )\n    \n    # Illiquidity premium by market state\n    state_premium = (\n        ftl_data.groupby('mkt_state')['illiq_premium']\n        .agg(['mean', 'std', 'count'])\n    )\n    state_premium['ann_premium'] = state_premium['mean'] * 12\n    state_premium['t_stat'] = (\n        state_premium['mean']\n        / (state_premium['std'] / np.sqrt(state_premium['count']))\n    )\n    \n    print(\"Illiquidity Premium by Market State:\")\n    print(state_premium[['ann_premium', 't_stat', 'count']].round(3))\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nif 'state_premium' in dir():\n    colors_state = ['#C0392B', '#F1C40F', '#27AE60']\n    bars = ax.bar(range(len(state_premium)),\n                   state_premium['ann_premium'] * 100,\n                   color=colors_state, alpha=0.85, edgecolor='white')\n    ax.set_xticks(range(len(state_premium)))\n    ax.set_xticklabels(state_premium.index)\n    ax.set_ylabel('Annualized Q5-Q1 Return (%)')\n    ax.set_title('Illiquidity Premium by Market State')\n    ax.axhline(y=0, color='gray', linewidth=0.8)\n    \n    for i, (_, row) in enumerate(state_premium.iterrows()):\n        ax.text(i, row['ann_premium'] * 100 + 0.3,\n                f\"t={row['t_stat']:.1f}\",\n                ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 14.6\n\n\n\n\n\n14.9.2 Liquidity Co-Movement with Global Risk\nVietnamese market liquidity may be driven by global risk factors, particularly for stocks held by foreign investors. We test whether global risk measures (VIX, USD strength) predict Vietnamese aggregate liquidity:\n\n# Merge aggregate liquidity with global variables\nglobal_vars = client.get_macro_data(\n    variables=['vix_close', 'dxy_index'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    frequency='monthly'\n)\n\nagg_liq_global = agg_liquidity.merge(\n    global_vars, on='month_end', how='inner'\n)\n\n# Changes in all variables\nfor col in ['amihud_median', 'vix_close', 'dxy_index']:\n    agg_liq_global[f'd_{col}'] = agg_liq_global[col].diff()\n\n# Regression\ny = agg_liq_global['d_amihud_median'].dropna()\nX = sm.add_constant(\n    agg_liq_global.loc[y.index, ['d_vix_close', 'd_dxy_index']]\n)\n\nmodel = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 3})\nprint(\"Aggregate Illiquidity ~ Global Risk:\")\nprint(model.summary().tables[1])",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-factor",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-factor",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.10 Constructing a Tradeable Liquidity Factor",
    "text": "14.10 Constructing a Tradeable Liquidity Factor\nFollowing Pástor and Stambaugh (2003), we construct an aggregate liquidity factor that can be used in asset pricing tests. The factor captures innovations in aggregate liquidity (unexpected changes in market-wide trading conditions).\n\n# Step 1: Compute market-level Amihud as EW average\nmkt_amihud = (\n    panel.groupby('month_end')['amihud']\n    .mean()\n    .to_frame('mkt_amihud')\n)\n\n# Step 2: Estimate AR(2) model for aggregate illiquidity\nmkt_amihud['mkt_amihud_lag1'] = mkt_amihud['mkt_amihud'].shift(1)\nmkt_amihud['mkt_amihud_lag2'] = mkt_amihud['mkt_amihud'].shift(2)\nmkt_amihud = mkt_amihud.dropna()\n\nar_model = sm.OLS(\n    mkt_amihud['mkt_amihud'],\n    sm.add_constant(mkt_amihud[['mkt_amihud_lag1', 'mkt_amihud_lag2']])\n).fit()\n\n# Step 3: Residuals = liquidity innovations\n# Negative innovation = liquidity improved (good)\n# Positive innovation = liquidity deteriorated (bad)\nmkt_amihud['liq_innovation'] = ar_model.resid\n\n# Step 4: Test whether liquidity innovations predict returns\n# Higher sensitivity to negative innovations = higher expected return\nprint(\"AR(2) Model for Aggregate Amihud:\")\nprint(f\"  R-squared: {ar_model.rsquared:.3f}\")\nprint(f\"  AR(1) coef: {ar_model.params['mkt_amihud_lag1']:.3f} \"\n      f\"(t={ar_model.tvalues['mkt_amihud_lag1']:.2f})\")\n\n# Step 5: Estimate liquidity betas for each firm\npanel_liq_beta = panel.merge(\n    mkt_amihud[['liq_innovation']],\n    left_on='month_end', right_index=True, how='inner'\n)\n\ndef estimate_liq_beta(group, min_obs=36):\n    g = group.dropna(subset=['monthly_return', 'liq_innovation'])\n    if len(g) &lt; min_obs:\n        return None\n    y = g['monthly_return']\n    X = sm.add_constant(g['liq_innovation'])\n    try:\n        model = sm.OLS(y, X).fit()\n        return model.params['liq_innovation']\n    except Exception:\n        return None\n\nliq_betas = (\n    panel_liq_beta\n    .groupby('ticker')\n    .apply(estimate_liq_beta)\n    .dropna()\n    .to_frame('liq_beta')\n)\n\nprint(f\"\\nLiquidity Beta Distribution:\")\nprint(liq_betas['liq_beta'].describe().round(4))",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-practical",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-practical",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.11 Practical Guidance for Vietnam",
    "text": "14.11 Practical Guidance for Vietnam\nThe analysis in this chapter yields the following recommendations:\nFor researchers: The Amihud illiquidity ratio is the single best all-purpose liquidity proxy for Vietnamese equities. It has the highest coverage, the strongest cross-sectional return predictability, and the most robust relationship with firm size. When a second measure is needed for robustness, the zero-return proportion is the natural complement—it captures a different dimension (transaction cost threshold) and has near-complete coverage.\nFor portfolio construction: Any backtest of a Vietnamese equity strategy should compute and report estimated round-trip costs by quintile. Strategies that load on the bottom two size quintiles face costs of 2–5% per round trip, making monthly rebalancing uneconomical. Quarterly or annual rebalancing with a turnover constraint is more realistic.\nFor risk management: Monitor aggregate liquidity conditions using the cross-sectional median Amihud or the market-wide zero-return fraction. Liquidity deterioration predicts negative market returns and wider spreads in subsequent months. Tighten risk limits when aggregate illiquidity exceeds its 90th historical percentile.\nFor international comparisons: When comparing Vietnamese factor premia to U.S. or other developed market evidence, always report results on a “liquid universe” subset (top 60% by market cap) alongside the full sample. Many anomalies that appear large in the full sample shrink substantially when restricted to stocks that can actually be traded at scale.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "16_liquidity_and_turnover_measures.html#sec-liquidity-summary",
    "href": "16_liquidity_and_turnover_measures.html#sec-liquidity-summary",
    "title": "14  Liquidity and Turnover Measures",
    "section": "14.12 Summary",
    "text": "14.12 Summary\nTable Table 14.1 shows different measures of liquidity.\n\n\n\nTable 14.1: Summary of liquidity measure properties in the Vietnamese market.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nDimension\nCoverage\nSize Gradient\nReturn Predictive\nRecommended Use\n\n\n\n\nAmihud\nPrice impact\nHigh\nVery steep\nStrong\nPrimary proxy; portfolio sorts; Fama-MacBeth\n\n\nZero-Return\nTransaction cost\nComplete\nSteep\nStrong\nRobustness check; emerging market studies\n\n\nTurnover\nTrading activity\nHigh\nModerate\nModerate\nVolume-based filters; flow analysis\n\n\nRoll Spread\nTightness\nModerate\nModerate\nModerate\nSpread estimation without bid-ask data\n\n\nCorwin-Schultz\nTightness\nModerate\nModerate\nModerate\nHigh-low based spread; calibration\n\n\nQuoted Spread\nTightness\nVariable\nSteep\nStrong\nDirect measure when available\n\n\nKyle Lambda\nPrice impact\nModerate\nSteep\nStrong\nMarket microstructure research\n\n\n\n\n\n\nLiquidity is not a secondary consideration for Vietnamese equity research; it is a first-order determinant of which strategies are implementable, which anomalies are real, and which results are artifacts of trading in stocks that cannot actually be traded. Every empirical finding in this book should be evaluated through the lens of the liquidity analysis developed in this chapter.\n\n\n\n\n\n\n\nAcharya, Viral V, and Lasse Heje Pedersen. 2005. “Asset Pricing with Liquidity Risk.” Journal of Financial Economics 77 (2): 375–410.\n\n\nAmihud, Yakov. 2002. “Illiquidity and Stock Returns: Cross-Section and Time-Series Effects.” Journal of Financial Markets 5 (1): 31–56.\n\n\nAmihud, Yakov, and Haim Mendelson. 1986. “Asset Pricing and the Bid-Ask Spread.” Journal of Financial Economics 17 (2): 223–49.\n\n\nBrunnermeier, Markus K, and Lasse Heje Pedersen. 2009. “Market Liquidity and Funding Liquidity.” The Review of Financial Studies 22 (6): 2201–38.\n\n\nChordia, Tarun, Richard Roll, and Avanidhar Subrahmanyam. 2000. “Commonality in Liquidity.” Journal of Financial Economics 56 (1): 3–28.\n\n\n———. 2001. “Market Liquidity and Trading Activity.” The Journal of Finance 56 (2): 501–30.\n\n\nCorwin, Shane A, and Paul Schultz. 2012. “A Simple Way to Estimate Bid-Ask Spreads from Daily High and Low Prices.” The Journal of Finance 67 (2): 719–60.\n\n\nDatar, Vinay T, Narayan Y Naik, and Robert Radcliffe. 1998. “Liquidity and Stock Returns: An Alternative Test.” Journal of Financial Markets 1 (2): 203–19.\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\nFong, Kingsley YL, Craig W Holden, and Charles A Trzcinka. 2017. “What Are the Best Liquidity Proxies for Global Research?” Review of Finance 21 (4): 1355–1401.\n\n\nGoyenko, Ruslan Y, Craig W Holden, and Charles A Trzcinka. 2009. “Do Liquidity Measures Measure Liquidity?” Journal of Financial Economics 92 (2): 153–81.\n\n\nHameed, Allaudeen, Wenjin Kang, and Shivesh Viswanathan. 2010. “Stock Market Declines and Liquidity.” The Journal of Finance 65 (1): 257–93.\n\n\nHasbrouck, Joel. 2009. “Trading Costs and Returns for US Equities: Estimating Effective Costs from Daily Data.” The Journal of Finance 64 (3): 1445–77.\n\n\nKyle, Albert S. 1985. “Continuous Auctions and Insider Trading.” Econometrica: Journal of the Econometric Society, 1315–35.\n\n\nLesmond, David A. 2005. “Liquidity of Emerging Markets.” Journal of Financial Economics 77 (2): 411–52.\n\n\nLesmond, David A, Joseph P Ogden, and Charles A Trzcinka. 1999. “A New Estimate of Transaction Costs.” The Review of Financial Studies 12 (5): 1113–41.\n\n\nPástor, L’uboš, and Robert F Stambaugh. 2003. “Liquidity Risk and Expected Stock Returns.” Journal of Political Economy 111 (3): 642–85.\n\n\nRoll, Richard. 1984. “A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market.” The Journal of Finance 39 (4): 1127–39.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Liquidity and Turnover Measures</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html",
    "href": "17_beta_estimation.html",
    "title": "15  Beta Estimation",
    "section": "",
    "text": "15.1 Theoretical Foundation\nThis chapter introduces one of the most fundamental concepts in financial economics: the exposure of an individual stock to systematic market risk. According to the Capital Asset Pricing Model (CAPM) developed by Sharpe (1964), Lintner (1965), and Mossin (1966), cross-sectional variation in expected asset returns should be determined by the covariance between an asset’s excess return and the excess return on the market portfolio. The regression coefficient that captures this relationship (commonly known as market beta) serves as the cornerstone of modern portfolio theory and remains widely used in practice for cost of capital estimation, performance attribution, and risk management.\nIn this chapter, we develop a complete framework for estimating market betas for Vietnamese stocks. We begin with a conceptual overview of the CAPM and its empirical implementation. We then demonstrate beta estimation using ordinary least squares regression, first for individual stocks and then scaled to the entire market using rolling-window estimation. To handle the computational demands of estimating betas for hundreds of stocks across many time periods, we introduce parallelization techniques that dramatically reduce processing time. Finally, we compare beta estimates derived from monthly versus daily returns and examine how betas vary across industries and over time in the Vietnamese market.\nThe chapter leverages several important computational concepts that extend beyond beta estimation itself. Rolling-window estimation is a technique applicable to any time-varying parameter, while parallelization provides a general solution for computationally intensive tasks that can be divided into independent subtasks.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#theoretical-foundation",
    "href": "17_beta_estimation.html#theoretical-foundation",
    "title": "15  Beta Estimation",
    "section": "",
    "text": "15.1.1 The Capital Asset Pricing Model\nThe CAPM provides a theoretical framework linking expected returns to systematic risk. Under the model’s assumptions—including mean-variance optimizing investors, homogeneous expectations, and frictionless markets—the expected excess return on any asset \\(i\\) is proportional to its covariance with the market portfolio:\n\\[\nE[r_i - r_f] = \\beta_i \\cdot E[r_m - r_f]\n\\]\nwhere \\(r_i\\) is the return on asset \\(i\\), \\(r_f\\) is the risk-free rate, \\(r_m\\) is the return on the market portfolio, and \\(\\beta_i\\) is defined as:\n\\[\n\\beta_i = \\frac{\\text{Cov}(r_i, r_m)}{\\text{Var}(r_m)}\n\\]\nThe market beta \\(\\beta_i\\) measures the sensitivity of asset \\(i\\)’s returns to market movements. A beta greater than one indicates the asset amplifies market movements, while a beta less than one indicates dampened sensitivity. A beta of zero would imply no systematic risk exposure, leaving only idiosyncratic risk that can be diversified away.\n\n\n15.1.2 Empirical Implementation\nIn practice, we estimate beta by regressing excess stock returns on excess market returns:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i(r_{m,t} - r_{f,t}) + \\varepsilon_{i,t}\n\\tag{15.1}\\]\nwhere \\(\\alpha_i\\) represents abnormal return (Jensen’s alpha), \\(\\beta_i\\) is the market beta we seek to estimate, and \\(\\varepsilon_{i,t}\\) is the idiosyncratic error term. Under the CAPM, \\(\\alpha_i\\) should equal zero for all assets—any non-zero alpha represents a deviation from the model’s predictions.\nSeveral practical considerations affect beta estimation:\n\nEstimation Window: Longer windows provide more observations and thus more precise estimates, but may include outdated information if betas change over time. Common choices range from 36 to 60 months for monthly data.\nReturn Frequency: Monthly returns reduce noise but provide fewer observations. Daily returns offer more data points but may introduce microstructure effects and non-synchronous trading biases.\nMarket Proxy: The theoretical market portfolio includes all assets, but in practice we use a broad equity index. For Vietnam, we use the value-weighted market return constructed from our stock universe.\nMinimum Observations: Requiring a minimum number of observations (e.g., 48 out of 60 months) helps avoid unreliable estimates from sparse data.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#setting-up-the-environment",
    "href": "17_beta_estimation.html#setting-up-the-environment",
    "title": "15  Beta Estimation",
    "section": "15.2 Setting Up the Environment",
    "text": "15.2 Setting Up the Environment\nWe begin by loading the necessary Python packages. The core packages handle data manipulation, statistical modeling, and database operations. We also import parallelization tools that will be essential when scaling our estimation to the full market.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nfrom scipy.stats.mstats import winsorize\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\nfrom joblib import Parallel, delayed, cpu_count\nfrom dateutil.relativedelta import relativedelta\n\nWe connect to our SQLite database containing the processed Vietnamese financial data from previous chapters.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#loading-and-preparing-data",
    "href": "17_beta_estimation.html#loading-and-preparing-data",
    "title": "15  Beta Estimation",
    "section": "15.3 Loading and Preparing Data",
    "text": "15.3 Loading and Preparing Data\n\n15.3.1 Stock Returns Data\nWe load the monthly stock returns data prepared in the Datacore chapter. The data includes excess returns (returns minus the risk-free rate) for all Vietnamese listed stocks.\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess \n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Add year for merging with fundamentals\nprices_monthly[\"year\"] = prices_monthly[\"date\"].dt.year\n\nprint(f\"Loaded {len(prices_monthly):,} monthly observations\")\nprint(f\"Covering {prices_monthly['symbol'].nunique():,} unique stocks\")\nprint(f\"Date range: {prices_monthly['date'].min():%Y-%m} to {prices_monthly['date'].max():%Y-%m}\")\n\nLoaded 209,495 monthly observations\nCovering 1,837 unique stocks\nDate range: 2010-01 to 2025-05\n\n\n\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess \n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n\n\n15.3.2 Company Information\nWe load company information to enable industry-level analysis of beta estimates.\n\ncomp_vn = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, datadate, icb_name_vi \n        FROM comp_vn\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\n# Extract year for merging\ncomp_vn[\"year\"] = comp_vn[\"datadate\"].dt.year\n\nprint(f\"Company data: {comp_vn['symbol'].nunique():,} firms\")\n\nCompany data: 1,502 firms\n\n\n\n\n15.3.3 Market Excess Returns\nFor the market portfolio proxy, we use the value-weighted market excess return. If you have constructed Fama-French factors in a previous chapter, load them here. Otherwise, we can construct a simple market return from our stock data.\n\n# Option 1: Load pre-computed market factor\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT date, mkt_excess FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Option 2: Construct market return from stock data (if factors not available)\n# This computes the value-weighted average return across all stocks\ndef compute_market_return(prices_df):\n    \"\"\"\n    Compute value-weighted market return from individual stock returns.\n    \n    Parameters\n    ----------\n    prices_df : pd.DataFrame\n        Stock returns with mktcap_lag for weighting\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly market excess returns\n    \"\"\"\n    market_return = (prices_df\n        .groupby(\"date\")\n        .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n        .reset_index(name=\"mkt_excess\")\n    )\n    return market_return\n\n\n\n15.3.4 Merging Datasets\nWe combine the stock returns with market returns and company information to create our estimation dataset.\n\n# Merge stock returns with market returns\nprices_monthly = prices_monthly.merge(\n    factors_ff3_monthly, \n    on=\"date\", \n    how=\"left\"\n)\n\n# Merge with company information for industry classification\nprices_monthly = prices_monthly.merge(\n    comp_vn[[\"symbol\", \"year\", \"icb_name_vi\"]], \n    on=[\"symbol\", \"year\"], \n    how=\"left\"\n)\n\n# Remove observations with missing data\nprices_monthly = prices_monthly.dropna(subset=[\"ret_excess\", \"mkt_excess\"])\n\nprint(f\"Final estimation sample: {len(prices_monthly):,} observations\")\n\nFinal estimation sample: 169,983 observations\n\n\n\n\n15.3.5 Handling Outliers\nExtreme returns can unduly influence regression estimates. We apply winsorization to limit the impact of outliers while preserving the general distribution of returns. Winsorization at the 1% level replaces values below the 1st percentile with the 1st percentile value, and values above the 99th percentile with the 99th percentile value.\n\ndef winsorize_returns(df, columns, limits=(0.01, 0.01)):\n    \"\"\"\n    Apply winsorization to return columns to limit outlier influence.\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame containing return columns\n    columns : list\n        Column names to winsorize\n    limits : tuple\n        Lower and upper percentile limits for winsorization\n        \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with winsorized columns\n    \"\"\"\n    df = df.copy()\n    for col in columns:\n        df[col] = winsorize(df[col], limits=limits)\n    return df\n\nprices_monthly = winsorize_returns(\n    prices_monthly, \n    columns=[\"ret_excess\", \"mkt_excess\"],\n    limits=(0.01, 0.01)\n)\n\nprint(\"Return distributions after winsorization:\")\nprint(prices_monthly[[\"ret_excess\", \"mkt_excess\"]].describe().round(4))\n\nReturn distributions after winsorization:\n        ret_excess   mkt_excess\ncount  169983.0000  169983.0000\nmean        0.0011      -0.0102\nstd         0.1548       0.0579\nmin        -0.4078      -0.1794\n25%        -0.0700      -0.0384\n50%        -0.0033      -0.0084\n75%         0.0531       0.0219\nmax         0.6117       0.1221",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#estimating-beta-for-individual-stocks",
    "href": "17_beta_estimation.html#estimating-beta-for-individual-stocks",
    "title": "15  Beta Estimation",
    "section": "15.4 Estimating Beta for Individual Stocks",
    "text": "15.4 Estimating Beta for Individual Stocks\n\n15.4.1 Single Stock Example\nBefore scaling to the full market, we demonstrate beta estimation for a single well-known Vietnamese stock. We use Vingroup (VIC), one of the largest conglomerates in Vietnam with significant exposure to real estate, retail, and automotive sectors.\n\n# Filter data for Vingroup\nvic_data = prices_monthly.query(\"symbol == 'VIC'\").copy()\n\nprint(f\"VIC observations: {len(vic_data)}\")\nprint(f\"Date range: {vic_data['date'].min():%Y-%m} to {vic_data['date'].max():%Y-%m}\")\n\nVIC observations: 150\nDate range: 2011-07 to 2023-12\n\n\nWe estimate the CAPM regression using ordinary least squares via the statsmodels package. The formula interface provides a convenient way to specify regression models.\n\n# Estimate CAPM for Vingroup\nmodel_vic = smf.ols(\n    formula=\"ret_excess ~ mkt_excess\",\n    data=vic_data\n).fit()\n\n# Display regression results\nprint(model_vic.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:             ret_excess   R-squared:                       0.153\nModel:                            OLS   Adj. R-squared:                  0.147\nMethod:                 Least Squares   F-statistic:                     26.67\nDate:                Sat, 14 Feb 2026   Prob (F-statistic):           7.66e-07\nTime:                        07:51:19   Log-Likelihood:                 131.96\nNo. Observations:                 150   AIC:                            -259.9\nDf Residuals:                     148   BIC:                            -253.9\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.0075      0.008     -0.895      0.372      -0.024       0.009\nmkt_excess     0.7503      0.145      5.164      0.000       0.463       1.037\n==============================================================================\nOmnibus:                       39.111   Durbin-Watson:                   2.039\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              107.620\nSkew:                          -1.015   Prob(JB):                     4.27e-24\nKurtosis:                       6.619   Cond. No.                         17.6\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nThe regression output provides several important pieces of information:\n\nBeta (mkt_excess coefficient): The estimated market sensitivity. A beta above 1 indicates VIC amplifies market movements.\nAlpha (Intercept): The abnormal return not explained by market exposure. Under CAPM, this should be zero.\nR-squared: The proportion of return variation explained by market movements.\nt-statistics: Test whether coefficients differ significantly from zero.\n\n\n# Extract key estimates\ncoefficients = model_vic.summary2().tables[1]\n\nprint(\"\\nKey estimates for Vingroup (VIC):\")\nprint(f\"  Beta:  {coefficients.loc['mkt_excess', 'Coef.']:.3f}\")\nprint(f\"  Alpha: {coefficients.loc['Intercept', 'Coef.']:.4f}\")\nprint(f\"  R²:    {model_vic.rsquared:.3f}\")\n\n\nKey estimates for Vingroup (VIC):\n  Beta:  0.750\n  Alpha: -0.0075\n  R²:    0.153\n\n\n\n\n15.4.2 CAPM Estimation Function\nWe create a reusable function that estimates the CAPM and returns results in a standardized format. The function includes a minimum observations requirement to avoid unreliable estimates from sparse data.\n\ndef estimate_capm(data, min_obs=48):\n    \"\"\"\n    Estimate CAPM regression and return coefficients.\n    \n    This function regresses excess stock returns on excess market returns\n    and extracts the coefficient estimates along with t-statistics.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        DataFrame with 'ret_excess' and 'mkt_excess' columns\n    min_obs : int\n        Minimum number of observations required for estimation\n        \n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with coefficient estimates and t-statistics,\n        or empty DataFrame if insufficient observations\n    \"\"\"\n    if len(data) &lt; min_obs:\n        return pd.DataFrame()\n    \n    try:\n        # Estimate OLS regression\n        model = smf.ols(\n            formula=\"ret_excess ~ mkt_excess\", \n            data=data\n        ).fit()\n        \n        # Extract coefficient table\n        coef_table = model.summary2().tables[1]\n        \n        # Format results\n        results = pd.DataFrame({\n            \"coefficient\": [\"alpha\", \"beta\"],\n            \"estimate\": [\n                coef_table.loc[\"Intercept\", \"Coef.\"],\n                coef_table.loc[\"mkt_excess\", \"Coef.\"]\n            ],\n            \"t_statistic\": [\n                coef_table.loc[\"Intercept\", \"t\"],\n                coef_table.loc[\"mkt_excess\", \"t\"]\n            ],\n            \"r_squared\": model.rsquared\n        })\n        \n        return results\n        \n    except Exception as e:\n        # Return empty DataFrame if estimation fails\n        return pd.DataFrame()",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#rolling-window-estimation",
    "href": "17_beta_estimation.html#rolling-window-estimation",
    "title": "15  Beta Estimation",
    "section": "15.5 Rolling-Window Estimation",
    "text": "15.5 Rolling-Window Estimation\n\n15.5.1 Motivation for Rolling Windows\nStock betas are not constant over time. A company’s business mix, leverage, and operating environment evolve, causing its systematic risk exposure to change. To capture this time variation, we use rolling-window estimation: at each point in time, we estimate beta using only data from a fixed lookback period (e.g., the past 60 months).\nRolling-window estimation involves a trade-off:\n\nLonger windows provide more observations and thus more precise estimates, but may include stale information.\nShorter windows are more responsive to changes but produce noisier estimates.\n\nA common choice in academic research is 60 months (5 years) of monthly data, requiring at least 48 valid observations for estimation.\n\n\n15.5.2 Rolling Window Implementation\nThe following function implements rolling-window CAPM estimation. For each month in the sample, it looks back over the specified window and estimates beta using all available data within that window.\n\ndef roll_capm_estimation(data, look_back=60, min_obs=48):\n    \"\"\"\n    Perform rolling-window CAPM estimation.\n    \n    This function slides a window across time, estimating the CAPM\n    regression at each point using the most recent 'look_back' months\n    of data.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        DataFrame with 'date', 'ret_excess', and 'mkt_excess' columns\n    look_back : int\n        Number of months in the estimation window\n    min_obs : int\n        Minimum observations required within each window\n        \n    Returns\n    -------\n    pd.DataFrame\n        Time series of coefficient estimates with dates\n    \"\"\"\n    # Ensure data is sorted by date\n    data = data.sort_values(\"date\").copy()\n    \n    # Get unique dates\n    dates = data[\"date\"].drop_duplicates().sort_values()\n    \n    # Container for results\n    results = []\n    \n    # Slide window across dates\n    for i in range(look_back - 1, len(dates)):\n        # Define window boundaries\n        end_date = dates.iloc[i]\n        start_date = end_date - relativedelta(months=look_back - 1)\n        \n        # Extract data within window\n        window_data = data.query(\"date &gt;= @start_date and date &lt;= @end_date\")\n        \n        # Estimate CAPM for this window\n        window_results = estimate_capm(window_data, min_obs=min_obs)\n        \n        if not window_results.empty:\n            window_results[\"date\"] = end_date\n            results.append(window_results)\n    \n    # Combine all results\n    if results:\n        return pd.concat(results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\n\n15.5.3 Example: Rolling Betas for Selected Stocks\nWe demonstrate rolling-window estimation for several well-known Vietnamese stocks spanning different industries.\n\n# Define example stocks\nexamples = pd.DataFrame({\n    \"symbol\": [\"FPT\", \"VNM\", \"VIC\", \"HPG\", \"VCB\"],\n    \"company\": [\n        \"FPT Corporation\",      # Technology\n        \"Vinamilk\",             # Consumer goods\n        \"Vingroup\",             # Real estate/conglomerate\n        \"Hoa Phat Group\",       # Steel/materials\n        \"Vietcombank\"           # Banking\n    ]\n})\n\n# Check data availability for each example\ndata_availability = (prices_monthly\n    .query(\"symbol in @examples['symbol']\")\n    .groupby(\"symbol\")\n    .agg(\n        n_obs=(\"date\", \"count\"),\n        first_date=(\"date\", \"min\"),\n        last_date=(\"date\", \"max\")\n    )\n    .reset_index()\n)\n\nprint(\"Data availability for example stocks:\")\nprint(data_availability)\n\nData availability for example stocks:\n  symbol  n_obs first_date  last_date\n0    FPT    150 2011-07-31 2023-12-31\n1    HPG    150 2011-07-31 2023-12-31\n2    VCB    150 2011-07-31 2023-12-31\n3    VIC    150 2011-07-31 2023-12-31\n4    VNM    150 2011-07-31 2023-12-31\n\n\n\n# Estimate rolling betas for example stocks\nexample_data = prices_monthly.query(\"symbol in @examples['symbol']\")\n\ncapm_examples = (example_data\n    .groupby(\"symbol\", group_keys=True)\n    .apply(lambda x: roll_capm_estimation(x), include_groups=False)\n    .reset_index()\n    .drop(columns=\"level_1\", errors=\"ignore\")\n)\n\n# Filter to beta estimates only\nbeta_examples = (capm_examples\n    .query(\"coefficient == 'beta'\")\n    .merge(examples, on=\"symbol\")\n)\n\nprint(f\"Rolling beta estimates: {len(beta_examples):,} observations\")\n\nRolling beta estimates: 455 observations\n\n\n\n\n15.5.4 Visualizing Rolling Betas\nFigure 22.6 displays the time series of beta estimates for our example stocks. The figure reveals how systematic risk exposure evolves differently across industries.\n\nrolling_beta_figure = (\n    ggplot(\n        beta_examples,\n        aes(x=\"date\", y=\"estimate\", color=\"company\")\n    )\n    + geom_line(size=0.8)\n    + geom_hline(yintercept=1, linetype=\"dashed\", color=\"gray\", alpha=0.7)\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        color=\"\",\n        title=\"Rolling Beta Estimates (60-Month Window)\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n    + theme(legend_position=\"bottom\")\n)\nrolling_beta_figure.show()\n\n\n\n\n\n\n\nFigure 15.1: Monthly rolling beta estimates for selected Vietnamese stocks using a 60-month estimation window. Different industries exhibit distinct patterns of market sensitivity over time.\n\n\n\n\n\nSeveral patterns emerge from the figure:\n\nIndustry differences: Technology and banking stocks may exhibit different beta patterns than real estate or consumer goods companies.\nTime variation: Betas are not constant. They respond to changes in business conditions, leverage, and market regimes.\nCrisis periods: Market stress periods (e.g., 2008 financial crisis, 2020 COVID-19) often see beta estimates change as correlations across stocks increase.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#parallelized-estimation-for-the-full-market",
    "href": "17_beta_estimation.html#parallelized-estimation-for-the-full-market",
    "title": "15  Beta Estimation",
    "section": "15.6 Parallelized Estimation for the Full Market",
    "text": "15.6 Parallelized Estimation for the Full Market\n\n15.6.1 The Computational Challenge\nEstimating rolling betas for all stocks in our database is computationally intensive. With hundreds of stocks, each requiring rolling estimation across many time periods, sequential processing would take considerable time. Fortunately, beta estimation for different stocks is independent (i.e., the estimate for stock A does not depend on the estimate for stock B). This independence makes the problem ideal for parallelization.\n\n\n15.6.2 Setting Up Parallel Processing\nWe use the joblib library to distribute computation across multiple CPU cores. The Parallel class manages worker processes, while delayed wraps function calls for deferred execution.\n\n# Determine available cores (reserve one for system operations)\nn_cores = max(1, cpu_count() - 1)\nprint(f\"Available cores for parallel processing: {n_cores}\")\n\nAvailable cores for parallel processing: 3\n\n\n\n\n15.6.3 Parallel Beta Estimation\nThe following code estimates rolling betas for all stocks in parallel. Each stock is processed independently by a separate worker.\n\ndef estimate_all_betas_parallel(data, n_cores, look_back=60, min_obs=48):\n    \"\"\"\n    Estimate rolling betas for all stocks using parallel processing.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Full dataset with all stocks\n    n_cores : int\n        Number of CPU cores to use\n    look_back : int\n        Months in estimation window\n    min_obs : int\n        Minimum observations required\n        \n    Returns\n    -------\n    pd.DataFrame\n        Beta estimates for all stocks and dates\n    \"\"\"\n    # Group data by stock\n    grouped = data.groupby(\"symbol\", group_keys=False)\n    \n    # Define worker function\n    def process_stock(name, group):\n        result = roll_capm_estimation(group, look_back=look_back, min_obs=min_obs)\n        if not result.empty:\n            result[\"symbol\"] = name\n        return result\n    \n    # Execute in parallel\n    results = Parallel(n_jobs=n_cores, verbose=1)(\n        delayed(process_stock)(name, group) \n        for name, group in grouped\n    )\n    \n    # Combine results\n    results = [r for r in results if not r.empty]\n    if results:\n        return pd.concat(results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\n# Estimate betas for all stocks\nprint(\"Estimating rolling betas for all stocks...\")\ncapm_monthly = estimate_all_betas_parallel(\n    prices_monthly, \n    n_cores=n_cores,\n    look_back=60,\n    min_obs=48\n)\n\nprint(f\"\\nCompleted: {len(capm_monthly):,} coefficient estimates\")\nprint(f\"Unique stocks: {capm_monthly['symbol'].nunique():,}\")\n\n\n\n15.6.4 Storing Results\nWe save the CAPM estimates to our database for use in subsequent chapters.\n\ncapm_monthly.to_sql(\n    name=\"capm_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"CAPM estimates saved to database.\")\n\nFor subsequent analysis, we load the pre-computed estimates:\n\ncapm_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM capm_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Loaded {len(capm_monthly):,} CAPM estimates\")\n\nLoaded 161,580 CAPM estimates",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#beta-estimation-using-daily-returns",
    "href": "17_beta_estimation.html#beta-estimation-using-daily-returns",
    "title": "15  Beta Estimation",
    "section": "15.7 Beta Estimation Using Daily Returns",
    "text": "15.7 Beta Estimation Using Daily Returns\nWhile monthly returns are standard in academic research, some applications benefit from higher-frequency data:\n\nShorter estimation windows: Daily data allows meaningful estimation over shorter periods (e.g., 3 months rather than 5 years).\nMore responsive estimates: Daily betas capture changes more quickly.\nEvent studies: High-frequency betas are useful for analyzing market reactions to specific events.\n\nHowever, daily data introduces additional challenges:\n\nMicrostructure noise: Bid-ask bounce and other trading frictions add noise to returns.\nNon-synchronous trading: Less liquid stocks may not trade every day, biasing beta estimates downward.\nComputational burden: Daily data is roughly 21 times larger than monthly data.\n\n\n15.7.1 Batch Processing for Daily Data\nGiven the size of daily data, we process stocks in batches to manage memory constraints. This approach loads and processes a subset of stocks, saves results, and proceeds to the next batch.\n\ndef compute_market_return_daily(tidy_finance):\n    \"\"\"\n    Compute daily value-weighted market excess return from stock data.\n    \"\"\"\n    # Load daily prices with market cap for weighting\n    prices_daily_full = pd.read_sql_query(\n        sql=\"\"\"\n            SELECT p.symbol, p.date, p.ret_excess, m.mktcap_lag\n            FROM prices_daily p\n            LEFT JOIN prices_monthly m ON p.symbol = m.symbol \n                AND strftime('%Y-%m', p.date) = strftime('%Y-%m', m.date)\n        \"\"\",\n        con=tidy_finance,\n        parse_dates={\"date\"}\n    )\n    \n    # Compute value-weighted market return each day\n    mkt_daily = (prices_daily_full\n        .dropna(subset=[\"ret_excess\", \"mktcap_lag\"])\n        .groupby(\"date\")\n        .apply(lambda x: np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]))\n        .reset_index(name=\"mkt_excess\")\n    )\n    \n    return mkt_daily\n\n\ndef roll_capm_estimation_daily(data, look_back_days=1260, min_obs=1000):\n    \"\"\"\n    Perform rolling-window CAPM estimation using daily data.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        DataFrame with 'date', 'ret_excess', and 'mkt_excess' columns\n    look_back_days : int\n        Number of trading days in the estimation window\n    min_obs : int\n        Minimum daily observations required within each window\n        \n    Returns\n    -------\n    pd.DataFrame\n        Time series of coefficient estimates with dates\n    \"\"\"\n    data = data.sort_values(\"date\").copy()\n    dates = data[\"date\"].drop_duplicates().sort_values().reset_index(drop=True)\n    \n    results = []\n    \n    for i in range(look_back_days - 1, len(dates)):\n        end_date = dates.iloc[i]\n        start_idx = max(0, i - look_back_days + 1)\n        start_date = dates.iloc[start_idx]\n        \n        window_data = data.query(\"date &gt;= @start_date and date &lt;= @end_date\")\n        window_results = estimate_capm(window_data, min_obs=min_obs)\n        \n        if not window_results.empty:\n            window_results[\"date\"] = end_date\n            results.append(window_results)\n    \n    if results:\n        return pd.concat(results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\ndef estimate_daily_betas_batch(symbols, tidy_finance, n_cores, batch_size=500, \n                                look_back_days=1260, min_obs=1000):\n    \"\"\"\n    Estimate rolling betas from daily data using batch processing.\n    \"\"\"\n    # First, compute or load market return\n    print(\"Computing daily market excess returns...\")\n    mkt_daily = compute_market_return_daily(tidy_finance)\n    print(f\"Market returns: {len(mkt_daily)} days\")\n    \n    n_batches = int(np.ceil(len(symbols) / batch_size))\n    all_results = []\n    \n    for j in range(n_batches):\n        batch_start = j * batch_size\n        batch_end = min((j + 1) * batch_size, len(symbols))\n        batch_symbols = symbols[batch_start:batch_end]\n        \n        symbol_list = \", \".join(f\"'{s}'\" for s in batch_symbols)\n        \n        query = f\"\"\"\n            SELECT symbol, date, ret_excess\n            FROM prices_daily\n            WHERE symbol IN ({symbol_list})\n        \"\"\"\n        \n        prices_daily_batch = pd.read_sql_query(\n            sql=query,\n            con=tidy_finance,\n            parse_dates={\"date\"}\n        )\n        \n        # Merge with market excess return\n        prices_daily_batch = prices_daily_batch.merge(\n            mkt_daily, \n            on=\"date\", \n            how=\"inner\"\n        )\n        \n        # Group by symbol and estimate betas\n        grouped = prices_daily_batch.groupby(\"symbol\", group_keys=False)\n        \n        # Parallel estimation\n        batch_results = Parallel(n_jobs=n_cores)(\n            delayed(lambda name, group: \n                roll_capm_estimation_daily(group, look_back_days=look_back_days, min_obs=min_obs)\n                .assign(symbol=name)\n            )(name, group)\n            for name, group in grouped\n        )\n        \n        batch_results = [r for r in batch_results if r is not None and not r.empty]\n        \n        if batch_results:\n            all_results.append(pd.concat(batch_results, ignore_index=True))\n        \n        print(f\"Batch {j+1}/{n_batches} complete\")\n    \n    if all_results:\n        return pd.concat(all_results, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n\nsymbols = prices_monthly[\"symbol\"].unique().tolist()\n\ncapm_daily = estimate_daily_betas_batch(\n    symbols=symbols,\n    tidy_finance=tidy_finance,\n    n_cores=n_cores,\n    batch_size=500,\n    look_back_days=1260,  # ~5 years of trading days\n    min_obs=1000\n)\n\nprint(f\"Daily beta estimates: {len(capm_daily):,}\")\n\n\ncapm_daily.to_sql(\n    name=\"capm_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"CAPM estimates saved to database.\")\n\nFor subsequent analysis, we load the pre-computed estimates:\n\ncapm_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM capm_daily\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Loaded {len(capm_daily):,} CAPM estimates\")\n\nLoaded 3,394,490 CAPM estimates",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#analyzing-beta-estimates",
    "href": "17_beta_estimation.html#analyzing-beta-estimates",
    "title": "15  Beta Estimation",
    "section": "15.8 Analyzing Beta Estimates",
    "text": "15.8 Analyzing Beta Estimates\n\n15.8.1 Extracting Beta Estimates\nWe extract the beta coefficient estimates from our CAPM results for analysis.\n\n# Extract monthly betas\nbeta_monthly = (capm_monthly\n    .query(\"coefficient == 'beta'\")\n    .rename(columns={\"estimate\": \"beta\"})\n    [[\"symbol\", \"date\", \"beta\"]]\n    .assign(frequency=\"monthly\")\n)\n\n# Save to database\nbeta_monthly.to_sql(\n    name=\"beta_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(f\"Monthly betas: {len(beta_monthly):,} observations\")\nprint(f\"Unique stocks: {beta_monthly['symbol'].nunique():,}\")\n\nMonthly betas: 80,790 observations\nUnique stocks: 1,383\n\n\n\n# Load pre-computed betas\nbeta_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM beta_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n\n\n15.8.2 Summary Statistics\nWe examine the distribution of beta estimates to verify their reasonableness.\n\nprint(\"Beta Summary Statistics:\")\nprint(beta_monthly[\"beta\"].describe().round(3))\n\n# Additional diagnostics\nprint(f\"\\nStocks with negative average beta: {(beta_monthly.groupby('symbol')['beta'].mean() &lt; 0).sum()}\")\nprint(f\"Stocks with beta &gt; 2: {(beta_monthly.groupby('symbol')['beta'].mean() &gt; 2).sum()}\")\n\nBeta Summary Statistics:\ncount    80790.000\nmean         0.501\nstd          0.539\nmin         -1.345\n25%          0.130\n50%          0.447\n75%          0.832\nmax          2.678\nName: beta, dtype: float64\n\nStocks with negative average beta: 177\nStocks with beta &gt; 2: 5\n\n\n\n\n15.8.3 Beta Distribution Across Industries\nDifferent industries have different exposures to systematic market risk based on their business models, operating leverage, and financial leverage. Figure 15.2 shows the distribution of firm-level average betas across Vietnamese industries.\n\n# Merge betas with industry information\nbeta_with_industry = (beta_monthly\n    .merge(\n        prices_monthly[[\"symbol\", \"date\", \"icb_name_vi\"]].drop_duplicates(),\n        on=[\"symbol\", \"date\"],\n        how=\"left\"\n    )\n    .dropna(subset=[\"icb_name_vi\"])\n)\n\n# Compute firm-level average beta by industry\nbeta_by_industry = (beta_with_industry\n    .groupby([\"icb_name_vi\", \"symbol\"])[\"beta\"]\n    .mean()\n    .reset_index()\n)\n\n# Order industries by median beta\nindustry_order = (beta_by_industry\n    .groupby(\"icb_name_vi\")[\"beta\"]\n    .median()\n    .sort_values()\n    .index.tolist()\n)\n\n# Select top 10 industries by number of firms for clearer visualization\ntop_industries = (beta_by_industry\n    .groupby(\"icb_name_vi\")\n    .size()\n    .nlargest(10)\n    .index.tolist()\n)\n\nbeta_by_industry_filtered = beta_by_industry.query(\"icb_name_vi in @top_industries\")\n\n\nbeta_industry_figure = (\n    ggplot(\n        beta_by_industry_filtered,\n        aes(x=\"icb_name_vi\", y=\"beta\")\n    )\n    + geom_boxplot(fill=\"steelblue\", alpha=0.7)\n    + geom_hline(yintercept=1, linetype=\"dashed\", color=\"red\", alpha=0.7)\n    + coord_flip()\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        title=\"Beta Distribution by Industry\"\n    )\n    + theme_minimal()\n)\nbeta_industry_figure.show()\n\n\n\n\n\n\n\nFigure 15.2: Distribution of firm-level average betas across Vietnamese industries. Box plots show the median, interquartile range, and outliers for each industry.\n\n\n\n\n\n\n\n15.8.4 Time Variation in Cross-Sectional Beta Distribution\nBetas vary not only across stocks but also over time. Figure 15.3 shows how the cross-sectional distribution of betas has evolved in the Vietnamese market.\n\n# Compute monthly quantiles\nbeta_quantiles = (beta_monthly\n    .groupby(\"date\")[\"beta\"]\n    .quantile(q=np.arange(0.1, 1.0, 0.1))\n    .reset_index()\n    .rename(columns={\"level_1\": \"quantile\"})\n    .assign(quantile=lambda x: (x[\"quantile\"] * 100).astype(int).astype(str) + \"%\")\n)\n\nbeta_quantiles_figure = (\n    ggplot(\n        beta_quantiles,\n        aes(x=\"date\", y=\"beta\", color=\"quantile\")\n    )\n    + geom_line(alpha=0.8)\n    + geom_hline(yintercept=1, linetype=\"dashed\", color=\"gray\")\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        color=\"Quantile\",\n        title=\"Cross-Sectional Distribution of Betas Over Time\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n)\nbeta_quantiles_figure.show()\n\n\n\n\n\n\n\nFigure 15.3: Monthly quantiles of beta estimates over time. Each line represents a decile of the cross-sectional beta distribution.\n\n\n\n\n\nThe figure reveals several interesting patterns:\n\nLevel shifts: The entire distribution of betas can shift over time, reflecting changes in market-wide correlation.\nDispersion changes: During market stress, the spread between high and low beta stocks may change as correlations move.\nTrends: Some periods show trending behavior in betas, possibly reflecting structural changes in the economy.\n\n\n\n15.8.5 Coverage Analysis\nWe verify that our estimation procedure produces reasonable coverage across the sample. Figure 15.4 shows the fraction of stocks with available beta estimates over time.\n\n# Count stocks with and without betas\ncoverage = (prices_monthly\n    .groupby(\"date\")[\"symbol\"]\n    .nunique()\n    .reset_index(name=\"total_stocks\")\n    .merge(\n        beta_monthly.groupby(\"date\")[\"symbol\"].nunique().reset_index(name=\"with_beta\"),\n        on=\"date\",\n        how=\"left\"\n    )\n    .fillna(0)\n    .assign(coverage=lambda x: x[\"with_beta\"] / x[\"total_stocks\"])\n)\n\ncoverage_figure = (\n    ggplot(coverage, aes(x=\"date\", y=\"coverage\"))\n    + geom_line(color=\"steelblue\", size=1)\n    + labs(\n        x=\"\",\n        y=\"Share with Beta Estimate\",\n        title=\"Beta Estimation Coverage Over Time\"\n    )\n    + scale_y_continuous(labels=percent_format(), limits=(0, 1))\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n)\ncoverage_figure.show()\n\n\n\n\n\n\n\nFigure 15.4: Share of stocks with available beta estimates over time. Coverage increases as more stocks accumulate sufficient return history.\n\n\n\n\n\nCoverage is lower in early years because stocks need sufficient return history (at least 48 months) before their betas can be estimated. As the market matures and stocks accumulate longer histories, coverage approaches 100%.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#comparing-monthly-and-daily-beta-estimates",
    "href": "17_beta_estimation.html#comparing-monthly-and-daily-beta-estimates",
    "title": "15  Beta Estimation",
    "section": "15.9 Comparing Monthly and Daily Beta Estimates",
    "text": "15.9 Comparing Monthly and Daily Beta Estimates\nWhen both monthly and daily beta estimates are available, we can compare them to understand how estimation frequency affects results.\n\n# Combine monthly and daily estimates\nbeta_daily = (capm_daily\n    .query(\"coefficient == 'beta'\")\n    .rename(columns={\"estimate\": \"beta\"})\n    [[\"symbol\", \"date\", \"beta\"]]\n    .assign(frequency=\"daily\")\n)\n\nbeta_combined = pd.concat([beta_monthly, beta_daily], ignore_index=True)\n\n\n# Filter to example stocks\nbeta_comparison = (beta_combined\n    .merge(examples, on=\"symbol\")\n    .query(\"symbol in ['VIC', 'FPT']\")  # Select two for clarity\n)\n\ncomparison_figure = (\n    ggplot(\n        beta_comparison,\n        aes(x=\"date\", y=\"beta\", color=\"frequency\", linetype=\"frequency\")\n    )\n    + geom_line(size=0.8)\n    + facet_wrap(\"~company\", ncol=1)\n    + labs(\n        x=\"\",\n        y=\"Beta\",\n        color=\"Data Frequency\",\n        linetype=\"Data Frequency\",\n        title=\"Monthly vs Daily Beta Estimates\"\n    )\n    + scale_x_datetime(date_breaks=\"2 years\", date_labels=\"%Y\")\n    + theme_minimal()\n    + theme(legend_position=\"bottom\")\n)\ncomparison_figure.show()\n\n\n\n\n\n\n\nFigure 15.5: Comparison of beta estimates using monthly versus daily returns for selected stocks. Daily estimates are smoother due to more observations per estimation window.\n\n\n\n\n\nThe comparison reveals that daily-based estimates are generally smoother due to the larger number of observations in each window. However, the level and trend of estimates are similar across frequencies, providing validation that both approaches capture the same underlying systematic risk exposure.\n\n# Correlation between monthly and daily estimates\ncorrelation_data = (beta_combined\n    .pivot_table(index=[\"symbol\", \"date\"], columns=\"frequency\", values=\"beta\")\n    .dropna()\n)\n\nprint(f\"Correlation between monthly and daily betas: {correlation_data.corr().iloc[0,1]:.3f}\")\n\nCorrelation between monthly and daily betas: 0.745\n\n\n\n\n\nTable 15.1: Theoretical Reasons for Imperfect Correlation\n\n\n\n\n\n\n\n\n\nFactor\nEffect\n\n\n\n\nNon-synchronous trading\nDaily betas can be biased downward for illiquid stocks\n\n\nMicrostructure noise\nBid-ask bounce adds noise to daily estimates\n\n\nDifferent effective windows\nSame calendar period but ~20x more observations for daily\n\n\nMean reversion speed\nDaily captures faster-moving risk dynamics\n\n\n\n\n\n\nTable 15.1 shows several reasons why we might observe imperfect correlation.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "17_beta_estimation.html#key-takeaways",
    "href": "17_beta_estimation.html#key-takeaways",
    "title": "15  Beta Estimation",
    "section": "15.10 Key Takeaways",
    "text": "15.10 Key Takeaways\n\nCAPM beta measures a stock’s sensitivity to systematic market risk and is fundamental to modern portfolio theory, cost of capital estimation, and risk management.\nRolling-window estimation captures time variation in betas, which reflects changes in companies’ business models, leverage, and market conditions.\nParallelization dramatically reduces computation time for large-scale estimation tasks by distributing work across multiple CPU cores.\nEstimation choices matter: Window length, return frequency, and minimum observation requirements all affect beta estimates. Researchers should choose parameters appropriate for their specific application.\nIndustry patterns: Vietnamese stocks show systematic differences in market sensitivity across industries, with cyclical sectors exhibiting higher betas than defensive sectors.\nTime variation: The cross-sectional distribution of betas in Vietnam has evolved over time, with notable shifts during market stress periods.\nFrequency comparison: Monthly and daily beta estimates are positively correlated but not identical. Daily estimates are smoother while monthly estimates may better capture lower-frequency variation.\nData quality checks: Coverage analysis and summary statistics help identify potential issues in estimation procedures before using results in downstream analyses.\n\n\n\n\n\n\n\nLintner, John. 1965. “Security prices, risk, and maximal gains from diversification.” The Journal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital asset market.” Econometrica 34 (4): 768–83. https://doi.org/10.2307/1910098.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "Home",
      "Xây dựng danh mục đầu tư, rủi ro và cơ học thực nghiệm",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Beta Estimation</span>"
    ]
  },
  {
    "objectID": "20_capm.html",
    "href": "20_capm.html",
    "title": "16  The Capital Asset Pricing Model",
    "section": "",
    "text": "16.1 From Efficient Portfolios to Equilibrium Prices\nThe previous chapter on Modern Portfolio Theory (MPT) showed how an investor can construct portfolios that optimally trade off risk and expected return. But MPT leaves a crucial question unanswered: What determines the expected returns themselves? Why do some assets command higher risk premiums than others?\nThe Capital Asset Pricing Model (CAPM) answers this question. Developed simultaneously by Sharpe (1964), Lintner (1965), and Mossin (1966), the CAPM extends MPT to explain how assets should be priced in equilibrium when all investors follow mean-variance optimization principles. The CAPM’s central insight is both elegant and counterintuitive: not all risk is rewarded. Only the component of risk that cannot be diversified away (i.e., systematic risk) commands a risk premium in equilibrium.\nThe CAPM remains the cornerstone of asset pricing theory, not because it perfectly describes reality, but because it provides the simplest coherent framework for understanding the relationship between risk and expected return. Every extension and alternative model in asset pricing (e.g., from the Fama-French factors to consumption-based pricing) builds upon or reacts against the CAPM’s foundational logic.\nIn this chapter, we derive the CAPM from first principles, illustrate its theoretical underpinnings, and show how to estimate its parameters empirically. We download stock market data, estimate betas using regression analysis, and evaluate asset performance relative to model predictions.\nimport pandas as pd\nimport numpy as np\nimport tidyfinance as tf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "20_capm.html#systematic-versus-idiosyncratic-risk",
    "href": "20_capm.html#systematic-versus-idiosyncratic-risk",
    "title": "16  The Capital Asset Pricing Model",
    "section": "16.2 Systematic versus Idiosyncratic Risk",
    "text": "16.2 Systematic versus Idiosyncratic Risk\nBefore diving into the mathematics, we need to understand the fundamental distinction that makes the CAPM work: the difference between systematic and idiosyncratic risk.\n\n16.2.1 Idiosyncratic Risk: Diversifiable and Unrewarded\nConsider events that affect individual companies but not the broader market: a CEO resigns unexpectedly, a product launch fails, earnings disappoint analysts, or a factory experiences a fire. These company-specific events can dramatically affect individual stock prices, but they tend to average out across a diversified portfolio. When one company has bad news, another often has good news; the shocks are largely uncorrelated.\nThis idiosyncratic (or firm-specific) risk can be eliminated through diversification. By holding a portfolio of many stocks, an investor can reduce idiosyncratic risk to nearly zero. Since this risk can be avoided at no cost, investors should not expect compensation for bearing it. In equilibrium, idiosyncratic risk earns no premium.\n\n\n16.2.2 Systematic Risk: Undiversifiable and Priced\nSystematic risk, by contrast, affects all assets simultaneously. Recessions, interest rate changes, geopolitical crises, and pandemics impact virtually every company to some degree. No amount of diversification can eliminate exposure to these economy-wide shocks, they are inherent to participating in the market.\nSince systematic risk cannot be diversified away, investors genuinely dislike it. They must be compensated for bearing it. The CAPM formalizes this intuition: expected returns should depend only on systematic risk, not total risk. Two assets with identical total volatility can have very different expected returns if their systematic risk exposures differ.\n\n\n16.2.3 A Simple Illustration\nImagine two stocks with identical 30% annual volatility. Stock A is a gold mining company whose returns move opposite to the overall market: it does well when the economy struggles and poorly when it booms. Stock B is a luxury retailer that amplifies market movements: soaring in good times and crashing in bad times.\nWhich stock should offer higher expected returns? Intuition might suggest they should be equal since both have the same volatility. But the CAPM says Stock B should offer substantially higher returns. Why? Because Stock B performs poorly precisely when investors’ overall wealth is already down (during market crashes), making its returns particularly painful. Stock A, by contrast, provides insurance. Its strong performance during market downturns partially offsets losses elsewhere in the portfolio. Investors value this insurance property and are willing to accept lower expected returns in exchange.\nThis is the CAPM’s core insight: expected returns compensate investors for systematic risk exposure, measured by how an asset’s returns co-move with the market portfolio.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "20_capm.html#data-preparation",
    "href": "20_capm.html#data-preparation",
    "title": "16  The Capital Asset Pricing Model",
    "section": "16.3 Data Preparation",
    "text": "16.3 Data Preparation\nBuilding on our analysis from the previous chapter, we examine the VN30 constituents as our asset universe. We download and prepare monthly return data:\n\nvn30_symbols = [\n    \"ACB\", \"BCM\", \"BID\", \"BVH\", \"CTG\", \"FPT\", \"GAS\", \"GVR\", \"HDB\", \"HPG\",\n    \"MBB\", \"MSN\", \"MWG\", \"PLX\", \"POW\", \"SAB\", \"SHB\", \"SSB\", \"STB\", \"TCB\",\n    \"TPB\", \"VCB\", \"VHM\", \"VIB\", \"VIC\", \"VJC\", \"VNM\", \"VPB\", \"VRE\", \"EIB\"\n]\n\n\nimport os\nimport boto3\nfrom botocore.client import Config\nfrom io import BytesIO\n\nclass ConnectMinio:\n    def __init__(self):\n        self.MINIO_ENDPOINT = os.environ[\"MINIO_ENDPOINT\"]\n        self.MINIO_ACCESS_KEY = os.environ[\"MINIO_ACCESS_KEY\"]\n        self.MINIO_SECRET_KEY = os.environ[\"MINIO_SECRET_KEY\"]\n        self.REGION = os.getenv(\"MINIO_REGION\", \"us-east-1\")\n\n        self.s3 = boto3.client(\n            \"s3\",\n            endpoint_url=self.MINIO_ENDPOINT,\n            aws_access_key_id=self.MINIO_ACCESS_KEY,\n            aws_secret_access_key=self.MINIO_SECRET_KEY,\n            region_name=self.REGION,\n            config=Config(signature_version=\"s3v4\"),\n        )\n\n    def test_connection(self):\n        resp = self.s3.list_buckets()\n        print(\"Connected. Buckets:\")\n        for b in resp.get(\"Buckets\", []):\n            print(\" -\", b[\"Name\"])\n\n\nconn = ConnectMinio()\ns3 = conn.s3\nconn.test_connection()\n\nbucket_name = os.environ[\"MINIO_BUCKET\"]\n\nprices = pd.read_csv(\n    BytesIO(\n        s3.get_object(\n            Bucket=bucket_name,\n            Key=\"historycal_price/dataset_historical_price.csv\"\n        )[\"Body\"].read()\n    ),\n    low_memory=False\n)\n\nConnected. Buckets:\n - dsteam-data\n - rawbctc\n\n\nWe process the raw price data to compute adjusted closing prices and standardize column names:\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\nprices = prices.rename(columns={\n    \"vol_total\": \"volume\",\n    \"open_price\": \"open\",\n    \"low_price\": \"low\",\n    \"high_price\": \"high\",\n    \"close_price\": \"close\"\n})\nprices = prices.sort_values([\"symbol\", \"date\"])\n\n\nprices_daily = prices[prices[\"symbol\"].isin(vn30_symbols)]\nprices_daily[[\"date\", \"symbol\", \"adjusted_close\"]].head(3)\n\n\n\n\n\n\n\n\ndate\nsymbol\nadjusted_close\n\n\n\n\n18176\n2010-01-04\nACB\n329.408244\n\n\n18177\n2010-01-05\nACB\n329.408244\n\n\n18178\n2010-01-06\nACB\n320.258015\n\n\n\n\n\n\n\n\n16.3.1 Computing Monthly Returns\nWe aggregate daily prices to monthly frequency. Using monthly returns rather than daily returns offers several advantages for portfolio analysis: monthly returns exhibit less noise, better approximate normality, and reduce the impact of microstructure effects like bid-ask bounce.\n\nreturns_monthly = (prices_daily\n    .assign(\n        date=prices_daily[\"date\"].dt.to_period(\"M\").dt.to_timestamp(\"M\")\n    )\n    .groupby([\"symbol\", \"date\"], as_index=False)\n    .agg(adjusted_close=(\"adjusted_close\", \"last\"))\n    .sort_values([\"symbol\", \"date\"])\n    .assign(\n        ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change()\n    )\n)\n\nreturns_monthly.head(3)\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\n\n\n\n\n0\nACB\n2010-01-31\n291.975489\nNaN\n\n\n1\nACB\n2010-02-28\n303.621235\n0.039886\n\n\n2\nACB\n2010-03-31\n273.784658\n-0.098269",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "20_capm.html#the-risk-free-asset-and-the-investment-opportunity-set",
    "href": "20_capm.html#the-risk-free-asset-and-the-investment-opportunity-set",
    "title": "16  The Capital Asset Pricing Model",
    "section": "16.4 The Risk-Free Asset and the Investment Opportunity Set",
    "text": "16.4 The Risk-Free Asset and the Investment Opportunity Set\n\n16.4.1 Adding a Risk-Free Asset\nThe previous chapter on MPT considered portfolios composed entirely of risky assets, requiring that portfolio weights sum to one. The CAPM introduces a crucial new element: a risk-free asset that pays a constant interest rate \\(r_f\\) with zero volatility.\nThis seemingly simple addition fundamentally transforms the investment opportunity set. With a risk-free asset available, investors can choose to park some wealth in the safe asset and invest the remainder in risky assets. They can also borrow at the risk-free rate to leverage their risky positions.\nLet \\(\\omega \\in \\mathbb{R}^N\\) denote the portfolio weights in the \\(N\\) risky assets. Unlike before, these weights need not sum to one. The remainder, \\(1 - \\iota'\\omega\\) (where \\(\\iota\\) is a vector of ones), is invested in the risk-free asset.\n\n\n16.4.2 Portfolio Return with a Risk-Free Asset\nThe expected return on this combined portfolio is:\n\\[\n\\mu_\\omega = \\omega'\\mu + (1 - \\iota'\\omega)r_f = r_f + \\omega'(\\mu - r_f) = r_f + \\omega'\\tilde{\\mu}\n\\]\nwhere \\(\\mu\\) is the vector of expected returns on risky assets and \\(\\tilde{\\mu} = \\mu - r_f\\) denotes the vector of excess returns (returns above the risk-free rate).\nThis expression reveals an important decomposition: the portfolio’s expected return equals the risk-free rate plus a risk premium determined by the exposure to risky assets.\n\n\n16.4.3 Portfolio Variance\nSince the risk-free asset has zero volatility and zero covariance with risky assets, only the risky portion contributes to portfolio variance:\n\\[\n\\sigma_\\omega^2 = \\omega'\\Sigma\\omega\n\\]\nwhere \\(\\Sigma\\) is the variance-covariance matrix of risky asset returns. The portfolio’s volatility (standard deviation) is:\n\\[\n\\sigma_\\omega = \\sqrt{\\omega'\\Sigma\\omega}\n\\]\n\n\n16.4.4 Setting Up the Risk-Free Rate\nFor a realistic proxy of the risk-free rate, we use the Vietnam government bond yield. Government bonds of stable economies are considered “risk-free” because the government can always print money to meet its obligations (though this may cause inflation).\n\nall_dates = pd.date_range(\n    start=returns_monthly[\"date\"].min(), \n    end=returns_monthly[\"date\"].max(), \n    freq=\"ME\"\n)\n\n# Vietnam 10-Year Government Bond Yield (approximately 2.52% annualized)\nrf_annual = 0.0252\nrf_monthly_val = (1 + rf_annual)**(1/12) - 1\n\nrisk_free_monthly = pd.DataFrame({\n    \"date\": all_dates,\n    \"risk_free\": rf_monthly_val\n})\n\nrisk_free_monthly[\"date\"] = (\n    pd.to_datetime(risk_free_monthly[\"date\"])\n    .dt.to_period(\"M\")\n    .dt.to_timestamp(\"M\")\n)\n\nrisk_free_monthly.head(3)\n\n\n\n\n\n\n\n\ndate\nrisk_free\n\n\n\n\n0\n2010-01-31\n0.002076\n\n\n1\n2010-02-28\n0.002076\n\n\n2\n2010-03-31\n0.002076\n\n\n\n\n\n\n\nWe merge the risk-free rate with our returns data and compute excess returns:\n\nreturns_monthly = returns_monthly.merge(\n    risk_free_monthly[[\"date\", \"risk_free\"]], \n    on=\"date\", \n    how=\"left\"\n)\n\nrf = risk_free_monthly[\"risk_free\"].mean()\n\nreturns_monthly = (returns_monthly\n    .assign(\n        ret_excess=lambda x: x[\"ret\"] - x[\"risk_free\"]\n    )\n    .assign(\n        ret_excess=lambda x: x[\"ret_excess\"].clip(lower=-1)\n    )\n)\n\nreturns_monthly.head(3)\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\nret\nrisk_free\nret_excess\n\n\n\n\n0\nACB\n2010-01-31\n291.975489\nNaN\n0.002076\nNaN\n\n\n1\nACB\n2010-02-28\n303.621235\n0.039886\n0.002076\n0.037810\n\n\n2\nACB\n2010-03-31\n273.784658\n-0.098269\n0.002076\n-0.100345",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "20_capm.html#the-tangency-portfolio-where-everyone-invests",
    "href": "20_capm.html#the-tangency-portfolio-where-everyone-invests",
    "title": "16  The Capital Asset Pricing Model",
    "section": "16.5 The Tangency Portfolio: Where Everyone Invests",
    "text": "16.5 The Tangency Portfolio: Where Everyone Invests\n\n16.5.1 Deriving the Optimal Risky Portfolio\nWith a risk-free asset available, how should an investor allocate wealth across risky assets? Consider an investor who wants to achieve a target expected excess return \\(\\bar{\\mu}\\) with minimum variance. The optimization problem becomes:\n\\[\n\\min_\\omega \\omega'\\Sigma\\omega \\quad \\text{subject to} \\quad \\omega'\\tilde{\\mu} = \\bar{\\mu}\n\\]\nUsing the Lagrangian method:\n\\[\n\\mathcal{L}(\\omega, \\lambda) = \\omega'\\Sigma\\omega - \\lambda(\\omega'\\tilde{\\mu} - \\bar{\\mu})\n\\]\nThe first-order condition with respect to \\(\\omega\\) is:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\omega} = 2\\Sigma\\omega - \\lambda\\tilde{\\mu} = 0\n\\]\nSolving for the optimal weights:\n\\[\n\\omega^* = \\frac{\\lambda}{2}\\Sigma^{-1}\\tilde{\\mu}\n\\]\nThe constraint \\(\\omega'\\tilde{\\mu} = \\bar{\\mu}\\) determines \\(\\lambda\\):\n\\[\n\\bar{\\mu} = \\tilde{\\mu}'\\omega^* = \\frac{\\lambda}{2}\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu} \\implies \\lambda = \\frac{2\\bar{\\mu}}{\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu}}\n\\]\nSubstituting back:\n\\[\n\\omega^* = \\frac{\\bar{\\mu}}{\\tilde{\\mu}'\\Sigma^{-1}\\tilde{\\mu}}\\Sigma^{-1}\\tilde{\\mu}\n\\]\n\n\n16.5.2 The Tangency Portfolio\nNotice something remarkable: the direction of \\(\\omega^*\\) is always \\(\\Sigma^{-1}\\tilde{\\mu}\\), regardless of the target return \\(\\bar{\\mu}\\). Only the scale changes. This means all investors, regardless of their risk preferences, hold the same portfolio of risky assets. They differ only in how much they allocate to this portfolio versus the risk-free asset.\nTo obtain the portfolio of risky assets that is fully invested (weights summing to one), we normalize:\n\\[\n\\omega_{\\text{tan}} = \\frac{\\omega^*}{\\iota'\\omega^*} = \\frac{\\Sigma^{-1}(\\mu - r_f)}{\\iota'\\Sigma^{-1}(\\mu - r_f)}\n\\]\nThis is called the tangency portfolio (or maximum Sharpe ratio portfolio) because it lies at the point where the efficient frontier is tangent to the capital market line.\n\n\n16.5.3 The Sharpe Ratio and the Capital Market Line\nThe Sharpe ratio measures excess return per unit of volatility:\n\\[\n\\text{Sharpe Ratio} = \\frac{\\mu_p - r_f}{\\sigma_p}\n\\]\nThe tangency portfolio maximizes the Sharpe ratio among all possible portfolios. Any combination of the risk-free asset and the tangency portfolio lies on a straight line in mean-standard deviation space, called the Capital Market Line (CML):\n\\[\n\\mu_p = r_f + \\left(\\frac{\\mu_{\\text{tan}} - r_f}{\\sigma_{\\text{tan}}}\\right)\\sigma_p\n\\]\nThe slope of this line equals the Sharpe ratio of the tangency portfolio (i.e., the highest achievable Sharpe ratio).\n\n\n16.5.4 Computing the Tangency Portfolio\nLet’s compute the tangency portfolio for our VN30 universe:\n\nassets = (returns_monthly\n    .groupby(\"symbol\", as_index=False)\n    .agg(\n        mu=(\"ret\", \"mean\"),\n        sigma=(\"ret\", \"std\")\n    )\n)\n\nsigma = (returns_monthly\n    .pivot(index=\"date\", columns=\"symbol\", values=\"ret\")\n    .cov()\n)\n\nmu = (returns_monthly\n    .groupby(\"symbol\")[\"ret\"]\n    .mean()\n    .values\n)\n\n\n# Compute tangency portfolio weights\nw_tan = np.linalg.solve(sigma, mu - rf)\nw_tan = w_tan / np.sum(w_tan)\n\n# Portfolio performance metrics\nmu_w = w_tan.T @ mu\nsigma_w = np.sqrt(w_tan.T @ sigma @ w_tan)\n\nefficient_portfolios = pd.DataFrame([\n    {\"symbol\": r\"$\\omega_{\\mathrm{tan}}$\", \"mu\": mu_w, \"sigma\": sigma_w},\n    {\"symbol\": r\"$r_f$\", \"mu\": rf, \"sigma\": 0}\n])\n\nsharpe_ratio = (mu_w - rf) / sigma_w\n\nprint(f\"Tangency Portfolio Sharpe Ratio: {sharpe_ratio:.4f}\")\nprint(efficient_portfolios)\n\nTangency Portfolio Sharpe Ratio: -0.5552\n                    symbol        mu     sigma\n0  $\\omega_{\\mathrm{tan}}$ -0.041157  0.077866\n1                    $r_f$  0.002076  0.000000\n\n\n\n\n16.5.5 Visualizing the Efficient Frontier with a Risk-Free Asset\nFigure 16.1 shows the efficient frontier when a risk-free asset is available. The frontier is now a straight line (the Capital Market Line) connecting the risk-free asset to the tangency portfolio and extending beyond.\n\nefficient_portfolios_figure = (\n    ggplot(efficient_portfolios, aes(x=\"sigma\", y=\"mu\"))\n    + geom_point(data=assets)\n    + geom_point(data=efficient_portfolios, color=\"blue\", size=3)\n    + geom_label(\n        aes(label=\"symbol\"), \n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Volatility (Standard Deviation)\", \n        y=\"Expected Return\",\n        title=\"Efficient Frontier with Risk-Free Asset (VN30)\"\n    )\n    + geom_abline(slope=sharpe_ratio, intercept=rf, linetype=\"dotted\")\n)\n\nefficient_portfolios_figure.show()\n\n\n\n\n\n\n\nFigure 16.1: The efficient frontier with a risk-free asset becomes a straight line (the Capital Market Line) connecting the risk-free rate to the tangency portfolio. Individual assets lie below this line, demonstrating the benefits of diversification.\n\n\n\n\n\nYou may notice that estimated expected returns appear quite low, some even negative. This is not a model failure but reflects the realities of estimation:\n\nSample period matters: If the estimation window includes market downturns (such as the 2022-2023 period), realized average returns can be near zero or negative. Mean-variance optimization takes sample means literally.\nEstimation noise in emerging markets: With volatile emerging market data, sample means are dominated by noise. A few extremely bad months can push the average below the risk-free rate even if the long-run equity premium is positive.\n\nThis highlights a fundamental challenge in portfolio optimization: the inputs we observe (historical returns) are noisy estimates of the true parameters we need (expected future returns).",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "20_capm.html#the-capm-equation-risk-and-expected-return",
    "href": "20_capm.html#the-capm-equation-risk-and-expected-return",
    "title": "16  The Capital Asset Pricing Model",
    "section": "16.6 The CAPM Equation: Risk and Expected Return",
    "text": "16.6 The CAPM Equation: Risk and Expected Return\n\n16.6.1 From Individual Optimization to Market Equilibrium\nSo far, we’ve focused on one investor’s optimization problem. The CAPM’s power comes from considering what happens when all investors optimize simultaneously.\nIf all investors follow mean-variance optimization, they all hold some combination of the risk-free asset and the tangency portfolio. The only difference between investors is their risk tolerance. More risk-averse investors hold more of the risk-free asset, while risk-tolerant investors may even borrow at the risk-free rate to leverage their position in the tangency portfolio.\n\n\n16.6.2 The Market Portfolio\nIn equilibrium, the total demand for each risky asset must equal its supply. Since all investors hold the same portfolio of risky assets (the tangency portfolio), the equilibrium portfolio weights must equal the market capitalization weights. The tangency portfolio is the market portfolio.\nThis insight has enormous practical implications: instead of estimating expected returns and covariances to compute the tangency portfolio, we can simply use the market portfolio (approximated by a broad market index) as a proxy.\n\n\n16.6.3 Deriving the CAPM Equation\nFrom the first-order conditions of the optimization problem, we derived that:\n\\[\n\\tilde{\\mu} = \\frac{2}{\\lambda}\\Sigma\\omega^*\n\\]\nSince \\(\\omega^*\\) is proportional to \\(\\omega_{\\text{tan}}\\), and in equilibrium \\(\\omega_{\\text{tan}}\\) equals the market portfolio \\(\\omega_m\\):\n\\[\n\\tilde{\\mu} = c \\cdot \\Sigma\\omega_m\n\\]\nfor some constant \\(c\\). The \\(i\\)-th element of \\(\\Sigma\\omega_m\\) is:\n\\[\n\\sum_{j=1}^N \\sigma_{ij}\\omega_{m,j} = \\text{Cov}(r_i, r_m)\n\\]\nwhere \\(r_m = \\sum_j \\omega_{m,j} r_j\\) is the return on the market portfolio.\nFor the market portfolio itself:\n\\[\n\\tilde{\\mu}_m = c \\cdot \\text{Var}(r_m) = c \\cdot \\sigma_m^2\n\\]\nTherefore \\(c = \\tilde{\\mu}_m / \\sigma_m^2\\), and for any asset \\(i\\):\n\\[\n\\tilde{\\mu}_i = \\frac{\\tilde{\\mu}_m}{\\sigma_m^2} \\text{Cov}(r_i, r_m) = \\beta_i \\tilde{\\mu}_m\n\\]\nwhere:\n\\[\n\\beta_i = \\frac{\\text{Cov}(r_i, r_m)}{\\text{Var}(r_m)}\n\\]\nThis is the famous CAPM equation:\n\\[\nE(r_i) - r_f = \\beta_i [E(r_m) - r_f]\n\\]\n\n\n16.6.4 Interpreting Beta\nBeta (\\(\\beta_i\\)) measures an asset’s systematic risk (i.e., its sensitivity to market movements). The interpretation is straightforward:\n\n\\(\\beta = 1\\): The asset moves one-for-one with the market (average systematic risk)\n\\(\\beta &gt; 1\\): The asset amplifies market movements (aggressive, high systematic risk)\n\\(\\beta &lt; 1\\): The asset dampens market movements (defensive, low systematic risk)\n\\(\\beta &lt; 0\\): The asset moves opposite to the market (provides insurance)\n\nThe CAPM says that expected excess return is proportional to beta, not to total volatility. This explains why:\n\nAn asset with zero beta earns only the risk-free rate (i.e., its risk is entirely idiosyncratic).\nAn asset with beta of 1 earns the market risk premium\nA negative-beta asset earns less than the risk-free rate (i.e., investors pay for its insurance properties).",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "20_capm.html#the-security-market-line",
    "href": "20_capm.html#the-security-market-line",
    "title": "16  The Capital Asset Pricing Model",
    "section": "16.7 The Security Market Line",
    "text": "16.7 The Security Market Line\nThe CAPM predicts a linear relationship between beta and expected return. This relationship is called the Security Market Line (SML):\n\\[\nE(r_i) = r_f + \\beta_i [E(r_m) - r_f]\n\\]\nUnlike the Capital Market Line (which plots expected return against total risk), the Security Market Line plots expected return against systematic risk (beta).\n\nbetas = (sigma @ w_tan) / (w_tan.T @ sigma @ w_tan)\nassets[\"beta\"] = betas.values\n\nprice_of_risk = float(w_tan.T @ mu - rf)\n\nassets_figure = (\n    ggplot(assets, aes(x=\"beta\", y=\"mu\"))\n    + geom_point()\n    + geom_abline(intercept=rf, slope=price_of_risk)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Beta (Systematic Risk)\", \n        y=\"Expected Return\",\n        title=\"Security Market Line\"\n    )\n    + annotate(\"text\", x=0.05, y=rf + 0.001, label=\"Risk-free rate\")\n)\n\nassets_figure.show()\n\n\n\n\n\n\n\n\nYou may observe that the estimated SML has a negative slope, which seems to contradict CAPM’s prediction. This reflects a negative estimated market risk premium in our sample period (i.e., the market portfolio earned less than the risk-free rate).\nWhen the market risk premium is negative, CAPM predicts that high-beta stocks should have lower expected returns than low-beta stocks. This is not a model failure, the model is behaving consistently. Rather, it reflects an unusual (but not impossible) sample period where risky assets underperformed safe assets.\nThis observation highlights an important distinction: CAPM describes expected returns in equilibrium, but realized returns over any particular period may differ substantially from expectations due to shocks and surprises.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "20_capm.html#empirical-estimation-of-capm-parameters",
    "href": "20_capm.html#empirical-estimation-of-capm-parameters",
    "title": "16  The Capital Asset Pricing Model",
    "section": "16.8 Empirical Estimation of CAPM Parameters",
    "text": "16.8 Empirical Estimation of CAPM Parameters\n\n16.8.1 The Regression Framework\nIn practice, we estimate CAPM parameters using time-series regression. The model implies:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i(r_{m,t} - r_{f,t}) + \\varepsilon_{i,t}\n\\]\nwhere:\n\n\\(r_{i,t}\\): Return on asset \\(i\\) at time \\(t\\)\n\\(r_{f,t}\\): Risk-free rate at time \\(t\\)\n\\(r_{m,t}\\): Market return at time \\(t\\)\n\\(\\alpha_i\\): Intercept (should be zero if CAPM holds)\n\\(\\beta_i\\): Systematic risk (slope coefficient)\n\\(\\varepsilon_{i,t}\\): Idiosyncratic shock (residual)\n\n\n\n16.8.2 Alpha: Risk-Adjusted Performance\nThe intercept \\(\\alpha_i\\) measures risk-adjusted performance. If CAPM holds perfectly, alpha should be zero for all assets (i.e., any excess return is exactly compensated by systematic risk).\n\n\\(\\alpha &gt; 0\\): The asset outperformed its CAPM-predicted return (positive abnormal return)\n\\(\\alpha &lt; 0\\): The asset underperformed its CAPM-predicted return (negative abnormal return)\n\nPositive alpha is the holy grail of active management: earning returns beyond what systematic risk exposure would justify.\n\n\n16.8.3 Loading Factor Data\nWe use Fama-French market excess returns as our market portfolio proxy. These data provide a widely accepted benchmark that is already adjusted for the risk-free rate:\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors = pd.read_sql_query(\n    sql=\"SELECT date, smb, hml, rmw, cma, mkt_excess FROM factors_ff5_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nfactors.head(3)\n\n\n\n\n\n\n\n\ndate\nsmb\nhml\nrmw\ncma\nmkt_excess\n\n\n\n\n0\n2011-07-31\n-0.015907\n-0.002812\n0.060525\n0.045291\n-0.078748\n\n\n1\n2011-08-31\n-0.061842\n0.006189\n-0.022700\n-0.023177\n0.029906\n\n\n2\n2011-09-30\n0.014387\n0.024301\n-0.006005\n0.003588\n-0.002173\n\n\n\n\n\n\n\n\n\n16.8.4 Running the Regressions\nWe estimate CAPM regressions for each stock in our universe:\n\nimport statsmodels.formula.api as smf\n\nreturns_excess_monthly = (returns_monthly\n    .merge(factors, on=\"date\", how=\"left\")\n    .assign(ret_excess=lambda x: x[\"ret\"] - x[\"risk_free\"])\n)\n\n\ndef estimate_capm(data):\n    model = smf.ols(\"ret_excess ~ mkt_excess\", data=data).fit()\n    result = pd.DataFrame({\n        \"coefficient\": [\"alpha\", \"beta\"],\n        \"estimate\": model.params.values,\n        \"t_statistic\": model.tvalues.values\n    })\n    return result\n\n\ncapm_results = (returns_excess_monthly\n    .groupby(\"symbol\", group_keys=True)\n    .apply(estimate_capm)\n    .reset_index()\n)\n\ncapm_results.head(4)\n\n\n\n\n\n\n\n\nsymbol\nlevel_1\ncoefficient\nestimate\nt_statistic\n\n\n\n\n0\nACB\n0\nalpha\n-0.000826\n-0.105475\n\n\n1\nACB\n1\nbeta\n0.604653\n4.575750\n\n\n2\nBCM\n0\nalpha\n0.035093\n2.368668\n\n\n3\nBCM\n1\nbeta\n1.032462\n4.581461\n\n\n\n\n\n\n\n\n\n16.8.5 Visualizing Alpha Estimates\nFigure 16.2 shows the estimated alphas across our VN30 sample. Statistical significance (at the 95% level) is indicated by color.\n\nalphas = (capm_results\n    .query(\"coefficient == 'alpha'\")\n    .assign(is_significant=lambda x: np.abs(x[\"t_statistic\"]) &gt;= 1.96)\n)\n\nalphas[\"symbol\"] = pd.Categorical(\n    alphas[\"symbol\"],\n    categories=alphas.sort_values(\"estimate\")[\"symbol\"],\n    ordered=True\n)\n\nalphas_figure = (\n    ggplot(alphas, aes(y=\"estimate\", x=\"symbol\", fill=\"is_significant\"))\n    + geom_col()\n    + scale_y_continuous(labels=percent_format())\n    + coord_flip()\n    + labs(\n        x=\"\", \n        y=\"Estimated Alpha (Monthly)\", \n        fill=\"Significant at 95%?\",\n        title=\"Estimated CAPM Alphas for VN30 Index Constituents\"\n    )\n)\n\nalphas_figure.show()\n\n\n\n\n\n\n\nFigure 16.2: Estimated CAPM alphas for VN30 index constituents. Color indicates statistical significance at the 95% confidence level. Most alphas are statistically indistinguishable from zero, consistent with CAPM predictions.\n\n\n\n\n\nThe distribution of alphas provides evidence on CAPM’s empirical validity. If the model holds, we expect:\n\nMost alphas close to zero\nFew statistically significant alphas\nRoughly equal numbers of positive and negative alphas\n\nSystematic patterns in alphas, such as consistently positive alphas for certain types of stocks, would suggest the CAPM is incomplete and that additional risk factors may be needed.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "20_capm.html#limitations-and-extensions",
    "href": "20_capm.html#limitations-and-extensions",
    "title": "16  The Capital Asset Pricing Model",
    "section": "16.9 Limitations and Extensions",
    "text": "16.9 Limitations and Extensions\n\n16.9.1 The Market Portfolio Problem\nA fundamental challenge in testing the CAPM is identifying the market portfolio. The theory requires a portfolio that includes all investable assets, not just stocks, but also bonds, real estate, private businesses, human capital, and even intangible assets. In practice, we use proxies like broad market indices (VNI, S&P 500), but these capture only publicly traded equities.\nThis limitation is profound. As Richard Roll famously argued, the CAPM is essentially untestable because the true market portfolio is unobservable. Any test of the CAPM is simultaneously a test of whether our proxy adequately represents the market.\n\n\n16.9.2 Time-Varying Betas\nThe CAPM assumes that betas are constant over time, but this assumption rarely holds in practice. Companies undergo changes that affect their market sensitivity:\n\nCapital structure changes: Increasing leverage raises beta\nBusiness model evolution: Diversification into new industries can alter systematic risk\nMarket conditions: Betas often increase during market stress\n\nConditional CAPM models (Jagannathan and Wang 1996) address this by allowing risk premiums and betas to vary with the business cycle.\n\n\n16.9.3 Empirical Anomalies\nDecades of empirical research have documented patterns in stock returns that CAPM cannot explain:\n\nSize effect: Small-cap stocks tend to outperform large-cap stocks, even after adjusting for beta\nValue effect: Stocks with high book-to-market ratios outperform growth stocks\nMomentum: Stocks that performed well recently tend to continue performing well\n\nThese anomalies suggest that systematic risk has multiple dimensions beyond market exposure.\n\n\n16.9.4 Multifactor Extensions\nThe limitations of CAPM have led to increasingly sophisticated asset pricing models. The Fama-French three-factor model (Fama and French 1992) adds two factors to capture size and value effects:\n\nSMB (Small Minus Big): Returns on small stocks minus large stocks\nHML (High Minus Low): Returns on value stocks minus growth stocks\n\nThe Fama-French five-factor model (Fama and French 2015) adds two more dimensions:\n\nRMW (Robust Minus Weak): Returns on profitable firms minus unprofitable firms\n\nCMA (Conservative Minus Aggressive): Returns on conservative investors minus aggressive investors\n\nThe Carhart four-factor model (Carhart 1997) adds momentum to the three-factor framework.\nOther theoretical developments include:\n\nConsumption CAPM: Links asset prices to macroeconomic consumption risk\nQ-factor model (Hou, Xue, and Zhang 2014): Derives factors from investment-based asset pricing theory\nArbitrage Pricing Theory: Allows for multiple sources of systematic risk without specifying their identity\n\nDespite its limitations, the CAPM remains valuable as a conceptual benchmark. Its core insight (i.e., only systematic, undiversifiable risk commands a premium) continues to inform how we think about risk and return.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "20_capm.html#key-takeaways",
    "href": "20_capm.html#key-takeaways",
    "title": "16  The Capital Asset Pricing Model",
    "section": "16.10 Key Takeaways",
    "text": "16.10 Key Takeaways\nThis chapter introduced the Capital Asset Pricing Model and its implications for understanding the relationship between risk and expected return. The main insights are:\n\nNot all risk is rewarded: The CAPM distinguishes between systematic risk (which cannot be diversified away and commands a premium) and idiosyncratic risk (which can be eliminated through diversification and earns no premium).\nThe tangency portfolio is universal: When a risk-free asset exists, all mean-variance investors hold the same portfolio of risky assets (i.e., the tangency or maximum Sharpe ratio portfolio). They differ only in how much they allocate to this portfolio versus the risk-free asset.\nIn equilibrium, the tangency portfolio is the market portfolio: Since all investors hold the same risky portfolio, and total demand must equal supply, the equilibrium portfolio weights are market capitalization weights.\nExpected returns depend on beta: The CAPM equation states that expected excess return equals beta times the market risk premium. Beta measures covariance with the market portfolio, normalized by market variance.\nAlpha measures risk-adjusted performance: Positive alpha indicates returns above what systematic risk would justify; negative alpha indicates underperformance.\nEmpirical challenges exist: Testing the CAPM requires identifying the market portfolio, which is unobservable in practice. Documented anomalies (size, value, momentum) suggest additional risk factors beyond market exposure.\nExtensions abound: Multifactor models like Fama-French extend the CAPM framework by adding factors that capture dimensions of systematic risk the market factor misses.\n\nThe CAPM’s elegance lies in its simplicity: a single factor (i.e., exposure to the market) should explain expected returns in equilibrium. While reality is more complex, this framework provides the foundation for all modern asset pricing theory.\n\n\n\n\n\n\nCarhart, Mark M. 1997. “On persistence in mutual fund performance.” The Journal of Finance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock returns.” The Journal of Finance 47 (2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2014. “Digesting anomalies: An investment approach.” Review of Financial Studies 28 (3): 650–705. https://doi.org/10.1093/rfs/hhu068.\n\n\nJagannathan, Ravi, and Zhenyu Wang. 1996. “The conditional CAPM and the cross-section of expected returns.” The Journal of Finance 51 (1): 3–53. https://doi.org/10.2307/2329301.\n\n\nLintner, John. 1965. “Security prices, risk, and maximal gains from diversification.” The Journal of Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital asset market.” Econometrica 34 (4): 768–83. https://doi.org/10.2307/1910098.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Capital Asset Pricing Model</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html",
    "href": "21_fama_french.html",
    "title": "17  Fama-French Factors",
    "section": "",
    "text": "17.1 Theoretical Background\nThis chapter provides a replication of the Fama-French factor portfolios for the Vietnamese stock market. The Fama-French factor models represent a cornerstone of empirical asset pricing, originating from the seminal work of Fama and French (1992) and later extended in Fama and French (2015). These models have transformed how academics and practitioners understand the cross-section of expected stock returns, moving beyond the single-factor Capital Asset Pricing Model to incorporate multiple sources of systematic risk.\nWe construct both the three-factor and five-factor models at monthly and daily frequencies. The monthly factors serve as the foundation for most asset pricing tests and portfolio analyses, while the daily factors enable higher-frequency applications including short-horizon event studies, market microstructure research, and daily beta estimation. By constructing factors at both frequencies, we create a complete toolkit for empirical finance research in the Vietnamese market.\nThe chapter proceeds as follows. We first discuss the theoretical motivation for each factor and the economic intuition behind the Fama-French methodology. We then prepare the necessary data, merging stock returns with accounting characteristics. Next, we implement the portfolio sorting procedures that form the basis of factor construction, carefully following the original Fama-French protocols while adapting them for Vietnamese market characteristics. We construct the three-factor model (market, size, and value) before extending to the five-factor model (adding profitability and investment). Finally, we construct daily factors and validate our replicated factors through various diagnostic checks.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html#theoretical-background",
    "href": "21_fama_french.html#theoretical-background",
    "title": "17  Fama-French Factors",
    "section": "",
    "text": "17.1.1 The Evolution from CAPM to Multi-Factor Models\nThe Capital Asset Pricing Model of Sharpe (1964) posits that a single factor—the market portfolio—should explain all cross-sectional variation in expected returns. However, decades of empirical research have documented persistent patterns that CAPM cannot explain. Fama and French (1992) demonstrated that two firm characteristics—size and book-to-market ratio—capture substantial variation in average returns that the market beta leaves unexplained.\nSmall firms tend to earn higher returns than large firms, a pattern known as the size effect. Similarly, firms with high book-to-market ratios (value stocks) tend to outperform firms with low book-to-market ratios (growth stocks), known as the value premium. The three-factor model formalizes these observations by constructing tradeable factor portfolios:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i^{MKT}(r_{m,t} - r_{f,t}) + \\beta_i^{SMB} \\cdot SMB_t + \\beta_i^{HML} \\cdot HML_t + \\varepsilon_{i,t}\n\\tag{17.1}\\]\nwhere:\n\n\\(r_{i,t} - r_{f,t}\\) is the excess return on asset \\(i\\)\n\\(r_{m,t} - r_{f,t}\\) is the market excess return\n\\(SMB_t\\) (Small Minus Big) is the size factor\n\\(HML_t\\) (High Minus Low) is the value factor\n\n\n\n17.1.2 The Five-Factor Extension\nFama and French (2015) extended the model to include two additional factors motivated by the dividend discount model. Firms with higher profitability should have higher expected returns (all else equal), and firms with aggressive investment policies should have lower expected returns:\n\\[\nr_{i,t} - r_{f,t} = \\alpha_i + \\beta_i^{MKT}MKT_t + \\beta_i^{SMB}SMB_t + \\beta_i^{HML}HML_t + \\beta_i^{RMW}RMW_t + \\beta_i^{CMA}CMA_t + \\varepsilon_{i,t}\n\\tag{17.2}\\]\nwhere:\n\n\\(RMW_t\\) (Robust Minus Weak) is the profitability factor\n\\(CMA_t\\) (Conservative Minus Aggressive) is the investment factor\n\n\n\n17.1.3 Factor Construction Methodology\nThe Fama-French methodology constructs factors through double-sorted portfolios:\n\nSize sorts: Stocks are divided into Small and Big groups based on median market capitalization.\nCharacteristic sorts: Within each size group, stocks are sorted into terciles based on book-to-market (for HML), operating profitability (for RMW), or investment (for CMA).\nFactor returns: Factors are computed as the difference between average returns of portfolios with high versus low characteristic values, averaging across size groups to neutralize size effects.\nTiming: Portfolios are formed at the end of June each year using accounting data from the prior fiscal year, ensuring all information was publicly available at formation.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html#setting-up-the-environment",
    "href": "21_fama_french.html#setting-up-the-environment",
    "title": "17  Fama-French Factors",
    "section": "17.2 Setting Up the Environment",
    "text": "17.2 Setting Up the Environment\nWe load the required Python packages for data manipulation, statistical analysis, and visualization.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nfrom scipy.stats.mstats import winsorize\nimport matplotlib.pyplot as plt\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\n\nWe connect to our SQLite database containing the processed Vietnamese financial data.\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html#data-preparation",
    "href": "21_fama_french.html#data-preparation",
    "title": "17  Fama-French Factors",
    "section": "17.3 Data Preparation",
    "text": "17.3 Data Preparation\n\n17.3.1 Loading Stock Returns\nWe load the monthly stock returns data, which includes excess returns, market capitalization, and the risk-free rate. These variables are essential for computing value-weighted portfolio returns and factor premiums.\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprint(f\"Monthly returns: {len(prices_monthly):,} observations\")\nprint(f\"Unique stocks: {prices_monthly['symbol'].nunique():,}\")\nprint(f\"Date range: {prices_monthly['date'].min():%Y-%m} to {prices_monthly['date'].max():%Y-%m}\")\n\nMonthly returns: 165,499 observations\nUnique stocks: 1,457\nDate range: 2010-02 to 2023-12\n\n\n\n\n17.3.2 Loading Company Fundamentals\nWe load the company fundamentals data containing book equity, operating profitability, and investment—the characteristics needed for constructing the Fama-French factors.\n\ncomp_vn = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, datadate, be, op, inv\n        FROM comp_vn\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n).dropna()\n\nprint(f\"Fundamentals: {len(comp_vn):,} firm-year observations\")\nprint(f\"Unique firms: {comp_vn['symbol'].nunique():,}\")\n\nFundamentals: 18,108 firm-year observations\nUnique firms: 1,496\n\n\n\n\n17.3.3 Constructing Sorting Variables\nFollowing Fama-French conventions, we construct the sorting variables with careful attention to timing. The key principles are:\n\nSize (June Market Cap): We use market capitalization at the end of June of year \\(t\\) to sort stocks into size groups. This ensures we capture the firm’s size at the moment of portfolio formation.\nBook-to-Market Ratio: We use book equity from fiscal year \\(t-1\\) divided by market equity at the end of December \\(t-1\\). This creates a six-month gap between the accounting data and portfolio formation, ensuring the information was publicly available.\nPortfolio Formation Date: Portfolios are formed on July 1st and held for twelve months until the following June.\n\n\ndef construct_sorting_variables(prices_monthly, comp_vn):\n    \"\"\"\n    Construct sorting variables following Fama-French methodology.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns with market cap\n    comp_vn : pd.DataFrame\n        Company fundamentals with book equity, profitability, investment\n        \n    Returns\n    -------\n    pd.DataFrame\n        Sorting variables aligned with July 1st formation dates\n    \"\"\"\n    \n    # 1. Size: June market capitalization\n    # Portfolio formation is July 1st, so we use June market cap\n    size = (prices_monthly\n        .query(\"date.dt.month == 6\")\n        .assign(\n            sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(1)\n        )\n        [[\"symbol\", \"sorting_date\", \"mktcap\"]]\n        .rename(columns={\"mktcap\": \"size\"})\n    )\n    \n    print(f\"Size observations: {len(size):,}\")\n    \n    # 2. Market Equity: December market cap for B/M calculation\n    # December t-1 market cap is used with fiscal year t-1 book equity\n    # This is then used for July t portfolio formation\n    market_equity = (prices_monthly\n        .query(\"date.dt.month == 12\")\n        .assign(\n            # December year t-1 maps to July year t formation\n            sorting_date=lambda x: x[\"date\"] + pd.offsets.MonthBegin(7)\n        )\n        [[\"symbol\", \"sorting_date\", \"mktcap\"]]\n        .rename(columns={\"mktcap\": \"me\"})\n    )\n    \n    print(f\"Market equity observations: {len(market_equity):,}\")\n    \n    # 3. Book-to-Market and other characteristics\n    # Fiscal year t-1 data is used for July t portfolio formation\n    book_to_market = (comp_vn\n        .assign(\n            # Fiscal year-end + 6 months = July formation\n            sorting_date=lambda x: pd.to_datetime(\n                (x[\"datadate\"].dt.year + 1).astype(str) + \"-07-01\"\n            )\n        )\n        .merge(market_equity, on=[\"symbol\", \"sorting_date\"], how=\"inner\")\n        .assign(\n            # Scale book equity to match market equity units\n            # BE is in VND, ME is in millions VND\n            bm=lambda x: x[\"be\"] / (x[\"me\"] * 1e9)\n        )\n        [[\"symbol\", \"sorting_date\", \"me\", \"bm\", \"op\", \"inv\"]]\n    )\n    \n    print(f\"Book-to-market observations: {len(book_to_market):,}\")\n    \n    # 4. Merge size with characteristics\n    sorting_variables = (size\n        .merge(book_to_market, on=[\"symbol\", \"sorting_date\"], how=\"inner\")\n        .dropna()\n        .drop_duplicates(subset=[\"symbol\", \"sorting_date\"])\n    )\n    \n    return sorting_variables\n\nsorting_variables = construct_sorting_variables(prices_monthly, comp_vn)\n\nprint(f\"\\nFinal sorting variables: {len(sorting_variables):,} stock-years\")\nprint(f\"Sorting date range: {sorting_variables['sorting_date'].min():%Y-%m} to {sorting_variables['sorting_date'].max():%Y-%m}\")\n\nSize observations: 13,756\nMarket equity observations: 14,286\nBook-to-market observations: 13,389\n\nFinal sorting variables: 12,046 stock-years\nSorting date range: 2011-07 to 2023-07\n\n\n\n\n17.3.4 Validating Sorting Variables\nBefore proceeding, we validate that our sorting variables have reasonable distributions. The book-to-market ratio should center around 1.0 for a typical market, though emerging markets may differ.\n\nprint(\"Sorting Variable Summary Statistics:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\n# Check for extreme values that might indicate data issues\nprint(f\"\\nB/M Median: {sorting_variables['bm'].median():.4f}\")\nprint(f\"B/M 1st percentile: {sorting_variables['bm'].quantile(0.01):.4f}\")\nprint(f\"B/M 99th percentile: {sorting_variables['bm'].quantile(0.99):.4f}\")\n\nSorting Variable Summary Statistics:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.7033      0.1852      0.1322\nstd     14680.4225      3.8683      0.2782      2.5410\nmin         0.4864      0.0014     -0.7529     -0.9569\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817    272.1893      1.4256    261.3355\n\nB/M Median: 1.1849\nB/M 1st percentile: 0.1710\nB/M 99th percentile: 8.0262\n\n\n\n\n17.3.5 Handling Outliers\nExtreme values in sorting characteristics can distort portfolio assignments and factor returns. We apply winsorization to limit the influence of outliers while preserving the general ranking of stocks.\n\n# Check BEFORE winsorization\nprint(\"BEFORE Winsorization:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\n# Apply winsorization\ndef winsorize_characteristics(df, columns, limits=(0.01, 0.99)):\n    \"\"\"\n    Apply winsorization using pandas clip.\n    \"\"\"\n    df = df.copy()\n    for col in columns:\n        if col in df.columns:\n            lower = df[col].quantile(limits[0])\n            upper = df[col].quantile(limits[1])\n            df[col] = df[col].clip(lower=lower, upper=upper)\n            print(f\"  {col}: clipped to [{lower:.4f}, {upper:.4f}]\")\n    return df\n\nsorting_variables = winsorize_characteristics(\n    sorting_variables,\n    columns=[\"bm\", \"op\", \"inv\"],  # Don't winsorize size\n    limits=(0.01, 0.99)\n)\n\n# Check AFTER winsorization\nprint(\"\\nAFTER Winsorization:\")\nprint(sorting_variables[[\"size\", \"bm\", \"op\", \"inv\"]].describe().round(4))\n\nBEFORE Winsorization:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.7033      0.1852      0.1322\nstd     14680.4225      3.8683      0.2782      2.5410\nmin         0.4864      0.0014     -0.7529     -0.9569\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817    272.1893      1.4256    261.3355\n  bm: clipped to [0.1710, 8.0262]\n  op: clipped to [-0.7319, 1.2192]\n  inv: clipped to [-0.3990, 1.5195]\n\nAFTER Winsorization:\n              size          bm          op         inv\ncount   12046.0000  12046.0000  12046.0000  12046.0000\nmean     2225.5648      1.5544      0.1837      0.0894\nstd     14680.4225      1.2843      0.2705      0.2721\nmin         0.4864      0.1710     -0.7319     -0.3990\n25%        62.7556      0.7595      0.0309     -0.0495\n50%       182.6410      1.1849      0.1367      0.0342\n75%       641.8896      1.8853      0.2952      0.1582\nmax    426020.9817      8.0262      1.2192      1.5195",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html#portfolio-assignment-functions",
    "href": "21_fama_french.html#portfolio-assignment-functions",
    "title": "17  Fama-French Factors",
    "section": "17.4 Portfolio Assignment Functions",
    "text": "17.4 Portfolio Assignment Functions\n\n17.4.1 The Portfolio Assignment Function\nWe create a flexible function for assigning stocks to portfolios based on quantile breakpoints. This function handles both independent sorts (where breakpoints are computed across all stocks) and dependent sorts (where breakpoints are computed within subgroups).\n\ndef assign_portfolio(data, sorting_variable, percentiles):\n    \"\"\"Assign portfolios to a bin according to a sorting variable.\"\"\"\n    \n    # Get the values\n    values = data[sorting_variable].dropna()\n    \n    if len(values) == 0:\n        return pd.Series([np.nan] * len(data), index=data.index)\n    \n    # Calculate breakpoints\n    breakpoints = values.quantile(percentiles, interpolation=\"linear\")\n    \n    # Handle duplicate breakpoints by using unique values\n    unique_breakpoints = np.unique(breakpoints)\n    \n    # If all values are the same, assign all to portfolio 1\n    if len(unique_breakpoints) &lt;= 1:\n        return pd.Series([1] * len(data), index=data.index)\n    \n    # Set boundaries to -inf and +inf\n    unique_breakpoints.iloc[0] = -np.inf\n    unique_breakpoints.iloc[unique_breakpoints.size-1] = np.inf\n    \n    # Assign to bins\n    assigned = pd.cut(\n        data[sorting_variable],\n        bins=unique_breakpoints,\n        labels=pd.Series(range(1, breakpoints.size)),\n        include_lowest=True,\n        right=False\n    )\n    \n    return assigned\n\n\n# Check the distribution of characteristics BEFORE portfolio assignment\nprint(\"Operating Profitability Distribution:\")\nprint(sorting_variables[\"op\"].describe())\nprint(f\"\\nUnique OP values: {sorting_variables['op'].nunique()}\")\n\nprint(\"\\nInvestment Distribution:\")\nprint(sorting_variables[\"inv\"].describe())\nprint(f\"\\nUnique INV values: {sorting_variables['inv'].nunique()}\")\n\n# Check breakpoints for a specific date\ntest_date = sorting_variables[\"sorting_date\"].iloc[0]\ntest_data = sorting_variables.query(\"sorting_date == @test_date\")\n\nprint(f\"\\nBreakpoints for {test_date}:\")\nprint(f\"OP 30th percentile: {test_data['op'].quantile(0.3):.4f}\")\nprint(f\"OP 70th percentile: {test_data['op'].quantile(0.7):.4f}\")\nprint(f\"INV 30th percentile: {test_data['inv'].quantile(0.3):.4f}\")\nprint(f\"INV 70th percentile: {test_data['inv'].quantile(0.7):.4f}\")\n\nOperating Profitability Distribution:\ncount    12046.000000\nmean         0.183738\nstd          0.270509\nmin         -0.731888\n25%          0.030913\n50%          0.136675\n75%          0.295185\nmax          1.219223\nName: op, dtype: float64\n\nUnique OP values: 11804\n\nInvestment Distribution:\ncount    12046.000000\nmean         0.089388\nstd          0.272147\nmin         -0.399042\n25%         -0.049497\n50%          0.034157\n75%          0.158155\nmax          1.519474\nName: inv, dtype: float64\n\nUnique INV values: 11805\n\nBreakpoints for 2019-07-01 00:00:00:\nOP 30th percentile: 0.0541\nOP 70th percentile: 0.2566\nINV 30th percentile: -0.0343\nINV 70th percentile: 0.1116\n\n\n\n\n17.4.2 Assigning Portfolios for Three-Factor Model\nFor the three-factor model, we perform independent double sorts on size and book-to-market. Size is split at the median (2 groups), and book-to-market is split at the 30th and 70th percentiles (3 groups), creating 6 portfolios.\n\ndef assign_ff3_portfolios(sorting_variables):\n    \"\"\"\n    Assign portfolios for Fama-French three-factor model.\n    Independent 2x3 sort on size and book-to-market.\n    \"\"\"\n    df = sorting_variables.copy()\n    \n    # Independent size sort (median split)\n    df[\"portfolio_size\"] = df.groupby(\"sorting_date\")[\"size\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.5, 1], labels=[1, 2], duplicates='drop')\n    )\n    \n    # Independent B/M sort (30/70 split)\n    df[\"portfolio_bm\"] = df.groupby(\"sorting_date\")[\"bm\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    return df\n\n# Assign portfolios\nportfolios_ff3 = assign_ff3_portfolios(sorting_variables)\n\n# Validate\nprint(\"FF3 Book-to-Market by Portfolio (should be INCREASING):\")\nprint(portfolios_ff3.groupby(\"portfolio_bm\", observed=True)[\"bm\"].median().round(4))\n\n\nprint(\"Three-Factor Portfolio Assignments:\")\nprint(portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]].head(10))\n\nFF3 Book-to-Market by Portfolio (should be INCREASING):\nportfolio_bm\n1    0.5836\n2    1.1891\n3    2.4552\nName: bm, dtype: float64\nThree-Factor Portfolio Assignments:\n  symbol sorting_date portfolio_size portfolio_bm\n0    A32   2019-07-01              1            2\n1    A32   2020-07-01              1            2\n2    A32   2021-07-01              1            2\n3    A32   2022-07-01              1            3\n4    A32   2023-07-01              1            2\n5    AAA   2011-07-01              2            2\n6    AAA   2012-07-01              2            3\n7    AAA   2013-07-01              2            3\n8    AAA   2014-07-01              2            2\n9    AAA   2015-07-01              2            3\n\n\n\n\n17.4.3 Validating Portfolio Assignments\nWe verify that the portfolio assignments create the expected 2×3 grid with reasonable stock counts in each cell.\n\n# Check portfolio distribution for most recent year\nlatest_date = portfolios_ff3[\"sorting_date\"].max()\n\nportfolio_counts = (portfolios_ff3\n    .query(\"sorting_date == @latest_date\")\n    .groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)\n    .size()\n    .unstack(fill_value=0)\n)\n\nprint(f\"Portfolio Counts for {latest_date:%Y-%m}:\")\nprint(portfolio_counts)\n\n# Verify characteristic monotonicity\nprint(\"\\nBook-to-Market by Portfolio (should be increasing):\")\nprint(portfolios_ff3.groupby(\"portfolio_bm\", observed=True)[\"bm\"].median().round(4))\n\nPortfolio Counts for 2023-07:\nportfolio_bm      1    2    3\nportfolio_size               \n1               113  271  263\n2               275  246  125\n\nBook-to-Market by Portfolio (should be increasing):\nportfolio_bm\n1    0.5836\n2    1.1891\n3    2.4552\nName: bm, dtype: float64\n\n\nWe verify that for a single stock, the portfolio assignment remains constant between July of one year and June of the next.\n\n# Trace a single symbol (e.g., 'A32') across a formation window\npersistence_check = (portfolios_ff3\n    .query(\"symbol == 'A32' & sorting_date &gt;= '2022-01-01' & sorting_date &lt;= '2023-12-31'\")\n    .sort_values(\"sorting_date\")\n    [['symbol', 'sorting_date', 'portfolio_size', 'portfolio_bm']]\n)\nprint(\"\\nTemporal Persistence Check (Symbol A32):\")\nprint(persistence_check.head(15))\n\n\nTemporal Persistence Check (Symbol A32):\n  symbol sorting_date portfolio_size portfolio_bm\n3    A32   2022-07-01              1            3\n4    A32   2023-07-01              1            2",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html#fama-french-three-factor-model-monthly",
    "href": "21_fama_french.html#fama-french-three-factor-model-monthly",
    "title": "17  Fama-French Factors",
    "section": "17.5 Fama-French Three-Factor Model (Monthly)",
    "text": "17.5 Fama-French Three-Factor Model (Monthly)\n\n17.5.1 Merging Portfolios with Returns\nWe merge the portfolio assignments with monthly returns. The key insight is that portfolios formed in July of year \\(t\\) are held through June of year \\(t+1\\). We implement this by computing a sorting_date for each monthly return observation.\n\ndef merge_portfolios_with_returns(prices_monthly, portfolio_assignments):\n    \"\"\"\n    Merge portfolio assignments with monthly returns.\n    \n    Portfolios formed in July t are held through June t+1.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns\n    portfolio_assignments : pd.DataFrame\n        Portfolio assignments with sorting_date\n        \n    Returns\n    -------\n    pd.DataFrame\n        Returns merged with portfolio assignments\n    \"\"\"\n    portfolios = (prices_monthly\n        .assign(\n            # Map each return month to its portfolio formation date\n            sorting_date=lambda x: pd.to_datetime(\n                np.where(\n                    x[\"date\"].dt.month &lt;= 6,\n                    (x[\"date\"].dt.year - 1).astype(str) + \"-07-01\",\n                    x[\"date\"].dt.year.astype(str) + \"-07-01\"\n                )\n            )\n        )\n        .merge(\n            portfolio_assignments,\n            on=[\"symbol\", \"sorting_date\"],\n            how=\"inner\"\n        )\n    )\n    \n    return portfolios\n\nportfolios_monthly_ff3 = merge_portfolios_with_returns(\n    prices_monthly,\n    portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]]\n)\n\n\nprint(f\"Merged observations: {len(portfolios_monthly_ff3):,}\")\n\nMerged observations: 136,444\n\n\n\n\n17.5.2 Computing Value-Weighted Portfolio Returns\nWe compute value-weighted returns for each of the six portfolios. Value-weighting uses lagged market capitalization to avoid look-ahead bias.\n\ndef compute_portfolio_returns(data, grouping_vars):\n    \"\"\"\n    Compute value-weighted portfolio returns.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Returns data with portfolio assignments and mktcap_lag\n    grouping_vars : list\n        Variables defining portfolio groups\n        \n    Returns\n    -------\n    pd.DataFrame\n        Value-weighted returns for each portfolio-date\n    \"\"\"\n    portfolio_returns = (data\n        .groupby(grouping_vars + [\"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]),\n            \"n_stocks\": len(x)\n        }))\n        .reset_index()\n    )\n    \n    return portfolio_returns\n\n\n# Compute portfolio returns\nportfolio_returns_ff3 = compute_portfolio_returns(\n    portfolios_monthly_ff3,\n    [\"portfolio_size\", \"portfolio_bm\"]\n)\n\nprint(\"Portfolio Returns Summary:\")\nprint(portfolio_returns_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True)[\"ret\"].describe().round(4))\n\nPortfolio Returns Summary:\n                             count    mean     std     min     25%     50%  \\\nportfolio_size portfolio_bm                                                  \n1              1             150.0 -0.0052  0.0379 -0.1268 -0.0262 -0.0058   \n               2             150.0 -0.0025  0.0414 -0.1080 -0.0236 -0.0053   \n               3             150.0  0.0039  0.0601 -0.1612 -0.0269 -0.0004   \n2              1             150.0 -0.0124  0.0594 -0.2222 -0.0449 -0.0107   \n               2             150.0 -0.0021  0.0671 -0.1701 -0.0403 -0.0046   \n               3             150.0  0.0024  0.0879 -0.2359 -0.0527 -0.0012   \n\n                                75%     max  \nportfolio_size portfolio_bm                  \n1              1             0.0161  0.0849  \n               2             0.0180  0.1195  \n               3             0.0314  0.2015  \n2              1             0.0210  0.1741  \n               2             0.0317  0.1770  \n               3             0.0437  0.2124  \n\n\n\n\n17.5.3 Constructing SMB and HML Factors\nWe now construct the SMB and HML factors from the portfolio returns.\nSMB (Small Minus Big): Average return of three small portfolios minus average return of three big portfolios.\nHML (High Minus Low): Average return of two high B/M portfolios minus average return of two low B/M portfolios.\n\ndef construct_ff3_factors(portfolio_returns):\n    \"\"\"\n    Construct Fama-French three factors from portfolio returns.\n    \n    Parameters\n    ----------\n    portfolio_returns : pd.DataFrame\n        Value-weighted returns for 2x3 portfolios\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly SMB and HML factors\n    \"\"\"\n    factors = (portfolio_returns\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            # SMB: Small minus Big (average across B/M groups)\n            \"smb\": (\n                x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean()\n            ),\n            # HML: High minus Low B/M (average across size groups)\n            \"hml\": (\n                x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean()\n            )\n        }))\n        .reset_index()\n    )\n    \n    return factors\n\nfactors_smb_hml = construct_ff3_factors(portfolio_returns_ff3)\n\nprint(\"SMB and HML Factors:\")\nprint(factors_smb_hml.head(10))\n\nSMB and HML Factors:\n        date       smb       hml\n0 2011-07-31 -0.007768  0.002754\n1 2011-08-31 -0.067309  0.011474\n2 2011-09-30  0.014884  0.022854\n3 2011-10-31 -0.003743  0.001631\n4 2011-11-30  0.063234  0.009103\n5 2011-12-31  0.014571  0.015280\n6 2012-01-31 -0.026080  0.009672\n7 2012-02-29 -0.035721  0.005474\n8 2012-03-31 -0.002344  0.032477\n9 2012-04-30 -0.033391  0.074191\n\n\n\n\n17.5.4 Computing the Market Factor\nThe market factor is the value-weighted return of all stocks minus the risk-free rate. We compute this independently from the sorted portfolios.\n\ndef compute_market_factor(prices_monthly):\n    \"\"\"\n    Compute value-weighted market excess return.\n    \n    Parameters\n    ----------\n    prices_monthly : pd.DataFrame\n        Monthly stock returns with mktcap_lag\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly market excess return\n    \"\"\"\n    market_factor = (prices_monthly\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"]),\n            \"n_stocks\": len(x)\n        }), include_groups=False)\n        .reset_index()\n    )\n    \n    return market_factor\n\nmarket_factor = compute_market_factor(prices_monthly)\n\nprint(\"Market Factor Summary:\")\nprint(market_factor[\"mkt_excess\"].describe().round(4))\n\nMarket Factor Summary:\ncount    167.0000\nmean      -0.0123\nstd        0.0595\nmin       -0.2149\n25%       -0.0394\n50%       -0.0106\n75%        0.0200\nmax        0.1677\nName: mkt_excess, dtype: float64\n\n\n\n\n17.5.5 Combining Three Factors\nWe combine SMB, HML, and the market factor into the complete three-factor dataset.\n\nfactors_ff3_monthly = (factors_smb_hml\n    .merge(market_factor[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"inner\")\n)\n\n# Add risk-free rate for completeness\nrf_monthly = (prices_monthly\n    .groupby(\"date\")[\"risk_free\"]\n    .first()\n    .reset_index()\n)\n\nfactors_ff3_monthly = factors_ff3_monthly.merge(rf_monthly, on=\"date\", how=\"left\")\n\nprint(\"Fama-French Three Factors (Monthly):\")\nprint(factors_ff3_monthly.head(10))\n\nprint(\"\\nFactor Summary Statistics:\")\nprint(factors_ff3_monthly[[\"mkt_excess\", \"smb\", \"hml\"]].describe().round(4))\n\nFama-French Three Factors (Monthly):\n        date       smb       hml  mkt_excess  risk_free\n0 2011-07-31 -0.007768  0.002754   -0.078748   0.003333\n1 2011-08-31 -0.067309  0.011474    0.029906   0.003333\n2 2011-09-30  0.014884  0.022854   -0.002173   0.003333\n3 2011-10-31 -0.003743  0.001631   -0.014005   0.003333\n4 2011-11-30  0.063234  0.009103   -0.179410   0.003333\n5 2011-12-31  0.014571  0.015280   -0.094802   0.003333\n6 2012-01-31 -0.026080  0.009672    0.081273   0.003333\n7 2012-02-29 -0.035721  0.005474    0.069655   0.003333\n8 2012-03-31 -0.002344  0.032477    0.029005   0.003333\n9 2012-04-30 -0.033391  0.074191    0.048791   0.003333\n\nFactor Summary Statistics:\n       mkt_excess       smb       hml\ncount    150.0000  150.0000  150.0000\nmean      -0.0101    0.0027    0.0120\nstd        0.0586    0.0420    0.0535\nmin       -0.2149   -0.1599   -0.1284\n25%       -0.0380   -0.0175   -0.0160\n50%       -0.0095    0.0070    0.0043\n75%        0.0214    0.0261    0.0340\nmax        0.1677    0.1175    0.1618\n\n\n\n\n17.5.6 Saving Three-Factor Data\n\nfactors_ff3_monthly.to_sql(\n    name=\"factors_ff3_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Three-factor monthly data saved to database.\")\n\nThree-factor monthly data saved to database.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html#fama-french-five-factor-model-monthly",
    "href": "21_fama_french.html#fama-french-five-factor-model-monthly",
    "title": "17  Fama-French Factors",
    "section": "17.6 Fama-French Five-Factor Model (Monthly)",
    "text": "17.6 Fama-French Five-Factor Model (Monthly)\n\n17.6.1 Portfolio Assignments with Dependent Sorts\nFor the five-factor model, we use dependent sorts: size is sorted independently, but profitability and investment are sorted within size groups. This controls for the correlation between size and these characteristics.\n\ndef assign_ff5_portfolios(sorting_variables):\n    \"\"\"\n    Assign portfolios for Fama-French five-factor model.\n    \"\"\"\n    df = sorting_variables.copy()\n    \n    # Independent size sort\n    df[\"portfolio_size\"] = df.groupby(\"sorting_date\")[\"size\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.5, 1], labels=[1, 2], duplicates='drop')\n    )\n    \n    # Dependent sorts within size groups\n    df[\"portfolio_bm\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"bm\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    df[\"portfolio_op\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"op\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    df[\"portfolio_inv\"] = df.groupby([\"sorting_date\", \"portfolio_size\"])[\"inv\"].transform(\n        lambda x: pd.qcut(x, q=[0, 0.3, 0.7, 1], labels=[1, 2, 3], duplicates='drop')\n    )\n    \n    return df\n\n# Run\nportfolios_ff5 = assign_ff5_portfolios(sorting_variables)\n\n\n\n17.6.2 Validating Five-Factor Portfolios\n\n# Check characteristic monotonicity for each dimension\nprint(\"Profitability by Portfolio (should be increasing):\")\nprint(portfolios_ff5.groupby(\"portfolio_op\", observed=True)[\"op\"].median().round(4))\n\nprint(\"\\nInvestment by Portfolio (should be increasing):\")\nprint(portfolios_ff5.groupby(\"portfolio_inv\", observed=True)[\"inv\"].median().round(4))\n\n# Check portfolio counts\nprint(\"\\nStocks per Size/Profitability Bin:\")\nprint(portfolios_ff5.groupby([\"portfolio_size\", \"portfolio_op\"], observed=True).size().unstack(fill_value=0))\n\n# Check number of unique firms per year\n(portfolios_ff5\n .groupby(\"sorting_date\")[\"symbol\"]\n .nunique())\n\nProfitability by Portfolio (should be increasing):\nportfolio_op\n1   -0.0053\n2    0.1366\n3    0.4098\nName: op, dtype: float64\n\nInvestment by Portfolio (should be increasing):\nportfolio_inv\n1   -0.1012\n2    0.0329\n3    0.2568\nName: inv, dtype: float64\n\nStocks per Size/Profitability Bin:\nportfolio_op       1     2     3\nportfolio_size                  \n1               1812  2403  1811\n2               1811  2401  1808\n\n\nsorting_date\n2011-07-01     556\n2012-07-01     632\n2013-07-01     643\n2014-07-01     650\n2015-07-01     669\n2016-07-01     737\n2017-07-01     842\n2018-07-01    1073\n2019-07-01    1183\n2020-07-01    1224\n2021-07-01    1258\n2022-07-01    1286\n2023-07-01    1293\nName: symbol, dtype: int64\n\n\n\n\n17.6.3 Merging and Computing Portfolio Returns\n\n# Merge with returns\nportfolios_monthly_ff5 = merge_portfolios_with_returns(\n    prices_monthly,\n    portfolios_ff5[[\"symbol\", \"sorting_date\", \"portfolio_size\", \n                    \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"]]\n)\n\nprint(f\"Five-factor merged observations: {len(portfolios_monthly_ff5):,}\")\n\nFive-factor merged observations: 136,444\n\n\n\n\n17.6.4 Constructing All Five Factors\nWe construct each factor from the appropriate portfolio sorts.\n\ndef construct_ff5_factors(portfolios_monthly):\n    \"\"\"\n    Construct Fama-French five factors from portfolio data.\n    \n    Parameters\n    ----------\n    portfolios_monthly : pd.DataFrame\n        Monthly returns with all portfolio assignments\n        \n    Returns\n    -------\n    pd.DataFrame\n        Monthly five-factor returns\n    \"\"\"\n    \n    # HML: Value factor from B/M sorts\n    portfolios_bm = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_hml = (portfolios_bm\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # RMW: Profitability factor from OP sorts\n    portfolios_op = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_rmw = (portfolios_op\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"rmw\": (x.loc[x[\"portfolio_op\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_op\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # CMA: Investment factor from INV sorts\n    # Note: CMA is Conservative minus Aggressive (low inv - high inv)\n    portfolios_inv = (portfolios_monthly\n        .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_cma = (portfolios_inv\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"cma\": (x.loc[x[\"portfolio_inv\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_inv\"] == 3, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # SMB: Size factor (average across all characteristic portfolios)\n    all_portfolios = pd.concat([portfolios_bm, portfolios_op, portfolios_inv])\n    \n    factors_smb = (all_portfolios\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # Combine all factors\n    factors = (factors_smb\n        .merge(factors_hml, on=\"date\", how=\"outer\")\n        .merge(factors_rmw, on=\"date\", how=\"outer\")\n        .merge(factors_cma, on=\"date\", how=\"outer\")\n    )\n    \n    return factors\n\nfactors_ff5 = construct_ff5_factors(portfolios_monthly_ff5)\n\n# Add market factor\nfactors_ff5_monthly = (factors_ff5\n    .merge(market_factor[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"inner\")\n    .merge(rf_monthly, on=\"date\", how=\"left\")\n)\n\nprint(\"Fama-French Five Factors (Monthly):\")\nprint(factors_ff5_monthly.head(10))\n\nprint(\"\\nFactor Summary Statistics:\")\nprint(factors_ff5_monthly[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].describe().round(4))\n\nFama-French Five Factors (Monthly):\n        date       smb       hml       rmw       cma  mkt_excess  risk_free\n0 2011-07-31 -0.015907 -0.002812  0.060525  0.045291   -0.078748   0.003333\n1 2011-08-31 -0.061842  0.006189 -0.022700 -0.023177    0.029906   0.003333\n2 2011-09-30  0.014387  0.024301 -0.006005  0.003588   -0.002173   0.003333\n3 2011-10-31 -0.006958 -0.006940  0.026694  0.003649   -0.014005   0.003333\n4 2011-11-30  0.074369  0.015617 -0.058766  0.044214   -0.179410   0.003333\n5 2011-12-31  0.006687  0.022494  0.062655  0.052444   -0.094802   0.003333\n6 2012-01-31 -0.016254  0.010513 -0.042191 -0.067170    0.081273   0.003333\n7 2012-02-29 -0.026606  0.024465 -0.030849 -0.036383    0.069655   0.003333\n8 2012-03-31  0.005096  0.050930 -0.018441  0.043488    0.029005   0.003333\n9 2012-04-30  0.000712  0.058214 -0.061434  0.009233    0.048791   0.003333\n\nFactor Summary Statistics:\n       mkt_excess       smb       hml       rmw       cma\ncount    150.0000  150.0000  150.0000  150.0000  150.0000\nmean      -0.0101    0.0077    0.0115   -0.0047    0.0083\nstd        0.0586    0.0419    0.0518    0.0477    0.0335\nmin       -0.2149   -0.1522   -0.1283   -0.2126   -0.0814\n25%       -0.0380   -0.0137   -0.0126   -0.0308   -0.0131\n50%       -0.0095    0.0104    0.0046    0.0010    0.0067\n75%        0.0214    0.0316    0.0323    0.0178    0.0289\nmax        0.1677    0.1284    0.1510    0.1297    0.1331\n\n\n\n\n17.6.5 Factor Correlations\nWe examine correlations between factors, which should generally be low for the factors to capture distinct sources of risk.\n\nprint(\"Factor Correlation Matrix:\")\ncorrelation_matrix = factors_ff5_monthly[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].corr()\nprint(correlation_matrix.round(3))\n\nFactor Correlation Matrix:\n            mkt_excess    smb    hml    rmw    cma\nmkt_excess       1.000 -0.712  0.230 -0.006 -0.104\nsmb             -0.712  1.000  0.256 -0.373  0.246\nhml              0.230  0.256  1.000 -0.694  0.479\nrmw             -0.006 -0.373 -0.694  1.000 -0.352\ncma             -0.104  0.246  0.479 -0.352  1.000\n\n\n\n\n17.6.6 Saving Five-Factor Data\n\nfactors_ff5_monthly.to_sql(\n    name=\"factors_ff5_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Five-factor monthly data saved to database.\")\n\nFive-factor monthly data saved to database.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html#daily-fama-french-factors",
    "href": "21_fama_french.html#daily-fama-french-factors",
    "title": "17  Fama-French Factors",
    "section": "17.7 Daily Fama-French Factors",
    "text": "17.7 Daily Fama-French Factors\n\n17.7.1 Motivation for Daily Factors\nDaily factors are essential for several applications:\n\nDaily beta estimation: CAPM regressions using daily data require daily market excess returns.\nEvent studies: Measuring abnormal returns around corporate events requires daily factor adjustments.\nHigh-frequency research: Market microstructure studies need daily or intraday factor data.\n\nThe construction methodology mirrors the monthly approach, but we compute portfolio returns at daily frequency while maintaining the same annual portfolio formation dates.\n\n\n17.7.2 Loading Daily Returns\n\n# Load daily price data\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess\n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Daily returns: {len(prices_daily):,} observations\")\nprint(f\"Date range: {prices_daily['date'].min():%Y-%m-%d} to {prices_daily['date'].max():%Y-%m-%d}\")\n\nDaily returns: 3,462,157 observations\nDate range: 2010-01-05 to 2023-12-29\n\n\n\n\n17.7.3 Adding Market Cap for Daily Weighting\nFor value-weighted daily returns, we need market capitalization. We use the most recent monthly market cap as the weight for daily returns within that month.\n\n# Get monthly market cap to use as weights for daily returns\nmktcap_monthly = (prices_monthly\n    [[\"symbol\", \"date\", \"mktcap_lag\"]]\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n)\n\n# Add year_month to daily data for merging\nprices_daily = (prices_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .merge(\n        mktcap_monthly[[\"symbol\", \"year_month\", \"mktcap_lag\"]],\n        on=[\"symbol\", \"year_month\"],\n        how=\"left\"\n    )\n    .dropna(subset=[\"ret_excess\", \"mktcap_lag\"])\n)\n\nprint(f\"Daily returns with weights: {len(prices_daily):,} observations\")\n\nDaily returns with weights: 3,443,815 observations\n\n\n\n\n17.7.4 Merging Daily Returns with Portfolios\nWe use the same portfolio assignments (formed annually in July) for daily returns.\n\n# Step 1: Ensure portfolios_ff3 has correct format\nportfolios_ff3_clean = portfolios_ff3[[\"symbol\", \"sorting_date\", \"portfolio_size\", \"portfolio_bm\"]].copy()\nportfolios_ff3_clean[\"sorting_date\"] = pd.to_datetime(portfolios_ff3_clean[\"sorting_date\"])\n\nprint(\"Portfolio sorting dates:\")\nprint(portfolios_ff3_clean[\"sorting_date\"].unique()[:5])\n\n# Step 2: Create sorting_date for daily data\nprices_daily_with_sort = prices_daily.copy()\nprices_daily_with_sort[\"sorting_date\"] = prices_daily_with_sort[\"date\"].apply(\n    lambda x: pd.Timestamp(f\"{x.year}-07-01\") if x.month &gt; 6 else pd.Timestamp(f\"{x.year - 1}-07-01\")\n)\n\nprint(\"\\nDaily sorting dates:\")\nprint(prices_daily_with_sort[\"sorting_date\"].unique()[:5])\n\n# Step 3: Merge\nportfolios_daily_ff3 = prices_daily_with_sort.merge(\n    portfolios_ff3_clean,\n    on=[\"symbol\", \"sorting_date\"],\n    how=\"inner\"\n)\n\nprint(f\"\\nMerged daily observations: {len(portfolios_daily_ff3):,}\")\nprint(f\"Unique dates: {portfolios_daily_ff3['date'].nunique():,}\")\n\n# Step 4: Verify portfolio distribution\nprint(\"\\nPortfolio distribution in daily data:\")\nprint(portfolios_daily_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\nPortfolio sorting dates:\n&lt;DatetimeArray&gt;\n['2019-07-01 00:00:00', '2020-07-01 00:00:00', '2021-07-01 00:00:00',\n '2022-07-01 00:00:00', '2023-07-01 00:00:00']\nLength: 5, dtype: datetime64[us]\n\nDaily sorting dates:\n&lt;DatetimeArray&gt;\n['2018-07-01 00:00:00', '2019-07-01 00:00:00', '2020-07-01 00:00:00',\n '2021-07-01 00:00:00', '2022-07-01 00:00:00']\nLength: 5, dtype: datetime64[us]\n\nMerged daily observations: 2,843,570\nUnique dates: 3,126\n\nPortfolio distribution in daily data:\nportfolio_bm         1       2       3\nportfolio_size                        \n1               218040  585561  617327\n2               636152  552114  234376\n\n\n\n# Diagnostic: Check the daily portfolio merge\nprint(\"=\"*50)\nprint(\"DIAGNOSTIC: Daily Portfolio Merge\")\nprint(\"=\"*50)\n\nprint(f\"\\nDaily prices rows: {len(prices_daily):,}\")\nprint(f\"Daily FF3 portfolios rows: {len(portfolios_daily_ff3):,}\")\nprint(f\"Match rate: {len(portfolios_daily_ff3)/len(prices_daily)*100:.1f}%\")\n\n# Check portfolio distribution in daily data\nprint(\"\\nDaily portfolio distribution:\")\nprint(portfolios_daily_ff3.groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\n# Check a specific date\ntest_date = portfolios_daily_ff3[\"date\"].iloc[1000]\nprint(f\"\\nSample date: {test_date}\")\nprint(portfolios_daily_ff3.query(\"date == @test_date\").groupby([\"portfolio_size\", \"portfolio_bm\"], observed=True).size().unstack(fill_value=0))\n\n==================================================\nDIAGNOSTIC: Daily Portfolio Merge\n==================================================\n\nDaily prices rows: 3,443,815\nDaily FF3 portfolios rows: 2,843,570\nMatch rate: 82.6%\n\nDaily portfolio distribution:\nportfolio_bm         1       2       3\nportfolio_size                        \n1               218040  585561  617327\n2               636152  552114  234376\n\nSample date: 2023-06-28 00:00:00\nportfolio_bm      1    2    3\nportfolio_size               \n1                93  232  312\n2               291  280   67\n\n\n\n\n17.7.5 Computing Daily Three Factors\n\ndef compute_daily_ff3_factors(portfolios_daily):\n    \"\"\"\n    Compute daily Fama-French three factors.\n    \n    Parameters\n    ----------\n    portfolios_daily : pd.DataFrame\n        Daily returns with portfolio assignments\n        \n    Returns\n    -------\n    pd.DataFrame\n        Daily SMB and HML factors\n    \"\"\"\n    # Compute daily portfolio returns\n    portfolio_returns = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    # Compute factors\n    factors = (portfolio_returns\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean()),\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    return factors\n\nfactors_daily_smb_hml = compute_daily_ff3_factors(portfolios_daily_ff3)\n\nprint(f\"Daily factor observations: {len(factors_daily_smb_hml):,}\")\nprint(factors_daily_smb_hml.head(10))\n\nDaily factor observations: 3,126\n        date       smb       hml\n0 2011-07-01  0.008587  0.000967\n1 2011-07-04  0.005099 -0.001099\n2 2011-07-05 -0.009088  0.010152\n3 2011-07-06  0.004875 -0.003918\n4 2011-07-07 -0.011239 -0.000584\n5 2011-07-08  0.005636 -0.008003\n6 2011-07-11  0.003940  0.006172\n7 2011-07-12  0.003205  0.006543\n8 2011-07-13 -0.000097 -0.001134\n9 2011-07-14 -0.001248  0.001669\n\n\n\n\n17.7.6 Computing Daily Market Factor\n\ndef compute_daily_market_factor(prices_daily):\n    \"\"\"\n    Compute daily value-weighted market excess return.\n    \n    Parameters\n    ----------\n    prices_daily : pd.DataFrame\n        Daily returns with mktcap_lag\n        \n    Returns\n    -------\n    pd.DataFrame\n        Daily market excess return\n    \"\"\"\n    market_daily = (prices_daily\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"mkt_excess\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    return market_daily\n\nmarket_factor_daily = compute_daily_market_factor(prices_daily)\n\nprint(f\"Daily market factor: {len(market_factor_daily):,} days\")\n\nDaily market factor: 3,474 days\n\n\n\n\n17.7.7 Combining Daily Three Factors\n\nfactors_ff3_daily = (factors_daily_smb_hml\n    .merge(market_factor_daily, on=\"date\", how=\"inner\")\n)\n\n# Add risk-free rate (use monthly rate / 21 as daily approximation, or load actual daily rate)\nfactors_ff3_daily[\"risk_free\"] = 0.04 / 252  # Approximate daily risk-free\n\nprint(\"Daily Fama-French Three Factors:\")\nprint(factors_ff3_daily.head(10))\n\nprint(\"\\nDaily Factor Summary Statistics:\")\nprint(factors_ff3_daily[[\"mkt_excess\", \"smb\", \"hml\"]].describe().round(6))\n\nDaily Fama-French Three Factors:\n        date       smb       hml  mkt_excess  risk_free\n0 2011-07-01  0.008587  0.000967   -0.019862   0.000159\n1 2011-07-04  0.005099 -0.001099   -0.000633   0.000159\n2 2011-07-05 -0.009088  0.010152    0.013314   0.000159\n3 2011-07-06  0.004875 -0.003918   -0.008045   0.000159\n4 2011-07-07 -0.011239 -0.000584    0.003391   0.000159\n5 2011-07-08  0.005636 -0.008003    0.000218   0.000159\n6 2011-07-11  0.003940  0.006172   -0.013393   0.000159\n7 2011-07-12  0.003205  0.006543   -0.017505   0.000159\n8 2011-07-13 -0.000097 -0.001134    0.000767   0.000159\n9 2011-07-14 -0.001248  0.001669   -0.000695   0.000159\n\nDaily Factor Summary Statistics:\n        mkt_excess          smb          hml\ncount  3126.000000  3126.000000  3126.000000\nmean     -0.000479     0.000236     0.000594\nstd       0.011269     0.008488     0.008585\nmin      -0.070268    -0.032671    -0.039418\n25%      -0.005074    -0.004882    -0.003941\n50%       0.000350    -0.000106     0.000522\n75%       0.005531     0.004307     0.005233\nmax       0.043386     0.042686     0.083889\n\n\n\n\n17.7.8 Computing Daily Five Factors\n\n# Step 1: Clean portfolios\nportfolios_ff5_clean = portfolios_ff5[[\"symbol\", \"sorting_date\", \"portfolio_size\", \n                                        \"portfolio_bm\", \"portfolio_op\", \"portfolio_inv\"]].copy()\nportfolios_ff5_clean[\"sorting_date\"] = pd.to_datetime(portfolios_ff5_clean[\"sorting_date\"])\n\n# Step 2: Merge with daily prices\nportfolios_daily_ff5 = prices_daily_with_sort.merge(\n    portfolios_ff5_clean,\n    on=[\"symbol\", \"sorting_date\"],\n    how=\"inner\"\n)\n\nprint(f\"FF5 Daily merged observations: {len(portfolios_daily_ff5):,}\")\n\nFF5 Daily merged observations: 2,843,570\n\n\n\ndef compute_daily_ff5_factors(portfolios_daily):\n    \"\"\"Compute daily Fama-French five factors.\"\"\"\n    \n    # HML from B/M sorts\n    portfolios_bm = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_bm\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_hml = (portfolios_bm\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"hml\": (x.loc[x[\"portfolio_bm\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_bm\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # RMW from OP sorts\n    portfolios_op = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_op\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_rmw = (portfolios_op\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"rmw\": (x.loc[x[\"portfolio_op\"] == 3, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_op\"] == 1, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # CMA from INV sorts (note: low minus high)\n    portfolios_inv = (portfolios_daily\n        .groupby([\"portfolio_size\", \"portfolio_inv\", \"date\"], observed=True)\n        .apply(lambda x: pd.Series({\n            \"ret\": np.average(x[\"ret_excess\"], weights=x[\"mktcap_lag\"])\n        }))\n        .reset_index()\n    )\n    \n    factors_cma = (portfolios_inv\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"cma\": (x.loc[x[\"portfolio_inv\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_inv\"] == 3, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # SMB from all sorts\n    all_portfolios = pd.concat([portfolios_bm, portfolios_op, portfolios_inv])\n    \n    factors_smb = (all_portfolios\n        .groupby(\"date\")\n        .apply(lambda x: pd.Series({\n            \"smb\": (x.loc[x[\"portfolio_size\"] == 1, \"ret\"].mean() -\n                   x.loc[x[\"portfolio_size\"] == 2, \"ret\"].mean())\n        }))\n        .reset_index()\n    )\n    \n    # Combine\n    factors = (factors_smb\n        .merge(factors_hml, on=\"date\", how=\"outer\")\n        .merge(factors_rmw, on=\"date\", how=\"outer\")\n        .merge(factors_cma, on=\"date\", how=\"outer\")\n    )\n    \n    return factors\n\n# Compute daily FF5 factors\nfactors_daily_ff5 = compute_daily_ff5_factors(portfolios_daily_ff5)\n\n# Add market factor\nfactors_ff5_daily = (factors_daily_ff5\n    .merge(market_factor_daily, on=\"date\", how=\"inner\")\n)\nfactors_ff5_daily[\"risk_free\"] = 0.04 / 252\n\nprint(\"Daily Fama-French Five Factors:\")\nprint(factors_ff5_daily.head(10))\n\nprint(\"\\nDaily Five-Factor Summary Statistics:\")\nprint(factors_ff5_daily[[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]].describe().round(6))\n\nDaily Fama-French Five Factors:\n        date       smb       hml       rmw       cma  mkt_excess  risk_free\n0 2011-07-01  0.006295  0.002515  0.013140  0.007680   -0.019862   0.000159\n1 2011-07-04  0.002880 -0.002875  0.006560 -0.004886   -0.000633   0.000159\n2 2011-07-05 -0.004260  0.009864 -0.012158 -0.004470    0.013314   0.000159\n3 2011-07-06  0.001544 -0.009847  0.012977  0.006286   -0.008045   0.000159\n4 2011-07-07 -0.009789 -0.003988 -0.000197 -0.006995    0.003391   0.000159\n5 2011-07-08  0.001537 -0.006700  0.010841 -0.007661    0.000218   0.000159\n6 2011-07-11  0.005396  0.004747  0.000655  0.013375   -0.013393   0.000159\n7 2011-07-12  0.004759  0.007367  0.001989  0.014669   -0.017505   0.000159\n8 2011-07-13 -0.000009  0.001110 -0.002052 -0.001633    0.000767   0.000159\n9 2011-07-14 -0.001668  0.002916 -0.005427  0.005388   -0.000695   0.000159\n\nDaily Five-Factor Summary Statistics:\n        mkt_excess          smb          hml          rmw          cma\ncount  3126.000000  3126.000000  3126.000000  3126.000000  3126.000000\nmean     -0.000479     0.000484     0.000549    -0.000136     0.000413\nstd       0.011269     0.008033     0.008312     0.008538     0.006756\nmin      -0.070268    -0.036283    -0.039155    -0.154013    -0.047698\n25%      -0.005074    -0.004122    -0.003681    -0.004212    -0.003364\n50%       0.000350     0.000105     0.000384     0.000030     0.000162\n75%       0.005531     0.004358     0.004819     0.004036     0.003800\nmax       0.043386     0.060307     0.086269     0.102001     0.089907\n\n\n\n\n17.7.9 Saving Daily Factors\n\nfactors_ff3_daily.to_sql(\n    name=\"factors_ff3_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nfactors_ff5_daily.to_sql(\n    name=\"factors_ff5_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\n\nprint(\"Daily factor data saved to database.\")\n\nDaily factor data saved to database.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html#factor-validation-and-diagnostics",
    "href": "21_fama_french.html#factor-validation-and-diagnostics",
    "title": "17  Fama-French Factors",
    "section": "17.8 Factor Validation and Diagnostics",
    "text": "17.8 Factor Validation and Diagnostics\n\n# Verify all tables are in database\nprint(\"\\n\" + \"=\"*50)\nprint(\"DATABASE SUMMARY\")\nprint(\"=\"*50)\n\ntables = [\"factors_ff3_monthly\", \"factors_ff5_monthly\", \n          \"factors_ff3_daily\", \"factors_ff5_daily\"]\n\nfor table in tables:\n    df = pd.read_sql_query(f\"SELECT COUNT(*) as n FROM {table}\", con=tidy_finance)\n    print(f\"{table}: {df['n'].iloc[0]:,} observations\")\n\n# Correlation check: Monthly vs Daily (aggregated)\nprint(\"\\n\" + \"=\"*50)\nprint(\"MONTHLY VS DAILY CONSISTENCY CHECK\")\nprint(\"=\"*50)\n\nfactors_daily_agg = (factors_ff3_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .groupby(\"year_month\")[[\"mkt_excess\", \"smb\", \"hml\"]]\n    .sum()\n    .reset_index()\n)\n\nfactors_monthly_check = (factors_ff3_monthly\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n)\n\ncomparison = factors_monthly_check.merge(\n    factors_daily_agg, on=\"year_month\", suffixes=(\"_monthly\", \"_daily\")\n)\n\nfor factor in [\"mkt_excess\", \"smb\", \"hml\"]:\n    corr = comparison[f\"{factor}_monthly\"].corr(comparison[f\"{factor}_daily\"])\n    print(f\"{factor}: Monthly-Daily correlation = {corr:.4f}\")\n\n\n==================================================\nDATABASE SUMMARY\n==================================================\nfactors_ff3_monthly: 150 observations\nfactors_ff5_monthly: 150 observations\nfactors_ff3_daily: 3,126 observations\nfactors_ff5_daily: 3,126 observations\n\n==================================================\nMONTHLY VS DAILY CONSISTENCY CHECK\n==================================================\nmkt_excess: Monthly-Daily correlation = 0.9980\nsmb: Monthly-Daily correlation = 0.9953\nhml: Monthly-Daily correlation = 0.9936\n\n\n\n17.8.1 Cumulative Factor Returns\nWe visualize the cumulative performance of each factor to assess whether the factors generate meaningful premiums over time.\n\n# Compute cumulative returns\nfactors_cumulative = (factors_ff5_monthly\n    .set_index(\"date\")\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .add(1)\n    .cumprod()\n)\n\n# Plot\nfig, ax = plt.subplots(figsize=(12, 6))\nfactors_cumulative.plot(ax=ax)\nax.set_title(\"Cumulative Factor Returns (Vietnam)\")\nax.set_xlabel(\"\")\nax.set_ylabel(\"Growth of $1\")\nax.legend(title=\"Factor\")\nax.axhline(y=1, color='gray', linestyle='--', alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 17.1: Cumulative returns of Fama-French factors for the Vietnamese market. The figure shows the growth of $1 invested in each factor portfolio.\n\n\n\n\n\n\n\n17.8.2 Average Factor Premiums\nWe compute annualized average factor premiums and their statistical significance.\n\n# Annualized average returns (monthly returns * 12)\nfactor_premiums = (factors_ff5_monthly\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .mean() * 12 * 100  # Annualized percentage\n)\n\n# Standard errors\nfactor_se = (factors_ff5_monthly\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .std() / np.sqrt(len(factors_ff5_monthly)) * np.sqrt(12) * 100\n)\n\n# T-statistics\nfactor_tstat = factor_premiums / factor_se\n\nprint(\"Annualized Factor Premiums (%):\")\nprint(factor_premiums.round(2))\n\nprint(\"\\nT-Statistics:\")\nprint(factor_tstat.round(2))\n\nAnnualized Factor Premiums (%):\nmkt_excess   -12.09\nsmb            9.19\nhml           13.75\nrmw           -5.69\ncma            9.94\ndtype: float64\n\nT-Statistics:\nmkt_excess    -7.30\nsmb            7.76\nhml            9.38\nrmw           -4.22\ncma           10.49\ndtype: float64\n\n\n\n\n17.8.3 Comparing Monthly and Daily Factors\nWe verify consistency between monthly and daily factors by computing correlations.\n\n# Aggregate daily factors to monthly for comparison\nfactors_daily_monthly = (factors_ff5_daily\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .groupby(\"year_month\")\n    [[\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]]\n    .sum()  # Sum daily returns to get monthly\n    .reset_index()\n)\n\n# Merge with actual monthly factors\ncomparison = (factors_ff5_monthly\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"M\"))\n    .merge(\n        factors_daily_monthly,\n        on=\"year_month\",\n        suffixes=(\"_monthly\", \"_daily\")\n    )\n)\n\n# Correlations\nfor factor in [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"]:\n    corr = comparison[f\"{factor}_monthly\"].corr(comparison[f\"{factor}_daily\"])\n    print(f\"{factor}: Monthly-Daily correlation = {corr:.4f}\")\n\nmkt_excess: Monthly-Daily correlation = 0.9980\nsmb: Monthly-Daily correlation = 0.9950\nhml: Monthly-Daily correlation = 0.9948\nrmw: Monthly-Daily correlation = 0.9929\ncma: Monthly-Daily correlation = 0.9884",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "21_fama_french.html#key-takeaways",
    "href": "21_fama_french.html#key-takeaways",
    "title": "17  Fama-French Factors",
    "section": "17.9 Key Takeaways",
    "text": "17.9 Key Takeaways\n\nFactor Models Explained: The Fama-French three-factor model adds size (SMB) and value (HML) factors to the CAPM, while the five-factor model further includes profitability (RMW) and investment (CMA) factors.\nConstruction Methodology: Factors are constructed through double-sorted portfolios with careful attention to timing. Portfolios are formed in July using accounting data from the prior fiscal year to ensure information was publicly available.\nIndependent vs. Dependent Sorts: The three-factor model uses independent sorts on size and book-to-market, creating a 2×3 grid. The five-factor model uses dependent sorts where characteristics are sorted within size groups.\nValue-Weighted Returns: Portfolio returns are computed using value-weighting with lagged market capitalization to avoid look-ahead bias.\nDaily Factors: Daily factors use the same annual portfolio assignments but compute returns at daily frequency, enabling higher-frequency applications like daily beta estimation.\nMarket Factor: The market factor is computed independently as the value-weighted return of all stocks minus the risk-free rate.\nValidation: Factor quality can be assessed through characteristic monotonicity, portfolio diversification, cumulative returns, and consistency between daily and monthly frequencies.\nVietnamese Market Adaptation: While following the original Fama-French methodology, we adapt for Vietnamese market characteristics including VAS accounting standards, reporting timelines, and currency units.\n\n\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 1992. “The cross-section of expected stock returns.” The Journal of Finance 47 (2): 427–65. https://doi.org/2329112.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Fama-French Factors</span>"
    ]
  },
  {
    "objectID": "22_momentum.html",
    "href": "22_momentum.html",
    "title": "18  Momentum Strategies",
    "section": "",
    "text": "18.1 Theoretical Background\nMomentum is one of the most robust and pervasive anomalies in financial economics. Stocks that have performed well over the past three to twelve months tend to continue performing well over the subsequent three to twelve months, and stocks that have performed poorly tend to continue underperforming. This pattern, first documented by Jegadeesh and Titman (1993), has been replicated across virtually every equity market, asset class, and time period examined, earning it a central place in the canon of empirical asset pricing.\nIn this chapter, we provide an implementation of momentum strategies following the methodology of Jegadeesh and Titman (1993). We construct momentum portfolios based on past cumulative returns, evaluate their performance across different formation and holding period combinations, and examine whether the momentum premium exists in the Vietnamese equity market..",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#theoretical-background",
    "href": "22_momentum.html#theoretical-background",
    "title": "18  Momentum Strategies",
    "section": "",
    "text": "18.1.1 The Momentum Effect\nThe momentum effect refers to the tendency of assets with high recent returns to continue generating high returns, and assets with low recent returns to continue generating low returns. More formally, if we define the cumulative return of stock \\(i\\) over the past \\(J\\) months as\n\\[\nR_{i,t-J:t-1} = \\prod_{s=t-J}^{t-1} (1 + r_{i,s}) - 1\n\\tag{18.1}\\]\nthen momentum predicts a positive cross-sectional relationship between \\(R_{i,t-J:t-1}\\) and future returns \\(r_{i,t}\\). That is, stocks in the top decile of past returns (winners) should outperform stocks in the bottom decile (losers) in subsequent months.\nThe Jegadeesh and Titman (1993) framework parameterizes momentum strategies by two key dimensions:\n\nFormation period (\\(J\\)): The number of months over which past returns are computed to rank stocks. Typical values range from 3 to 12 months.\nHolding period (\\(K\\)): The number of months for which the momentum portfolios are held after formation. Typical values also range from 3 to 12 months.\n\nA strategy is therefore characterized by the pair \\((J, K)\\). For example, a \\((6, 6)\\) strategy ranks stocks based on their cumulative returns over the past 6 months and holds the resulting portfolios for 6 months.\n\n\n18.1.2 Overlapping Portfolios and Calendar-Time Returns\nA critical implementation detail in Jegadeesh and Titman (1993) is the use of overlapping portfolios. In each month \\(t\\), a new set of portfolios is formed based on the most recent \\(J\\)-month returns. However, portfolios formed in months \\(t-1\\), \\(t-2\\), \\(\\ldots\\), \\(t-K+1\\) are still within their holding periods and therefore remain active. The return of the momentum strategy in month \\(t\\) is thus the equally weighted average across all \\(K\\) active cohorts:\n\\[\nr_{p,t} = \\frac{1}{K} \\sum_{k=0}^{K-1} r_{p,t}^{(t-k)}\n\\tag{18.2}\\]\nwhere \\(r_{p,t}^{(t-k)}\\) denotes the return in month \\(t\\) of the portfolio formed in month \\(t-k\\). This overlapping portfolio approach serves two purposes. First, it reduces the impact of any single formation date on the strategy’s performance. Second, it produces a monthly return series that can be analyzed using standard time-series methods, even when the holding period \\(K\\) exceeds one month.\n\n\n18.1.3 Theoretical Explanations\nThe academic literature offers two broad classes of explanations for the momentum effect.\n\nBehavioral explanations attribute momentum to systematic cognitive biases among investors. Daniel, Hirshleifer, and Subrahmanyam (1998) propose a model based on investor overconfidence and biased self-attribution: investors overweight private signals and attribute confirming outcomes to their own skill, leading to initial underreaction followed by delayed overreaction. Hong and Stein (1999) develop a model in which information diffuses gradually across heterogeneous investors, generating underreaction to firm-specific news. Barberis, Shleifer, and Vishny (1998) formalize a model combining conservatism bias (slow updating of beliefs in response to new evidence) and representativeness heuristic (extrapolation of recent trends).\nRisk-based explanations argue that momentum profits represent compensation for systematic risk that varies over time. Johnson (2002) show that momentum can arise in a rational framework if expected returns are stochastic and time-varying. Grundy and Martin (2001) document that momentum portfolios have substantial time-varying factor exposures, suggesting that at least part of the momentum premium reflects dynamic risk. However, standard risk models such as the Fama-French three-factor model have generally struggled to explain momentum returns, which motivated the development of explicit momentum factors such as the Winners-Minus-Losers (WML) factor in Carhart (1997).\n\n\n\n18.1.4 Momentum in Emerging Markets\nThe behavior of momentum in emerging markets differs substantially from developed markets, and understanding these differences is essential for interpreting our Vietnamese market results.\nRouwenhorst (1998) provides early evidence that momentum profits exist in European markets. Chan, Hameed, and Tong (2000) extend the analysis to international equity markets and find that momentum profits are present in most developed markets but are weaker or absent in several Asian markets. Chui, Titman, and Wei (2010) offer a cultural explanation, documenting that momentum profits are positively related to a country’s degree of individualism as measured by Hofstede (2001). Countries with collectivist cultures, including many in East and Southeast Asia, tend to exhibit weaker momentum effects.\nSeveral market microstructure features common in emerging markets may attenuate momentum:\n\nTrading band limits constrain daily price movements, potentially slowing the adjustment process that generates momentum. Vietnam’s HOSE imposes a \\(\\pm 7\\%\\) daily limit, while HNX allows \\(\\pm 10\\%\\).\nLower liquidity and higher transaction costs can erode momentum profits, as documented by Lesmond, Schill, and Zhou (2004).\nForeign ownership limits segment the investor base, potentially altering the information diffusion dynamics that underlie momentum.\nShorter market history limits the statistical power available to detect the effect.\n\nThese considerations motivate our careful empirical analysis of whether, and to what extent, momentum manifests in Vietnamese equities.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#setting-up-the-environment",
    "href": "22_momentum.html#setting-up-the-environment",
    "title": "18  Momentum Strategies",
    "section": "18.2 Setting Up the Environment",
    "text": "18.2 Setting Up the Environment\nWe begin by loading the necessary Python packages. The core packages include pandas for data manipulation, numpy for numerical operations, and sqlite3 for database connectivity. We also import plotnine for creating publication-quality figures and scipy for statistical tests.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nfrom datetime import datetime\nfrom itertools import product\n\nfrom plotnine import *\nfrom mizani.formatters import comma_format, percent_format\nfrom scipy import stats\n\nWe connect to our SQLite database, which stores the cleaned datasets prepared previous chapters.\n\ntidy_finance = sqlite3.connect(\n    database=\"data/tidy_finance_python.sqlite\"\n)",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#data-preparation",
    "href": "22_momentum.html#data-preparation",
    "title": "18  Momentum Strategies",
    "section": "18.3 Data Preparation",
    "text": "18.3 Data Preparation\n\n18.3.1 Loading Monthly Stock Returns\nWe load the monthly stock price data from our database. The prices_monthly table contains adjusted returns, market capitalizations, and other variables for all stocks listed on HOSE and HNX.\n\nprices_monthly = pd.read_sql_query(\n    # can add \"exchange\" variable\n    sql=\"\"\"\n        SELECT symbol, date, ret, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprint(f\"Total monthly observations: {len(prices_monthly):,}\")\nprint(f\"Unique stocks: {prices_monthly['symbol'].nunique():,}\")\nprint(f\"Date range: {prices_monthly['date'].min().date()} \"\n      f\"to {prices_monthly['date'].max().date()}\")\n\nTotal monthly observations: 165,499\nUnique stocks: 1,457\nDate range: 2010-02-28 to 2023-12-31\n\n\n\n\n18.3.2 Inspecting the Data\nBefore proceeding with portfolio construction, we examine the key variables in our dataset. Table 18.1 presents the summary statistics for the main variables used in momentum portfolio construction.\n\nsummary_stats = (prices_monthly\n    [[\"ret\", \"ret_excess\", \"mktcap\"]]\n    .describe()\n    .T\n    .round(4)\n)\nsummary_stats\n\n\n\nTable 18.1: Summary statistics for monthly stock return data. The table reports the count, mean, standard deviation, minimum, 25th percentile, median, 75th percentile, and maximum for monthly returns (ret), excess returns (ret_excess), and market capitalization (mktcap, in billion VND).\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nret\n165499.0\n0.0042\n0.1862\n-0.9900\n-0.0703\n0.0000\n0.0553\n12.7500\n\n\nret_excess\n165499.0\n0.0008\n0.1862\n-0.9933\n-0.0736\n-0.0033\n0.0519\n12.7467\n\n\nmktcap\n165499.0\n2183.1646\n13983.9977\n0.3536\n60.3728\n180.6224\n660.0000\n463886.6454\n\n\n\n\n\n\n\n\n\n\nWe also examine the cross-sectional distribution of stocks over time, which is important for understanding whether we have sufficient breadth for decile portfolio construction.\n\nstock_counts = (prices_monthly\n    .groupby(\"date\")[\"symbol\"]\n    .nunique()\n    .reset_index()\n    .rename(columns={\"symbol\": \"n_stocks\"})\n)\n\nplot_stock_counts = (\n    ggplot(stock_counts, aes(x=\"date\", y=\"n_stocks\")) +\n    geom_line(color=\"#1f77b4\") +\n    labs(\n        x=\"\", y=\"Number of stocks\",\n        title=\"Cross-Sectional Breadth Over Time\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_stock_counts\n\n\n\n\n\n\n\nFigure 18.1: Number of stocks with available return data over time. The figure shows the monthly cross-section of stocks available for momentum portfolio construction. A minimum number of stocks is needed to form meaningful decile portfolios.\n\n\n\n\n\n\n\n18.3.3 Loading Factor Data\nWe also load the Fama-French factor returns for risk-adjusted performance evaluation. These factors were constructed in previous chapters.\n\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT date, mkt_excess, smb, hml, risk_free FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Factor observations: {len(factors_ff3_monthly):,}\")\nprint(f\"Date range: {factors_ff3_monthly['date'].min().date()} \"\n      f\"to {factors_ff3_monthly['date'].max().date()}\")\n\nFactor observations: 150\nDate range: 2011-07-31 to 2023-12-31\n\n\n\n\n18.3.4 Data Quality Filters\nMomentum strategies require continuous return histories for the formation period. We apply several filters to ensure data quality:\n\nMinimum price filter: We exclude penny stocks with prices below 1,000 VND, which are subject to extreme microstructure noise.\nReturn availability: We require non-missing returns for the entire formation period.\nMarket capitalization: We require positive lagged market capitalization for portfolio weighting.\n\n\n# Filter for stocks with positive market cap and non-missing returns\nprices_clean = (prices_monthly\n    .dropna(subset=[\"ret\", \"mktcap\"])\n    .query(\"mktcap &gt; 0\")\n    .sort_values([\"symbol\", \"date\"])\n    .reset_index(drop=True)\n)\n\nprint(f\"Observations after filtering: {len(prices_clean):,}\")\nprint(f\"Stocks after filtering: {prices_clean['symbol'].nunique():,}\")\n\nObservations after filtering: 165,499\nStocks after filtering: 1,457",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#momentum-portfolio-construction",
    "href": "22_momentum.html#momentum-portfolio-construction",
    "title": "18  Momentum Strategies",
    "section": "18.4 Momentum Portfolio Construction",
    "text": "18.4 Momentum Portfolio Construction\n\n18.4.1 Computing Formation Period Returns\nThe first step in constructing momentum portfolios is to compute cumulative returns over the formation period. For a formation period of \\(J\\) months, we compute the cumulative return for each stock \\(i\\) at the end of month \\(t\\) as specified in Equation 35.7.\nWe implement this using a rolling product of gross returns. A key detail is that we require non-missing returns for all \\(J\\) months in the formation window. If any monthly return is missing, the cumulative return is set to missing, and the stock is excluded from portfolio formation in that month.\n\ndef compute_formation_returns(data, J):\n    \"\"\"\n    Compute J-month cumulative returns for momentum portfolio formation.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Panel data with columns 'symbol', 'date', 'ret'.\n    J : int\n        Formation period length in months (typically 3-12).\n    \n    Returns\n    -------\n    pd.DataFrame\n        Original data augmented with 'cum_return' column.\n    \"\"\"\n    df = data.sort_values([\"symbol\", \"date\"]).copy()\n    \n    # Compute rolling J-month cumulative return\n    # Using gross returns: (1+r1)*(1+r2)*...*(1+rJ) - 1\n    df[\"gross_ret\"] = 1 + df[\"ret\"]\n    \n    df[\"cum_return\"] = (\n        df.groupby(\"symbol\")[\"gross_ret\"]\n        .rolling(window=J, min_periods=J)\n        .apply(np.prod, raw=True)\n        .reset_index(level=0, drop=True)\n        - 1\n    )\n    \n    df = df.drop(columns=[\"gross_ret\"])\n    \n    return df\n\nLet us apply this function for our baseline case with \\(J = 6\\) months:\n\nJ = 6  # Formation period: 6 months\n\nprices_with_cumret = compute_formation_returns(prices_clean, J)\n\n# Check the result\nprint(f\"Observations with valid {J}-month cumulative returns: \"\n      f\"{prices_with_cumret['cum_return'].notna().sum():,}\")\nprint(f\"\\nCumulative return distribution:\")\nprint(prices_with_cumret[\"cum_return\"].describe().round(4))\n\nObservations with valid 6-month cumulative returns: 158,227\n\nCumulative return distribution:\ncount    158227.0000\nmean          0.0171\nstd           0.5053\nmin          -0.9999\n25%          -0.2196\n50%          -0.0400\n75%           0.1404\nmax          35.7136\nName: cum_return, dtype: float64\n\n\n\n\n18.4.2 Assigning Momentum Decile Portfolios\nEach month, we sort all stocks with valid cumulative returns into decile portfolios based on their past \\(J\\)-month performance. Portfolio 1 contains the bottom decile (losers) and portfolio 10 contains the top decile (winners).\n\ndef assign_momentum_portfolios(data, n_portfolios=10):\n    \"\"\"\n    Assign stocks to momentum portfolios based on formation-period returns.\n\n    For each cross-section (month), stocks are sorted into \n    n_portfolios quantile groups according to their cumulative return.\n    \n    Portfolio 1 = lowest past returns (Losers)\n    Portfolio n_portfolios = highest past returns (Winners)\n\n    This implementation uses `groupby().transform()` rather than \n    `groupby().apply()` to preserve the original DataFrame structure \n    and avoid index mutation issues.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        Panel data containing at minimum:\n        - 'symbol' : stock identifier\n        - 'date' : formation month\n        - 'cum_return' : cumulative return over formation window\n\n    n_portfolios : int, optional\n        Number of portfolios to form (default = 10 for deciles).\n\n    Returns\n    -------\n    pd.DataFrame\n        Original data augmented with:\n        - 'momr' : integer momentum rank (1 to n_portfolios)\n    \"\"\"\n\n    # Drop observations without formation-period returns\n    # These cannot be ranked into portfolios\n    df = data.dropna(subset=[\"cum_return\"]).copy()\n\n    def safe_qcut(x):\n        \"\"\"\n        Assign quantile portfolio labels within a single cross-section.\n\n        Uses pd.qcut to create approximately equal-sized portfolios.\n        If too few unique return values exist (which can happen in \n        small markets or illiquid samples), fall back to rank-based \n        binning to ensure portfolios are still formed.\n        \"\"\"\n        try:\n            # Standard quantile sorting\n            return pd.qcut(\n                x,\n                q=n_portfolios,\n                labels=range(1, n_portfolios + 1),\n                duplicates=\"drop\"  # prevents crash if quantile edges duplicate\n            )\n        except ValueError:\n            # Fallback: rank then evenly cut into bins\n            return pd.cut(\n                x.rank(method=\"first\"),\n                bins=n_portfolios,\n                labels=range(1, n_portfolios + 1)\n            )\n\n    # Cross-sectional portfolio assignment:\n    # For each month, compute portfolio ranks based on cum_return.\n    # transform ensures:\n    # - original row order preserved\n    # - no index mutation\n    # - no loss of 'date' column\n    df[\"momr\"] = (\n        df.groupby(\"date\")[\"cum_return\"]\n          .transform(safe_qcut)\n          .astype(int)\n    )\n\n    return df\n\n\nportfolios = assign_momentum_portfolios(prices_with_cumret)\n\nprint(f\"Stocks assigned to portfolios: {len(portfolios):,}\")\nprint(f\"\\nPortfolio distribution (should be approximately equal):\")\nprint(portfolios.groupby(\"momr\")[\"symbol\"].count().to_frame(\"count\"))\n\nStocks assigned to portfolios: 158,227\n\nPortfolio distribution (should be approximately equal):\n      count\nmomr       \n1     15894\n2     15815\n3     16021\n4     15759\n5     15738\n6     16568\n7     15288\n8     15582\n9     15697\n10    15865\n\n\n\n\n18.4.3 Defining Holding Period Dates\nAfter forming portfolios at the end of month \\(t\\), we hold them from the beginning of month \\(t+1\\) through the end of month \\(t+K\\). This one-month gap between the formation period and the start of the holding period is standard in the literature and avoids the well-documented short-term reversal effect at the one-month horizon (Jegadeesh 1990).\n\nK = 6  # Holding period: 6 months\n\nportfolios_with_dates = portfolios.copy()\nportfolios_with_dates = portfolios_with_dates.rename(\n    columns={\"date\": \"form_date\"}\n)\n\n# Define holding period start and end dates\nportfolios_with_dates[\"hdate1\"] = (\n    portfolios_with_dates[\"form_date\"] + pd.offsets.MonthBegin(1)\n)\nportfolios_with_dates[\"hdate2\"] = (\n    portfolios_with_dates[\"form_date\"] + pd.offsets.MonthEnd(K)\n)\n\nprint(portfolios_with_dates[\n    [\"symbol\", \"form_date\", \"cum_return\", \"momr\", \"hdate1\", \"hdate2\"]\n].head(10))\n\n   symbol  form_date  cum_return  momr     hdate1     hdate2\n5     A32 2019-04-30   -0.061599     4 2019-05-01 2019-10-31\n6     A32 2019-05-31   -0.213675     2 2019-06-01 2019-11-30\n7     A32 2019-06-30   -0.308720     2 2019-07-01 2019-12-31\n8     A32 2019-07-31   -0.249936     2 2019-08-01 2020-01-31\n9     A32 2019-08-31    0.061986     8 2019-09-01 2020-02-29\n10    A32 2019-09-30    0.030751     8 2019-10-01 2020-03-31\n11    A32 2019-10-31    0.030751     8 2019-11-01 2020-04-30\n12    A32 2019-11-30    0.032486     8 2019-12-01 2020-05-31\n13    A32 2019-12-31    0.180073     9 2020-01-01 2020-06-30\n14    A32 2020-01-31    0.412322    10 2020-02-01 2020-07-31\n\n\n\n\n18.4.4 Computing Portfolio Holding Period Returns\nWe now compute the returns of each portfolio during its holding period. For each stock-formation date combination, we merge in the actual returns realized during the holding window. This produces a panel of stock returns indexed by formation date, holding date, and momentum portfolio rank.\n\n# Merge: for each portfolio assignment, get all returns during holding period\nportfolio_returns = portfolios_with_dates.merge(\n    prices_clean[[\"symbol\", \"date\", \"ret\"]].rename(\n        columns={\"ret\": \"hret\", \"date\": \"hdate\"}\n    ),\n    on=\"symbol\",\n    how=\"inner\"\n)\n\n# Keep only returns within the holding period\nportfolio_returns = portfolio_returns.query(\n    \"hdate &gt;= hdate1 and hdate &lt;= hdate2\"\n)\n\nprint(f\"Portfolio-holding period observations: {len(portfolio_returns):,}\")\n\nPortfolio-holding period observations: 918,072\n\n\n\n\n18.4.5 Computing Equally Weighted Portfolio Returns\nThe Jegadeesh and Titman (1993) methodology uses equally weighted portfolios. For each holding month, each momentum decile has \\(K\\) active cohorts (one formed in each of the past \\(K\\) months). We first compute the equally weighted return within each cohort, then average across cohorts to get the final monthly portfolio return.\n\n# Step 1: Average return within each cohort (form_date × momr × hdate)\ncohort_returns = (portfolio_returns\n    .groupby([\"hdate\", \"momr\", \"form_date\"])\n    .agg(cohort_ret=(\"hret\", \"mean\"))\n    .reset_index()\n)\n\n# Step 2: Average across cohorts for each momentum portfolio × month\newret = (cohort_returns\n    .groupby([\"hdate\", \"momr\"])\n    .agg(\n        ewret=(\"cohort_ret\", \"mean\"),\n        ewret_std=(\"cohort_ret\", \"std\"),\n        n_cohorts=(\"cohort_ret\", \"count\")\n    )\n    .reset_index()\n    .rename(columns={\"hdate\": \"date\"})\n)\n\nprint(f\"Monthly portfolio return observations: {len(ewret):,}\")\nprint(f\"Date range: {ewret['date'].min().date()} to {ewret['date'].max().date()}\")\n\nMonthly portfolio return observations: 1,610\nDate range: 2010-08-31 to 2023-12-31\n\n\nWe should verify that we have the expected number of active cohorts per month. Once the strategy has been running for at least \\(K\\) months, each momentum decile should have exactly \\(K\\) active cohorts.\n\ncohort_check = (ewret\n    .groupby(\"date\")[\"n_cohorts\"]\n    .mean()\n    .reset_index()\n    .rename(columns={\"n_cohorts\": \"avg_cohorts\"})\n)\n\nplot_cohorts = (\n    ggplot(cohort_check, aes(x=\"date\", y=\"avg_cohorts\")) +\n    geom_line(color=\"#1f77b4\") +\n    geom_hline(yintercept=K, linetype=\"dashed\", color=\"red\") +\n    labs(\n        x=\"\", y=\"Average number of cohorts\",\n        title=f\"Active Cohorts per Portfolio (K={K})\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 4))\n)\nplot_cohorts\n\n\n\n\n\n\n\nFigure 18.2: Number of active cohorts per momentum portfolio over time. After an initial ramp-up period of K months, each portfolio should have exactly K active cohorts contributing to its monthly return.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#baseline-results-j6-k6-strategy",
    "href": "22_momentum.html#baseline-results-j6-k6-strategy",
    "title": "18  Momentum Strategies",
    "section": "18.5 Baseline Results: J=6, K=6 Strategy",
    "text": "18.5 Baseline Results: J=6, K=6 Strategy\n\n18.5.1 Summary Statistics by Momentum Decile\nWe now examine the average monthly returns for each momentum decile portfolio. Table 18.2 presents the mean, \\(t\\)-statistic, and \\(p\\)-value for each of the ten portfolios.\n\ndef compute_portfolio_stats(group):\n    \"\"\"Compute mean, t-stat, and p-value for a return series.\"\"\"\n    n = len(group)\n    mean_ret = group.mean()\n    std_ret = group.std()\n    t_stat = mean_ret / (std_ret / np.sqrt(n)) if std_ret &gt; 0 else np.nan\n    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1)) if not np.isnan(t_stat) else np.nan\n    return pd.Series({\n        \"N\": n,\n        \"Mean (%)\": mean_ret * 100,\n        \"Std (%)\": std_ret * 100,\n        \"t-stat\": t_stat,\n        \"p-value\": p_val\n    })\n\nmomentum_stats = (ewret\n    .groupby(\"momr\")[\"ewret\"]\n    .apply(compute_portfolio_stats)\n    .unstack()\n    .round(4)\n)\n\nmomentum_stats\n\n\n\nTable 18.2: Average monthly returns of momentum decile portfolios for the J=6, K=6 strategy. Portfolio 1 contains past losers and portfolio 10 contains past winners. The table reports the number of months, mean monthly return, t-statistic, and p-value for each decile.\n\n\n\n\n\n\n\n\n\n\nN\nMean (%)\nStd (%)\nt-stat\np-value\n\n\nmomr\n\n\n\n\n\n\n\n\n\n1\n161.0\n1.4190\n6.5319\n2.7565\n0.0065\n\n\n2\n161.0\n0.6602\n5.9895\n1.3985\n0.1639\n\n\n3\n161.0\n0.3658\n5.5080\n0.8427\n0.4007\n\n\n4\n161.0\n0.1623\n5.0085\n0.4112\n0.6815\n\n\n5\n161.0\n0.0111\n4.7561\n0.0297\n0.9763\n\n\n6\n161.0\n0.0408\n4.6864\n0.1104\n0.9122\n\n\n7\n161.0\n-0.0674\n4.8881\n-0.1749\n0.8614\n\n\n8\n161.0\n-0.2066\n4.9208\n-0.5329\n0.5949\n\n\n9\n161.0\n-0.2228\n5.3013\n-0.5332\n0.5946\n\n\n10\n161.0\n-0.6812\n5.7131\n-1.5130\n0.1323\n\n\n\n\n\n\n\n\n\n\n\n\n18.5.2 The Long-Short Momentum Portfolio\nThe key test of the momentum effect is whether the spread between winners and losers—the long-short momentum portfolio—generates statistically significant positive returns. We construct this spread portfolio by going long the top decile (portfolio 10) and short the bottom decile (portfolio 1).\n\n# Pivot to wide format\newret_wide = ewret.pivot(\n    index=\"date\", columns=\"momr\", values=\"ewret\"\n).reset_index()\n\newret_wide.columns = [\"date\"] + [f\"port{i}\" for i in range(1, 11)]\n\n# Compute long-short return\newret_wide[\"winners\"] = ewret_wide[\"port10\"]\newret_wide[\"losers\"] = ewret_wide[\"port1\"]\newret_wide[\"long_short\"] = ewret_wide[\"winners\"] - ewret_wide[\"losers\"]\n\n\nls_stats = pd.DataFrame()\nfor col in [\"winners\", \"losers\", \"long_short\"]:\n    series = ewret_wide[col].dropna()\n    n = len(series)\n    mean_val = series.mean()\n    std_val = series.std()\n    t_val = mean_val / (std_val / np.sqrt(n))\n    p_val = 2 * (1 - stats.t.cdf(abs(t_val), df=n-1))\n    ls_stats[col] = [n, mean_val * 100, std_val * 100, t_val, p_val]\n\nls_stats.index = [\"N\", \"Mean (%)\", \"Std (%)\", \"t-stat\", \"p-value\"]\nls_stats.columns = [\"Winners\", \"Losers\", \"Long-Short\"]\nls_stats = ls_stats.round(4)\nls_stats\n\n\n\nTable 18.3: Performance of the momentum long-short strategy (J=6, K=6). Winners is the top decile, Losers is the bottom decile, and Long-Short is Winners minus Losers. The table reports the number of months, mean monthly return, t-statistic, and p-value.\n\n\n\n\n\n\n\n\n\n\nWinners\nLosers\nLong-Short\n\n\n\n\nN\n161.0000\n161.0000\n161.0000\n\n\nMean (%)\n-0.6812\n1.4190\n-2.1002\n\n\nStd (%)\n5.7131\n6.5319\n4.8969\n\n\nt-stat\n-1.5130\n2.7565\n-5.4420\n\n\np-value\n0.1323\n0.0065\n0.0000\n\n\n\n\n\n\n\n\n\n\n\n\n18.5.3 Cumulative Returns\nThe time-series evolution of cumulative returns provides visual evidence of the momentum strategy’s performance. Figure 18.3 shows the cumulative returns of the winner and loser portfolios, while Figure 18.4 shows the cumulative return of the long-short momentum strategy.\n\n# Compute cumulative returns\newret_wide = ewret_wide.sort_values(\"date\").reset_index(drop=True)\n\nfor col in [\"winners\", \"losers\", \"long_short\"]:\n    ewret_wide[f\"cumret_{col}\"] = (1 + ewret_wide[col]).cumprod() - 1\n\n\ncumret_plot_data = (ewret_wide\n    [[\"date\", \"cumret_winners\", \"cumret_losers\"]]\n    .melt(id_vars=\"date\", var_name=\"portfolio\", value_name=\"cumret\")\n)\ncumret_plot_data[\"portfolio\"] = cumret_plot_data[\"portfolio\"].map({\n    \"cumret_winners\": \"Winners (P10)\",\n    \"cumret_losers\": \"Losers (P1)\"\n})\n\nplot_cumret = (\n    ggplot(cumret_plot_data, \n           aes(x=\"date\", y=\"cumret\", color=\"portfolio\")) +\n    geom_line(size=1) +\n    scale_y_continuous(labels=percent_format()) +\n    scale_color_manual(values=[\"#d62728\", \"#1f77b4\"]) +\n    labs(\n        x=\"\", y=\"Cumulative return\",\n        color=\"Portfolio\",\n        title=\"Cumulative Returns: Winners vs. Losers\"\n    ) +\n    theme_minimal() +\n    theme(\n        figure_size=(10, 6),\n        legend_position=\"bottom\"\n    )\n)\nplot_cumret\n\n\n\n\n\n\n\nFigure 18.3: Cumulative returns of momentum winner and loser portfolios. The winner portfolio (blue) contains stocks in the top decile of past 6-month returns, and the loser portfolio (red) contains stocks in the bottom decile. The spread between the two lines reflects the cumulative momentum premium.\n\n\n\n\n\n\nplot_ls = (\n    ggplot(ewret_wide, aes(x=\"date\", y=\"cumret_long_short\")) +\n    geom_line(size=1, color=\"#2ca02c\") +\n    geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\") +\n    scale_y_continuous(labels=percent_format()) +\n    labs(\n        x=\"\", y=\"Cumulative return\",\n        title=\"Cumulative Return: Long-Short Momentum Strategy\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 6))\n)\nplot_ls\n\n\n\n\n\n\n\nFigure 18.4: Cumulative return of the momentum long-short strategy (Winners minus Losers). Upward-sloping periods indicate momentum profits; sharp drawdowns often coincide with market reversals or crisis episodes.\n\n\n\n\n\n\n\n18.5.4 Monthly Return Distribution\nBeyond the mean, the full distribution of monthly long-short returns provides insight into the risk profile of the momentum strategy. Figure 39.1 shows the histogram of monthly returns.\n\nmean_ls = ewret_wide[\"long_short\"].mean()\n\nplot_dist = (\n    ggplot(ewret_wide, aes(x=\"long_short\")) +\n    geom_histogram(bins=50, fill=\"#1f77b4\", alpha=0.7) +\n    geom_vline(xintercept=mean_ls, linetype=\"dashed\", color=\"red\") +\n    scale_x_continuous(labels=percent_format()) +\n    labs(\n        x=\"Monthly long-short return\",\n        y=\"Frequency\",\n        title=\"Distribution of Monthly Momentum Returns\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_dist\n\n\n\n\n\n\n\nFigure 18.5: Distribution of monthly long-short momentum returns. The histogram shows the frequency distribution of monthly Winners-minus-Losers returns. The vertical dashed line indicates the mean monthly return.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#extending-to-multiple-formation-and-holding-periods",
    "href": "22_momentum.html#extending-to-multiple-formation-and-holding-periods",
    "title": "18  Momentum Strategies",
    "section": "18.6 Extending to Multiple Formation and Holding Periods",
    "text": "18.6 Extending to Multiple Formation and Holding Periods\n\n18.6.1 The J × K Grid\nJegadeesh and Titman (1993) evaluate momentum strategies across a comprehensive grid of formation periods \\(J \\in \\{3, 6, 9, 12\\}\\) and holding periods \\(K \\in \\{3, 6, 9, 12\\}\\). This produces 16 different strategy specifications, allowing us to assess the robustness of the momentum effect across different horizons.\nWe now implement a function that computes the full momentum strategy for any given \\((J, K)\\) pair, wrapping the steps developed above into a single reusable pipeline.\n\nfrom joblib import Parallel, delayed\nimport time\n\ndef momentum_strategy(data, J, K, n_portfolios=10):\n    \"\"\"\n    Implement the full Jegadeesh-Titman momentum strategy.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Panel of stock returns with columns: symbol, date, ret.\n    J : int\n        Formation period in months.\n    K : int\n        Holding period in months.\n    n_portfolios : int\n        Number of portfolios (default: 10 for deciles).\n    \n    Returns\n    -------\n    pd.DataFrame\n        Monthly returns for each momentum portfolio and the long-short spread.\n    \"\"\"\n    # Step 1: Compute formation period cumulative returns\n    df = data.sort_values([\"symbol\", \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[\"ret\"]\n    df[\"cum_return\"] = (\n        df.groupby(\"symbol\")[\"gross_ret\"]\n        .rolling(window=J, min_periods=J)\n        .apply(np.prod, raw=True)\n        .reset_index(level=0, drop=True)\n        - 1\n    )\n    df = df.drop(columns=[\"gross_ret\"]).dropna(subset=[\"cum_return\"])\n    \n    # Step 2: Assign to momentum portfolios using transform\n    df[\"momr\"] = df.groupby(\"date\", observed=True)[\"cum_return\"].transform(\n        lambda x: pd.qcut(\n            x,\n            q=n_portfolios,\n            labels=range(1, n_portfolios + 1),\n            duplicates=\"drop\"\n        )\n    ).astype('Int64')\n    \n    # If any NaNs in momr, fill with rank-based assignment\n    mask = df[\"momr\"].isna()\n    if mask.any():\n        df.loc[mask, \"momr\"] = df.loc[mask].groupby(\"date\")[\"cum_return\"].transform(\n            lambda x: pd.qcut(\n                x.rank(method=\"first\"),\n                q=min(n_portfolios, len(x.unique())),\n                labels=False,\n                duplicates=\"drop\"\n            )\n        ).astype('Int64') + 1\n    \n    # Step 3: Define holding period dates\n    df = df.rename(columns={\"date\": \"form_date\"})\n    df[\"hdate1\"] = df[\"form_date\"] + pd.offsets.MonthBegin(1)\n    df[\"hdate2\"] = df[\"form_date\"] + pd.offsets.MonthEnd(K)\n    \n    # Step 4: Merge with holding period returns\n    port_ret = df[[\"symbol\", \"form_date\", \"momr\", \"hdate1\", \"hdate2\"]].merge(\n        data[[\"symbol\", \"date\", \"ret\"]].rename(\n            columns={\"ret\": \"hret\", \"date\": \"hdate\"}\n        ),\n        on=\"symbol\",\n        how=\"inner\"\n    )\n    \n    # Use boolean indexing\n    port_ret = port_ret[\n        (port_ret[\"hdate\"] &gt;= port_ret[\"hdate1\"]) & \n        (port_ret[\"hdate\"] &lt;= port_ret[\"hdate2\"])\n    ]\n    \n    # Step 5: Compute equally weighted returns (two-stage averaging)\n    cohort_ret = (port_ret\n        .groupby([\"hdate\", \"momr\", \"form_date\"])\n        .agg(cohort_ret=(\"hret\", \"mean\"))\n        .reset_index()\n    )\n    \n    monthly_ret = (cohort_ret\n        .groupby([\"hdate\", \"momr\"])\n        .agg(ewret=(\"cohort_ret\", \"mean\"))\n        .reset_index()\n        .rename(columns={\"hdate\": \"date\"})\n    )\n    \n    # Step 6: Compute long-short spread\n    wide = monthly_ret.pivot(\n        index=\"date\", columns=\"momr\", values=\"ewret\"\n    ).reset_index()\n    \n    # Handle variable number of portfolios\n    n_cols = len(wide.columns) - 1\n    wide.columns = [\"date\"] + [f\"port{i}\" for i in range(1, n_cols + 1)]\n    wide[\"winners\"] = wide[f\"port{n_cols}\"]\n    wide[\"losers\"] = wide[\"port1\"]\n    wide[\"long_short\"] = wide[\"winners\"] - wide[\"losers\"]\n    \n    return monthly_ret, wide\n\n\n\n18.6.2 Running the Full Grid\nWe now run the momentum strategy for all 16 \\((J, K)\\) combinations. This is computationally intensive, so we store the key summary statistics for each specification.\n\n18.6.2.1 Sequential Calculation\n\nJ_values = [3, 6, 9, 12]\nK_values = [3, 6, 9, 12]\n\nresults_grid = []\n\nfor J_val, K_val in product(J_values, K_values):\n    print(f\"Computing J={J_val}, K={K_val}...\", end=\" \")\n    \n    try:\n        _, wide_result = momentum_strategy(prices_clean, J_val, K_val)\n        \n        for portfolio_name in [\"winners\", \"losers\", \"long_short\"]:\n            series = wide_result[portfolio_name].dropna()\n            n = len(series)\n            mean_ret = series.mean()\n            std_ret = series.std()\n            t_stat = mean_ret / (std_ret / np.sqrt(n)) if std_ret &gt; 0 else np.nan\n            p_val = (2 * (1 - stats.t.cdf(abs(t_stat), df=n-1)) \n                     if not np.isnan(t_stat) else np.nan)\n            \n            results_grid.append({\n                \"J\": J_val,\n                \"K\": K_val,\n                \"portfolio\": portfolio_name,\n                \"n_months\": n,\n                \"mean_ret\": mean_ret,\n                \"std_ret\": std_ret,\n                \"t_stat\": t_stat,\n                \"p_value\": p_val\n            })\n        \n        print(\"Done.\")\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nresults_df = pd.DataFrame(results_grid)\n\n\n\n18.6.2.2 Parallel Calculation\n\ndef compute_single_strategy(data, J, K):\n    \"\"\"\n    Compute statistics for a single (J, K) strategy.\n    Returns a list of result dicts, one per portfolio.\n    \"\"\"\n    try:\n        _, wide_result = momentum_strategy(data, J, K)\n        \n        results = []\n        for portfolio_name in [\"winners\", \"losers\", \"long_short\"]:\n            series = wide_result[portfolio_name].dropna()\n            n = len(series)\n            mean_ret = series.mean()\n            std_ret = series.std()\n            t_stat = mean_ret / (std_ret / np.sqrt(n)) if std_ret &gt; 0 else np.nan\n            p_val = (2 * (1 - stats.t.cdf(abs(t_stat), df=n-1)) \n                     if not np.isnan(t_stat) else np.nan)\n            \n            results.append({\n                \"J\": J,\n                \"K\": K,\n                \"portfolio\": portfolio_name,\n                \"n_months\": n,\n                \"mean_ret\": mean_ret,\n                \"std_ret\": std_ret,\n                \"t_stat\": t_stat,\n                \"p_value\": p_val\n            })\n        return results\n    except Exception as e:\n        print(f\"Error in J={J}, K={K}: {e}\")\n        return []\n\n\nJ_values = [3, 6, 9, 12]\nK_values = [3, 6, 9, 12]\n\n# Create list of (J, K) pairs\nparams = list(product(J_values, K_values))\n\nprint(f\"Running {len(params)} momentum strategies in parallel with 4 cores...\")\nstart_time = time.time()\n\n# Parallel execution with 4 workers\nresults_list = Parallel(n_jobs=4, verbose=10)(\n    delayed(compute_single_strategy)(prices_clean, J, K)\n    for J, K in params\n)\n\n# Flatten results\nresults_grid = [item for sublist in results_list for item in sublist]\nresults_df = pd.DataFrame(results_grid)\n\nelapsed = time.time() - start_time\nprint(f\"\\nCompleted in {elapsed:.2f} seconds\")\n\nRunning 16 momentum strategies in parallel with 4 cores...\n\n\n[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    8.4s\n[Parallel(n_jobs=4)]: Done  11 out of  16 | elapsed:   14.4s remaining:    6.5s\n[Parallel(n_jobs=4)]: Done  13 out of  16 | elapsed:   18.7s remaining:    4.3s\n\n\n\nCompleted in 19.68 seconds\n\n\n[Parallel(n_jobs=4)]: Done  16 out of  16 | elapsed:   19.6s finished\n\n\n\n# Summary statistics\nprint(\"\\nResults Summary by Formation Period:\")\nprint(results_df.groupby(\"J\").agg({\n    \"mean_ret\": \"mean\",\n    \"std_ret\": \"mean\",\n    \"t_stat\": [\"mean\", lambda x: (x.abs() &gt; 1.96).sum()],\n    \"n_months\": \"first\"\n}).round(6))\n\n\nResults Summary by Formation Period:\n    mean_ret   std_ret    t_stat            n_months\n        mean      mean      mean &lt;lambda_0&gt;    first\nJ                                                   \n6  -0.004661  0.055672 -1.517507          9      161\n9  -0.003265  0.056520 -1.071904          9      158\n12 -0.002759  0.058194 -0.931273          9      155\n\n\n\n\n\n18.6.3 Winners Portfolio Returns\nTable 18.4 presents the average monthly returns of the winner portfolio (portfolio 10) across all \\((J, K)\\) combinations.\n\nwinners_grid = (results_df\n    .query(\"portfolio == 'winners'\")\n    .assign(\n        display=lambda x: (\n            x[\"mean_ret\"].apply(lambda v: f\"{v*100:.2f}\") + \n            \"\\n(\" + x[\"t_stat\"].apply(lambda v: f\"{v:.2f}\") + \")\"\n        )\n    )\n    .pivot(index=\"J\", columns=\"K\", values=\"display\")\n)\nwinners_grid.columns = [f\"K={k}\" for k in winners_grid.columns]\nwinners_grid.index = [f\"J={j}\" for j in winners_grid.index]\nwinners_grid\n\n\n\nTable 18.4: Average monthly returns (%) of the winner portfolio across formation periods (J) and holding periods (K). t-statistics are reported in parentheses.\n\n\n\n\n\n\n\n\n\n\nK=3\nK=6\nK=9\nK=12\n\n\n\n\nJ=6\n-1.18\\n(-2.65)\n-0.68\\n(-1.51)\n-0.55\\n(-1.23)\n-0.39\\n(-0.87)\n\n\nJ=9\n-1.00\\n(-2.36)\n-0.51\\n(-1.22)\n-0.28\\n(-0.68)\n-0.16\\n(-0.39)\n\n\nJ=12\n-0.88\\n(-2.09)\n-0.39\\n(-0.91)\n-0.24\\n(-0.56)\n-0.14\\n(-0.34)\n\n\n\n\n\n\n\n\n\n\n\n\n18.6.4 Losers Portfolio Returns\nTable 18.5 presents the corresponding results for the loser portfolio.\n\nlosers_grid = (results_df\n    .query(\"portfolio == 'losers'\")\n    .assign(\n        display=lambda x: (\n            x[\"mean_ret\"].apply(lambda v: f\"{v*100:.2f}\") + \n            \"\\n(\" + x[\"t_stat\"].apply(lambda v: f\"{v:.2f}\") + \")\"\n        )\n    )\n    .pivot(index=\"J\", columns=\"K\", values=\"display\")\n)\nlosers_grid.columns = [f\"K={k}\" for k in losers_grid.columns]\nlosers_grid.index = [f\"J={j}\" for j in losers_grid.index]\nlosers_grid\n\n\n\nTable 18.5: Average monthly returns (%) of the loser portfolio across formation periods (J) and holding periods (K). t-statistics are reported in parentheses.\n\n\n\n\n\n\n\n\n\n\nK=3\nK=6\nK=9\nK=12\n\n\n\n\nJ=6\n1.87\\n(3.49)\n1.42\\n(2.76)\n1.10\\n(2.19)\n0.99\\n(1.99)\n\n\nJ=9\n1.94\\n(3.51)\n1.38\\n(2.56)\n1.21\\n(2.31)\n1.15\\n(2.22)\n\n\nJ=12\n1.57\\n(2.71)\n1.29\\n(2.28)\n1.19\\n(2.13)\n1.15\\n(2.09)\n\n\n\n\n\n\n\n\n\n\n\n\n18.6.5 Long-Short Momentum Returns\nTable 18.6 presents the most important results: the average monthly returns of the long-short momentum portfolio across all specifications.\n\nls_grid = (results_df\n    .query(\"portfolio == 'long_short'\")\n    .assign(\n        display=lambda x: (\n            x[\"mean_ret\"].apply(lambda v: f\"{v*100:.2f}\") + \n            \"\\n(\" + x[\"t_stat\"].apply(lambda v: f\"{v:.2f}\") + \")\"\n        )\n    )\n    .pivot(index=\"J\", columns=\"K\", values=\"display\")\n)\nls_grid.columns = [f\"K={k}\" for k in ls_grid.columns]\nls_grid.index = [f\"J={j}\" for j in ls_grid.index]\nls_grid\n\n\n\nTable 18.6: Average monthly returns (%) of the momentum long-short portfolio (Winners minus Losers) across formation periods (J) and holding periods (K). t-statistics are reported in parentheses. This table replicates the format of Table 1 in Jegadeesh and Titman (1993).\n\n\n\n\n\n\n\n\n\n\nK=3\nK=6\nK=9\nK=12\n\n\n\n\nJ=6\n-3.04\\n(-7.38)\n-2.10\\n(-5.44)\n-1.65\\n(-4.94)\n-1.37\\n(-4.61)\n\n\nJ=9\n-2.94\\n(-6.41)\n-1.89\\n(-4.55)\n-1.49\\n(-3.98)\n-1.31\\n(-3.87)\n\n\nJ=12\n-2.45\\n(-5.42)\n-1.68\\n(-3.92)\n-1.43\\n(-3.60)\n-1.30\\n(-3.55)\n\n\n\n\n\n\n\n\n\n\n\n\n18.6.6 Visualizing the Momentum Premium Across Specifications\nFigure 18.6 provides a visual summary of the long-short momentum premium across all \\((J, K)\\) combinations.\n\nls_heatmap_data = (results_df\n    .query(\"portfolio == 'long_short'\")\n    .assign(\n        mean_pct=lambda x: x[\"mean_ret\"] * 100,\n        significant=lambda x: x[\"p_value\"] &lt; 0.05,\n        label=lambda x: x.apply(\n            lambda row: f\"{row['mean_ret']*100:.2f}{'*' if row['p_value'] &lt; 0.05 else ''}\", \n            axis=1\n        )\n    )\n)\n\nplot_heatmap = (\n    ggplot(ls_heatmap_data, \n           aes(x=\"K.astype(str)\", y=\"J.astype(str)\", fill=\"mean_pct\")) +\n    geom_tile(color=\"white\", size=2) +\n    geom_text(aes(label=\"label\"), size=10, color=\"white\") +\n    scale_fill_gradient2(\n        low=\"#d62728\", mid=\"#f7f7f7\", high=\"#1f77b4\", midpoint=0,\n        name=\"Monthly\\nReturn (%)\"\n    ) +\n    labs(\n        x=\"Holding Period (K months)\",\n        y=\"Formation Period (J months)\",\n        title=\"Momentum Premium Across J×K Specifications\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(8, 6))\n)\nplot_heatmap\n\n\n\n\n\n\n\nFigure 18.6: Heatmap of average monthly long-short momentum returns (%) across formation periods (J) and holding periods (K). Darker shades indicate higher momentum profits. Asterisks denote statistical significance at the 5% level.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#risk-adjusted-performance",
    "href": "22_momentum.html#risk-adjusted-performance",
    "title": "18  Momentum Strategies",
    "section": "18.7 Risk-Adjusted Performance",
    "text": "18.7 Risk-Adjusted Performance\n\n18.7.1 CAPM Alpha\nA natural question is whether the momentum premium is explained by exposure to the market factor. We estimate the CAPM alpha of the long-short momentum portfolio by regressing its returns on the market excess return:\n\\[\nr_{\\text{WML},t} = \\alpha + \\beta \\cdot r_{\\text{MKT},t} + \\epsilon_t\n\\tag{18.3}\\]\n\n# Merge momentum returns with factor data (baseline J=6, K=6)\newret_factors = (ewret_wide\n    [[\"date\", \"winners\", \"losers\", \"long_short\"]]\n    .merge(factors_ff3_monthly, on=\"date\", how=\"inner\")\n)\n\n# CAPM regression for long-short portfolio\nfrom statsmodels.formula.api import ols as ols_formula\nimport statsmodels.api as sm\n\nX_capm = sm.add_constant(ewret_factors[\"mkt_excess\"])\ny_ls = ewret_factors[\"long_short\"]\n\ncapm_model = sm.OLS(y_ls, X_capm).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n\nprint(\"CAPM Regression: Long-Short Momentum Returns\")\nprint(\"=\" * 60)\nprint(f\"Alpha (monthly):  {capm_model.params['const']*100:.4f}% \"\n      f\"(t={capm_model.tvalues['const']:.2f})\")\nprint(f\"Market Beta:      {capm_model.params['mkt_excess']:.4f} \"\n      f\"(t={capm_model.tvalues['mkt_excess']:.2f})\")\nprint(f\"R-squared:        {capm_model.rsquared:.4f}\")\nprint(f\"N observations:   {capm_model.nobs:.0f}\")\n\nCAPM Regression: Long-Short Momentum Returns\n============================================================\nAlpha (monthly):  -2.2703% (t=-5.05)\nMarket Beta:      -0.1779 (t=-1.75)\nR-squared:        0.0470\nN observations:   150\n\n\n\n\n18.7.2 Fama-French Three-Factor Alpha\nWe extend the risk adjustment to the Fama-French three-factor model, which includes the size (SMB) and value (HML) factors in addition to the market factor:\n\\[\nr_{\\text{WML},t} = \\alpha + \\beta_1 \\cdot r_{\\text{MKT},t} + \\beta_2 \\cdot \\text{SMB}_t + \\beta_3 \\cdot \\text{HML}_t + \\epsilon_t\n\\tag{18.4}\\]\n\nX_ff3 = sm.add_constant(\n    ewret_factors[[\"mkt_excess\", \"smb\", \"hml\"]]\n)\ny_ls = ewret_factors[\"long_short\"]\n\nff3_model = sm.OLS(y_ls, X_ff3).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n\n# Display results as a clean table\nff3_results = pd.DataFrame({\n    \"Coefficient\": ff3_model.params,\n    \"Std Error\": ff3_model.bse,\n    \"t-stat\": ff3_model.tvalues,\n    \"p-value\": ff3_model.pvalues\n}).round(4)\n\nff3_results.index = [\"Alpha\", \"MKT\", \"SMB\", \"HML\"]\nff3_results\n\n\n\nTable 18.7: Fama-French three-factor regression for the momentum long-short portfolio. The dependent variable is the monthly return of the Winners-minus-Losers portfolio. Alpha is the intercept, representing the risk-adjusted abnormal return. HAC standard errors with 6 lags are used.\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd Error\nt-stat\np-value\n\n\n\n\nAlpha\n-0.0234\n0.0041\n-5.7189\n0.0000\n\n\nMKT\n-0.1269\n0.1558\n-0.8145\n0.4154\n\n\nSMB\n0.1123\n0.1882\n0.5969\n0.5506\n\n\nHML\n0.0716\n0.1414\n0.5065\n0.6125\n\n\n\n\n\n\n\n\n\n\n\nprint(f\"\\nR-squared: {ff3_model.rsquared:.4f}\")\nprint(f\"Adjusted R-squared: {ff3_model.rsquared_adj:.4f}\")\nprint(f\"Alpha (annualized): {ff3_model.params['const'] * 12 * 100:.2f}%\")\n\n\nR-squared: 0.0553\nAdjusted R-squared: 0.0359\nAlpha (annualized): -28.02%\n\n\n\n\n18.7.3 Interpretation of Risk Exposures\nThe factor loadings from the three-factor regression reveal the risk characteristics of the momentum strategy in the Vietnamese market. Several patterns are commonly observed:\n\nMarket beta: Momentum portfolios typically have moderate market exposure. In the U.S., Grundy and Martin (2001) document that the market beta of the long-short portfolio is close to zero on average but highly time-varying, spiking during market reversals.\nSize exposure (SMB): Momentum strategies often load positively on the size factor, reflecting the tendency for smaller stocks to exhibit stronger momentum patterns.\nValue exposure (HML): The long-short momentum portfolio typically loads negatively on HML, indicating that winners tend to be growth stocks while losers tend to be value stocks. This creates a natural tension between momentum and value strategies.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#momentum-and-market-states",
    "href": "22_momentum.html#momentum-and-market-states",
    "title": "18  Momentum Strategies",
    "section": "18.8 Momentum and Market States",
    "text": "18.8 Momentum and Market States\n\n18.8.1 Conditional Performance\nAn important finding in the momentum literature is that momentum profits vary with market conditions. Cooper, Gutierrez Jr, and Hameed (2004) document that momentum strategies perform well following market gains (UP markets) but experience severe losses following market declines (DOWN markets). This asymmetry is particularly relevant for emerging markets, which experience more extreme market states.\nWe define market states based on the cumulative market return over the prior 12 months:\n\\[\n\\text{Market State}_t = \\begin{cases} \\text{UP} & \\text{if } \\prod_{s=t-12}^{t-1}(1 + r_{m,s}) - 1 &gt; 0 \\\\ \\text{DOWN} & \\text{otherwise} \\end{cases}\n\\tag{18.5}\\]\n\n# Compute 12-month lagged market return\nmarket_returns = factors_ff3_monthly[[\"date\", \"mkt_excess\"]].copy()\nmarket_returns = market_returns.sort_values(\"date\")\nmarket_returns[\"mkt_cum_12m\"] = (\n    (1 + market_returns[\"mkt_excess\"])\n    .rolling(window=12, min_periods=12)\n    .apply(np.prod, raw=True)\n    - 1\n)\nmarket_returns[\"market_state\"] = np.where(\n    market_returns[\"mkt_cum_12m\"] &gt; 0, \"UP\", \"DOWN\"\n)\n\n# Merge with momentum returns\newret_states = ewret_wide[[\"date\", \"long_short\"]].merge(\n    market_returns[[\"date\", \"market_state\"]], on=\"date\", how=\"inner\"\n).dropna()\n\n\nstate_stats = []\nfor state in [\"UP\", \"DOWN\"]:\n    subset = ewret_states.query(f\"market_state == '{state}'\")[\"long_short\"]\n    n = len(subset)\n    mean_ret = subset.mean()\n    std_ret = subset.std()\n    t_stat = mean_ret / (std_ret / np.sqrt(n)) if std_ret &gt; 0 else np.nan\n    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1)) if not np.isnan(t_stat) else np.nan\n    state_stats.append({\n        \"Market State\": state,\n        \"N Months\": n,\n        \"Mean (%)\": mean_ret * 100,\n        \"Std (%)\": std_ret * 100,\n        \"t-stat\": t_stat,\n        \"p-value\": p_val\n    })\n\nstate_stats_df = pd.DataFrame(state_stats).round(4)\nstate_stats_df\n\n\n\nTable 18.8: Momentum long-short returns conditional on market states. UP markets are defined as periods where the cumulative market return over the prior 12 months is positive; DOWN markets are periods with negative prior 12-month returns.\n\n\n\n\n\n\n\n\n\n\nMarket State\nN Months\nMean (%)\nStd (%)\nt-stat\np-value\n\n\n\n\n0\nUP\n39\n-1.1972\n4.5814\n-1.6320\n0.1109\n\n\n1\nDOWN\n111\n-2.4050\n4.8669\n-5.2063\n0.0000\n\n\n\n\n\n\n\n\n\n\n\nplot_states = (\n    ggplot(ewret_states, aes(x=\"market_state\", y=\"long_short\", \n                              fill=\"market_state\")) +\n    geom_boxplot(alpha=0.7) +\n    geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\") +\n    scale_y_continuous(labels=percent_format()) +\n    scale_fill_manual(values={\"UP\": \"#1f77b4\", \"DOWN\": \"#d62728\"}) +\n    labs(\n        x=\"Market State (Prior 12-Month Return)\",\n        y=\"Monthly Long-Short Return\",\n        title=\"Momentum Returns by Market State\"\n    ) +\n    theme_minimal() +\n    theme(\n        figure_size=(8, 6),\n        legend_position=\"none\"\n    )\n)\nplot_states\n\n\n\n\n\n\n\nFigure 18.7: Distribution of monthly momentum returns by market state. The box plots show the distribution of long-short momentum returns separately for UP and DOWN market states, defined by the sign of the prior 12-month cumulative market return.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#momentum-crashes",
    "href": "22_momentum.html#momentum-crashes",
    "title": "18  Momentum Strategies",
    "section": "18.9 Momentum Crashes",
    "text": "18.9 Momentum Crashes\n\n18.9.1 Understanding Momentum Drawdowns\nOne of the most important risk characteristics of momentum strategies is their susceptibility to sudden, severe losses—known as momentum crashes. Daniel and Moskowitz (2016) document that momentum strategies experience infrequent but extreme losses, typically during market rebounds following bear markets. These crashes occur because the loser portfolio, which has been short, is heavily loaded with high-beta stocks that surge when markets reverse.\nWe identify the worst drawdowns of the momentum strategy and examine their market context.\n\nworst_months = (ewret_wide\n    [[\"date\", \"long_short\", \"winners\", \"losers\"]]\n    .merge(factors_ff3_monthly[[\"date\", \"mkt_excess\"]], on=\"date\", how=\"left\")\n    .sort_values(\"long_short\")\n    .head(10)\n    .assign(\n        long_short_pct=lambda x: (x[\"long_short\"] * 100).round(2),\n        winners_pct=lambda x: (x[\"winners\"] * 100).round(2),\n        losers_pct=lambda x: (x[\"losers\"] * 100).round(2),\n        mkt_pct=lambda x: (x[\"mkt_excess\"] * 100).round(2)\n    )\n    [[\"date\", \"long_short_pct\", \"winners_pct\", \"losers_pct\", \"mkt_pct\"]]\n    .rename(columns={\n        \"date\": \"Date\",\n        \"long_short_pct\": \"L/S (%)\",\n        \"winners_pct\": \"Winners (%)\",\n        \"losers_pct\": \"Losers (%)\",\n        \"mkt_pct\": \"Market (%)\"\n    })\n)\nworst_months\n\n\n\nTable 18.9: Ten worst months for the momentum long-short strategy. The table reports the date, long-short return, winner return, loser return, and concurrent market return for the ten months with the largest momentum losses.\n\n\n\n\n\n\n\n\n\n\nDate\nL/S (%)\nWinners (%)\nLosers (%)\nMarket (%)\n\n\n\n\n153\n2023-05-31\n-16.39\n-2.69\n13.70\n2.78\n\n\n39\n2013-11-30\n-14.26\n4.24\n18.50\n2.57\n\n\n20\n2012-04-30\n-13.68\n1.51\n15.19\n4.88\n\n\n78\n2017-02-28\n-13.56\n2.10\n15.67\n1.49\n\n\n107\n2019-07-31\n-13.15\n-2.46\n10.69\n1.61\n\n\n140\n2022-04-30\n-11.71\n-17.57\n-5.87\n-11.19\n\n\n149\n2023-01-31\n-11.53\n1.37\n12.90\n6.96\n\n\n28\n2012-12-31\n-11.52\n4.03\n15.54\n-1.61\n\n\n0\n2010-08-31\n-11.31\n-25.03\n-13.72\nNaN\n\n\n10\n2011-06-30\n-10.44\n-3.15\n7.29\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n18.9.2 Maximum Drawdown Analysis\nThe maximum drawdown provides a measure of the worst peak-to-trough decline experienced by the strategy. This metric is particularly relevant for practitioners evaluating the risk of momentum strategies.\n\n# Compute running maximum and drawdown\newret_wide[\"cum_wealth\"] = (1 + ewret_wide[\"long_short\"]).cumprod()\newret_wide[\"running_max\"] = ewret_wide[\"cum_wealth\"].cummax()\newret_wide[\"drawdown\"] = (\n    ewret_wide[\"cum_wealth\"] / ewret_wide[\"running_max\"] - 1\n)\n\nmax_dd = ewret_wide[\"drawdown\"].min()\nmax_dd_date = ewret_wide.loc[ewret_wide[\"drawdown\"].idxmin(), \"date\"]\n\nprint(f\"Maximum drawdown: {max_dd*100:.2f}%\")\nprint(f\"Date of maximum drawdown: {max_dd_date.date()}\")\n\nMaximum drawdown: -97.08%\nDate of maximum drawdown: 2023-10-31\n\n\n\nplot_dd = (\n    ggplot(ewret_wide, aes(x=\"date\", y=\"drawdown\")) +\n    geom_area(fill=\"#d62728\", alpha=0.5) +\n    geom_line(color=\"#d62728\", size=0.5) +\n    scale_y_continuous(labels=percent_format()) +\n    labs(\n        x=\"\", y=\"Drawdown\",\n        title=\"Momentum Strategy Drawdown\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_dd\n\n\n\n\n\n\n\nFigure 18.8: Drawdown of the momentum long-short strategy over time. The chart shows the percentage decline from the previous peak in cumulative wealth. Deeper drawdowns represent more severe momentum crashes.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#value-weighted-momentum-portfolios",
    "href": "22_momentum.html#value-weighted-momentum-portfolios",
    "title": "18  Momentum Strategies",
    "section": "18.10 Value-Weighted Momentum Portfolios",
    "text": "18.10 Value-Weighted Momentum Portfolios\nThe baseline Jegadeesh and Titman (1993) implementation uses equally weighted portfolios. However, equally weighted returns can be dominated by small, illiquid stocks that may be difficult to trade in practice. Value-weighted portfolios, where each stock’s contribution is proportional to its market capitalization, provide a more investable benchmark and are more representative of the returns that large investors could actually achieve.\n\ndef momentum_strategy_vw(data, J, K, n_portfolios=10):\n    \"\"\"\n    Value-weighted momentum strategy implementation.\n    \n    Same as the equally-weighted version but uses lagged market \n    capitalization as weights when computing portfolio returns.\n    \n    Parameters\n    ----------\n    data : pd.DataFrame\n        Panel with columns: symbol, date, ret, mktcap_lag.\n    J : int\n        Formation period in months.\n    K : int\n        Holding period in months.\n    n_portfolios : int\n        Number of portfolios.\n    \n    Returns\n    -------\n    pd.DataFrame\n        Monthly value-weighted portfolio returns.\n    \"\"\"\n    # Step 1: Formation period returns\n    df = data.sort_values([\"symbol\", \"date\"]).copy()\n    df[\"gross_ret\"] = 1 + df[\"ret\"]\n    df[\"cum_return\"] = (\n        df.groupby(\"symbol\")[\"gross_ret\"]\n        .rolling(window=J, min_periods=J)\n        .apply(np.prod, raw=True)\n        .reset_index(level=0, drop=True)\n        - 1\n    )\n    df = df.drop(columns=[\"gross_ret\"]).dropna(subset=[\"cum_return\"])\n    \n    # Step 2: Portfolio assignment using transform (fast)\n    df[\"momr\"] = df.groupby(\"date\", observed=True)[\"cum_return\"].transform(\n        lambda x: pd.qcut(\n            x,\n            q=n_portfolios,\n            labels=range(1, n_portfolios + 1),\n            duplicates=\"drop\"\n        )\n    ).astype('Int64')\n    \n    # Fill NaNs with rank-based assignment\n    mask = df[\"momr\"].isna()\n    if mask.any():\n        df.loc[mask, \"momr\"] = df.loc[mask].groupby(\"date\")[\"cum_return\"].transform(\n            lambda x: pd.qcut(\n                x.rank(method=\"first\"),\n                q=min(n_portfolios, len(x.unique())),\n                labels=False,\n                duplicates=\"drop\"\n            )\n        ).astype('Int64') + 1\n    \n    # Step 3: Holding period\n    df = df.rename(columns={\"date\": \"form_date\"})\n    df[\"hdate1\"] = df[\"form_date\"] + pd.offsets.MonthBegin(1)\n    df[\"hdate2\"] = df[\"form_date\"] + pd.offsets.MonthEnd(K)\n    \n    # Step 4: Merge with holding period returns AND weights\n    port_ret = df[\n        [\"symbol\", \"form_date\", \"momr\", \"hdate1\", \"hdate2\"]\n    ].merge(\n        data[[\"symbol\", \"date\", \"ret\", \"mktcap_lag\"]].rename(\n            columns={\"ret\": \"hret\", \"date\": \"hdate\"}\n        ),\n        on=\"symbol\",\n        how=\"inner\"\n    )\n    \n    # Use boolean indexing instead of query (faster)\n    port_ret = port_ret[\n        (port_ret[\"hdate\"] &gt;= port_ret[\"hdate1\"]) & \n        (port_ret[\"hdate\"] &lt;= port_ret[\"hdate2\"])\n    ]\n    port_ret = port_ret.dropna(subset=[\"mktcap_lag\"])\n    port_ret = port_ret[port_ret[\"mktcap_lag\"] &gt; 0]\n    \n    # Step 5: Value-weighted returns within each cohort\n    def vw_mean(group):\n        weights = group[\"mktcap_lag\"]\n        if weights.sum() == 0:\n            return np.nan\n        return np.average(group[\"hret\"], weights=weights)\n    \n    cohort_ret = (port_ret\n        .groupby([\"hdate\", \"momr\", \"form_date\"])\n        .apply(vw_mean, include_groups=False)\n        .reset_index(name=\"cohort_ret\")\n    )\n    \n    monthly_ret = (cohort_ret\n        .groupby([\"hdate\", \"momr\"])\n        .agg(vwret=(\"cohort_ret\", \"mean\"))\n        .reset_index()\n        .rename(columns={\"hdate\": \"date\"})\n    )\n    \n    # Step 6: Long-short\n    wide = monthly_ret.pivot(\n        index=\"date\", columns=\"momr\", values=\"vwret\"\n    ).reset_index()\n    \n    # Handle variable number of portfolios\n    n_cols = len(wide.columns) - 1\n    wide.columns = [\"date\"] + [f\"port{i}\" for i in range(1, n_cols + 1)]\n    wide[\"winners\"] = wide[f\"port{n_cols}\"]\n    wide[\"losers\"] = wide[\"port1\"]\n    wide[\"long_short\"] = wide[\"winners\"] - wide[\"losers\"]\n    \n    return monthly_ret, wide\n\n\n# Run value-weighted J=6, K=6 strategy\n_, vw_results = momentum_strategy_vw(prices_clean, J=6, K=6)\n\nprint(\"Value-Weighted Momentum Strategy (J=6, K=6)\")\nprint(\"=\" * 50)\nprint(\"\\nPortfolio Statistics:\")\nprint(vw_results[[\"winners\", \"losers\", \"long_short\"]].describe().round(4))\n\nValue-Weighted Momentum Strategy (J=6, K=6)\n==================================================\n\nPortfolio Statistics:\n        winners    losers  long_short\ncount  161.0000  161.0000    161.0000\nmean    -0.0129    0.0006     -0.0135\nstd      0.0763    0.0713      0.0653\nmin     -0.2519   -0.2178     -0.3813\n25%     -0.0540   -0.0397     -0.0465\n50%     -0.0067    0.0010     -0.0126\n75%      0.0354    0.0443      0.0173\nmax      0.1765    0.2498      0.2498\n\n\n\ncomparison = []\nfor scheme, df in [(\"EW\", ewret_wide), (\"VW\", vw_results)]:\n    for col in [\"winners\", \"losers\", \"long_short\"]:\n        series = df[col].dropna()\n        n = len(series)\n        mean_ret = series.mean()\n        std_ret = series.std()\n        t_stat = mean_ret / (std_ret / np.sqrt(n))\n        p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df=n-1))\n        comparison.append({\n            \"Weighting\": scheme,\n            \"Portfolio\": col.replace(\"_\", \" \").title(),\n            \"Mean (%)\": round(mean_ret * 100, 4),\n            \"Std (%)\": round(std_ret * 100, 4),\n            \"t-stat\": round(t_stat, 2),\n            \"p-value\": round(p_val, 4)\n        })\n\npd.DataFrame(comparison)\n\n\n\nTable 18.10: Comparison of equally weighted (EW) and value-weighted (VW) momentum strategies for J=6, K=6. The table reports mean monthly returns, t-statistics, and p-values for winners, losers, and long-short portfolios under both weighting schemes.\n\n\n\n\n\n\n\n\n\n\nWeighting\nPortfolio\nMean (%)\nStd (%)\nt-stat\np-value\n\n\n\n\n0\nEW\nWinners\n-0.6812\n5.7131\n-1.51\n0.1323\n\n\n1\nEW\nLosers\n1.4190\n6.5319\n2.76\n0.0065\n\n\n2\nEW\nLong Short\n-2.1002\n4.8969\n-5.44\n0.0000\n\n\n3\nVW\nWinners\n-1.2943\n7.6274\n-2.15\n0.0328\n\n\n4\nVW\nLosers\n0.0589\n7.1275\n0.10\n0.9166\n\n\n5\nVW\nLong Short\n-1.3533\n6.5312\n-2.63\n0.0094\n\n\n\n\n\n\n\n\n\n\n\nvw_results = vw_results.sort_values(\"date\")\nvw_results[\"cumret_ls_vw\"] = (1 + vw_results[\"long_short\"]).cumprod() - 1\n\new_data = (ewret_wide[[\"date\", \"long_short\"]]\n    .rename(columns={\"long_short\": \"cumret\"})\n    .assign(scheme=\"Equally Weighted\")\n)\new_data[\"cumret\"] = (1 + ew_data[\"cumret\"]).cumprod() - 1\n\nvw_data = (vw_results[[\"date\", \"cumret_ls_vw\"]]\n    .rename(columns={\"cumret_ls_vw\": \"cumret\"})\n    .assign(scheme=\"Value Weighted\")\n)\n\new_vs_vw = pd.concat([ew_data, vw_data], ignore_index=True)\n\nplot_ew_vw = (\n    ggplot(ew_vs_vw, aes(x=\"date\", y=\"cumret\", color=\"scheme\")) +\n    geom_line(size=1) +\n    scale_y_continuous(labels=percent_format()) +\n    scale_color_manual(values=[\"#1f77b4\", \"#ff7f0e\"]) +\n    labs(\n        x=\"\", y=\"Cumulative return\",\n        color=\"Weighting Scheme\",\n        title=\"EW vs. VW Momentum Long-Short Strategy\"\n    ) +\n    theme_minimal() +\n    theme(\n        figure_size=(10, 6),\n        legend_position=\"bottom\"\n    )\n)\nplot_ew_vw\n\n\n\n\n\n\n\nFigure 18.9: Cumulative returns of equally weighted (EW) versus value-weighted (VW) long-short momentum strategies. Differences between the two lines reflect the impact of firm size on momentum profitability.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#daily-momentum-analysis",
    "href": "22_momentum.html#daily-momentum-analysis",
    "title": "18  Momentum Strategies",
    "section": "18.11 Daily Momentum Analysis",
    "text": "18.11 Daily Momentum Analysis\nWhile momentum strategies are typically evaluated at the monthly frequency following Jegadeesh and Titman (1993), analyzing daily return patterns provides additional insights into the dynamics of momentum profits. Daily data allows us to examine how momentum profits accrue within the holding period, measure intra-month volatility of the strategy, and compute more precise risk measures.\n\n18.11.1 Loading Daily Data\n\nprices_daily = pd.read_sql_query(\n    sql=(\"SELECT symbol, date, ret, ret_excess, mktcap_lag \"\n         \"FROM prices_daily\"),\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nprint(f\"Daily observations: {len(prices_daily):,}\")\nprint(f\"Unique stocks: {prices_daily['symbol'].nunique():,}\")\nprint(f\"Date range: {prices_daily['date'].min().date()} \"\n      f\"to {prices_daily['date'].max().date()}\")\n\nDaily observations: 3,462,157\nUnique stocks: 1,459\nDate range: 2010-01-05 to 2023-12-29\n\n\n\n\n18.11.2 Daily Returns of Monthly Momentum Portfolios\nRather than forming momentum portfolios at the daily frequency (which would require daily rebalancing and is impractical), we use the monthly portfolio assignments and track their daily returns. This gives us the daily return series of the monthly momentum strategy.\n\n# # # Use the monthly portfolio assignments from the J=6 baseline\nmonthly_assignments = portfolios_with_dates[\n    [\"symbol\", \"form_date\", \"momr\", \"hdate1\", \"hdate2\"]\n].copy()\n\n# Merge with daily returns\ndaily_mom_returns = monthly_assignments.merge(\n    prices_daily[[\"symbol\", \"date\", \"ret\"]].rename(\n        columns={\"ret\": \"dret\", \"date\": \"ddate\"}\n    ),\n    on=\"symbol\",\n    how=\"inner\"\n)\n\n# Use boolean indexing instead of query\ndaily_mom_returns = daily_mom_returns[\n    (daily_mom_returns[\"ddate\"] &gt;= daily_mom_returns[\"hdate1\"]) & \n    (daily_mom_returns[\"ddate\"] &lt;= daily_mom_returns[\"hdate2\"])\n]\n\nprint(f\"Daily portfolio return observations: {len(daily_mom_returns):,}\")\n\nDaily portfolio return observations: 19,112,536\n\n\n\n# Compute daily equally weighted portfolio returns\n# Stage 1: Average within each cohort\ndaily_cohort_ret = (daily_mom_returns\n    .groupby([\"ddate\", \"momr\", \"form_date\"])\n    .agg(cohort_ret=(\"dret\", \"mean\"))\n    .reset_index()\n)\n\n# Stage 2: Average across cohorts\ndaily_ewret = (daily_cohort_ret\n    .groupby([\"ddate\", \"momr\"])\n    .agg(ewret=(\"cohort_ret\", \"mean\"))\n    .reset_index()\n    .rename(columns={\"ddate\": \"date\"})\n)\n\n# Compute daily long-short returns\ndaily_wide = daily_ewret.pivot(\n    index=\"date\", columns=\"momr\", values=\"ewret\"\n).reset_index()\ndaily_wide.columns = [\"date\"] + [f\"port{i}\" for i in range(1, 11)]\ndaily_wide[\"winners\"] = daily_wide[\"port10\"]\ndaily_wide[\"losers\"] = daily_wide[\"port1\"]\ndaily_wide[\"long_short\"] = daily_wide[\"winners\"] - daily_wide[\"losers\"]\n\nprint(f\"Daily long-short return observations: {len(daily_wide):,}\")\n\nDaily long-short return observations: 3,352\n\n\n\n\n18.11.3 Daily Cumulative Returns\n\ndaily_wide = daily_wide.sort_values(\"date\")\ndaily_wide[\"cumret_ls\"] = (1 + daily_wide[\"long_short\"]).cumprod() - 1\n\nplot_daily_cumret = (\n    ggplot(daily_wide, aes(x=\"date\", y=\"cumret_ls\")) +\n    geom_line(size=0.5, color=\"#2ca02c\") +\n    geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\") +\n    scale_y_continuous(labels=percent_format()) +\n    labs(\n        x=\"\", y=\"Cumulative return\",\n        title=\"Daily Cumulative Return: Momentum Long-Short Strategy\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 6))\n)\nplot_daily_cumret\n\n\n\n\n\n\n\nFigure 18.10: Cumulative daily returns of the momentum long-short strategy. This figure shows the same strategy as the monthly analysis but tracked at daily frequency, revealing intra-month dynamics and the precise timing of momentum gains and losses.\n\n\n\n\n\n\n\n18.11.4 Annualized Risk Metrics from Daily Data\nDaily data enables more precise estimation of risk metrics through higher-frequency sampling.\n\ndaily_ls = daily_wide[\"long_short\"].dropna()\n\n# Annualized metrics\nann_mean = daily_ls.mean() * 252\nann_vol = daily_ls.std() * np.sqrt(252)\nsharpe = ann_mean / ann_vol if ann_vol &gt; 0 else np.nan\n\n# Drawdown\ndaily_wealth = (1 + daily_ls).cumprod()\ndaily_running_max = daily_wealth.cummax()\ndaily_dd = (daily_wealth / daily_running_max - 1).min()\n\n# Higher moments\nskew = daily_ls.skew()\nkurt = daily_ls.kurtosis()\n\n# VaR and CVaR\nvar_95 = daily_ls.quantile(0.05)\ncvar_95 = daily_ls[daily_ls &lt;= var_95].mean()\n\nrisk_metrics = pd.DataFrame({\n    \"Metric\": [\n        \"Annualized Mean Return\",\n        \"Annualized Volatility\", \n        \"Sharpe Ratio\",\n        \"Maximum Drawdown\",\n        \"Skewness\",\n        \"Excess Kurtosis\",\n        \"Daily VaR (5%)\",\n        \"Daily CVaR (5%)\"\n    ],\n    \"Value\": [\n        f\"{ann_mean*100:.2f}%\",\n        f\"{ann_vol*100:.2f}%\",\n        f\"{sharpe:.2f}\",\n        f\"{daily_dd*100:.2f}%\",\n        f\"{skew:.2f}\",\n        f\"{kurt:.2f}\",\n        f\"{var_95*100:.2f}%\",\n        f\"{cvar_95*100:.2f}%\"\n    ]\n})\nrisk_metrics\n\n\n\nTable 18.11: Annualized risk metrics for the momentum long-short strategy computed from daily returns. Volatility is annualized using the square root of 252 rule. The Sharpe ratio, maximum drawdown, skewness, and kurtosis provide a comprehensive risk profile.\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\n0\nAnnualized Mean Return\n-26.90%\n\n\n1\nAnnualized Volatility\n15.01%\n\n\n2\nSharpe Ratio\n-1.79\n\n\n3\nMaximum Drawdown\n-97.62%\n\n\n4\nSkewness\n-11.23\n\n\n5\nExcess Kurtosis\n378.11\n\n\n6\nDaily VaR (5%)\n-1.33%\n\n\n7\nDaily CVaR (5%)\n-2.10%\n\n\n\n\n\n\n\n\n\n\n\n\n18.11.5 Realized Volatility of Momentum Returns\nUsing daily returns, we can compute the monthly realized volatility of the momentum strategy and examine how it varies over time.\n\ndaily_wide[\"year_month\"] = daily_wide[\"date\"].dt.to_period(\"M\")\n\nrealized_vol = (daily_wide\n    .groupby(\"year_month\")[\"long_short\"]\n    .std()\n    .reset_index()\n    .rename(columns={\"long_short\": \"realized_vol\"})\n)\nrealized_vol[\"date\"] = realized_vol[\"year_month\"].dt.to_timestamp()\nrealized_vol[\"realized_vol_ann\"] = realized_vol[\"realized_vol\"] * np.sqrt(252)\n\nplot_rvol = (\n    ggplot(realized_vol, aes(x=\"date\", y=\"realized_vol_ann\")) +\n    geom_line(color=\"#d62728\", size=0.8) +\n    scale_y_continuous(labels=percent_format()) +\n    labs(\n        x=\"\", y=\"Annualized Realized Volatility\",\n        title=\"Realized Volatility of Momentum Strategy\"\n    ) +\n    theme_minimal() +\n    theme(figure_size=(10, 5))\n)\nplot_rvol\n\n\n\n\n\n\n\nFigure 18.11: Monthly realized volatility of the momentum long-short strategy, computed from daily returns within each month. Higher values indicate periods of greater uncertainty in momentum profits, often coinciding with market stress.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#saving-results-to-the-database",
    "href": "22_momentum.html#saving-results-to-the-database",
    "title": "18  Momentum Strategies",
    "section": "18.12 Saving Results to the Database",
    "text": "18.12 Saving Results to the Database\nWe save the momentum portfolio returns to our database for use in subsequent chapters, including factor model construction and portfolio optimization.\n\n# Save monthly equally weighted momentum portfolio returns\newret_to_save = ewret[[\"date\", \"momr\", \"ewret\"]].copy()\newret_to_save.to_sql(\n    name=\"momentum_portfolios_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(f\"Saved {len(ewret_to_save):,} monthly momentum portfolio observations.\")\n\n# Save the long-short return series\nmomentum_factor = ewret_wide[[\"date\", \"long_short\"]].dropna().copy()\nmomentum_factor = momentum_factor.rename(columns={\"long_short\": \"wml\"})\nmomentum_factor.to_sql(\n    name=\"momentum_factor_monthly\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(f\"Saved {len(momentum_factor):,} monthly WML factor observations.\")\n\n# Save the daily long-short return series\ndaily_momentum_factor = daily_wide[[\"date\", \"long_short\"]].dropna().copy()\ndaily_momentum_factor = daily_momentum_factor.rename(\n    columns={\"long_short\": \"wml\"}\n)\ndaily_momentum_factor.to_sql(\n    name=\"momentum_factor_daily\",\n    con=tidy_finance,\n    if_exists=\"replace\",\n    index=False\n)\nprint(f\"Saved {len(daily_momentum_factor):,} daily WML factor observations.\")\n\nSaved 1,610 monthly momentum portfolio observations.\nSaved 161 monthly WML factor observations.\nSaved 3,352 daily WML factor observations.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#practical-considerations",
    "href": "22_momentum.html#practical-considerations",
    "title": "18  Momentum Strategies",
    "section": "18.13 Practical Considerations",
    "text": "18.13 Practical Considerations\n\n18.13.1 Transaction Costs\nMomentum strategies involve substantial portfolio turnover, as stocks enter and exit the extreme decile portfolios each month. Korajczyk and Sadka (2004) examine whether momentum profits survive transaction costs and find that profitability declines significantly for large institutional investors, though smaller portfolios can still capture meaningful returns.\nIn the Vietnamese market, transaction costs include:\n\nBrokerage commissions: Typically 0.15%–0.25% of transaction value for institutional investors.\nExchange fees: Approximately 0.03% per trade.\nMarket impact: Particularly relevant for smaller, less liquid stocks that dominate the extreme momentum portfolios. Vietnam’s lower liquidity compared to developed markets may amplify this cost.\nTrading band limits: The \\(\\pm 7\\%\\) daily price limit on HOSE can prevent immediate execution of trades, introducing tracking error relative to the theoretical portfolio.\n\n\n\n18.13.2 Implementation Lag\nOur baseline implementation assumes that portfolios can be formed and rebalanced instantaneously at the end of each month. In practice, there is a lag between observing the formation period returns and executing the portfolio trades. The one-month gap between the formation period and the start of the holding period (Jegadeesh (1990)) partially addresses this concern, but practitioners should consider additional implementation delays.\n\n\n18.13.3 Survivorship Bias\nOur dataset from DataCore includes both active and delisted stocks, which mitigates survivorship bias. However, the treatment of delisted stocks can affect momentum results. Stocks that are delisted during the holding period may generate extreme returns (both positive for acquisitions and negative for failures). We retain delisted returns as reported in the database, which is consistent with the treatment in Jegadeesh and Titman (1993).\n\n\n18.13.4 Small Sample Considerations\nThe Vietnamese stock market has a relatively short history compared to the U.S. market studied in Jegadeesh and Titman (1993). Our sample spans approximately two decades, compared to the nearly three decades in the original study. This shorter sample period implies wider confidence intervals and greater sensitivity to specific episodes (such as the 2007–2009 financial crisis, which had a severe impact on Vietnamese equities). Results should be interpreted with this caveat in mind.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "22_momentum.html#key-takeaways",
    "href": "22_momentum.html#key-takeaways",
    "title": "18  Momentum Strategies",
    "section": "18.14 Key Takeaways",
    "text": "18.14 Key Takeaways\nThis chapter has provided an implementation and analysis of momentum strategies in the Vietnamese equity market, following the methodology of Jegadeesh and Titman (1993). The main findings and methodological contributions are:\n\nMethodology: We implemented the full Jegadeesh and Titman (1993) overlapping portfolio methodology, including the two-stage averaging procedure (within-cohort, then across-cohort) that handles the \\(K\\) active portfolios in each month.\nBaseline results: The \\(J=6\\), \\(K=6\\) strategy provides a natural benchmark. The spread between winner and loser portfolios reveals whether cross-sectional momentum exists in Vietnamese equities.\nRobustness across horizons: By computing the full \\(J \\times K\\) grid with \\(J, K \\in \\{3, 6, 9, 12\\}\\), we assessed whether the momentum premium is robust to the choice of formation and holding periods, following the approach in Jegadeesh and Titman (1993) Table 1.\nRisk adjustment: CAPM and Fama-French three-factor alphas measure whether the momentum premium is explained by standard risk factors, building on the analysis in Fama and French (1996) who show that their three-factor model fails to explain momentum.\nMarket state dependence: Following Cooper, Gutierrez Jr, and Hameed (2004), we examined whether momentum profits vary with market conditions, which is particularly relevant in emerging markets with pronounced boom-bust cycles.\nValue weighting: The comparison of equally weighted and value-weighted strategies addresses practical implementability and isolates the role of firm size in driving momentum profits.\nDaily analysis: By tracking momentum portfolios at the daily frequency, we computed precise risk metrics including realized volatility, maximum drawdown, VaR, and CVaR, providing a complete risk profile of the strategy.\nEmerging market context: Throughout the analysis, we have highlighted features specific to the Vietnamese market—trading band limits, foreign ownership restrictions, shorter sample history, and higher transaction costs—that affect the interpretation and practical viability of momentum strategies.\n\n\n\n\n\n\n\n\nBarberis, Nicholas, Andrei Shleifer, and Robert Vishny. 1998. “A Model of Investor Sentiment.” Journal of Financial Economics 49 (3): 307–43.\n\n\nCarhart, Mark M. 1997. “On Persistence in Mutual Fund Performance.” The Journal of Finance 52 (1): 57–82.\n\n\nChan, Kalok, Allaudeen Hameed, and Wilson Tong. 2000. “Profitability of Momentum Strategies in the International Equity Markets.” Journal of Financial and Quantitative Analysis, 153–72.\n\n\nChui, Andy CW, Sheridan Titman, and KC John Wei. 2010. “Individualism and Momentum Around the World.” The Journal of Finance 65 (1): 361–92.\n\n\nCooper, Michael J, Roberto C Gutierrez Jr, and Allaudeen Hameed. 2004. “Market States and Momentum.” The Journal of Finance 59 (3): 1345–65.\n\n\nDaniel, Kent, David Hirshleifer, and Avanidhar Subrahmanyam. 1998. “Investor Psychology and Security Market Under-and Overreactions.” The Journal of Finance 53 (6): 1839–85.\n\n\nDaniel, Kent, and Tobias J Moskowitz. 2016. “Momentum Crashes.” Journal of Financial Economics 122 (2): 221–47.\n\n\nFama, Eugene F, and Kenneth R French. 1996. “Multifactor Explanations of Asset Pricing Anomalies.” The Journal of Finance 51 (1): 55–84.\n\n\nGrundy, Bruce D, and J Spencer Martin Martin. 2001. “Understanding the Nature of the Risks and the Source of the Rewards to Momentum Investing.” The Review of Financial Studies 14 (1): 29–78.\n\n\nHofstede, Geert. 2001. “Culture’s Consequences: Comparing Values, Behaviors, Institutions and Organizations Across Nations.” International Educational and Professional.\n\n\nHong, Harrison, and Jeremy C Stein. 1999. “A Unified Theory of Underreaction, Momentum Trading, and Overreaction in Asset Markets.” The Journal of Finance 54 (6): 2143–84.\n\n\nJegadeesh, Narasimhan. 1990. “Evidence of Predictable Behavior of Security Returns.” The Journal of Finance 45 (3): 881–98.\n\n\nJegadeesh, Narasimhan, and Sheridan Titman. 1993. “Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency.” The Journal of Finance 48 (1): 65–91.\n\n\nJohnson, Timothy C. 2002. “Rational Momentum Effects.” The Journal of Finance 57 (2): 585–608.\n\n\nKorajczyk, Robert A, and Ronnie Sadka. 2004. “Are Momentum Profits Robust to Trading Costs?” The Journal of Finance 59 (3): 1039–82.\n\n\nLesmond, David A, Michael J Schill, and Chunsheng Zhou. 2004. “The Illusory Nature of Momentum Profits.” Journal of Financial Economics 71 (2): 349–80.\n\n\nRouwenhorst, K Geert. 1998. “International Momentum Strategies.” The Journal of Finance 53 (1): 267–84.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Momentum Strategies</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html",
    "href": "23_factor_construction_principles.html",
    "title": "19  Factor Construction Principles",
    "section": "",
    "text": "19.1 The Factor Construction Pipeline\nThe previous chapters introduced specific asset pricing models, including the CAPM, the Fama-French three-factor model, and momentum. Each of those chapters presented its factor as given. This chapter steps behind the curtain and addresses the engineering question: how exactly do you build a factor? The question matters because seemingly minor methodological decisions, such as where to set breakpoints, whether to value-weight or equal-weight, how to handle missing accounting data, which stocks to exclude, can alter the magnitude, statistical significance, and even the sign of a factor premium.\nIn the U.S. context, Fama and French (1993) established a canonical procedure: sort stocks independently on size and a characteristic, form six value-weighted portfolios from 2×3 intersections, and define the factor as the average return of the two high-characteristic portfolios minus the average return of the two low-characteristic portfolios. This procedure has been replicated thousands of times. But it was designed for the U.S. market circa 1990, with its deep liquidity, broad cross-section, and CRSP/Compustat data infrastructure. Applying it mechanically to Vietnam, a market with 700 listed stocks, extreme illiquidity in the bottom tercile, high concentration in the top decile, and accounting data that arrives with variable lags, requires careful adaptation.\nHou, Xue, and Zhang (2020) replicated 452 anomalies from the U.S. literature and found that over half fail to replicate even in U.S. data with minor methodological variations. The replication crisis in empirical asset pricing makes it essential that researchers understand and document every construction choice. This chapter provides the tools to do so transparently.\nEvery tradeable factor follows the same logical pipeline:\nEach step involves choices that interact with each other. A breakpoint that works well for a liquid universe may be inappropriate for the full cross-section. A weighting scheme that reduces noise in the U.S. may amplify it in Vietnam. We address each step systematically.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-pipeline",
    "href": "23_factor_construction_principles.html#sec-factor-con-pipeline",
    "title": "19  Factor Construction Principles",
    "section": "",
    "text": "Define the universe: Select the eligible securities (e.g., common stocks with liquidity and size filters).\nCompute the signal: Calculate the characteristic of interest (e.g., value, momentum, profitability).\nSet breakpoints: Determine how stocks will be sorted (e.g., median, quintiles, deciles).\nAssign portfolios: Group stocks into high and low (or multiple) portfolios based on the signal.\nCompute returns: Calculate portfolio returns (equal- or value-weighted).\nConstruct the factor: Take Long (high) - Short (low).\nValidate: Test performance, significance, and robustness.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-data",
    "href": "23_factor_construction_principles.html#sec-factor-con-data",
    "title": "19  Factor Construction Principles",
    "section": "19.2 Data Construction",
    "text": "19.2 Data Construction\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom itertools import product\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Monthly returns (survivorship-bias-free)\nmonthly = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'shares_outstanding', 'volume_avg_20d', 'turnover_value_avg_20d',\n        'n_zero_volume_days', 'exchange'\n    ]\n)\n\n# Annual accounting data\naccounting = client.get_fundamentals(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2006-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    frequency='annual',\n    fields=[\n        'ticker', 'fiscal_year', 'filing_date',\n        'total_assets', 'total_equity', 'book_equity',\n        'net_income', 'revenue', 'gross_profit',\n        'operating_profit', 'total_debt', 'retained_earnings',\n        'dividends_paid', 'capex', 'depreciation',\n        'shares_outstanding_fy'\n    ]\n)\n\n# Daily prices for momentum and volatility signals\ndaily = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=['ticker', 'date', 'adjusted_close', 'volume', 'turnover_value']\n)\n\nmonthly['month_end'] = pd.to_datetime(monthly['month_end'])\nmonthly = monthly.sort_values(['ticker', 'month_end'])\n\nprint(f\"Monthly returns: {len(monthly):,} firm-months\")\nprint(f\"Accounting: {len(accounting):,} firm-years\")\nprint(f\"Unique tickers: {monthly['ticker'].nunique()}\")",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-universe",
    "href": "23_factor_construction_principles.html#sec-factor-con-universe",
    "title": "19  Factor Construction Principles",
    "section": "19.3 Step 1: Universe Definition",
    "text": "19.3 Step 1: Universe Definition\nThe first and most consequential choice is which stocks enter the factor construction universe. The universe definition determines what population the factor premium describes and whether it is implementable.\n\n19.3.1 The Universe Problem in Vietnam\nVietnam presents a specific challenge: the cross-section is small (600-800 stocks on HOSE and HNX combined), and the size distribution is extremely skewed. The top 10 stocks by market capitalization account for roughly 50% of the total market cap on HOSE. The bottom tercile consists of micro-cap stocks that often trade fewer than 5 days per month. Including these stocks inflates apparent factor premia because their prices are noisy and stale, but excluding them shrinks the already small cross-section.\n\ndef apply_universe_filters(df, filters='standard'):\n    \"\"\"\n    Apply universe filters to the monthly return panel.\n    \n    Parameters\n    ----------\n    filters : str\n        'none': all stocks\n        'minimal': exclude zero market cap and extreme returns\n        'standard': + minimum listing age + positive volume\n        'strict': + minimum market cap + minimum turnover\n    \n    Returns\n    -------\n    Filtered DataFrame with 'in_universe' column\n    \"\"\"\n    d = df.copy()\n    d['in_universe'] = True\n    \n    # Always: remove missing returns and market cap\n    d.loc[d['monthly_return'].isna(), 'in_universe'] = False\n    d.loc[d['market_cap'].isna() | (d['market_cap'] &lt;= 0), 'in_universe'] = False\n    \n    # Minimal: winsorize extreme returns (likely data errors)\n    if filters in ['minimal', 'standard', 'strict']:\n        d.loc[d['monthly_return'].abs() &gt; 1.0, 'in_universe'] = False\n    \n    # Standard: listing age &gt;= 6 months\n    if filters in ['standard', 'strict']:\n        d['listing_age'] = (\n            d.groupby('ticker').cumcount() + 1\n        )\n        d.loc[d['listing_age'] &lt; 6, 'in_universe'] = False\n        \n        # Require at least 10 positive-volume days in the month\n        d.loc[d['n_zero_volume_days'] &gt; 12, 'in_universe'] = False\n    \n    # Strict: minimum market cap (20th percentile of HOSE)\n    if filters == 'strict':\n        mcap_threshold = (\n            d[d['exchange'] == 'HOSE']\n            .groupby('month_end')['market_cap']\n            .transform(lambda x: x.quantile(0.20))\n        )\n        # Apply HOSE threshold to all stocks\n        d['mcap_threshold'] = (\n            d.groupby('month_end')['market_cap']\n            .transform(lambda x: x.quantile(0.20))\n        )\n        d.loc[d['market_cap'] &lt; d['mcap_threshold'], 'in_universe'] = False\n        \n        # Minimum average daily turnover (VND 200 million)\n        d.loc[d['turnover_value_avg_20d'] &lt; 2e8, 'in_universe'] = False\n    \n    return d\n\n# Apply all filter levels and compare\nfilter_summary = {}\nfor level in ['none', 'minimal', 'standard', 'strict']:\n    filtered = apply_universe_filters(monthly, filters=level)\n    in_univ = filtered[filtered['in_universe']]\n    filter_summary[level] = {\n        'Firm-months': len(in_univ),\n        'Avg stocks/month': in_univ.groupby('month_end')['ticker'].nunique().mean(),\n        'Avg MCap coverage (%)': (\n            in_univ.groupby('month_end')['market_cap'].sum()\n            / filtered.groupby('month_end')['market_cap'].sum()\n        ).mean() * 100\n    }\n\nfilter_df = pd.DataFrame(filter_summary).T\nprint(\"Universe Filter Effects:\")\nprint(filter_df.round(1).to_string())\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ncolors_filter = {\n    'none': '#BDC3C7', 'minimal': '#3498DB',\n    'standard': '#2C5F8A', 'strict': '#C0392B'\n}\n\nfor level in ['none', 'minimal', 'standard', 'strict']:\n    filtered = apply_universe_filters(monthly, filters=level)\n    counts = (\n        filtered[filtered['in_universe']]\n        .groupby('month_end')['ticker']\n        .nunique()\n    )\n    axes[0].plot(counts.index, counts.values,\n                 color=colors_filter[level], linewidth=1.5, label=level)\n\naxes[0].set_ylabel('Number of Stocks')\naxes[0].set_title('Panel A: Universe Size')\naxes[0].legend()\n\n# Panel B: Market cap coverage\nfor level in ['minimal', 'standard', 'strict']:\n    filtered = apply_universe_filters(monthly, filters=level)\n    total_mcap = filtered.groupby('month_end')['market_cap'].sum()\n    filtered_mcap = (\n        filtered[filtered['in_universe']]\n        .groupby('month_end')['market_cap']\n        .sum()\n    )\n    coverage = (filtered_mcap / total_mcap * 100).dropna()\n    axes[1].plot(coverage.index, coverage.values,\n                 color=colors_filter[level], linewidth=1.5, label=level)\n\naxes[1].set_ylabel('Market Cap Coverage (%)')\naxes[1].set_title('Panel B: Market Capitalization Coverage')\naxes[1].legend()\naxes[1].set_ylim([60, 102])\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 19.1",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-signals",
    "href": "23_factor_construction_principles.html#sec-factor-con-signals",
    "title": "19  Factor Construction Principles",
    "section": "19.4 Step 2: Signal Construction",
    "text": "19.4 Step 2: Signal Construction\n\n19.4.1 Point-in-Time Accounting Data\nAs discussed in the missing data chapter, accounting signals must be aligned with their public availability date to avoid look-ahead bias. We implement a general-purpose point-in-time merge:\n\ndef pit_merge_accounting(monthly_df, accounting_df, lag_months=4):\n    \"\"\"\n    Merge accounting data with monthly returns respecting\n    the point-in-time availability constraint.\n    \n    Vietnamese annual reports are due within 90 days of fiscal\n    year-end. We use a conservative 4-month lag.\n    \n    Parameters\n    ----------\n    lag_months : int\n        Number of months after fiscal year-end before data\n        are assumed to be publicly available.\n    \"\"\"\n    acc = accounting_df.copy()\n    \n    # Accounting data becomes available lag_months after FY end\n    # If filing_date is available, use it; otherwise use FY-end + lag\n    if 'filing_date' in acc.columns:\n        acc['filing_date'] = pd.to_datetime(acc['filing_date'])\n        acc['available_date'] = acc['filing_date']\n        # Fallback for missing filing dates\n        acc['fy_end'] = pd.to_datetime(\n            acc['fiscal_year'].astype(str) + '-12-31'\n        )\n        acc['available_date'] = acc['available_date'].fillna(\n            acc['fy_end'] + pd.DateOffset(months=lag_months)\n        )\n    else:\n        acc['available_date'] = pd.to_datetime(\n            acc['fiscal_year'].astype(str) + '-12-31'\n        ) + pd.DateOffset(months=lag_months)\n    \n    # For each firm-month, find the most recent available accounting data\n    merged = monthly_df.copy()\n    \n    # Efficient approach: for June rebalancing, use FY t-1 data\n    # which is available by April of year t (4-month lag)\n    merged['year'] = merged['month_end'].dt.year\n    merged['month'] = merged['month_end'].dt.month\n    \n    # Map: if month &gt;= (lag_months + 1), use current year's FY-1 data\n    # Otherwise, use FY-2 data\n    merged['data_fy'] = np.where(\n        merged['month'] &gt;= lag_months + 1,\n        merged['year'] - 1,\n        merged['year'] - 2\n    )\n    \n    # Merge\n    acc_cols = [c for c in acc.columns if c not in\n                ['filing_date', 'available_date', 'fy_end']]\n    merged = merged.merge(\n        acc[acc_cols].rename(columns={'fiscal_year': 'data_fy'}),\n        on=['ticker', 'data_fy'],\n        how='left'\n    )\n    \n    return merged\n\n# Apply point-in-time merge\npanel = pit_merge_accounting(monthly, accounting, lag_months=4)\n\n# Construct common signals\npanel['log_mcap'] = np.log(panel['market_cap'].clip(lower=1))\n\n# Book-to-market\npanel['bm'] = panel['book_equity'] / panel['market_cap']\npanel.loc[panel['bm'] &lt;= 0, 'bm'] = np.nan  # Negative BE firms\n\n# Gross profitability (Novy-Marx 2013)\npanel['gp_at'] = panel['gross_profit'] / panel['total_assets']\n\n# Operating profitability (Fama-French 2015)\npanel['op'] = panel['operating_profit'] / panel['book_equity']\n\n# Investment (asset growth)\npanel['investment'] = (\n    panel.groupby('ticker')['total_assets']\n    .pct_change(periods=1)\n)\n\n# Leverage\npanel['leverage'] = panel['total_debt'] / panel['total_assets']\n\nprint(\"Signal Coverage:\")\nfor sig in ['bm', 'gp_at', 'op', 'investment', 'leverage']:\n    pct = panel[sig].notna().mean()\n    print(f\"  {sig:&lt;15}: {pct:.1%}\")\n\n\n\n19.4.2 Momentum and Volatility Signals\nPrice-based signals require return history, not accounting data, so they have different timing requirements.\n\n# Past returns for momentum signals\npanel = panel.sort_values(['ticker', 'month_end'])\n\n# Momentum: cumulative return from month t-12 to t-2 (skip most recent month)\npanel['ret_12_2'] = (\n    panel.groupby('ticker')['monthly_return']\n    .transform(lambda x: x.shift(2).rolling(11).apply(\n        lambda r: (1 + r).prod() - 1, raw=True))\n)\n\n# Short-term reversal: month t-1 return\npanel['ret_1'] = panel.groupby('ticker')['monthly_return'].shift(1)\n\n# Idiosyncratic volatility (from daily data, rolling 60 days)\ndaily['date'] = pd.to_datetime(daily['date'])\ndaily['daily_return'] = daily.groupby('ticker')['adjusted_close'].pct_change()\n\nivol = (\n    daily.groupby('ticker')\n    .apply(lambda g: g.set_index('date')['daily_return']\n           .rolling(60, min_periods=40).std() * np.sqrt(252))\n    .reset_index(name='ivol')\n)\nivol['month_end'] = ivol['date'].dt.to_period('M').dt.to_timestamp('M')\nivol_monthly = (\n    ivol.groupby(['ticker', 'month_end'])['ivol']\n    .last()\n    .reset_index()\n)\n\npanel = panel.merge(ivol_monthly, on=['ticker', 'month_end'], how='left')\n\nprint(\"Price Signal Coverage:\")\nfor sig in ['ret_12_2', 'ret_1', 'ivol']:\n    pct = panel[sig].notna().mean()\n    print(f\"  {sig:&lt;15}: {pct:.1%}\")",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-breakpoints",
    "href": "23_factor_construction_principles.html#sec-factor-con-breakpoints",
    "title": "19  Factor Construction Principles",
    "section": "19.5 Step 3: Breakpoint Computation",
    "text": "19.5 Step 3: Breakpoint Computation\n\n19.5.1 The Breakpoint Decision\nBreakpoints determine which stocks are “high” versus “low” on a given characteristic. The two key choices are:\n\nBreakpoint universe: Should breakpoints be computed from all stocks or from a subset (e.g., HOSE only)?\nNumber of groups: 2×3 (Fama-French standard), 5×5 (for finer sorts), or independent terciles/quintiles?\n\nFama and French (1993) use NYSE breakpoints for U.S. sorts because this prevents the large number of small Nasdaq/AMEX stocks from dominating the breakpoint distribution. The analog in Vietnam is to use HOSE breakpoints, since HOSE lists the larger, more liquid firms and HNX lists smaller firms. Using all-stock breakpoints would place most HOSE stocks in the upper size groups and most HNX stocks in the lower groups, producing mechanically different results.\n\ndef compute_breakpoints(df, signal_col, n_groups, bp_universe='hose',\n                          exchange_col='exchange'):\n    \"\"\"\n    Compute cross-sectional breakpoints for portfolio sorting.\n    \n    Parameters\n    ----------\n    bp_universe : str\n        'all': use all stocks in universe\n        'hose': use only HOSE stocks (analogous to NYSE breakpoints)\n    n_groups : int\n        Number of groups (2, 3, 5, or 10)\n    \n    Returns\n    -------\n    Series of breakpoints (quantiles)\n    \"\"\"\n    signal = df[signal_col].dropna()\n    \n    if bp_universe == 'hose':\n        mask = df[exchange_col] == 'HOSE'\n        signal = df.loc[mask, signal_col].dropna()\n    \n    quantiles = np.linspace(0, 1, n_groups + 1)[1:-1]\n    breakpoints = signal.quantile(quantiles)\n    \n    return breakpoints\n\n# Example: compare HOSE vs all-stock breakpoints for book-to-market\nexample_month = panel[panel['month_end'] == '2023-06-30'].copy()\nexample_month = example_month[example_month['bm'].notna()]\n\nbp_hose = compute_breakpoints(example_month, 'bm', 3, bp_universe='hose')\nbp_all = compute_breakpoints(example_month, 'bm', 3, bp_universe='all')\n\nprint(\"BM Tercile Breakpoints (June 2023):\")\nprint(f\"  HOSE-only: {bp_hose.values.round(3)}\")\nprint(f\"  All stocks: {bp_all.values.round(3)}\")\nprint(f\"\\n  Difference: HOSE breakpoints are \"\n      f\"{'higher' if bp_hose.values[0] &gt; bp_all.values[0] else 'lower'} \"\n      f\"than all-stock breakpoints\")\n\n\n\n\n# Compute breakpoints for every month\nbp_comparison = []\nfor month, group in panel.dropna(subset=['bm']).groupby('month_end'):\n    bp_h = compute_breakpoints(group, 'bm', 3, bp_universe='hose')\n    bp_a = compute_breakpoints(group, 'bm', 3, bp_universe='all')\n    \n    # Count stocks in each tercile under each rule\n    for bp_name, bp_vals in [('HOSE', bp_h), ('All', bp_a)]:\n        low = (group['bm'] &lt;= bp_vals.iloc[0]).sum()\n        mid = ((group['bm'] &gt; bp_vals.iloc[0]) &\n               (group['bm'] &lt;= bp_vals.iloc[1])).sum()\n        high = (group['bm'] &gt; bp_vals.iloc[1]).sum()\n        bp_comparison.append({\n            'month_end': month, 'bp_rule': bp_name,\n            'median_bp': bp_vals.iloc[0],\n            'n_low': low, 'n_mid': mid, 'n_high': high\n        })\n\nbp_df = pd.DataFrame(bp_comparison)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Median breakpoint over time\nfor rule, color in [('HOSE', '#2C5F8A'), ('All', '#C0392B')]:\n    subset = bp_df[bp_df['bp_rule'] == rule]\n    axes[0].plot(subset['month_end'], subset['median_bp'],\n                 color=color, linewidth=1.5, label=f'{rule} breakpoints')\naxes[0].set_ylabel('Lower Tercile Breakpoint (BM)')\naxes[0].set_title('Panel A: Breakpoint Time Series')\naxes[0].legend()\n\n# Panel B: Number of stocks in high BM group\nfor rule, color in [('HOSE', '#2C5F8A'), ('All', '#C0392B')]:\n    subset = bp_df[bp_df['bp_rule'] == rule]\n    axes[1].plot(subset['month_end'], subset['n_high'],\n                 color=color, linewidth=1.5, label=f'{rule} breakpoints')\naxes[1].set_ylabel('Stocks in High BM Group')\naxes[1].set_title('Panel B: High BM Portfolio Size')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 19.2",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-formation",
    "href": "23_factor_construction_principles.html#sec-factor-con-formation",
    "title": "19  Factor Construction Principles",
    "section": "19.6 Step 4: Portfolio Formation",
    "text": "19.6 Step 4: Portfolio Formation\n\n19.6.1 The Generic Factor Engine\nWe implement a general-purpose factor construction function that takes any signal column and produces a long-short factor return series. The function encapsulates all methodological choices as parameters, making it easy to test sensitivity.\n\ndef construct_factor(\n    panel_df,\n    signal_col,\n    size_col='market_cap',\n    return_col='monthly_return',\n    date_col='month_end',\n    exchange_col='exchange',\n    formation_month=6,\n    rebalance_freq='annual',\n    n_signal_groups=3,\n    n_size_groups=2,\n    weighting='value',\n    bp_universe='hose',\n    independent_sorts=True,\n    long_group='high',\n    min_stocks_per_portfolio=5,\n    signal_lag=0,\n    universe_filter='standard'\n):\n    \"\"\"\n    Construct a tradeable factor following the Fama-French methodology.\n    \n    Parameters\n    ----------\n    signal_col : str\n        Column with the sorting variable.\n    formation_month : int\n        Month of year for portfolio formation (6 = June for FF).\n    rebalance_freq : str\n        'annual' (FF standard), 'semi', 'quarterly', 'monthly'.\n    n_signal_groups : int\n        Number of signal groups (3 for FF standard, 5 for quintiles).\n    n_size_groups : int\n        Number of size groups (2 for FF standard).\n    weighting : str\n        'value' (VW) or 'equal' (EW).\n    bp_universe : str\n        'hose' or 'all' for breakpoint computation.\n    independent_sorts : bool\n        True for independent double sorts (FF standard).\n    long_group : str\n        'high' or 'low'—which signal group is the long leg.\n    signal_lag : int\n        Additional months to lag the signal beyond the\n        standard point-in-time alignment.\n    \n    Returns\n    -------\n    Dictionary with 'factor_returns', 'portfolio_returns',\n    'diagnostics'.\n    \"\"\"\n    df = panel_df.copy()\n    \n    # Apply universe filter\n    df = apply_universe_filters(df, filters=universe_filter)\n    df = df[df['in_universe']].copy()\n    \n    # Lag the signal if requested\n    if signal_lag &gt; 0:\n        df[signal_col] = (\n            df.groupby('ticker')[signal_col].shift(signal_lag)\n        )\n    \n    # Determine formation dates\n    if rebalance_freq == 'annual':\n        # Form portfolios in formation_month, hold for 12 months\n        df['formation_date'] = df[date_col].apply(\n            lambda d: pd.Timestamp(\n                year=d.year if d.month &gt;= formation_month else d.year - 1,\n                month=formation_month, day=30\n            )\n        )\n    elif rebalance_freq == 'monthly':\n        df['formation_date'] = df[date_col] - pd.DateOffset(months=1)\n    elif rebalance_freq == 'quarterly':\n        df['formation_date'] = df[date_col].apply(\n            lambda d: pd.Timestamp(\n                year=d.year,\n                month=((d.month - 1) // 3) * 3 + 1,\n                day=1\n            ) - pd.DateOffset(days=1)\n        )\n    \n    # Assign signal and size groups at each formation date\n    all_portfolios = []\n    \n    formation_dates = sorted(df['formation_date'].unique())\n    \n    for f_date in formation_dates:\n        # Stocks available at formation\n        formation_data = df[df['formation_date'] == f_date].copy()\n        \n        # Get signal values at formation\n        available = formation_data.dropna(subset=[signal_col, size_col])\n        if len(available) &lt; min_stocks_per_portfolio * n_signal_groups * n_size_groups:\n            continue\n        \n        # Compute breakpoints\n        size_bp = compute_breakpoints(\n            available, size_col, n_size_groups, bp_universe\n        )\n        signal_bp = compute_breakpoints(\n            available, signal_col, n_signal_groups, bp_universe\n        )\n        \n        # Assign groups\n        available['size_group'] = np.searchsorted(\n            size_bp.values, available[size_col].values\n        )\n        available['signal_group'] = np.searchsorted(\n            signal_bp.values, available[signal_col].values\n        )\n        \n        all_portfolios.append(available)\n    \n    if not all_portfolios:\n        return None\n    \n    portfolios = pd.concat(all_portfolios, ignore_index=True)\n    \n    # Compute portfolio returns\n    def weighted_return(group):\n        if weighting == 'value':\n            if group[size_col].sum() &gt; 0:\n                return np.average(group[return_col], weights=group[size_col])\n            else:\n                return group[return_col].mean()\n        else:\n            return group[return_col].mean()\n    \n    port_returns = (\n        portfolios\n        .groupby([date_col, 'size_group', 'signal_group'])\n        .apply(weighted_return)\n        .reset_index(name='port_return')\n    )\n    \n    # Construct factor: average of high-signal portfolios minus\n    # average of low-signal portfolios (across size groups)\n    high_label = n_signal_groups - 1 if long_group == 'high' else 0\n    low_label = 0 if long_group == 'high' else n_signal_groups - 1\n    \n    high_ports = port_returns[port_returns['signal_group'] == high_label]\n    low_ports = port_returns[port_returns['signal_group'] == low_label]\n    \n    high_avg = high_ports.groupby(date_col)['port_return'].mean()\n    low_avg = low_ports.groupby(date_col)['port_return'].mean()\n    \n    factor_returns = (high_avg - low_avg).to_frame('factor_return')\n    \n    # Diagnostics\n    port_counts = (\n        portfolios\n        .groupby([date_col, 'size_group', 'signal_group'])['ticker']\n        .nunique()\n        .reset_index(name='n_stocks')\n    )\n    \n    diagnostics = {\n        'avg_stocks_per_portfolio': port_counts['n_stocks'].mean(),\n        'min_stocks_per_portfolio': port_counts['n_stocks'].min(),\n        'ann_return': factor_returns['factor_return'].mean() * 12,\n        'ann_vol': factor_returns['factor_return'].std() * np.sqrt(12),\n        'sharpe': (factor_returns['factor_return'].mean()\n                   / factor_returns['factor_return'].std() * np.sqrt(12)),\n        't_stat': (factor_returns['factor_return'].mean()\n                   / (factor_returns['factor_return'].std()\n                      / np.sqrt(len(factor_returns)))),\n        'n_months': len(factor_returns)\n    }\n    \n    return {\n        'factor_returns': factor_returns,\n        'portfolio_returns': port_returns,\n        'portfolios': portfolios,\n        'diagnostics': diagnostics\n    }\n\n\n\n19.6.2 Building the Core Factors\nWe now use the engine to construct the standard Fama-French factors for Vietnam:\n\n# SMB (Size): small minus big\n# Signal = market cap; long_group = 'low' (small stocks)\nsmb_result = construct_factor(\n    panel, signal_col='log_mcap', long_group='low',\n    formation_month=6, rebalance_freq='annual',\n    n_signal_groups=2, n_size_groups=1,  # No double sort for size itself\n    weighting='value', bp_universe='hose'\n)\n\n# HML (Value): high BM minus low BM\nhml_result = construct_factor(\n    panel, signal_col='bm', long_group='high',\n    formation_month=6, rebalance_freq='annual',\n    n_signal_groups=3, n_size_groups=2,\n    weighting='value', bp_universe='hose'\n)\n\n# RMW (Profitability): robust minus weak\nrmw_result = construct_factor(\n    panel, signal_col='op', long_group='high',\n    formation_month=6, rebalance_freq='annual',\n    n_signal_groups=3, n_size_groups=2,\n    weighting='value', bp_universe='hose'\n)\n\n# CMA (Investment): conservative minus aggressive\ncma_result = construct_factor(\n    panel, signal_col='investment', long_group='low',\n    formation_month=6, rebalance_freq='annual',\n    n_signal_groups=3, n_size_groups=2,\n    weighting='value', bp_universe='hose'\n)\n\n# WML (Momentum): winners minus losers\nwml_result = construct_factor(\n    panel, signal_col='ret_12_2', long_group='high',\n    formation_month=6, rebalance_freq='monthly',\n    n_signal_groups=3, n_size_groups=2,\n    weighting='value', bp_universe='hose'\n)\n\n# Summary table\nprint(\"Vietnamese Factor Summary:\")\nprint(f\"{'Factor':&lt;8} {'Ann. Ret':&gt;10} {'Ann. Vol':&gt;10} {'Sharpe':&gt;8} \"\n      f\"{'t-stat':&gt;8} {'Avg N':&gt;8}\")\nprint(\"-\" * 54)\nfor name, result in [('SMB', smb_result), ('HML', hml_result),\n                       ('RMW', rmw_result), ('CMA', cma_result),\n                       ('WML', wml_result)]:\n    if result is None:\n        continue\n    d = result['diagnostics']\n    print(f\"{name:&lt;8} {d['ann_return']:&gt;10.4f} {d['ann_vol']:&gt;10.4f} \"\n          f\"{d['sharpe']:&gt;8.2f} {d['t_stat']:&gt;8.2f} \"\n          f\"{d['avg_stocks_per_portfolio']:&gt;8.1f}\")\n\n\n\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\nfactor_colors = {\n    'SMB': '#2C5F8A', 'HML': '#C0392B', 'RMW': '#27AE60',\n    'CMA': '#E67E22', 'WML': '#8E44AD'\n}\n\nfor name, result in [('SMB', smb_result), ('HML', hml_result),\n                       ('RMW', rmw_result), ('CMA', cma_result),\n                       ('WML', wml_result)]:\n    if result is None:\n        continue\n    fr = result['factor_returns']\n    cum = (1 + fr['factor_return']).cumprod()\n    ax.plot(cum.index, cum.values, color=factor_colors[name],\n            linewidth=2, label=name)\n\nax.axhline(y=1, color='gray', linewidth=0.5)\nax.set_ylabel('Cumulative Return')\nax.set_xlabel('Date')\nax.set_title('Vietnamese Factor Cumulative Returns (2×3 VW, HOSE Breakpoints)')\nax.legend(ncol=5)\nax.set_yscale('log')\nplt.tight_layout()\nplt.show()\n\n\nFigure 19.3",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-sensitivity",
    "href": "23_factor_construction_principles.html#sec-factor-con-sensitivity",
    "title": "19  Factor Construction Principles",
    "section": "19.7 Step 5: Sensitivity to Construction Choices",
    "text": "19.7 Step 5: Sensitivity to Construction Choices\nThe most important lesson in factor construction is that the resulting factor premium is not uniquely determined by the economic hypothesis; it depends substantially on implementation choices. We systematically vary each choice and examine how the factor changes.\n\n19.7.1 Weighting: Value-Weighted vs. Equal-Weighted\n\nsensitivity_results = {}\n\nfor name, signal, long_grp in [\n    ('HML', 'bm', 'high'), ('RMW', 'op', 'high'), ('WML', 'ret_12_2', 'high')\n]:\n    for wt in ['value', 'equal']:\n        result = construct_factor(\n            panel, signal_col=signal, long_group=long_grp,\n            weighting=wt, bp_universe='hose',\n            rebalance_freq='annual' if name != 'WML' else 'monthly'\n        )\n        if result:\n            d = result['diagnostics']\n            sensitivity_results[f\"{name}_{wt}\"] = {\n                'Factor': name, 'Weighting': wt,\n                'Ann. Return': d['ann_return'],\n                't-stat': d['t_stat']\n            }\n\nsens_df = pd.DataFrame(sensitivity_results).T\nprint(\"VW vs EW Factor Returns:\")\nprint(sens_df.round(3).to_string())\n\n\n\n19.7.2 Breakpoint Universe: HOSE vs. All Stocks\n\nfor name, signal, long_grp in [\n    ('HML', 'bm', 'high'), ('WML', 'ret_12_2', 'high')\n]:\n    for bp in ['hose', 'all']:\n        result = construct_factor(\n            panel, signal_col=signal, long_group=long_grp,\n            bp_universe=bp, weighting='value',\n            rebalance_freq='annual' if name != 'WML' else 'monthly'\n        )\n        if result:\n            d = result['diagnostics']\n            sensitivity_results[f\"{name}_bp_{bp}\"] = {\n                'Factor': name, 'Breakpoints': bp,\n                'Ann. Return': d['ann_return'],\n                't-stat': d['t_stat'],\n                'Avg N': d['avg_stocks_per_portfolio']\n            }\n\nbp_sens = pd.DataFrame({k: v for k, v in sensitivity_results.items()\n                          if 'bp_' in k}).T\nprint(\"\\nBreakpoint Universe Sensitivity:\")\nprint(bp_sens.round(3).to_string())\n\n\n\n19.7.3 Number of Groups: 2×3 vs. 5×5 vs. Deciles\n\ngroup_configs = [\n    (2, 3, '2x3 (FF standard)'),\n    (2, 5, '2x5 (quintiles)'),\n    (1, 10, '1x10 (deciles)')\n]\n\ngroup_results = {}\nfor n_size, n_signal, label in group_configs:\n    result = construct_factor(\n        panel, signal_col='bm', long_group='high',\n        n_size_groups=n_size, n_signal_groups=n_signal,\n        weighting='value', bp_universe='hose',\n        rebalance_freq='annual'\n    )\n    if result:\n        d = result['diagnostics']\n        group_results[label] = {\n            'Ann. Return': d['ann_return'],\n            't-stat': d['t_stat'],\n            'Avg N per port': d['avg_stocks_per_portfolio'],\n            'Min N per port': d['min_stocks_per_portfolio']\n        }\n\nprint(\"HML: Sorting Granularity Sensitivity:\")\nprint(pd.DataFrame(group_results).T.round(3).to_string())\n\n\n\n19.7.4 Rebalancing Frequency\n\nrebal_results = {}\nfor freq in ['annual', 'quarterly', 'monthly']:\n    result = construct_factor(\n        panel, signal_col='bm', long_group='high',\n        rebalance_freq=freq, weighting='value', bp_universe='hose'\n    )\n    if result:\n        d = result['diagnostics']\n        rebal_results[freq] = {\n            'Ann. Return': d['ann_return'],\n            'Ann. Vol': d['ann_vol'],\n            't-stat': d['t_stat']\n        }\n\nprint(\"HML: Rebalancing Frequency Sensitivity:\")\nprint(pd.DataFrame(rebal_results).T.round(4).to_string())\n\n\n\n19.7.5 Comprehensive Sensitivity Summary\n\n\n\n# Systematic grid search for HML\nconfigs = list(product(\n    ['value', 'equal'],        # Weighting\n    ['hose', 'all'],           # Breakpoint universe\n    [(2, 3), (2, 5), (1, 10)]  # (n_size, n_signal)\n))\n\ngrid_results = []\nfor wt, bp, (ns, nsig) in configs:\n    result = construct_factor(\n        panel, signal_col='bm', long_group='high',\n        weighting=wt, bp_universe=bp,\n        n_size_groups=ns, n_signal_groups=nsig,\n        rebalance_freq='annual'\n    )\n    if result:\n        d = result['diagnostics']\n        grid_results.append({\n            'Weighting': 'VW' if wt == 'value' else 'EW',\n            'Breakpoints': bp.upper(),\n            'Sort': f'{ns}x{nsig}',\n            'Ann. Return': d['ann_return'],\n            't-stat': d['t_stat']\n        })\n\ngrid_df = pd.DataFrame(grid_results)\nprint(\"HML Factor: Full Sensitivity Grid:\")\nprint(grid_df.to_string(index=False))\n\n# Pivot for heatmap\npivot = grid_df.pivot_table(\n    values='Ann. Return', index=['Weighting', 'Breakpoints'],\n    columns='Sort'\n)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nsns.heatmap(pivot * 100, annot=True, fmt='.1f', cmap='RdYlGn',\n            center=0, linewidths=0.5, ax=ax,\n            cbar_kws={'label': 'Ann. Return (%)'})\nax.set_title('HML Premium: Sensitivity to Construction Choices')\nplt.tight_layout()\nplt.show()\n\n\nFigure 19.4",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-correlation",
    "href": "23_factor_construction_principles.html#sec-factor-con-correlation",
    "title": "19  Factor Construction Principles",
    "section": "19.8 Factor Correlation Structure",
    "text": "19.8 Factor Correlation Structure\n\n\n\n# Merge all factor return series\nfactor_panel = pd.DataFrame()\nfor name, result in [('SMB', smb_result), ('HML', hml_result),\n                       ('RMW', rmw_result), ('CMA', cma_result),\n                       ('WML', wml_result)]:\n    if result is None:\n        continue\n    fr = result['factor_returns'].rename(columns={'factor_return': name})\n    if factor_panel.empty:\n        factor_panel = fr\n    else:\n        factor_panel = factor_panel.merge(fr, left_index=True,\n                                           right_index=True, how='outer')\n\ncorr = factor_panel.corr()\n\nfig, ax = plt.subplots(figsize=(7, 6))\nmask = np.triu(np.ones_like(corr, dtype=bool), k=1)\nsns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n            center=0, vmin=-1, vmax=1, square=True,\n            linewidths=0.5, ax=ax)\nax.set_title('Factor Return Correlations')\nplt.tight_layout()\nplt.show()\n\n\nFigure 19.5",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-validation",
    "href": "23_factor_construction_principles.html#sec-factor-con-validation",
    "title": "19  Factor Construction Principles",
    "section": "19.9 Factor Validation",
    "text": "19.9 Factor Validation\nA well-constructed factor should pass several diagnostic tests before being used in asset pricing research.\n\n19.9.1 Diagnostic Checklist\n\ndef validate_factor(factor_result, name):\n    \"\"\"\n    Run standard diagnostic tests on a constructed factor.\n    \"\"\"\n    fr = factor_result['factor_returns']['factor_return']\n    diag = factor_result['diagnostics']\n    ports = factor_result['portfolios']\n    \n    tests = {}\n    \n    # 1. Statistical significance (t &gt; 2)\n    tests['t-stat'] = diag['t_stat']\n    tests['t &gt; 2'] = abs(diag['t_stat']) &gt; 2.0\n    \n    # 2. Economic magnitude\n    tests['Ann. Return'] = diag['ann_return']\n    tests['Ann. Vol'] = diag['ann_vol']\n    tests['Sharpe'] = diag['sharpe']\n    \n    # 3. Adequate portfolio diversification\n    tests['Avg N per portfolio'] = diag['avg_stocks_per_portfolio']\n    tests['Min N per portfolio'] = diag['min_stocks_per_portfolio']\n    tests['Min N &gt;= 5'] = diag['min_stocks_per_portfolio'] &gt;= 5\n    \n    # 4. Not dominated by a single month\n    tests['Max monthly return'] = fr.max()\n    tests['Min monthly return'] = fr.min()\n    tests['Fraction &gt; 0'] = (fr &gt; 0).mean()\n    \n    # 5. Persistence (ACF at lag 1)\n    if len(fr) &gt; 12:\n        tests['ACF(1)'] = fr.autocorr(lag=1)\n    \n    # 6. Consistency across subperiods\n    mid = len(fr) // 2\n    first_half = fr.iloc[:mid]\n    second_half = fr.iloc[mid:]\n    tests['Return (1st half)'] = first_half.mean() * 12\n    tests['Return (2nd half)'] = second_half.mean() * 12\n    tests['Same sign both halves'] = (\n        np.sign(first_half.mean()) == np.sign(second_half.mean())\n    )\n    \n    return tests\n\nprint(\"Factor Validation Summary:\")\nprint(\"=\" * 70)\nfor name, result in [('SMB', smb_result), ('HML', hml_result),\n                       ('RMW', rmw_result), ('CMA', cma_result),\n                       ('WML', wml_result)]:\n    if result is None:\n        continue\n    tests = validate_factor(result, name)\n    print(f\"\\n{name}:\")\n    for test, value in tests.items():\n        if isinstance(value, bool):\n            status = 'PASS' if value else 'FAIL'\n            print(f\"  {test:&lt;30}: {status}\")\n        elif isinstance(value, float):\n            print(f\"  {test:&lt;30}: {value:.4f}\")\n        else:\n            print(f\"  {test:&lt;30}: {value}\")\n\n\n\n19.9.2 Monotonicity Test\nA factor built from a characteristic sort should produce monotonically increasing (or decreasing) average returns across quantiles. Violations of monotonicity suggest the signal-return relationship is nonlinear or absent.\n\n\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 10))\naxes = axes.flatten()\n\nsignals = [\n    ('bm', 'Book-to-Market', 'high'),\n    ('op', 'Operating Profit.', 'high'),\n    ('investment', 'Investment', 'low'),\n    ('ret_12_2', 'Momentum (12-2)', 'high'),\n    ('ivol', 'Idio. Volatility', 'low'),\n]\n\nfor i, (sig, label, long_grp) in enumerate(signals):\n    result = construct_factor(\n        panel, signal_col=sig, long_group=long_grp,\n        n_size_groups=1, n_signal_groups=10,\n        weighting='value', bp_universe='hose',\n        rebalance_freq='annual' if sig not in ['ret_12_2'] else 'monthly'\n    )\n    if result is None:\n        continue\n    \n    port_ret = result['portfolio_returns']\n    decile_means = (\n        port_ret.groupby('signal_group')['port_return']\n        .mean() * 12 * 100\n    )\n    \n    colors_mono = plt.cm.RdYlGn_r(np.linspace(0.1, 0.9, len(decile_means)))\n    axes[i].bar(range(len(decile_means)), decile_means.values,\n                color=colors_mono, edgecolor='white')\n    axes[i].set_xticks(range(len(decile_means)))\n    axes[i].set_xticklabels([f'D{d+1}' for d in range(len(decile_means))],\n                             fontsize=8)\n    axes[i].set_ylabel('Ann. Return (%)')\n    axes[i].set_title(label)\n    axes[i].axhline(y=0, color='gray', linewidth=0.5)\n\naxes[5].set_visible(False)\nplt.suptitle('Decile Portfolio Returns by Signal', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\nFigure 19.6\n\n\n\n\n\n19.9.3 Spanning Tests\nDoes a new factor add information beyond existing factors? We test this by regressing each factor on all other factors and examining the intercept (alpha). A significant alpha means the factor captures return variation not spanned by the others:\n\nfactor_names = [c for c in factor_panel.columns if factor_panel[c].notna().sum() &gt; 24]\nspanning_data = factor_panel[factor_names].dropna()\n\nprint(\"Spanning Tests (alpha = intercept when regressed on other factors):\")\nprint(f\"{'Factor':&lt;8} {'Alpha (ann.)':&gt;12} {'t-stat':&gt;8} {'R²':&gt;6}\")\nprint(\"-\" * 36)\n\nfor target in factor_names:\n    others = [f for f in factor_names if f != target]\n    y = spanning_data[target]\n    X = sm.add_constant(spanning_data[others])\n    model = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags': 6})\n    \n    alpha_ann = model.params['const'] * 12\n    alpha_t = model.tvalues['const']\n    r2 = model.rsquared\n    \n    print(f\"{target:&lt;8} {alpha_ann:&gt;12.4f} {alpha_t:&gt;8.2f} {r2:&gt;6.3f}\")",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-vietnam",
    "href": "23_factor_construction_principles.html#sec-factor-con-vietnam",
    "title": "19  Factor Construction Principles",
    "section": "19.10 Vietnamese-Specific Considerations",
    "text": "19.10 Vietnamese-Specific Considerations\n\n19.10.1 State-Owned Enterprise Classification\nA unique feature of Vietnamese equities is the high proportion of state-owned enterprises (SOEs). The state retains majority or significant minority stakes in many listed firms, which affects governance, information environment, and trading dynamics. Factors may behave differently within SOE and non-SOE subsamples:\n\n# Get SOE classification\nfirm_info = client.get_firm_info(\n    exchanges=['HOSE', 'HNX'],\n    fields=['ticker', 'state_ownership_pct', 'is_soe']\n)\n\npanel_soe = panel.merge(firm_info[['ticker', 'is_soe']], on='ticker', how='left')\n\nfor label, subset in [('SOE', panel_soe[panel_soe['is_soe'] == True]),\n                        ('Non-SOE', panel_soe[panel_soe['is_soe'] == False])]:\n    hml = construct_factor(\n        subset, signal_col='bm', long_group='high',\n        weighting='value', bp_universe='all',\n        rebalance_freq='annual'\n    )\n    if hml:\n        d = hml['diagnostics']\n        print(f\"HML ({label}): Ann = {d['ann_return']:.4f}, \"\n              f\"t = {d['t_stat']:.2f}, N = {d['avg_stocks_per_portfolio']:.0f}\")\n\n\n\n19.10.2 Foreign Ownership Limits\nForeign ownership caps (49% for most sectors, 30% for banking) affect the investable universe for international investors. Factors constructed from the full universe may not be achievable by foreign investors if the long leg concentrates in stocks at the foreign ownership limit:\n\nfol = client.get_foreign_ownership(\n    exchanges=['HOSE', 'HNX'],\n    fields=['ticker', 'month_end', 'foreign_pct', 'foreign_limit_pct']\n)\n\n# Merge with HML portfolios\nif hml_result:\n    hml_ports = hml_result['portfolios'].merge(\n        fol, on=['ticker', 'month_end'], how='left'\n    )\n    hml_ports['near_limit'] = (\n        (hml_ports['foreign_limit_pct'] - hml_ports['foreign_pct']) &lt; 5\n    )\n    \n    fol_by_group = (\n        hml_ports.groupby('signal_group')\n        .agg(\n            avg_foreign_pct=('foreign_pct', 'mean'),\n            pct_near_limit=('near_limit', 'mean')\n        )\n    )\n    print(\"Foreign Ownership in HML Portfolios:\")\n    print(fol_by_group.round(3))",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-recommendations",
    "href": "23_factor_construction_principles.html#sec-factor-con-recommendations",
    "title": "19  Factor Construction Principles",
    "section": "19.11 Recommended Factor Specifications for Vietnam",
    "text": "19.11 Recommended Factor Specifications for Vietnam\nBased on the sensitivity analysis, we recommend the following baseline specifications (Table 19.1).\n\n\n\nTable 19.1: Recommended factor construction parameters for Vietnamese equities.\n\n\n\n\n\n\n\n\n\n\nChoice\nRecommendation\nRationale\n\n\n\n\nUniverse\nStandard filter (listing age ≥ 6m, volume &gt; 0)\nExcludes shell firms without losing too much breadth\n\n\nBreakpoints\nHOSE stocks only\nPrevents HNX micro-caps from dominating breakpoints\n\n\nSize groups\n2 (median split)\nSufficient control with limited cross-section\n\n\nSignal groups\n3 (terciles) for factor construction; 5 or 10 for portfolio analysis\n2×3 = adequate diversification per portfolio\n\n\nWeighting\nValue-weighted\nInvestable; less noisy than EW\n\n\nRebalancing\nAnnual (June) for accounting signals; monthly for momentum\nStandard; consistent with Fama-French\n\n\nAccounting lag\n4 months (available by April for Dec FY)\nConservative PIT alignment\n\n\nMinimum stocks\n≥ 5 per portfolio per month\nBelow this, single-stock idiosyncratic risk dominates\n\n\n\n\n\n\nAlways report results under the baseline and at least one alternative specification (e.g., EW, all-stock breakpoints) to demonstrate robustness.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "23_factor_construction_principles.html#sec-factor-con-summary",
    "href": "23_factor_construction_principles.html#sec-factor-con-summary",
    "title": "19  Factor Construction Principles",
    "section": "19.12 Summary",
    "text": "19.12 Summary\nThis chapter has developed a modular, transparent factor construction framework for Vietnamese equities. The key insights are:\n\nThe Fama and French (1993) 2×3 methodology translates well to Vietnam with one critical adaptation: breakpoints should be computed from HOSE stocks only. Using all-stock breakpoints allows HNX micro-caps to dominate the small-stock groups, inflating apparent premia with economically untradeable returns.\nValue weighting produces more conservative (and more implementable) factor premia than equal weighting. The difference is particularly large for signals correlated with size (BM, investment), where EW overweights the smallest stocks that contribute most to the premium but least to investable returns.\nNo single construction choice determines whether a factor “exists.” A factor premium that appears only under one specific combination of breakpoints, weighting, and rebalancing frequency is fragile and should be treated with skepticism. Robust factors survive a grid of specifications. The sensitivity analysis framework developed here (e.g., varying weighting, breakpoints, sort granularity, and rebalancing simultaneously) should be standard practice.\n\nThe construct_factor() function developed in this chapter is designed for reuse throughout the book. Any anomaly variable can be fed through the same pipeline, producing a tradeable factor with full diagnostics. This ensures methodological consistency across chapters and makes it easy to compare premia on an apples-to-apples basis.\n\n\n\n\n\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2020. “Replicating Anomalies.” The Review of Financial Studies 33 (5): 2019–2133.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Factor Construction Principles</span>"
    ]
  },
  {
    "objectID": "24_fama_macbeth.html",
    "href": "24_fama_macbeth.html",
    "title": "20  Fama-MacBeth Regressions",
    "section": "",
    "text": "20.1 The Econometric Framework\nIn this chapter, we delve into the implementation of the Fama and MacBeth (1973) regression approach, a cornerstone of empirical asset pricing. While portfolio sorts provide a robust, non-parametric view of the relationship between characteristics and returns, they struggle when we need to control for multiple factors simultaneously. For instance, in the Vietnamese stock market (HOSE and HNX), small-cap stocks often exhibit high illiquidity. Does the “Size effect” exist because small stocks are risky, or simply because they are illiquid? Fama-MacBeth (FM) regressions allow us to disentangle these effects in a linear framework.\nWe will implement a version of the FM procedure, accounting for:\nThe Fama-MacBeth procedure is essentially a two-step filter that separates the cross-sectional variation in returns from the time-series variation.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "24_fama_macbeth.html#the-econometric-framework",
    "href": "24_fama_macbeth.html#the-econometric-framework",
    "title": "20  Fama-MacBeth Regressions",
    "section": "",
    "text": "20.1.1 Intuition: Why not Panel OLS?\nA naive approach would be to pool all data (\\(N\\) stocks \\(\\times\\) \\(T\\) months) and run a single Ordinary Least Squares (OLS) regression:\n\\[\nr_{i,t+1} = \\alpha + \\beta_{i,t} \\lambda + \\epsilon_{i,t+1}\n\\]\nHowever, this assumes that the error terms \\(\\epsilon_{i,t+1}\\) are independent across firms. In reality, stock returns are highly cross-sectionally correlated (if the VN-Index crashes, most stocks fall together). A pooled OLS would underestimate the standard errors, leading to “false positive” discoveries of risk factors. Fama-MacBeth solves this by running \\(T\\) separate cross-sectional regressions, effectively treating each month as a single independent observation of the risk premium.\n\n\n20.1.2 Mathematical Derivation\n\n20.1.2.1 Step 1: Cross-Sectional Regressions\nFor each month \\(t\\), we estimate the premium \\(\\lambda_{k,t}\\) for \\(K\\) factors. Let \\(r_{i,t+1}\\) be the excess return of asset \\(i\\) at time \\(t+1\\). Let \\(\\boldsymbol{\\beta}_{i,t}\\) be a vector of \\(K\\) characteristics (e.g., Market Beta, Book-to-Market, Size) known at time \\(t\\).\nThe model for a specific month \\(t\\) is: \\[\n\\mathbf{r}_{t+1} = \\mathbf{X}_t \\boldsymbol{\\lambda}_{t+1} + \\boldsymbol{\\alpha}_{t+1} + \\boldsymbol{\\epsilon}_{t+1}\n\\]\nWhere:\n\n\\(\\mathbf{r}_{t+1}\\) is an \\(N \\times 1\\) vector of returns.\n\\(\\mathbf{X}_t\\) is an \\(N \\times (K+1)\\) matrix of factor exposures (including a column of ones for the intercept).\n\\(\\boldsymbol{\\lambda}_{t+1}\\) is the vector of risk premiums realized in month \\(t+1\\).\n\nTo use Weighted Least Squares (WLS), We define a weighting matrix \\(\\mathbf{W}_t\\) (typically diagonal with market capitalizations). The estimator for month \\(t\\) is: \\[\n\\hat{\\boldsymbol{\\lambda}}_{t+1} = (\\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{X}_t)^{-1} \\mathbf{X}_t^\\top \\mathbf{W}_t \\mathbf{r}_{t+1}\n\\]\n\n\n20.1.2.2 Step 2: Time-Series Aggregation\nWe now have a time-series of \\(T\\) estimates: \\(\\hat{\\lambda}_1, \\hat{\\lambda}_2, \\dots, \\hat{\\lambda}_T\\). The final estimate of the risk premium is the time-series average: \\[\n\\hat{\\lambda}_k = \\frac{1}{T} \\sum_{t=1}^T \\hat{\\lambda}_{k,t}\n\\]\nThe standard error is derived from the standard deviation of these monthly estimates: \\[\n\\sigma(\\hat{\\lambda}_k) = \\sqrt{\\frac{1}{T^2} \\sum_{t=1}^T (\\hat{\\lambda}_{k,t} - \\hat{\\lambda}_k)^2}\n\\]",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "24_fama_macbeth.html#data-preparation",
    "href": "24_fama_macbeth.html#data-preparation",
    "title": "20  Fama-MacBeth Regressions",
    "section": "20.2 Data Preparation",
    "text": "20.2 Data Preparation\nWe utilize data from our local SQLite database. In Vietnam, the fiscal year typically ends in December, and audited reports are required by April. To ensure no look-ahead bias, we lag accounting data (Book Equity) to match returns starting in July (a 6-month conservative lag, similar to Fama-French, but adapted for Vietnamese reporting delays).\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom pandas.tseries.offsets import MonthEnd\n\n# Connect to the Vietnamese data\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\n# Load Monthly Prices (HOSE & HNX)\nprices_monthly = pd.read_sql_query(\n  sql=\"SELECT symbol, date, ret_excess, mktcap, mktcap_lag FROM prices_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\n# Load Book Equity (derived from Vietnamese Financial Statements)\ncomp_vn = pd.read_sql_query(\n  sql=\"SELECT datadate, symbol, be FROM comp_vn\",\n  con=tidy_finance,\n  parse_dates={\"datadate\"}\n)\n\n# Load Rolling Market Betas (Pre-calculated in Chapter 'Beta Estimation')\nbeta_monthly = pd.read_sql_query(\n  sql=\"SELECT symbol, date, beta FROM beta_monthly\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\nWe construct our testing characteristics:\n\n(Market Beta): The sensitivity to the VN-Index.\nSize (ln(ME)): The natural log of market capitalization.\nValue (BM): The ratio of Book Equity to Market Equity.\n\n\n# Prepare Characteristics\ncharacteristics = (\n    comp_vn\n    # Align reporting date to month end\n    .assign(date=lambda x: pd.to_datetime(x[\"datadate\"]) + MonthEnd(0))\n    # Merge with price data to get Market Cap at fiscal year end\n    .merge(prices_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n    .merge(beta_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n    .assign(\n        # Compute Book-to-Market\n        bm=lambda x: x[\"be\"] / x[\"mktcap\"],\n        log_mktcap=lambda x: np.log(x[\"mktcap\"]),\n        # Create sorting date: Financials valid from July of year t+1\n        sorting_date=lambda x: x[\"date\"] + pd.DateOffset(months=6) + MonthEnd(0),\n    )\n    .get([\"symbol\", \"bm\", \"beta\", \"sorting_date\"]) \n    .dropna()\n)\n\ncharacteristics.head()\n\n\n\n\n\n\n\n\nsymbol\nbm\nbeta\nsorting_date\n\n\n\n\n8729\nVTV\n7.034945e+08\n0.847809\n2017-06-30\n\n\n8732\nMTG\n2.670306e+09\n1.140066\n2017-06-30\n\n\n8739\nMKV\n6.505031e+08\n-0.448319\n2017-06-30\n\n\n8740\nMIC\n1.243127e+09\n0.772140\n2017-06-30\n\n\n8742\nMCP\n6.657350e+08\n0.348139\n2017-06-30\n\n\n\n\n\n\n\n\n# Merge back to monthly return panel\ndata_fm = (prices_monthly\n  .merge(characteristics, \n         left_on=[\"symbol\", \"date\"], \n         right_on=[\"symbol\", \"sorting_date\"], \n         how=\"left\")\n#   .merge(beta_monthly, on=[\"symbol\", \"date\"], how=\"left\")\n  .sort_values([\"symbol\", \"date\"])\n)\n\n# Forward fill characteristics for 12 months (valid until next report)\ndata_fm[[\"bm\"]] = data_fm.groupby(\"symbol\")[[\"bm\"]].ffill(limit=12)\n\n# Log Market Cap is updated monthly\ndata_fm[\"log_mktcap\"] = np.log(data_fm[\"mktcap\"])\n\n# Lead returns: We use characteristics at t to predict return at t+1\ndata_fm[\"ret_excess_lead\"] = data_fm.groupby(\"symbol\")[\"ret_excess\"].shift(-1)\n\n# Cleaning: Remove rows with missing future returns or characteristics\ndata_fm = data_fm.dropna(subset=[\"ret_excess_lead\", \"beta\", \"log_mktcap\", \"bm\"])\n\nprint(data_fm.head())\n\nprint(f\"Data ready: {len(data_fm):,} observations from {data_fm.date.min().date()} to {data_fm.date.max().date()}\")\n\n    symbol       date  ret_excess       mktcap   mktcap_lag            bm  \\\n163    AAA 2017-06-30    0.129454  2078.455619  1834.816104  7.929854e+08   \n175    AAA 2018-06-30   -0.067690  2758.426126  2948.159140  8.161755e+08   \n187    AAA 2019-06-30    0.030469  3141.519560  3038.799575  1.389438e+09   \n199    AAA 2020-06-30   -0.035462  2311.250278  2387.972279  1.497272e+09   \n211    AAA 2021-06-30    0.275355  5423.280296  4241.283308  1.456989e+09   \n\n         beta sorting_date  log_mktcap  ret_excess_lead  \n163  1.479060   2017-06-30    7.639380        -0.051090  \n175  1.090411   2018-06-30    7.922416        -0.095926  \n187  1.099956   2019-06-30    8.052462        -0.027856  \n199  0.954144   2020-06-30    7.745544        -0.098769  \n211  1.245004   2021-06-30    8.598456        -0.175128  \nData ready: 5,075 observations from 2017-06-30 to 2023-06-30",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "24_fama_macbeth.html#step-1-cross-sectional-regressions-with-wls",
    "href": "24_fama_macbeth.html#step-1-cross-sectional-regressions-with-wls",
    "title": "20  Fama-MacBeth Regressions",
    "section": "20.3 Step 1: Cross-Sectional Regressions with WLS",
    "text": "20.3 Step 1: Cross-Sectional Regressions with WLS\nHou, Xue, and Zhang (2020) argue that micro-cap stocks distorts inference because they have high transaction costs and idiosyncratic volatility. In Vietnam, this is exacerbated by “penny stock” speculation.\nWe implement Weighted Least Squares (WLS) where weights are the market capitalization of the prior month. This tests if the factors are priced in the investable universe, not just the equal-weighted average of tiny stocks.\n\ndef run_cross_section(df):\n    # Standardize inputs for numerical stability\n    # Note: We do NOT standardize the dependent variable (returns)\n    # We standardize regressors to interpret coefficients as \"per 1 SD change\" if desired,\n    # BUT for pure risk premium estimation, we usually keep raw units.\n    # Here we use raw units to interpret lambda as % return per unit of characteristic.\n    \n    # Define Weighted Least Squares\n    model = smf.wls(\n        formula=\"ret_excess_lead ~ beta + log_mktcap + bm\",\n        data=df,\n        weights=df[\"mktcap_lag\"] # Weight by size\n    )\n    results = model.fit()\n    \n    return results.params\n\n# Apply to every month\nrisk_premiums = (data_fm\n  .groupby(\"date\")\n  .apply(run_cross_section)\n  .reset_index()\n)\n\nprint(risk_premiums.head())\n\n        date  Intercept      beta  log_mktcap            bm\n0 2017-06-30  -0.089116 -0.063799    0.010284  2.897813e-11\n1 2018-06-30  -0.023221 -0.008252    0.001890  1.377518e-11\n2 2019-06-30  -0.079373  0.035622    0.006224 -8.139910e-12\n3 2020-06-30  -0.031213 -0.114968    0.008999 -2.306768e-11\n4 2021-06-30   0.081397 -0.011407   -0.007330 -5.211290e-11",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "24_fama_macbeth.html#step-2-time-series-aggregation-hypothesis-testing",
    "href": "24_fama_macbeth.html#step-2-time-series-aggregation-hypothesis-testing",
    "title": "20  Fama-MacBeth Regressions",
    "section": "20.4 Step 2: Time-Series Aggregation & Hypothesis Testing",
    "text": "20.4 Step 2: Time-Series Aggregation & Hypothesis Testing\nWe now possess the time-series of risk premiums. We calculate the arithmetic mean and the -statistics.\nCrucially, we use Newey-West (HAC) standard errors. Risk premiums in Vietnam often exhibit autocorrelation (momentum in factor performance). A simple standard error formula would be invalid.\n\ndef calculate_fama_macbeth_stats(df, lags=6):\n    summary = []\n    \n    for col in [\"Intercept\", \"beta\", \"log_mktcap\", \"bm\"]:\n        series = df[col]\n        \n        # 1. Point Estimate (Average Risk Premium)\n        mean_premium = series.mean()\n        \n        # 2. Newey-West Standard Error\n        # We regress the series on a constant (ones) to get the SE of the mean\n        exog = sm.add_constant(np.ones(len(series)))\n        nw_model = sm.OLS(series, exog).fit(\n            cov_type='HAC', cov_kwds={'maxlags': lags}\n        )\n\n        se = nw_model.bse.iloc[0]\n        t_stat = nw_model.tvalues.iloc[0]\n        \n        summary.append({\n            \"Factor\": col,\n            \"Premium (%)\": mean_premium * 100,\n            \"Std Error\": se * 100,\n            \"t-statistic\": t_stat,\n            \"Significance\": \"*\" if abs(t_stat) &gt; 1.96 else \"\"\n        })\n        \n    return pd.DataFrame(summary)\n\nprice_of_risk = calculate_fama_macbeth_stats(risk_premiums)\nprint(price_of_risk.round(4))\n\n       Factor  Premium (%)  Std Error  t-statistic Significance\n0   Intercept      -1.8174     1.9117      -0.9507             \n1        beta      -1.7859     1.0407      -1.7161             \n2  log_mktcap       0.2347     0.2048       1.1457             \n3          bm      -0.0000     0.0000      -0.0928             \n\n\n\n20.4.1 Visualizing the Time-Varying Risk Premium\nOne major advantage of the FM approach is that we can inspect the volatility of the risk premiums over time. In Vietnam, we expect the “Size” premium to be highly volatile during periods of retail liquidity injection (e.g., 2020-2021).\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\n# Calculate cumulative returns of the factors (as if they were tradable portfolios)\ncumulative_premiums = (risk_premiums\n    .set_index(\"date\")\n    .drop(columns=[\"Intercept\"])\n    .cumsum()\n)\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncumulative_premiums.plot(ax=ax, linewidth=2)\nax.set_title(\"Cumulative Risk Premiums in Vietnam (Fama-MacBeth)\", fontsize=14)\nax.set_ylabel(\"Cumulative Coefficient Return\")\nax.legend(title=\"Factor\")\nax.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\nFigure 20.1: Cumulative Risk Premiums in Vietnam.\n\n\n\n\n\n\nMarket Beta: In many empirical studies (including the US), the market beta premium is often insignificant or even negative (the “Betting Against Beta” anomaly). In Vietnam, if the -stat is , it implies the CAPM does not explain the cross-section of returns.\nSize (Log Mktcap): A negative coefficient confirms the “Size Effect”—smaller firms have higher expected returns. However, using WLS often weakens this result compared to OLS, suggesting the size premium is concentrated in micro-caps.\nValue (BM): A positive coefficient confirms the Value premium. In Vietnam, value stocks (high B/M) often outperform growth stocks, particularly in the manufacturing and banking sectors.\n\nFigure 20.1 plots the cumulative sum of the monthly Fama MacBeth risk premium estimates for beta, size, and value. Because these lines cumulate estimated cross sectional prices of risk rather than actual portfolio returns, the figure should be interpreted as showing the time variation and persistence of estimated premia, not investable performance.\nThe beta premium displays a clear regime shift around 2020, with a sharp decline that only partially reverses afterward. This pattern suggests that the pricing of systematic risk in Vietnam is unstable over short samples and may be heavily influenced by episodic market conditions such as the post COVID retail trading boom. The size premium is comparatively smoother but small in magnitude, indicating only weak and time varying evidence that firm size is priced in the cross section during this period. The value premium remains close to zero throughout, implying little consistent cross sectional reward to high book to market firms in this sample window.\nOverall, the figure highlights that estimated risk premia in the Vietnamese market are highly time varying and sensitive to specific macro and market regimes, reinforcing the need for caution when drawing conclusions from short samples.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "24_fama_macbeth.html#sanity-checks",
    "href": "24_fama_macbeth.html#sanity-checks",
    "title": "20  Fama-MacBeth Regressions",
    "section": "20.5 Sanity Checks",
    "text": "20.5 Sanity Checks\n\n20.5.1 Time-Series Volatility Check\nFama-MacBeth relies on the assumption that the risk premium varies over time. If your bm premium is truly near zero every month, the method fails.\nAction: Plot the time series of the estimated coefficients . You want to see “noise” around a mean. If you see a flat line or a single massive spike, your data is corrupted.\n\nimport matplotlib.pyplot as plt\n\n# Plot the time series of the BM risk premium\nfig, ax = plt.subplots(figsize=(10, 5))\nrisk_premiums[\"bm\"].plot(ax=ax, title=\"Monthly Value Premium (BM) Coefficient\")\nax.axhline(0, color=\"black\", linestyle=\"--\")\nax.set_ylabel(\"Slope Coefficient\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n20.5.2 Correlation of Characteristics (Multicollinearity)\nIn Vietnam, large-cap stocks (high log_mktcap) are often the ones with high Book-to-Market ratios (banks/utilities) or specific Betas. If your factors are highly correlated, the Fama-MacBeth coefficients will be unstable and insignificant (low t-stats), even if the factors actually matter.\nAction: Check the cross-sectional correlation.\n\n# Check correlation of the characteristics\ncorr_matrix = data_fm[[\"beta\", \"log_mktcap\", \"bm\"]].corr()\nprint(corr_matrix)\n\n                beta  log_mktcap        bm\nbeta        1.000000    0.392776 -0.033748\nlog_mktcap  0.392776    1.000000 -0.203307\nbm         -0.033748   -0.203307  1.000000\n\n\nInterpretation:\n\nIf correlation &gt; 0.7 (absolute value), the regression struggles to distinguish between the two factors.\nFor example, if Size and Liquidity are -0.8 correlated, the model cannot tell which one is driving the return, often resulting in both having insignificant t-stats.\n\n\n\n\n\n\n\nFama, Eugene F., and James D. MacBeth. 1973. “Risk, return, and equilibrium: Empirical tests.” Journal of Political Economy 81 (3): 607–36. https://doi.org/10.1086/260061.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2020. “Replicating anomalies.” Review of Financial Studies 33 (5): 2019–2133. https://doi.org/10.1093/rfs/hhy131.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Fama-MacBeth Regressions</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html",
    "href": "25_factor_zoo_and_multiple_testing.html",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "",
    "text": "21.1 The Scale of the Problem\nBy 2016, academic finance had documented over 300 variables that purportedly predict the cross-section of stock returns. Cochrane (2011) memorably called this proliferation a “zoo of factors.” Harvey, Liu, and Zhu (2016) cataloged over 300 published factors and argued that the standard significance threshold of \\(t &gt; 2.0\\) was far too low given the sheer number of hypotheses tested, either explicitly or implicitly, across decades of research. Their proposed threshold of \\(t &gt; 3.0\\) (and in some cases \\(t &gt; 3.78\\)) would eliminate the majority of published anomalies.\nThe problem is not merely academic. Every researcher who sorts stocks on a new variable and finds \\(t &gt; 2\\) is implicitly testing a hypothesis that has already been tested hundreds of times with different variables. The probability that at least one of 300 independent tests produces \\(t &gt; 2\\) by chance (even when no true effect exists) is essentially 1.0. This is the multiple testing problem, and failing to account for it means that a substantial fraction of the “factor zoo” consists of false discoveries.\nFor Vietnamese equity research, the problem is arguably worse. The cross-section is smaller (600-800 stocks vs. 4,000+ in the U.S.), the time series is shorter (2008- vs. 1963-), and the data are noisier (illiquidity, zero-trading days, accounting irregularities). All of these amplify the risk of both Type I errors (finding something that isn’t there) and Type II errors (missing something that is). This chapter provides the statistical tools to navigate this landscape rigorously.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-scale",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-scale",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "",
    "text": "21.1.1 How Many Factors Exist?\nHarvey, Liu, and Zhu (2016) counted over 300 factors published in top journals through 2012. Chen and Zimmermann (2022) constructed and made publicly available over 300 “clearly significant” anomalies from the U.S. literature. Hou, Xue, and Zhang (2020) attempted to replicate 452 anomalies and found that 64% (with VW returns) failed to achieve \\(t &gt; 1.96\\) in their sample. Jensen, Kelly, and Pedersen (2023) took a more optimistic view, finding that most factors replicate in direction if not always in magnitude.\nThe key insight is that the number of factors tested far exceeds the number published. For every published factor with \\(t &gt; 2\\), there may be dozens of unpublished tests with \\(t &lt; 2\\) that never saw print, which is the classic file drawer problem. The true number of tests conducted collectively by the profession is unknowable, but certainly in the thousands.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.stats.multitest import multipletests\nfrom itertools import product\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Monthly returns (survivorship-bias-free)\nmonthly = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'month_end', 'monthly_return', 'market_cap',\n        'exchange', 'volume_avg_20d', 'turnover_value_avg_20d',\n        'n_zero_volume_days'\n    ]\n)\n\n# Pre-computed characteristic panel (from factor construction chapter)\ncharacteristics = client.get_characteristics_panel(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True\n)\n\n# Factor returns for spanning tests\nfactors_ff = client.get_factor_returns(\n    market='vietnam',\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    factors=['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml']\n)\n\nmonthly['month_end'] = pd.to_datetime(monthly['month_end'])\n\nprint(f\"Monthly returns: {len(monthly):,}\")\nprint(f\"Characteristics: {characteristics.shape}\")\nprint(f\"Available signals: {[c for c in characteristics.columns if c not in ['ticker', 'month_end']]}\")",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-zoo",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-zoo",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.2 Building the Anomaly Zoo",
    "text": "21.2 Building the Anomaly Zoo\n\n21.2.1 Signal Definitions\nWe construct a comprehensive set of anomaly variables spanning the major categories from the literature. Each signal is lagged appropriately to avoid look-ahead bias (accounting signals use point-in-time alignment; price signals use the prior month’s value).\n\n# Merge returns with characteristics\npanel = monthly.merge(characteristics, on=['ticker', 'month_end'], how='left')\npanel = panel.sort_values(['ticker', 'month_end'])\n\n# ============================================================\n# Category 1: VALUE (8 signals)\n# ============================================================\n# Already in characteristics panel from PIT merge:\n# bm, ep, cfp, sp, dp, ev_ebitda\n# Add earnings yield variants\nvalue_signals = ['bm', 'ep', 'cfp', 'sp', 'dp']\n\n# ============================================================\n# Category 2: SIZE (3 signals)\n# ============================================================\npanel['log_mcap'] = np.log(panel['market_cap'].clip(lower=1))\npanel['log_assets'] = np.log(panel['total_assets'].clip(lower=1))\npanel['log_revenue'] = np.log(panel['revenue'].clip(lower=1))\nsize_signals = ['log_mcap', 'log_assets', 'log_revenue']\n\n# ============================================================\n# Category 3: PROFITABILITY (8 signals)\n# ============================================================\n# GP/A (Novy-Marx 2013), OP (FF 2015), ROE, ROA, etc.\nprof_signals = ['gp_at', 'op', 'roe', 'roa', 'profit_margin',\n                 'asset_turnover', 'gross_margin', 'oper_margin']\n\n# ============================================================\n# Category 4: INVESTMENT / GROWTH (6 signals)\n# ============================================================\ninv_signals = ['asset_growth', 'capex_at', 'investment',\n                'sales_growth', 'equity_issuance', 'debt_issuance']\n\n# ============================================================\n# Category 5: MOMENTUM / REVERSAL (6 signals)\n# ============================================================\n# Compute from returns\nfor lag_start, lag_end, name in [\n    (2, 12, 'ret_12_2'), (2, 6, 'ret_6_2'), (7, 12, 'ret_12_7'),\n    (1, 1, 'ret_1'), (1, 3, 'ret_3_1')\n]:\n    if name not in panel.columns:\n        panel[name] = (\n            panel.groupby('ticker')['monthly_return']\n            .transform(lambda x: x.shift(lag_start).rolling(lag_end - lag_start + 1)\n                       .apply(lambda r: (1 + r).prod() - 1, raw=True))\n        )\n# Earnings momentum (SUE) should already be in characteristics\nmom_signals = ['ret_12_2', 'ret_6_2', 'ret_12_7', 'ret_1', 'ret_3_1', 'sue']\n\n# ============================================================\n# Category 6: RISK (5 signals)\n# ============================================================\nrisk_signals = ['beta', 'ivol', 'tvol', 'max_ret', 'skewness']\n\n# ============================================================\n# Category 7: LIQUIDITY / TRADING (6 signals)\n# ============================================================\nliq_signals = ['amihud', 'zero_return_pct', 'log_turnover',\n                'roll_spread', 'cs_spread', 'volume_cv']\n\n# ============================================================\n# Category 8: QUALITY / FINANCIAL HEALTH (5 signals)\n# ============================================================\nqual_signals = ['leverage', 'current_ratio', 'accruals',\n                 'earnings_variability', 'piotroski_f']\n\n# ============================================================\n# Category 9: DIVIDEND / PAYOUT (3 signals)\n# ============================================================\ndiv_signals = ['div_yield', 'payout_ratio', 'net_repurchase']\n\n# Combine all\nall_signals = (value_signals + size_signals + prof_signals +\n               inv_signals + mom_signals + risk_signals +\n               liq_signals + qual_signals + div_signals)\n\n# Remove signals with too little coverage\nsignal_coverage = {}\nfor sig in all_signals:\n    if sig in panel.columns:\n        cov = panel[sig].notna().mean()\n        signal_coverage[sig] = cov\n\nsignal_coverage = pd.Series(signal_coverage).sort_values(ascending=False)\nvalid_signals = signal_coverage[signal_coverage &gt; 0.30].index.tolist()\n\nprint(f\"Total signals defined: {len(all_signals)}\")\nprint(f\"Signals with &gt;30% coverage: {len(valid_signals)}\")\nprint(f\"\\nSignal categories:\")\nfor cat_name, sigs in [\n    ('Value', value_signals), ('Size', size_signals),\n    ('Profitability', prof_signals), ('Investment', inv_signals),\n    ('Momentum', mom_signals), ('Risk', risk_signals),\n    ('Liquidity', liq_signals), ('Quality', qual_signals),\n    ('Dividend', div_signals)\n]:\n    n_valid = sum(1 for s in sigs if s in valid_signals)\n    print(f\"  {cat_name:&lt;15}: {n_valid}/{len(sigs)}\")\n\n\n\n21.2.2 Mass Factor Construction\nWe run the factor construction engine (from the previous chapter) across all valid signals to produce a “zoo” of factor returns:\n\n# Import the factor engine from previous chapter\n# (In practice, this would be imported from a shared module)\n\ndef construct_factor_simple(panel_df, signal_col, long_high=True,\n                              n_groups=5, weighting='value',\n                              min_stocks=20):\n    \"\"\"\n    Simplified factor construction: quintile sort on signal,\n    long-short = Q5 - Q1 (or reversed if long_high=False).\n    Returns a Series of monthly long-short returns.\n    \"\"\"\n    df = panel_df[['ticker', 'month_end', 'monthly_return',\n                    'market_cap', signal_col]].dropna().copy()\n    df = df.sort_values(['ticker', 'month_end'])\n    \n    # Lag the signal by one month\n    df['signal_lag'] = df.groupby('ticker')[signal_col].shift(1)\n    df = df.dropna(subset=['signal_lag', 'monthly_return'])\n    \n    results = []\n    for month, group in df.groupby('month_end'):\n        if len(group) &lt; min_stocks:\n            continue\n        \n        group['quintile'] = pd.qcut(\n            group['signal_lag'].rank(method='first'),\n            n_groups, labels=False, duplicates='drop'\n        )\n        \n        if weighting == 'value':\n            def wret(g):\n                if g['market_cap'].sum() &gt; 0:\n                    return np.average(g['monthly_return'],\n                                       weights=g['market_cap'])\n                return g['monthly_return'].mean()\n            port_ret = group.groupby('quintile').apply(wret)\n        else:\n            port_ret = group.groupby('quintile')['monthly_return'].mean()\n        \n        if 0 in port_ret.index and (n_groups - 1) in port_ret.index:\n            if long_high:\n                ls = port_ret[n_groups - 1] - port_ret[0]\n            else:\n                ls = port_ret[0] - port_ret[n_groups - 1]\n            results.append({'month_end': month, 'ls_return': ls})\n    \n    if not results:\n        return None\n    return pd.DataFrame(results).set_index('month_end')['ls_return']\n\n# Determine expected sign for each signal\n# (positive = high signal predicts high returns)\nsignal_directions = {\n    # Value: high = higher returns\n    'bm': True, 'ep': True, 'cfp': True, 'sp': True, 'dp': True,\n    # Size: small = higher returns (low signal = high returns)\n    'log_mcap': False, 'log_assets': False, 'log_revenue': False,\n    # Profitability: high = higher returns\n    'gp_at': True, 'op': True, 'roe': True, 'roa': True,\n    'profit_margin': True, 'asset_turnover': True,\n    'gross_margin': True, 'oper_margin': True,\n    # Investment: low growth = higher returns\n    'asset_growth': False, 'capex_at': False, 'investment': False,\n    'sales_growth': False, 'equity_issuance': False,\n    'debt_issuance': False,\n    # Momentum: high = higher returns (except reversal)\n    'ret_12_2': True, 'ret_6_2': True, 'ret_12_7': True,\n    'ret_1': False, 'ret_3_1': False, 'sue': True,\n    # Risk: low risk = higher returns (anomaly direction)\n    'beta': False, 'ivol': False, 'tvol': False,\n    'max_ret': False, 'skewness': False,\n    # Liquidity: illiquid = higher returns\n    'amihud': True, 'zero_return_pct': True,\n    'log_turnover': False, 'roll_spread': True,\n    'cs_spread': True, 'volume_cv': True,\n    # Quality: high quality = higher returns\n    'leverage': False, 'current_ratio': True, 'accruals': False,\n    'earnings_variability': False, 'piotroski_f': True,\n    # Dividend: high = higher returns\n    'div_yield': True, 'payout_ratio': True, 'net_repurchase': False,\n}\n\n# Construct all factors\nzoo_results = {}\nfor sig in valid_signals:\n    long_high = signal_directions.get(sig, True)\n    ls = construct_factor_simple(\n        panel, sig, long_high=long_high,\n        n_groups=5, weighting='value'\n    )\n    if ls is not None and len(ls) &gt;= 60:\n        mean_ret = ls.mean()\n        se = ls.std() / np.sqrt(len(ls))\n        t = mean_ret / se if se &gt; 0 else 0\n        \n        zoo_results[sig] = {\n            'mean_monthly': mean_ret,\n            'ann_return': mean_ret * 12,\n            'ann_vol': ls.std() * np.sqrt(12),\n            'sharpe': mean_ret / ls.std() * np.sqrt(12) if ls.std() &gt; 0 else 0,\n            't_stat': t,\n            'abs_t': abs(t),\n            'p_value': 2 * (1 - stats.t.cdf(abs(t), df=len(ls) - 1)),\n            'n_months': len(ls),\n            'direction': 'Long High' if long_high else 'Long Low',\n            'category': next(\n                (cat for cat, sigs in [\n                    ('Value', value_signals), ('Size', size_signals),\n                    ('Profitability', prof_signals),\n                    ('Investment', inv_signals),\n                    ('Momentum', mom_signals), ('Risk', risk_signals),\n                    ('Liquidity', liq_signals),\n                    ('Quality', qual_signals),\n                    ('Dividend', div_signals)\n                ] if sig in sigs), 'Other'\n            ),\n            'returns': ls  # Store for later use\n        }\n\nzoo_df = pd.DataFrame({k: {kk: vv for kk, vv in v.items() if kk != 'returns'}\n                         for k, v in zoo_results.items()}).T\nzoo_df = zoo_df.sort_values('abs_t', ascending=False)\n\nprint(f\"Factors successfully constructed: {len(zoo_df)}\")\nprint(f\"Factors with |t| &gt; 2.0: {(zoo_df['abs_t'] &gt; 2.0).sum()} \"\n      f\"({(zoo_df['abs_t'] &gt; 2.0).mean():.1%})\")\nprint(f\"Factors with |t| &gt; 3.0: {(zoo_df['abs_t'] &gt; 3.0).sum()} \"\n      f\"({(zoo_df['abs_t'] &gt; 3.0).mean():.1%})\")\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Panel A: Histogram\nt_vals = zoo_df['t_stat'].values\naxes[0].hist(t_vals, bins=30, color='#2C5F8A', alpha=0.7,\n             edgecolor='white', density=True)\n\n# Overlay normal distribution under null\nx_range = np.linspace(-5, 5, 200)\naxes[0].plot(x_range, stats.norm.pdf(x_range), 'k--', linewidth=1.5,\n             label='N(0,1) null')\n\n# Threshold lines\naxes[0].axvline(x=1.96, color='#E67E22', linestyle='--', linewidth=1.5,\n                label='|t| = 1.96')\naxes[0].axvline(x=-1.96, color='#E67E22', linestyle='--', linewidth=1.5)\naxes[0].axvline(x=3.0, color='#C0392B', linestyle='--', linewidth=1.5,\n                label='|t| = 3.0 (HLZ)')\naxes[0].axvline(x=-3.0, color='#C0392B', linestyle='--', linewidth=1.5)\naxes[0].set_xlabel('t-statistic')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Panel A: t-Statistic Distribution')\naxes[0].legend(fontsize=9)\n\n# Panel B: Ranked bar chart\ntop_n = min(40, len(zoo_df))\ntop = zoo_df.head(top_n).copy()\n\ncategory_colors = {\n    'Value': '#2C5F8A', 'Size': '#1ABC9C', 'Profitability': '#27AE60',\n    'Investment': '#E67E22', 'Momentum': '#C0392B', 'Risk': '#8E44AD',\n    'Liquidity': '#3498DB', 'Quality': '#F1C40F', 'Dividend': '#95A5A6'\n}\nbar_colors = [category_colors.get(top.iloc[i]['category'], '#BDC3C7')\n              for i in range(len(top))]\n\naxes[1].barh(range(top_n), top['abs_t'].values,\n             color=bar_colors, alpha=0.85, edgecolor='white')\naxes[1].set_yticks(range(top_n))\naxes[1].set_yticklabels(top.index, fontsize=7)\naxes[1].axvline(x=1.96, color='#E67E22', linestyle='--', linewidth=1)\naxes[1].axvline(x=3.0, color='#C0392B', linestyle='--', linewidth=1)\naxes[1].set_xlabel('|t-statistic|')\naxes[1].set_title('Panel B: Anomalies Ranked by |t|')\naxes[1].invert_yaxis()\n\n# Legend for categories\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=c, label=cat)\n                    for cat, c in category_colors.items()]\naxes[1].legend(handles=legend_elements, fontsize=7,\n                loc='lower right', ncol=2)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.1",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-multiple-testing",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-multiple-testing",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.3 The Multiple Testing Problem",
    "text": "21.3 The Multiple Testing Problem\n\n21.3.1 Why Single-Test p-Values Are Misleading\nSuppose you test \\(M\\) independent hypotheses, each at significance level \\(\\alpha = 0.05\\). Under the global null (no true effects), the expected number of false rejections is \\(M \\times \\alpha\\). For \\(M = 50\\) anomalies, you expect 2.5 “significant” results by pure chance.\nThe probability of at least one false rejection is:\n\\[\nP(\\text{at least one false discovery}) = 1 - (1 - \\alpha)^M\n\\tag{21.1}\\]\nFor \\(M = 50\\) and \\(\\alpha = 0.05\\), this is \\(1 - 0.95^{50} = 0.923\\). For \\(M = 300\\), it is essentially 1. The single-test p-value is useless for deciding which of many tested factors are genuine.\n\n\n21.3.2 Two Error Rate Concepts\nMultiple testing corrections control one of two error rates:\nFamily-Wise Error Rate (FWER). The probability of making at least one Type I error across all tests. This is the most conservative criterion. If FWER = 0.05, you can be 95% confident that every rejected null is a true discovery. Methods: Bonferroni (1936) correction, Holm step-down, Romano and Wolf (2005).\nFalse Discovery Rate (FDR). The expected proportion of rejected hypotheses that are false. This is less conservative. If FDR = 0.05, you expect that 5% of your discoveries are false, but the other 95% are real. Methods: Benjamini and Hochberg (1995) (BH), Storey (2003).\nIn asset pricing, FDR control is generally more appropriate than FWER because we are not trying to guarantee zero false discoveries, we are trying to identify a set of factors where most are genuine. A researcher who controls FDR at 5% and discovers 10 factors expects approximately 0.5 of them to be false, which is acceptable for building a factor model.\n\n\n\nM_range = np.arange(1, 301)\nalpha = 0.05\n\nfwer_unadj = 1 - (1 - alpha) ** M_range\nfwer_bonf = np.minimum(1, M_range * alpha)  # Not actual FWER, just threshold\n\n# Expected false discoveries at alpha = 0.05\nexpected_false = M_range * alpha\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(M_range, fwer_unadj, color='#C0392B', linewidth=2,\n             label='Unadjusted FWER')\naxes[0].axhline(y=0.05, color='gray', linestyle='--', linewidth=1,\n                label='α = 0.05 target')\naxes[0].set_xlabel('Number of Tests (M)')\naxes[0].set_ylabel('P(≥1 False Discovery)')\naxes[0].set_title('Panel A: Family-Wise Error Rate')\naxes[0].legend()\n\naxes[1].plot(M_range, expected_false, color='#2C5F8A', linewidth=2)\naxes[1].set_xlabel('Number of Tests (M)')\naxes[1].set_ylabel('Expected False Discoveries')\naxes[1].set_title('Panel B: Expected False Discoveries at α = 0.05')\naxes[1].axhline(y=1, color='gray', linestyle='--', linewidth=0.8)\naxes[1].text(250, 1.5, '1 false discovery', color='gray', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.2",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-corrections",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-corrections",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.4 Correction Methods",
    "text": "21.4 Correction Methods\n\n21.4.1 Bonferroni Correction (FWER Control)\nThe simplest and most conservative correction: reject hypothesis \\(i\\) only if its p-value is below \\(\\alpha / M\\), where \\(M\\) is the total number of tests:\n\\[\n\\text{Reject } H_{0,i} \\text{ if } p_i &lt; \\frac{\\alpha}{M}\n\\tag{21.2}\\]\nThis controls FWER at \\(\\alpha\\) regardless of the dependence structure among tests. The cost is severe loss of power: for \\(M = 50\\) tests at \\(\\alpha = 0.05\\), the per-test threshold becomes \\(0.001\\), requiring \\(|t| &gt; 3.29\\).\n\nM = len(zoo_df)\nalpha = 0.05\n\n# Bonferroni threshold\nbonf_threshold = alpha / M\nbonf_t_threshold = stats.norm.ppf(1 - bonf_threshold / 2)\n\nzoo_df['bonf_reject'] = zoo_df['p_value'] &lt; bonf_threshold\n\nprint(f\"Number of tests: {M}\")\nprint(f\"Bonferroni p-value threshold: {bonf_threshold:.6f}\")\nprint(f\"Equivalent t-statistic threshold: {bonf_t_threshold:.2f}\")\nprint(f\"Rejections: {zoo_df['bonf_reject'].sum()} / {M}\")\nprint(f\"\\nSurviving anomalies:\")\nsurvivors = zoo_df[zoo_df['bonf_reject']]\nif len(survivors) &gt; 0:\n    print(survivors[['ann_return', 't_stat', 'category']].to_string())\nelse:\n    print(\"  None survive Bonferroni correction\")\n\n\n\n21.4.2 Holm Step-Down Procedure (FWER Control)\nThe Holm procedure is uniformly more powerful than Bonferroni while still controlling FWER. It ranks p-values from smallest to largest and applies progressively less stringent thresholds:\n\nSort p-values: \\(p_{(1)} \\leq p_{(2)} \\leq \\ldots \\leq p_{(M)}\\)\nFor \\(i = 1, 2, \\ldots\\), reject \\(H_{0,(i)}\\) if \\(p_{(i)} &lt; \\alpha / (M - i + 1)\\)\nStop at the first \\(i\\) where \\(H_{0,(i)}\\) is not rejected\n\n\np_values = zoo_df['p_value'].values\nsignal_names = zoo_df.index.values\n\n# Using statsmodels implementation\nreject_holm, pvals_corrected_holm, _, _ = multipletests(\n    p_values, alpha=0.05, method='holm'\n)\n\nzoo_df['holm_reject'] = reject_holm\nzoo_df['holm_p_corrected'] = pvals_corrected_holm\n\nprint(f\"Holm rejections: {reject_holm.sum()} / {M}\")\nholm_survivors = zoo_df[zoo_df['holm_reject']]\nif len(holm_survivors) &gt; 0:\n    print(\"\\nSurviving anomalies (Holm):\")\n    print(holm_survivors[['ann_return', 't_stat', 'holm_p_corrected',\n                           'category']].to_string())\n\n\n\n21.4.3 Benjamini-Hochberg Procedure (FDR Control)\nThe Benjamini and Hochberg (1995) (BH) procedure controls the False Discovery Rate (i.e., the expected proportion of rejections that are false) at level \\(q\\):\n\nSort p-values: \\(p_{(1)} \\leq p_{(2)} \\leq \\ldots \\leq p_{(M)}\\)\nFind the largest \\(k\\) such that \\(p_{(k)} \\leq \\frac{k}{M} q\\)\nReject all \\(H_{0,(i)}\\) for \\(i = 1, \\ldots, k\\)\n\nThe BH procedure is valid under independence or positive dependence (PRDS), a condition that is generally satisfied for financial factor tests where factors tend to be positively correlated under the null.\n\n# BH procedure at q = 0.05 (expect 5% false discoveries)\nreject_bh, pvals_corrected_bh, _, _ = multipletests(\n    p_values, alpha=0.05, method='fdr_bh'\n)\n\nzoo_df['bh_reject'] = reject_bh\nzoo_df['bh_p_corrected'] = pvals_corrected_bh\n\n# Also at q = 0.10 (more liberal)\nreject_bh10, _, _, _ = multipletests(\n    p_values, alpha=0.10, method='fdr_bh'\n)\nzoo_df['bh10_reject'] = reject_bh10\n\nprint(f\"BH rejections (q=0.05): {reject_bh.sum()} / {M}\")\nprint(f\"BH rejections (q=0.10): {reject_bh10.sum()} / {M}\")\n\nbh_survivors = zoo_df[zoo_df['bh_reject']].sort_values('abs_t', ascending=False)\nif len(bh_survivors) &gt; 0:\n    print(f\"\\nSurviving anomalies (BH, q=0.05):\")\n    print(bh_survivors[['ann_return', 't_stat', 'bh_p_corrected',\n                          'category']].head(20).to_string())\n\n\n\n\nsorted_pvals = np.sort(p_values)\nk = np.arange(1, M + 1)\nbh_line_05 = k / M * 0.05\nbh_line_10 = k / M * 0.10\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.scatter(k, sorted_pvals, s=15, color='#2C5F8A', alpha=0.7,\n           label='Sorted p-values', zorder=3)\nax.plot(k, bh_line_05, color='#C0392B', linewidth=2,\n        label='BH line (q=0.05)')\nax.plot(k, bh_line_10, color='#E67E22', linewidth=2,\n        linestyle='--', label='BH line (q=0.10)')\n\n# Highlight rejections\nn_reject_05 = reject_bh.sum()\nif n_reject_05 &gt; 0:\n    ax.scatter(k[:n_reject_05], sorted_pvals[:n_reject_05],\n               s=40, color='#C0392B', zorder=4, label=f'Rejected (q=0.05)')\n\nax.set_xlabel('Rank (k)')\nax.set_ylabel('p-value')\nax.set_title('Benjamini-Hochberg Procedure')\nax.legend()\nax.set_ylim([-0.01, 0.15])\nax.set_xlim([0, min(M, 60)])\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.3\n\n\n\n\n\n21.4.4 The Harvey-Liu-Zhu Threshold\nHarvey, Liu, and Zhu (2016) take a different approach: rather than applying a formal multiple-testing correction to a specific set of tests, they estimate the hurdle t-statistic that accounts for the cumulative number of factors tested by the entire profession. Using a Bayesian framework calibrated to the 316 published factors, they derive:\n\nFor a single new factor tested in isolation: \\(|t| &gt; 2.0\\) (conventional)\nAccounting for 100 prior tests: \\(|t| &gt; 2.8\\)\nAccounting for 316 prior tests: \\(|t| &gt; 3.0\\)\nAccounting for all conceivable tests: \\(|t| &gt; 3.78\\)\n\nThe logic is that even if your study tests only one factor, the fact that hundreds of researchers have tested hundreds of signals before you means the prior probability that your specific signal is a true predictor is low.\n\nhlz_thresholds = {\n    'Conventional (|t| &gt; 2.0)': 2.0,\n    'HLZ 100 tests (|t| &gt; 2.8)': 2.8,\n    'HLZ 316 tests (|t| &gt; 3.0)': 3.0,\n    'HLZ conservative (|t| &gt; 3.78)': 3.78\n}\n\nprint(\"Anomalies Surviving Different t-Thresholds:\")\nprint(f\"{'Threshold':&lt;35} {'Surviving':&gt;10} {'% of Zoo':&gt;10}\")\nprint(\"-\" * 55)\n\nfor name, thresh in hlz_thresholds.items():\n    n_survive = (zoo_df['abs_t'] &gt; thresh).sum()\n    pct = n_survive / len(zoo_df) * 100\n    print(f\"{name:&lt;35} {n_survive:&gt;10} {pct:&gt;10.1f}%\")",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-comparison",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-comparison",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.5 Comparison of Methods",
    "text": "21.5 Comparison of Methods\n\n\n\ncomparison = {\n    'Unadjusted (|t| &gt; 2)': (zoo_df['abs_t'] &gt; 2.0).sum(),\n    'Bonferroni': zoo_df['bonf_reject'].sum(),\n    'Holm': zoo_df['holm_reject'].sum(),\n    'BH (q=0.05)': zoo_df['bh_reject'].sum(),\n    'BH (q=0.10)': zoo_df['bh10_reject'].sum(),\n    'HLZ (|t| &gt; 3.0)': (zoo_df['abs_t'] &gt; 3.0).sum(),\n    'HLZ (|t| &gt; 3.78)': (zoo_df['abs_t'] &gt; 3.78).sum(),\n}\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nnames = list(comparison.keys())\ncounts = list(comparison.values())\ncolors = ['#BDC3C7', '#C0392B', '#E74C3C', '#27AE60', '#2ECC71',\n          '#2C5F8A', '#1A3C5A']\n\nbars = ax.barh(range(len(names)), counts, color=colors,\n               alpha=0.85, edgecolor='white')\nax.set_yticks(range(len(names)))\nax.set_yticklabels(names)\nax.set_xlabel('Number of Surviving Anomalies')\nax.set_title('Anomalies Surviving Multiple Testing Corrections')\n\nfor i, (name, count) in enumerate(zip(names, counts)):\n    ax.text(count + 0.3, i, str(count), va='center', fontsize=10)\n\nax.invert_yaxis()\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.4\n\n\n\n\n\n\ncat_survival = (\n    zoo_df.groupby('category')\n    .agg(\n        n_total=('abs_t', 'count'),\n        n_survive_bh=('bh_reject', 'sum'),\n        n_survive_hlz=('abs_t', lambda x: (x &gt; 3.0).sum()),\n        mean_abs_t=('abs_t', 'mean')\n    )\n    .sort_values('mean_abs_t', ascending=False)\n)\ncat_survival['survival_rate_bh'] = (\n    cat_survival['n_survive_bh'] / cat_survival['n_total']\n)\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\nx = np.arange(len(cat_survival))\nax.bar(x, cat_survival['n_total'], color='#BDC3C7', alpha=0.5,\n       label='Total tested', edgecolor='white')\nax.bar(x, cat_survival['n_survive_bh'], color='#27AE60', alpha=0.85,\n       label='Survive BH (q=0.05)', edgecolor='white')\n\nax.set_xticks(x)\nax.set_xticklabels(cat_survival.index, rotation=30, fontsize=9)\nax.set_ylabel('Number of Anomalies')\nax.set_title('Anomaly Survival by Category')\nax.legend()\n\n# Add survival rate text\nfor i, (_, row) in enumerate(cat_survival.iterrows()):\n    if row['n_total'] &gt; 0:\n        ax.text(i, row['n_total'] + 0.2,\n                f\"{row['survival_rate_bh']:.0%}\",\n                ha='center', fontsize=9, color='#27AE60')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.5",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-fdp",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-fdp",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.6 Estimating the False Discovery Proportion",
    "text": "21.6 Estimating the False Discovery Proportion\n\n21.6.1 The Storey Approach\nRather than controlling FDR at a pre-specified level, we can estimate the proportion of tested hypotheses that are truly null (\\(\\pi_0\\)) and the implied false discovery proportion (FDP) at any given threshold. Storey (2003) proposes estimating \\(\\pi_0\\) from the distribution of p-values: under the null, p-values are uniformly distributed on \\([0, 1]\\), so the density of p-values in the “flat” region (away from zero) reveals \\(\\pi_0\\).\n\\[\n\\hat{\\pi}_0(\\lambda) = \\frac{\\#\\{p_i &gt; \\lambda\\}}{M(1 - \\lambda)}\n\\tag{21.3}\\]\nfor a tuning parameter \\(\\lambda\\). The estimated false discovery proportion at threshold \\(t^*\\) is then:\n\\[\n\\widehat{\\text{FDP}}(t^*) = \\frac{\\hat{\\pi}_0 \\cdot M \\cdot P(|t| &gt; t^* | H_0)}{\\#\\{|t_i| &gt; t^*\\}}\n\\tag{21.4}\\]\n\ndef estimate_pi0(p_values, lambdas=np.arange(0.05, 0.95, 0.05)):\n    \"\"\"\n    Estimate pi_0 (proportion of true nulls) using Storey (2003).\n    Uses bootstrap to select optimal lambda.\n    \"\"\"\n    M = len(p_values)\n    pi0_estimates = []\n    \n    for lam in lambdas:\n        n_above = (p_values &gt; lam).sum()\n        pi0 = n_above / (M * (1 - lam))\n        pi0_estimates.append({'lambda': lam, 'pi0': min(pi0, 1.0)})\n    \n    pi0_df = pd.DataFrame(pi0_estimates)\n    \n    # Use the median of estimates for robustness\n    # (alternatives: bootstrap, spline smoothing)\n    pi0_hat = pi0_df['pi0'].median()\n    \n    return pi0_hat, pi0_df\n\npi0_hat, pi0_df = estimate_pi0(zoo_df['p_value'].values)\n\nprint(f\"Estimated π₀ (proportion of true nulls): {pi0_hat:.3f}\")\nprint(f\"Implied proportion of true factors: {1 - pi0_hat:.3f}\")\nprint(f\"Estimated number of true factors: {(1 - pi0_hat) * len(zoo_df):.0f} \"\n      f\"out of {len(zoo_df)}\")\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: p-value histogram\naxes[0].hist(zoo_df['p_value'], bins=20, density=True,\n             color='#2C5F8A', alpha=0.7, edgecolor='white')\naxes[0].axhline(y=pi0_hat, color='#C0392B', linewidth=2, linestyle='--',\n                label=f'π₀ = {pi0_hat:.2f} (null density)')\naxes[0].axhline(y=1, color='gray', linewidth=1, linestyle=':',\n                label='Uniform (all null)')\naxes[0].set_xlabel('p-value')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Panel A: p-Value Distribution')\naxes[0].legend()\n\n# Panel B: pi_0 estimates across lambda\naxes[1].plot(pi0_df['lambda'], pi0_df['pi0'],\n             color='#2C5F8A', linewidth=2, marker='o', markersize=4)\naxes[1].axhline(y=pi0_hat, color='#C0392B', linestyle='--',\n                linewidth=1.5, label=f'Median π₀ = {pi0_hat:.2f}')\naxes[1].set_xlabel('λ')\naxes[1].set_ylabel('π₀(λ)')\naxes[1].set_title('Panel B: π₀ Estimates by λ')\naxes[1].legend()\naxes[1].set_ylim([0, 1.05])\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.6\n\n\n\n\n\n21.6.2 FDP Curve\nUsing \\(\\hat{\\pi}_0\\), we compute the estimated FDP at every possible t-threshold:\n\n\n\nt_thresholds = np.arange(1.0, 5.01, 0.1)\nfdp_curve = []\n\nfor t_thresh in t_thresholds:\n    n_rejected = (zoo_df['abs_t'] &gt; t_thresh).sum()\n    if n_rejected == 0:\n        fdp_curve.append({'t_threshold': t_thresh, 'n_rejected': 0,\n                           'fdp': 0})\n        continue\n    \n    # Expected false discoveries under null\n    p_null_reject = 2 * (1 - stats.norm.cdf(t_thresh))\n    expected_false = pi0_hat * len(zoo_df) * p_null_reject\n    \n    fdp = min(expected_false / n_rejected, 1.0)\n    fdp_curve.append({'t_threshold': t_thresh, 'n_rejected': n_rejected,\n                       'fdp': fdp})\n\nfdp_df = pd.DataFrame(fdp_curve)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: FDP curve\naxes[0].plot(fdp_df['t_threshold'], fdp_df['fdp'] * 100,\n             color='#C0392B', linewidth=2)\naxes[0].axhline(y=5, color='gray', linestyle='--', linewidth=1,\n                label='5% FDP')\naxes[0].axhline(y=10, color='gray', linestyle=':', linewidth=1,\n                label='10% FDP')\naxes[0].axvline(x=2.0, color='#E67E22', linestyle='--',\n                linewidth=1, label='|t| = 2.0')\naxes[0].axvline(x=3.0, color='#2C5F8A', linestyle='--',\n                linewidth=1, label='|t| = 3.0')\naxes[0].set_xlabel('|t| Threshold')\naxes[0].set_ylabel('Estimated FDP (%)')\naxes[0].set_title('Panel A: False Discovery Proportion')\naxes[0].legend(fontsize=8)\naxes[0].set_ylim([-2, 80])\n\n# Panel B: Number of rejections\naxes[1].plot(fdp_df['t_threshold'], fdp_df['n_rejected'],\n             color='#2C5F8A', linewidth=2)\naxes[1].set_xlabel('|t| Threshold')\naxes[1].set_ylabel('Number of Discoveries')\naxes[1].set_title('Panel B: Discoveries vs Threshold')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.7",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-bootstrap",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-bootstrap",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.7 Bootstrap-Based Multiple Testing",
    "text": "21.7 Bootstrap-Based Multiple Testing\n\n21.7.1 The White Reality Check\nAnalytical corrections assume known distributions. When return distributions are non-normal (heavy tails, skewness, serial correlation), as they are in Vietnam, bootstrap methods provide more reliable inference.\nWhite (2000) proposes a bootstrap reality check that accounts for data snooping across multiple strategies. The null hypothesis is that the best strategy has zero expected return. The procedure:\n\nGenerate \\(B\\) bootstrap samples of the time series (block bootstrap to preserve serial dependence).\nFor each bootstrap sample, compute the t-statistic for every factor.\nThe p-value for factor \\(i\\) is the proportion of bootstrap samples in which the maximum t-statistic across all factors exceeds the observed t-statistic of factor \\(i\\).\n\n\ndef white_reality_check(factor_returns_dict, n_bootstrap=1000,\n                          block_size=6, seed=42):\n    \"\"\"\n    White (2000) Reality Check for data snooping.\n    \n    Parameters\n    ----------\n    factor_returns_dict : dict\n        {factor_name: pd.Series of monthly returns}\n    n_bootstrap : int\n        Number of bootstrap replications\n    block_size : int\n        Block length for circular block bootstrap\n    \n    Returns\n    -------\n    DataFrame with original t-stats and bootstrap p-values\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Align all factor returns to common dates\n    all_names = list(factor_returns_dict.keys())\n    common_dates = None\n    for name in all_names:\n        dates = set(factor_returns_dict[name].index)\n        common_dates = dates if common_dates is None else common_dates & dates\n    common_dates = sorted(common_dates)\n    T = len(common_dates)\n    \n    # Build return matrix (T x M)\n    M = len(all_names)\n    return_matrix = np.column_stack([\n        factor_returns_dict[name].reindex(common_dates).values\n        for name in all_names\n    ])\n    \n    # Observed t-statistics\n    means = np.nanmean(return_matrix, axis=0)\n    ses = np.nanstd(return_matrix, axis=0) / np.sqrt(T)\n    t_obs = means / np.where(ses &gt; 0, ses, np.nan)\n    \n    # Block bootstrap\n    n_blocks = int(np.ceil(T / block_size))\n    max_t_bootstrap = np.zeros(n_bootstrap)\n    \n    for b in range(n_bootstrap):\n        # Circular block bootstrap\n        block_starts = rng.integers(0, T, size=n_blocks)\n        indices = np.concatenate([\n            np.arange(start, start + block_size) % T\n            for start in block_starts\n        ])[:T]\n        \n        boot_returns = return_matrix[indices, :]\n        \n        # Center under null (subtract original mean)\n        boot_centered = boot_returns - means[np.newaxis, :]\n        \n        # Compute t-stats under null\n        boot_means = np.nanmean(boot_centered, axis=0)\n        boot_ses = np.nanstd(boot_centered, axis=0) / np.sqrt(T)\n        boot_t = boot_means / np.where(boot_ses &gt; 0, boot_ses, np.nan)\n        \n        max_t_bootstrap[b] = np.nanmax(np.abs(boot_t))\n    \n    # Bootstrap p-values (one-sided: is obs_t extreme relative to max?)\n    boot_pvals = np.array([\n        np.mean(max_t_bootstrap &gt;= abs(t)) if np.isfinite(t) else 1.0\n        for t in t_obs\n    ])\n    \n    results = pd.DataFrame({\n        'factor': all_names,\n        't_stat_obs': t_obs,\n        'boot_pval': boot_pvals\n    }).set_index('factor')\n    \n    return results, max_t_bootstrap\n\n# Collect factor return series\nfactor_returns_dict = {\n    name: zoo_results[name]['returns']\n    for name in zoo_results\n    if 'returns' in zoo_results[name]\n}\n\nboot_results, max_t_dist = white_reality_check(\n    factor_returns_dict, n_bootstrap=1000, block_size=6\n)\n\nboot_results = boot_results.sort_values('t_stat_obs', ascending=False)\n\nprint(\"White Reality Check Results (top 20):\")\nprint(boot_results.head(20).round(4).to_string())\nprint(f\"\\nFactors surviving (boot p &lt; 0.05): \"\n      f\"{(boot_results['boot_pval'] &lt; 0.05).sum()}\")\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\nax.hist(max_t_dist, bins=40, density=True, color='#BDC3C7',\n        alpha=0.7, edgecolor='white', label='Bootstrap null\\n(max |t| across all factors)')\n\n# 95th percentile\np95 = np.percentile(max_t_dist, 95)\nax.axvline(x=p95, color='#C0392B', linewidth=2, linestyle='--',\n           label=f'95th percentile = {p95:.2f}')\n\n# Observed top factor t-statistics\ntop_5 = boot_results.head(5)\nfor i, (name, row) in enumerate(top_5.iterrows()):\n    ax.axvline(x=abs(row['t_stat_obs']),\n               color=plt.cm.Set1(i), linewidth=1.5, linestyle=':',\n               label=f'{name}: |t| = {abs(row[\"t_stat_obs\"]):.2f}')\n\nax.set_xlabel('Maximum |t-statistic|')\nax.set_ylabel('Density')\nax.set_title(\"White's Reality Check: Bootstrap Null Distribution\")\nax.legend(fontsize=8, loc='upper right')\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.8\n\n\n\n\n\n21.7.2 Romano-Wolf Step-Down Bootstrap\nThe Romano and Wolf (2005) step-down procedure improves on the White Reality Check by iteratively removing rejected hypotheses and re-testing the remaining ones, gaining power at each step:\n\ndef romano_wolf_stepdown(factor_returns_dict, n_bootstrap=1000,\n                           block_size=6, alpha=0.05, seed=42):\n    \"\"\"\n    Romano-Wolf (2005) step-down procedure for FWER control.\n    More powerful than single-step methods because rejected\n    hypotheses are removed before re-testing.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    all_names = list(factor_returns_dict.keys())\n    common_dates = None\n    for name in all_names:\n        dates = set(factor_returns_dict[name].index)\n        common_dates = dates if common_dates is None else common_dates & dates\n    common_dates = sorted(common_dates)\n    T = len(common_dates)\n    M = len(all_names)\n    \n    return_matrix = np.column_stack([\n        factor_returns_dict[name].reindex(common_dates).values\n        for name in all_names\n    ])\n    \n    means = np.nanmean(return_matrix, axis=0)\n    ses = np.nanstd(return_matrix, axis=0) / np.sqrt(T)\n    t_obs = np.abs(means / np.where(ses &gt; 0, ses, np.nan))\n    \n    # Sort by observed t-stat (descending)\n    order = np.argsort(-t_obs)\n    t_sorted = t_obs[order]\n    names_sorted = [all_names[i] for i in order]\n    \n    # Step-down procedure\n    rejected = set()\n    remaining = set(range(M))\n    \n    for step in range(M):\n        if not remaining:\n            break\n        \n        remaining_idx = sorted(remaining)\n        \n        # Bootstrap max t-stat among remaining\n        n_blocks = int(np.ceil(T / block_size))\n        max_t_boot = np.zeros(n_bootstrap)\n        \n        for b in range(n_bootstrap):\n            block_starts = rng.integers(0, T, size=n_blocks)\n            indices = np.concatenate([\n                np.arange(start, start + block_size) % T\n                for start in block_starts\n            ])[:T]\n            \n            boot_returns = return_matrix[indices][:, remaining_idx]\n            boot_centered = boot_returns - means[remaining_idx]\n            \n            boot_means = np.nanmean(boot_centered, axis=0)\n            boot_ses = np.nanstd(boot_centered, axis=0) / np.sqrt(T)\n            boot_t = np.abs(boot_means / np.where(boot_ses &gt; 0, boot_ses, np.nan))\n            \n            max_t_boot[b] = np.nanmax(boot_t)\n        \n        # Critical value\n        cv = np.percentile(max_t_boot, (1 - alpha) * 100)\n        \n        # Test the most significant remaining factor\n        # Find the factor with highest t among remaining\n        best_remaining = max(remaining, key=lambda j: t_obs[j])\n        \n        if t_obs[best_remaining] &gt; cv:\n            rejected.add(best_remaining)\n            remaining.remove(best_remaining)\n        else:\n            break  # Cannot reject any more\n    \n    results = pd.DataFrame({\n        'factor': all_names,\n        't_stat': t_obs,\n        'rejected': [i in rejected for i in range(M)]\n    }).sort_values('t_stat', ascending=False)\n    \n    return results\n\nrw_results = romano_wolf_stepdown(\n    factor_returns_dict, n_bootstrap=500, block_size=6\n)\n\nprint(f\"Romano-Wolf rejections: {rw_results['rejected'].sum()} / {len(rw_results)}\")\nprint(\"\\nRejected factors:\")\nprint(rw_results[rw_results['rejected']][['factor', 't_stat']].to_string())",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-oos",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-oos",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.8 Out-of-Sample Validation",
    "text": "21.8 Out-of-Sample Validation\n\n21.8.1 Pre-Publication vs. Post-Publication Decay\nMcLean and Pontiff (2016) find that anomaly returns decline by 32% after publication in the U.S. market, suggesting that roughly one-third of the premium was due to mispricing that was corrected by informed trading. For Vietnam, we use a time-series split as a proxy for out-of-sample testing:\n\n# Split at midpoint of available data\nall_dates = sorted(panel['month_end'].unique())\nmid_date = all_dates[len(all_dates) // 2]\n\noos_comparison = []\n\nfor name in zoo_results:\n    ls = zoo_results[name]['returns']\n    \n    in_sample = ls[ls.index &lt;= mid_date]\n    out_sample = ls[ls.index &gt; mid_date]\n    \n    if len(in_sample) &lt; 36 or len(out_sample) &lt; 36:\n        continue\n    \n    is_mean = in_sample.mean() * 12\n    is_t = in_sample.mean() / (in_sample.std() / np.sqrt(len(in_sample)))\n    \n    oos_mean = out_sample.mean() * 12\n    oos_t = out_sample.mean() / (out_sample.std() / np.sqrt(len(out_sample)))\n    \n    oos_comparison.append({\n        'signal': name,\n        'is_return': is_mean,\n        'is_t': is_t,\n        'oos_return': oos_mean,\n        'oos_t': oos_t,\n        'decay_pct': (1 - oos_mean / is_mean) * 100 if is_mean != 0 else np.nan,\n        'same_sign': np.sign(is_mean) == np.sign(oos_mean),\n        'category': zoo_results[name].get('category', 'Other')\n    })\n\noos_df = pd.DataFrame(oos_comparison)\n\nprint(f\"Split date: {mid_date.strftime('%Y-%m')}\")\nprint(f\"Factors with same sign IS and OOS: \"\n      f\"{oos_df['same_sign'].mean():.1%}\")\nprint(f\"Average OOS decay: {oos_df['decay_pct'].median():.1f}%\")\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Scatter\ncategory_colors = {\n    'Value': '#2C5F8A', 'Size': '#1ABC9C', 'Profitability': '#27AE60',\n    'Investment': '#E67E22', 'Momentum': '#C0392B', 'Risk': '#8E44AD',\n    'Liquidity': '#3498DB', 'Quality': '#F1C40F', 'Dividend': '#95A5A6'\n}\n\nfor cat in oos_df['category'].unique():\n    subset = oos_df[oos_df['category'] == cat]\n    axes[0].scatter(subset['is_return'] * 100, subset['oos_return'] * 100,\n                     color=category_colors.get(cat, '#BDC3C7'),\n                     s=50, alpha=0.7, label=cat, edgecolors='white')\n\nlim = max(abs(oos_df['is_return'].max()), abs(oos_df['oos_return'].max())) * 100 + 2\naxes[0].plot([-lim, lim], [-lim, lim], 'k--', linewidth=1, alpha=0.5)\naxes[0].plot([-lim, lim], [0, 0], color='gray', linewidth=0.5)\naxes[0].plot([0, 0], [-lim, lim], color='gray', linewidth=0.5)\naxes[0].set_xlabel('In-Sample Return (% ann.)')\naxes[0].set_ylabel('Out-of-Sample Return (% ann.)')\naxes[0].set_title('Panel A: IS vs OOS Factor Returns')\naxes[0].legend(fontsize=7, ncol=2)\n\n# Panel B: Decay by category\ncat_decay = (\n    oos_df.groupby('category')\n    .agg(\n        median_decay=('decay_pct', 'median'),\n        pct_same_sign=('same_sign', 'mean'),\n        n=('signal', 'count')\n    )\n    .sort_values('median_decay')\n)\n\ncolors_bar = [category_colors.get(cat, '#BDC3C7') for cat in cat_decay.index]\naxes[1].barh(range(len(cat_decay)), cat_decay['median_decay'],\n             color=colors_bar, alpha=0.85, edgecolor='white')\naxes[1].set_yticks(range(len(cat_decay)))\naxes[1].set_yticklabels(cat_decay.index)\naxes[1].set_xlabel('Median OOS Decay (%)')\naxes[1].set_title('Panel B: OOS Decay by Category')\naxes[1].axvline(x=0, color='gray', linewidth=0.8)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.9",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-survivors",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-survivors",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.9 Which Factors Survive?",
    "text": "21.9 Which Factors Survive?\n\n21.9.1 A Multi-Hurdle Filter\nWe now combine all the evidence, including multiple testing corrections, out-of-sample performance, and economic plausibility, to identify the factors that deserve credence in the Vietnamese market.\n\n# Merge all test results\nfinal = zoo_df.copy()\n\n# Add OOS results\noos_lookup = oos_df.set_index('signal')\nfor col in ['oos_return', 'oos_t', 'same_sign']:\n    final[col] = final.index.map(lambda x: oos_lookup.loc[x, col]\n                                  if x in oos_lookup.index else np.nan)\n\n# Add bootstrap results\nboot_lookup = boot_results\nfinal['boot_pval'] = final.index.map(\n    lambda x: boot_lookup.loc[x, 'boot_pval']\n    if x in boot_lookup.index else np.nan\n)\n\n# Multi-hurdle criteria\nfinal['pass_t2'] = final['abs_t'] &gt; 2.0\nfinal['pass_t3'] = final['abs_t'] &gt; 3.0\nfinal['pass_bh'] = final['bh_reject']\nfinal['pass_boot'] = final['boot_pval'] &lt; 0.05\nfinal['pass_oos_sign'] = final['same_sign'] == True\nfinal['pass_oos_t'] = final['oos_t'].abs() &gt; 1.5  # Relaxed OOS threshold\n\n# Count hurdles passed\nhurdle_cols = ['pass_t2', 'pass_t3', 'pass_bh', 'pass_boot',\n                'pass_oos_sign', 'pass_oos_t']\nfinal['n_hurdles'] = final[hurdle_cols].sum(axis=1)\n\n# \"Robust\" = passes at least 5 of 6 hurdles\nfinal['robust'] = final['n_hurdles'] &gt;= 5\n\nrobust_factors = final[final['robust']].sort_values('abs_t', ascending=False)\n\nprint(f\"Multi-Hurdle Filter Results:\")\nprint(f\"  Pass |t| &gt; 2.0:        {final['pass_t2'].sum()}\")\nprint(f\"  Pass |t| &gt; 3.0:        {final['pass_t3'].sum()}\")\nprint(f\"  Pass BH (q=0.05):      {final['pass_bh'].sum()}\")\nprint(f\"  Pass bootstrap:        {final['pass_boot'].sum()}\")\nprint(f\"  Pass OOS sign:         {final['pass_oos_sign'].sum()}\")\nprint(f\"  Pass OOS |t| &gt; 1.5:    {final['pass_oos_t'].sum()}\")\nprint(f\"\\n  ROBUST (≥5/6 hurdles): {final['robust'].sum()}\")\n\nif len(robust_factors) &gt; 0:\n    print(f\"\\nRobust Factors:\")\n    display_cols = ['ann_return', 't_stat', 'bh_p_corrected',\n                     'oos_return', 'oos_t', 'n_hurdles', 'category']\n    available_cols = [c for c in display_cols if c in robust_factors.columns]\n    print(robust_factors[available_cols].round(3).to_string())\n\n\n\n\ntop25 = final.sort_values('abs_t', ascending=False).head(25)\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\nhurdle_matrix = top25[hurdle_cols].astype(float).values\n\nsns.heatmap(hurdle_matrix, ax=ax,\n            xticklabels=['|t|&gt;2', '|t|&gt;3', 'BH', 'Boot', 'OOS sign', 'OOS |t|'],\n            yticklabels=top25.index,\n            cmap=['#E74C3C', '#27AE60'],\n            cbar=False, linewidths=0.5, linecolor='white',\n            annot=np.where(hurdle_matrix == 1, '✓', '✗'),\n            fmt='')\n\n# Add hurdle count column\nfor i, (_, row) in enumerate(top25.iterrows()):\n    ax.text(len(hurdle_cols) + 0.3, i + 0.5,\n            f\"{int(row['n_hurdles'])}/6\",\n            va='center', fontsize=9,\n            fontweight='bold' if row['n_hurdles'] &gt;= 5 else 'normal',\n            color='#27AE60' if row['n_hurdles'] &gt;= 5 else '#C0392B')\n\nax.set_title('Multi-Hurdle Test Results (Top 25 Anomalies)')\nplt.tight_layout()\nplt.show()\n\n\nFigure 21.10",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-implications",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-implications",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.10 Implications for Vietnamese Factor Research",
    "text": "21.10 Implications for Vietnamese Factor Research\nThe analysis in this chapter yields several practical implications:\nThe conventional threshold is insufficient. With \\(M \\approx 50\\) anomalies tested simultaneously, requiring only \\(|t| &gt; 2.0\\) virtually guarantees false discoveries. Vietnamese researchers should adopt a minimum threshold of \\(|t| &gt; 3.0\\) for individual factors, consistent with Harvey, Liu, and Zhu (2016), and should report BH-adjusted p-values for any study testing multiple signals.\nThe small cross-section amplifies noise. With 600-800 stocks, quintile portfolios contain only 120-160 stocks each. This creates noisier portfolio returns and wider confidence intervals than in U.S. studies with 4,000+ stocks. Strategies that appear significant in the U.S. at \\(|t| = 2.5\\) may have \\(|t| &lt; 1.5\\) in Vietnam simply due to the smaller sample. This is not evidence that the factor doesn’t exist in Vietnam, it is evidence that the Vietnamese data lack power to detect it.\nOut-of-sample validation is essential. The time-series split reveals substantial decay for many anomalies, consistent with McLean and Pontiff (2016). Factors that survive both in-sample and out-of-sample with consistent sign and magnitude are rare and valuable.\nCategory matters. Momentum and profitability signals tend to have the highest replication rates across markets (Fama and French 2012; Jacobs and Müller 2020). Value and investment signals are more country-specific (Griffin 2002). Liquidity signals are likely to be strongest in emerging markets like Vietnam, where trading frictions are largest.\nReport the full battery. Any study that presents a new factor for the Vietnamese market should report: (i) the raw t-statistic, (ii) the BH-adjusted p-value given the number of tests in the study, (iii) out-of-sample performance in a held-out period, and (iv) sensitivity to construction choices (from the previous chapter). This reporting standard would dramatically improve the credibility of Vietnamese factor research.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-summary",
    "href": "25_factor_zoo_and_multiple_testing.html#sec-factor-zoo-summary",
    "title": "21  Factor Zoo and Multiple Testing",
    "section": "21.11 Summary",
    "text": "21.11 Summary\n\n\n\nTable 21.1: Summary of multiple testing methods and their properties.\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nControls\nThreshold (approx.)\nSurviving\nPhilosophy\n\n\n\n\nUnadjusted\nSingle test\n|t| &gt; 2.0\nMany\nNo correction\n\n\nBonferroni\nFWER\n|t| &gt; 3.3–4.0\nFew\nZero false positives\n\n\nHolm\nFWER\nStepwise\nFew\nSlightly less conservative\n\n\nBH\nFDR at q\nAdaptive\nModerate\nTolerates some false positives\n\n\nHLZ\nProfession-wide\n|t| &gt; 3.0–3.78\nModerate\nPrior over all tested factors\n\n\nWhite/RW Bootstrap\nFWER (data-driven)\nBootstrap CV\nFew\nAccounts for non-normality\n\n\nStorey FDP\nFDP estimate\nContinuous\nDiagnostic\nEstimates true null proportion\n\n\nMulti-hurdle\nCombined\nMultiple tests\nMost robust\nRequires convergent evidence\n\n\n\n\n\n\nThe factor zoo is not a uniquely American phenomenon. The temptation to test many signals and report the best-performing ones exists in every market. In Vietnam, where the data are shorter, noisier, and the academic community is growing rapidly, the risk of false discovery is high. The tools developed in this chapter, including BH adjustment, bootstrap reality checks, Storey’s \\(\\pi_0\\) estimation, out-of-sample splits, and the multi-hurdle filter, provide a principled framework for separating genuine factors from statistical noise.\nThe honest conclusion is likely to be humbling: of the dozens of anomalies documented in developed markets, only a handful survive rigorous multiple testing in Vietnamese data. Those survivors (probably concentrated in momentum, profitability, and liquidity) form the foundation for credible asset pricing research in this market.\n\n\n\n\n\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300.\n\n\nBonferroni, Carlo. 1936. “Teoria Statistica Delle Classi e Calcolo Delle Probabilita.” Pubblicazioni Del R Istituto Superiore Di Scienze Economiche e Commericiali Di Firenze 8: 3–62.\n\n\nChen, Andrew, and Tom Zimmermann. 2022. “Open Source Cross-Sectional Asset Pricing.” &gt; Critical Finance Review 11 (02): 207–64.\n\n\nCochrane, John H. 2011. “Presidential Address: Discount Rates.” The Journal of Finance 66 (4): 1047–1108.\n\n\nFama, Eugene F, and Kenneth R French. 2012. “Size, Value, and Momentum in International Stock Returns.” Journal of Financial Economics 105 (3): 457–72.\n\n\nGriffin, John M. 2002. “Are the Fama and French Factors Global or Country Specific?” The Review of Financial Studies 15 (3): 783–803.\n\n\nHarvey, Campbell R, Yan Liu, and Heqing Zhu. 2016. “… and the Cross-Section of Expected Returns.” The Review of Financial Studies 29 (1): 5–68.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2020. “Replicating Anomalies.” The Review of Financial Studies 33 (5): 2019–2133.\n\n\nJacobs, Heiko, and Sebastian Müller. 2020. “Anomalies Across the Globe: Once Public, No Longer Existent?” Journal of Financial Economics 135 (1): 213–30.\n\n\nJensen, Theis Ingerslev, Bryan Kelly, and Lasse Heje Pedersen. 2023. “Is There a Replication Crisis in Finance?” The Journal of Finance 78 (5): 2465–2518.\n\n\nMcLean, R David, and Jeffrey Pontiff. 2016. “Does Academic Research Destroy Stock Return Predictability?” The Journal of Finance 71 (1): 5–32.\n\n\nRomano, Joseph P, and Michael Wolf. 2005. “Exact and Approximate Stepdown Methods for Multiple Hypothesis Testing.” Journal of the American Statistical Association 100 (469): 94–108.\n\n\nStorey, John D. 2003. “The Positive False Discovery Rate: A Bayesian Interpretation and the q-Value.” The Annals of Statistics 31 (6): 2013–35.\n\n\nWhite, Halbert. 2000. “A Reality Check for Data Snooping.” Econometrica 68 (5): 1097–1126.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Factor Zoo and Multiple Testing</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html",
    "href": "26_time_series_vs_cross_section.html",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "",
    "text": "22.1 The Two Testing Frameworks\nEvery factor model makes two distinct claims. The time-series claim is that a set of factors explains the common variation in individual stock returns (i.e., when the factors move, stocks move with them in proportion to their exposures). The cross-sectional claim is that differences in average returns across stocks are explained by differences in their factor exposures (i.e., stocks that load more heavily on a factor earn higher (or lower) average returns in proportion to the factor’s risk premium).\nThese claims are logically related but empirically distinct. A factor can explain time-series variation without explaining the cross-section (if the factor’s risk premium is zero, exposure to it creates no return differential). Conversely, a characteristic can predict the cross-section of returns without operating through a traded factor (if the characteristic captures mispricing rather than risk).\nFama and French (2020) formalize this distinction and show that the two approaches can give materially different answers about which factors matter. Goyal and Jegadeesh (2018) go further and demonstrate that time-series and cross-sectional tests can disagree about the same model (e.g., a factor can have a significant time-series alpha in one test and an insignificant cross-sectional premium in the other, or vice versa). Understanding why they disagree is essential for interpreting asset pricing evidence.\nThis chapter equips the reader with both toolkits, applied to Vietnamese data, and develops the intuition for when each is appropriate.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-framework",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-framework",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "",
    "text": "22.1.1 Time-Series Regressions\nThe time-series approach tests whether a factor model explains the returns of a set of test assets (typically portfolios). For each test asset \\(i\\), estimate:\n\\[\nR_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,1} f_{1,t} + \\beta_{i,2} f_{2,t} + \\ldots + \\beta_{i,K} f_{K,t} + \\varepsilon_{i,t}\n\\tag{22.1}\\]\nwhere \\(f_{k,t}\\) are the \\(K\\) factor returns. The intercept \\(\\alpha_i\\) measures the average return of asset \\(i\\) that is not explained by the factors. Under the null that the model correctly prices asset \\(i\\), \\(\\alpha_i = 0\\).\nThe joint test of \\(H_0: \\alpha_1 = \\alpha_2 = \\ldots = \\alpha_N = 0\\) across all \\(N\\) test assets is performed using the Gibbons, Ross, and Shanken (1989) (GRS) test statistic:\n\\[\n\\text{GRS} = \\frac{T - N - K}{N} \\cdot \\frac{1}{1 + \\bar{f}' \\hat{\\Sigma}_f^{-1} \\bar{f}} \\cdot \\hat{\\alpha}' \\hat{\\Sigma}_\\varepsilon^{-1} \\hat{\\alpha} \\sim F(N, T - N - K)\n\\tag{22.2}\\]\nwhere \\(\\bar{f}\\) is the vector of factor means, \\(\\hat{\\Sigma}_f\\) is the factor covariance matrix, and \\(\\hat{\\Sigma}_\\varepsilon\\) is the residual covariance matrix. A large GRS statistic rejects the model.\n\n\n22.1.2 Cross-Sectional Regressions (Fama-MacBeth)\nThe Fama and MacBeth (1973) cross-sectional approach tests whether factor exposures are priced (i.e., whether stocks with higher betas on a factor earn higher average returns). The procedure has two stages:\nStage 1 (Time-Series). For each asset, estimate factor betas from a time-series regression over a rolling or full-sample window:\n\\[\nR_{i,t} - R_{f,t} = \\alpha_i + \\beta_{i,1} f_{1,t} + \\ldots + \\beta_{i,K} f_{K,t} + \\varepsilon_{i,t}\n\\tag{22.3}\\]\nStage 2 (Cross-Section). Each month \\(t\\), regress the cross-section of realized excess returns on the estimated betas:\n\\[\nR_{i,t} - R_{f,t} = \\gamma_{0,t} + \\gamma_{1,t} \\hat{\\beta}_{i,1} + \\ldots + \\gamma_{K,t} \\hat{\\beta}_{i,K} + \\eta_{i,t}\n\\tag{22.4}\\]\nThe time-series averages \\(\\bar{\\gamma}_k = \\frac{1}{T} \\sum_t \\gamma_{k,t}\\) estimate the risk premia, and the standard errors use the time-series standard deviation of \\(\\{\\gamma_{k,t}\\}\\).\n\n\n22.1.3 Key Differences\n\n\n\nTable 22.1: Comparison of time-series and cross-sectional testing frameworks.\n\n\n\n\n\n\n\n\n\n\nDimension\nTime-Series\nCross-Section\n\n\n\n\nWhat it tests\nDo factors explain return variation?\nDo factor exposures explain average returns?\n\n\nKey statistic\nAlpha (intercept)\nRisk premium (\\(\\gamma\\))\n\n\nJoint test\nGRS F-test\nChi-squared on \\(\\gamma\\)’s\n\n\nTest assets\nPortfolios (usually)\nIndividual stocks or portfolios\n\n\nFactors\nMust be traded returns\nCan be non-traded (macro, characteristics)\n\n\nNull hypothesis\n\\(\\alpha_i = 0\\) for all \\(i\\)\n\\(\\gamma_k\\) equals factor mean return\n\n\nErrors-in-variables\nNot an issue (factors observed)\nEstimated betas create EIV bias",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-data",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-data",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "22.2 Data Construction",
    "text": "22.2 Data Construction\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom scipy.linalg import inv\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Factor returns\nfactors = client.get_factor_returns(\n    market='vietnam',\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    factors=['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml']\n)\nfactors['month_end'] = pd.to_datetime(factors['month_end'])\nfactors = factors.set_index('month_end')\n\n# Test portfolios: 25 size-BM portfolios (5x5 independent sorts)\ntest_portfolios_25 = client.get_sorted_portfolios(\n    market='vietnam',\n    sort_vars=['size', 'bm'],\n    n_groups=[5, 5],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    weighting='value'\n)\n\n# Test portfolios: 25 size-momentum portfolios\ntest_portfolios_mom = client.get_sorted_portfolios(\n    market='vietnam',\n    sort_vars=['size', 'momentum_12_2'],\n    n_groups=[5, 5],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    weighting='value'\n)\n\n# Test portfolios: 10 industry portfolios\ntest_portfolios_ind = client.get_sorted_portfolios(\n    market='vietnam',\n    sort_vars=['icb_sector'],\n    n_groups=[10],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    weighting='value'\n)\n\n# Monthly individual stock returns for Fama-MacBeth\nstock_returns = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=['ticker', 'month_end', 'monthly_return', 'market_cap']\n)\n\n# Risk-free rate\nrf = client.get_risk_free_rate(\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    frequency='monthly'\n)\n\nprint(f\"Factor months: {len(factors)}\")\nprint(f\"25 Size-BM portfolios: {test_portfolios_25.shape}\")\nprint(f\"25 Size-Mom portfolios: {test_portfolios_mom.shape}\")\nprint(f\"10 Industry portfolios: {test_portfolios_ind.shape}\")\nprint(f\"Stock-months: {len(stock_returns):,}\")\n\n\n# Convert test portfolios to excess returns\n# Each portfolio set is a DataFrame: rows = months, columns = portfolios\ndef prepare_test_assets(port_df, rf_df):\n    \"\"\"Convert portfolio returns to excess returns matrix.\"\"\"\n    port = port_df.set_index('month_end')\n    rf_aligned = rf_df.set_index('month_end')['rf'].reindex(port.index)\n    excess = port.subtract(rf_aligned, axis=0)\n    return excess.dropna()\n\nexcess_25_bm = prepare_test_assets(test_portfolios_25, rf)\nexcess_25_mom = prepare_test_assets(test_portfolios_mom, rf)\nexcess_10_ind = prepare_test_assets(test_portfolios_ind, rf)\n\nprint(f\"Size-BM test assets: {excess_25_bm.shape[1]} portfolios, \"\n      f\"{excess_25_bm.shape[0]} months\")\nprint(f\"Size-Mom test assets: {excess_25_mom.shape[1]} portfolios, \"\n      f\"{excess_25_mom.shape[0]} months\")\nprint(f\"Industry test assets: {excess_10_ind.shape[1]} portfolios, \"\n      f\"{excess_10_ind.shape[0]} months\")",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-ts-tests",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-ts-tests",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "22.3 Time-Series Tests",
    "text": "22.3 Time-Series Tests\n\n22.3.1 Full-Sample Time-Series Regressions\nWe begin by running full-sample time-series regressions of each test portfolio on the Fama-French five-factor model plus momentum:\n\ndef time_series_regressions(excess_returns, factor_df, factor_cols,\n                              cov_type='HAC', maxlags=6):\n    \"\"\"\n    Run time-series regressions of test assets on factors.\n    \n    Returns DataFrame of alphas, betas, t-stats, and R-squared.\n    \"\"\"\n    # Align dates\n    common = excess_returns.index.intersection(factor_df.index)\n    Y = excess_returns.loc[common]\n    X = factor_df.loc[common, factor_cols]\n    X_const = sm.add_constant(X)\n    \n    results = {}\n    for col in Y.columns:\n        y = Y[col].dropna()\n        x = X_const.loc[y.index]\n        \n        model = sm.OLS(y, x).fit(\n            cov_type=cov_type,\n            cov_kwds={'maxlags': maxlags} if cov_type == 'HAC' else {}\n        )\n        \n        result = {\n            'alpha': model.params['const'],\n            'alpha_t': model.tvalues['const'],\n            'alpha_p': model.pvalues['const'],\n            'r_squared': model.rsquared,\n            'r_squared_adj': model.rsquared_adj\n        }\n        for f in factor_cols:\n            result[f'beta_{f}'] = model.params[f]\n            result[f't_{f}'] = model.tvalues[f]\n        \n        results[col] = result\n    \n    return pd.DataFrame(results).T\n\n# Define models to test\nmodels = {\n    'CAPM': ['mkt_excess'],\n    'FF3': ['mkt_excess', 'smb', 'hml'],\n    'FF5': ['mkt_excess', 'smb', 'hml', 'rmw', 'cma'],\n    'FF5+WML': ['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml'],\n}\n\n# Run for 25 Size-BM portfolios\nprint(\"Time-Series Alphas: 25 Size-BM Portfolios\")\nprint(\"=\" * 60)\n\nts_results = {}\nfor model_name, factor_cols in models.items():\n    result = time_series_regressions(\n        excess_25_bm, factors, factor_cols\n    )\n    ts_results[model_name] = result\n    \n    mean_alpha = result['alpha'].mean() * 12\n    mean_abs_alpha = result['alpha'].abs().mean() * 12\n    pct_sig = (result['alpha_p'] &lt; 0.05).mean()\n    mean_r2 = result['r_squared'].mean()\n    \n    print(f\"\\n{model_name}:\")\n    print(f\"  Mean alpha (ann.): {mean_alpha:.4f}\")\n    print(f\"  Mean |alpha| (ann.): {mean_abs_alpha:.4f}\")\n    print(f\"  % sig. at 5%: {pct_sig:.1%}\")\n    print(f\"  Mean R²: {mean_r2:.3f}\")\n\n\n\n22.3.2 The GRS Test\n\ndef grs_test(excess_returns, factor_df, factor_cols):\n    \"\"\"\n    Gibbons, Ross, Shanken (1989) test of H0: all alphas = 0.\n    \n    Returns GRS statistic, p-value, and related diagnostics.\n    \"\"\"\n    common = excess_returns.index.intersection(factor_df.index)\n    Y = excess_returns.loc[common].values  # T x N\n    F = factor_df.loc[common, factor_cols].values  # T x K\n    \n    T, N = Y.shape\n    K = F.shape[1]\n    \n    # Add constant and run OLS for each asset\n    F_const = np.column_stack([np.ones(T), F])\n    \n    # OLS: beta = (X'X)^-1 X'Y\n    XtX_inv = inv(F_const.T @ F_const)\n    B = XtX_inv @ F_const.T @ Y  # (K+1) x N\n    \n    alpha = B[0, :]  # N-vector of intercepts\n    residuals = Y - F_const @ B  # T x N\n    \n    # Residual covariance matrix\n    Sigma_eps = residuals.T @ residuals / (T - K - 1)\n    \n    # Factor mean and covariance\n    f_bar = F.mean(axis=0)\n    Sigma_f = np.cov(F, rowvar=False, ddof=1)\n    \n    # GRS statistic\n    Sigma_eps_inv = inv(Sigma_eps)\n    Sigma_f_inv = inv(Sigma_f) if K &gt; 1 else np.array([[1 / Sigma_f]])\n    \n    sharpe_sq_alpha = alpha @ Sigma_eps_inv @ alpha\n    sharpe_sq_f = f_bar @ Sigma_f_inv @ f_bar if K &gt; 1 else (f_bar[0] ** 2 / Sigma_f)\n    \n    grs_stat = ((T - N - K) / N) * (1 / (1 + sharpe_sq_f)) * sharpe_sq_alpha\n    \n    # p-value from F distribution\n    p_value = 1 - stats.f.cdf(grs_stat, N, T - N - K)\n    \n    # Additional diagnostics\n    # Sharpe ratio of tangency portfolio of factors\n    sr_factors = np.sqrt(sharpe_sq_f)\n    # Sharpe ratio of tangency portfolio including alphas\n    sr_alpha = np.sqrt(sharpe_sq_alpha + sharpe_sq_f)\n    \n    return {\n        'GRS': grs_stat,\n        'p_value': p_value,\n        'df1': N,\n        'df2': T - N - K,\n        'mean_abs_alpha': np.abs(alpha).mean(),\n        'sr_factors': sr_factors,\n        'sr_alpha_plus_factors': sr_alpha,\n        'sr_improvement': sr_alpha - sr_factors,\n        'T': T, 'N': N, 'K': K\n    }\n\n# Run GRS for each model on each set of test assets\nprint(\"GRS Test Results\")\nprint(\"=\" * 80)\nprint(f\"{'Model':&lt;12} {'Test Assets':&lt;18} {'GRS':&gt;8} {'p-value':&gt;10} \"\n      f\"{'|α| ann':&gt;10} {'SR(f)':&gt;8} {'SR(α+f)':&gt;8}\")\nprint(\"-\" * 80)\n\ntest_asset_sets = {\n    '25 Size-BM': excess_25_bm,\n    '25 Size-Mom': excess_25_mom,\n    '10 Industry': excess_10_ind\n}\n\ngrs_all = {}\nfor model_name, factor_cols in models.items():\n    for ta_name, ta_data in test_asset_sets.items():\n        key = f\"{model_name}_{ta_name}\"\n        result = grs_test(ta_data, factors, factor_cols)\n        grs_all[key] = result\n        \n        print(f\"{model_name:&lt;12} {ta_name:&lt;18} \"\n              f\"{result['GRS']:&gt;8.2f} {result['p_value']:&gt;10.4f} \"\n              f\"{result['mean_abs_alpha']*12:&gt;10.4f} \"\n              f\"{result['sr_factors']:&gt;8.3f} \"\n              f\"{result['sr_alpha_plus_factors']:&gt;8.3f}\")\n\n\n\n\nff5_result = ts_results['FF5']\n\n# Reshape alphas into 5x5 matrix\nalpha_matrix = ff5_result['alpha'].values.reshape(5, 5) * 12 * 100\nt_matrix = ff5_result['alpha_t'].values.reshape(5, 5)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Alpha magnitudes\nim = axes[0].imshow(alpha_matrix, cmap='RdBu_r', aspect='auto',\n                     vmin=-np.max(np.abs(alpha_matrix)),\n                     vmax=np.max(np.abs(alpha_matrix)))\nfor i in range(5):\n    for j in range(5):\n        sig = '*' if abs(t_matrix[i, j]) &gt; 2 else ''\n        axes[0].text(j, i, f'{alpha_matrix[i, j]:.1f}{sig}',\n                     ha='center', va='center', fontsize=9,\n                     color='white' if abs(alpha_matrix[i, j]) &gt; 3 else 'black')\n\naxes[0].set_xticks(range(5))\naxes[0].set_xticklabels(['Growth', '2', '3', '4', 'Value'])\naxes[0].set_yticks(range(5))\naxes[0].set_yticklabels(['Small', '2', '3', '4', 'Big'])\naxes[0].set_xlabel('Book-to-Market')\naxes[0].set_ylabel('Size')\naxes[0].set_title('Panel A: FF5 Alphas (% ann.)')\nplt.colorbar(im, ax=axes[0], label='Alpha (% ann.)')\n\n# Panel B: R-squared\nr2_matrix = ff5_result['r_squared'].values.reshape(5, 5) * 100\nim2 = axes[1].imshow(r2_matrix, cmap='YlGn', aspect='auto',\n                       vmin=50, vmax=100)\nfor i in range(5):\n    for j in range(5):\n        axes[1].text(j, i, f'{r2_matrix[i, j]:.0f}%',\n                     ha='center', va='center', fontsize=9)\n\naxes[1].set_xticks(range(5))\naxes[1].set_xticklabels(['Growth', '2', '3', '4', 'Value'])\naxes[1].set_yticks(range(5))\naxes[1].set_yticklabels(['Small', '2', '3', '4', 'Big'])\naxes[1].set_xlabel('Book-to-Market')\naxes[1].set_ylabel('Size')\naxes[1].set_title('Panel B: R-squared (%)')\nplt.colorbar(im2, ax=axes[1], label='R² (%)')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 22.1\n\n\n\n\n\n22.3.3 Model Comparison via GRS\n\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\n\nmodel_names = list(models.keys())\nta_names = list(test_asset_sets.keys())\nx = np.arange(len(model_names))\nwidth = 0.25\n\ncolors_ta = ['#2C5F8A', '#C0392B', '#27AE60']\n\nfor i, ta_name in enumerate(ta_names):\n    grs_vals = [grs_all[f\"{m}_{ta_name}\"]['GRS'] for m in model_names]\n    bars = ax.bar(x + i * width, grs_vals, width, alpha=0.85,\n                   color=colors_ta[i], label=ta_name, edgecolor='white')\n\nax.set_xticks(x + width)\nax.set_xticklabels(model_names)\nax.set_ylabel('GRS Statistic')\nax.set_title('GRS Test: Model Comparison Across Test Asset Sets')\nax.legend()\n\n# Add critical value line (5% significance)\n# Approximate: depends on N, T, K\nax.axhline(y=1.8, color='gray', linestyle='--', linewidth=1,\n           label='~5% critical value')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 22.2",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-cs-tests",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-cs-tests",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "22.4 Cross-Sectional Tests",
    "text": "22.4 Cross-Sectional Tests\n\n22.4.1 Fama-MacBeth with Portfolio Test Assets\nWe implement the Fama-MacBeth two-pass procedure using the 25 size-BM portfolios as test assets:\n\ndef fama_macbeth_two_pass(excess_returns, factor_df, factor_cols,\n                            beta_window='full', rolling_window=60,\n                            shanken_correction=True):\n    \"\"\"\n    Fama-MacBeth (1973) two-pass cross-sectional regression.\n    \n    Parameters\n    ----------\n    beta_window : str\n        'full' for full-sample betas, 'rolling' for rolling betas.\n    shanken_correction : bool\n        Apply Shanken (1992) errors-in-variables correction.\n    \n    Returns\n    -------\n    Dictionary with estimated risk premia, t-statistics, and diagnostics.\n    \"\"\"\n    common = excess_returns.index.intersection(factor_df.index)\n    Y = excess_returns.loc[common]\n    F = factor_df.loc[common, factor_cols]\n    \n    T = len(common)\n    N = Y.shape[1]\n    K = len(factor_cols)\n    months = sorted(common)\n    \n    # ===== STAGE 1: Estimate betas =====\n    if beta_window == 'full':\n        # Full-sample betas (simpler, used in GRS context)\n        F_const = sm.add_constant(F)\n        betas = {}\n        for col in Y.columns:\n            model = sm.OLS(Y[col], F_const).fit()\n            betas[col] = {f: model.params[f] for f in factor_cols}\n        beta_df = pd.DataFrame(betas).T  # N x K\n        \n        # Same betas used every month\n        beta_by_month = {m: beta_df for m in months}\n    \n    elif beta_window == 'rolling':\n        # Rolling betas (more realistic, avoids look-ahead)\n        beta_by_month = {}\n        for t_idx in range(rolling_window, T):\n            month = months[t_idx]\n            window_start = months[t_idx - rolling_window]\n            \n            Y_win = Y.loc[window_start:months[t_idx - 1]]\n            F_win = sm.add_constant(F.loc[window_start:months[t_idx - 1]])\n            \n            betas_t = {}\n            for col in Y.columns:\n                y = Y_win[col].dropna()\n                x = F_win.loc[y.index]\n                if len(y) &lt; 24:\n                    betas_t[col] = {f: np.nan for f in factor_cols}\n                    continue\n                model = sm.OLS(y, x).fit()\n                betas_t[col] = {f: model.params[f] for f in factor_cols}\n            \n            beta_by_month[month] = pd.DataFrame(betas_t).T\n    \n    # ===== STAGE 2: Cross-sectional regressions =====\n    gamma_list = []\n    \n    for month in months:\n        if month not in beta_by_month:\n            continue\n        \n        betas_t = beta_by_month[month]\n        returns_t = Y.loc[month]\n        \n        # Align\n        valid = betas_t.dropna().index.intersection(returns_t.dropna().index)\n        if len(valid) &lt; K + 5:\n            continue\n        \n        y = returns_t[valid].values\n        X = sm.add_constant(betas_t.loc[valid, factor_cols].values)\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            gammas = {'month': month, 'gamma_0': model.params[0]}\n            for j, f in enumerate(factor_cols):\n                gammas[f'gamma_{f}'] = model.params[j + 1]\n            gammas['r_squared'] = model.rsquared\n            gammas['n_assets'] = len(valid)\n            gamma_list.append(gammas)\n        except Exception:\n            pass\n    \n    gamma_df = pd.DataFrame(gamma_list)\n    \n    if len(gamma_df) == 0:\n        return None\n    \n    # ===== INFERENCE =====\n    # Time-series averages of gammas\n    gamma_cols = ['gamma_0'] + [f'gamma_{f}' for f in factor_cols]\n    \n    results = {}\n    for col in gamma_cols:\n        g = gamma_df[col]\n        mean = g.mean()\n        se_fm = g.std() / np.sqrt(len(g))\n        t_fm = mean / se_fm if se_fm &gt; 0 else np.nan\n        \n        results[col] = {\n            'estimate': mean,\n            'se_fm': se_fm,\n            't_fm': t_fm,\n        }\n    \n    # Shanken (1992) correction for errors-in-variables\n    if shanken_correction and beta_window == 'full':\n        Sigma_f = F[factor_cols].cov().values\n        \n        # Shanken correction factor: (1 + lambda' Sigma_f^-1 lambda)\n        gamma_factor = np.array([results[f'gamma_{f}']['estimate']\n                                  for f in factor_cols])\n        try:\n            Sigma_f_inv = inv(Sigma_f)\n            c_shanken = 1 + gamma_factor @ Sigma_f_inv @ gamma_factor\n        except Exception:\n            c_shanken = 1.0\n        \n        for col in gamma_cols:\n            results[col]['se_shanken'] = results[col]['se_fm'] * np.sqrt(c_shanken)\n            results[col]['t_shanken'] = (results[col]['estimate']\n                                          / results[col]['se_shanken']\n                                          if results[col]['se_shanken'] &gt; 0 else np.nan)\n    \n    # Compare estimated premia to factor mean returns\n    factor_means = F[factor_cols].mean()\n    for f in factor_cols:\n        results[f'gamma_{f}']['factor_mean'] = factor_means[f]\n    \n    # Cross-sectional R-squared\n    results['avg_cs_r2'] = gamma_df['r_squared'].mean()\n    \n    return {\n        'results': pd.DataFrame(results).T,\n        'gamma_ts': gamma_df,\n        'beta_df': beta_by_month.get(months[-1], None)\n    }\n\n# Run Fama-MacBeth for each model\nprint(\"Fama-MacBeth Cross-Sectional Results: 25 Size-BM Portfolios\")\nprint(\"=\" * 80)\n\nfm_results = {}\nfor model_name, factor_cols in models.items():\n    for beta_type in ['full', 'rolling']:\n        key = f\"{model_name}_{beta_type}\"\n        fm = fama_macbeth_two_pass(\n            excess_25_bm, factors, factor_cols,\n            beta_window=beta_type,\n            rolling_window=60,\n            shanken_correction=(beta_type == 'full')\n        )\n        if fm is None:\n            continue\n        fm_results[key] = fm\n        \n        print(f\"\\n{model_name} (betas: {beta_type}):\")\n        res = fm['results']\n        for idx, row in res.iterrows():\n            if 'gamma' in idx:\n                t_col = 't_shanken' if 't_shanken' in row and pd.notna(row.get('t_shanken')) else 't_fm'\n                f_mean = row.get('factor_mean', '')\n                f_str = f\"  f_mean={f_mean:.4f}\" if isinstance(f_mean, float) else \"\"\n                print(f\"  {idx:&lt;16}: γ = {row['estimate']:.5f}, \"\n                      f\"t = {row[t_col]:.2f}{f_str}\")\n        if 'avg_cs_r2' in res.index:\n            print(f\"  Avg CS R²: {res.loc['avg_cs_r2', 'estimate']:.3f}\")\n\n\n\n22.4.2 The Shanken Correction\nThe Shanken (1992) correction addresses the errors-in-variables (EIV) problem inherent in the two-pass procedure. Because betas are estimated with error in Stage 1, the Stage 2 standard errors are understated. The correction inflates the standard errors by a factor that depends on the estimated risk premia and the factor covariance matrix:\n\\[\n\\text{Var}(\\hat{\\gamma})_{\\text{Shanken}} = \\text{Var}(\\hat{\\gamma})_{\\text{FM}} \\times \\left(1 + \\hat{\\gamma}' \\hat{\\Sigma}_f^{-1} \\hat{\\gamma}\\right)\n\\tag{22.5}\\]\n\nprint(\"Impact of Shanken Correction on t-statistics:\")\nprint(f\"{'Model':&lt;12} {'Factor':&lt;12} {'t (FM)':&gt;10} {'t (Shanken)':&gt;12} {'Ratio':&gt;8}\")\nprint(\"-\" * 54)\n\nfor model_name in models:\n    key = f\"{model_name}_full\"\n    if key not in fm_results:\n        continue\n    res = fm_results[key]['results']\n    for idx, row in res.iterrows():\n        if 'gamma_' in str(idx) and idx != 'gamma_0':\n            t_fm = row.get('t_fm', np.nan)\n            t_sh = row.get('t_shanken', np.nan)\n            ratio = t_fm / t_sh if pd.notna(t_sh) and t_sh != 0 else np.nan\n            factor_name = idx.replace('gamma_', '')\n            print(f\"{model_name:&lt;12} {factor_name:&lt;12} {t_fm:&gt;10.2f} \"\n                  f\"{t_sh:&gt;12.2f} {ratio:&gt;8.2f}\")\n\n\n\n22.4.3 Fama-MacBeth with Individual Stocks\nUsing individual stocks instead of portfolios as test assets avoids the Lewellen, Nagel, and Shanken (2010) critique that sorted portfolios create artificially strong factor structure:\n\ndef fama_macbeth_individual(stock_df, factor_df, factor_cols,\n                              beta_window=60, min_obs=36,\n                              min_stocks=100):\n    \"\"\"\n    Fama-MacBeth with individual stocks and rolling betas.\n    \"\"\"\n    stock_df = stock_df.copy()\n    stock_df['month_end'] = pd.to_datetime(stock_df['month_end'])\n    factor_df = factor_df.copy()\n    \n    months = sorted(stock_df['month_end'].unique())\n    \n    # Pre-compute rolling betas for all stocks\n    gamma_list = []\n    \n    for t_idx, month in enumerate(months):\n        if t_idx &lt; beta_window:\n            continue\n        \n        window_months = months[t_idx - beta_window:t_idx]\n        \n        # Get betas for each stock from rolling window\n        betas_t = {}\n        window_data = stock_df[stock_df['month_end'].isin(window_months)]\n        \n        for ticker, group in window_data.groupby('ticker'):\n            if len(group) &lt; min_obs:\n                continue\n            \n            y = group.set_index('month_end')['monthly_return']\n            x = factor_df.loc[y.index, factor_cols]\n            x = sm.add_constant(x)\n            \n            valid = y.index.intersection(x.index)\n            if len(valid) &lt; min_obs:\n                continue\n            \n            try:\n                model = sm.OLS(y[valid], x.loc[valid]).fit()\n                betas_t[ticker] = {f: model.params[f] for f in factor_cols}\n            except Exception:\n                pass\n        \n        if len(betas_t) &lt; min_stocks:\n            continue\n        \n        beta_df = pd.DataFrame(betas_t).T\n        \n        # Current month returns\n        current = stock_df[stock_df['month_end'] == month]\n        current = current.set_index('ticker')\n        \n        # Align\n        valid_tickers = beta_df.index.intersection(current.index)\n        if len(valid_tickers) &lt; min_stocks:\n            continue\n        \n        y = current.loc[valid_tickers, 'monthly_return'].values\n        X = sm.add_constant(beta_df.loc[valid_tickers].values)\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            gammas = {'month': month, 'gamma_0': model.params[0]}\n            for j, f in enumerate(factor_cols):\n                gammas[f'gamma_{f}'] = model.params[j + 1]\n            gammas['n_stocks'] = len(valid_tickers)\n            gammas['r_squared'] = model.rsquared\n            gamma_list.append(gammas)\n        except Exception:\n            pass\n    \n    gamma_df = pd.DataFrame(gamma_list)\n    \n    # Inference\n    gamma_cols = ['gamma_0'] + [f'gamma_{f}' for f in factor_cols]\n    summary = {}\n    for col in gamma_cols:\n        g = gamma_df[col]\n        mean = g.mean()\n        se = g.std() / np.sqrt(len(g))\n        t = mean / se if se &gt; 0 else np.nan\n        summary[col] = {'estimate': mean, 'se': se, 't': t}\n    \n    summary['avg_n_stocks'] = gamma_df['n_stocks'].mean()\n    summary['avg_cs_r2'] = gamma_df['r_squared'].mean()\n    \n    return pd.DataFrame(summary).T, gamma_df\n\n# Run for FF5\nprint(\"\\nFama-MacBeth with Individual Stocks (FF5):\")\nfm_ind_results, fm_ind_gamma = fama_macbeth_individual(\n    stock_returns, factors,\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma'],\n    beta_window=60, min_obs=36\n)\nprint(fm_ind_results.round(4).to_string())",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-disagreement",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-disagreement",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "22.5 When Do the Tests Disagree?",
    "text": "22.5 When Do the Tests Disagree?\n\n22.5.1 Theoretical Sources of Disagreement\nGoyal and Jegadeesh (2018) identify three sources of discrepancy between time-series and cross-sectional tests:\n1. Factor structure of test assets. If test assets have a strong factor structure (as sorted portfolios do by construction), the cross-sectional \\(R^2\\) will be high regardless of whether the factors truly price the assets. Lewellen, Nagel, and Shanken (2010) show that even random factors can achieve high cross-sectional \\(R^2\\) when test assets are size-BM sorted portfolios, because such portfolios have a strong linear structure in the size-value space.\n2. Time-variation in betas. The time-series regression assumes constant betas. If betas vary over time and co-move with the factor risk premium, the unconditional time-series alpha can be nonzero even if the conditional model holds (Jagannathan and Wang 1996).\n3. Misspecified risk premia. The Fama-MacBeth cross-sectional regression estimates the risk premium from the data. The time-series regression implicitly sets the risk premium equal to the factor’s sample mean return. When these differ (e.g., because the factor is not a perfect proxy for the underlying risk), the two approaches disagree.\n\n\n22.5.2 Empirical Comparison for Vietnam\n\n# For FF5 on 25 Size-BM portfolios:\n# Time-series: alpha from OLS regression\n# Cross-section: pricing error from Fama-MacBeth\n\nts_alphas = ts_results['FF5']['alpha'].values * 12  # Annualized\nts_names = ts_results['FF5'].index.tolist()\n\n# Cross-sectional pricing errors:\n# Predicted return = beta' * estimated_gamma\n# Pricing error = actual mean return - predicted\nfm_full = fm_results.get('FF5_full')\nif fm_full:\n    betas = fm_full['beta_df']  # N x K\n    factor_cols = ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']\n    gammas = np.array([fm_full['results'].loc[f'gamma_{f}', 'estimate']\n                        for f in factor_cols])\n    gamma_0 = fm_full['results'].loc['gamma_0', 'estimate']\n    \n    actual_means = excess_25_bm.mean() * 12  # Annualized\n    predicted = (gamma_0 + betas[factor_cols].values @ gammas) * 12\n    cs_errors = actual_means.values - predicted\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: TS alpha vs CS pricing error\nif fm_full:\n    axes[0].scatter(ts_alphas * 100, cs_errors * 100,\n                     color='#2C5F8A', s=60, alpha=0.7, edgecolors='white')\n    lim = max(np.max(np.abs(ts_alphas)), np.max(np.abs(cs_errors))) * 100 + 1\n    axes[0].plot([-lim, lim], [-lim, lim], 'k--', linewidth=1)\n    axes[0].plot([-lim, lim], [0, 0], color='gray', linewidth=0.5)\n    axes[0].plot([0, 0], [-lim, lim], color='gray', linewidth=0.5)\n    axes[0].set_xlabel('Time-Series Alpha (% ann.)')\n    axes[0].set_ylabel('Cross-Sectional Pricing Error (% ann.)')\n    axes[0].set_title('Panel A: TS Alpha vs CS Pricing Error')\n\n    # Correlation\n    rho = np.corrcoef(ts_alphas, cs_errors)[0, 1]\n    axes[0].text(0.05, 0.95, f'ρ = {rho:.2f}',\n                  transform=axes[0].transAxes, fontsize=11)\n\n# Panel B: Actual vs Predicted (CS)\nif fm_full:\n    axes[1].scatter(predicted * 100, actual_means.values * 100,\n                     color='#C0392B', s=60, alpha=0.7, edgecolors='white')\n    lim2 = max(np.max(np.abs(predicted)), np.max(np.abs(actual_means))) * 100 + 2\n    axes[1].plot([-5, lim2], [-5, lim2], 'k--', linewidth=1)\n    axes[1].set_xlabel('Predicted Average Return (% ann.)')\n    axes[1].set_ylabel('Actual Average Return (% ann.)')\n    axes[1].set_title('Panel B: Actual vs Predicted (FM Cross-Section)')\n    \n    # CS R-squared\n    ss_res = np.sum((actual_means.values - predicted) ** 2)\n    ss_tot = np.sum((actual_means.values - actual_means.values.mean()) ** 2)\n    r2_cs = 1 - ss_res / ss_tot\n    axes[1].text(0.05, 0.95, f'CS R² = {r2_cs:.2f}',\n                  transform=axes[1].transAxes, fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 22.3\n\n\n\n\n\n22.5.3 The Lewellen-Nagel-Shanken Critique\nLewellen, Nagel, and Shanken (2010) demonstrate that the high cross-sectional \\(R^2\\) from Fama-MacBeth regressions on sorted portfolios can be misleading. Sorted portfolios have a strong factor structure by construction, so even irrelevant factors can produce high \\(R^2\\). They recommend:\n\nIncluding industry portfolios alongside sorted portfolios to break the mechanical factor structure.\nConstraining the estimated risk premia to equal the factor mean returns (this is what the time-series test implicitly does).\nReporting the cross-sectional \\(R^2\\) from the constrained model alongside the unconstrained estimate.\n\n\ndef lns_diagnostic(excess_returns, factor_df, factor_cols):\n    \"\"\"\n    Lewellen-Nagel-Shanken (2010) diagnostic:\n    Compare unconstrained CS R² to constrained (factor mean = premium).\n    \"\"\"\n    common = excess_returns.index.intersection(factor_df.index)\n    Y = excess_returns.loc[common]\n    F = factor_df.loc[common, factor_cols]\n    \n    # Full-sample betas\n    betas = {}\n    F_const = sm.add_constant(F)\n    for col in Y.columns:\n        model = sm.OLS(Y[col], F_const).fit()\n        betas[col] = {f: model.params[f] for f in factor_cols}\n    beta_mat = pd.DataFrame(betas).T  # N x K\n    \n    actual = Y.mean() * 12  # Annualized\n    \n    # Unconstrained: FM regression\n    X = sm.add_constant(beta_mat.values)\n    fm_model = sm.OLS(actual.values, X).fit()\n    predicted_unc = fm_model.fittedvalues\n    r2_unconstrained = fm_model.rsquared\n    \n    # Constrained: premium = factor mean return\n    factor_means = F.mean().values * 12  # Annualized\n    predicted_con = beta_mat.values @ factor_means\n    # Add best-fitting intercept\n    intercept_con = np.mean(actual.values - predicted_con)\n    predicted_con += intercept_con\n    \n    ss_res_con = np.sum((actual.values - predicted_con) ** 2)\n    ss_tot = np.sum((actual.values - actual.values.mean()) ** 2)\n    r2_constrained = 1 - ss_res_con / ss_tot\n    \n    return {\n        'r2_unconstrained': r2_unconstrained,\n        'r2_constrained': r2_constrained,\n        'r2_drop': r2_unconstrained - r2_constrained,\n        'gamma_unconstrained': fm_model.params[1:],\n        'gamma_constrained': factor_means\n    }\n\nprint(\"Lewellen-Nagel-Shanken Diagnostic:\")\nprint(f\"{'Model':&lt;12} {'Test Assets':&lt;18} {'R² (unc)':&gt;10} {'R² (con)':&gt;10} \"\n      f\"{'Drop':&gt;8}\")\nprint(\"-\" * 58)\n\nfor model_name, factor_cols in models.items():\n    for ta_name, ta_data in test_asset_sets.items():\n        diag = lns_diagnostic(ta_data, factors, factor_cols)\n        print(f\"{model_name:&lt;12} {ta_name:&lt;18} \"\n              f\"{diag['r2_unconstrained']:&gt;10.3f} \"\n              f\"{diag['r2_constrained']:&gt;10.3f} \"\n              f\"{diag['r2_drop']:&gt;8.3f}\")\n\n\n\n\ndiag_ff5 = lns_diagnostic(excess_25_bm, factors,\n                            ['mkt_excess', 'smb', 'hml', 'rmw', 'cma'])\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nactual = excess_25_bm.mean().values * 12 * 100\n\n# Panel A: Unconstrained\nF_const = sm.add_constant(factors[['mkt_excess', 'smb', 'hml', 'rmw', 'cma']])\nbetas_full = {}\nfor col in excess_25_bm.columns:\n    common = excess_25_bm.index.intersection(factors.index)\n    model = sm.OLS(excess_25_bm.loc[common, col],\n                    F_const.loc[common]).fit()\n    betas_full[col] = {f: model.params[f] for f in\n                        ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']}\nbeta_mat = pd.DataFrame(betas_full).T\nX_cs = sm.add_constant(beta_mat.values)\nfm_mod = sm.OLS(actual, X_cs).fit()\npred_unc = fm_mod.fittedvalues\n\naxes[0].scatter(pred_unc, actual, color='#2C5F8A', s=60, alpha=0.7,\n                 edgecolors='white')\nrng_plot = [min(pred_unc.min(), actual.min()) - 1,\n            max(pred_unc.max(), actual.max()) + 1]\naxes[0].plot(rng_plot, rng_plot, 'k--', linewidth=1)\naxes[0].set_xlabel('Predicted (% ann.)')\naxes[0].set_ylabel('Actual (% ann.)')\naxes[0].set_title(f\"Panel A: Unconstrained (R² = \"\n                    f\"{diag_ff5['r2_unconstrained']:.2f})\")\n\n# Panel B: Constrained\nfactor_means = factors[['mkt_excess', 'smb', 'hml', 'rmw', 'cma']].mean().values * 12 * 100\npred_con = beta_mat.values @ factor_means\npred_con += np.mean(actual - pred_con)\n\naxes[1].scatter(pred_con, actual, color='#C0392B', s=60, alpha=0.7,\n                 edgecolors='white')\nrng2 = [min(pred_con.min(), actual.min()) - 1,\n        max(pred_con.max(), actual.max()) + 1]\naxes[1].plot(rng2, rng2, 'k--', linewidth=1)\naxes[1].set_xlabel('Predicted (% ann.)')\naxes[1].set_ylabel('Actual (% ann.)')\naxes[1].set_title(f\"Panel B: Constrained (R² = \"\n                    f\"{diag_ff5['r2_constrained']:.2f})\")\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 22.4",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-gmm",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-gmm",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "22.6 GMM-Based Tests",
    "text": "22.6 GMM-Based Tests\n\n22.6.1 The Stochastic Discount Factor Framework\nBoth time-series and cross-sectional tests are special cases of the Hansen (1982) Generalized Method of Moments (GMM) framework. The fundamental pricing equation is:\n\\[\nE[M_t R_{i,t}^e] = 0 \\quad \\text{for all } i\n\\tag{22.6}\\]\nwhere \\(M_t\\) is the stochastic discount factor (SDF) and \\(R_{i,t}^e\\) is the excess return of asset \\(i\\). A linear factor model parameterizes the SDF as:\n\\[\nM_t = 1 - b' (f_t - \\mu_f)\n\\tag{22.7}\\]\nwhere \\(b\\) is the vector of SDF loadings and \\(f_t\\) are the factors. The GMM approach estimates \\(b\\) by minimizing the quadratic form of the pricing errors:\n\\[\n\\hat{b} = \\arg\\min_b \\ g(b)' W \\ g(b)\n\\tag{22.8}\\]\nwhere \\(g(b) = \\frac{1}{T} \\sum_t M_t(b) R_t^e\\) is the sample analog of the moment conditions.\n\ndef gmm_sdf(excess_returns, factor_df, factor_cols,\n              weighting='identity', n_iter=2):\n    \"\"\"\n    GMM estimation of the linear SDF model.\n    \n    Parameters\n    ----------\n    weighting : str\n        'identity': first-stage identity weighting matrix\n        'optimal': Hansen optimal weighting (iterated)\n    n_iter : int\n        Number of GMM iterations (2 = efficient two-step)\n    \"\"\"\n    common = excess_returns.index.intersection(factor_df.index)\n    Re = excess_returns.loc[common].values  # T x N\n    F = factor_df.loc[common, factor_cols].values  # T x K\n    \n    T, N = Re.shape\n    K = F.shape[1]\n    \n    # Demean factors\n    F_dm = F - F.mean(axis=0)\n    \n    # Moment conditions: E[M * Re] = 0\n    # M = 1 - b' (f - mu_f) = 1 - b' F_dm\n    def pricing_errors(b):\n        M = 1 - F_dm @ b  # T-vector\n        g = (M[:, np.newaxis] * Re).mean(axis=0)  # N-vector\n        return g\n    \n    # GMM objective\n    W = np.eye(N)  # Initial weighting matrix\n    \n    from scipy.optimize import minimize\n    \n    for iteration in range(n_iter):\n        def objective(b):\n            g = pricing_errors(b)\n            return T * g @ W @ g\n        \n        result = minimize(objective, np.zeros(K), method='L-BFGS-B')\n        b_hat = result.x\n        \n        if iteration &lt; n_iter - 1:\n            # Update weighting matrix (Hansen optimal)\n            g_t = np.array([(1 - F_dm[t] @ b_hat) * Re[t]\n                             for t in range(T)])  # T x N\n            S = np.cov(g_t, rowvar=False, ddof=0)\n            \n            # Newey-West adjustment for serial correlation\n            max_lag = int(np.floor(4 * (T / 100) ** (2 / 9)))\n            for lag in range(1, max_lag + 1):\n                w = 1 - lag / (max_lag + 1)\n                Gamma = np.cov(g_t[lag:].T, g_t[:-lag].T, ddof=0)[:N, N:]\n                S += w * (Gamma + Gamma.T)\n            \n            try:\n                W = inv(S)\n            except Exception:\n                W = np.eye(N)\n    \n    # Pricing errors at optimum\n    g_hat = pricing_errors(b_hat)\n    \n    # Implied risk premia: lambda = Sigma_f @ b\n    Sigma_f = np.cov(F, rowvar=False, ddof=1)\n    lambda_hat = Sigma_f @ b_hat\n    \n    # J-test (overidentification)\n    J = T * g_hat @ W @ g_hat\n    df_J = N - K\n    p_J = 1 - stats.chi2.cdf(J, df_J) if df_J &gt; 0 else np.nan\n    \n    # HJ distance (model misspecification)\n    hj_dist = np.sqrt(g_hat @ inv(np.cov(Re, rowvar=False)) @ g_hat)\n    \n    return {\n        'b_hat': b_hat,\n        'lambda_hat': lambda_hat,\n        'pricing_errors': g_hat,\n        'J_stat': J,\n        'J_pvalue': p_J,\n        'J_df': df_J,\n        'hj_distance': hj_dist,\n        'mean_abs_error': np.abs(g_hat).mean() * 12\n    }\n\nprint(\"GMM SDF Estimation:\")\nprint(f\"{'Model':&lt;12} {'Test Assets':&lt;18} {'J-stat':&gt;8} {'J p-val':&gt;10} \"\n      f\"{'HJ dist':&gt;8} {'|e| ann':&gt;10}\")\nprint(\"-\" * 66)\n\nfor model_name, factor_cols in models.items():\n    for ta_name, ta_data in test_asset_sets.items():\n        gmm = gmm_sdf(ta_data, factors, factor_cols, n_iter=2)\n        print(f\"{model_name:&lt;12} {ta_name:&lt;18} \"\n              f\"{gmm['J_stat']:&gt;8.2f} {gmm['J_pvalue']:&gt;10.4f} \"\n              f\"{gmm['hj_distance']:&gt;8.4f} {gmm['mean_abs_error']:&gt;10.4f}\")\n        \n        # Print implied risk premia\n        for j, f in enumerate(factor_cols):\n            print(f\"  λ_{f}: {gmm['lambda_hat'][j]*12:.4f} \"\n                  f\"(factor mean: {factors[f].mean()*12:.4f})\")",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-model-comparison",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-model-comparison",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "22.7 Model Comparison",
    "text": "22.7 Model Comparison\n\n22.7.1 The Barillas-Shanken Framework\nBarillas and Shanken (2018) propose a Bayesian framework for comparing non-nested factor models. The key insight is that model comparison should be based on the factors’ ability to explain each other’s returns, not on their ability to price a fixed set of test assets. Two models differ only in their excluded factors, so the relevant comparison is whether the factors unique to each model have alpha with respect to the other model.\n\ndef barillas_shanken_compare(factor_df, model_a_cols, model_b_cols):\n    \"\"\"\n    Barillas-Shanken (2018) pairwise model comparison.\n    \n    Compare Model A vs Model B by testing whether the factors\n    unique to each model have alpha with respect to the other.\n    \"\"\"\n    # Factors unique to each model\n    unique_a = [f for f in model_a_cols if f not in model_b_cols]\n    unique_b = [f for f in model_b_cols if f not in model_a_cols]\n    \n    results = {}\n    \n    # Test: do factors unique to A have alpha w.r.t. B?\n    # If yes, A adds value beyond B\n    if unique_a:\n        for f in unique_a:\n            y = factor_df[f]\n            X = sm.add_constant(factor_df[model_b_cols])\n            common = y.dropna().index.intersection(X.dropna().index)\n            model = sm.OLS(y[common], X.loc[common]).fit(\n                cov_type='HAC', cov_kwds={'maxlags': 6}\n            )\n            results[f'alpha_{f}_vs_B'] = {\n                'alpha': model.params['const'] * 12,\n                't': model.tvalues['const'],\n                'r2': model.rsquared\n            }\n    \n    if unique_b:\n        for f in unique_b:\n            y = factor_df[f]\n            X = sm.add_constant(factor_df[model_a_cols])\n            common = y.dropna().index.intersection(X.dropna().index)\n            model = sm.OLS(y[common], X.loc[common]).fit(\n                cov_type='HAC', cov_kwds={'maxlags': 6}\n            )\n            results[f'alpha_{f}_vs_A'] = {\n                'alpha': model.params['const'] * 12,\n                't': model.tvalues['const'],\n                'r2': model.rsquared\n            }\n    \n    return pd.DataFrame(results).T\n\n# Compare FF3 vs FF5\nprint(\"Barillas-Shanken: FF3 vs FF5\")\nbs_3v5 = barillas_shanken_compare(\n    factors,\n    ['mkt_excess', 'smb', 'hml'],\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']\n)\nprint(bs_3v5.round(4).to_string())\n\n# Compare FF5 vs FF5+WML\nprint(\"\\nBarillas-Shanken: FF5 vs FF5+WML\")\nbs_5vw = barillas_shanken_compare(\n    factors,\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma'],\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml']\n)\nprint(bs_5vw.round(4).to_string())\n\n# Compare FF3+WML vs FF5+WML\nprint(\"\\nBarillas-Shanken: FF3+WML vs FF5+WML\")\nbs_3wv5w = barillas_shanken_compare(\n    factors,\n    ['mkt_excess', 'smb', 'hml', 'wml'],\n    ['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml']\n)\nprint(bs_3wv5w.round(4).to_string())\n\n\n\n22.7.2 Comprehensive Model Scorecard\n\n\n\nscorecard = {}\n\nfor model_name, factor_cols in models.items():\n    metrics = {'Model': model_name}\n    \n    # Time-series (25 Size-BM)\n    grs_result = grs_all.get(f\"{model_name}_25 Size-BM\")\n    if grs_result:\n        metrics['GRS (BM)'] = grs_result['GRS']\n        metrics['GRS p'] = grs_result['p_value']\n        metrics['|α| (BM)'] = grs_result['mean_abs_alpha'] * 12\n    \n    # Cross-section\n    fm_key = f\"{model_name}_full\"\n    if fm_key in fm_results:\n        metrics['CS R² (unc)'] = fm_results[fm_key]['results'].get(\n            'avg_cs_r2', {}).get('estimate', np.nan)\n    \n    # LNS constrained\n    lns = lns_diagnostic(excess_25_bm, factors, factor_cols)\n    metrics['CS R² (con)'] = lns['r2_constrained']\n    \n    # GMM\n    gmm = gmm_sdf(excess_25_bm, factors, factor_cols, n_iter=2)\n    metrics['HJ dist'] = gmm['hj_distance']\n    metrics['J p-val'] = gmm['J_pvalue']\n    \n    # Time-series R²\n    ts_res = time_series_regressions(excess_25_bm, factors, factor_cols)\n    metrics['Avg R²'] = ts_res['r_squared'].mean()\n    \n    scorecard[model_name] = metrics\n\nscorecard_df = pd.DataFrame(scorecard).T\nprint(\"Model Scorecard:\")\nprint(scorecard_df.round(3).to_string())\n\n\nFigure 22.5",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-conditional",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-conditional",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "22.8 Conditional vs. Unconditional Tests",
    "text": "22.8 Conditional vs. Unconditional Tests\n\n22.8.1 Time-Varying Betas\nUnconditional tests assume constant betas. In practice, Vietnamese stock betas vary substantially over time due to changing market conditions, foreign ownership shifts, and sectoral rotations. We estimate rolling betas to assess the degree of time-variation:\n\n# Rolling 36-month betas for the 25 Size-BM portfolios on FF5\nrolling_window = 36\nfactor_cols = ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']\n\n# Select a few representative portfolios\nrepresentative = [excess_25_bm.columns[0],   # Small-Growth\n                   excess_25_bm.columns[4],   # Small-Value\n                   excess_25_bm.columns[20],  # Big-Growth\n                   excess_25_bm.columns[24]]  # Big-Value\n\nrolling_betas = {}\ncommon = excess_25_bm.index.intersection(factors.index)\n\nfor port in representative:\n    betas_t = []\n    for t in range(rolling_window, len(common)):\n        window = common[t - rolling_window:t]\n        y = excess_25_bm.loc[window, port]\n        X = sm.add_constant(factors.loc[window, factor_cols])\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            entry = {'month': common[t]}\n            for f in factor_cols:\n                entry[f'beta_{f}'] = model.params[f]\n            betas_t.append(entry)\n        except Exception:\n            pass\n    \n    rolling_betas[port] = pd.DataFrame(betas_t)\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\naxes = axes.flatten()\n\nlabels = ['Small-Growth', 'Small-Value', 'Big-Growth', 'Big-Value']\ncolors_port = ['#C0392B', '#2C5F8A', '#E67E22', '#27AE60']\n\nfor i, (port, label, color) in enumerate(zip(representative, labels, colors_port)):\n    rb = rolling_betas[port]\n    \n    axes[i].plot(pd.to_datetime(rb['month']), rb['beta_mkt_excess'],\n                 color=color, linewidth=1.5, label='Market β')\n    axes[i].axhline(y=1, color='gray', linewidth=0.5, linestyle='--')\n    \n    # Add HML beta\n    axes[i].plot(pd.to_datetime(rb['month']), rb['beta_hml'],\n                 color='gray', linewidth=1, linestyle=':', label='HML β')\n    \n    axes[i].set_ylabel('Beta')\n    axes[i].set_title(label)\n    axes[i].legend(fontsize=8)\n\nplt.suptitle('Rolling 36-Month Factor Betas', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Quantify time-variation\nprint(\"\\nBeta Time-Variation (Std Dev of Rolling Betas):\")\nfor port, label in zip(representative, labels):\n    rb = rolling_betas[port]\n    mkt_std = rb['beta_mkt_excess'].std()\n    hml_std = rb['beta_hml'].std()\n    print(f\"  {label:&lt;15}: σ(β_MKT) = {mkt_std:.3f}, σ(β_HML) = {hml_std:.3f}\")\n\n\nFigure 22.6",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-recommendations",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-recommendations",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "22.9 Practical Recommendations",
    "text": "22.9 Practical Recommendations\nThe analysis in this chapter yields the following guidelines for Vietnamese asset pricing research:\nUse both approaches. Time-series tests (GRS) and cross-sectional tests (Fama-MacBeth) answer different questions. Always report both. If they disagree, investigate why rather than cherry-picking the more favorable result.\nBe cautious with cross-sectional \\(R^2\\). High \\(R^2\\) from Fama-MacBeth on sorted portfolios is nearly guaranteed by the test asset structure. Always report the Lewellen, Nagel, and Shanken (2010) constrained \\(R^2\\) alongside the unconstrained value. Include industry portfolios as additional test assets.\nApply the Shanken correction. The EIV bias from estimated betas inflates Fama-MacBeth t-statistics. Always report Shanken-corrected standard errors for full-sample betas. For rolling betas, the correction is not straightforward, but the bias is smaller because beta estimation error averages out over time.\nUse individual stocks with caution. Fama-MacBeth with individual stocks avoids the Lewellen-Nagel-Shanken critique but introduces severe noise: individual Vietnamese stocks are thin, volatile, and the monthly cross-section is small (500-700). The resulting risk premia will have wide confidence intervals. Report the average number of stocks per cross-section and the cross-sectional \\(R^2\\).\nExamine conditional models. Rolling betas reveal substantial time-variation in Vietnamese stock exposures. If the research question is about risk compensation (does beta predict returns?), use rolling betas and control for time-variation. If the question is about model adequacy (do the factors span the mean-variance frontier?), full-sample betas and the GRS test are appropriate.\nFor model selection, use Barillas-Shanken. When comparing competing factor models (e.g., FF3 vs. FF5), the Barillas and Shanken (2018) approach (i.e., testing whether each model’s unique factors have alpha with respect to the other) is more informative than comparing GRS statistics on the same test assets.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "26_time_series_vs_cross_section.html#sec-ts-cs-summary",
    "href": "26_time_series_vs_cross_section.html#sec-ts-cs-summary",
    "title": "22  Time-Series vs. Cross-Sectional Factor Tests",
    "section": "22.10 Summary",
    "text": "22.10 Summary\n\n\n\nTable 22.2: Summary of testing approaches for factor models.\n\n\n\n\n\n\n\n\n\n\n\n\nTest\nQuestion\nStatistic\nStrength\nWeakness\n\n\n\n\nTS regression (GRS)\nDo all alphas = 0?\nF-statistic\nClean null; joint test\nDepends on test assets\n\n\nFama-MacBeth (portfolios)\nAre betas priced?\n\\(\\bar{\\gamma}\\), t-stat\nEstimates risk premia\nEIV bias; LNS critique\n\n\nFM (individual stocks)\nAre betas priced?\n\\(\\bar{\\gamma}\\), t-stat\nAvoids sorted portfolios\nNoisy; small cross-section\n\n\nLNS diagnostic\nIs CS R² spurious?\nConstrained R²\nReveals false positives\nOnly applicable to portfolios\n\n\nGMM / SDF\nPricing errors jointly\nJ-stat, HJ distance\nFlexible; model-free\nRequires large N relative to K\n\n\nBarillas-Shanken\nWhich model is better?\nAlpha of unique factors\nModel comparison\nRequires traded factors\n\n\n\n\n\n\nThe gap between time-series and cross-sectional evidence is not a technicality—it reflects a fundamental ambiguity in what “a factor model works” means. In a market like Vietnam, where the cross-section is small, betas are time-varying, and test assets are inherently noisy, the two approaches will often disagree. The honest researcher reports both, investigates the source of disagreement, and treats any single test result with appropriate humility.\n\n\n\n\n\n\n\nBarillas, Francisco, and Jay Shanken. 2018. “Comparing Asset Pricing Models.” The Journal of Finance 73 (2): 715–54.\n\n\nFama, Eugene F, and Kenneth R French. 2020. “Comparing Cross-Section and Time-Series Factor Models.” The Review of Financial Studies 33 (5): 1891–1926.\n\n\nFama, Eugene F, and James D MacBeth. 1973. “Risk, Return, and Equilibrium: Empirical Tests.” Journal of Political Economy 81 (3): 607–36.\n\n\nGibbons, Michael R, Stephen A Ross, and Jay Shanken. 1989. “A Test of the Efficiency of a Given Portfolio.” Econometrica: Journal of the Econometric Society, 1121–52.\n\n\nGoyal, Amit, and Narasimhan Jegadeesh. 2018. “Cross-Sectional and Time-Series Tests of Return Predictability: What Is the Difference?” The Review of Financial Studies 31 (5): 1784–824.\n\n\nHansen, Lars Peter. 1982. “Large Sample Properties of Generalized Method of Moments Estimators.” Econometrica: Journal of the Econometric Society, 1029–54.\n\n\nJagannathan, Ravi, and Zhenyu Wang. 1996. “The Conditional CAPM and the Cross-Section of Expected Returns.” The Journal of Finance 51 (1): 3–53.\n\n\nLewellen, Jonathan, Stefan Nagel, and Jay Shanken. 2010. “A Skeptical Appraisal of Asset Pricing Tests.” Journal of Financial Economics 96 (2): 175–94.\n\n\nShanken, Jay. 1992. “On the Estimation of Beta-Pricing Models.” The Review of Financial Studies 5 (1): 1–33.",
    "crumbs": [
      "Home",
      "Mô hình định giá tài sản",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Time-Series vs. Cross-Sectional Factor Tests</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html",
    "href": "30_financial_statement_analysis.html",
    "title": "23  Financial Statement Analysis",
    "section": "",
    "text": "23.1 From Market Prices to Fundamental Value\nThe previous chapters focused on how financial markets price assets in equilibrium. The Capital Asset Pricing Model showed that expected returns depend on systematic risk exposure, while Modern Portfolio Theory demonstrated how to construct efficient portfolios. But these frameworks take expected returns and risk as given, they don’t explain where these expectations come from.\nFinancial statement analysis addresses this gap. By examining a company’s accounting records, investors can form independent assessments of firm value, identify mispriced securities, and understand the economic forces driving business performance. Financial statements provide the primary source of standardized information about a company’s operations, financial position, and cash generation. Their legal requirements and standardized formats make them particularly valuable. Every publicly traded company must file them, creating a level playing field for analysis.\nThis chapter introduces the three primary financial statements: the balance sheet, income statement, and cash flow statement. We then demonstrate how to transform raw accounting data into meaningful financial ratios that facilitate comparison across companies and over time. These ratios serve multiple purposes: they enable investors to benchmark companies against peers, help creditors assess default risk, and provide inputs for asset pricing models like the Fama-French factors we will encounter in later chapters.\nOur analysis combines theoretical frameworks with practical implementation using Vietnamese market data. By the end of this chapter, you will understand how to access financial statements, calculate key ratios across multiple categories, and interpret these metrics in context.\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format\nfrom adjustText import adjust_text",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#the-three-financial-statements",
    "href": "30_financial_statement_analysis.html#the-three-financial-statements",
    "title": "23  Financial Statement Analysis",
    "section": "23.2 The Three Financial Statements",
    "text": "23.2 The Three Financial Statements\nBefore diving into ratios and analysis, we need to understand the three interconnected statements that form the foundation of financial reporting. Each statement answers a different question about the company, and together they provide a comprehensive picture of financial health.\n\n23.2.1 The Balance Sheet: A Snapshot of Financial Position\nThe balance sheet captures a company’s financial position at a specific moment in time, think of it as a photograph rather than a movie. It lists everything the company owns (assets), everything it owes (liabilities), and the residual claim belonging to shareholders (equity). These three components are linked by the fundamental accounting equation:\n\\[\n\\text{Assets} = \\text{Liabilities} + \\text{Equity}\n\\]\nThis equation is not merely a definition, it reflects a core economic principle. A company’s resources (assets) must be financed from somewhere: either borrowed from creditors (liabilities) or contributed by owners (equity). Every transaction affects both sides equally, maintaining the balance.\nAssets represent resources the company controls that are expected to generate future economic benefits:\n\nCurrent assets can be converted to cash within one year: cash and equivalents, short-term investments, accounts receivable (money owed by customers), and inventory (raw materials, work-in-progress, and finished goods)\nNon-current assets support operations beyond one year: property, plant, and equipment (PP&E), long-term investments, and intangible assets like patents, trademarks, and goodwill\n\nLiabilities encompass obligations to external parties:\n\nCurrent liabilities come due within one year: accounts payable (money owed to suppliers), short-term debt, accrued expenses, and the current portion of long-term debt\nNon-current liabilities extend beyond one year: long-term debt, bonds payable, pension obligations, and deferred tax liabilities\n\nShareholders’ equity represents the owners’ residual claim:\n\nCommon stock and additional paid-in capital from share issuance\nRetained earnings (i.e., accumulated profits reinvested rather than distributed as dividends)\nTreasury stock: shares repurchased by the company\n\nUnderstanding these categories is essential for ratio analysis. Current assets and liabilities determine short-term liquidity, while the mix of debt and equity reveals capital structure choices.\n\n\n23.2.2 The Income Statement: Performance Over Time\nWhile the balance sheet provides a snapshot, the income statement (also called the profit and loss statement, or P&L) measures financial performance over a period (e.g., a quarter or year). It follows a hierarchical structure that progressively captures different levels of profitability:\n\\[\n\\text{Revenue} - \\text{COGS} = \\text{Gross Profit}\n\\]\n\\[\n\\text{Gross Profit} - \\text{Operating Expenses} = \\text{Operating Income (EBIT)}\n\\]\n\\[\n\\text{EBIT} - \\text{Interest} - \\text{Taxes} = \\text{Net Income}\n\\]\nEach line reveals something different about the business:\n\nRevenue (Sales): Total income from goods or services sold (i.e., the “top line”)\nCost of Goods Sold (COGS): Direct costs of producing what was sold (materials, direct labor, manufacturing overhead)\nGross Profit: Revenue minus COGS, measuring basic profitability from core operations\nOperating Expenses: Costs of running the business beyond production (selling, general & administrative expenses, research & development)\nOperating Income (EBIT): Earnings Before Interest and Taxes, measuring profitability from operations before financing decisions and taxes\nInterest Expense: The cost of debt financing\nNet Income: The “bottom line” (i.e., total profit after all expenses)\n\nThe income statement’s hierarchical structure allows analysts to identify where profitability problems originate. A company with strong gross margins but weak net income might have bloated overhead costs. One with weak gross margins faces fundamental pricing or production challenges.\n\n\n23.2.3 The Cash Flow Statement: Following the Money\nThe cash flow statement bridges a critical gap: profitable companies can run out of cash, and unprofitable companies can generate positive cash flow. This happens because accrual accounting (used in the income statement) recognizes revenue when earned and expenses when incurred, not when cash changes hands.\nThe cash flow statement tracks actual cash movements, divided into three categories:\n\nOperating activities: Cash generated from core business operations. Starts with net income, then adjusts for non-cash items (depreciation, changes in working capital)\nInvesting activities: Cash spent on or received from long-term investments (e.g., purchasing equipment, acquiring businesses, selling assets)\nFinancing activities: Cash flows from capital structure decisions (e.g., issuing stock, borrowing, repaying debt, paying dividends, buying back shares)\n\nA company can show strong net income while burning cash if it’s building inventory, extending generous credit terms, or making large capital expenditures. Conversely, a company reporting losses might generate positive operating cash flow by collecting receivables faster than it pays suppliers.\n\n\n23.2.4 Illustrating with FPT’s Financial Statements\nTo see these concepts in practice, let’s examine FPT Corporation’s 2023 financial statements. FPT is one of Vietnam’s largest technology companies, providing IT services, telecommunications, and education.\n\n# Placeholder for FPT balance sheet visualization\n# In practice, this would display the actual PDF or cleaned data\n# from DataCore's acquisition pipeline\n\n# Example structure of what the balance sheet data looks like:\n# Assets: Current assets (cash, receivables, inventory) + Non-current assets (PP&E, intangibles)\n# Liabilities: Current liabilities (payables, short-term debt) + Non-current liabilities (long-term debt)\n# Equity: Common stock + Retained earnings\n\nThe balance sheet demonstrates the fundamental accounting equation in action. FPT’s assets (e.g., spanning cash, receivables, technology infrastructure, and intangible assets like software) exactly equal the sum of its liabilities and equity.\n\n# Placeholder for FPT income statement visualization\n# Shows the progression from revenue through various profit measures to net income\n\nFPT’s income statement reveals how the company transforms revenue into profit. The progression from gross profit through operating income to net income shows the impact of operating expenses, interest costs, and taxes.\n\n# Placeholder for FPT cash flow statement visualization\n# Reconciles net income with actual cash generation\n\nThe cash flow statement shows how FPT’s reported profits translate into actual cash. Differences between net income and operating cash flow reveal the impact of working capital management and non-cash expenses.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#loading-financial-statement-data",
    "href": "30_financial_statement_analysis.html#loading-financial-statement-data",
    "title": "23  Financial Statement Analysis",
    "section": "23.3 Loading Financial Statement Data",
    "text": "23.3 Loading Financial Statement Data\nWe now turn to systematic analysis across multiple companies. We load financial statement data for the VN30 index constituents (i.e., the 30 largest and most liquid stocks on Vietnam’s Ho Chi Minh Stock Exchange).\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncomp_vn = pd.read_sql_query(\n    sql=\"SELECT * FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\ncomp_vn.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nxint\nni\noibdp\noancf\ncapx\nbe\nop\nat_lag\ninv\ntotal_debt\n\n\n\n\n0\nAGF\n1998\n8.845141e+10\nNone\n5.469709e+09\n0.000000e+00\nNone\nNone\n0.0\n1.110705e+10\n...\n0.0\n9.809218e+09\n1.523680e+10\nNaN\nNaN\n2.656020e+10\n0.711195\nNaN\nNaN\n0.000000e+00\n\n\n1\nBBC\n1999\n5.672574e+10\nNone\n5.354939e+09\n5.354939e+09\nNone\nNone\n0.0\n0.000000e+00\n...\n0.0\n8.763745e+09\n8.088393e+09\n2.687635e+10\n1.097031e+10\n3.211410e+10\n0.728193\nNaN\nNaN\n1.505529e+09\n\n\n2\nAGF\n1999\n9.558392e+10\nNone\n2.609276e+09\n0.000000e+00\nNone\nNone\n0.0\n1.008298e+10\n...\n0.0\n1.426227e+10\n1.766353e+10\n1.675607e+10\n3.970966e+09\n3.576596e+10\n0.816972\n1.068410e+11\n0.090477\n0.000000e+00\n\n\n\n\n3 rows × 334 columns\n\n\n\n\nvn30_symbols = [\n    \"ACB\", \"BCM\", \"BID\", \"BVH\", \"CTG\", \"FPT\", \"GAS\", \"GVR\", \"HDB\", \"HPG\",\n    \"MBB\", \"MSN\", \"MWG\", \"PLX\", \"POW\", \"SAB\", \"SHB\", \"SSB\", \"STB\", \"TCB\",\n    \"TPB\", \"VCB\", \"VHM\", \"VIB\", \"VIC\", \"VJC\", \"VNM\", \"VPB\", \"VRE\", \"EIB\"\n]\n\ncomp_vn30 = comp_vn[comp_vn[\"symbol\"].isin(vn30_symbols)]\ncomp_vn30.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nxint\nni\noibdp\noancf\ncapx\nbe\nop\nat_lag\ninv\ntotal_debt\n\n\n\n\n11\nFPT\n2002\n5.098910e+11\nNone\n1.027470e+11\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n9.008000e+09\n1.797900e+10\n1.794200e+10\n3.203600e+10\n2.202800e+10\n3.125400e+10\n1.425576\nNaN\nNaN\n0.0\n\n\n17\nVNM\n2003\n2.101406e+12\nNone\n6.925924e+11\n6.925924e+11\nNone\nNone\n0.0\n0.0\n...\n1.698909e+10\n5.124662e+11\n7.136218e+11\nNaN\nNaN\n1.560789e+12\n0.663257\nNaN\nNaN\n0.0\n\n\n21\nFPT\n2003\n9.171390e+11\nNone\n7.995600e+10\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n1.286700e+10\n4.389400e+10\n4.381900e+10\n-2.905420e+11\n3.753300e+10\n1.697000e+11\n0.940218\n5.504080e+11\n0.779104\n0.0\n\n\n\n\n3 rows × 334 columns\n\n\n\nThis dataset provides the foundation for calculating financial ratios and conducting cross-sectional comparisons. Each row contains balance sheet, income statement, and cash flow items for a company-year observation.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#liquidity-ratios-can-the-company-pay-its-bills",
    "href": "30_financial_statement_analysis.html#liquidity-ratios-can-the-company-pay-its-bills",
    "title": "23  Financial Statement Analysis",
    "section": "23.4 Liquidity Ratios: Can the Company Pay Its Bills?",
    "text": "23.4 Liquidity Ratios: Can the Company Pay Its Bills?\nLiquidity ratios assess a company’s ability to meet short-term obligations. These metrics matter most to creditors, suppliers, and employees who need assurance that the company can pay its bills. They’re calculated using balance sheet items, comparing liquid assets against near-term liabilities.\n\n23.4.1 The Current Ratio\nThe most basic liquidity measure compares all current assets to current liabilities:\n\\[\n\\text{Current Ratio} = \\frac{\\text{Current Assets}}{\\text{Current Liabilities}}\n\\]\nA ratio above one indicates the company has enough current assets to cover obligations due within one year. However, the interpretation depends heavily on the composition of current assets. A company with current assets tied up in slow-moving inventory is less liquid than one holding cash.\n\n\n23.4.2 The Quick Ratio\nThe quick ratio (or “acid test”) provides a more stringent measure by excluding inventory:\n\\[\n\\text{Quick Ratio} = \\frac{\\text{Current Assets} - \\text{Inventory}}{\\text{Current Liabilities}}\n\\]\nWhy exclude inventory? Inventory is typically the least liquid current asset. It must be sold (potentially at a discount) before generating cash. A company facing a liquidity crisis cannot easily convert raw materials or finished goods into immediate cash. The quick ratio answers: “Can we pay our bills without relying on inventory sales?”\n\n\n23.4.3 The Cash Ratio\nThe most conservative liquidity measure focuses solely on the most liquid assets:\n\\[\n\\text{Cash Ratio} = \\frac{\\text{Cash and Cash Equivalents}}{\\text{Current Liabilities}}\n\\]\nWhile a ratio of one indicates robust liquidity, most companies maintain lower cash ratios to avoid holding excessive non-productive assets. Cash sitting in bank accounts could otherwise be invested in growth opportunities, returned to shareholders, or used to pay down costly debt.\n\n\n23.4.4 Calculating Liquidity Ratios\nLet’s compute these ratios for our VN30 sample:\n\nbalance_sheet_statements = (comp_vn30\n    .assign(\n        fiscal_year=lambda x: x[\"year\"].astype(int),\n        \n        # Current Ratio: Current Assets / Current Liabilities\n        current_ratio=lambda x: x[\"act\"] / x[\"lct\"],\n        \n        # Quick Ratio: (Current Assets - Inventory) / Current Liabilities\n        quick_ratio=lambda x: (x[\"act\"] - x[\"inv\"]) / x[\"lct\"],\n        \n        # Cash Ratio: Cash and Equivalents / Current Liabilities\n        cash_ratio=lambda x: x[\"ca_cce\"] / x[\"lct\"],\n        \n        label=lambda x: np.where(\n            x[\"symbol\"].isin(vn30_symbols), x[\"symbol\"], np.nan\n        )\n    )\n)\n\nbalance_sheet_statements.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\ntotal_current_asset\nca_fin\nca_cce\nca_cash\nca_cash_inbank\nca_cash_attransit\nca_cash_equivalent\nca_fin_invest\n...\nbe\nop\nat_lag\ninv\ntotal_debt\nfiscal_year\ncurrent_ratio\nquick_ratio\ncash_ratio\nlabel\n\n\n\n\n11\nFPT\n2002\n5.098910e+11\nNone\n1.027470e+11\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n3.125400e+10\n1.425576\nNaN\nNaN\n0.0\n2002\n1.211413\nNaN\n0.244109\nFPT\n\n\n17\nVNM\n2003\n2.101406e+12\nNone\n6.925924e+11\n6.925924e+11\nNone\nNone\n0.0\n0.0\n...\n1.560789e+12\n0.663257\nNaN\nNaN\n0.0\n2003\n2.195772\nNaN\n0.723694\nVNM\n\n\n21\nFPT\n2003\n9.171390e+11\nNone\n7.995600e+10\n0.000000e+00\nNone\nNone\n0.0\n0.0\n...\n1.697000e+11\n0.940218\n5.504080e+11\n0.779104\n0.0\n2003\n1.274633\n1.274633\n0.111122\nFPT\n\n\n\n\n3 rows × 339 columns\n\n\n\n\n\n23.4.5 Cross-Sectional Comparison of Liquidity\nFigure 23.1 compares liquidity ratios across companies for the most recent fiscal year. This cross-sectional view reveals how different business models and industries maintain different liquidity profiles.\n\nliquidity_ratios = (balance_sheet_statements\n    .query(\"year == 2023 & label.notna()\")\n    .get([\"symbol\", \"current_ratio\", \"quick_ratio\", \"cash_ratio\"])\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        name=lambda x: x[\"name\"].str.replace(\"_\", \" \").str.title()\n    )\n)\n\nliquidity_ratios_figure = (\n    ggplot(liquidity_ratios, aes(y=\"value\", x=\"name\", fill=\"symbol\"))\n    + geom_col(position=\"dodge\")\n    + coord_flip()\n    + labs(\n        x=\"\", y=\"Ratio Value\", fill=\"\",\n        title=\"Liquidity Ratios for VN30 Stocks (2023)\"\n    )\n)\n\nliquidity_ratios_figure.show()\n\n\n\n\n\n\n\nFigure 23.1: Liquidity ratios measure a company’s ability to meet short-term obligations. Higher values indicate greater liquidity, though excessively high ratios may suggest inefficient use of assets.\n\n\n\n\n\nSeveral patterns emerge from this comparison. Banks and financial institutions typically show different liquidity profiles than industrial companies due to their unique business models. Companies with high inventory (retailers, manufacturers) often show larger gaps between current and quick ratios.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#leverage-ratios-how-is-the-company-financed",
    "href": "30_financial_statement_analysis.html#leverage-ratios-how-is-the-company-financed",
    "title": "23  Financial Statement Analysis",
    "section": "23.5 Leverage Ratios: How Is the Company Financed?",
    "text": "23.5 Leverage Ratios: How Is the Company Financed?\nLeverage ratios examine a company’s capital structure (i.e., the mix of debt and equity financing). These metrics reveal financial risk and long-term solvency, helping investors understand how much of the company’s operations are funded by borrowed money.\n\n23.5.1 Why Capital Structure Matters\nA company’s financing choice involves fundamental trade-offs:\n\nDebt offers tax advantages (interest is deductible) and doesn’t dilute ownership, but creates fixed obligations that must be met regardless of business performance\nEquity provides flexibility (no required payments) but dilutes existing shareholders and may be more expensive than debt\n\nCompanies with high leverage amplify both gains and losses. In good times, shareholders capture more upside because profits aren’t shared with additional equity holders. In bad times, fixed interest payments can push the company toward distress. This is why beta (systematic risk) tends to increase with leverage.\n\n\n23.5.2 Debt-to-Equity Ratio\nThis ratio indicates how much debt financing the company uses relative to shareholder investment:\n\\[\n\\text{Debt-to-Equity} = \\frac{\\text{Total Debt}}{\\text{Total Equity}}\n\\]\nA ratio of 1.0 means equal parts debt and equity financing. Higher ratios indicate more aggressive use of leverage, which can enhance returns in good times but increases bankruptcy risk.\n\n\n23.5.3 Debt-to-Asset Ratio\nThis ratio shows what percentage of assets are financed through debt:\n\\[\n\\text{Debt-to-Asset} = \\frac{\\text{Total Debt}}{\\text{Total Assets}}\n\\]\nA ratio of 0.5 means half the company’s assets are debt-financed. This metric is bounded between 0 and 1 (assuming positive equity), making it easier to compare across companies than the debt-to-equity ratio.\n\n\n23.5.4 Interest Coverage Ratio\nWhile the above ratios measure leverage levels, interest coverage assesses the ability to service that debt:\n\\[\n\\text{Interest Coverage} = \\frac{\\text{EBIT}}{\\text{Interest Expense}}\n\\]\nThis ratio answers: “How many times over can current operating profits cover interest obligations?” A ratio below 1.0 means operating income doesn’t cover interest payments, which is a dangerous position. Ratios above 3-5 generally indicate comfortable coverage.\n\n\n23.5.5 Calculating Leverage Ratios\n\nbalance_sheet_statements = balance_sheet_statements.assign(\n    debt_to_equity=lambda x: x[\"total_debt\"] / x[\"total_equity\"],\n    debt_to_asset=lambda x: x[\"total_debt\"] / x[\"at\"]\n)\n\nincome_statements = (comp_vn30\n    .assign(\n        year=lambda x: x[\"year\"].astype(int),\n        # Handle zero interest expense to avoid infinity\n        interest_coverage=lambda x: np.where(\n            x[\"cfo_interest_expense\"] &gt; 0,\n            x[\"is_net_business_profit\"] / x[\"cfo_interest_expense\"],\n            np.nan\n        ),\n        label=lambda x: np.where(\n            x[\"symbol\"].isin(vn30_symbols), x[\"symbol\"], np.nan\n        )\n    )\n)\n\n\n\n23.5.6 Leverage Trends Over Time\nFigure 23.2 tracks how debt-to-asset ratios have evolved over time. Time-series analysis reveals whether companies are becoming more or less leveraged.\n\ndebt_to_asset = balance_sheet_statements.query(\"symbol in @vn30_symbols\")\n\ndebt_to_asset_figure = (\n    ggplot(debt_to_asset, aes(x=\"year\", y=\"debt_to_asset\", color=\"symbol\"))\n    + geom_line(size=1)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Debt-to-Asset Ratio\", color=\"\",\n        title=\"Debt-to-Asset Ratios of VN30 Stocks Over Time\"\n    )\n)\n\ndebt_to_asset_figure.show()\n\n\n\n\n\n\n\nFigure 23.2: Debt-to-asset ratios show the proportion of assets financed by debt. Changes over time reflect evolving capital structure strategies and market conditions.\n\n\n\n\n\n\n\n23.5.7 Cross-Sectional Leverage Comparison\nFigure 23.3 provides a snapshot of leverage across all VN30 constituents for the most recent year.\n\ndebt_to_asset_comparison = balance_sheet_statements.query(\"year == 2023\")\n\ndebt_to_asset_comparison[\"symbol\"] = pd.Categorical(\n    debt_to_asset_comparison[\"symbol\"],\n    categories=debt_to_asset_comparison.sort_values(\"debt_to_asset\")[\"symbol\"],\n    ordered=True\n)\n\ndebt_to_asset_comparison_figure = (\n    ggplot(\n        debt_to_asset_comparison,\n        aes(y=\"debt_to_asset\", x=\"symbol\", fill=\"label\")\n    )\n    + geom_col()\n    + coord_flip()\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Debt-to-Asset Ratio\", fill=\"\",\n        title=\"Debt-to-Asset Ratios of VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\ndebt_to_asset_comparison_figure.show()\n\n\n\n\n\n\n\nFigure 23.3: Cross-sectional comparison of debt-to-asset ratios reveals industry patterns and company-specific financing strategies.\n\n\n\n\n\n\n\n23.5.8 The Leverage-Coverage Trade-off\nFigure 23.4 examines the relationship between leverage levels and debt-servicing ability. Companies with higher debt loads should ideally have stronger interest coverage to maintain financial stability.\n\ninterest_coverage = (income_statements\n    .query(\"year == 2023\")\n    .get([\"symbol\", \"year\", \"interest_coverage\"])\n    .merge(balance_sheet_statements, on=[\"symbol\", \"year\"], how=\"left\")\n)\n\ninterest_coverage_figure = (\n    ggplot(\n        interest_coverage,\n        aes(x=\"debt_to_asset\", y=\"interest_coverage\", color=\"label\")\n    )\n    + geom_point(size=2)\n    + geom_label(\n        aes(label=\"label\"),\n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + labs(\n        x=\"Debt-to-Asset Ratio\", y=\"Interest Coverage Ratio\",\n        title=\"Leverage versus Interest Coverage for VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\ninterest_coverage_figure.show()\n\n\n\n\n\n\n\nFigure 23.4: The relationship between leverage and interest coverage reveals whether companies can comfortably service their debt. High leverage with low coverage indicates elevated financial risk.\n\n\n\n\n\nThe scatter plot reveals important patterns. Companies in the upper-left quadrant (low leverage, high coverage) have conservative financing with ample debt capacity. Those in the lower-right (high leverage, low coverage) face elevated financial risk.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#efficiency-ratios-how-well-are-assets-managed",
    "href": "30_financial_statement_analysis.html#efficiency-ratios-how-well-are-assets-managed",
    "title": "23  Financial Statement Analysis",
    "section": "23.6 Efficiency Ratios: How Well Are Assets Managed?",
    "text": "23.6 Efficiency Ratios: How Well Are Assets Managed?\nEfficiency ratios measure how effectively a company utilizes its assets and manages operations. These metrics help identify whether management is extracting maximum value from the company’s resource base.\n\n23.6.1 Asset Turnover\nThis ratio measures how efficiently a company uses total assets to generate revenue:\n\\[\n\\text{Asset Turnover} = \\frac{\\text{Revenue}}{\\text{Total Assets}}\n\\]\nA higher ratio indicates more efficient asset utilization: the company generates more sales per dollar of assets. However, optimal levels vary dramatically across industries. Retailers with minimal fixed assets might achieve turnovers above 2.0, while capital-intensive manufacturers might operate below 0.5.\n\n\n23.6.2 Inventory Turnover\nFor companies carrying inventory, this ratio reveals how quickly stock moves through the business:\n\\[\n\\text{Inventory Turnover} = \\frac{\\text{Cost of Goods Sold}}{\\text{Inventory}}\n\\]\nHigher turnover suggests efficient inventory management (i.e., goods don’t sit on shelves collecting dust). However, extremely high turnover might indicate stockout risks, while very low turnover could signal obsolete inventory or overinvestment in working capital.\nWe use COGS rather than revenue in the numerator because inventory is recorded at cost, not selling price. Using revenue would overstate turnover for high-margin businesses.\n\n\n23.6.3 Receivables Turnover\nThis ratio measures how effectively a company collects payments from customers:\n\\[\n\\text{Receivables Turnover} = \\frac{\\text{Revenue}}{\\text{Accounts Receivable}}\n\\]\nHigher turnover indicates faster collection (i.e., customers pay promptly). Converting this to “days sales outstanding” (365 / turnover) gives the average collection period in days. Companies must balance collection efficiency against the sales impact of restrictive credit policies.\n\n\n23.6.4 Calculating Efficiency Ratios\n\ncombined_statements = (balance_sheet_statements\n    .get([\n        \"symbol\", \"year\", \"label\", \"current_ratio\", \"quick_ratio\",\n        \"cash_ratio\", \"debt_to_equity\", \"debt_to_asset\", \"total_asset\",\n        \"total_equity\"\n    ])\n    .merge(\n        (income_statements\n            .get([\n                \"symbol\", \"year\", \"interest_coverage\", \"is_revenue\",\n                \"is_cogs\", \n                # \"selling_general_and_administrative_expenses\",\n                \"is_interest_expense\", \"is_gross_profit\", \"is_eat\"\n            ])\n        ),\n        on=[\"symbol\", \"year\"],\n        how=\"left\"\n    )\n    .merge(\n        (comp_vn30\n            .assign(year=lambda x: x[\"year\"].astype(int))\n            .get([\"symbol\", \"year\", \"ca_total_inventory\", \"ca_acc_receiv\"])\n        ),\n        on=[\"symbol\", \"year\"],\n        how=\"left\"\n    )\n)\n\ncombined_statements = combined_statements.assign(\n    asset_turnover=lambda x: x[\"is_revenue\"] / x[\"total_asset\"],\n    inventory_turnover=lambda x: x[\"is_cogs\"] / x[\"ca_total_inventory\"],\n    receivables_turnover=lambda x: x[\"is_revenue\"] / x[\"ca_acc_receiv\"]\n)\n\nEfficiency ratios vary dramatically across industries, making peer comparison essential. A grocery store and a shipbuilder will have fundamentally different asset and inventory dynamics.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#profitability-ratios-is-the-company-making-money",
    "href": "30_financial_statement_analysis.html#profitability-ratios-is-the-company-making-money",
    "title": "23  Financial Statement Analysis",
    "section": "23.7 Profitability Ratios: Is the Company Making Money?",
    "text": "23.7 Profitability Ratios: Is the Company Making Money?\nProfitability ratios evaluate how effectively a company converts activity into earnings. These metrics directly measure financial success and are among the most closely watched indicators by investors.\n\n23.7.1 Gross Margin\nThe gross margin reveals what percentage of revenue remains after direct production costs:\n\\[\n\\text{Gross Margin} = \\frac{\\text{Gross Profit}}{\\text{Revenue}} = \\frac{\\text{Revenue} - \\text{COGS}}{\\text{Revenue}}\n\\]\nHigher gross margins indicate stronger pricing power, more efficient production, or a favorable product mix. This metric is particularly useful for comparing companies within an industry, as it reveals relative efficiency in core operations before overhead costs.\n\n\n23.7.2 Profit Margin\nThe profit margin shows what percentage of revenue ultimately becomes net income:\n\\[\n\\text{Profit Margin} = \\frac{\\text{Net Income}}{\\text{Revenue}}\n\\]\nThis comprehensive measure accounts for all costs (e.g., production, operations, interest, and taxes). Higher profit margins suggest effective overall cost management. However, optimal margins vary by industry: software companies routinely achieve 20%+ margins, while grocery stores operate on razor-thin 2-3% margins.\n\n\n23.7.3 Return on Equity (ROE)\nROE measures how efficiently a company uses shareholders’ investment to generate profits:\n\\[\n\\text{Return on Equity} = \\frac{\\text{Net Income}}{\\text{Total Equity}}\n\\]\nThis metric directly addresses what shareholders care about: returns on their invested capital. Higher ROE indicates more effective use of equity, though interpretation requires caution. High leverage can artificially inflate ROE by reducing the equity base (e.g., a company financed 90% by debt will show spectacular ROE on modest profits).\n\n\n23.7.4 The DuPont Decomposition\nThe DuPont framework decomposes ROE into three components that reveal different aspects of performance:\n\\[\n\\text{ROE} = \\underbrace{\\frac{\\text{Net Income}}{\\text{Revenue}}}_{\\text{Profit Margin}} \\times \\underbrace{\\frac{\\text{Revenue}}{\\text{Assets}}}_{\\text{Asset Turnover}} \\times \\underbrace{\\frac{\\text{Assets}}{\\text{Equity}}}_{\\text{Leverage}}\n\\]\nThis decomposition shows that high ROE can come from different sources: strong profit margins (pricing power, cost control), efficient asset use (high turnover), or aggressive leverage. Understanding which driver dominates helps assess sustainability. ROE driven by margins is generally more sustainable than ROE driven by leverage.\n\n\n23.7.5 Calculating Profitability Ratios\n\ncombined_statements = combined_statements.assign(\n    gross_margin=lambda x: x[\"is_gross_profit\"] / x[\"is_revenue\"],\n    profit_margin=lambda x: x[\"is_eat\"] / x[\"is_revenue\"],\n    after_tax_roe=lambda x: x[\"is_eat\"] / x[\"total_equity\"]\n)\n\n\n\n23.7.6 Gross Margin Trends\nFigure 23.5 tracks gross margin evolution over time, revealing whether companies are maintaining pricing power and production efficiency.\n\ngross_margins = combined_statements.query(\"symbol in @vn30_symbols\")\n\ngross_margins_figure = (\n    ggplot(gross_margins, aes(x=\"year\", y=\"gross_margin\", color=\"symbol\"))\n    + geom_line()\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Gross Margin\", color=\"\",\n        title=\"Gross Margins for VN30 Stocks (2019-2023)\"\n    )\n)\n\ngross_margins_figure.show()\n\n\n\n\n\n\n\nFigure 23.5: Gross margin trends reveal changes in pricing power and production efficiency. Declining margins may signal increased competition or rising input costs.\n\n\n\n\n\n\n\n23.7.7 From Gross to Net: Where Do Profits Go?\nFigure 23.6 examines the relationship between gross and profit margins. The gap between them reveals the impact of operating expenses, interest, and taxes.\n\nprofit_margins = combined_statements.query(\"year == 2023\")\n\nprofit_margins_figure = (\n    ggplot(\n        profit_margins,\n        aes(x=\"gross_margin\", y=\"profit_margin\", color=\"label\")\n    )\n    + geom_point(size=2)\n    + geom_label(\n        aes(label=\"label\"),\n        adjust_text={\"arrowprops\": {\"arrowstyle\": \"-\"}}\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"Gross Margin\", y=\"Profit Margin\",\n        title=\"Gross versus Profit Margins for VN30 Stocks (2023)\"\n    )\n    + theme(legend_position=\"none\")\n)\n\nprofit_margins_figure.show()\n\n\n\n\n\n\n\nFigure 23.6: Comparing gross and profit margins reveals how much of gross profit survives operating expenses, interest, and taxes. Companies far below the diagonal have high overhead relative to gross profit.\n\n\n\n\n\nCompanies along the diagonal convert gross profit to net income efficiently. Those well below the diagonal face high operating costs, interest burdens, or tax rates that erode profitability.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#combining-financial-ratios-a-holistic-view",
    "href": "30_financial_statement_analysis.html#combining-financial-ratios-a-holistic-view",
    "title": "23  Financial Statement Analysis",
    "section": "23.8 Combining Financial Ratios: A Holistic View",
    "text": "23.8 Combining Financial Ratios: A Holistic View\nIndividual ratios provide specific insights, but combining them offers a more complete picture. A company might excel in profitability while struggling with liquidity, or maintain conservative leverage while underperforming on efficiency.\n\n23.8.1 Ranking Companies Across Categories\nFigure 23.7 compares company rankings across four ratio categories. Rankings closer to 1 indicate better performance within each category, enabling quick identification of relative strengths and weaknesses.\n\nfinancial_ratios = (combined_statements\n    .query(\"year == 2023\")\n    .filter(\n        items=[\"symbol\"] + [\n            col for col in combined_statements.columns\n            if any(x in col for x in [\n                \"ratio\", \"margin\", \"roe\", \"_to_\", \"turnover\", \"interest_coverage\"\n            ])\n        ]\n    )\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        type=lambda x: np.select(\n            [\n                x[\"name\"].isin([\"current_ratio\", \"quick_ratio\", \"cash_ratio\"]),\n                x[\"name\"].isin([\"debt_to_equity\", \"debt_to_asset\", \"interest_coverage\"]),\n                x[\"name\"].isin([\"asset_turnover\", \"inventory_turnover\", \"receivables_turnover\"]),\n                x[\"name\"].isin([\"gross_margin\", \"profit_margin\", \"after_tax_roe\"]),\n            ],\n            [\n                \"Liquidity Ratios\",\n                \"Leverage Ratios\",\n                \"Efficiency Ratios\",\n                \"Profitability Ratios\"\n            ],\n            default=\"Other\"\n        )\n    )\n)\n\nfinancial_ratios[\"rank\"] = (financial_ratios\n    .sort_values([\"type\", \"name\", \"value\"], ascending=[True, True, False])\n    .groupby([\"type\", \"name\"])\n    .cumcount() + 1\n)\n\nfinal_ranks = (financial_ratios\n    .groupby([\"symbol\", \"type\"], as_index=False)\n    .agg(rank=(\"rank\", \"mean\"))\n    .query(\"symbol in @vn30_symbols\")\n)\n\nfinal_ranks_figure = (\n    ggplot(final_ranks, aes(x=\"rank\", y=\"type\", color=\"symbol\"))\n    + geom_point(shape=\"^\", size=4)\n    + labs(\n        x=\"Average Rank (Lower is Better)\", y=\"\", color=\"\",\n        title=\"Average Rank Across Financial Ratio Categories\"\n    )\n    + coord_cartesian(xlim=[1, 30])\n)\n\nfinal_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 23.7: Ranking companies across multiple ratio categories reveals overall financial profiles. Companies with consistently low ranks across categories demonstrate broad-based financial strength.\n\n\n\n\n\nThe combined view reveals how different business strategies manifest in financial profiles. A company might deliberately accept lower profitability rankings in exchange for stronger liquidity, or use aggressive leverage to boost returns at the cost of financial flexibility.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#financial-ratios-in-asset-pricing",
    "href": "30_financial_statement_analysis.html#financial-ratios-in-asset-pricing",
    "title": "23  Financial Statement Analysis",
    "section": "23.9 Financial Ratios in Asset Pricing",
    "text": "23.9 Financial Ratios in Asset Pricing\nBeyond evaluating individual companies, financial ratios serve as crucial inputs for asset pricing models. The Fama-French five-factor model, which we explore in detail in Fama-French Factors, uses several accounting-based measures to explain cross-sectional variation in stock returns.\n\n23.9.1 The Fama-French Factors\nThe model incorporates four company characteristics derived from financial statements:\nSize is measured as the logarithm of market capitalization: \\[\n\\text{Size} = \\ln(\\text{Market Cap})\n\\]\nThis captures the empirical finding that smaller firms tend to outperform larger firms on a risk-adjusted basis (i.e., the “size premium”).\nBook-to-Market relates accounting value to market value: \\[\n\\text{Book-to-Market} = \\frac{\\text{Book Equity}}{\\text{Market Cap}}\n\\]\nHigh book-to-market stocks (“value” stocks) have historically outperformed low book-to-market stocks (“growth” stocks) (i.e., the “value premium”).\nOperating Profitability measures profit generation relative to equity: \\[\n\\text{Profitability} = \\frac{\\text{Revenue} - \\text{COGS} - \\text{SG\\&A} - \\text{Interest}}{\\text{Book Equity}}\n\\]\nMore profitable firms tend to earn higher returns (i.e., the “profitability premium”).\nInvestment captures asset growth: \\[\n\\text{Investment} = \\frac{\\text{Total Assets}_t}{\\text{Total Assets}_{t-1}} - 1\n\\] Firms investing aggressively tend to underperform conservative investors (i.e., the “investment premium”).\n\n\n23.9.2 Calculating Fama-French Variables\n\nprices_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM prices_monthly\",\n    con=tidy_finance,\n    parse_dates={\"datadate\"}\n)\n\n# Use December prices for annual calculations\nprices_december = (prices_monthly\n    .assign(date=lambda x: pd.to_datetime(x[\"date\"]))\n    .query(\"date.dt.month == 12\")\n)\n\n\ncombined_statements_ff = (combined_statements\n    .query(\"year == 2023\")\n    .merge(prices_december, on=[\"symbol\", \"year\"], how=\"left\")\n    .merge(\n        (balance_sheet_statements\n            .query(\"year == 2022\")\n            .get([\"symbol\", \"total_asset\"])\n            .rename(columns={\"total_asset\": \"total_assets_lag\"})\n        ),\n        on=\"symbol\",\n        how=\"left\"\n    )\n    .assign(\n        size=lambda x: np.log(x[\"mktcap\"]),\n        book_to_market=lambda x: x[\"total_equity\"] / x[\"mktcap\"],\n        operating_profitability=lambda x: (\n            (x[\"is_revenue\"] - x[\"is_cogs\"] -\n            #  x[\"selling_general_and_administrative_expenses\"] -\n             x[\"is_interest_expense\"]) / x[\"total_equity\"]\n        ),\n        investment=lambda x: x[\"total_asset\"] / x[\"total_assets_lag\"] - 1\n    )\n)\n\ncombined_statements_ff.head(3)\n\n\n\n\n\n\n\n\nsymbol\nyear\nlabel\ncurrent_ratio\nquick_ratio\ncash_ratio\ndebt_to_equity\ndebt_to_asset\ntotal_asset\ntotal_equity\n...\nmonth\nret\nrisk_free\nret_excess\nmktcap_lag\ntotal_assets_lag\nsize\nbook_to_market\noperating_profitability\ninvestment\n\n\n\n\n0\nPOW\n2023\nPOW\n1.084255\n1.084255\n0.315089\n0.0\n0.0\n7.036209e+13\n3.411943e+13\n...\n12.0\n0.000000\n0.003333\n-0.003333\n26346.055367\n5.684324e+13\n10.179074\n1.295049e+09\n0.062706\n0.237827\n\n\n1\nHPG\n2023\nHPG\n1.156655\n1.156655\n0.171324\n0.0\n0.0\n1.877826e+14\n1.028364e+14\n...\n12.0\n0.052731\n0.003333\n0.049397\n154382.560242\n1.703355e+14\n11.998576\n6.327489e+08\n0.104583\n0.102428\n\n\n2\nMWG\n2023\nMWG\n1.688604\n1.688604\n0.174408\n0.0\n0.0\n6.011124e+13\n2.335956e+13\n...\n12.0\n0.111688\n0.003333\n0.108355\n56323.247628\n5.583410e+13\n11.044743\n3.730731e+08\n0.942967\n0.076604\n\n\n\n\n3 rows × 43 columns\n\n\n\n\n\n23.9.3 Fama-French Factor Rankings\nFigure 23.8 shows how VN30 companies rank on each Fama-French variable, connecting fundamental analysis to asset pricing.\n\nfactors_ranks = (combined_statements_ff\n    .get([\"symbol\", \"size\", \"book_to_market\", \"operating_profitability\", \"investment\"])\n    .rename(columns={\n        \"size\": \"Size\",\n        \"book_to_market\": \"Book-to-Market\",\n        \"operating_profitability\": \"Profitability\",\n        \"investment\": \"Investment\"\n    })\n    .melt(id_vars=[\"symbol\"], var_name=\"name\", value_name=\"value\")\n    .assign(\n        rank=lambda x: (\n            x.sort_values([\"name\", \"value\"], ascending=[True, False])\n            .groupby(\"name\")\n            .cumcount() + 1\n        )\n    )\n    .query(\"symbol in @vn30_symbols\")\n)\n\nfactors_ranks_figure = (\n    ggplot(factors_ranks, aes(x=\"rank\", y=\"name\", color=\"symbol\"))\n    + geom_point(shape=\"^\", size=4)\n    + labs(\n        x=\"Rank\", y=\"\", color=\"\",\n        title=\"Rank in Fama-French Variables for VN30 Stocks\"\n    )\n    + coord_cartesian(xlim=[1, 30])\n)\n\nfactors_ranks_figure.show()\n\n\n\n\n\n\n\nFigure 23.8: Rankings on Fama-French variables connect financial statement analysis to asset pricing. According to factor models, smaller, higher book-to-market, more profitable, and lower-investment firms should earn higher expected returns.\n\n\n\n\n\nThese rankings have implications for expected returns according to factor models. A small, high book-to-market, highly profitable company with conservative investment should, in theory, earn higher risk-adjusted returns than its opposite.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#limitations-and-practical-considerations",
    "href": "30_financial_statement_analysis.html#limitations-and-practical-considerations",
    "title": "23  Financial Statement Analysis",
    "section": "23.10 Limitations and Practical Considerations",
    "text": "23.10 Limitations and Practical Considerations\nWhile financial ratios provide powerful analytical tools, several limitations deserve attention:\n\n23.10.1 Accounting Discretion\nCompanies have significant discretion in how they apply accounting standards. Revenue recognition timing, depreciation methods, inventory valuation (FIFO vs. LIFO), and capitalization versus expensing decisions all affect reported numbers. Sophisticated analysis requires understanding these choices and their impact.\n\n\n23.10.2 Industry Comparability\nRatios vary dramatically across industries. Comparing a bank’s leverage to a retailer’s is meaningless (e.g., banks naturally operate with much higher leverage due to their business model). Always benchmark against industry peers rather than absolute standards.\n\n\n23.10.3 Point-in-Time Limitations\nBalance sheet ratios capture a single moment, which may not represent typical conditions. Companies often “window dress” by temporarily improving metrics at reporting dates. Trend analysis and quarter-over-quarter comparisons can reveal such practices.\n\n\n23.10.4 Backward-Looking Nature\nFinancial statements report historical results. Past profitability doesn’t guarantee future performance, especially for companies in rapidly changing industries or facing disruption.\n\n\n23.10.5 Quality of Earnings\nNot all profits are created equal. Earnings driven by one-time gains, accounting adjustments, or aggressive revenue recognition may not recur. Cash flow analysis helps assess earnings quality. Profits that don’t convert to cash warrant skepticism.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "30_financial_statement_analysis.html#key-takeaways",
    "href": "30_financial_statement_analysis.html#key-takeaways",
    "title": "23  Financial Statement Analysis",
    "section": "23.11 Key Takeaways",
    "text": "23.11 Key Takeaways\nThis chapter introduced financial statement analysis as a tool for understanding company fundamentals. The main insights are:\n\nThree statements, three perspectives: The balance sheet shows financial position at a point in time, the income statement measures performance over a period, and the cash flow statement tracks actual cash movements. Together, they provide a complete picture of financial health.\nLiquidity ratios assess short-term survival: Current, quick, and cash ratios measure the ability to meet near-term obligations. Higher ratios indicate greater liquidity but may suggest inefficient asset use.\nLeverage ratios reveal capital structure risk: Debt-to-equity, debt-to-asset, and interest coverage ratios show how the company finances operations and whether it can service its debt. Higher leverage amplifies both returns and risk.\nEfficiency ratios measure management effectiveness: Asset turnover, inventory turnover, and receivables turnover reveal how well the company converts resources into revenue. Industry context is essential for interpretation.\nProfitability ratios quantify financial success: Gross margin, profit margin, and ROE measure the ability to generate earnings. The DuPont decomposition reveals whether ROE comes from margins, turnover, or leverage.\nRatios connect to asset pricing: Financial statement variables like book-to-market, profitability, and investment form the basis of factor models that explain cross-sectional return differences.\nContext matters for interpretation: Ratios must be compared against industry peers, tracked over time, and considered alongside qualitative factors. No single ratio tells the complete story.\n\nLooking ahead, subsequent chapters will explore how these fundamental variables interact with market prices in asset pricing models, and how to construct factor portfolios based on financial statement characteristics.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Financial Statement Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html",
    "href": "31_discounted_cash_flow.html",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "",
    "text": "24.1 What Is a Company Worth?\nThe previous chapters examined how markets price securities in equilibrium and how financial statements reveal company fundamentals. But these approaches leave a central question unanswered: What is the intrinsic value of a business, independent of its current market price?\nDiscounted Cash Flow (DCF) analysis answers this question by valuing a company based on its ability to generate cash for investors. The core insight is simple: a business is worth the present value of all future cash it will produce. This principle that value equals discounted future cash flows underlies virtually all of finance, from bond pricing to real estate valuation.\nDCF analysis stands apart from other valuation approaches in three important ways. First, it explicitly accounts for the time value of money (i.e., the principle that a dollar today is worth more than a dollar tomorrow). By discounting future cash flows at an appropriate rate, we incorporate both time preferences and risk. Second, DCF is forward-looking, making it particularly suitable for companies where historical performance may not reflect future potential. Third, DCF is flexible enough to accommodate various business models and capital structures, making it applicable across industries and company sizes.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#what-is-a-company-worth",
    "href": "31_discounted_cash_flow.html#what-is-a-company-worth",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "",
    "text": "24.1.1 Valuation Methods Overview\nCompany valuation methods broadly fall into three categories:\n\nMarket-based approaches compare companies using relative metrics like Price-to-Earnings or EV/EBITDA ratios. These are quick but assume comparable companies are fairly valued.\nAsset-based methods focus on the net value of tangible and intangible assets. These work well for liquidation scenarios but miss going-concern value.\nIncome-based techniques value companies based on their ability to generate future cash flows. DCF is the most rigorous income-based method.\n\nWe focus on DCF because it forces analysts to make explicit assumptions about growth, profitability, and risk. These assumptions are often hidden in other methods. Even when DCF isn’t the final word on valuation, the discipline of building a DCF model deepens understanding of what drives value.\n\n\n24.1.2 The Three Pillars of DCF\nEvery DCF analysis rests on three components:\n\nFree Cash Flow (FCF) forecasts: The expected future cash available for distribution to investors after operating expenses, taxes, and investments\nTerminal value: The company’s value beyond the explicit forecast period, often representing a majority of total valuation\nDiscount rate: Typically the Weighted Average Cost of Capital (WACC), which adjusts future cash flows to present value by incorporating risk and capital structure\n\nWe make simplifying assumptions throughout this chapter. In particular, we assume firms conduct only operating activities (i.e., financial statements do not include non-operating items like excess cash or investment securities). Real-world valuations require valuing these separately. Entire textbooks are devoted to valuation nuances; our goal is to establish the conceptual framework and practical implementation.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\n\nfrom plotnine import *\nfrom mizani.formatters import percent_format, comma_format\nfrom itertools import product",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#understanding-free-cash-flow",
    "href": "31_discounted_cash_flow.html#understanding-free-cash-flow",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.2 Understanding Free Cash Flow",
    "text": "24.2 Understanding Free Cash Flow\nBefore diving into calculations, we need to understand what Free Cash Flow represents and why it matters for valuation.\n\n24.2.1 Why Free Cash Flow, Not Net Income?\nAccountants report net income, but DCF uses free cash flow. Why the difference?\nNet income includes non-cash items (like depreciation) and ignores cash needs (like capital expenditures and working capital investments). A company can report strong profits while burning cash, or generate substantial cash while reporting losses. Free cash flow captures what actually matters for valuation: the cash available to distribute to all capital providers (both debt holders and equity holders) after funding operations and investments.\n\n\n24.2.2 The Free Cash Flow Formula\nWe calculate FCF using the following formula:\n\\[\n\\text{FCF} = \\text{EBIT} \\times (1 - \\tau) + \\text{D\\&A} - \\Delta\\text{WC} - \\text{CAPEX}\n\\]\nwhere:\n\nEBIT (Earnings Before Interest and Taxes): Core operating profit before financing costs and taxes\n\\(\\tau\\): Corporate tax rate applied to operating profits\nD&A (Depreciation & Amortization): Non-cash charges that reduce reported earnings but don’t consume cash\n\\(\\Delta\\)WC (Change in Working Capital): Cash tied up in (or released from) operations (increases in receivables and inventory consume cash, while increases in payables provide cash)\nCAPEX (Capital Expenditures): Investments in long-term assets required to maintain and grow operations\n\nAn alternative formulation starts from EBIT directly:\n\\[\n\\text{FCF} = \\text{EBIT} + \\text{D\\&A} - \\text{Taxes} - \\Delta\\text{WC} - \\text{CAPEX}\n\\]\nBoth formulations yield the same result when taxes are calculated consistently. The key insight is that FCF represents cash generated from operations after all reinvestment needs (i.e., cash that could theoretically be distributed to investors without impairing the business).",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#loading-historical-financial-data",
    "href": "31_discounted_cash_flow.html#loading-historical-financial-data",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.3 Loading Historical Financial Data",
    "text": "24.3 Loading Historical Financial Data\nWe use FPT Corporation, one of Vietnam’s largest technology companies, as our case study. FPT provides IT services, telecommunications, and education. It’s a diversified business with meaningful capital requirements and growth potential.\n\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\ncomp_vn = pd.read_sql_query(\n    sql=\"SELECT * FROM comp_vn\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\n# Filter to FPT and examine the data structure\nfpt_data = comp_vn[comp_vn[\"symbol\"] == \"FPT\"].copy()\nfpt_data[\"year\"] = fpt_data[\"year\"].astype(int)\nfpt_data = fpt_data.sort_values(\"year\").reset_index(drop=True)\n\nprint(f\"Available years: {fpt_data['year'].min()} to {fpt_data['year'].max()}\")\nprint(f\"Number of observations: {len(fpt_data)}\")\n\nAvailable years: 2002 to 2023\nNumber of observations: 22\n\n\n\n24.3.1 Computing Historical Free Cash Flow\nLet’s calculate the components needed for FCF from the financial statement data:\n\n# Extract and compute FCF components\nhistorical_data = (fpt_data\n    .assign(\n        # Revenue for ratio calculations\n        revenue=lambda x: x[\"is_net_revenue\"],\n        \n        # EBIT = Earnings before interest and taxes\n        # Approximate as EBT + Interest Expense\n        ebit=lambda x: x[\"is_ebt\"] + x[\"is_interest_expense\"],\n        \n        # Tax payments (use actual tax expense)\n        taxes=lambda x: x[\"is_cit_expense\"],\n        \n        # Depreciation and amortization (non-cash add-back)\n        depreciation=lambda x: x[\"cfo_depreciation\"],\n        \n        # Change in working capital components\n        # Positive delta_wc means cash is consumed (tied up in working capital)\n        delta_working_capital=lambda x: (\n            x[\"cfo_receive\"] +      # Change in receivables\n            x[\"cfo_inventory\"] -    # Change in inventory  \n            x[\"cfo_payale\"]         # Change in payables (negative = cash source)\n        ),\n        \n        # Capital expenditures\n        capex=lambda x: x[\"capex\"]\n    )\n    .loc[:, [\n        \"year\", \"revenue\", \"ebit\", \"taxes\", \"depreciation\",\n        \"delta_working_capital\", \"capex\"\n    ]]\n)\n\n# Calculate Free Cash Flow\nhistorical_data[\"fcf\"] = (\n    historical_data[\"ebit\"] \n    - historical_data[\"taxes\"]\n    + historical_data[\"depreciation\"]\n    - historical_data[\"delta_working_capital\"]\n    - historical_data[\"capex\"]\n)\n\nhistorical_data\n\n\n\n\n\n\n\n\nyear\nrevenue\nebit\ntaxes\ndepreciation\ndelta_working_capital\ncapex\nfcf\n\n\n\n\n0\n2002\n1.514961e+12\n2.698700e+10\n0.000000e+00\n1.261500e+10\n-2.561760e+11\n2.202800e+10\n2.737500e+11\n\n\n1\n2003\n4.148298e+12\n5.676100e+10\n0.000000e+00\n1.837700e+10\n-5.078740e+11\n3.753300e+10\n5.454790e+11\n\n\n2\n2004\n8.734781e+12\n2.145902e+11\n1.795700e+10\n2.947900e+10\n-4.280270e+11\n5.252100e+10\n6.016182e+11\n\n\n3\n2005\n1.410079e+13\n3.753490e+11\n4.251500e+10\n5.381700e+10\n-4.471110e+11\n1.428320e+11\n6.909300e+11\n\n\n4\n2006\n2.139975e+13\n6.672593e+11\n7.368682e+10\n1.068192e+11\n-1.173099e+12\n2.459780e+11\n1.627513e+12\n\n\n5\n2007\n1.349889e+13\n1.071941e+12\n1.487146e+11\n1.709335e+11\n-1.873794e+12\n4.802762e+11\n2.487677e+12\n\n\n6\n2008\n1.638184e+13\n1.320573e+12\n1.890384e+11\n2.395799e+11\n-1.419506e+11\n6.690461e+11\n8.440192e+11\n\n\n7\n2009\n1.840403e+13\n1.807221e+12\n2.916482e+11\n3.041813e+11\n-8.065011e+11\n7.632280e+11\n1.863027e+12\n\n\n8\n2010\n2.001730e+13\n2.261341e+12\n3.314359e+11\n3.294060e+11\n-2.360993e+12\n8.672138e+11\n3.753090e+12\n\n\n9\n2011\n2.537025e+13\n2.751044e+12\n4.223952e+11\n3.759567e+11\n-2.099380e+12\n4.524081e+11\n4.351578e+12\n\n\n10\n2012\n2.459430e+13\n2.635219e+12\n4.210738e+11\n3.995598e+11\n8.043763e+11\n7.083318e+11\n1.100997e+12\n\n\n11\n2013\n2.702789e+13\n2.690568e+12\n4.503170e+11\n4.429860e+11\n-1.947751e+12\n9.110216e+11\n3.719967e+12\n\n\n12\n2014\n3.264466e+13\n2.625389e+12\n3.800994e+11\n5.472736e+11\n-3.078130e+12\n1.417399e+12\n4.453295e+12\n\n\n13\n2015\n3.795970e+13\n3.113651e+12\n4.130641e+11\n7.328801e+11\n-1.951778e+12\n1.974295e+12\n3.410951e+12\n\n\n14\n2016\n3.953147e+13\n3.388085e+12\n4.382078e+11\n9.334397e+11\n-9.242713e+11\n1.428472e+12\n3.379116e+12\n\n\n15\n2017\n4.265861e+13\n4.623663e+12\n7.270039e+11\n1.039417e+12\n-4.638788e+12\n1.100498e+12\n8.474367e+12\n\n\n16\n2018\n2.321354e+13\n4.095947e+12\n6.236054e+11\n1.164692e+12\n-1.033438e+12\n2.452902e+12\n3.217569e+12\n\n\n17\n2019\n2.771696e+13\n5.023518e+12\n7.528183e+11\n1.354613e+12\n-5.308818e+11\n3.230818e+12\n2.925377e+12\n\n\n18\n2020\n2.983040e+13\n5.648794e+12\n8.397114e+11\n1.490607e+12\n-8.040730e+11\n3.014322e+12\n4.089441e+12\n\n\n19\n2021\n3.565726e+13\n6.821202e+12\n9.879053e+11\n1.643916e+12\n-2.821825e+12\n2.908134e+12\n7.390903e+12\n\n\n20\n2022\n4.400953e+13\n8.308009e+12\n1.170940e+12\n1.833064e+12\n-3.746661e+12\n3.209581e+12\n9.507213e+12\n\n\n21\n2023\n5.261790e+13\n1.003565e+13\n1.414956e+12\n2.286514e+12\n-2.147304e+12\n3.948982e+12\n9.105534e+12\n\n\n\n\n\n\n\n\n\n24.3.2 Understanding the Historical Pattern\nBefore forecasting, we should understand the historical trends in FCF and its components:\n\n# Calculate key ratios relative to revenue\nhistorical_ratios = (historical_data\n    .assign(\n        # Revenue growth (year-over-year)\n        revenue_growth=lambda x: x[\"revenue\"].pct_change(),\n        \n        # Operating margin: EBIT as % of revenue\n        operating_margin=lambda x: x[\"ebit\"] / x[\"revenue\"],\n        \n        # Depreciation as % of revenue\n        depreciation_margin=lambda x: x[\"depreciation\"] / x[\"revenue\"],\n        \n        # Tax rate (taxes as % of revenue, for simplicity)\n        tax_margin=lambda x: x[\"taxes\"] / x[\"revenue\"],\n        \n        # Working capital intensity\n        working_capital_margin=lambda x: x[\"delta_working_capital\"] / x[\"revenue\"],\n        \n        # Capital intensity\n        capex_margin=lambda x: x[\"capex\"] / x[\"revenue\"],\n        \n        # FCF margin\n        fcf_margin=lambda x: x[\"fcf\"] / x[\"revenue\"]\n    )\n)\n\n# Display key metrics\ndisplay_cols = [\n    \"year\", \"revenue_growth\", \"operating_margin\", \"depreciation_margin\",\n    \"tax_margin\", \"working_capital_margin\", \"capex_margin\", \"fcf_margin\"\n]\n\nhistorical_ratios[display_cols].round(3)\n\n\n\n\n\n\n\n\nyear\nrevenue_growth\noperating_margin\ndepreciation_margin\ntax_margin\nworking_capital_margin\ncapex_margin\nfcf_margin\n\n\n\n\n0\n2002\nNaN\n0.018\n0.008\n0.000\n-0.169\n0.015\n0.181\n\n\n1\n2003\n1.738\n0.014\n0.004\n0.000\n-0.122\n0.009\n0.131\n\n\n2\n2004\n1.106\n0.025\n0.003\n0.002\n-0.049\n0.006\n0.069\n\n\n3\n2005\n0.614\n0.027\n0.004\n0.003\n-0.032\n0.010\n0.049\n\n\n4\n2006\n0.518\n0.031\n0.005\n0.003\n-0.055\n0.011\n0.076\n\n\n5\n2007\n-0.369\n0.079\n0.013\n0.011\n-0.139\n0.036\n0.184\n\n\n6\n2008\n0.214\n0.081\n0.015\n0.012\n-0.009\n0.041\n0.052\n\n\n7\n2009\n0.123\n0.098\n0.017\n0.016\n-0.044\n0.041\n0.101\n\n\n8\n2010\n0.088\n0.113\n0.016\n0.017\n-0.118\n0.043\n0.187\n\n\n9\n2011\n0.267\n0.108\n0.015\n0.017\n-0.083\n0.018\n0.172\n\n\n10\n2012\n-0.031\n0.107\n0.016\n0.017\n0.033\n0.029\n0.045\n\n\n11\n2013\n0.099\n0.100\n0.016\n0.017\n-0.072\n0.034\n0.138\n\n\n12\n2014\n0.208\n0.080\n0.017\n0.012\n-0.094\n0.043\n0.136\n\n\n13\n2015\n0.163\n0.082\n0.019\n0.011\n-0.051\n0.052\n0.090\n\n\n14\n2016\n0.041\n0.086\n0.024\n0.011\n-0.023\n0.036\n0.085\n\n\n15\n2017\n0.079\n0.108\n0.024\n0.017\n-0.109\n0.026\n0.199\n\n\n16\n2018\n-0.456\n0.176\n0.050\n0.027\n-0.045\n0.106\n0.139\n\n\n17\n2019\n0.194\n0.181\n0.049\n0.027\n-0.019\n0.117\n0.106\n\n\n18\n2020\n0.076\n0.189\n0.050\n0.028\n-0.027\n0.101\n0.137\n\n\n19\n2021\n0.195\n0.191\n0.046\n0.028\n-0.079\n0.082\n0.207\n\n\n20\n2022\n0.234\n0.189\n0.042\n0.027\n-0.085\n0.073\n0.216\n\n\n21\n2023\n0.196\n0.191\n0.043\n0.027\n-0.041\n0.075\n0.173",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#visualizing-historical-ratios",
    "href": "31_discounted_cash_flow.html#visualizing-historical-ratios",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.4 Visualizing Historical Ratios",
    "text": "24.4 Visualizing Historical Ratios\nFigure 24.1 shows the historical evolution of key financial ratios that drive FCF. Understanding these patterns helps inform our forecasts.\n\n# Prepare data for plotting\nratio_columns = [\n    \"operating_margin\", \"depreciation_margin\", \"tax_margin\",\n    \"working_capital_margin\", \"capex_margin\"\n]\n\nratios_long = (historical_ratios\n    .melt(\n        id_vars=[\"year\"],\n        value_vars=ratio_columns,\n        var_name=\"ratio\",\n        value_name=\"value\"\n    )\n    .assign(\n        ratio=lambda x: x[\"ratio\"].str.replace(\"_\", \" \").str.title()\n    )\n)\n\nratios_figure = (\n    ggplot(ratios_long, aes(x=\"year\", y=\"value\", color=\"ratio\"))\n    + geom_line(size=1)\n    + geom_point(size=2)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Ratio (% of Revenue)\", color=\"\",\n        title=\"Key Financial Ratios of FPT Over Time\"\n    )\n    + theme(legend_position=\"right\")\n)\n\nratios_figure.show()\n\n\n\n\n\n\n\nFigure 24.1: Historical financial ratios reveal the operating characteristics of FPT. These patterns inform our forecast assumptions.\n\n\n\n\n\nSeveral patterns emerge from the historical data. Operating margins show the profitability of core operations. Depreciation margins indicate asset intensity. CAPEX margins reveal investment requirements. Working capital margins can be volatile, reflecting changes in credit terms and inventory management.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#forecasting-free-cash-flow",
    "href": "31_discounted_cash_flow.html#forecasting-free-cash-flow",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.5 Forecasting Free Cash Flow",
    "text": "24.5 Forecasting Free Cash Flow\nWith historical patterns established, we now project FCF into the future. This requires forecasting both revenue growth and the ratios that convert revenue into cash flow.\n\n24.5.1 The Ratio-Based Forecasting Approach\nWe use a ratio-based approach that links all FCF components to revenue. This makes forecasting tractable: rather than projecting absolute dollar amounts for each component, we forecast (1) revenue growth and (2) how each component scales with revenue.\nThis approach embeds a key assumption: that the relationship between revenue and FCF components remains stable. In reality, operating leverage, investment needs, and working capital requirements may change as companies mature. Sophisticated valuations model these dynamics explicitly.\n\n\n24.5.2 Setting Forecast Assumptions\nFor our five-year forecast, we make the following assumptions about FPT’s financial ratios. These should reflect industry analysis, company guidance, and competitive dynamics. Here we use estimates for illustration:\n\n# Define the forecast horizon\nlast_historical_year = historical_data[\"year\"].max()\nforecast_years = list(range(last_historical_year + 1, last_historical_year + 6))\nn_forecast_years = len(forecast_years)\n\nprint(f\"Forecast period: {forecast_years[0]} to {forecast_years[-1]}\")\n\n# Define forecast ratios\n# In practice, these would come from detailed analysis\nforecast_assumptions = pd.DataFrame({\n    \"year\": forecast_years,\n    # Operating margin: slight improvement as scale increases\n    \"operating_margin\": [0.12, 0.125, 0.13, 0.13, 0.135],\n    # Depreciation: stable as % of revenue\n    \"depreciation_margin\": [0.03, 0.03, 0.03, 0.028, 0.028],\n    # Tax rate: stable\n    \"tax_margin\": [0.02, 0.02, 0.02, 0.02, 0.02],\n    # Working capital: modest cash consumption\n    \"working_capital_margin\": [0.01, 0.01, 0.008, 0.008, 0.008],\n    # CAPEX: declining as % of revenue as growth moderates\n    \"capex_margin\": [0.05, 0.048, 0.045, 0.042, 0.04]\n})\n\nforecast_assumptions\n\nForecast period: 2024 to 2028\n\n\n\n\n\n\n\n\n\nyear\noperating_margin\ndepreciation_margin\ntax_margin\nworking_capital_margin\ncapex_margin\n\n\n\n\n0\n2024\n0.120\n0.030\n0.02\n0.010\n0.050\n\n\n1\n2025\n0.125\n0.030\n0.02\n0.010\n0.048\n\n\n2\n2026\n0.130\n0.030\n0.02\n0.008\n0.045\n\n\n3\n2027\n0.130\n0.028\n0.02\n0.008\n0.042\n\n\n4\n2028\n0.135\n0.028\n0.02\n0.008\n0.040\n\n\n\n\n\n\n\n\n\n24.5.3 Forecasting Revenue Growth\nRevenue growth is often the most important and most uncertain assumption in DCF analysis. We demonstrate two approaches: using historical averages and linking growth to macroeconomic forecasts.\nApproach 1: Historical Average\nA simple approach uses the historical average growth rate:\n\nhistorical_growth = historical_ratios[\"revenue_growth\"].dropna()\navg_historical_growth = historical_growth.mean()\n\nprint(f\"Average historical revenue growth: {avg_historical_growth:.1%}\")\n\nAverage historical revenue growth: 25.2%\n\n\nApproach 2: GDP-Linked Growth\nA more sophisticated approach links company growth to GDP forecasts from institutions like the IMF. This captures the intuition that company revenues often move with broader economic activity.\n\n# Vietnam GDP growth forecasts (illustrative, based on IMF WEO style projections)\n# In practice, download from IMF WEO database\ngdp_forecasts = pd.DataFrame({\n    \"year\": forecast_years,\n    \"gdp_growth\": [0.065, 0.063, 0.060, 0.058, 0.055]  # Gradually declining to long-term\n})\n\n# Assume FPT grows at a premium to GDP (tech sector outperformance)\n# This premium should reflect company-specific factors\ngrowth_premium = 0.05  # 5 percentage points above GDP\n\nforecast_assumptions = forecast_assumptions.merge(gdp_forecasts, on=\"year\")\nforecast_assumptions[\"revenue_growth\"] = (\n    forecast_assumptions[\"gdp_growth\"] + growth_premium\n)\n\nforecast_assumptions[[\"year\", \"gdp_growth\", \"revenue_growth\"]]\n\n\n\n\n\n\n\n\nyear\ngdp_growth\nrevenue_growth\n\n\n\n\n0\n2024\n0.065\n0.115\n\n\n1\n2025\n0.063\n0.113\n\n\n2\n2026\n0.060\n0.110\n\n\n3\n2027\n0.058\n0.108\n\n\n4\n2028\n0.055\n0.105\n\n\n\n\n\n\n\n\n\n24.5.4 Building the Forecast\nNow we combine our assumptions to project revenue and FCF:\n\n# Get the last historical revenue as our starting point\nlast_revenue = historical_data.loc[\n    historical_data[\"year\"] == last_historical_year, \"revenue\"\n].values[0]\n\nprint(f\"Last historical revenue ({last_historical_year}): {last_revenue/1e12:.2f} trillion VND\")\n\n# Project revenue forward\nforecast_data = forecast_assumptions.copy()\nforecast_data[\"revenue\"] = None\n\n# Calculate revenue for each forecast year\nfor i, row in forecast_data.iterrows():\n    if i == 0:\n        # First forecast year: grow from last historical\n        forecast_data.loc[i, \"revenue\"] = last_revenue * (1 + row[\"revenue_growth\"])\n    else:\n        # Subsequent years: grow from previous forecast\n        prev_revenue = forecast_data.loc[i-1, \"revenue\"]\n        forecast_data.loc[i, \"revenue\"] = prev_revenue * (1 + row[\"revenue_growth\"])\n\n# Convert revenue to numeric\nforecast_data[\"revenue\"] = forecast_data[\"revenue\"].astype(float)\n\n# Calculate FCF components from ratios\nforecast_data[\"ebit\"] = forecast_data[\"operating_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"depreciation\"] = forecast_data[\"depreciation_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"taxes\"] = forecast_data[\"tax_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"delta_working_capital\"] = forecast_data[\"working_capital_margin\"] * forecast_data[\"revenue\"]\nforecast_data[\"capex\"] = forecast_data[\"capex_margin\"] * forecast_data[\"revenue\"]\n\n# Calculate FCF\nforecast_data[\"fcf\"] = (\n    forecast_data[\"ebit\"]\n    - forecast_data[\"taxes\"]\n    + forecast_data[\"depreciation\"]\n    - forecast_data[\"delta_working_capital\"]\n    - forecast_data[\"capex\"]\n)\n\nforecast_data[[\"year\", \"revenue\", \"ebit\", \"fcf\"]].round(0)\n\nLast historical revenue (2023): 52.62 trillion VND\n\n\n\n\n\n\n\n\n\nyear\nrevenue\nebit\nfcf\n\n\n\n\n0\n2024\n5.866896e+13\n7.040275e+12\n4.106827e+12\n\n\n1\n2025\n6.529855e+13\n8.162319e+12\n5.027988e+12\n\n\n2\n2026\n7.248139e+13\n9.422581e+12\n6.305881e+12\n\n\n3\n2027\n8.030938e+13\n1.044022e+13\n7.067226e+12\n\n\n4\n2028\n8.874187e+13\n1.198015e+13\n8.430477e+12",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#visualizing-the-forecast",
    "href": "31_discounted_cash_flow.html#visualizing-the-forecast",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.6 Visualizing the Forecast",
    "text": "24.6 Visualizing the Forecast\nFigure 24.2 compares our forecast ratios with historical values, showing the transition from realized to projected performance.\n\n# Prepare historical data for plotting\nhistorical_plot = (historical_ratios\n    .loc[:, [\"year\", \"operating_margin\", \"depreciation_margin\", \n             \"tax_margin\", \"working_capital_margin\", \"capex_margin\"]]\n    .assign(type=\"Historical\")\n)\n\n# Prepare forecast data for plotting\nforecast_plot = (forecast_data\n    .loc[:, [\"year\", \"operating_margin\", \"depreciation_margin\",\n             \"tax_margin\", \"working_capital_margin\", \"capex_margin\"]]\n    .assign(type=\"Forecast\")\n)\n\n# Combine\ncombined_ratios = pd.concat([historical_plot, forecast_plot], ignore_index=True)\n\n# Reshape for plotting\ncombined_long = combined_ratios.melt(\n    id_vars=[\"year\", \"type\"],\n    var_name=\"ratio\",\n    value_name=\"value\"\n)\n\ncombined_long[\"type\"] = pd.Categorical(\n    combined_long[\"type\"], \n    categories=[\"Historical\", \"Forecast\"]\n)\n\nforecast_ratios_figure = (\n    ggplot(combined_long, aes(x=\"year\", y=\"value\", color=\"ratio\", linetype=\"type\"))\n    + geom_line(size=1)\n    + geom_point(size=2)\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Ratio (% of Revenue)\", color=\"\", linetype=\"\",\n        title=\"Historical and Forecast Financial Ratios for FPT\"\n    )\n    + theme(legend_position=\"right\")\n)\n\nforecast_ratios_figure.show()\n\n\n\n\n\n\n\nFigure 24.2: Historical ratios (solid lines) and forecast assumptions (dashed lines) for key financial metrics. The forecast period begins after the last historical observation.\n\n\n\n\n\nFigure 24.3 shows the revenue growth trajectory, comparing historical performance with our GDP-linked forecasts.\n\n# Prepare growth data\nhistorical_growth_df = (historical_ratios\n    .loc[:, [\"year\", \"revenue_growth\"]]\n    .dropna()\n    .assign(type=\"Historical\")\n)\n\nforecast_growth_df = (forecast_data\n    .loc[:, [\"year\", \"revenue_growth\", \"gdp_growth\"]]\n    .assign(type=\"Forecast\")\n)\n\n# Combine for revenue growth\ngrowth_combined = pd.concat([\n    historical_growth_df,\n    forecast_growth_df[[\"year\", \"revenue_growth\", \"type\"]]\n], ignore_index=True)\n\ngrowth_combined[\"type\"] = pd.Categorical(\n    growth_combined[\"type\"],\n    categories=[\"Historical\", \"Forecast\"]\n)\n\ngrowth_figure = (\n    ggplot(growth_combined, aes(x=\"year\", y=\"revenue_growth\", linetype=\"type\"))\n    + geom_line(size=1, color=\"steelblue\")\n    + geom_point(size=2, color=\"steelblue\")\n    + scale_y_continuous(labels=percent_format())\n    + labs(\n        x=\"\", y=\"Revenue Growth Rate\", linetype=\"\",\n        title=\"Historical and Forecast Revenue Growth for FPT\"\n    )\n)\n\ngrowth_figure.show()\n\n\n\n\n\n\n\nFigure 24.3: Revenue growth rates: historical (realized) and forecast (GDP-linked with company premium). The forecast assumes FPT grows at a premium to Vietnam’s GDP growth.\n\n\n\n\n\nFigure 24.4 presents the resulting FCF projections alongside historical values.\n\n# Combine historical and forecast FCF\nfcf_historical = (historical_data\n    .loc[:, [\"year\", \"fcf\"]]\n    .assign(type=\"Historical\")\n)\n\nfcf_forecast = (forecast_data\n    .loc[:, [\"year\", \"fcf\"]]\n    .assign(type=\"Forecast\")\n)\n\nfcf_combined = pd.concat([fcf_historical, fcf_forecast], ignore_index=True)\nfcf_combined[\"type\"] = pd.Categorical(\n    fcf_combined[\"type\"],\n    categories=[\"Historical\", \"Forecast\"]\n)\n\nfcf_figure = (\n    ggplot(fcf_combined, aes(x=\"year\", y=\"fcf/1e12\", fill=\"type\"))\n    + geom_col()\n    + labs(\n        x=\"\", y=\"Free Cash Flow (Trillion VND)\", fill=\"\",\n        title=\"Historical and Forecast Free Cash Flow for FPT\"\n    )\n)\n\nfcf_figure.show()\n\n\n\n\n\n\n\nFigure 24.4: Free Cash Flow: historical (realized) and forecast (projected). The forecast reflects our assumptions about revenue growth and operating ratios.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#terminal-value-capturing-long-term-value",
    "href": "31_discounted_cash_flow.html#terminal-value-capturing-long-term-value",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.7 Terminal Value: Capturing Long-Term Value",
    "text": "24.7 Terminal Value: Capturing Long-Term Value\nA critical component of DCF analysis is the terminal value (or continuation value), which represents the company’s value beyond the explicit forecast period. In most valuations, terminal value constitutes 50-80% of total enterprise value, making its estimation particularly important.\n\n24.7.1 The Perpetuity Growth Model\nThe most common approach is the Perpetuity Growth Model (also called the Gordon Growth Model), which assumes FCF grows at a constant rate forever:\n\\[\nTV_T = \\frac{FCF_{T+1}}{r - g} = \\frac{FCF_T \\times (1 + g)}{r - g}\n\\]\nwhere:\n\n\\(TV_T\\): Terminal value at the end of year \\(T\\)\n\\(FCF_T\\): Free cash flow in the final forecast year\n\\(g\\): Perpetual growth rate\n\\(r\\): Discount rate (WACC)\n\n\n\n24.7.2 Choosing the Perpetual Growth Rate\nThe perpetual growth rate \\(g\\) should reflect long-term sustainable growth. Key considerations:\n\nNo company can grow faster than the economy forever. If it did, the company would eventually become larger than GDP, which is an impossibility. Long-term GDP growth (nominal, including inflation) provides an upper bound.\nMature companies typically grow at or below GDP growth. The perpetual growth rate should reflect the company in its “steady state,” not its current high-growth phase.\nFor Vietnam, long-term nominal GDP growth might be 6-8% given current development stage, but this will moderate over time. A perpetual growth rate of 3-5% is often reasonable.\n\n\ndef compute_terminal_value(last_fcf, growth_rate, discount_rate):\n    \"\"\"\n    Compute terminal value using the perpetuity growth model.\n    \n    Parameters:\n    -----------\n    last_fcf : float\n        Free cash flow in the final forecast year\n    growth_rate : float\n        Perpetual growth rate (g)\n    discount_rate : float\n        Discount rate / WACC (r)\n        \n    Returns:\n    --------\n    float : Terminal value\n    \"\"\"\n    if discount_rate &lt;= growth_rate:\n        raise ValueError(\"Discount rate must exceed growth rate for finite terminal value\")\n    \n    return last_fcf * (1 + growth_rate) / (discount_rate - growth_rate)\n\n\n# Example calculation\nlast_fcf = forecast_data[\"fcf\"].iloc[-1]\nperpetual_growth = 0.04  # 4% perpetual growth\ndiscount_rate = 0.10     # 10% WACC (placeholder)\n\nterminal_value = compute_terminal_value(last_fcf, perpetual_growth, discount_rate)\n\nprint(f\"Last forecast FCF: {last_fcf/1e12:.2f} trillion VND\")\nprint(f\"Terminal value (at {perpetual_growth:.0%} growth, {discount_rate:.0%} WACC): {terminal_value/1e12:.1f} trillion VND\")\n\nLast forecast FCF: 8.43 trillion VND\nTerminal value (at 4% growth, 10% WACC): 146.1 trillion VND\n\n\n\n\n24.7.3 Alternative: Exit Multiple Approach\nPractitioners often cross-check terminal value using the exit multiple approach, which assumes the company is sold at the end of the forecast period at a multiple of EBITDA, EBIT, or revenue comparable to similar companies today.\nFor example, if comparable companies trade at 10x EBITDA, the terminal value would be:\n\\[\nTV_T = \\text{EBITDA}_T \\times \\text{Exit Multiple}\n\\]\nThis approach is simpler but embeds the assumption that current market multiples will persist (a strong assumption that may not hold).",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#the-discount-rate-weighted-average-cost-of-capital",
    "href": "31_discounted_cash_flow.html#the-discount-rate-weighted-average-cost-of-capital",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.8 The Discount Rate: Weighted Average Cost of Capital",
    "text": "24.8 The Discount Rate: Weighted Average Cost of Capital\nThe discount rate converts future cash flows to present value. For FCF (which goes to all capital providers), we use the Weighted Average Cost of Capital (WACC):\n\\[\nWACC = \\frac{E}{E+D} \\times r_E + \\frac{D}{E+D} \\times r_D \\times (1 - \\tau)\n\\]\nwhere:\n\n\\(E\\): Market value of equity\n\\(D\\): Market value of debt\n\\(r_E\\): Cost of equity (typically estimated using CAPM)\n\\(r_D\\): Cost of debt (pre-tax)\n\\(\\tau\\): Corporate tax rate\n\nThe \\((1-\\tau)\\) term on debt reflects the tax shield. Interest payments are tax-deductible, reducing the effective cost of debt.\n\n24.8.1 Estimating WACC Components\nCost of Equity is typically estimated using the Capital Asset Pricing Model. See the CAPM chapter:\n\\[\nr_E = r_f + \\beta \\times (r_m - r_f)\n\\]\nwhere \\(r_f\\) is the risk-free rate, \\(\\beta\\) measures systematic risk, and \\((r_m - r_f)\\) is the market risk premium.\nCost of Debt can be estimated from:\n\nInterest expense divided by total debt (effective rate)\nYields on the company’s traded bonds\nYields on bonds with similar credit ratings\n\nCapital Structure Weights should use market values when available. For equity, market capitalization is straightforward. For debt, book value is often used when market values aren’t observable.\n\n\n24.8.2 Using Industry WACC Data\nProfessor Aswath Damodaran at NYU Stern maintains comprehensive industry WACC and country risk premium data. We use these datasets to estimate appropriate discount rates for Vietnamese companies.\n\n24.8.2.1 Downloading the Data\nThe following code downloads the required datasets. Run this manually when you need to update the data (typically annually), but it is not executed during book rendering to avoid dependency on external servers.\n\nimport requests\nfrom pathlib import Path\n\n# Create data directory if needed\ndata_dir = Path(\"data\")\ndata_dir.mkdir(exist_ok=True)\n\n# Download WACC data\nwacc_url = \"https://pages.stern.nyu.edu/~adamodar/pc/datasets/wacc.xls\"\nresponse = requests.get(wacc_url, timeout=30)\nresponse.raise_for_status()\n(data_dir / \"damodaran_wacc.xls\").write_bytes(response.content)\nprint(\"Downloaded: damodaran_wacc.xls\")\n\n# Download country risk premium data\ncrp_url = \"https://pages.stern.nyu.edu/~adamodar/pc/datasets/ctryprem.xlsx\"\nresponse = requests.get(crp_url, timeout=30)\nresponse.raise_for_status()\n(data_dir / \"damodaran_crp.xlsx\").write_bytes(response.content)\nprint(\"Downloaded: damodaran_crp.xlsx\")\n\n\n\n24.8.2.2 Industry WACC\nWe extract the cost of capital for the Computer Services industry, which most closely matches FPT’s business profile:\n\nimport pandas as pd\n\n# Read local WACC data\nwacc_data = pd.read_excel(\n    \"data/damodaran_wacc.xls\", \n    sheet_name=1, \n    skiprows=18\n)\n\n# Find WACC for Computer Services\nindustry_wacc = wacc_data.loc[\n    wacc_data[\"Industry Name\"] == \"Computer Services\",\n    \"Cost of Capital\"\n].values[0]\n\nprint(f\"Industry WACC (Computer Services): {industry_wacc:.2%}\")\n\nIndustry WACC (Computer Services): 7.83%\n\n\n\n\n24.8.2.3 Country Risk Premium\nFor Vietnamese companies, we must adjust for country-specific risk. Damodaran calculates country risk premiums based on sovereign credit ratings and relative equity market volatility:\n\n# Read local country risk premium data\ncrp_data = pd.read_excel(\n    \"data/damodaran_crp.xlsx\",\n    sheet_name=\"ERPs by country\",\n    skiprows=7\n)\n\n# Find Vietnam's country risk premium\nvietnam_row = crp_data[\n    crp_data.iloc[:, 0].str.contains(\"Vietnam\", case=False, na=False)\n]\n\ncountry_risk_premium = vietnam_row.iloc[0, 8]\nprint(f\"Vietnam Country Risk Premium: {country_risk_premium:.2%}\")\n\nVietnam Country Risk Premium: 2.09%\n\n\n\n\n24.8.2.4 Adjusted WACC for Vietnam\nCombining the industry benchmark with the country risk premium gives us an appropriate discount rate:\n\nwacc_vietnam = industry_wacc + country_risk_premium\n\nprint(f\"Industry WACC (US):        {industry_wacc:.2%}\")\nprint(f\"Country Risk Premium:      {country_risk_premium:.2%}\")\nprint(f\"Adjusted WACC (Vietnam):   {wacc_vietnam:.2%}\")\n\nwacc = wacc_vietnam\n\nIndustry WACC (US):        7.83%\nCountry Risk Premium:      2.09%\nAdjusted WACC (Vietnam):   9.92%\n\n\nNote: This approach assumes Vietnamese companies in the same industry face similar operating risks as their US counterparts, with the country risk premium capturing additional macroeconomic and political risks. For company-specific analysis, further adjustments for leverage differences and firm-specific risk factors may be warranted.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#computing-enterprise-value",
    "href": "31_discounted_cash_flow.html#computing-enterprise-value",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.9 Computing Enterprise Value",
    "text": "24.9 Computing Enterprise Value\nWith all components in place, we can now compute enterprise value. The DCF formula is:\n\\[\n\\text{Enterprise Value} = \\sum_{t=1}^{T} \\frac{FCF_t}{(1 + WACC)^t} + \\frac{TV_T}{(1 + WACC)^T}\n\\]\nThe first term is the present value of forecast-period cash flows; the second is the present value of terminal value.\n\ndef compute_dcf_value(fcf_series, wacc, perpetual_growth):\n    \"\"\"\n    Compute enterprise value using DCF analysis.\n    \n    Parameters:\n    -----------\n    fcf_series : array-like\n        Free cash flows for forecast period\n    wacc : float\n        Weighted average cost of capital\n    perpetual_growth : float\n        Perpetual growth rate for terminal value\n        \n    Returns:\n    --------\n    dict : Components of DCF valuation\n    \"\"\"\n    fcf = np.array(fcf_series)\n    n_years = len(fcf)\n    \n    # Discount factors\n    discount_factors = (1 + wacc) ** np.arange(1, n_years + 1)\n    \n    # Present value of forecast period cash flows\n    pv_fcf = fcf / discount_factors\n    pv_fcf_total = pv_fcf.sum()\n    \n    # Terminal value and its present value\n    terminal_value = compute_terminal_value(fcf[-1], perpetual_growth, wacc)\n    pv_terminal = terminal_value / discount_factors[-1]\n    \n    # Total enterprise value\n    enterprise_value = pv_fcf_total + pv_terminal\n    \n    return {\n        \"pv_fcf\": pv_fcf_total,\n        \"terminal_value\": terminal_value,\n        \"pv_terminal\": pv_terminal,\n        \"enterprise_value\": enterprise_value,\n        \"terminal_pct\": pv_terminal / enterprise_value\n    }\n\n\n# Compute DCF value\nperpetual_growth = 0.04  # 4% perpetual growth\n\ndcf_result = compute_dcf_value(\n    fcf_series=forecast_data[\"fcf\"].values,\n    wacc=wacc,\n    perpetual_growth=perpetual_growth\n)\n\nprint(\"DCF Valuation Results\")\nprint(\"=\" * 50)\nprint(f\"PV of Forecast Period FCF: {dcf_result['pv_fcf']/1e12:.1f} trillion VND\")\nprint(f\"Terminal Value: {dcf_result['terminal_value']/1e12:.1f} trillion VND\")\nprint(f\"PV of Terminal Value: {dcf_result['pv_terminal']/1e12:.1f} trillion VND\")\nprint(f\"Enterprise Value: {dcf_result['enterprise_value']/1e12:.1f} trillion VND\")\nprint(f\"Terminal Value as % of EV: {dcf_result['terminal_pct']:.1%}\")\n\nDCF Valuation Results\n==================================================\nPV of Forecast Period FCF: 22.7 trillion VND\nTerminal Value: 148.1 trillion VND\nPV of Terminal Value: 92.3 trillion VND\nEnterprise Value: 115.1 trillion VND\nTerminal Value as % of EV: 80.2%\n\n\nNote that terminal value often represents 60-80% of enterprise value. This highlights the importance of terminal value assumptions and the inherent uncertainty in DCF analysis.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#sensitivity-analysis",
    "href": "31_discounted_cash_flow.html#sensitivity-analysis",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.10 Sensitivity Analysis",
    "text": "24.10 Sensitivity Analysis\nGiven the uncertainty in DCF inputs, sensitivity analysis is essential. We examine how enterprise value changes with different assumptions about WACC and perpetual growth.\n\n# Define ranges for sensitivity analysis\nwacc_range = np.arange(0.08, 0.14, 0.01)  # 8% to 13%\ngrowth_range = np.arange(0.02, 0.06, 0.01)  # 2% to 5%\n\n# Create all combinations\nsensitivity_results = []\n\nfor w in wacc_range:\n    for g in growth_range:\n        if w &gt; g:  # Must have WACC &gt; growth for valid terminal value\n            result = compute_dcf_value(\n                fcf_series=forecast_data[\"fcf\"].values,\n                wacc=w,\n                perpetual_growth=g\n            )\n            sensitivity_results.append({\n                \"wacc\": w,\n                \"growth_rate\": g,\n                \"enterprise_value\": result[\"enterprise_value\"] / 1e12  # In trillions\n            })\n\nsensitivity_df = pd.DataFrame(sensitivity_results)\n\n# Create heatmap\nsensitivity_figure = (\n    ggplot(sensitivity_df, aes(x=\"wacc\", y=\"growth_rate\", fill=\"enterprise_value\"))\n    + geom_tile()\n    + geom_text(\n        aes(label=\"enterprise_value\"),\n        format_string=\"{:.0f}\",\n        color=\"white\",\n        size=9\n    )\n    + scale_x_continuous(labels=percent_format())\n    + scale_y_continuous(labels=percent_format())\n    + scale_fill_gradient(low=\"darkblue\", high=\"lightblue\")\n    + labs(\n        x=\"WACC\", y=\"Perpetual Growth Rate\",\n        fill=\"EV\\n(Trillion VND)\",\n        title=\"DCF Sensitivity: Enterprise Value by WACC and Growth Rate\"\n    )\n)\n\nsensitivity_figure.show()\n\n\n\n\n\n\n\nFigure 24.5: Sensitivity of enterprise value to WACC and perpetual growth rate assumptions. Small changes in these inputs can substantially affect valuation.\n\n\n\n\n\nThe sensitivity analysis reveals several important insights:\n\nValuation is highly sensitive to inputs: Small changes in WACC or growth rate produce large changes in enterprise value. A 1 percentage point change in WACC can shift value by 20% or more.\nThe relationship is non-linear: The impact of growth rate changes is amplified at lower WACCs because the terminal value formula has \\((r-g)\\) in the denominator.\nReasonable people can disagree: Given input uncertainty, DCF should be thought of as producing a range of values, not a single precise number.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#from-enterprise-value-to-equity-value",
    "href": "31_discounted_cash_flow.html#from-enterprise-value-to-equity-value",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.11 From Enterprise Value to Equity Value",
    "text": "24.11 From Enterprise Value to Equity Value\nOur DCF analysis yields enterprise value (i.e., the total value of the company’s operations to all capital providers). To determine equity value (what shareholders own), we must adjust for the claims of debt holders and any non-operating assets:\n\\[\n\\text{Equity Value} = \\text{Enterprise Value} + \\text{Non-Operating Assets} - \\text{Debt}\n\\]\nNon-Operating Assets include:\n\nExcess cash beyond operating needs\nMarketable securities\nNon-core real estate or investments\n\nDebt includes:\n\nShort-term debt\nLong-term debt\nCapital lease obligations\nPreferred stock (if treated as debt-like)\n\n\n# Get most recent balance sheet data for FPT\nlatest_year = fpt_data[\"year\"].max()\nlatest_data = fpt_data[fpt_data[\"year\"] == latest_year].iloc[0]\n\n# Extract debt and cash (column names may vary)\ntotal_debt = latest_data.get(\"total_debt\", 0)\ncash = latest_data.get(\"ca_cce\", 0)\n\n# Compute equity value\nenterprise_value = dcf_result[\"enterprise_value\"]\nequity_value = enterprise_value - total_debt + cash\n\nprint(\"From Enterprise Value to Equity Value\")\nprint(\"=\" * 50)\nprint(f\"Enterprise Value: {enterprise_value/1e12:.1f} trillion VND\")\nprint(f\"Less: Total Debt: {total_debt/1e12:.1f} trillion VND\")\nprint(f\"Plus: Cash: {cash/1e12:.1f} trillion VND\")\nprint(f\"Equity Value: {equity_value/1e12:.1f} trillion VND\")\n\nFrom Enterprise Value to Equity Value\n==================================================\nEnterprise Value: 115.1 trillion VND\nLess: Total Debt: 0.0 trillion VND\nPlus: Cash: 8.3 trillion VND\nEquity Value: 123.3 trillion VND\n\n\n\n24.11.1 Implied Share Price\nIf we know the number of shares outstanding, we can compute an implied share price:\n\n# Get shares outstanding (this would come from market data)\n# Using placeholder - in practice, get from exchange data\nshares_outstanding = latest_data.get(\"total_equity\", equity_value) / 25000  # Rough estimate\n\nimplied_price = equity_value / shares_outstanding\n\nprint(f\"\\nImplied Share Price: {implied_price:,.0f} VND\")\n\n\nImplied Share Price: 103,008 VND\n\n\nComparing the implied price to the current market price tells us whether the stock appears under- or overvalued according to our DCF model.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#limitations-and-practical-considerations",
    "href": "31_discounted_cash_flow.html#limitations-and-practical-considerations",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.12 Limitations and Practical Considerations",
    "text": "24.12 Limitations and Practical Considerations\nDCF analysis is powerful but has important limitations:\n\n24.12.1 Sensitivity to Assumptions\nAs our sensitivity analysis showed, small changes in inputs produce large changes in value. This is particularly problematic because the most influential inputs (long-term growth, WACC) are the hardest to estimate accurately.\n\n\n24.12.2 Terminal Value Dominance\nTerminal value often represents 60-80% of total value, yet it’s based on assumptions about the very distant future. This concentrates valuation risk in the most uncertain component.\n\n\n24.12.3 Garbage In, Garbage Out\nDCF is only as good as its inputs. Unrealistic growth assumptions, optimistic margins, or inappropriate discount rates produce meaningless valuations. The discipline of DCF lies in forcing analysts to justify their assumptions.\n\n\n24.12.4 Not Suitable for All Companies\nDCF works best for companies with:\n\nPositive and predictable cash flows\nStable or predictably changing margins\nReasonable visibility into future operations\n\nIt struggles with:\n\nEarly-stage companies with no profits\nHighly cyclical businesses\nCompanies undergoing major transitions\nFinancial institutions (which require different approaches)\n\n\n\n24.12.5 Complement with Other Methods\nWise practitioners use DCF alongside other valuation methods:\n\nComparable company analysis: How do similar companies trade?\nPrecedent transactions: What have acquirers paid for similar businesses?\nSum-of-the-parts: Value divisions separately and add\n\nWhen methods converge, confidence increases. When they diverge, it prompts investigation into why.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "31_discounted_cash_flow.html#key-takeaways",
    "href": "31_discounted_cash_flow.html#key-takeaways",
    "title": "24  Discounted Cash Flow Analysis",
    "section": "24.13 Key Takeaways",
    "text": "24.13 Key Takeaways\nThis chapter introduced Discounted Cash Flow analysis as a framework for intrinsic valuation. The main insights are:\n\nFree Cash Flow is the foundation: FCF represents cash available to all investors after operating expenses, taxes, and investments. It differs from net income by excluding non-cash items and including investment needs.\nRatio-based forecasting links components to revenue: By expressing FCF components as percentages of revenue, we can systematically forecast cash flows based on revenue growth assumptions and operating ratio projections.\nTerminal value captures long-term value: The perpetuity growth model assumes FCF grows at a constant rate forever. The perpetual growth rate should not exceed long-term economic growth.\nWACC is the appropriate discount rate: The Weighted Average Cost of Capital reflects the blended cost of debt and equity financing, adjusted for the tax shield on interest.\nDCF produces enterprise value: To derive equity value, subtract debt and add non-operating assets. Dividing by shares outstanding yields an implied share price.\nSensitivity analysis is essential: Given input uncertainty, presenting a range of values based on different assumptions is more honest than a single point estimate.\nDCF complements other methods: No single valuation method is definitive. Cross-checking DCF with market multiples and transaction comparables provides a more complete picture.\n\nThe true value of DCF analysis lies not in producing a precise number but in forcing rigorous thinking about what drives company value. The process of building a DCF model (i.e., forecasting growth, projecting margins, estimating risk) develops deep understanding of the business being valued.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Discounted Cash Flow Analysis</span>"
    ]
  },
  {
    "objectID": "32_info_earnings.html",
    "href": "32_info_earnings.html",
    "title": "25  Information Content of Earnings Announcements",
    "section": "",
    "text": "25.0.1 Two Distinct Signals: Volume vs. Volatility\nA foundational question in capital markets research is whether accounting information matters to investors. If financial statements merely repackage what the market already knows from other sources, such as press coverage, analyst reports, management guidance, macroeconomic data, then the elaborate apparatus of financial reporting is largely redundant for price discovery. Conversely, if earnings announcements trigger measurable changes in trading behavior, we have evidence that the accounting system produces information the market did not previously possess.\nBeaver (1968) addressed this question by examining whether trading volume and return volatility increase around the dates when firms announce annual earnings. The logic is direct: if an earnings announcement contains new information, it should alter investors’ beliefs. Altered beliefs lead some investors to trade, increasing volume. And to the extent that the announcement resolves uncertainty, the stock price should adjust, generating abnormal return volatility. Both channels (i.e., volume and volatility) provide distinct windows into the information content of earnings.\nThe empirical design compares a firm’s trading activity during a narrow announcement window (the days immediately around the earnings release) with its own trading activity during a non-announcement window (otherwise comparable days away from the event). If the announcement conveys information, the announcement window should show elevated volume and volatility relative to the non-announcement period.\nWhile volume and return volatility are both used to measure investor reaction, they capture different economic phenomena. This distinction, formalized in subsequent theoretical work, is important for interpreting results:\nKarpoff (1987) surveys the theoretical and empirical relationship between these two variables. The key insight is that volume is not simply a noisier version of volatility; it provides independent information about how the market processes accounting data.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Information Content of Earnings Announcements</span>"
    ]
  },
  {
    "objectID": "32_info_earnings.html#sec-info-earnings-theory",
    "href": "32_info_earnings.html#sec-info-earnings-theory",
    "title": "25  Information Content of Earnings Announcements",
    "section": "25.1 Theoretical Framework",
    "text": "25.1 Theoretical Framework\n\n25.1.1 The Information Content Hypothesis\nLet \\(P_{i,t}\\) denote the price of stock \\(i\\) on date \\(t\\). Under the efficient markets hypothesis, the price reflects all publicly available information:\n\\[\nP_{i,t} = E\\left[\\sum_{s=1}^{\\infty} \\frac{D_{i,t+s}}{(1+r)^s} \\;\\bigg|\\; \\Omega_t \\right]\n\\tag{25.1}\\]\nwhere \\(D_{i,t+s}\\) represents future dividends, \\(r\\) is the discount rate, and \\(\\Omega_t\\) is the information set at time \\(t\\).\nWhen firm \\(i\\) announces earnings \\(X_i\\) at time \\(\\tau\\), the information set expands: \\(\\Omega_{\\tau^+} = \\Omega_{\\tau^-} \\cup \\{X_i\\}\\). If the announcement contains new information, meaning \\(X_i \\notin \\Omega_{\\tau^-}\\), then in general:\n\\[\nP_{i,\\tau^+} \\neq P_{i,\\tau^-}\n\\tag{25.2}\\]\nThe magnitude of the price change reflects the surprise component of the announcement:\n\\[\n|P_{i,\\tau^+} - P_{i,\\tau^-}| \\propto |X_i - E[X_i | \\Omega_{\\tau^-}]|\n\\tag{25.3}\\]\n\n\n25.1.2 Volume as a Signal of Heterogeneous Beliefs\nKim and Verrecchia (1991) develop a model where trading volume is driven by the precision of investors’ private information relative to the precision of the public signal. Define \\(V_{i,\\tau}\\) as the trading volume for stock \\(i\\) at time \\(\\tau\\). In their framework:\n\\[\nV_{i,\\tau} = f\\left(\\frac{\\text{Var}[X_i | \\Omega^{(j)}_{\\tau^-}]}{\\text{Var}[\\varepsilon_i]}\\right)\n\\tag{25.4}\\]\nwhere \\(\\Omega^{(j)}_{\\tau^-}\\) is investor \\(j\\)’s private information set and \\(\\varepsilon_i\\) is noise. Volume increases when investors disagree about what the announcement means, a condition that Kandel and Pearson (1995) term differential interpretation.\nThis distinction matters for Vietnam. In a market dominated by retail investors with heterogeneous information and analytical capacity, we might expect the volume reaction to be disproportionately large relative to the price reaction, because disagreement is high even when the aggregate price revision is modest.\n\n\n25.1.3 Measuring Information Content\nWe use two complementary measures:\nAbnormal return volatility. For each firm-day, compute the squared market-adjusted return:\n\\[\nU^2_{i,t} = (R_{i,t} - R_{m,t})^2\n\\tag{25.5}\\]\nwhere \\(R_{i,t}\\) is the return on stock \\(i\\) and \\(R_{m,t}\\) is the market return. Under the null of no information content, \\(E[U^2_{i,t}]\\) should be the same in the announcement and non-announcement windows. Beaver (1968) standardized this measure by dividing by the firm’s average \\(U^2\\) during non-announcement weeks.\nRelative volume. For each firm-day, compute:\n\\[\nRV_{i,t} = \\frac{Vol_{i,t}}{\\overline{Vol}_i}\n\\tag{25.6}\\]\nwhere \\(\\overline{Vol}_i\\) is the firm’s average daily volume over the event window. A value of \\(RV &gt; 1\\) indicates above-average trading. Beaver (1968) used a similar approach, computing a “volume ratio” relative to non-report-period averages.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Information Content of Earnings Announcements</span>"
    ]
  },
  {
    "objectID": "32_info_earnings.html#sec-info-earnings-measures",
    "href": "32_info_earnings.html#sec-info-earnings-measures",
    "title": "25  Information Content of Earnings Announcements",
    "section": "25.2 Data and Variable Construction",
    "text": "25.2 Data and Variable Construction\n\n25.2.1 Data Requirements\nThe analysis requires three data elements:\n\n\n\nTable 25.1: Data sources for the information content analysis\n\n\n\n\n\n\n\n\n\n\nData\nSource\nKey Fields\n\n\n\n\nEarnings announcement dates\nFinancial statements/filings\nTicker, fiscal year-end, announcement date\n\n\nDaily stock returns\nExchange data (HOSE/HNX)\nTicker, date, closing price, return\n\n\nDaily trading volume\nExchange data (HOSE/HNX)\nTicker, date, volume (shares or VND)\n\n\n\n\n\n\n\n\n25.2.2 Trading Day Calendar\nA critical infrastructure element is a trading day calendar that maps calendar dates to sequential trading day indices. This allows us to count “20 trading days before the announcement” correctly, skipping weekends and holidays.\n\ndef build_trading_calendar(\n    start_date: str = \"2008-01-01\",\n    end_date: str = \"2024-12-31\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Build a trading day calendar for Vietnamese stock exchanges.\n    \n    Vietnamese exchanges trade Monday–Friday, with closures for:\n    - Tết Nguyên Đán (Lunar New Year, ~5 days)\n    - Ngày Giỗ Tổ Hùng Vương (Hung Kings' Commemoration)\n    - Ngày Giải phóng miền Nam (Reunification Day, Apr 30)\n    - Ngày Quốc tế Lao động (Labour Day, May 1)\n    - Quốc khánh (National Day, Sep 2)\n    - Tết Dương lịch (New Year, Jan 1)\n    \n    For simulation, we approximate by removing weekends and ~10 holidays/year.\n    \"\"\"\n    all_dates = pd.date_range(start_date, end_date, freq=\"B\")  # business days\n    \n    # Remove approximate Vietnamese public holidays\n    # (In practice, use the exact holiday calendar from SSC/HOSE)\n    rng = np.random.default_rng(42)\n    n_holidays_per_year = 10\n    years = all_dates.year.unique()\n    holidays = []\n    for yr in years:\n        yr_dates = all_dates[all_dates.year == yr]\n        if len(yr_dates) &gt; n_holidays_per_year:\n            idx = rng.choice(len(yr_dates), n_holidays_per_year, replace=False)\n            holidays.extend(yr_dates[idx])\n    \n    trading_dates = all_dates.difference(pd.DatetimeIndex(holidays))\n    \n    cal = pd.DataFrame({\n        \"date\": trading_dates,\n        \"td\": np.arange(1, len(trading_dates) + 1),\n    })\n    return cal\n\ntrading_cal = build_trading_calendar()\nprint(f\"Trading calendar: {len(trading_cal):,} trading days, \"\n      f\"{trading_cal['date'].min().date()} to {trading_cal['date'].max().date()}\")\n\nTrading calendar: 4,266 trading days, 2008-01-01 to 2024-12-31\n\n\n\n\n25.2.3 Announcement Date Alignment\nEarnings announcements may occur on non-trading days (weekends, holidays). When this happens, we assign the announcement to the next available trading day, since that is the first opportunity for the market to react. We also need to handle the case where announcements occur after market close. Ideally, these would be assigned to the following trading day. Without intraday timing data (common in Vietnamese disclosures), we use the announcement date as reported.\n\ndef build_announcement_mapper(trading_cal: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a mapping from every calendar date to the next available\n    trading day index. This handles announcements on weekends/holidays.\n    \"\"\"\n    min_date = trading_cal[\"date\"].min()\n    max_date = trading_cal[\"date\"].max()\n    all_dates = pd.date_range(min_date, max_date, freq=\"D\")\n    \n    mapper = pd.DataFrame({\"annc_date\": all_dates})\n    mapper = mapper.merge(\n        trading_cal.rename(columns={\"date\": \"annc_date\"}),\n        on=\"annc_date\",\n        how=\"left\",\n    )\n    # Forward-fill: assign non-trading dates to next trading day\n    mapper[\"td\"] = mapper[\"td\"].bfill()\n    mapper[\"td\"] = mapper[\"td\"].astype(\"Int64\")\n    return mapper\n\nannc_mapper = build_announcement_mapper(trading_cal)\n\n\n\n25.2.4 Generating Synthetic Vietnamese Data\nWe generate a synthetic panel of Vietnamese listed firms with realistic properties to demonstrate the methodology. The simulation embeds an earnings announcement effect—elevated volume and volatility on days 0 and +1, that we then seek to detect using the Beaver (1968) framework.\n\ndef generate_beaver_data(\n    n_firms: int = 300,\n    years: list[int] = None,\n    seed: int = 2024,\n    days_window: int = 20,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Generate:\n    1. earn_annc: DataFrame of earnings announcement dates\n    2. daily_data: DataFrame of daily returns and volume\n    \n    The DGP embeds:\n    - An announcement-day return volatility spike (2-3x normal)\n    - An announcement-day volume spike (1.5-2.5x normal)\n    - Price limit truncation at ±7% (HOSE)\n    - Cross-sectional variation by firm size and exchange\n    \"\"\"\n    if years is None:\n        years = list(range(2012, 2024))\n    \n    rng = np.random.default_rng(seed)\n    tc = build_trading_calendar()\n    am = build_announcement_mapper(tc)\n    \n    exchanges = [\"HOSE\", \"HNX\"]\n    price_limits = {\"HOSE\": 0.07, \"HNX\": 0.10}\n    \n    # ── Generate announcement dates ──\n    annc_records = []\n    for i in range(n_firms):\n        ticker = f\"VN{i:04d}\"\n        exchange = rng.choice(exchanges, p=[0.6, 0.4])\n        # Firm size (log market cap in VND billions)\n        log_mcap = rng.normal(6.5, 1.5)\n        \n        for yr in years:\n            # Fiscal year ends Dec 31; announcement 60-90 days later\n            fy_end = pd.Timestamp(f\"{yr}-12-31\")\n            annc_lag = rng.integers(45, 90)\n            annc_date = fy_end + pd.Timedelta(days=int(annc_lag))\n            \n            annc_records.append({\n                \"ticker\": ticker,\n                \"fiscal_year\": yr,\n                \"datadate\": fy_end,\n                \"annc_date\": annc_date,\n                \"exchange\": exchange,\n                \"log_mcap\": log_mcap,\n            })\n    \n    earn_annc = pd.DataFrame(annc_records)\n    \n    # Map to trading day index\n    earn_annc[\"annc_date_dt\"] = pd.to_datetime(earn_annc[\"annc_date\"])\n    earn_annc = earn_annc.merge(\n        am[[\"annc_date\", \"td\"]].rename(columns={\"td\": \"event_td\"}),\n        left_on=\"annc_date_dt\",\n        right_on=\"annc_date\",\n        how=\"left\",\n        suffixes=(\"\", \"_map\"),\n    )\n    earn_annc = earn_annc.dropna(subset=[\"event_td\"])\n    earn_annc[\"event_td\"] = earn_annc[\"event_td\"].astype(int)\n    earn_annc[\"start_td\"] = earn_annc[\"event_td\"] - days_window\n    earn_annc[\"end_td\"] = earn_annc[\"event_td\"] + days_window\n    \n    # Map start/end td back to dates\n    td_to_date = tc.set_index(\"td\")[\"date\"]\n    earn_annc[\"start_date\"] = earn_annc[\"start_td\"].map(td_to_date)\n    earn_annc[\"end_date\"] = earn_annc[\"end_td\"].map(td_to_date)\n    earn_annc = earn_annc.dropna(subset=[\"start_date\", \"end_date\"])\n    \n    # ── Generate daily return and volume data ──\n    daily_records = []\n    for _, row in earn_annc.iterrows():\n        ticker = row[\"ticker\"]\n        exchange = row[\"exchange\"]\n        plimit = price_limits[exchange]\n        log_mcap = row[\"log_mcap\"]\n        event_td = row[\"event_td\"]\n        \n        # Base volatility inversely related to size\n        base_vol = 0.015 + 0.01 * np.exp(-0.3 * log_mcap)\n        # Base volume (shares/day)\n        base_volume = np.exp(log_mcap + rng.normal(0, 0.5)) * 1000\n        \n        # Window trading days\n        window_tds = tc[\n            (tc[\"td\"] &gt;= row[\"start_td\"]) & (tc[\"td\"] &lt;= row[\"end_td\"])\n        ].copy()\n        \n        for _, td_row in window_tds.iterrows():\n            date = td_row[\"date\"]\n            td = td_row[\"td\"]\n            relative_td = td - event_td\n            \n            # Market return (common factor)\n            mkt_ret = rng.normal(0.0003, 0.012)\n            \n            # Firm-specific return\n            # Announcement effect: elevated volatility on days 0, +1\n            if relative_td in [0, 1]:\n                vol_mult = rng.uniform(2.0, 3.5)\n            elif abs(relative_td) &lt;= 2:\n                vol_mult = rng.uniform(1.2, 1.8)\n            else:\n                vol_mult = 1.0\n            \n            firm_shock = rng.normal(0, base_vol * vol_mult)\n            raw_ret = mkt_ret + firm_shock\n            \n            # Apply price limits\n            ret = np.clip(raw_ret, -plimit, plimit)\n            ret_mkt = ret - mkt_ret\n            \n            # Volume: spike on announcement days\n            if relative_td in [0, 1]:\n                vol_spike = rng.uniform(1.5, 2.8)\n            elif abs(relative_td) &lt;= 3:\n                vol_spike = rng.uniform(1.0, 1.5)\n            else:\n                vol_spike = rng.uniform(0.7, 1.3)\n            \n            volume = base_volume * vol_spike * np.exp(rng.normal(0, 0.3))\n            \n            daily_records.append({\n                \"ticker\": ticker,\n                \"fiscal_year\": row[\"fiscal_year\"],\n                \"date\": date,\n                \"td\": td,\n                \"relative_td\": relative_td,\n                \"ret\": ret,\n                \"mkt_ret\": mkt_ret,\n                \"ret_mkt\": ret_mkt,\n                \"vol\": max(volume, 100),\n                \"exchange\": exchange,\n                \"log_mcap\": log_mcap,\n            })\n    \n    daily_data = pd.DataFrame(daily_records)\n    \n    return earn_annc, daily_data\n\nearn_annc, daily_data = generate_beaver_data(n_firms=300, seed=2024)\nprint(f\"Announcements: {len(earn_annc):,}\")\nprint(f\"Daily observations: {len(daily_data):,}\")\n\nAnnouncements: 3,600\nDaily observations: 147,600",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Information Content of Earnings Announcements</span>"
    ]
  },
  {
    "objectID": "32_info_earnings.html#sec-info-earnings-single",
    "href": "32_info_earnings.html#sec-info-earnings-single",
    "title": "25  Information Content of Earnings Announcements",
    "section": "25.3 Single-Event Walkthrough",
    "text": "25.3 Single-Event Walkthrough\nBefore scaling to the full sample, we trace the methodology for a single earnings announcement to build intuition and verify each step.\n\n25.3.1 Selecting the Event\n\nsingle_ticker = \"VN0042\"\nsingle_fy = 2019\n\nsingle_event = earn_annc[\n    (earn_annc[\"ticker\"] == single_ticker) &\n    (earn_annc[\"fiscal_year\"] == single_fy)\n].iloc[0]\n\nprint(f\"Firm: {single_event['ticker']}\")\nprint(f\"Fiscal year-end: {single_event['datadate'].date()}\")\nprint(f\"Announcement date: {single_event['annc_date'].date()}\")\nprint(f\"Exchange: {single_event['exchange']}\")\nprint(f\"Event trading day: {single_event['event_td']}\")\n\nFirm: VN0042\nFiscal year-end: 2019-12-31\nAnnouncement date: 2020-03-28\nExchange: HOSE\nEvent trading day: 3073\n\n\n\n\n25.3.2 Event Window Data\nWe extract 41 trading days of data (20 before through 20 after the announcement) and compute the key measures.\n\nsingle_daily = daily_data[\n    (daily_data[\"ticker\"] == single_ticker) &\n    (daily_data[\"fiscal_year\"] == single_fy)\n].copy()\n\n# Compute measures\nsingle_daily[\"sq_ret_mkt\"] = single_daily[\"ret_mkt\"] ** 2\nsingle_daily[\"abs_ret_mkt\"] = single_daily[\"ret_mkt\"].abs()\n\n# Relative volume: normalize by own average\navg_vol = single_daily[\"vol\"].mean()\nsingle_daily[\"rel_vol\"] = single_daily[\"vol\"] / avg_vol\n\nprint(f\"Trading days in window: {len(single_daily)}\")\nprint(f\"Average daily volume: {avg_vol:,.0f} shares\")\n\nTrading days in window: 41\nAverage daily volume: 3,780,757 shares\n\n\n\n\n25.3.3 Visualizing the Single Event\nFigure 25.1 and Figure 25.2 display the volume and volatility patterns for this single earnings announcement.\n\nfig, ax = plt.subplots()\ncolors = [\"firebrick\" if abs(t) &lt;= 1 else \"steelblue\"\n          for t in single_daily[\"relative_td\"]]\nax.bar(single_daily[\"relative_td\"], single_daily[\"vol\"],\n       color=colors, edgecolor=\"white\", width=0.8)\nax.axhline(avg_vol, color=\"grey\", linestyle=\"--\", linewidth=1,\n           label=\"Window average\")\nax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\nax.set_xlabel(\"Trading Days Relative to Announcement\")\nax.set_ylabel(\"Volume (shares)\")\nax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f\"{x/1e6:.1f}M\"))\nax.legend(fontsize=9)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 25.1: Daily trading volume around a single earnings announcement. Day 0 is the first trading day on or after the announcement date. The dashed line marks the event-window average.\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.bar(single_daily[\"relative_td\"], single_daily[\"sq_ret_mkt\"],\n       color=[\"firebrick\" if abs(t) &lt;= 1 else \"steelblue\"\n              for t in single_daily[\"relative_td\"]],\n       edgecolor=\"white\", width=0.8)\nax.axhline(single_daily[\"sq_ret_mkt\"].mean(), color=\"grey\",\n           linestyle=\"--\", linewidth=1, label=\"Window average\")\nax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\nax.set_xlabel(\"Trading Days Relative to Announcement\")\nax.set_ylabel(\"Squared Market-Adjusted Return\")\nax.legend(fontsize=9)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 25.2: Squared market-adjusted returns around a single earnings announcement. Elevated values on days 0 and +1 indicate that the announcement moved the stock price beyond what market-wide movements would predict.\n\n\n\n\n\n\n\n\n\n\n\nTipReading the Plots\n\n\n\nA single event is inherently noisy, other firm-specific news (M&A rumors, analyst reports, regulatory actions) can generate volume and volatility spikes on non-announcement dates. The value of the Beaver (1968) approach lies in averaging across many events to isolate the systematic announcement effect from idiosyncratic noise.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Information Content of Earnings Announcements</span>"
    ]
  },
  {
    "objectID": "32_info_earnings.html#sec-info-earnings-full-sample",
    "href": "32_info_earnings.html#sec-info-earnings-full-sample",
    "title": "25  Information Content of Earnings Announcements",
    "section": "25.4 Full-Sample Analysis",
    "text": "25.4 Full-Sample Analysis\n\n25.4.1 Computing Event-Time Aggregates\nFor each event day \\(\\tau\\) relative to the announcement, we compute the cross-sectional average of our two measures across all announcements:\n\\[\n\\overline{U^2}_\\tau = \\frac{1}{N_\\tau} \\sum_{i=1}^{N_\\tau} U^2_{i,\\tau}\n\\tag{25.7}\\]\n\\[\n\\overline{RV}_\\tau = \\frac{1}{N_\\tau} \\sum_{i=1}^{N_\\tau} RV_{i,\\tau}\n\\tag{25.8}\\]\nwhere \\(N_\\tau\\) is the number of announcements with available data on event day \\(\\tau\\).\n\n# Squared and absolute market-adjusted returns\ndaily_data[\"sq_ret_mkt\"] = daily_data[\"ret_mkt\"] ** 2\ndaily_data[\"abs_ret_mkt\"] = daily_data[\"ret_mkt\"].abs()\n\n# Relative volume: normalize within each event window\ndaily_data[\"rel_vol\"] = (\n    daily_data\n    .groupby([\"ticker\", \"fiscal_year\"])[\"vol\"]\n    .transform(lambda x: x / x.mean())\n)\n\n# Standardized squared return: normalize within each event window\ndaily_data[\"std_sq_ret\"] = (\n    daily_data\n    .groupby([\"ticker\", \"fiscal_year\"])[\"sq_ret_mkt\"]\n    .transform(lambda x: x / x.mean())\n)\n\n# Add fiscal year for panel breakdowns\ndaily_data[\"year\"] = daily_data[\"fiscal_year\"]\n\n\nevent_summary = (\n    daily_data\n    .groupby([\"relative_td\", \"year\"])\n    .agg(\n        n_obs=(\"ret\", \"size\"),\n        mean_ret=(\"ret\", \"mean\"),\n        mean_ret_mkt=(\"ret_mkt\", \"mean\"),\n        sd_ret_mkt=(\"ret_mkt\", \"std\"),\n        mean_sq_ret=(\"sq_ret_mkt\", \"mean\"),\n        mean_abs_ret=(\"abs_ret_mkt\", \"mean\"),\n        mean_rel_vol=(\"rel_vol\", \"mean\"),\n        mean_std_sq_ret=(\"std_sq_ret\", \"mean\"),\n        median_rel_vol=(\"rel_vol\", \"median\"),\n    )\n    .reset_index()\n)\n\n# Also compute pooled (all-year) summary\nevent_summary_pooled = (\n    daily_data\n    .groupby(\"relative_td\")\n    .agg(\n        n_obs=(\"ret\", \"size\"),\n        mean_ret=(\"ret\", \"mean\"),\n        mean_ret_mkt=(\"ret_mkt\", \"mean\"),\n        sd_ret_mkt=(\"ret_mkt\", \"std\"),\n        mean_sq_ret=(\"sq_ret_mkt\", \"mean\"),\n        mean_abs_ret=(\"abs_ret_mkt\", \"mean\"),\n        mean_rel_vol=(\"rel_vol\", \"mean\"),\n        mean_std_sq_ret=(\"std_sq_ret\", \"mean\"),\n        median_rel_vol=(\"rel_vol\", \"median\"),\n    )\n    .reset_index()\n)\n\n\n\n25.4.2 Return Volatility Around Announcements\nFigure 25.3 plots the standard deviation of market-adjusted returns by event day, separately for each fiscal year. A clear spike on days 0 and +1 is the hallmark of information content.\n\nfig, ax = plt.subplots(figsize=(9, 5))\nyears_plot = sorted(event_summary[\"year\"].unique())\n# Use a colormap\ncmap = plt.cm.viridis(np.linspace(0.1, 0.9, len(years_plot)))\n\nfor idx, yr in enumerate(years_plot):\n    sub = event_summary[event_summary[\"year\"] == yr].sort_values(\"relative_td\")\n    ax.plot(sub[\"relative_td\"], sub[\"sd_ret_mkt\"],\n            color=cmap[idx], alpha=0.6, linewidth=1.0, label=str(yr))\n\n# Overlay pooled mean\npooled = event_summary_pooled.sort_values(\"relative_td\")\nax.plot(pooled[\"relative_td\"], pooled[\"sd_ret_mkt\"],\n        color=\"firebrick\", linewidth=2.5, linestyle=\"-\", label=\"Pooled\",\n        zorder=10)\n\nax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\nax.set_xlabel(\"Trading Days Relative to Announcement\")\nax.set_ylabel(\"Std Dev of Market-Adjusted Returns\")\nax.legend(fontsize=7, ncol=4, loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 25.3: Standard deviation of market-adjusted returns around earnings announcements, by fiscal year. The consistent spike on days 0 and +1 across years provides strong evidence that annual earnings announcements convey new information to Vietnamese equity markets.\n\n\n\n\n\n\n\n25.4.3 Trading Volume Around Announcements\nFigure 25.4 shows relative trading volume by event day. The volume response provides an independent confirmation of the volatility finding.\n\nfig, ax = plt.subplots(figsize=(9, 5))\n\nfor idx, yr in enumerate(years_plot):\n    sub = event_summary[event_summary[\"year\"] == yr].sort_values(\"relative_td\")\n    ax.plot(sub[\"relative_td\"], sub[\"mean_rel_vol\"],\n            color=cmap[idx], alpha=0.6, linewidth=1.0, label=str(yr))\n\nax.plot(pooled[\"relative_td\"], pooled[\"mean_rel_vol\"],\n        color=\"firebrick\", linewidth=2.5, linestyle=\"-\", label=\"Pooled\",\n        zorder=10)\n\nax.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.8)\nax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\nax.set_xlabel(\"Trading Days Relative to Announcement\")\nax.set_ylabel(\"Relative Volume (1.0 = window average)\")\nax.legend(fontsize=7, ncol=4, loc=\"upper right\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 25.4: Mean relative trading volume around earnings announcements, by fiscal year. Volume spikes sharply on days 0 and +1, consistent with investors actively trading on the new information. The elevated volume persists slightly longer than the volatility spike, suggesting that disagreement about the announcement’s implications takes time to resolve.\n\n\n\n\n\n\n\n25.4.4 Formal Statistical Tests\nBeaver (1968) relied on visual inspection and informal statistical arguments. Modern practice demands formal tests. We test whether the announcement-window measures differ significantly from the non-announcement window using a paired comparison.\nDefine the announcement window as days \\(\\{-1, 0, +1\\}\\) and the non-announcement window as days \\(\\{-20, \\ldots, -5\\} \\cup \\{+5, \\ldots, +20\\}\\). For each event \\(i\\), we compute:\n\\[\n\\Delta U^2_i = \\overline{U^2}_{i,\\text{annc}} - \\overline{U^2}_{i,\\text{non-annc}}\n\\tag{25.9}\\]\nUnder the null, \\(E[\\Delta U^2_i] = 0\\). We test this with a \\(t\\)-test across events.\n\nannc_window = [-1, 0, 1]\nnon_annc_window = list(range(-20, -4)) + list(range(5, 21))\n\ndef compute_window_means(df, var, window):\n    \"\"\"Compute mean of var within a window for each event.\"\"\"\n    return (\n        df[df[\"relative_td\"].isin(window)]\n        .groupby([\"ticker\", \"fiscal_year\"])[var]\n        .mean()\n    )\n\ntest_results = {}\nfor var, label in [(\"sq_ret_mkt\", \"Squared Mkt-Adj Return\"),\n                    (\"abs_ret_mkt\", \"Absolute Mkt-Adj Return\"),\n                    (\"rel_vol\", \"Relative Volume\")]:\n    annc_mean = compute_window_means(daily_data, var, annc_window)\n    non_annc_mean = compute_window_means(daily_data, var, non_annc_window)\n    \n    # Align on common events\n    common = annc_mean.index.intersection(non_annc_mean.index)\n    diff = annc_mean.loc[common] - non_annc_mean.loc[common]\n    diff = diff.dropna()\n    \n    t_stat, p_val = stats.ttest_1samp(diff, 0)\n    \n    test_results[label] = {\n        \"Annc Window Mean\": annc_mean.mean(),\n        \"Non-Annc Window Mean\": non_annc_mean.mean(),\n        \"Difference\": diff.mean(),\n        \"t-statistic\": t_stat,\n        \"p-value\": p_val,\n        \"N events\": len(diff),\n    }\n\ntest_df = pd.DataFrame(test_results).T.round(4)\ntest_df[\"N events\"] = test_df[\"N events\"].astype(int)\ntest_df\n\n\n\nTable 25.2: Formal tests of announcement-period information content. The announcement window is days {-1, 0, +1}; the non-announcement window excludes days {-4, …, +4}. Large t-statistics and small p-values provide strong evidence that both return volatility and trading volume are significantly elevated during the announcement window.\n\n\n\n\n\n\n\n\n\n\nAnnc Window Mean\nNon-Annc Window Mean\nDifference\nt-statistic\np-value\nN events\n\n\n\n\nSquared Mkt-Adj Return\n0.0014\n0.0003\n0.0011\n64.1353\n0.0\n3600\n\n\nAbsolute Mkt-Adj Return\n0.0294\n0.0132\n0.0162\n79.7318\n0.0\n3600\n\n\nRelative Volume\n1.7020\n0.9206\n0.7814\n135.9351\n0.0\n3600\n\n\n\n\n\n\n\n\n\n\n\n\n25.4.5 Standardized Volatility Ratio\nFollowing the approach of Beaver (1968), we compute the ratio of announcement-window squared returns to non-announcement-window squared returns. Under the null, this ratio equals 1.0.\n\nfig, ax = plt.subplots()\nratio = pooled[\"mean_std_sq_ret\"]\ncolors = [\"firebrick\" if abs(t) &lt;= 1 else \"steelblue\"\n          for t in pooled[\"relative_td\"]]\nax.bar(pooled[\"relative_td\"], ratio, color=colors, edgecolor=\"white\")\nax.axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=1, label=\"Expected under null\")\nax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\nax.set_xlabel(\"Trading Days Relative to Announcement\")\nax.set_ylabel(\"Standardized Squared Return (ratio)\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 25.5: Standardized squared market-adjusted return by event day (pooled across all years). A value of 1.0 indicates normal volatility. The announcement-day ratio substantially exceeds 1.0, indicating that earnings releases generate return variation well above normal levels.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Information Content of Earnings Announcements</span>"
    ]
  },
  {
    "objectID": "32_info_earnings.html#sec-info-earnings-cross",
    "href": "32_info_earnings.html#sec-info-earnings-cross",
    "title": "25  Information Content of Earnings Announcements",
    "section": "25.5 Cross-Sectional Variation",
    "text": "25.5 Cross-Sectional Variation\nA uniform announcement effect across all firms would be surprising. Theory predicts that the magnitude of the market reaction depends on the pre-announcement information environment: firms with richer information sets prior to the announcement should exhibit smaller reactions because less of the earnings number is news (Atiase 1985; Verrecchia 2001).\n\n25.5.1 By Firm Size\nLarger firms receive more attention from analysts, media, and institutional investors, so their earnings announcements should contain less incremental information.\n\n# Assign size terciles\ndaily_data[\"size_tercile\"] = pd.qcut(\n    daily_data[\"log_mcap\"], 3, labels=[\"Small\", \"Medium\", \"Large\"]\n)\n\nsize_summary = (\n    daily_data\n    .groupby([\"relative_td\", \"size_tercile\"])\n    .agg(\n        mean_std_sq_ret=(\"std_sq_ret\", \"mean\"),\n        mean_rel_vol=(\"rel_vol\", \"mean\"),\n    )\n    .reset_index()\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nsize_colors = {\"Small\": \"#E91E63\", \"Medium\": \"#FF9800\", \"Large\": \"#2196F3\"}\n\nfor label, color in size_colors.items():\n    sub = size_summary[size_summary[\"size_tercile\"] == label].sort_values(\"relative_td\")\n    axes[0].plot(sub[\"relative_td\"], sub[\"mean_std_sq_ret\"],\n                 label=label, color=color, linewidth=1.5)\n    axes[1].plot(sub[\"relative_td\"], sub[\"mean_rel_vol\"],\n                 label=label, color=color, linewidth=1.5)\n\nfor ax in axes:\n    ax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\n    ax.set_xlabel(\"Trading Days Relative to Announcement\")\n    ax.legend()\n\naxes[0].axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.7)\naxes[0].set_ylabel(\"Standardized Squared Return\")\naxes[0].set_title(\"Volatility Reaction by Size\")\n\naxes[1].axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.7)\naxes[1].set_ylabel(\"Relative Volume\")\naxes[1].set_title(\"Volume Reaction by Size\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 25.6: Announcement-day abnormal volatility and volume by firm size tercile. Smaller firms exhibit substantially larger reactions, consistent with the prediction that pre-disclosure information is increasing in firm size.\n\n\n\n\n\n\n\n25.5.2 By Exchange (HOSE vs. HNX)\nHOSE-listed firms are generally larger and more liquid than HNX-listed firms. Additionally, the tighter price limits on HOSE (\\(\\pm\\) 7% vs. \\(\\pm\\) 10% on HNX) may truncate announcement-day returns, compressing measured volatility and potentially spreading the adjustment over multiple days.\n\nexch_summary = (\n    daily_data\n    .groupby([\"relative_td\", \"exchange\"])\n    .agg(\n        mean_std_sq_ret=(\"std_sq_ret\", \"mean\"),\n        mean_rel_vol=(\"rel_vol\", \"mean\"),\n    )\n    .reset_index()\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nexch_colors = {\"HOSE\": \"#1565C0\", \"HNX\": \"#E65100\"}\n\nfor exch, color in exch_colors.items():\n    sub = exch_summary[exch_summary[\"exchange\"] == exch].sort_values(\"relative_td\")\n    axes[0].plot(sub[\"relative_td\"], sub[\"mean_std_sq_ret\"],\n                 label=exch, color=color, linewidth=1.5)\n    axes[1].plot(sub[\"relative_td\"], sub[\"mean_rel_vol\"],\n                 label=exch, color=color, linewidth=1.5)\n\nfor ax in axes:\n    ax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\n    ax.legend()\n    ax.set_xlabel(\"Trading Days Relative to Announcement\")\n\naxes[0].axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.7)\naxes[0].set_ylabel(\"Standardized Squared Return\")\naxes[0].set_title(\"Volatility by Exchange\")\n\naxes[1].axhline(1.0, color=\"grey\", linestyle=\"--\", linewidth=0.7)\naxes[1].set_ylabel(\"Relative Volume\")\naxes[1].set_title(\"Volume by Exchange\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 25.7: Announcement reactions by exchange. HNX firms (generally smaller, less liquid) show larger volatility spikes. The tighter HOSE price limit may compress announcement-day returns, but the wider HNX limit allows larger single-day adjustments.\n\n\n\n\n\n\n\n25.5.3 Regression Analysis\nWe formalize the cross-sectional analysis with an event-level regression:\n\\[\n\\Delta U^2_i = \\beta_0 + \\beta_1 \\text{Size}_i + \\beta_2 \\text{HNX}_i + \\beta_3 \\text{AnnLag}_i + \\varepsilon_i\n\\tag{25.10}\\]\nwhere \\(\\text{Size}_i\\) is log market capitalization, \\(\\text{HNX}_i\\) is an indicator for HNX listing, and \\(\\text{AnnLag}_i\\) is the number of calendar days between fiscal year-end and the announcement date (longer lags may allow more information leakage, reducing the surprise).\n\n# Build event-level dataset\nannc_data = daily_data[daily_data[\"relative_td\"].isin(annc_window)].copy()\nnon_annc_data = daily_data[daily_data[\"relative_td\"].isin(non_annc_window)].copy()\n\nannc_means = (\n    annc_data\n    .groupby([\"ticker\", \"fiscal_year\"])\n    .agg(annc_sq_ret=(\"sq_ret_mkt\", \"mean\"), annc_vol=(\"rel_vol\", \"mean\"))\n    .reset_index()\n)\nnon_annc_means = (\n    non_annc_data\n    .groupby([\"ticker\", \"fiscal_year\"])\n    .agg(non_annc_sq_ret=(\"sq_ret_mkt\", \"mean\"), non_annc_vol=(\"rel_vol\", \"mean\"))\n    .reset_index()\n)\n\nevent_df = annc_means.merge(non_annc_means, on=[\"ticker\", \"fiscal_year\"])\nevent_df[\"delta_sq_ret\"] = event_df[\"annc_sq_ret\"] - event_df[\"non_annc_sq_ret\"]\nevent_df[\"delta_vol\"] = event_df[\"annc_vol\"] - event_df[\"non_annc_vol\"]\n\n# Add firm characteristics\nfirm_chars = (\n    earn_annc[[\"ticker\", \"fiscal_year\", \"exchange\", \"log_mcap\", \"datadate\", \"annc_date\"]]\n    .drop_duplicates([\"ticker\", \"fiscal_year\"])\n)\nfirm_chars[\"hnx\"] = (firm_chars[\"exchange\"] == \"HNX\").astype(int)\nfirm_chars[\"annc_lag\"] = (\n    pd.to_datetime(firm_chars[\"annc_date\"]) - pd.to_datetime(firm_chars[\"datadate\"])\n).dt.days\n\nevent_df = event_df.merge(firm_chars, on=[\"ticker\", \"fiscal_year\"], how=\"left\")\nevent_df = event_df.dropna(subset=[\"log_mcap\", \"hnx\", \"annc_lag\", \"delta_sq_ret\"])\n\n# Regression\nmod_vol = smf.ols(\"delta_sq_ret ~ log_mcap + hnx + annc_lag\", data=event_df).fit()\nmod_volume = smf.ols(\"delta_vol ~ log_mcap + hnx + annc_lag\", data=event_df).fit()\n\nreg_table = pd.DataFrame({\n    \"Volatility\": {\n        \"Intercept\": f\"{mod_vol.params['Intercept']:.6f} ({mod_vol.bse['Intercept']:.6f})\",\n        \"Size (log mcap)\": f\"{mod_vol.params['log_mcap']:.6f} ({mod_vol.bse['log_mcap']:.6f})\",\n        \"HNX\": f\"{mod_vol.params['hnx']:.6f} ({mod_vol.bse['hnx']:.6f})\",\n        \"Annc Lag (days)\": f\"{mod_vol.params['annc_lag']:.6f} ({mod_vol.bse['annc_lag']:.6f})\",\n        \"N\": f\"{int(mod_vol.nobs):,}\",\n        \"R²\": f\"{mod_vol.rsquared:.4f}\",\n    },\n    \"Volume\": {\n        \"Intercept\": f\"{mod_volume.params['Intercept']:.4f} ({mod_volume.bse['Intercept']:.4f})\",\n        \"Size (log mcap)\": f\"{mod_volume.params['log_mcap']:.4f} ({mod_volume.bse['log_mcap']:.4f})\",\n        \"HNX\": f\"{mod_volume.params['hnx']:.4f} ({mod_volume.bse['hnx']:.4f})\",\n        \"Annc Lag (days)\": f\"{mod_volume.params['annc_lag']:.6f} ({mod_volume.bse['annc_lag']:.6f})\",\n        \"N\": f\"{int(mod_volume.nobs):,}\",\n        \"R²\": f\"{mod_volume.rsquared:.4f}\",\n    },\n})\nreg_table\n\n\n\nTable 25.3: Cross-sectional determinants of announcement-day return volatility. Negative size coefficient confirms that larger firms have less informative announcements. The announcement lag coefficient captures information leakage before the official release.\n\n\n\n\n\n\n\n\n\n\nVolatility\nVolume\n\n\n\n\nIntercept\n0.001040 (0.000116)\n0.7676 (0.0396)\n\n\nSize (log mcap)\n-0.000039 (0.000011)\n0.0017 (0.0038)\n\n\nHNX\n0.000266 (0.000034)\n0.0118 (0.0117)\n\n\nAnnc Lag (days)\n0.000003 (0.000001)\n-0.000037 (0.000444)\n\n\nN\n3,600\n3,600\n\n\nR²\n0.0210\n0.0003",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Information Content of Earnings Announcements</span>"
    ]
  },
  {
    "objectID": "32_info_earnings.html#sec-info-earnings-discussion",
    "href": "32_info_earnings.html#sec-info-earnings-discussion",
    "title": "25  Information Content of Earnings Announcements",
    "section": "25.6 Discussion and Contemporary Perspectives",
    "text": "25.6 Discussion and Contemporary Perspectives\n\n25.6.1 The Bamber et al. (2000) Critique\nBamber, Christensen, and Gaver (2000) raised two important methodological concerns about the original Beaver (1968) findings:\n\nMean vs. median effects. The original study focused on mean volume and volatility, which can be driven by a small number of extreme events. When Bamber, Christensen, and Gaver (2000) examined the proportion of individual announcements that generate unusual reactions (rather than the mean reaction), the evidence was far less dramatic.\nSample selection. Beaver’s original sample of 143 firms was not randomly drawn but reflected specific selection criteria that may have biased toward firms with more informative announcements.\n\nWe can assess the first concern directly. Figure 25.8 plots the fraction of announcements with above-median volatility or volume on each event day, following the spirit of Bamber et al.’s critique.\n\n# For each event, determine if announcement-day measures exceed\n# that event's median non-announcement-day measures\nevent_medians = (\n    daily_data[daily_data[\"relative_td\"].isin(non_annc_window)]\n    .groupby([\"ticker\", \"fiscal_year\"])\n    .agg(median_sq_ret=(\"sq_ret_mkt\", \"median\"), median_vol=(\"rel_vol\", \"median\"))\n    .reset_index()\n)\n\ndaily_with_median = daily_data.merge(\n    event_medians, on=[\"ticker\", \"fiscal_year\"], how=\"left\"\n)\ndaily_with_median[\"above_med_vol\"] = (\n    daily_with_median[\"sq_ret_mkt\"] &gt; daily_with_median[\"median_sq_ret\"]\n).astype(int)\ndaily_with_median[\"above_med_volume\"] = (\n    daily_with_median[\"rel_vol\"] &gt; daily_with_median[\"median_vol\"]\n).astype(int)\n\nprop_summary = (\n    daily_with_median\n    .groupby(\"relative_td\")\n    .agg(\n        prop_above_vol=(\"above_med_vol\", \"mean\"),\n        prop_above_volume=(\"above_med_volume\", \"mean\"),\n    )\n    .reset_index()\n    .sort_values(\"relative_td\")\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].bar(prop_summary[\"relative_td\"], prop_summary[\"prop_above_vol\"],\n            color=\"steelblue\", edgecolor=\"white\")\naxes[0].axhline(0.5, color=\"grey\", linestyle=\"--\", linewidth=1)\naxes[0].axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\naxes[0].set_xlabel(\"Trading Days Relative to Announcement\")\naxes[0].set_ylabel(\"Proportion &gt; Median\")\naxes[0].set_title(\"Proportion with Above-Median Volatility\")\naxes[0].yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1))\n\naxes[1].bar(prop_summary[\"relative_td\"], prop_summary[\"prop_above_volume\"],\n            color=\"darkorange\", edgecolor=\"white\")\naxes[1].axhline(0.5, color=\"grey\", linestyle=\"--\", linewidth=1)\naxes[1].axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\naxes[1].set_xlabel(\"Trading Days Relative to Announcement\")\naxes[1].set_ylabel(\"Proportion &gt; Median\")\naxes[1].set_title(\"Proportion with Above-Median Volume\")\naxes[1].yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 25.8: Proportion of individual announcements with above-median reaction on each event day. Unlike mean-based measures, this approach is robust to outliers. The fraction still rises notably on days 0 and +1, though the magnitude is more modest than in the mean-based plots.\n\n\n\n\n\n\n\n25.6.2 How Much Information Do Earnings Announcements Convey?\nBall and Shivakumar (2008) pose a provocative question: even if earnings announcements generate detectable market reactions, how much of the total annual information flow do they account for? Their analysis suggests that quarterly earnings announcements explain only about 12–18% of annual return variation for U.S. firms, implying that the vast majority of value-relevant information reaches the market through other channels.\nWe can compute an analogous statistic for our Vietnamese sample:\n\n# Compute fraction of event-window variance in announcement days\nvariance_shares = []\nfor (ticker, fy), group in daily_data.groupby([\"ticker\", \"fiscal_year\"]):\n    total_var = group[\"ret_mkt\"].var()\n    annc_var = group[group[\"relative_td\"].isin(annc_window)][\"ret_mkt\"].var()\n    \n    if total_var &gt; 0 and not np.isnan(annc_var):\n        variance_shares.append({\n            \"ticker\": ticker,\n            \"fiscal_year\": fy,\n            \"annc_var_share\": annc_var / total_var,\n        })\n\nvar_share_df = pd.DataFrame(variance_shares)\nexpected_share = len(annc_window) / 41  # 3 days out of 41\n\nshare_stats = pd.DataFrame({\n    \"Statistic\": [\"Mean share\", \"Median share\", \"Expected under null\",\n                  \"N events\", \"% above expected\"],\n    \"Value\": [\n        f\"{var_share_df['annc_var_share'].mean():.3f}\",\n        f\"{var_share_df['annc_var_share'].median():.3f}\",\n        f\"{expected_share:.3f}\",\n        f\"{len(var_share_df):,}\",\n        f\"{(var_share_df['annc_var_share'] &gt; expected_share).mean():.1%}\",\n    ],\n}).set_index(\"Statistic\")\nshare_stats\n\n\n\nTable 25.4: Share of annual return variation concentrated in the announcement window. The announcement window is days {-1, 0, +1}. If announcements carried no information, the expected share would be 3/41 ≈ 7.3% (three days out of a 41-day window). Values substantially above this indicate that a disproportionate share of return variation is concentrated around announcements.\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nStatistic\n\n\n\n\n\nMean share\n3.426\n\n\nMedian share\n2.979\n\n\nExpected under null\n0.073\n\n\nN events\n3,600\n\n\n% above expected\n98.5%\n\n\n\n\n\n\n\n\n\n\n\n\n25.6.3 Price Limits and Multi-Day Adjustment\nA distinctive feature of Vietnamese markets is that daily price limits can prevent full single-day price adjustment. When an announcement is sufficiently surprising, the stock may hit the \\(\\pm\\) 7% limit on HOSE (or \\(\\pm\\) 10% on HNX), and the remaining adjustment spills into subsequent trading days. This creates a pattern of limit hits followed by continued drift, a phenomenon that would bias downward any single-day measure of announcement impact and redistribute information content across days 0, +1, and potentially +2.\n\n# Identify limit hits\ndef is_limit_hit(row):\n    limit = 0.07 if row[\"exchange\"] == \"HOSE\" else 0.10\n    return abs(row[\"ret\"]) &gt;= (limit - 0.005)\n\ndaily_data[\"limit_hit\"] = daily_data.apply(is_limit_hit, axis=1).astype(int)\n\nlimit_summary = (\n    daily_data\n    .groupby(\"relative_td\")[\"limit_hit\"]\n    .mean()\n    .reset_index()\n    .sort_values(\"relative_td\")\n)\n\nfig, ax = plt.subplots()\ncolors = [\"firebrick\" if abs(t) &lt;= 1 else \"steelblue\"\n          for t in limit_summary[\"relative_td\"]]\nax.bar(limit_summary[\"relative_td\"], limit_summary[\"limit_hit\"],\n       color=colors, edgecolor=\"white\")\nax.set_xlabel(\"Trading Days Relative to Announcement\")\nax.set_ylabel(\"Proportion Hitting Price Limit\")\nax.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1, decimals=1))\nax.axvline(0, color=\"black\", linewidth=0.5, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 25.9: Proportion of stocks hitting the daily price limit (within 0.5% of the ±7% HOSE limit or ±10% HNX limit) by event day. Limit hits are concentrated on the announcement day, indicating that some announcements are sufficiently surprising to exhaust single-day adjustment capacity.\n\n\n\n\n\n\n\n25.6.4 International Context\nLandsman, Maydew, and Thornock (2012) conduct a cross-country study of announcement reactions and find that information content is higher in countries with: (i) stronger investor protection, (ii) greater financial transparency, and (iii) mandatory IFRS adoption. Vietnam’s position on these dimensions, developing institutional framework, VAS rather than IFRS, moderate transparency, suggests that announcement reactions may differ systematically from developed-market benchmarks.\nTheir framework generates a testable prediction: as Vietnam progresses toward IFRS adoption (a process underway as of 2025), the information content of earnings announcements should increase to the extent that IFRS improves the mapping from economic events to reported numbers.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Information Content of Earnings Announcements</span>"
    ]
  },
  {
    "objectID": "32_info_earnings.html#summary",
    "href": "32_info_earnings.html#summary",
    "title": "25  Information Content of Earnings Announcements",
    "section": "25.7 Summary",
    "text": "25.7 Summary\nThis chapter examined whether earnings announcements convey new information to investors in Vietnamese equity markets, following the foundational research design of Beaver (1968). The key findings are:\n\nBoth return volatility and trading volume spike sharply on the announcement day and the following day, providing robust evidence of information content. This result holds across fiscal years, consistent with earnings being a persistent source of new information.\nFormal statistical tests reject the null of no information content with high confidence, using both mean-based and proportion-based measures.\nCross-sectional analysis reveals that the announcement reaction is stronger for smaller firms and HNX-listed firms, consistent with the theoretical prediction that the information content of earnings is inversely related to the richness of the pre-disclosure information environment.\nVietnamese-specific features—price limits, retail dominance, and sparse analyst coverage—create distinctive empirical patterns that enrich the standard Beaver (1968) framework. Price limits, in particular, generate multi-day adjustment dynamics that researchers must account for when measuring single-day information content.\nThe Bamber, Christensen, and Gaver (2000) critique—that mean effects can be driven by outliers—is addressed by examining the proportion of events with above-median reactions, which still shows clear announcement-day elevation.\n\nThe evidence strongly supports the conclusion that annual earnings announcements convey new information to the Vietnamese market, consistent with the original Beaver (1968) findings and the broader international evidence in Landsman, Maydew, and Thornock (2012).\n\n\n\n\n\n\nAtiase, Rowland Kwame. 1985. “Predisclosure Information, Firm Capitalization, and Security Price Behavior Around Earnings Announcements.” Journal of Accounting Research, 21–36.\n\n\nBall, Ray, and Lakshmanan Shivakumar. 2008. “How Much New Information Is There in Earnings?” Journal of Accounting Research 46 (5): 975–1016.\n\n\nBamber, Linda Smith, Theodore E Christensen, and Kenneth M Gaver. 2000. “Do We Really ‘Know’what We Think We Know? A Case Study of Seminal Research and Its Subsequent Overgeneralization.” Accounting, Organizations and Society 25 (2): 103–29.\n\n\nBeaver, William H. 1968. “The Information Content of Annual Earnings Announcements.” Journal of Accounting Research, 67–92.\n\n\nKandel, Eugene, and Neil D Pearson. 1995. “Differential Interpretation of Public Signals and Trade in Speculative Markets.” Journal of Political Economy 103 (4): 831–72.\n\n\nKarpoff, Jonathan M. 1987. “The Relation Between Price Changes and Trading Volume: A Survey.” Journal of Financial and Quantitative Analysis 22 (1): 109–26.\n\n\nKim, Oliver, and Robert E Verrecchia. 1991. “Market Reaction to Anticipated Announcements.” Journal of Financial Economics 30 (2): 273–309.\n\n\nLandsman, Wayne R, Edward L Maydew, and Jacob R Thornock. 2012. “The Information Content of Annual Earnings Announcements and Mandatory Adoption of IFRS.” Journal of Accounting and Economics 53 (1-2): 34–54.\n\n\nVerrecchia, Robert E. 2001. “Essays on Disclosure.” Journal of Accounting and Economics 32 (1-3): 97–180.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Information Content of Earnings Announcements</span>"
    ]
  },
  {
    "objectID": "33_accruals.html",
    "href": "33_accruals.html",
    "title": "26  Accruals, Earnings Persistence, and Market Efficiency",
    "section": "",
    "text": "26.0.1 Why Vietnam?\nAccrual accounting is the foundation of modern financial reporting. Under accrual principles, revenues are recognized when earned and expenses when incurred, regardless of when cash changes hands. This timing wedge between economic events and cash realization generates accruals (i.e., the non-cash component of reported earnings). Formally, we define total accruals as:\n\\[\n\\text{Accruals}_t = \\text{Earnings}_t - \\text{Cash Flow from Operations}_t\n\\tag{26.1}\\]\nThe distinction between accruals and cash flows matters because these two components of earnings exhibit markedly different statistical properties. Cash flows from operations tend to be more persistent and harder for managers to manipulate, while accruals are inherently more transient and subject to managerial discretion through estimates, assumptions, and timing choices (Dechow, Ge, and Schrand 2010).\nSloan (1996) demonstrated two important empirical regularities in U.S. data:\nVietnam’s equity markets provide a compelling laboratory for studying accrual dynamics for several reasons:",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Accruals, Earnings Persistence, and Market Efficiency</span>"
    ]
  },
  {
    "objectID": "33_accruals.html#sec-accruals-measuring",
    "href": "33_accruals.html#sec-accruals-measuring",
    "title": "26  Accruals, Earnings Persistence, and Market Efficiency",
    "section": "26.1 Measuring Accruals",
    "text": "26.1 Measuring Accruals\n\n26.1.1 The Balance Sheet Approach\nThe traditional approach to measuring accruals, used in much of the early literature, constructs total accruals from successive balance sheet snapshots. Define the following balance sheet items for firm \\(i\\) in period \\(t\\) (Table 26.1).\n\n\n\nTable 26.1: Key balance sheet items for accrual measurement\n\n\n\n\n\n\n\n\n\n\nSymbol\nDescription\nVAS Equivalent\n\n\n\n\n\\(\\Delta CA_{i,t}\\)\nChange in current assets\nThay đổi tài sản ngắn hạn\n\n\n\\(\\Delta Cash_{i,t}\\)\nChange in cash and equivalents\nThay đổi tiền và tương đương tiền\n\n\n\\(\\Delta CL_{i,t}\\)\nChange in current liabilities\nThay đổi nợ ngắn hạn\n\n\n\\(\\Delta STD_{i,t}\\)\nChange in short-term debt\nThay đổi vay ngắn hạn\n\n\n\\(\\Delta TP_{i,t}\\)\nChange in taxes payable\nThay đổi thuế phải nộp\n\n\n\\(Dep_{i,t}\\)\nDepreciation and amortization\nKhấu hao TSCĐ\n\n\n\n\n\n\nTotal accruals under the balance sheet approach are:\n\\[\nACC^{BS}_{i,t} = (\\Delta CA_{i,t} - \\Delta Cash_{i,t})\n  - (\\Delta CL_{i,t} - \\Delta STD_{i,t} - \\Delta TP_{i,t})\n  - Dep_{i,t}\n\\tag{26.2}\\]\nThe intuition behind Equation 26.2 is straightforward. Changes in non-cash current assets (such as accounts receivable and inventory) represent revenues recognized or expenses deferred without corresponding cash flows. Changes in operating current liabilities (excluding debt) capture expenses recognized without cash outflows. Short-term debt and taxes payable are excluded because they relate to financing and tax timing rather than operating accruals. Depreciation is subtracted as a non-cash charge against earnings.\nAll variables are typically scaled by average total assets to control for firm size:\n\\[\nacc_{i,t} = \\frac{ACC^{BS}_{i,t}}{(\\text{Assets}_{i,t} + \\text{Assets}_{i,t-1})/2}\n\\tag{26.3}\\]\n\n\n26.1.2 The Cash Flow Statement Approach\nHribar and Collins (2002) demonstrated that the balance sheet approach introduces measurement error because non-operating events, such as mergers, acquisitions, divestitures, and foreign currency translations, affect current asset and liability balances without corresponding earnings impacts. They advocate a simpler and more accurate approach:\n\\[\nACC^{CF}_{i,t} = \\text{Earnings}_{i,t} - \\text{CFO}_{i,t}\n\\tag{26.4}\\]\nwhere \\(\\text{CFO}_{i,t}\\) is cash flow from operations reported directly on the cash flow statement.\n\n\n26.1.3 Vietnamese Context\nFor Vietnamese listed firms, the cash flow statement approach is generally preferable, but practitioners should be aware of several data considerations:\n\nCircular 200/2014/TT-BTC standardizes the chart of accounts and financial statement templates. Cash flow from operations is reported using the indirect method by most firms (starting from net income and adjusting for non-cash items), though some use the direct method.\nVAS 24 (Cash Flow Statements) governs disclosure. Unlike IFRS, VAS treatment of interest paid and dividends received in the operating section is less flexible, potentially affecting comparability.\n\nFor the remainder of this chapter, we use the balance sheet approach, where demonstrated on simulated data (for pedagogical transparency) and the cash flow statement approach on actual firm data where available.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Accruals, Earnings Persistence, and Market Efficiency</span>"
    ]
  },
  {
    "objectID": "33_accruals.html#sec-accruals-simulation",
    "href": "33_accruals.html#sec-accruals-simulation",
    "title": "26  Accruals, Earnings Persistence, and Market Efficiency",
    "section": "26.2 Simulation Analysis",
    "text": "26.2 Simulation Analysis\nBefore turning to Vietnamese market data, we construct a simulation to build economic intuition for why accrual persistence differs from cash flow persistence, and what role estimation error in accrual accounting plays in generating this difference.\n\n26.2.1 The Model\nConsider a simplified firm that:\n\nSells goods on credit at a constant gross margin \\(\\mu\\)\nFollows an AR(1) sales process: \\(S_t = \\bar{S} + \\rho(S_{t-1} - \\bar{S}) + \\varepsilon_t\\), where \\(\\varepsilon_t \\sim N(0, \\sigma^2_S)\\)\nCollects receivables in the following period, with a true default rate \\(\\delta\\)\nRecords an allowance for doubtful debts as a fraction \\(\\alpha\\) of current sales\nPays dividends equal to 100% of net income\n\nThe key parameter of interest is \\(\\alpha\\), the managerial estimate of bad debts. When \\(\\alpha = \\delta\\) (the true default rate), the accounting system is unbiased. When \\(\\alpha \\neq \\delta\\), accruals contain estimation error that affects persistence.\n\ndef simulate_firm(\n    alpha: float = 0.03,\n    n_years: int = 20,\n    delta: float = 0.03,\n    gross_margin: float = 0.80,\n    mean_sales: float = 1000.0,\n    sd_sales: float = 100.0,\n    rho: float = 0.9,\n    beg_cash: float = 1500.0,\n    rng: np.random.Generator = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Simulate financial statements for a single firm.\n    \n    Parameters\n    ----------\n    alpha : float\n        Managerial estimate of doubtful debt rate (allowance parameter).\n    n_years : int\n        Number of years to simulate.\n    delta : float\n        True economic default rate on receivables.\n    gross_margin : float\n        Gross margin on sales.\n    mean_sales : float\n        Long-run mean of the AR(1) sales process.\n    sd_sales : float\n        Standard deviation of the sales innovation.\n    rho : float\n        AR(1) persistence parameter for sales.\n    beg_cash : float\n        Beginning cash balance.\n    rng : numpy Generator\n        Random number generator for reproducibility.\n        \n    Returns\n    -------\n    pd.DataFrame\n        Simulated financial statement data.\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng(42)\n\n    # Generate AR(1) sales\n    errors = rng.normal(0, sd_sales, n_years)\n    sales = np.zeros(n_years)\n    sales[0] = mean_sales + errors[0]\n    for t in range(1, n_years):\n        sales[t] = mean_sales + rho * (sales[t - 1] - mean_sales) + errors[t]\n\n    # Allocate arrays\n    cogs = (1 - gross_margin) * sales\n    ar = sales.copy()                     # all sales on credit\n    allowance = alpha * sales             # allowance for doubtful debts\n\n    writeoffs = np.zeros(n_years)\n    collections = np.zeros(n_years)\n    bad_debt_exp = np.zeros(n_years)\n    net_income = np.zeros(n_years)\n    dividends = np.zeros(n_years)\n    cash = np.zeros(n_years)\n    equity = np.zeros(n_years)\n\n    # Year 1\n    bad_debt_exp[0] = allowance[0]\n    net_income[0] = sales[0] - cogs[0] - bad_debt_exp[0]\n    dividends[0] = net_income[0]\n    cash[0] = beg_cash + collections[0] - cogs[0] - dividends[0]\n    equity[0] = beg_cash + net_income[0] - dividends[0]\n\n    # Years 2 through n_years\n    for t in range(1, n_years):\n        writeoffs[t] = delta * ar[t - 1]\n        collections[t] = (1 - delta) * ar[t - 1]\n        bad_debt_exp[t] = allowance[t] - allowance[t - 1] + writeoffs[t]\n        net_income[t] = sales[t] - cogs[t] - bad_debt_exp[t]\n        dividends[t] = net_income[t]\n        cash[t] = cash[t - 1] + collections[t] - cogs[t] - dividends[t]\n        equity[t] = equity[t - 1] + net_income[t] - dividends[t]\n\n    return pd.DataFrame({\n        \"year\": np.arange(1, n_years + 1),\n        \"alpha\": alpha,\n        \"sales\": sales,\n        \"cogs\": cogs,\n        \"ar\": ar,\n        \"allowance\": allowance,\n        \"writeoffs\": writeoffs,\n        \"collections\": collections,\n        \"bad_debt_exp\": bad_debt_exp,\n        \"net_income\": net_income,\n        \"dividends\": dividends,\n        \"cash\": cash,\n        \"equity\": equity,\n    })\n\n\n\n26.2.2 Generating a Single Firm\nWe first generate 1,000 years of data for a single firm with \\(\\alpha = \\delta = 0.03\\) (unbiased accounting) to visualize the sales process.\n\nrng = np.random.default_rng(2024)\ndf_long = simulate_firm(alpha=0.03, n_years=1000, rng=rng)\n\nfig, ax = plt.subplots()\nsubset = df_long.query(\"year &lt;= 20\")\nax.plot(subset[\"year\"], subset[\"sales\"], color=\"firebrick\", linewidth=1.5)\nax.axhline(\n    df_long[\"sales\"].mean(), color=\"steelblue\",\n    linestyle=\"--\", linewidth=1, label=\"Long-run mean\"\n)\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Sales (millions VND, simulated)\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 26.1: Simulated sales for a single firm over 20 years. The blue dashed line shows mean sales. The AR(1) structure creates smooth, persistent fluctuations.\n\n\n\n\n\n\n\n26.2.3 Cross-Sectional Simulation: Persistence and Estimation Error\nThe central economic insight is that when the managerial estimate \\(\\alpha\\) departs from the true rate \\(\\delta\\), accruals contain a systematic estimation error that reduces the persistence of earnings. To demonstrate this, we simulate 5,000 firms, each with a randomly drawn \\(\\alpha \\in [0.01, 0.05]\\) while keeping the true default rate fixed at \\(\\delta = 0.03\\).\nFor each firm, we estimate earnings persistence as the slope coefficient \\(\\hat{\\beta}_1\\) from an AR(1) regression of net income:\n\\[\nNI_{i,t} = \\beta_0 + \\beta_1 NI_{i,t-1} + u_{i,t}\n\\tag{26.5}\\]\n\ndef estimate_persistence(df: pd.DataFrame) -&gt; dict:\n    \"\"\"Estimate AR(1) persistence of net income for one firm.\"\"\"\n    temp = df[[\"year\", \"net_income\"]].copy()\n    temp[\"lag_ni\"] = temp[\"net_income\"].shift(1)\n    temp = temp.dropna()\n    if len(temp) &lt; 10:\n        return {\"alpha\": df[\"alpha\"].iloc[0], \"persistence\": np.nan}\n    X = sm.add_constant(temp[\"lag_ni\"])\n    model = sm.OLS(temp[\"net_income\"], X).fit()\n    return {\"alpha\": df[\"alpha\"].iloc[0], \"persistence\": model.params[\"lag_ni\"]}\n\nn_firms = 5000\nrng_main = np.random.default_rng(2024)\nalphas = rng_main.uniform(0.01, 0.05, n_firms)\n\nresults = []\nfor i, a in enumerate(alphas):\n    firm_rng = np.random.default_rng(2024 + i)\n    firm_df = simulate_firm(alpha=a, n_years=200, rng=firm_rng)\n    results.append(estimate_persistence(firm_df))\n\nsim_results = pd.DataFrame(results)\n\n\nfig, ax = plt.subplots()\nax.scatter(\n    sim_results[\"alpha\"], sim_results[\"persistence\"],\n    alpha=0.15, s=8, color=\"steelblue\", edgecolors=\"none\"\n)\nax.set_xlabel(r\"Allowance parameter ($\\alpha$)\")\nax.set_ylabel(\"Estimated earnings persistence\")\nax.axvline(0.03, color=\"firebrick\", linestyle=\"--\", linewidth=1, label=r\"$\\delta = 0.03$ (true rate)\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 26.2: Earnings persistence vs. allowance parameter (\\(\\alpha\\)). Persistence peaks near \\(\\alpha = 0.03\\), the true default rate, and declines symmetrically as the accounting estimate deviates from economic reality in either direction.\n\n\n\n\n\n\n\n26.2.4 Decomposing Persistence by Earnings Component\nThe simulation allows us to verify the core theoretical prediction: cash flow persistence exceeds accrual persistence. We compute operating cash flows and accruals for each simulated firm and estimate component-specific persistence.\n\ndef component_persistence(df: pd.DataFrame) -&gt; dict:\n    \"\"\"Estimate persistence of NI, CFO, and accruals.\"\"\"\n    temp = df.copy()\n    # CFO = collections - COGS (direct method, simplified)\n    temp[\"cfo\"] = temp[\"collections\"] - temp[\"cogs\"]\n    temp[\"acc\"] = temp[\"net_income\"] - temp[\"cfo\"]\n\n    out = {\"alpha\": temp[\"alpha\"].iloc[0]}\n    for var in [\"net_income\", \"cfo\", \"acc\"]:\n        t = temp[[\"year\", var]].copy()\n        t[\"lag\"] = t[var].shift(1)\n        t = t.dropna()\n        if len(t) &lt; 10:\n            out[f\"persist_{var}\"] = np.nan\n            continue\n        X = sm.add_constant(t[\"lag\"])\n        model = sm.OLS(t[var], X).fit()\n        out[f\"persist_{var}\"] = model.params[\"lag\"]\n    return out\n\n# Run for a subset at alpha = 0.03 (unbiased) and alpha = 0.05 (biased)\ncomponent_results = []\nfor a_val in [0.01, 0.02, 0.03, 0.04, 0.05]:\n    for seed_offset in range(500):\n        frng = np.random.default_rng(10000 + seed_offset)\n        fdf = simulate_firm(alpha=a_val, n_years=200, rng=frng)\n        component_results.append(component_persistence(fdf))\n\ncomp_df = pd.DataFrame(component_results)\n\n\nsummary = (\n    comp_df\n    .groupby(\"alpha\")[[\"persist_net_income\", \"persist_cfo\", \"persist_acc\"]]\n    .mean()\n    .round(4)\n    .rename(columns={\n        \"persist_net_income\": \"Earnings\",\n        \"persist_cfo\": \"Cash Flows\",\n        \"persist_acc\": \"Accruals\",\n    })\n)\nsummary.index.name = \"α\"\nsummary\n\n\n\nTable 26.2: Mean earnings component persistence by allowance parameter. Cash flow persistence is stable across \\(\\alpha\\) values, while accrual and overall earnings persistence peak when accounting estimates match the true default rate (\\(\\delta = 0.03\\)).\n\n\n\n\n\n\n\n\n\n\nEarnings\nCash Flows\nAccruals\n\n\nα\n\n\n\n\n\n\n\n0.01\n0.8728\n0.6922\n-0.0357\n\n\n0.02\n0.8758\n0.6922\n-0.0357\n\n\n0.03\n0.8788\n0.6922\n-0.0357\n\n\n0.04\n0.8818\n0.6922\n-0.0357\n\n\n0.05\n0.8846\n0.6922\n-0.0357\n\n\n\n\n\n\n\n\n\n\nThe simulation confirms a key insight: cash flow persistence is largely invariant to the quality of accrual estimation, whereas accrual persistence, and consequently total earnings persistence, degrades when managerial estimates deviate from economic truth. This provides a structural explanation for the empirical regularity documented in Sloan (1996): accruals are less persistent because they embed estimation error that mean-reverts as the accounting system self-corrects over time.\n\n\n\n\n\n\nNoteVietnamese Interpretation\n\n\n\nIn the Vietnamese context, this mechanism is amplified by several institutional factors. VAS provisions for doubtful debts (Circular 228/2009/TT-BTC, updated by Circular 48/2019/TT-BTC) prescribe specific aging-based percentages rather than allowing full managerial discretion. While this rules-based approach reduces some forms of manipulation, it can increase estimation error when the prescribed rates diverge from firm-specific default experiences, particularly relevant for firms in rapidly changing sectors like real estate development or export manufacturing.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Accruals, Earnings Persistence, and Market Efficiency</span>"
    ]
  },
  {
    "objectID": "33_accruals.html#sec-accruals-persistence",
    "href": "33_accruals.html#sec-accruals-persistence",
    "title": "26  Accruals, Earnings Persistence, and Market Efficiency",
    "section": "26.3 Earnings Persistence in Vietnamese Data",
    "text": "26.3 Earnings Persistence in Vietnamese Data\nWe now turn to empirical analysis using Vietnamese listed firm data. The data requirements are:\n\nAnnual financial statements for firms listed on HOSE and HNX\nStock return data: Monthly adjusted closing prices for return computation\nIndustry classifications: VSIC (Vietnam Standard Industrial Classification) codes\n\n\n26.3.1 Data Preparation\nThe following code constructs the accrual measures from Vietnamese firm financial statements. We demonstrate the workflow assuming data is loaded from a local database or CSV files. Adapt the data loading step to match your source.\n\n# ─── Data Loading (adapt to your source) ───────────────────────────\n# funda = pd.read_parquet(\"data/vn_annual_financials.parquet\")\n# prices = pd.read_parquet(\"data/vn_monthly_prices.parquet\")\n\n# ─── Variable Construction ──────────────────────────────────────────\ndef construct_accruals(funda: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Construct accrual measures from Vietnamese annual financial data.\n    \n    Expected columns (VAS-aligned):\n        ticker, year, datadate,\n        act (current assets), lct (current liabilities),\n        che (cash & equivalents), dlc (short-term borrowings),\n        txp (taxes payable), dp (depreciation & amortization),\n        oiadp (operating income after depreciation),\n        at (total assets), cfo (cash flow from operations),\n        exchange (HOSE=1, HNX=2), vsic2 (2-digit industry code)\n    \"\"\"\n    df = funda.sort_values([\"ticker\", \"year\"]).copy()\n    \n    # Fill missing with zero where economically appropriate\n    for col in [\"che\", \"dlc\", \"txp\"]:\n        df[col] = df[col].fillna(0)\n\n    # Lagged total assets and average total assets\n    df[\"lag_at\"] = df.groupby(\"ticker\")[\"at\"].shift(1)\n    df[\"avg_at\"] = (df[\"at\"] + df[\"lag_at\"]) / 2\n\n    # Balance sheet approach to accruals\n    for col in [\"act\", \"che\", \"lct\", \"dlc\", \"txp\"]:\n        df[f\"d_{col}\"] = df.groupby(\"ticker\")[col].diff()\n\n    df[\"acc_bs\"] = (\n        (df[\"d_act\"] - df[\"d_che\"])\n        - (df[\"d_lct\"] - df[\"d_dlc\"] - df[\"d_txp\"])\n        - df[\"dp\"]\n    )\n\n    # Scaled variables\n    df[\"earn\"] = df[\"oiadp\"] / df[\"avg_at\"]\n    df[\"acc\"] = df[\"acc_bs\"] / df[\"avg_at\"]\n    df[\"cfo\"] = df[\"earn\"] - df[\"acc\"]\n\n    # If CFO from cash flow statement is available, prefer it\n    if \"cfo_stmt\" in df.columns:\n        df[\"cfo_direct\"] = df[\"cfo_stmt\"] / df[\"avg_at\"]\n        df[\"acc_cf\"] = df[\"earn\"] - df[\"cfo_direct\"]\n\n    # Lead earnings (next year)\n    df[\"lead_earn\"] = df.groupby(\"ticker\")[\"earn\"].shift(-1)\n\n    # Decile ranks (within each year for cross-sectional analysis)\n    for var in [\"acc\", \"earn\", \"cfo\"]:\n        df[f\"{var}_decile\"] = (\n            df.groupby(\"year\")[var]\n            .transform(lambda x: pd.qcut(x, 10, labels=False, duplicates=\"drop\") + 1)\n        )\n    df[\"lead_earn_decile\"] = (\n        df.groupby(\"year\")[\"lead_earn\"]\n        .transform(lambda x: pd.qcut(x, 10, labels=False, duplicates=\"drop\") + 1)\n    )\n\n    # Filter\n    df = df.query(\"avg_at &gt; 0\").dropna(subset=[\"acc\", \"earn\", \"cfo\", \"lead_earn\"])\n\n    return df\n\n\n\n26.3.2 Simulated Vietnamese-Style Data for Demonstration\nSince we cannot distribute proprietary data in this chapter, we generate synthetic data that mimics the cross-sectional properties of Vietnamese listed firms. The parameters are calibrated to approximate values observed in the Vietnamese market.\n\ndef generate_vn_panel(\n    n_firms: int = 400,\n    n_years: int = 15,\n    seed: int = 2024,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a synthetic panel dataset with properties resembling\n    Vietnamese listed firms for demonstration purposes.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    records = []\n    for i in range(n_firms):\n        # Firm-specific parameters\n        mean_earn = rng.normal(0.08, 0.04)      # mean ROA ~8%\n        persist = rng.uniform(0.3, 0.85)         # earnings persistence\n        acc_share = rng.normal(0.0, 0.03)        # mean accrual level\n        acc_noise = rng.uniform(0.02, 0.06)      # accrual volatility\n        \n        # SOE indicator (≈30% of Vietnamese listed firms)\n        is_soe = int(rng.random() &lt; 0.30)\n        # Exchange: HOSE (1) vs HNX (2)\n        exchange = 1 if rng.random() &lt; 0.6 else 2\n        # VSIC 2-digit code\n        vsic2 = rng.choice([10, 20, 25, 41, 46, 47, 52, 62, 64, 68])\n\n        earn_t = mean_earn\n        for t in range(n_years):\n            year = 2009 + t\n            shock = rng.normal(0, 0.04)\n            earn_t = mean_earn + persist * (earn_t - mean_earn) + shock\n\n            # Accruals: base + firm-specific noise\n            # SOEs tend to have slightly smoother accruals\n            soe_adj = -0.005 if is_soe else 0.0\n            acc_t = acc_share + soe_adj + rng.normal(0, acc_noise)\n            cfo_t = earn_t - acc_t\n\n            records.append({\n                \"ticker\": f\"VN{i:04d}\",\n                \"year\": year,\n                \"earn\": earn_t,\n                \"acc\": acc_t,\n                \"cfo\": cfo_t,\n                \"is_soe\": is_soe,\n                \"exchange\": exchange,\n                \"vsic2\": vsic2,\n            })\n\n    df = pd.DataFrame(records)\n\n    # Lead earnings\n    df = df.sort_values([\"ticker\", \"year\"])\n    df[\"lead_earn\"] = df.groupby(\"ticker\")[\"earn\"].shift(-1)\n\n    # Decile ranks within year\n    for var in [\"acc\", \"earn\", \"cfo\"]:\n        df[f\"{var}_decile\"] = (\n            df.groupby(\"year\")[var]\n            .transform(lambda x: pd.qcut(\n                x, 10, labels=False, duplicates=\"drop\"\n            ) + 1)\n        )\n    df[\"lead_earn_decile\"] = (\n        df.groupby(\"year\")[\"lead_earn\"]\n        .transform(lambda x: pd.qcut(\n            x, 10, labels=False, duplicates=\"drop\"\n        ) + 1)\n    )\n\n    # Simulate size-adjusted returns\n    # Returns load on contemporaneous earnings surprise and lagged mispricing\n    df[\"size_adj_ret\"] = (\n        1.5 * (df[\"lead_earn\"] - df[\"earn\"])  # earnings surprise\n        - 0.25 * df[\"acc\"]                     # accrual mispricing\n        + rng.normal(0, 0.30, len(df))         # noise\n    )\n\n    df = df.dropna(subset=[\"lead_earn\", \"size_adj_ret\"])\n    return df\n\npanel = generate_vn_panel(n_firms=500, n_years=15, seed=2024)\nprint(f\"Panel: {panel.shape[0]:,} firm-years, \"\n      f\"{panel['ticker'].nunique()} firms, \"\n      f\"{panel['year'].nunique()} years\")\n\nPanel: 7,000 firm-years, 500 firms, 14 years\n\n\n\n\n26.3.3 Descriptive Statistics by Accrual Decile\nTable 26.3 reports mean values of earnings, accruals, and cash flows by accrual decile. The pattern should mirror the mechanical decomposition: \\(\\text{earn} = \\text{acc} + \\text{cfo}\\).\n\ndesc = (\n    panel\n    .groupby(\"acc_decile\")[[\"acc\", \"earn\", \"cfo\"]]\n    .mean()\n    .round(4)\n)\ndesc.index.name = \"Accrual Decile\"\ndesc.columns = [\"Accruals\", \"Earnings\", \"Cash Flows\"]\ndesc\n\n\n\nTable 26.3: Mean earnings, accruals, and cash flows by accrual decile. Firms in the lowest accrual decile have the highest cash flows, while high-accrual firms show compressed or negative cash flows—a pattern consistent with the accounting identity.\n\n\n\n\n\n\n\n\n\n\nAccruals\nEarnings\nCash Flows\n\n\nAccrual Decile\n\n\n\n\n\n\n\n1\n-0.0898\n0.0760\n0.1657\n\n\n2\n-0.0509\n0.0814\n0.1324\n\n\n3\n-0.0321\n0.0769\n0.1090\n\n\n4\n-0.0180\n0.0800\n0.0980\n\n\n5\n-0.0057\n0.0792\n0.0849\n\n\n6\n0.0071\n0.0776\n0.0705\n\n\n7\n0.0193\n0.0814\n0.0621\n\n\n8\n0.0334\n0.0760\n0.0425\n\n\n9\n0.0521\n0.0760\n0.0239\n\n\n10\n0.0896\n0.0780\n-0.0116\n\n\n\n\n\n\n\n\n\n\n\n\n26.3.4 Persistence Regressions\nWe estimate the pooled persistence regression analogous to Table 2 of Sloan (1996). The baseline specification is:\n\\[\n\\text{Earnings}_{i,t+1} = \\gamma_0 + \\gamma_1 \\text{Earnings}_{i,t} + \\epsilon_{i,t}\n\\tag{26.6}\\]\nfollowed by the decomposition:\n\\[\n\\text{Earnings}_{i,t+1} = \\gamma_0 + \\gamma_a \\text{Accruals}_{i,t}\n  + \\gamma_c \\text{CFO}_{i,t} + \\epsilon_{i,t}\n\\tag{26.7}\\]\nThe hypothesis of differential persistence is \\(H_0: \\gamma_a = \\gamma_c\\), which we test with a standard \\(F\\)-test.\n\n# Column (1): Aggregate persistence\nmod1 = smf.ols(\"lead_earn ~ earn\", data=panel).fit()\n\n# Column (2): Component persistence\nmod2 = smf.ols(\"lead_earn ~ acc + cfo\", data=panel).fit()\n\n# F-test for H0: coef(acc) = coef(cfo)\nf_test = mod2.f_test(\"acc = cfo\")\n\nresults_table = pd.DataFrame({\n    \"(1) Aggregate\": {\n        \"Intercept\": f\"{mod1.params['Intercept']:.4f} ({mod1.bse['Intercept']:.4f})\",\n        \"Earnings\": f\"{mod1.params['earn']:.4f} ({mod1.bse['earn']:.4f})\",\n        \"Accruals\": \"\",\n        \"Cash Flows\": \"\",\n        \"N\": f\"{int(mod1.nobs):,}\",\n        \"R²\": f\"{mod1.rsquared:.3f}\",\n        \"F-test (acc=cfo)\": \"\",\n    },\n    \"(2) Components\": {\n        \"Intercept\": f\"{mod2.params['Intercept']:.4f} ({mod2.bse['Intercept']:.4f})\",\n        \"Earnings\": \"\",\n        \"Accruals\": f\"{mod2.params['acc']:.4f} ({mod2.bse['acc']:.4f})\",\n        \"Cash Flows\": f\"{mod2.params['cfo']:.4f} ({mod2.bse['cfo']:.4f})\",\n        \"N\": f\"{int(mod2.nobs):,}\",\n        \"R²\": f\"{mod2.rsquared:.3f}\",\n        \"F-test (acc=cfo)\": f\"F = {float(f_test.fvalue):.2f}, p = {float(f_test.pvalue):.4f}\", \n        \n        \n    },\n})\nresults_table\n\n\n\nTable 26.4: Earnings persistence regressions. Column (1) estimates aggregate persistence. Column (2) decomposes earnings into accrual and cash flow components. The coefficient on accruals is significantly lower than on cash flows, confirming differential persistence.\n\n\n\n\n\n\n\n\n\n\n(1) Aggregate\n(2) Components\n\n\n\n\nIntercept\n0.0189 (0.0008)\n0.0189 (0.0008)\n\n\nEarnings\n0.7618 (0.0079)\n\n\n\nAccruals\n\n0.7602 (0.0127)\n\n\nCash Flows\n\n0.7618 (0.0079)\n\n\nN\n7,000\n7,000\n\n\nR²\n0.569\n0.569\n\n\nF-test (acc=cfo)\n\nF = 0.02, p = 0.8746\n\n\n\n\n\n\n\n\n\n\n\n\n26.3.5 Industry-Level Persistence\nMarket-wide regressions may mask heterogeneity across industries. Vietnamese industry structure is concentrated in manufacturing, real estate, banking, and retail (i.e., sectors with very different accrual profiles). We estimate persistence separately by two-digit VSIC code.\n\ndef industry_persistence(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Estimate persistence regressions by VSIC 2-digit code.\"\"\"\n    coefs = []\n    for vsic, group in df.groupby(\"vsic2\"):\n        if len(group) &lt; 30:\n            continue\n        try:\n            mod = smf.ols(\"lead_earn ~ acc + cfo\", data=group).fit()\n            coefs.append({\n                \"vsic2\": vsic,\n                \"intercept\": mod.params[\"Intercept\"],\n                \"acc\": mod.params[\"acc\"],\n                \"cfo\": mod.params[\"cfo\"],\n            })\n        except Exception:\n            continue\n    return pd.DataFrame(coefs)\n\nind_coefs = industry_persistence(panel)\n\nind_summary = ind_coefs[[\"intercept\", \"acc\", \"cfo\"]].describe().loc[\n    [\"mean\", \"25%\", \"50%\", \"75%\"]\n].round(4)\nind_summary.index = [\"Mean\", \"Q1\", \"Median\", \"Q3\"]\nind_summary.columns = [\"Intercept\", \"Accruals (γ_a)\", \"Cash Flows (γ_c)\"]\nind_summary\n\n\n\nTable 26.5: Distribution of industry-level persistence coefficients. Each row reports statistics for the distribution of estimated coefficients across 2-digit VSIC industries.\n\n\n\n\n\n\n\n\n\n\nIntercept\nAccruals (γ_a)\nCash Flows (γ_c)\n\n\n\n\nMean\n0.0192\n0.7573\n0.7587\n\n\nQ1\n0.0171\n0.7310\n0.7323\n\n\nMedian\n0.0193\n0.7621\n0.7558\n\n\nQ3\n0.0213\n0.7914\n0.7750\n\n\n\n\n\n\n\n\n\n\n\n\n26.3.6 Persistence Visualization\nFigure 26.3 compares the accrual and cash flow persistence coefficients across industries, providing a visual test of the differential persistence hypothesis.\n\nif len(ind_coefs) &gt; 2:\n    ind_plot = ind_coefs.sort_values(\"cfo\").reset_index(drop=True)\n    fig, ax = plt.subplots(figsize=(8, 5))\n    x = np.arange(len(ind_plot))\n    width = 0.35\n    ax.barh(x - width / 2, ind_plot[\"acc\"], width, label=\"Accruals\", color=\"steelblue\")\n    ax.barh(x + width / 2, ind_plot[\"cfo\"], width, label=\"Cash Flows\", color=\"firebrick\")\n    ax.set_yticks(x)\n    ax.set_yticklabels([f\"VSIC {int(v)}\" for v in ind_plot[\"vsic2\"]])\n    ax.set_xlabel(\"Persistence Coefficient\")\n    ax.legend()\n    ax.set_title(\"Earnings Component Persistence by Industry\")\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\nFigure 26.3: Accrual vs. cash flow persistence coefficients by VSIC industry. In nearly all industries, cash flow persistence exceeds accrual persistence, consistent with the differential persistence hypothesis.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Accruals, Earnings Persistence, and Market Efficiency</span>"
    ]
  },
  {
    "objectID": "33_accruals.html#sec-accruals-mispricing",
    "href": "33_accruals.html#sec-accruals-mispricing",
    "title": "26  Accruals, Earnings Persistence, and Market Efficiency",
    "section": "26.4 Market Pricing of Earnings Components",
    "text": "26.4 Market Pricing of Earnings Components\nThe persistence analysis establishes that accruals are less persistent than cash flows. The market efficiency question is whether stock prices reflect this difference. If investors naïvely treat both components as equally persistent, what Sloan (1996) termed “earnings fixation,” then a predictable component of future returns should be related to the accrual composition of current earnings.\n\n26.4.1 Theoretical Framework\nSuppose the market prices earnings using:\n\\[\nP_t = \\frac{1}{r} \\left[ \\hat{\\gamma}^* \\cdot E_t \\right]\n\\tag{26.8}\\]\nwhere \\(\\hat{\\gamma}^*\\) is the market’s perceived persistence of aggregate earnings. Rational pricing requires:\n\\[\nP_t = \\frac{1}{r} \\left[ \\hat{\\gamma}_a \\cdot ACC_t + \\hat{\\gamma}_c \\cdot CFO_t \\right]\n\\tag{26.9}\\]\nwith \\(\\hat{\\gamma}_a &lt; \\hat{\\gamma}_c\\). If the market uses Equation 26.8 instead of Equation 26.9, then high-accrual firms are overpriced (because the market overestimates the persistence of their accrual-heavy earnings) and low-accrual firms are underpriced.\n\n\n26.4.2 The Abel–Mishkin Test\nTesting whether market pricing coefficients match rational forecasting coefficients can be done via the approach of Abel (1983), which avoids the complexity of the full Abel and Mishkin (1983), Mishkin (1983) and Mishkin (2007) nonlinear system. The intuition is simple: if lagged earnings components are mispriced, this should be detectable in a regression of future abnormal returns on those components:\n\\[\nAR_{i,t+1} = \\alpha + \\beta_a \\cdot ACC_{i,t} + \\beta_c \\cdot CFO_{i,t} + \\eta_{i,t}\n\\tag{26.10}\\]\nUnder efficient pricing, \\(\\beta_a = \\beta_c = 0\\): lagged public information should have no predictive power for abnormal returns. A finding of \\(\\beta_a &lt; 0\\) implies that the market overprices accruals and subsequently corrects.\n\n# Abel-Mishkin style test\nmod_am1 = smf.ols(\"size_adj_ret ~ acc + cfo\", data=panel).fit()\nmod_am2 = smf.ols(\"size_adj_ret ~ acc_decile + cfo_decile\", data=panel).fit()\n\nam_table = pd.DataFrame({\n    \"(1) Continuous\": {\n        \"Intercept\": f\"{mod_am1.params['Intercept']:.4f} ({mod_am1.bse['Intercept']:.4f})\",\n        \"Accruals\": f\"{mod_am1.params['acc']:.4f} ({mod_am1.bse['acc']:.4f})\",\n        \"Cash Flows\": f\"{mod_am1.params['cfo']:.4f} ({mod_am1.bse['cfo']:.4f})\",\n        \"N\": f\"{int(mod_am1.nobs):,}\",\n        \"R²\": f\"{mod_am1.rsquared:.4f}\",\n    },\n    \"(2) Decile Ranks\": {\n        \"Intercept\": f\"{mod_am2.params['Intercept']:.4f} ({mod_am2.bse['Intercept']:.4f})\",\n        \"Accruals\": f\"{mod_am2.params['acc_decile']:.4f} ({mod_am2.bse['acc_decile']:.4f})\",\n        \"Cash Flows\": f\"{mod_am2.params['cfo_decile']:.4f} ({mod_am2.bse['cfo_decile']:.4f})\",\n        \"N\": f\"{int(mod_am2.nobs):,}\",\n        \"R²\": f\"{mod_am2.rsquared:.4f}\",\n    },\n})\nam_table\n\n\n\nTable 26.6: Abnormal return regressions. Column (1) regresses size-adjusted returns on lagged accruals and cash flows. Column (2) uses decile ranks. A negative coefficient on accruals indicates that the market overprices the accrual component and corrects in the subsequent period.\n\n\n\n\n\n\n\n\n\n\n(1) Continuous\n(2) Decile Ranks\n\n\n\n\nIntercept\n0.0263 (0.0058)\n0.0962 (0.0161)\n\n\nAccruals\n-0.5686 (0.0920)\n-0.0091 (0.0016)\n\n\nCash Flows\n-0.3538 (0.0575)\n-0.0087 (0.0016)\n\n\nN\n7,000\n7,000\n\n\nR²\n0.0067\n0.0055\n\n\n\n\n\n\n\n\n\n\n\n\n26.4.3 Interpretation for Vietnam\nThe limited attention hypothesis of Hirshleifer and Teoh (2003) provides a particularly appealing explanation for Vietnamese markets. When retail investors dominate trading, as in Vietnam, where retail participation exceeds 80% of daily turnover, information processing capacity is limited. Investors may anchor on headline earnings without decomposing into accrual and cash flow components. The absence of short-selling mechanisms (Vietnam did not introduce covered short selling until 2024) further impedes the correction of overpricing, potentially allowing the accrual anomaly to persist longer than in developed markets.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Accruals, Earnings Persistence, and Market Efficiency</span>"
    ]
  },
  {
    "objectID": "33_accruals.html#sec-accruals-anomaly",
    "href": "33_accruals.html#sec-accruals-anomaly",
    "title": "26  Accruals, Earnings Persistence, and Market Efficiency",
    "section": "26.5 The Accrual Anomaly",
    "text": "26.5 The Accrual Anomaly\n\n26.5.1 Portfolio Returns by Accrual Decile\nFollowing the portfolio approach of Sloan (1996), we compute mean size-adjusted returns for each accrual decile. If the market misprices accruals, we expect a monotonically decreasing pattern: low-accrual firms (decile 1) earn positive abnormal returns, while high-accrual firms (decile 10) earn negative abnormal returns.\n\n# Compute annual portfolio returns (equal-weighted within decile-year)\nport_rets = (\n    panel\n    .groupby([\"year\", \"acc_decile\"])[\"size_adj_ret\"]\n    .mean()\n    .reset_index()\n)\n\n# Regression approach: decile dummies (no intercept)\nport_rets[\"acc_decile_str\"] = \"D\" + port_rets[\"acc_decile\"].astype(str).str.zfill(2)\nmod_port = smf.ols(\n    \"size_adj_ret ~ C(acc_decile, Treatment(reference=0)) - 1\",\n    data=port_rets.assign(acc_decile=port_rets[\"acc_decile\"].astype(\"category\")),\n).fit()\n\n# Mean returns by decile\ndecile_means = (\n    port_rets\n    .groupby(\"acc_decile\")[\"size_adj_ret\"]\n    .agg([\"mean\", \"std\", \"count\"])\n    .round(4)\n)\ndecile_means[\"t_stat\"] = (\n    decile_means[\"mean\"] / (decile_means[\"std\"] / np.sqrt(decile_means[\"count\"]))\n).round(2)\ndecile_means.columns = [\"Mean Return\", \"Std Dev\", \"N Years\", \"t-stat\"]\n\n# Hedge portfolio\nhedge_series = (\n    port_rets.query(\"acc_decile == 1\").set_index(\"year\")[\"size_adj_ret\"]\n    - port_rets.query(\"acc_decile == 10\").set_index(\"year\")[\"size_adj_ret\"]\n)\nhedge_mean = hedge_series.mean()\nhedge_t = hedge_mean / (hedge_series.std() / np.sqrt(len(hedge_series)))\n\nprint(f\"Hedge portfolio (D1 - D10): {hedge_mean:.4f}\")\nprint(f\"t-statistic: {hedge_t:.2f}\")\nprint()\ndecile_means\n\nHedge portfolio (D1 - D10): 0.0397\nt-statistic: 2.21\n\n\n\n\n\nTable 26.7: Mean size-adjusted returns by accrual decile. The hedge portfolio (long decile 1, short decile 10) return and its t-statistic are reported at the bottom.\n\n\n\n\n\n\n\n\n\n\nMean Return\nStd Dev\nN Years\nt-stat\n\n\nacc_decile\n\n\n\n\n\n\n\n\n1\n0.0242\n0.0471\n14\n1.92\n\n\n2\n0.0135\n0.0370\n14\n1.37\n\n\n3\n-0.0008\n0.0596\n14\n-0.05\n\n\n4\n-0.0111\n0.0317\n14\n-1.31\n\n\n5\n0.0034\n0.0337\n14\n0.38\n\n\n6\n0.0086\n0.0422\n14\n0.76\n\n\n7\n-0.0097\n0.0318\n14\n-1.14\n\n\n8\n0.0005\n0.0580\n14\n0.03\n\n\n9\n-0.0280\n0.0337\n14\n-3.11\n\n\n10\n-0.0155\n0.0354\n14\n-1.64\n\n\n\n\n\n\n\n\n\n\n\n\n26.5.2 Visualizing the Anomaly\n\ndecile_plot = port_rets.groupby(\"acc_decile\")[\"size_adj_ret\"].mean()\n\nfig, ax = plt.subplots()\ncolors = [\"steelblue\" if r &gt; 0 else \"firebrick\" for r in decile_plot.values]\nax.bar(decile_plot.index, decile_plot.values, color=colors, edgecolor=\"white\")\nax.axhline(0, color=\"black\", linewidth=0.5)\nax.set_xlabel(\"Accrual Decile\")\nax.set_ylabel(\"Mean Size-Adjusted Return\")\nax.set_xticks(range(1, 11))\nax.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1, decimals=1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 26.4: Mean size-adjusted returns by accrual decile. The downward slope from low-accrual to high-accrual deciles is consistent with market overpricing of the accrual component of earnings.\n\n\n\n\n\n\n\n26.5.3 Time-Series Stability\nA critical question for any anomaly is whether it persists over time or is concentrated in specific sub-periods. Figure 26.5 plots the hedge portfolio return by year.\n\nfig, ax = plt.subplots()\nax.bar(hedge_series.index, hedge_series.values,\n       color=[\"steelblue\" if v &gt; 0 else \"firebrick\" for v in hedge_series.values],\n       edgecolor=\"white\")\nax.axhline(hedge_series.mean(), color=\"black\", linestyle=\"--\", linewidth=1,\n           label=f\"Mean = {hedge_series.mean():.1%}\")\nax.axhline(0, color=\"black\", linewidth=0.5)\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Hedge Portfolio Return\")\nax.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1, decimals=0))\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 26.5: Annual hedge portfolio returns (decile 1 minus decile 10) over time. Positive values indicate that low-accrual firms outperformed high-accrual firms. Year-to-year variation is substantial, reflecting the noisy nature of annual return measurement.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Accruals, Earnings Persistence, and Market Efficiency</span>"
    ]
  },
  {
    "objectID": "33_accruals.html#sec-accruals-discussion",
    "href": "33_accruals.html#sec-accruals-discussion",
    "title": "26  Accruals, Earnings Persistence, and Market Efficiency",
    "section": "26.6 Discussion and Contemporary Perspectives",
    "text": "26.6 Discussion and Contemporary Perspectives\n\n26.6.1 Alternative Explanations\nThe accrual anomaly has generated an extensive debate in the accounting and finance literature. Several alternative explanations merit consideration in the Vietnamese context:\nRisk-based explanations. Khan (2008) argues that high-accrual firms may earn lower returns because they are less risky, not because they are mispriced. If the CAPM or multi-factor models fail to capture the true risk structure, what appears as an anomaly may simply be compensation for omitted risk factors. This explanation is harder to evaluate in Vietnam given the limited history of factor models calibrated to local data.\nGrowth and investment. Fairfield, Whisenant, and Yohn (2003) show that the low persistence of accruals extends to long-term accruals (capital expenditures), suggesting that the phenomenon reflects diminishing marginal returns to investment rather than earnings manipulation. In Vietnam’s high-growth environment, this channel may be particularly important as firms invest aggressively during credit booms.\nData artifacts. Kraft, Leone, and Wasley (2006) identify sensitivity to outliers and look-ahead bias in the original evidence. Vietnamese financial data, with its shorter history and higher incidence of reporting errors, warrants extra vigilance on these points.\nInstitutional constraints. Short-selling restrictions, prevalent in Vietnam until recently, prevent arbitrageurs from correcting overpricing. Combined with high transaction costs and limited institutional investor participation, this creates an environment where mispricing can be sustained even if sophisticated investors identify it.\n\n\n26.6.2 State-Owned Enterprises and Accrual Quality\nA distinctive feature of Vietnamese markets is the prevalence of SOEs and former SOEs. Several testable hypotheses arise:\n\nPolitical smoothing: SOE managers may smooth earnings to meet government targets, generating accruals that differ in nature from those of private firms.\nAudit quality: Khanh and Nguyen (2018) find that audit quality varies systematically with ownership structure in Vietnam. Lower audit quality may allow greater accrual manipulation.\nInformation environment: SOEs may have weaker voluntary disclosure, increasing the information asymmetry between accruals and cash flows.\n\nThese hypotheses suggest that the accrual anomaly may exhibit cross-sectional variation along ownership dimensions unique to Vietnam.\n\n\n26.6.3 Methodological Considerations\nWhen implementing this analysis with actual Vietnamese data, several practical issues require attention:\nStandard errors. Pooled OLS standard errors assume independence across firm-years. In practice, both cross-sectional correlation (common shocks within a year) and time-series correlation (firm-level persistence) are present. Clustering standard errors by firm and year (two-way clustering) or using the Fama-MacBeth procedure (Fama and MacBeth 1973) with Newey-West corrections is recommended.\nVAS-specific items. Vietnamese financial statements include items without direct equivalents in Compustat, such as chi phí trả trước (prepaid expenses reported separately from current assets in some VAS templates). Researchers must map these carefully.\nPrice limits. Daily price limits on HOSE (\\(\\pm\\) 7%) and HNX (\\(\\pm\\) 10%) can truncate return distributions and create serial correlation in measured returns. For return measurement over 12-month windows, this is less problematic but should be documented.\nExchange effects. HOSE-listed firms tend to be larger and more liquid than HNX-listed firms. Separate analysis by exchange, or inclusion of exchange fixed effects, can help ensure results are not driven by liquidity differences.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Accruals, Earnings Persistence, and Market Efficiency</span>"
    ]
  },
  {
    "objectID": "33_accruals.html#summary",
    "href": "33_accruals.html#summary",
    "title": "26  Accruals, Earnings Persistence, and Market Efficiency",
    "section": "26.7 Summary",
    "text": "26.7 Summary\nThis chapter examined accrual accounting and its implications for earnings persistence and market efficiency in the Vietnamese context. The key findings are:\n\nAccruals are the portion of earnings not backed by contemporaneous cash flows. They arise naturally from accrual accounting but also embed estimation errors and managerial discretion.\nSimulation analysis demonstrates that when accounting estimates deviate from economic truth, accrual persistence falls below cash flow persistence—providing a structural explanation for the empirical regularity.\nPersistence regressions confirm that the accrual component of earnings is less persistent than the cash flow component, a finding that holds across industries.\nMarket pricing tests suggest that stock prices in Vietnamese markets may not fully incorporate the differential persistence of earnings components, giving rise to the accrual anomaly.\n\nThe Vietnamese institutional environment provides rich variation for future research on the interplay between accounting quality, investor sophistication, and market efficiency.\n\n\n\n\n\n\n\nAbel, Andrew B. 1983. “Optimal Investment Under Uncertainty.” The American Economic Review 73 (1): 228–33.\n\n\nAbel, Andrew B, and Frederic S Mishkin. 1983. “An Integrated View of Tests of Rationality, Market Efficiency and the Short-Run Neutrality of Monetary Policy.” Journal of Monetary Economics 11 (1): 3–24.\n\n\nDechow, Patricia, Weili Ge, and Catherine Schrand. 2010. “Understanding Earnings Quality: A Review of the Proxies, Their Determinants and Their Consequences.” Journal of Accounting and Economics 50 (2-3): 344–401.\n\n\nFairfield, Patricia M, J Scott Whisenant, and Teri Lombardi Yohn. 2003. “Accrued Earnings and Growth: Implications for Future Profitability and Market Mispricing.” The Accounting Review 78 (1): 353–71.\n\n\nFama, Eugene F, and James D MacBeth. 1973. “Risk, Return, and Equilibrium: Empirical Tests.” Journal of Political Economy 81 (3): 607–36.\n\n\nHirshleifer, David, and Siew Hong Teoh. 2003. “Limited Attention, Information Disclosure, and Financial Reporting.” Journal of Accounting and Economics 36 (1-3): 337–86.\n\n\nHribar, Paul, and Daniel W Collins. 2002. “Errors in Estimating Accruals: Implications for Empirical Research.” Journal of Accounting Research 40 (1): 105–34.\n\n\nKhan, Mozaffar. 2008. “Are Accruals Mispriced? Evidence from Tests of an Intertemporal Capital Asset Pricing Model.” Journal of Accounting and Economics 45 (1): 55–77.\n\n\nKhanh, Hoang Thi Mai, and Vinh Khuong Nguyen. 2018. “Audit Quality, Firm Characteristics and Real Earnings Management: The Case of Listed Vietnamese Firms.” International Journal of Economics and Financial Issues 8 (4): 243.\n\n\nKraft, Arthur, Andrew J Leone, and Charles Wasley. 2006. “An Analysis of the Theories and Explanations Offered for the Mispricing of Accruals and Accrual Components.” Journal of Accounting Research 44 (2): 297–339.\n\n\nMishkin, Frederic S. 1983. “Are Market Forecasts Rational?” In A Rational Expectations Approach to Macroeconometrics: Testing Policy Ineffectiveness and Efficient-Markets Models, 59–75. University of Chicago Press.\n\n\n———. 2007. A Rational Expectations Approach to Macroeconometrics: Testing Policy Ineffectiveness and Efficient-Markets Models. University of Chicago Press.\n\n\nSloan, Richard G. 1996. “Do Stock Prices Fully Reflect Information in Accruals and Cash Flows about Future Earnings?” Accounting Review, 289–315.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Accruals, Earnings Persistence, and Market Efficiency</span>"
    ]
  },
  {
    "objectID": "34_earning_management.html",
    "href": "34_earning_management.html",
    "title": "27  Earnings Management: Detection and Measurement",
    "section": "",
    "text": "27.0.1 Defining Earnings Management\nEarnings management refers to the purposeful intervention by managers in the financial reporting process to achieve outcomes that serve private objectives, whether to meet analyst forecasts, trigger bonus thresholds, avoid covenant violations, or influence equity valuations. While the concept is intuitive, its rigorous detection poses one of the most enduring methodological challenges in empirical accounting research.\nThe difficulty is fundamental: researchers observe total accruals, which combine a legitimate component (reflecting genuine economic activity) with a discretionary component (reflecting managerial intervention). Separating these two components requires a model of what accruals should be absent manipulation, what the literature calls non-discretionary accruals (NDA). Any residual, the gap between observed accruals and the model’s prediction, is then attributed to managerial discretion. The quality of the detection method therefore hinges entirely on the quality of this model.\nThis chapter examines three foundational approaches to measuring earnings management (i.e., the Healy (1985) model, the Jones (1991) model, and the modified Jones model of Dechow, Sloan, and Sweeney (1995)) and evaluates their statistical properties using simulation. We then adapt the analysis to the institutional setting of Vietnam, where distinct governance structures, accounting standards, and enforcement regimes create both unique incentives for earnings management and unique challenges for its detection.\nA useful taxonomy distinguishes three forms:",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Earnings Management: Detection and Measurement</span>"
    ]
  },
  {
    "objectID": "34_earning_management.html#sec-models",
    "href": "34_earning_management.html#sec-models",
    "title": "27  Earnings Management: Detection and Measurement",
    "section": "27.1 Models of Non-Discretionary Accruals",
    "text": "27.1 Models of Non-Discretionary Accruals\n\n27.1.1 Notation and Setup\nLet \\(i\\) index firms and \\(t\\) index fiscal years. Define:\n\n\n\nTable 27.1: Notation for earnings management models\n\n\n\n\n\n\n\n\n\nSymbol\nDefinition\n\n\n\n\n\\(TA_{i,t}\\)\nTotal accruals (scaled by lagged assets)\n\n\n\\(NDA_{i,t}\\)\nNon-discretionary accruals (model prediction)\n\n\n\\(DA_{i,t}\\)\nDiscretionary accruals: \\(DA_{i,t} = TA_{i,t} - NDA_{i,t}\\)\n\n\n\\(A_{i,t}\\)\nTotal assets\n\n\n\\(\\Delta Rev_{i,t}\\)\nChange in revenues, scaled by \\(A_{i,t-1}\\)\n\n\n\\(\\Delta Rec_{i,t}\\)\nChange in net receivables, scaled by \\(A_{i,t-1}\\)\n\n\n\\(PPE_{i,t}\\)\nGross property, plant, and equipment, scaled by \\(A_{i,t-1}\\)\n\n\n\\(PART_{i,t}\\)\nIndicator equal to 1 for the test (event) year\n\n\n\n\n\n\nTotal accruals are computed using the balance sheet approach:\n\\[\nTA_{i,t} = \\frac{(\\Delta CA_{i,t} - \\Delta Cash_{i,t}) - (\\Delta CL_{i,t} - \\Delta STD_{i,t}) - Dep_{i,t}}{A_{i,t-1}}\n\\tag{27.1}\\]\nwhere \\(\\Delta CA\\) is the change in current assets, \\(\\Delta Cash\\) is the change in cash, \\(\\Delta CL\\) is the change in current liabilities, \\(\\Delta STD\\) is the change in short-term debt, and \\(Dep\\) is depreciation expense.\n\n\n27.1.2 Five Models\nWe implement five models, each estimating \\(NDA\\) during a firm-specific estimation window and computing \\(DA\\) for the test year as the residual.\nModel 1: Healy (1985). Non-discretionary accruals equal the mean of total accruals during the estimation period:\n\\[\nNDA^{Healy}_{i,t} = \\frac{1}{T} \\sum_{s \\in \\text{est}} TA_{i,s}\n\\tag{27.2}\\]\nThis is the simplest possible benchmark. Its limitation is obvious: it treats all time-variation in accruals as discretionary, even variation driven by changes in the firm’s economic environment.\nModel 2: DeAngelo (1986). Non-discretionary accruals equal last period’s total accruals:\n\\[\nNDA^{DeAngelo}_{i,t} = TA_{i,t-1}\n\\tag{27.3}\\]\nThis is equivalent to assuming that the change in total accruals is entirely discretionary. It performs well when accruals follow a random walk, but poorly when they exhibit mean-reversion or trend.\nModel 3: Jones (1991). This model controls for changes in a firm’s economic environment by regressing accruals on revenue changes and the level of fixed assets:\n\\[\nTA_{i,t} = \\alpha_1 \\frac{1}{A_{i,t-1}} + \\alpha_2 \\Delta Rev_{i,t} + \\alpha_3 PPE_{i,t} + \\varepsilon_{i,t}\n\\tag{27.4}\\]\nThe parameters \\(\\hat{\\alpha}_1, \\hat{\\alpha}_2, \\hat{\\alpha}_3\\) are estimated on the estimation-period data. Non-discretionary accruals for the test year are the fitted values from this regression applied to test-year covariates.\nThe economic logic is that revenue growth generates legitimate working capital accruals (higher receivables and inventory), while fixed assets proxy for non-discretionary depreciation charges. The intercept is scaled by lagged assets rather than included as a conventional constant, following Jones (1991).\nModel 4: Modified Jones (Dechow, Sloan, and Sweeney 1995). The modification adjusts revenue changes for changes in receivables during the test year, on the premise that credit revenue growth is more susceptible to manipulation than cash revenue growth:\n\\[\nNDA^{ModJones}_{i,t} = \\hat{\\alpha}_1 \\frac{1}{A_{i,t-1}} + \\hat{\\alpha}_2 (\\Delta Rev_{i,t} - \\Delta Rec_{i,t}) + \\hat{\\alpha}_3 PPE_{i,t}\n\\tag{27.5}\\]\nThe coefficients are still estimated from the unadjusted Jones model (Equation 27.4) on estimation-period data, but receivables are subtracted from revenues only when computing fitted values for the test year.\nModel 5: Industry Model. This model assumes that the common component of accruals within an industry captures non-discretionary variation:\n\\[\nTA_{i,t} = \\phi_0 + \\phi_1 \\cdot \\widetilde{TA}_{j,t} + \\eta_{i,t}\n\\tag{27.6}\\]\nwhere \\(\\widetilde{TA}_{j,t}\\) is the median total accrual across all firms in industry \\(j\\) (excluding firm \\(i\\)), estimated during the estimation period.\n\n\n27.1.3 Implementation\n\ndef calc_accruals(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute total accruals using the balance sheet approach.\n    \n    Expected columns: gvkey/ticker, year/fyear, at, act, che, lct, dlc,\n                      dp, sale, rect, ppegt\n    \"\"\"\n    firm_col = \"ticker\" if \"ticker\" in df.columns else \"gvkey\"\n    year_col = \"year\" if \"year\" in df.columns else \"fyear\"\n    \n    df = df.sort_values([firm_col, year_col]).copy()\n    \n    # Lagged values and changes\n    g = df.groupby(firm_col)\n    df[\"lag_at\"] = g[\"at\"].shift(1)\n    df[\"d_ca\"] = g[\"act\"].diff()\n    df[\"d_cash\"] = g[\"che\"].diff()\n    df[\"d_cl\"] = g[\"lct\"].diff()\n    df[\"d_std\"] = g[\"dlc\"].diff()\n    df[\"d_rev\"] = g[\"sale\"].diff()\n    df[\"d_rec\"] = g[\"rect\"].diff()\n    \n    # Total accruals (raw)\n    df[\"acc_raw\"] = (df[\"d_ca\"] - df[\"d_cash\"] - df[\"d_cl\"] + df[\"d_std\"]) - df[\"dp\"]\n    \n    return df\n\n\ndef fit_healy(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Healy (1985): NDA = mean accruals in estimation period.\"\"\"\n    est_mean = df.loc[~df[\"part\"], \"acc_at\"].mean()\n    df = df.copy()\n    df[\"nda_healy\"] = est_mean\n    df[\"da_healy\"] = df[\"acc_at\"] - df[\"nda_healy\"]\n    return df\n\n\ndef fit_deangelo(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"DeAngelo (1986): NDA = prior-period total accruals.\"\"\"\n    df = df.copy()\n    df[\"nda_deangelo\"] = df[\"acc_at\"].shift(1)\n    df[\"da_deangelo\"] = df[\"acc_at\"] - df[\"nda_deangelo\"]\n    return df\n\n\ndef fit_jones(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Jones (1991): Regression-based NDA controlling for revenue and PPE.\"\"\"\n    df = df.copy()\n    est = df[~df[\"part\"]].dropna(subset=[\"acc_at\", \"one_at\", \"d_rev_at\", \"ppe_at\"])\n    \n    if len(est) &lt; 5:\n        df[\"nda_jones\"] = np.nan\n        df[\"da_jones\"] = np.nan\n        return df\n    \n    y = est[\"acc_at\"]\n    X = est[[\"one_at\", \"d_rev_at\", \"ppe_at\"]]\n    model = sm.OLS(y, X).fit()\n    \n    pred_X = df[[\"one_at\", \"d_rev_at\", \"ppe_at\"]].copy()\n    df[\"nda_jones\"] = model.predict(pred_X)\n    df[\"da_jones\"] = df[\"acc_at\"] - df[\"nda_jones\"]\n    return df\n\n\ndef fit_mod_jones(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Modified Jones (Dechow et al., 1995): Adjust revenues for receivables.\"\"\"\n    df = df.copy()\n    est = df[~df[\"part\"]].dropna(subset=[\"acc_at\", \"one_at\", \"d_rev_at\", \"ppe_at\"])\n    \n    if len(est) &lt; 5:\n        df[\"nda_mod_jones\"] = np.nan\n        df[\"da_mod_jones\"] = np.nan\n        return df\n    \n    # Estimate on unadjusted Jones model\n    y = est[\"acc_at\"]\n    X = est[[\"one_at\", \"d_rev_at\", \"ppe_at\"]]\n    model = sm.OLS(y, X).fit()\n    \n    # Predict using adjusted revenue (subtract receivable changes)\n    pred_X = df[[\"one_at\", \"d_rev_alt_at\", \"ppe_at\"]].copy()\n    pred_X.columns = [\"one_at\", \"d_rev_at\", \"ppe_at\"]\n    df[\"nda_mod_jones\"] = model.predict(pred_X)\n    df[\"da_mod_jones\"] = df[\"acc_at\"] - df[\"nda_mod_jones\"]\n    return df\n\n\ndef fit_industry(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Industry model: NDA = f(industry median accruals).\"\"\"\n    df = df.copy()\n    est = df[~df[\"part\"]].dropna(subset=[\"acc_at\", \"acc_ind\"])\n    \n    if len(est) &lt; 5:\n        df[\"nda_industry\"] = np.nan\n        df[\"da_industry\"] = np.nan\n        return df\n    \n    y = est[\"acc_at\"]\n    X = sm.add_constant(est[\"acc_ind\"])\n    model = sm.OLS(y, X).fit()\n    \n    pred_X = sm.add_constant(df[\"acc_ind\"])\n    df[\"nda_industry\"] = model.predict(pred_X)\n    df[\"da_industry\"] = df[\"acc_at\"] - df[\"nda_industry\"]\n    return df\n\n\ndef prepare_model_vars(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Add scaled variables needed by the five NDA models.\"\"\"\n    df = calc_accruals(df)\n    firm_col = \"ticker\" if \"ticker\" in df.columns else \"gvkey\"\n    \n    df[\"sic2\"] = df[\"sic\"].astype(str).str[:2]\n    df[\"acc_at\"] = df[\"acc_raw\"] / df[\"lag_at\"]\n    df[\"one_at\"] = 1.0 / df[\"lag_at\"]\n    df[\"d_rev_at\"] = df[\"d_rev\"] / df[\"lag_at\"]\n    df[\"d_rev_alt_at\"] = (df[\"d_rev\"] - df[\"d_rec\"]) / df[\"lag_at\"]\n    df[\"ppe_at\"] = df[\"ppegt\"] / df[\"lag_at\"]\n    \n    # Industry median accruals (estimation period only)\n    # est_acc = df.loc[~df[\"part\"], [\"sic2\", \"acc_at\"]].copy()\n    est_acc = df.loc[df[\"part\"] == False, [\"sic2\", \"acc_at\"]].copy()\n    ind_median = est_acc.groupby(\"sic2\")[\"acc_at\"].median().rename(\"acc_ind\")\n    df = df.merge(ind_median, on=\"sic2\", how=\"left\")\n    \n    return df\n\n\ndef get_all_nda(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply all five NDA models on a firm-by-firm basis.\n    Returns a DataFrame with DA columns for each model.\n    \"\"\"\n    firm_col = \"ticker\" if \"ticker\" in df.columns else \"gvkey\"\n    year_col = \"year\" if \"year\" in df.columns else \"fyear\"\n    \n    df_mod = prepare_model_vars(df)\n    df_mod[\"part\"] = df_mod[\"part\"].astype(bool)\n    \n    results = []\n    for firm, group in df_mod.groupby(firm_col):\n        g = group.sort_values(year_col).copy()\n        g = fit_healy(g)\n        g = fit_deangelo(g)\n        g = fit_jones(g)\n        g = fit_mod_jones(g)\n        g = fit_industry(g)\n        results.append(g)\n    \n    return pd.concat(results, ignore_index=True)\n\n\n\n27.1.4 The Salkever (1976) Correction\nAn important but underappreciated methodological issue arises when computing standard errors for discretionary accruals under the Jones-type models. In the standard two-stage procedure, the researcher first estimates the Jones model on the estimation period, then computes discretionary accruals for the test year as a prediction error. But the standard error of a prediction error is larger than the standard error of a fitted residual, because it incorporates parameter uncertainty from the first-stage estimation. Ignoring this distinction leads to understated standard errors and inflated rejection rates,exactly the problem documented in Dechow, Sloan, and Sweeney (1995).\nSalkever (1976) provides an elegant solution: run a single regression on the combined estimation and test periods, including a dummy variable \\(PART\\) for the test year. The coefficient on \\(PART\\) equals the prediction error (discretionary accruals), and its standard error correctly accounts for first-stage estimation uncertainty.\nFor the Jones model, the Salkever single-stage regression is:\n\\[\nTA_{i,t} = \\alpha_1 \\frac{1}{A_{i,t-1}} + \\alpha_2 \\Delta Rev_{i,t} + \\alpha_3 PPE_{i,t} + \\delta \\cdot PART_{i,t} + u_{i,t}\n\\tag{27.7}\\]\nThe coefficient \\(\\hat{\\delta}\\) is numerically identical to the two-stage \\(DA\\) estimate, but its standard error \\(\\text{se}(\\hat{\\delta})\\) is the correct prediction error standard error.\n\ndef demonstrate_salkever(df_firm: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    For a single firm, show that the two-stage Jones DA equals\n    the Salkever one-stage coefficient on PART.\n    \"\"\"\n    df = prepare_model_vars(df_firm)\n    needed = [\"acc_at\", \"one_at\", \"d_rev_at\", \"ppe_at\", \"part\"]\n    df = df.dropna(subset=needed).copy()\n    \n    # ── Two-stage approach ──\n    est = df[~df[\"part\"]]\n    y_est = est[\"acc_at\"]\n    X_est = est[[\"one_at\", \"d_rev_at\", \"ppe_at\"]]\n    fm_stage1 = sm.OLS(y_est, X_est).fit()\n    \n    df[\"nda_two_stage\"] = fm_stage1.predict(df[[\"one_at\", \"d_rev_at\", \"ppe_at\"]])\n    df[\"da_two_stage\"] = df[\"acc_at\"] - df[\"nda_two_stage\"]\n    \n    # Test-year DA from two-stage\n    da_two_stage = df.loc[df[\"part\"], \"da_two_stage\"].values\n    \n    # ── Salkever one-stage ──\n    df[\"part_float\"] = df[\"part\"].astype(float)\n    y_full = df[\"acc_at\"]\n    X_full = df[[\"one_at\", \"d_rev_at\", \"ppe_at\", \"part_float\"]]\n    fm_salkever = sm.OLS(y_full, X_full).fit()\n    \n    da_salkever = fm_salkever.params[\"part_float\"]\n    se_two_stage_wrong = np.nan  # two-stage doesn't give correct SE\n    se_salkever = fm_salkever.bse[\"part_float\"]\n    \n    return pd.DataFrame({\n        \"Method\": [\"Two-stage Jones\", \"Salkever one-stage\"],\n        \"DA estimate\": [da_two_stage[0] if len(da_two_stage) else np.nan,\n                        da_salkever],\n        \"Correct SE\": [\"Not available\", f\"{se_salkever:.6f}\"],\n        \"t-statistic\": [\"Biased\", f\"{fm_salkever.tvalues['part_float']:.4f}\"],\n    })",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Earnings Management: Detection and Measurement</span>"
    ]
  },
  {
    "objectID": "34_earning_management.html#sec-type1",
    "href": "34_earning_management.html#sec-type1",
    "title": "27  Earnings Management: Detection and Measurement",
    "section": "27.2 Type I Error Under the Null Hypothesis",
    "text": "27.2 Type I Error Under the Null Hypothesis\n\n27.2.1 Experimental Design\nTo evaluate whether the five models produce well-calibrated test statistics, we conduct a simulation experiment parallel to Table 2 of Dechow, Sloan, and Sweeney (1995). The procedure is:\n\nGenerate a panel of \\(N\\) firms, each with \\(T\\) years of financial statement data.\nFor each firm, randomly designate one year as the test year (\\(PART = 1\\)). By construction, no earnings management occurs in this year.\nEstimate discretionary accruals using each of the five models.\nRegress \\(DA\\) on \\(PART\\) for each firm and record whether the null \\(H_0:\n\\delta = 0\\) is rejected at the 5% and 1% significance levels.\nCompute the rejection rate across all \\(N\\) firms.\n\nIf the model is well-specified, rejection rates should equal the nominal test size (5% or 1%). Systematic over-rejection indicates that the model produces biased test statistics, a critical flaw for research that relies on these measures to draw causal inferences.\n\n\n27.2.2 Data Generation\nWe generate synthetic panel data that preserves the key cross-sectional and time-series properties of Vietnamese listed firms while allowing us to know with certainty that no manipulation exists.\n\ndef generate_em_panel(\n    n_firms: int = 500,\n    n_years: int = 15,\n    seed: int = 2024,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate a synthetic panel of Vietnamese-style financial data.\n    No earnings management is present by construction.\n    \n    The data generation process captures:\n    - AR(1) revenue process\n    - Accruals driven by revenue growth and PPE levels\n    - Industry-level common shocks\n    - SOE/non-SOE heterogeneity\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    industries = [10, 20, 25, 41, 46, 47, 52, 62, 64, 68]\n    \n    records = []\n    for i in range(n_firms):\n        # Firm characteristics\n        sic = rng.choice(industries)\n        is_soe = int(rng.random() &lt; 0.30)\n        base_assets = rng.lognormal(mean=12, sigma=1.5)  # VND billions\n        growth_rate = rng.normal(0.08, 0.04)\n        \n        # Jones model parameters (firm-specific true DGP)\n        true_alpha1 = rng.normal(0, 0.02)\n        true_alpha2 = rng.normal(0.06, 0.03)   # revenue-accrual sensitivity\n        true_alpha3 = rng.normal(-0.04, 0.02)  # depreciation effect\n        \n        at_prev = base_assets\n        sale_prev = base_assets * rng.uniform(0.5, 1.5)\n        rect_prev = sale_prev * rng.uniform(0.05, 0.25)\n        \n        for t in range(n_years):\n            year = 2009 + t\n            \n            # Evolve fundamentals\n            at = at_prev * (1 + growth_rate + rng.normal(0, 0.05))\n            sale = sale_prev * (1 + rng.normal(0.06, 0.08))\n            rect = sale * rng.uniform(0.05, 0.25)\n            ppegt = at * rng.uniform(0.3, 0.7)\n            \n            # Generate accruals from true Jones DGP + noise\n            d_rev = sale - sale_prev\n            one_at = 1.0 / at_prev\n            d_rev_at = d_rev / at_prev\n            ppe_at = ppegt / at_prev\n            \n            acc_at = (true_alpha1 * one_at \n                      + true_alpha2 * d_rev_at \n                      + true_alpha3 * ppe_at\n                      + rng.normal(0, 0.03))\n            \n            acc_raw = acc_at * at_prev\n            \n            # Reverse-engineer balance sheet items consistent with accruals\n            dp = ppegt * rng.uniform(0.05, 0.12)\n            d_cl = rng.normal(0, at * 0.02)\n            d_std = rng.normal(0, at * 0.01)\n            d_cash = rng.normal(0, at * 0.02)\n            d_ca = acc_raw + dp + d_cl - d_std + d_cash\n            \n            act = at * rng.uniform(0.3, 0.6)\n            che = act * rng.uniform(0.05, 0.2)\n            lct = at * rng.uniform(0.15, 0.35)\n            dlc = lct * rng.uniform(0.1, 0.4)\n            ni = sale * rng.uniform(0.03, 0.12)\n            ib = ni\n            \n            records.append({\n                \"ticker\": f\"VN{i:04d}\",\n                \"fyear\": year,\n                \"at\": at,\n                \"act\": act,\n                \"che\": che,\n                \"lct\": lct,\n                \"dlc\": dlc,\n                \"dp\": dp,\n                \"sale\": sale,\n                \"rect\": rect,\n                \"ppegt\": ppegt,\n                \"ni\": ni,\n                \"ib\": ib,\n                \"sic\": sic,\n                \"is_soe\": is_soe,\n            })\n            \n            at_prev = at\n            sale_prev = sale\n            rect_prev = rect\n    \n    df = pd.DataFrame(records)\n    return df\n\npanel_raw = generate_em_panel(n_firms=500, n_years=15, seed=2024)\nprint(f\"Panel: {panel_raw.shape[0]:,} firm-years, \"\n      f\"{panel_raw['ticker'].nunique()} firms\")\n\nPanel: 7,500 firm-years, 500 firms\n\n\n\n\n27.2.3 Sample Construction\nWe construct a sample of 500 firms, each with a randomly designated test year, mirroring the design of Dechow, Sloan, and Sweeney (1995).\n\ndef construct_sample(\n    df: pd.DataFrame,\n    n_sample: int = 500,\n    min_est_years: int = 10,\n    seed: int = 42,\n    selection_filter: Optional[callable] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    For each firm, randomly assign one year as the test year (part=True).\n    Require at least min_est_years of estimation data.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    firm_col = \"ticker\"\n    year_col = \"fyear\"\n    \n    # Compute accruals and filter for data availability\n    df = calc_accruals(df)\n    required = [\"acc_raw\", \"lag_at\", \"d_rev\", \"d_rec\", \"ppegt\"]\n    df = df.dropna(subset=required)\n    df = df[df[\"lag_at\"] &gt; 0]\n    \n    # Require minimum years\n    firm_counts = df.groupby(firm_col)[year_col].count()\n    eligible = firm_counts[firm_counts &gt;= (min_est_years + 1)].index\n    df = df[df[firm_col].isin(eligible)]\n    \n    if selection_filter is not None:\n        df = selection_filter(df)\n    \n    # Sample n_sample firms\n    firms = df[firm_col].unique()\n    if len(firms) &gt; n_sample:\n        firms = rng.choice(firms, n_sample, replace=False)\n    df = df[df[firm_col].isin(firms)].copy()\n    \n    # For each firm, randomly pick one test year (not the first year)\n    parts = []\n    for firm, group in df.groupby(firm_col):\n        years = group[year_col].sort_values().values\n        if len(years) &lt; 2:\n            continue\n        test_year = rng.choice(years[1:])\n        parts.append({firm_col: firm, year_col: test_year, \"part\": True})\n    \n    part_df = pd.DataFrame(parts)\n    df = df.merge(part_df, on=[firm_col, year_col], how=\"left\")\n    df[\"part\"] = df[\"part\"].fillna(False)\n    df[\"part\"] = df[\"part\"].astype(bool)\n\n    return df\n\nsample_1 = construct_sample(panel_raw, n_sample=500, seed=2024)\nprint(f\"Sample 1: {sample_1.shape[0]:,} firm-years, \"\n      f\"{sample_1['ticker'].nunique()} firms, \"\n      f\"{sample_1['part'].sum()} test years\")\n\nSample 1: 7,000 firm-years, 500 firms, 500 test years\n\n\n\n\n27.2.4 Estimating Discretionary Accruals\n\nda_results = get_all_nda(sample_1)\n\n# Verify: peek at test-year DA across models\nda_cols = [\"da_healy\", \"da_deangelo\", \"da_jones\", \"da_mod_jones\", \"da_industry\"]\ntest_da = da_results[da_results[\"part\"]][da_cols]\nprint(\"Test-year discretionary accruals (first 5 firms):\")\ntest_da.head().round(4)\n\nTest-year discretionary accruals (first 5 firms):\n\n\n\n\n\n\n\n\n\nda_healy\nda_deangelo\nda_jones\nda_mod_jones\nda_industry\n\n\n\n\n4\n-0.0791\n-0.1320\n-0.1252\n-0.1511\n-0.0791\n\n\n23\n-0.0792\n0.0813\n-0.1799\n-0.1426\n-0.0792\n\n\n30\n0.0618\n0.1403\n0.1458\n0.1308\n0.0618\n\n\n45\n0.2106\n0.2069\n0.0578\n0.5023\n0.2106\n\n\n61\n-0.1326\n0.0865\n-0.1425\n-0.1193\n-0.1326\n\n\n\n\n\n\n\n\n\n27.2.5 Firm-Level Regressions and Rejection Rates\nFor each firm and each model, we regress \\(DA\\) on \\(PART\\) and record whether the null hypothesis of zero discretionary accruals in the test year is rejected.\n\ndef firm_regressions(df: pd.DataFrame, models: list[str]) -&gt; pd.DataFrame:\n    \"\"\"\n    For each firm and model, regress DA on PART.\n    Return coefficients, std errors, t-stats, and rejection indicators.\n    \"\"\"\n    firm_col = \"ticker\" if \"ticker\" in df.columns else \"gvkey\"\n    records = []\n    \n    for firm, group in df.groupby(firm_col):\n        for model in models:\n            da_col = f\"da_{model}\"\n            g = group.dropna(subset=[da_col]).copy()\n            g[\"part_float\"] = g[\"part\"].astype(float)\n            \n            if len(g) &lt; 5 or g[\"part\"].sum() == 0:\n                continue\n            \n            try:\n                fm = sm.OLS(\n                    g[da_col], sm.add_constant(g[\"part_float\"])\n                ).fit()\n                \n                coef = fm.params[\"part_float\"]\n                se = fm.bse[\"part_float\"]\n                t_stat = fm.tvalues[\"part_float\"]\n                df_resid = fm.df_resid\n                \n                # One-sided p-values\n                p_neg = stats.t.cdf(t_stat, df_resid)\n                p_pos = 1 - p_neg\n                \n                records.append({\n                    firm_col: firm,\n                    \"model\": model,\n                    \"coef\": coef,\n                    \"se\": se,\n                    \"t_stat\": t_stat,\n                    \"neg_p01\": p_neg &lt; 0.01,\n                    \"neg_p05\": p_neg &lt; 0.05,\n                    \"pos_p01\": p_pos &lt; 0.01,\n                    \"pos_p05\": p_pos &lt; 0.05,\n                })\n            except Exception:\n                continue\n    \n    return pd.DataFrame(records)\n\nmodels = [\"healy\", \"deangelo\", \"jones\", \"mod_jones\", \"industry\"]\nreg_results = firm_regressions(da_results, models)\n\n\n\n27.2.6 Results\nTable 27.2 reports the distribution of estimated coefficients on \\(PART\\) across firms. Under the null of no manipulation, we expect the mean coefficient to be approximately zero.\n\ncoef_stats = (\n    reg_results\n    .groupby(\"model\")[\"coef\"]\n    .agg([\"mean\", \"std\", lambda x: x.quantile(0.25),\n          \"median\", lambda x: x.quantile(0.75)])\n)\ncoef_stats.columns = [\"Mean\", \"Std Dev\", \"Q1\", \"Median\", \"Q3\"]\ncoef_stats.index.name = \"Model\"\ncoef_stats.round(4)\n\n\n\nTable 27.2: Distribution of firm-level discretionary accrual estimates (coefficient on PART) under the null hypothesis of no earnings management. All five models produce mean estimates near zero, as expected.\n\n\n\n\n\n\n\n\n\n\nMean\nStd Dev\nQ1\nMedian\nQ3\n\n\nModel\n\n\n\n\n\n\n\n\n\ndeangelo\n0.0110\n0.2584\n-0.1604\n0.0140\n0.1839\n\n\nhealy\n0.0047\n0.1506\n-0.0993\n0.0050\n0.1066\n\n\nindustry\n0.0047\n0.1506\n-0.0993\n0.0050\n0.1066\n\n\njones\n0.0035\n0.1739\n-0.1134\n0.0069\n0.1125\n\n\nmod_jones\n0.0068\n0.1807\n-0.1127\n0.0108\n0.1177\n\n\n\n\n\n\n\n\n\n\nTable 27.3 reports rejection rates. The critical comparison is whether these rates approximate the nominal test size.\n\nrejection_rates = (\n    reg_results\n    .groupby(\"model\")[[\"neg_p01\", \"neg_p05\", \"pos_p01\", \"pos_p05\"]]\n    .mean()\n    .round(4)\n)\nrejection_rates.columns = [\n    \"Neg (1%)\", \"Neg (5%)\", \"Pos (1%)\", \"Pos (5%)\"\n]\nrejection_rates.index.name = \"Model\"\nrejection_rates\n\n\n\nTable 27.3: Type I error rates for one-sided tests of earnings management under the null hypothesis. Rates exceeding the nominal size (5% or 1%) indicate that the model over-rejects—a significant concern for the Jones and Modified Jones models.\n\n\n\n\n\n\n\n\n\n\nNeg (1%)\nNeg (5%)\nPos (1%)\nPos (5%)\n\n\nModel\n\n\n\n\n\n\n\n\ndeangelo\n0.0109\n0.0457\n0.0043\n0.0478\n\n\nhealy\n0.0100\n0.0380\n0.0080\n0.0500\n\n\nindustry\n0.0100\n0.0380\n0.0080\n0.0500\n\n\njones\n0.0280\n0.0800\n0.0380\n0.1000\n\n\nmod_jones\n0.0320\n0.0740\n0.0300\n0.0900\n\n\n\n\n\n\n\n\n\n\n\n\n27.2.7 Binomial Test for Size Distortion\nWe formally test whether observed rejection rates differ from nominal sizes using a two-sided binomial test. Small \\(p\\)-values indicate significant mis-calibration of the test statistic.\n\ndef binom_test_rate(series: pd.Series, nominal: float) -&gt; float:\n    \"\"\"Two-sided binomial test for rejection rate = nominal.\"\"\"\n    x = series.dropna()\n    k = int(x.sum())\n    n = len(x)\n    if n == 0:\n        return np.nan\n    return stats.binomtest(k, n, nominal, alternative=\"two-sided\").pvalue\n\nbinom_results = {}\nfor model, group in reg_results.groupby(\"model\"):\n    binom_results[model] = {\n        \"Neg (1%)\": binom_test_rate(group[\"neg_p01\"], 0.01),\n        \"Neg (5%)\": binom_test_rate(group[\"neg_p05\"], 0.05),\n        \"Pos (1%)\": binom_test_rate(group[\"pos_p01\"], 0.01),\n        \"Pos (5%)\": binom_test_rate(group[\"pos_p05\"], 0.05),\n    }\n\nbinom_df = pd.DataFrame(binom_results).T.round(4)\nbinom_df.index.name = \"Model\"\nbinom_df\n\n\n\nTable 27.4: Binomial test p-values for whether rejection rates equal nominal test sizes. Small values (e.g., &lt; 0.05) indicate statistically significant size distortion.\n\n\n\n\n\n\n\n\n\n\nNeg (1%)\nNeg (5%)\nPos (1%)\nPos (5%)\n\n\nModel\n\n\n\n\n\n\n\n\ndeangelo\n0.8117\n0.7486\n0.3423\n0.9150\n\n\nhealy\n1.0000\n0.2581\n0.8236\n1.0000\n\n\nindustry\n1.0000\n0.2581\n0.8236\n1.0000\n\n\njones\n0.0006\n0.0038\n0.0000\n0.0000\n\n\nmod_jones\n0.0001\n0.0179\n0.0002\n0.0002\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningInterpreting Over-Rejection\n\n\n\nIf the Jones and Modified Jones models show rejection rates significantly above 5%, this signals that the standard two-stage procedure produces anti-conservative test statistics. The Salkever (1976) correction addresses this by computing standard errors that reflect first-stage estimation uncertainty. In practical research on Vietnamese firms, where sample sizes per firm are often short (10–15 years of listed history), this correction is especially important because prediction error variance is a larger fraction of residual variance with fewer estimation-period observations.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Earnings Management: Detection and Measurement</span>"
    ]
  },
  {
    "objectID": "34_earning_management.html#sec-extreme",
    "href": "34_earning_management.html#sec-extreme",
    "title": "27  Earnings Management: Detection and Measurement",
    "section": "27.3 Extreme Performance Firms",
    "text": "27.3 Extreme Performance Firms\nA well-known weakness of accrual-based models is their poor performance when test firms experience extreme economic performance. Dechow, Sloan, and Sweeney (1995) documented that all five models over-reject the null hypothesis when test firm-years are drawn from the tails of the earnings or cash flow distribution. Kothari, Leone, and Wasley (2005) subsequently proposed “performance matching” as a partial remedy.\nThe intuition for the problem is straightforward: the Jones model assumes a linear, symmetric relationship between revenue changes and accruals. But firms experiencing extreme growth or contraction generate accruals that deviate nonlinearly from the model’s predictions, even absent any manipulation. This nonlinearity is misattributed to discretionary accruals.\n\n27.3.1 Constructing Extreme-Performance Samples\n\ndef add_earnings_deciles(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute firm-year earnings (scaled) and assign to deciles.\"\"\"\n    firm_col = \"ticker\" if \"ticker\" in df.columns else \"gvkey\"\n    df = df.copy()\n    g = df.groupby(firm_col)\n    df[\"lag_at_earn\"] = g[\"at\"].shift(1)\n    df[\"earn\"] = df[\"ib\"] / df[\"lag_at_earn\"]\n    df[\"earn_decile\"] = pd.qcut(\n        df[\"earn\"], 10, labels=False, duplicates=\"drop\"\n    ) + 1\n    return df\n\npanel_with_earn = add_earnings_deciles(panel_raw)\n\n# High-earners sample (top decile)\ndef filter_high_earn(df):\n    df = add_earnings_deciles(df)\n    return df[df[\"earn_decile\"] == 10]\n\n# Low-earners sample (bottom decile)\ndef filter_low_earn(df):\n    df = add_earnings_deciles(df)\n    return df[df[\"earn_decile\"] == 1]\n\nsample_high = construct_sample(\n    panel_raw, n_sample=300, seed=100,\n    selection_filter=filter_high_earn\n)\nsample_low = construct_sample(\n    panel_raw, n_sample=300, seed=200,\n    selection_filter=filter_low_earn\n)\n\nprint(f\"High-earnings sample: {sample_high['ticker'].nunique()} firms\")\nprint(f\"Low-earnings sample: {sample_low['ticker'].nunique()} firms\")\n\nHigh-earnings sample: 164 firms\nLow-earnings sample: 203 firms\n\n\n\n\n\nTable 27.5: Type I error rates (5% one-sided) when test firm-years are drawn from extreme earnings deciles. Over-rejection is expected because accrual models misattribute performance-driven accrual variation to managerial discretion.\n\n\nextreme_results = {}\nfor label, sample_df in [(\"High earners\", sample_high), (\"Low earners\", sample_low)]:\n    da = get_all_nda(sample_df)\n    regs = firm_regressions(da, models)\n    rates = regs.groupby(\"model\")[[\"neg_p05\", \"pos_p05\"]].mean().round(4)\n    rates.columns = [f\"{label} Neg(5%)\", f\"{label} Pos(5%)\"]\n    extreme_results[label] = rates\n\nif extreme_results:\n    extreme_df = pd.concat(extreme_results.values(), axis=1)\n    extreme_df.index.name = \"Model\"\n    extreme_df\n\n\n\n\n\n27.3.2 Performance Matching\nKothari, Leone, and Wasley (2005) propose adjusting discretionary accruals by subtracting the DA of a performance-matched firm (one in the same industry with similar ROA). This removes the systematic component of accruals correlated with performance. The matched discretionary accrual is:\n\\[\nDA^{PM}_{i,t} = DA_{i,t} - DA_{i^*,t}\n\\tag{27.8}\\]\nwhere \\(i^*\\) is the matched control firm. This approach is particularly relevant in Vietnam, where the cross-section of listed firms includes many high-growth firms alongside stagnant SOEs, which is performance heterogeneity that standard models may mischaracterize.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Earnings Management: Detection and Measurement</span>"
    ]
  },
  {
    "objectID": "34_earning_management.html#sec-power",
    "href": "34_earning_management.html#sec-power",
    "title": "27  Earnings Management: Detection and Measurement",
    "section": "27.4 Power Analysis",
    "text": "27.4 Power Analysis\n\n27.4.1 Artificially Introducing Earnings Management\nA model that never rejects is useless even if its Type I error rate is perfect. We need to evaluate power: the probability of detecting manipulation when it truly exists. Following Dechow, Sloan, and Sweeney (1995), we introduce known artificial manipulation into test-year financial statements and measure detection rates.\nWe consider three forms of manipulation at varying magnitudes:\n\nExpense manipulation. Decrease current liabilities by the manipulation amount (e.g., delaying recognition of accrued expenses): \\[LCT'_{i,t} = LCT_{i,t} - \\lambda \\cdot A_{i,t-1}\\]\nRevenue manipulation. Increase sales and receivables by the manipulation amount (e.g., premature revenue recognition or channel stuffing): \\[Sale'_{i,t} = Sale_{i,t} + \\lambda \\cdot A_{i,t-1}, \\quad Rect'_{i,t} = Rect_{i,t} + \\lambda \\cdot A_{i,t-1}\\]\nMargin manipulation. Increase sales by the gross amount needed to inflate net income by \\(\\lambda \\cdot A_{i,t-1}\\), increasing both receivables and current liabilities proportionally.\n\nThe parameter \\(\\lambda\\) represents manipulation as a fraction of lagged total assets, ranging from 0% to 50%.\n\ndef manipulate(\n    df: pd.DataFrame,\n    level: float = 0.0,\n    manip_type: str = \"expense\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Introduce artificial earnings management of a given type and level\n    into test-year (part=True) observations.\n    \n    Parameters\n    ----------\n    df : DataFrame with 'part' indicator and required financial variables.\n    level : Manipulation as fraction of lagged total assets.\n    manip_type : One of 'expense', 'revenue', 'margin'.\n    \"\"\"\n    firm_col = \"ticker\" if \"ticker\" in df.columns else \"gvkey\"\n    year_col = \"fyear\" if \"fyear\" in df.columns else \"year\"\n    \n    df = df.sort_values([firm_col, year_col]).copy()\n    g = df.groupby(firm_col)\n    lag_at = g[\"at\"].shift(1)\n    manip_amt = lag_at * level\n    \n    if manip_type == \"expense\":\n        # Decrease current liabilities in test year\n        df.loc[df[\"part\"], \"lct\"] -= manip_amt[df[\"part\"]]\n        \n    elif manip_type == \"revenue\":\n        # Increase sales and receivables in test year\n        df.loc[df[\"part\"], \"sale\"] += manip_amt[df[\"part\"]]\n        df.loc[df[\"part\"], \"rect\"] += manip_amt[df[\"part\"]]\n        df.loc[df[\"part\"], \"act\"] += manip_amt[df[\"part\"]]\n        # Reverse in following year\n        next_part = g[\"part\"].shift(1).fillna(False)\n        df.loc[next_part, \"sale\"] -= manip_amt[next_part]\n        \n    elif manip_type == \"margin\":\n        # Compute net income ratio for scaling\n        est_mask = ~df[\"part\"]\n        ni_ratio = g.apply(\n            lambda x: (x.loc[~x[\"part\"], \"ni\"] / x.loc[~x[\"part\"], \"sale\"]).median()\n        )\n        df[\"_ni_ratio\"] = df[firm_col].map(ni_ratio)\n        gross_amt = np.where(\n            df[\"_ni_ratio\"] &gt; 0, manip_amt / df[\"_ni_ratio\"], 0\n        )\n        df.loc[df[\"part\"], \"sale\"] += gross_amt[df[\"part\"]]\n        df.loc[df[\"part\"], \"rect\"] += gross_amt[df[\"part\"]]\n        df.loc[df[\"part\"], \"act\"] += gross_amt[df[\"part\"]]\n        net_effect = gross_amt - manip_amt\n        df.loc[df[\"part\"], \"lct\"] += net_effect[df[\"part\"]]\n        df.drop(columns=[\"_ni_ratio\"], inplace=True)\n    \n    return df\n\n\nlevels = [0.0, 0.05, 0.10, 0.20, 0.30, 0.50]\nmanip_types = [\"expense\", \"revenue\", \"margin\"]\n\npower_records = []\n\nfor level in levels:\n    for mtype in manip_types:\n        # Copy base sample and introduce manipulation\n        manip_sample = sample_1.copy()\n        if level &gt; 0:\n            manip_sample = manipulate(manip_sample, level=level, manip_type=mtype)\n        \n        # Estimate DA and run tests\n        da = get_all_nda(manip_sample)\n        regs = firm_regressions(da, models)\n        \n        # Power = positive one-sided rejection rate at 5%\n        power = regs.groupby(\"model\")[\"pos_p05\"].mean()\n        for model_name, pwr in power.items():\n            power_records.append({\n                \"level\": level,\n                \"type\": mtype,\n                \"model\": model_name,\n                \"power\": pwr,\n            })\n\npower_df = pd.DataFrame(power_records)\n\n\nlevels = [0.0, 0.05, 0.10, 0.20, 0.30, 0.50]\nmanip_types = [\"expense\", \"revenue\", \"margin\"]\n\nfrom joblib import Parallel, delayed\nfrom itertools import product\n\ndef _run_one(level, mtype, base_sample, models):\n    manip_sample = base_sample.copy()\n    if level &gt; 0:\n        manip_sample = manipulate(manip_sample, level=level, manip_type=mtype)\n    da = get_all_nda(manip_sample)\n    regs = firm_regressions(da, models)\n    power = regs.groupby(\"model\")[\"pos_p05\"].mean()\n    return [\n        {\"level\": level, \"type\": mtype, \"model\": m, \"power\": p}\n        for m, p in power.items()\n    ]\n\n# Skip redundant level=0 runs (all manip_types identical when level=0)\ncombos = [(0.0, \"expense\")] + [\n    (l, m) for l in levels if l &gt; 0 for m in manip_types\n]\n\nresults = Parallel(n_jobs=-1, verbose=1)(\n    delayed(_run_one)(lvl, mt, sample_1, models)\n    for lvl, mt in combos\n)\n\n# Expand level=0 result across all manip_types\nflat = []\nfor (lvl, mt), batch in zip(combos, results):\n    if lvl == 0:\n        for mtype in manip_types:\n            flat.extend([{**r, \"type\": mtype} for r in batch])\n    else:\n        flat.extend(batch)\n\npower_df = pd.DataFrame(flat)\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:  2.0min finished\n\n\n\n\n27.4.2 Power Functions\nFigure 27.1 plots the estimated power functions. The key questions are: (i) which model has the highest power for a given manipulation level and type, and (ii) at what magnitudes does manipulation become reliably detectable?\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 5), sharey=True)\n\nmodel_colors = {\n    \"healy\": \"#2196F3\",\n    \"deangelo\": \"#FF9800\",\n    \"jones\": \"#4CAF50\",\n    \"mod_jones\": \"#E91E63\",\n    \"industry\": \"#9C27B0\",\n}\n\nfor idx, mtype in enumerate(manip_types):\n    ax = axes[idx]\n    subset = power_df[power_df[\"type\"] == mtype]\n    for model_name in models:\n        m_data = subset[subset[\"model\"] == model_name].sort_values(\"level\")\n        ax.plot(\n            m_data[\"level\"] * 100, m_data[\"power\"],\n            marker=\"o\", markersize=4,\n            label=model_name.replace(\"_\", \" \").title(),\n            color=model_colors[model_name],\n            linewidth=1.5,\n        )\n    ax.set_title(mtype.title(), fontweight=\"bold\")\n    ax.set_xlabel(\"Manipulation (% of assets)\")\n    ax.set_ylim(-0.02, 1.02)\n    ax.yaxis.set_major_formatter(mticker.PercentFormatter(xmax=1))\n    ax.axhline(0.05, color=\"grey\", linestyle=\"--\", linewidth=0.7, label=\"5% size\")\n\naxes[0].set_ylabel(\"Rejection Rate (Power)\")\naxes[2].legend(fontsize=8, loc=\"center left\", bbox_to_anchor=(1.02, 0.5))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 27.1: Power functions for the five earnings management detection models across three manipulation types. Power increases with manipulation magnitude but remains low for economically plausible levels (&lt; 10% of assets), highlighting the fundamental difficulty of detecting earnings management.\n\n\n\n\n\n\n\n27.4.3 Summary Statistics\nTable 27.6 reports power at selected manipulation levels for the Jones and Modified Jones models, which are most commonly used in applied research.\n\npower_summary = (\n    power_df[power_df[\"model\"].isin([\"jones\", \"mod_jones\"])]\n    .pivot_table(index=[\"model\", \"level\"], columns=\"type\", values=\"power\")\n    .round(3)\n)\npower_summary.index.names = [\"Model\", \"Level (% assets)\"]\npower_summary\n\n\n\nTable 27.6: Power of the Jones and Modified Jones models at selected manipulation levels. Even at 10% of assets—a large amount of manipulation—detection rates remain well below 50% for most manipulation types, underscoring the low statistical power of standard tests.\n\n\n\n\n\n\n\n\n\n\ntype\nexpense\nmargin\nrevenue\n\n\nModel\nLevel (% assets)\n\n\n\n\n\n\n\njones\n0.00\n0.100\n0.100\n0.100\n\n\n0.05\n0.154\n0.132\n0.142\n\n\n0.10\n0.218\n0.126\n0.178\n\n\n0.20\n0.432\n0.122\n0.300\n\n\n0.30\n0.660\n0.122\n0.436\n\n\n0.50\n0.912\n0.122\n0.710\n\n\nmod_jones\n0.00\n0.090\n0.090\n0.090\n\n\n0.05\n0.130\n0.106\n0.128\n\n\n0.10\n0.198\n0.174\n0.190\n\n\n0.20\n0.404\n0.368\n0.422\n\n\n0.30\n0.636\n0.598\n0.652\n\n\n0.50\n0.884\n0.878\n0.922\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantImplications for Vietnamese Research\n\n\n\nThe power analysis has stark implications for earnings management research in Vietnam. The typical Vietnamese listed firm has been listed for 10–15 years, providing far fewer estimation-period observations than in the U.S. context where Dechow, Sloan, and Sweeney (1995) had decades of Compustat data. Shorter estimation windows increase parameter uncertainty in the Jones model, further reducing power. Combined with the noisier financial data common in emerging markets, researchers should interpret non-rejection of the null as uninformative rather than as evidence of clean financial reporting.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Earnings Management: Detection and Measurement</span>"
    ]
  },
  {
    "objectID": "34_earning_management.html#sec-vietnam",
    "href": "34_earning_management.html#sec-vietnam",
    "title": "27  Earnings Management: Detection and Measurement",
    "section": "27.5 Vietnamese Institutional Context",
    "text": "27.5 Vietnamese Institutional Context\n\n27.5.1 Channels of Earnings Management\nSeveral features of the Vietnamese institutional environment create distinctive earnings management incentives and opportunities:\nTax-driven manipulation. Vietnamese corporate income tax (CIT) rates have declined from 28% (pre-2009) to 20% (2016 onwards), with preferential rates for firms in Special Economic Zones and high-tech sectors. The close alignment between VAS accounting and tax accounting creates incentives to manage earnings downward to reduce tax obligations—a pattern documented in developing economies with code-law accounting traditions (Ball, Robin, and Wu 2003).\nIPO and seasoned equity offering (SEO) incentives. Vietnam has experienced several waves of SOE equitization (partial privatization). Managers have incentives to inflate earnings before share offerings to maximize proceeds. The SSC requires minimum profitability thresholds for listing eligibility, creating sharp incentives around these regulatory cutoffs, which is a natural setting for the discontinuity analysis of Burgstahler and Dichev (1997).\nReal earnings management in manufacturing. Roychowdhury (2006) identifies overproduction, discretionary expenditure cuts, and sales manipulation as the three main channels of real earnings management. Vietnam’s large manufacturing sector (textiles, electronics assembly, food processing) provides ample scope for overproduction-based REM, where unit costs are reduced by spreading fixed overhead across larger production runs.\nRelated-party transactions. Transactions with affiliated entities are a well-documented channel for earnings manipulation in Asian markets. Vietnamese conglomerates (tập đoàn) often feature complex cross-ownership structures where transfer pricing between subsidiaries can shift profits across reporting entities.\n\n\n27.5.2 The Earnings Distribution Test\nBurgstahler and Dichev (1997) observed a striking discontinuity in the distribution of reported earnings around zero: far more firms report small positive earnings than small losses, relative to what a smooth distribution would predict. This pattern is interpreted as evidence that firms manage earnings to avoid reporting losses.\nWe apply this test to Vietnamese-style data to illustrate the methodology.\n\n# Generate earnings for a larger sample with benchmark-beating behavior\nrng_dist = np.random.default_rng(2024)\nn_firms_dist = 2000\nn_years_dist = 10\n\nearnings = []\nfor i in range(n_firms_dist):\n    base_earn = rng_dist.normal(0.06, 0.08)\n    for t in range(n_years_dist):\n        e = base_earn + rng_dist.normal(0, 0.04)\n        # Simulate benchmark-beating: firms near zero bump earnings up\n        if -0.01 &lt; e &lt; 0.005:\n            e += rng_dist.uniform(0.005, 0.015) * (rng_dist.random() &lt; 0.6)\n        earnings.append(e)\n\nearn_array = np.array(earnings)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nbins = np.arange(-0.30, 0.35, 0.01)\nax.hist(earn_array, bins=bins, color=\"steelblue\", edgecolor=\"white\",\n        alpha=0.85, density=True)\nax.axvline(0, color=\"firebrick\", linestyle=\"--\", linewidth=1.5, label=\"Zero threshold\")\nax.set_xlabel(\"Earnings / Total Assets\")\nax.set_ylabel(\"Density\")\nax.set_title(\"Earnings Distribution Around Zero\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 27.2: Distribution of scaled earnings around zero. A discontinuity—excess density just above zero and a deficit just below—would suggest benchmark-beating behavior. The red dashed line marks the zero threshold.\n\n\n\n\n\n\n# Year-over-year changes\nearn_changes = np.diff(earn_array.reshape(n_firms_dist, n_years_dist), axis=1).flatten()\n\nfig, ax = plt.subplots(figsize=(8, 5))\nbins_chg = np.arange(-0.15, 0.15, 0.005)\nax.hist(earn_changes, bins=bins_chg, color=\"darkorange\", edgecolor=\"white\",\n        alpha=0.85, density=True)\nax.axvline(0, color=\"firebrick\", linestyle=\"--\", linewidth=1.5, label=\"Zero change\")\nax.set_xlabel(\"Change in Earnings / Total Assets\")\nax.set_ylabel(\"Density\")\nax.set_title(\"Earnings Change Distribution Around Zero\")\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 27.3: Distribution of year-over-year earnings changes around zero. A discontinuity here suggests that firms manage earnings to avoid reporting declines, even when small.\n\n\n\n\n\n\n\n27.5.3 Accrual-Based vs. Real Earnings Management\nCohen, Dey, and Lys (2008) document a shift from AEM to REM following the passage of the Sarbanes-Oxley Act (SOX) in 2002, suggesting that tighter regulatory scrutiny redirects manipulation toward less detectable channels. Zang (2012) provides further evidence that firms trade off between AEM and REM based on their relative costs.\nIn Vietnam, where regulatory enforcement of accounting standards is weaker than in post-SOX America, we might expect AEM to remain the dominant channel. However, as Vietnam moves toward IFRS adoption and strengthens SSC oversight, the AEM-to-REM substitution hypothesis becomes testable.\nWe can measure REM using the Roychowdhury (2006) approach. Three proxies capture different manipulation channels:\nAbnormal cash flow from operations. Estimate normal CFO as a function of sales and sales changes:\n\\[\n\\frac{CFO_{i,t}}{A_{i,t-1}} = \\beta_0 + \\beta_1 \\frac{1}{A_{i,t-1}} + \\beta_2 \\frac{S_{i,t}}{A_{i,t-1}} + \\beta_3 \\frac{\\Delta S_{i,t}}{A_{i,t-1}} + \\varepsilon_{i,t}\n\\tag{27.9}\\]\nAbnormal production costs. Production costs = COGS + change in inventory. Normal levels are modeled as:\n\\[\n\\frac{PROD_{i,t}}{A_{i,t-1}} = \\beta_0 + \\beta_1 \\frac{1}{A_{i,t-1}} + \\beta_2 \\frac{S_{i,t}}{A_{i,t-1}} + \\beta_3 \\frac{\\Delta S_{i,t}}{A_{i,t-1}} + \\beta_4 \\frac{\\Delta S_{i,t-1}}{A_{i,t-1}} + \\varepsilon_{i,t}\n\\tag{27.10}\\]\nAbnormal discretionary expenditures. R&D + advertising + SGA, modeled as:\n\\[\n\\frac{DISC_{i,t}}{A_{i,t-1}} = \\beta_0 + \\beta_1 \\frac{1}{A_{i,t-1}} + \\beta_2 \\frac{S_{i,t-1}}{A_{i,t-1}} + \\varepsilon_{i,t}\n\\tag{27.11}\\]\nResiduals from these regressions serve as proxies for real manipulation. A firm that overproduces will show abnormally high production costs and abnormally low CFO (cash tied up in inventory).",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Earnings Management: Detection and Measurement</span>"
    ]
  },
  {
    "objectID": "34_earning_management.html#summary",
    "href": "34_earning_management.html#summary",
    "title": "27  Earnings Management: Detection and Measurement",
    "section": "27.6 Summary",
    "text": "27.6 Summary\nThis chapter examined the measurement and detection of earnings management, with a focus on methodological rigor and adaptation to Vietnam’s institutional environment. The key takeaways are:\n\nDetecting earnings management requires separating discretionary from non-discretionary accruals, which depends critically on the quality of the non-discretionary accrual model. All five canonical models have known weaknesses.\nUnder the null hypothesis of no manipulation, the Jones and Modified Jones models over-reject when standard two-stage standard errors are used. The Salkever (1976) correction provides properly calibrated inference.\nWhen test firms experience extreme financial performance, all models exhibit severe size distortion, misattributing performance-driven accrual variation to managerial discretion. Performance matching (Kothari, Leone, and Wasley 2005) partially addresses this.\nPower analysis reveals that economically plausible levels of manipulation (below 10% of assets) are detected with very low probability. This casts doubt on studies that report null results as evidence of no manipulation.\nVietnam’s institutional features, such aSOE governance, VAS accounting rules, weak enforcement, and thin audit coverage—create a rich setting for earnings management research, but the methodological challenges are amplified by shorter time series and noisier data.\n\nResearchers working with Vietnamese data should: (i) use the Salkever correction for proper inference, (ii) implement performance matching, (iii) consider multiple models and triangulate results, and (iv) supplement accrual-based approaches with real earnings management and distributional tests.\n\n\n\n\n\n\n\nBall, Ray, Ashok Robin, and Joanna Shuang Wu. 2003. “Incentives Versus Standards: Properties of Accounting Income in Four East Asian Countries.” Journal of Accounting and Economics 36 (1-3): 235–70.\n\n\nBurgstahler, David, and Ilia Dichev. 1997. “Earnings Management to Avoid Earnings Decreases and Losses.” Journal of Accounting and Economics 24 (1): 99–126.\n\n\nCohen, Daniel A, Aiyesha Dey, and Thomas Z Lys. 2008. “Real and Accrual-Based Earnings Management in the Pre-and Post-Sarbanes-Oxley Periods.” The Accounting Review 83 (3): 757–87.\n\n\nDeAngelo, Linda Elizabeth. 1986. “Accounting Numbers as Market Valuation Substitutes: A Study of Management Buyouts of Public Stockholders.” Accounting Review, 400–420.\n\n\nDechow, Patricia M, Richard G Sloan, and Amy P Sweeney. 1995. “Detecting Earnings Management.” Accounting Review, 193–225.\n\n\nHealy, Paul M. 1985. “The Effect of Bonus Schemes on Accounting Decisions.” Journal of Accounting and Economics 7 (1-3): 85–107.\n\n\nJones, Jennifer J. 1991. “Earnings Management During Import Relief Investigations.” Journal of Accounting Research 29 (2): 193–228.\n\n\nKothari, Sagar P, Andrew J Leone, and Charles E Wasley. 2005. “Performance Matched Discretionary Accrual Measures.” Journal of Accounting and Economics 39 (1): 163–97.\n\n\nLeuz, Christian, Dhananjay Nanda, and Peter D Wysocki. 2003. “Earnings Management and Investor Protection: An International Comparison.” Journal of Financial Economics 69 (3): 505–27.\n\n\nRoychowdhury, Sugata. 2006. “Earnings Management Through Real Activities Manipulation.” Journal of Accounting and Economics 42 (3): 335–70.\n\n\nSalkever, David S. 1976. “The Use of Dummy Variables to Compute Predictions, Prediction Errors, and Confidence Intervals.” Journal of Econometrics 4 (4): 393–97.\n\n\nZang, Amy Y. 2012. “Evidence on the Trade-Off Between Real Activities Manipulation and Accrual-Based Earnings Management.” The Accounting Review 87 (2): 675–703.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Earnings Management: Detection and Measurement</span>"
    ]
  },
  {
    "objectID": "35_pe_ratio.html",
    "href": "35_pe_ratio.html",
    "title": "28  P/E Ratio",
    "section": "",
    "text": "28.0.1 Technical Schema and Variable Engineering\nThe systematic analysis of equity valuations in frontier and emerging markets requires a robust integration of high-fidelity financial data, rigorous mathematical modeling, and an acute understanding of local regulatory frameworks. In the context of the Vietnamese equity market, the Price-Earnings (P/E) ratio serves as a primary instrument for assessing corporate performance and market sentiment. This chapter provides an exploration of P/E valuation methodologies, ranging from firm-specific trailing and forward metrics to cyclically adjusted and unlevered variations.\nThe primary identifier for equities is the symbol, typically a three-letter uppercase code, though special indices such as E1VFVN30 (ETF) or market indices like HNX-INDEX and UPCOM-INDEX follow unique naming conventions. Price data is categorized as OHLC type, encompassing ‘open’, ‘high’, ‘low’, and ‘close’. For valuation purposes, the ‘adjusted close’ is preferred to account for corporate actions such as stock splits and dividend payments.\nThe accounting variables provided in the Vietnam Fundamentals product follow a standardized classification that reconciles different reporting standards across sectors. This reconciliation is vital because Vietnamese firms may have differing interpretations of revenue and net profit under Vietnamese Accounting Standards (VAS).",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "35_pe_ratio.html#firm-specific-pe-valuation-metrics",
    "href": "35_pe_ratio.html#firm-specific-pe-valuation-metrics",
    "title": "28  P/E Ratio",
    "section": "28.1 Firm-Specific P/E Valuation Metrics",
    "text": "28.1 Firm-Specific P/E Valuation Metrics\nAt the individual firm level, the P/E ratio is the most fundamental measure of how much an investor is willing to pay for each unit of current or future profit. However, the calculation of “E” (Earnings) can take several forms, each offering a different insight into the company’s value.\n\n28.1.1 Trailing P/E and the TTM Methodology\nThe Trailing P/E ratio is calculated using the reported earnings from the last four quarters, known as Trailing Twelve Months (TTM). In the Vietnamese market, this is often the default metric reported by exchanges and financial news outlets.\nThe mathematical representation is:\n\\[\nP/E_{Trailing} = \\frac{P_t}{\\sum_{i=0}^{3} EPS_{q-i}}\n\\]\nWhere \\(P_t\\) is the current market price and \\(EPS_{q-i}\\) represents the earnings per share for each of the four most recent quarters.\n\n\n28.1.2 Forward P/E and Analyst Forecast Reliability\nForward P/E shifts the focus from historical performance to future expectations by using consensus earnings forecasts for the next fiscal year.\n\\[\nP/E_{Forward} = \\frac{P_t}{EPS_{forecasted, t+1}}\n\\]\nIn Vietnam, these forecasts are typically generated by research departments within major securities firms such as SSI, VCSC, MBS, and FPTS. These analysts rely on financial statements, projections, models, and subjective evaluations of market sentiment to generate their estimates. However, the accuracy of these forecasts is a significant area of research. However, analysts in Vietnam are often influenced by anchoring and adjustment bias, where they base future predictions heavily on past records and then make insufficient adjustments.\nThe accuracy of analyst earnings forecasts in Vietnam is significantly impacted by corporate governance characteristics. Specifically, state ownership has been found to have a negative impact on forecast accuracy, while institutional ownership has a positive effect. This implies that for firms with high state-controlled stakes, which are common in Vietnam’s strategic industries, researchers should apply a higher discount or “fudge factor” to forward P/E estimates provided by consensus sources.\n\n\n28.1.3 Cyclically Adjusted Price-Earnings (CAPE) Ratio\nThe Shiller P/E, or CAPE ratio, is designed to smooth out the volatility of the business cycle by using a 10-year average of inflation-adjusted earnings. For a market like Vietnam, which has seen periods of extreme growth followed by stabilization, the CAPE ratio provides a more tempered view of valuation than trailing metrics.\nThe calculation requires adjusting historical earnings by the Consumer Price Index (CPI). Vietnam’s CPI data is updated monthly and is available from January 1996 through early 2026, with an average year-on-year growth rate of approximately 12.2%.\n\\[\nEPS_{real, t} = EPS_{nominal, t} \\times \\frac{CPI_{current}}{CPI_t}\n\\]\n\\[\nCAPE = \\frac{P_t}{\\frac{1}{10} \\sum_{i=0}^{9} EPS_{real, t-i}}\n\\]\nHistorically, Vietnam’s CPI has reached an all-time high of 28.3% YoY in August 2008 and a record low of -2.6% in July 2000. These extreme swings mean that nominal earnings in the mid-2000s are not comparable to earnings today without the Shiller adjustment. Researchers must be mindful of the different base years used by the National Statistics Office (GSO), such as 2024=100, 2019=100, and 2014=100.\n\n\n28.1.4 Unlevered P/E and the Leibowitz Framework\nThe concept of the “Unlevered P/E” ratio attempts to view a company’s valuation independent of its capital structure. This is particularly relevant in the Vietnamese market, where debt loads vary significantly between state-owned enterprises (SOEs) and private firms. Research by Leibowitz (2002) highlights that for the investment analyst, a company is already levered, and the task is to estimate its theoretical value by inferring the underlying structure of returns.\nAccording to Leibowitz, leverage always moves the P/E toward a lower value than what would be obtained from a standard Gordon growth formula. As a company adds debt, the market discount rate for equity increases to compensate for the additional risk, which in turn compresses the P/E multiple. For example, a debt-free company with a theoretical P/E of 30 might see its P/E drop to 23 with a 40% debt ratio, and down to 20 with a 50% debt ratio.\nThe Unlevered P/E (often proxied by Enterprise Value to EBIT or EBITDA) can be calculated using fundamental variables:\n\\[\nEV = (\\text{Shares Outstanding} \\times \\text{Price}) + \\text{Total Debt} - \\text{Cash}\n\\]\n\\[\nP/E_{\\text{Unlevered}} \\approx \\frac{EV}{EBIT \\times (1 - \\tau)}\n\\]\nWhere \\(\\tau\\) is the corporate income tax rate. The standard CIT rate in Vietnam is 20%, which has been stable since the mid-2010s but is subject to new revisions under the Law on CIT ratified in June 2025.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "35_pe_ratio.html#market-level-aggregation-techniques",
    "href": "35_pe_ratio.html#market-level-aggregation-techniques",
    "title": "28  P/E Ratio",
    "section": "28.2 Market-Level Aggregation Techniques",
    "text": "28.2 Market-Level Aggregation Techniques\nAggregating firm-specific metrics into a single market-level index P/E is a complex task that requires careful consideration of weighting and the inclusion of loss-making entities. In the Vietnamese market, two primary indices (i.e., the VN-Index and the VN30), provide the benchmarks for performance and valuation.\n\n28.2.1 The VN-Index: Capitalization-Weighted Aggregation\nThe VN-Index is a capitalization-weighted index of all companies listed on HOSE. It serves as a broad indicator of market health, where an increase in the index reflects a general rise in the stock prices of listed companies.\nThe index value is calculated as:\n\\[\n\\text{VN-Index} = \\frac{\\sum (Price_i \\times Shares_i)}{\\text{Base Factor}} \\times \\text{Adjustment Factor}\n\\]\nSimilarly, the index-level P/E is typically calculated as the sum of all market capitalizations divided by the sum of all net incomes (earnings) of the constituents.\n\\[\nP/E_{Market} = \\frac{\\sum MarketCap_i}{\\sum Earnings_i}\n\\]\nAs of early 2026, the total market capitalization for 701 tracked companies was approximately 8,629.8 trillion VND, with total earnings of 588.9 trillion VND, yielding a median P/E of approximately 14.7x.\n\n\n28.2.2 The VN30 and Free-Float Adjustments\nThe VN30 Index represents a basket of the 30 largest and most liquid stocks on HOSE, screened through layers of capitalization, free-float ratio, and transactional volume. Unlike the broad VN-Index, the VN30 applies a free-float adjustment to its capitalization weighting to ensure that only shares available for public trading affect the index movement.\nThe free-float ratio (\\(f\\)) is calculated as:\n\\(f = \\frac{N - N_l}{N}\\)\nWhere \\(N\\) is the total number of outstanding shares and \\(N_l\\) is the number of limited trading shares (held by founders, state, or strategic partners).\nWhen aggregating P/E for the VN30, researchers must account for this adjustment, as it more accurately reflects the valuation of the investable universe. In the Vietnamese market, a handful of large-cap stocks can significantly skew the broad index. For instance, Vingroup (VIC) and its related companies have at times represented nearly a quarter of the VN-Index’s total weighting, creating a “closet indexing” dilemma for fund managers. Removing these high-impact stocks can reveal a much different valuation profile; while the VN-Index might trade at 13x, the market excluding VIC-related stocks could be closer to 11x.\n\n\n28.2.3 Median vs. Mean Aggregation\nQuantitative researchers often prefer median P/E ratios over mean P/E ratios to mitigate the influence of extreme outliers (i.e., companies with either exceptionally high P/E ratios due to temporarily low earnings or those that are technically “expensive” because of high growth expectations). Table 28.1 shows different aggregation methods.\n\n\n\nTable 28.1: Median vs. Mean Aggregation\n\n\n\n\n\n\n\n\n\n\nAggregation Method\nMathematical Definition\nResilience to Outliers\n\n\nSimple Mean\n\\(\\frac{1}{n} \\sum (P/E)_i\\)\nLow (highly skewed by extreme values)\n\n\nWeighted Mean\n\\(\\frac{\\sum (Cap_i \\times (P/E)_i)}{\\sum Cap_i}\\)\nMedium (skewed by large-cap valuation)\n\n\nMedian\n\\(\\text{Median}((P/E)_1, \\dots, (P/E)_n)\\)\nHigh (most robust for market sentiment)\n\n\nTotal-to-Total\n\\(\\frac{\\sum MarketCap_i}{\\sum Earnings_i}\\)\nHigh (standard for index providers)\n\n\n\n\n\n\nThe Vietnamese market currently trades near its 3-year average P/E of 14.8x, suggesting that investors are relatively neutral on current valuations, expecting earnings to grow in line with historical rates.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "35_pe_ratio.html#implementation",
    "href": "35_pe_ratio.html#implementation",
    "title": "28  P/E Ratio",
    "section": "28.3 Implementation",
    "text": "28.3 Implementation\n\n28.3.1 Data Ingestion and Processing\n\n\n28.3.2 Shiller CAPE\nThe Shiller CAPE requires a more sophisticated approach, involving the merging of earnings data with monthly CPI series.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "35_pe_ratio.html#macroeconomic-and-regulatory-factors-in-valuation",
    "href": "35_pe_ratio.html#macroeconomic-and-regulatory-factors-in-valuation",
    "title": "28  P/E Ratio",
    "section": "28.4 Macroeconomic and Regulatory Factors in Valuation",
    "text": "28.4 Macroeconomic and Regulatory Factors in Valuation\nThe P/E ratio does not exist in a vacuum; it is deeply influenced by the macroeconomic environment and the regulatory framework within which companies operate.\n\n28.4.1 The Role of Corporate Income Tax (CIT)\nThe earnings component of the P/E ratio is a post-tax metric. In Vietnam, the standard CIT rate is 20%. However, there is a significant shift occurring with the ratification of the new Law on CIT in June 2025, taking effect on October 1, 2025. This law introduces:\n\nA 15% rate for enterprises with annual revenue not exceeding 3 billion VND.\nA 17% rate for enterprises with revenue between 3 and 50 billion VND.\nThe maintenance of higher rates (up to 50%) for the oil, gas, and mineral resource sectors.\n\nFurthermore, Vietnam has adopted the Global Minimum Tax (GMT) rules under Pillar Two, effective January 1, 2024, to protect its tax revenue from inbound and outbound investments. These changes mean that historical P/E ratios may not be directly comparable to future ratios for companies that previously benefited from generous tax incentives, as the effective tax rate is likely to rise for large multinational subsidiaries operating in Vietnam.\n\n\n28.4.2 Macroeconomic Determinants: Inflation and Interest Rates\nStock price indices, and by extension P/E ratios, are heavily influenced by fundamental macroeconomic factors such as inflation, exchange rates, and interest rates. The interest rate serves as the cost of capital for enterprises. When interest rates fall, the cost of borrowing drops, increasing corporate profits and potentially raising the P/E that investors are willing to pay.\nThe Vietnamese dong (VND) exchange rate also plays a crucial role. In early 2025, the VND depreciated by 1.33% YTD, reflecting volatility in the timeline for Federal Reserve rate cuts. Exchange rate depreciation can be inflationary by default, which may prompt the State Bank of Vietnam to tighten monetary policy, thereby putting downward pressure on equity valuations.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "35_pe_ratio.html#synthesis-of-valuation-dynamics",
    "href": "35_pe_ratio.html#synthesis-of-valuation-dynamics",
    "title": "28  P/E Ratio",
    "section": "28.5 Synthesis of Valuation Dynamics",
    "text": "28.5 Synthesis of Valuation Dynamics\nThe Vietnamese equity market is currently in a state of transition. While the market has seen robust growth—the VN-Index rose 38% in the year leading up to early 2026—investors remain neutral on overall valuations, as reflected by the market’s alignment with its 3-year average P/E of 14.8x. This neutrality suggests that investors expect earnings growth (forecast at 14% annually) to keep pace with price appreciation.\nThe reliability of these valuations, however, remains dependent on the quality of non-financial disclosure. While companies are increasingly required by law to provide non-financial information, the level of transparency remains at a medium level (approximately 58.5% of required disclosures). Researchers must therefore balance quantitative P/E analysis with qualitative assessments of corporate governance and sustainability strategies. Companies that effectively integrate ESG principles have been shown to experience fewer negative impacts on abnormal stock returns during uncertain periods, such as the COVID-19 pandemic.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "35_pe_ratio.html#conclusions",
    "href": "35_pe_ratio.html#conclusions",
    "title": "28  P/E Ratio",
    "section": "28.6 Conclusions",
    "text": "28.6 Conclusions\nThe exploration of P/E ratio valuation and aggregation within the Vietnamese equity market reveals a sophisticated landscape where traditional metrics must be adapted to local realities. The reliance on trailing metrics, while common, fails to account for the cyclicality and high inflation history of the Vietnamese economy, a gap that the Shiller CAPE ratio effectively bridges through CPI-adjusted normalization. Furthermore, the sensitivity of P/E to capital structure, as defined in the Leibowitz framework, highlights the necessity of unlevered valuation metrics in a market with diverse corporate funding models.\nThe aggregation of these metrics into market-level indices like the VN-Index and VN30 requires an awareness of concentration risks and the importance of free-float adjustments. The “Vingroup Effect” demonstrates how a single corporate ecosystem can skew index-wide valuations, necessitating a “closet indexing” awareness for fund managers and researchers.\nLooking forward, the evolution of the regulatory environment—specifically the CIT law of 2025 and the Global Minimum Tax—will introduce new variables into the valuation equation. The shift toward higher-quality non-financial disclosures and the increasing accuracy of analyst forecasts in institutionalized firms suggest that the Vietnamese equity market is steadily maturing, offering a more transparent and predictable environment for the application of advanced quantitative valuation techniques.\n\n\n\n\n\n\nLeibowitz, Martin L. 2002. “The Levered p/e Ratio.” Financial Analysts Journal 58 (6): 68–77.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>P/E Ratio</span>"
    ]
  },
  {
    "objectID": "36_valuation.html",
    "href": "36_valuation.html",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "",
    "text": "29.1 Required Packages\nUnderstanding firm valuation, financial health, and corporate maturity is fundamental to financial analysis and investment decision-making. Three widely used measures (e.g., Tobin’s Q, the Altman Z-Score, and company age) capture distinct but complementary dimensions of a firm’s economic standing. Tobin’s Q reflects the market’s assessment of a firm’s value relative to its asset base, the Altman Z-Score predicts the likelihood of financial distress, and company age proxies for organizational maturity and operational stability.\nWhile these measures have been extensively studied in developed markets, particularly the United States (see Lindenberg and Ross 1981; Altman 1968; Gompers, Ishii, and Metrick 2003), their application in emerging markets like Vietnam presents unique challenges and opportunities. The Vietnamese stock market has grown rapidly but retains structural features, such as ownership concentration, state-owned enterprise dominance, limited bond market depth, and evolving accounting standards, which necessitate careful adaptation of standard valuation and distress metrics.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams.update({\n    \"figure.figsize\": (10, 6),\n    \"font.size\": 12,\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 12,\n    \"xtick.labelsize\": 10,\n    \"ytick.labelsize\": 10,\n    \"legend.fontsize\": 10,\n    \"figure.dpi\": 150,\n    \"axes.spines.top\": False,\n    \"axes.spines.right\": False,\n})\n\n# Color palette consistent with Tidy Finance style\ncolors = {\n    \"primary\": \"#1f77b4\",\n    \"secondary\": \"#ff7f0e\",\n    \"tertiary\": \"#2ca02c\",\n    \"quaternary\": \"#d62728\",\n    \"quinary\": \"#9467bd\",\n    \"safe\": \"#2ca02c\",\n    \"alert\": \"#ff7f0e\",\n    \"danger\": \"#d62728\",\n    \"distress\": \"#8b0000\",\n}",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#sec-valuation-tobinq-theory",
    "href": "36_valuation.html#sec-valuation-tobinq-theory",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "30.1 Tobin’s Q: Market Valuation of the Firm",
    "text": "30.1 Tobin’s Q: Market Valuation of the Firm\n\n30.1.1 The Original Concept\nTobin’s Q was introduced by Tobin (1969) as a theoretical link between financial markets and real investment decisions. The fundamental idea is elegant: if the market values a firm’s assets at more than their replacement cost, the firm has an incentive to invest in new capital; if the market values them at less, the firm should not invest, and may even benefit from selling assets.\nFormally, Tobin’s Q is defined as:\n\\[\nQ = \\frac{\\text{Market Value of the Firm}}{\\text{Replacement Cost of Assets}}\n\\tag{30.1}\\]\nWhen \\(Q &gt; 1\\), the market perceives the firm as possessing valuable intangible assets, such as brand equity, managerial talent, proprietary technology, or growth opportunities, that exceed the cost of its tangible asset base. When \\(Q &lt; 1\\), the market effectively values the firm at less than what it would cost to reassemble its assets, suggesting potential undervaluation or the presence of agency costs and organizational inefficiencies.\n\n\n30.1.2 Market Value Decomposition\nThe numerator of Tobin’s Q represents the total market value of all claims on the firm:\n\\[\nMV = CS + PS + ST + LT\n\\tag{30.2}\\]\nwhere:\n\n\\(CS\\) = Market value of common stock (shares outstanding \\(\\times\\) market price)\n\\(PS\\) = Market value of preferred stock\n\\(ST\\) = Market value of short-term debt\n\\(LT\\) = Market value of long-term debt\n\n\n\n30.1.3 Replacement Cost of Assets\nThe denominator captures the replacement cost of the firm’s asset base:\n\\[\nRC = TA + (RNP - HNP) + (RINV - HINV)\n\\tag{30.3}\\]\nwhere:\n\n\\(TA\\) = Total assets as reported\n\\(RNP\\) = Replacement cost of net plant and equipment\n\\(HNP\\) = Historical (book) value of net plant and equipment\n\\(RINV\\) = Replacement cost of inventories\n\\(HINV\\) = Historical (book) value of inventories\n\nThe replacement cost adjustments for plant and equipment involve recursive calculations that account for depreciation rates and, in more detailed estimations, technical progress rates (Lindenberg and Ross 1981). For inventories, the adjustment depends on the inventory accounting method: LIFO (Last In, First Out), FIFO (First In, First Out), average cost, or retail cost.\n\n\n30.1.4 Simplified Tobin’s Q\nIn practice, the full replacement cost computation requires data that are often unavailable, particularly in emerging markets. Gompers, Ishii, and Metrick (2003) popularized a simplified version:\n\\[\nQ_{\\text{simple}} = \\frac{TA + ME - BE}{TA}\n\\tag{30.4}\\]\nwhere \\(ME\\) is the market value of equity and \\(BE\\) is the book value of equity. This formulation assumes that the book values of debt and preferred stock approximate their market values, and that total assets approximate the replacement cost of the firm’s asset base. Despite its simplicity, this measure has been shown to correlate well with more elaborate constructions and has become the standard in empirical corporate finance research.\n\n\n30.1.5 Chung and Pruitt Approximation\nChung and Pruitt (1994) proposed another widely used approximation:\n\\[\nQ_{\\text{CP}} = \\frac{ME + PS + DEBT}{TA}\n\\tag{30.5}\\]\nwhere \\(DEBT = \\text{Current Liabilities} - \\text{Current Assets} + \\text{Book Value of Inventories} + \\text{Long-term Debt}\\). This formulation captures the net debt position more precisely than the Gompers, Ishii, and Metrick (2003) version.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#sec-valuation-zscore-theory",
    "href": "36_valuation.html#sec-valuation-zscore-theory",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "30.2 Altman Z-Score: Predicting Financial Distress",
    "text": "30.2 Altman Z-Score: Predicting Financial Distress\n\n30.2.1 The Original Model\nThe Altman Z-Score, developed by Altman (1968), is a multivariate discriminant analysis model that predicts the probability of corporate bankruptcy within a two-year horizon. The model was originally estimated using a matched sample of bankrupt and non-bankrupt U.S. manufacturing firms and takes the form:\n\\[\nZ = 3.3 \\cdot X_1 + 0.999 \\cdot X_2 + 0.6 \\cdot X_3 + 1.2 \\cdot X_4 + 1.4 \\cdot X_5\n\\tag{30.6}\\]\nwhere the five financial ratios are in Table 30.1\n\n\n\nTable 30.1: Components of the Altman Z-Score\n\n\n\n\n\n\n\n\n\n\nVariable\nFormula\nInterpretation\n\n\n\n\n\\(X_1\\)\n\\(\\frac{\\text{EBIT}}{\\text{Total Assets}}\\)\nEarning power of assets\n\n\n\\(X_2\\)\n\\(\\frac{\\text{Net Sales}}{\\text{Total Assets}}\\)\nTotal asset turnover\n\n\n\\(X_3\\)\n\\(\\frac{\\text{Market Value of Equity}}{\\text{Total Liabilities}}\\)\nLeverage ratio (inverse)\n\n\n\\(X_4\\)\n\\(\\frac{\\text{Working Capital}}{\\text{Total Assets}}\\)\nShort-term liquidity\n\n\n\\(X_5\\)\n\\(\\frac{\\text{Retained Earnings}}{\\text{Total Assets}}\\)\nCumulative profitability\n\n\n\n\n\n\nThe interpretation zones are in Table 30.2\n\n\n\nTable 30.2: Altman Z-Score interpretation zones\n\n\n\n\n\n\n\n\n\nZ-Score Range\nInterpretation\n\n\n\n\n\\(Z &gt; 2.99\\)\nSafe zone-low probability of financial distress\n\n\n\\(2.70 \\leq Z \\leq 2.99\\)\nGrey zone-on alert, moderate risk\n\n\n\\(1.80 \\leq Z &lt; 2.70\\)\nDistress zone-significant bankruptcy risk within 2 years\n\n\n\\(Z &lt; 1.80\\)\nHigh distress-very high probability of financial failure\n\n\n\n\n\n\n\n\n30.2.2 The Z’-Score Model for Private Firms\nBecause the original model uses market value of equity in \\(X_3\\), Altman and Hotchkiss (2010) developed the Z’-Score for private (non-publicly traded) firms by replacing market capitalization with book value of equity:\n\\[\nZ' = 0.717 \\cdot X_1' + 0.847 \\cdot X_2' + 3.107 \\cdot X_3' + 0.420 \\cdot X_4' + 0.998 \\cdot X_5'\n\\tag{30.7}\\]\nwhere \\(X_4' = \\frac{\\text{Book Value of Equity}}{\\text{Total Liabilities}}\\), and the remaining variables are defined as in the original model but with re-estimated coefficients.\n\n\n30.2.3 The Z’’-Score Model for Emerging Markets\nMost relevant for Vietnam, Altman and Hotchkiss (2010) also developed the Z’’-Score for non-manufacturing and emerging market firms:\n\\[\nZ'' = 3.25 + 6.56 \\cdot X_1'' + 3.26 \\cdot X_2'' + 6.72 \\cdot X_3'' + 1.05 \\cdot X_4''\n\\tag{30.8}\\]\nwhere:\n\n\\(X_1'' = \\frac{\\text{Working Capital}}{\\text{Total Assets}}\\)\n\\(X_2'' = \\frac{\\text{Retained Earnings}}{\\text{Total Assets}}\\)\n\\(X_3'' = \\frac{\\text{EBIT}}{\\text{Total Assets}}\\)\n\\(X_4'' = \\frac{\\text{Book Value of Equity}}{\\text{Total Liabilities}}\\)\n\nNote that this model drops the sales/total assets ratio (\\(X_2\\) in the original model) to minimize industry effects and adds an intercept. The classification zones shift accordingly (Table 30.3).\n\n\n\nTable 30.3: Z’’-Score interpretation zones for emerging markets\n\n\n\n\n\nZ’’-Score Range\nInterpretation\n\n\n\n\n\\(Z'' &gt; 2.60\\)\nSafe zone\n\n\n\\(1.10 \\leq Z'' \\leq 2.60\\)\nGrey zone\n\n\n\\(Z'' &lt; 1.10\\)\nDistress zone",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#sec-valuation-age-theory",
    "href": "36_valuation.html#sec-valuation-age-theory",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "30.3 Company Age: Measuring Corporate Maturity",
    "text": "30.3 Company Age: Measuring Corporate Maturity\nCompany age captures the accumulated experience, reputation, and organizational capital of a firm. Older firms typically exhibit more stable operations, established competitive advantages, and predictable cash flow patterns (Coad et al. 2018). Age is commonly used as a control variable in corporate finance research, and its relationship with performance is theoretically ambiguous: older firms benefit from learning effects and reputation but may suffer from organizational rigidity and declining innovation (Huergo and Jaumandreu 2004).\nThe ideal measure is the number of years since founding. When founding dates are unavailable, common proxies include:\n\nListing age: Years since the first IPO or listing on a stock exchange.\nData age: Years since the first appearance in financial databases.\nIncorporation age: Years since the date of legal incorporation.\n\nIn the Vietnamese context, we can use multiple proxies, including the date of first listing on HOSE or HNX, the first available financial statement date, and (when available) the founding date from company profiles.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#institutional-features-affecting-valuation-measures",
    "href": "36_valuation.html#institutional-features-affecting-valuation-measures",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "31.1 Institutional Features Affecting Valuation Measures",
    "text": "31.1 Institutional Features Affecting Valuation Measures\nThe Vietnamese stock market has several institutional features that are important to consider when computing and interpreting Tobin’s Q, Altman Z-Score, and company age.\n\n31.1.1 State Ownership and Equitization\nA significant proportion of Vietnamese listed firms are former state-owned enterprises (SOEs) that underwent equitization (cổ phần hóa). These firms often retain substantial state ownership, which can affect:\n\nTobin’s Q: State ownership may depress Q if markets perceive government influence as reducing efficiency, or it may elevate Q if state connections provide preferential access to resources and contracts.\nZ-Score: SOEs may have implicit government guarantees that reduce actual bankruptcy risk below what the Z-Score predicts.\nAge: The true operational age of equitized SOEs may far exceed their listing age, creating measurement challenges.\n\n\n\n31.1.2 Foreign Ownership Limits\nVietnam imposes foreign ownership limits (FOL) on listed companies, typically capped at 49% for most sectors (with some exceptions). This constraint can create price premiums for stocks approaching the FOL ceiling, potentially inflating Tobin’s Q for these firms (Vo 2015).\n\n\n31.1.3 Accounting Standards\nVietnamese Accounting Standards (VAS) differ from International Financial Reporting Standards (IFRS) in several ways relevant to our measures:\n\nHistorical cost basis: VAS relies more heavily on historical cost, which can cause book values to diverge substantially from replacement costs, affecting both Q and Z-Score calculations.\nLimited fair value measurement: Unlike IFRS, VAS limits fair value measurement for many asset classes, making book-value-based proxies less reliable.\nInventory methods: Vietnamese firms use various inventory valuation methods (FIFO, weighted average), which affect the book value of inventories and hence the Z-Score’s working capital component.\n\n\n\n31.1.4 Market Microstructure\nVietnam’s market features daily price limits (currently \\(\\pm\\) 7% on HOSE, \\(\\pm\\) 10% on HNX, \\(\\pm\\) 15% on UPCoM), which can prevent prices from reaching equilibrium values quickly. This means that market capitalization at any point may not fully reflect available information, introducing noise into market-value-based measures like Tobin’s Q.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#loading-and-cleaning-financial-statement-data",
    "href": "36_valuation.html#loading-and-cleaning-financial-statement-data",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "32.1 Loading and Cleaning Financial Statement Data",
    "text": "32.1 Loading and Cleaning Financial Statement Data\nWe begin by loading the annual financial statement data and constructing the variables needed for our three measures.\n\n# ============================================================================\n# In practice, replace this section with actual DataCore.vn API calls:\n#\n#   from datacore import DataCoreClient\n#   client = DataCoreClient(api_key=\"your_api_key\")\n#   financials = client.get_financial_statements(\n#       frequency=\"annual\",\n#       start_date=\"2005-01-01\",\n#       end_date=\"2024-12-31\"\n#   )\n#   market = client.get_market_data(frequency=\"daily\")\n#   profiles = client.get_company_profiles()\n#\n# For demonstration purposes, we simulate realistic Vietnamese market data.\n# ============================================================================\n\nnp.random.seed(42)\n\nn_firms = 300\nyears = range(2008, 2025)\nexchanges = [\"HOSE\", \"HNX\", \"UPCoM\"]\nindustries = [\n    \"Bất động sản\", \"Ngân hàng\", \"Thực phẩm & Đồ uống\",\n    \"Xây dựng & Vật liệu\", \"Công nghệ thông tin\", \"Bán lẻ\",\n    \"Dầu khí\", \"Thép\", \"Dệt may\", \"Dược phẩm\",\n    \"Điện lực\", \"Vận tải & Logistics\", \"Hóa chất\",\n    \"Chứng khoán\", \"Bảo hiểm\"\n]\n\n# Generate firm profiles\nfirm_ids = [f\"VN{str(i).zfill(4)}\" for i in range(1, n_firms + 1)]\nfirm_profiles = pd.DataFrame({\n    \"ticker\": firm_ids,\n    \"exchange\": np.random.choice(exchanges, n_firms, p=[0.5, 0.3, 0.2]),\n    \"industry\": np.random.choice(industries, n_firms),\n    \"founding_year\": np.random.randint(1975, 2015, n_firms),\n    \"listing_year\": np.random.randint(2000, 2020, n_firms),\n    \"state_ownership_pct\": np.clip(\n        np.random.beta(2, 5, n_firms) * 100, 0, 75\n    ).round(1),\n})\nfirm_profiles[\"listing_year\"] = np.maximum(\n    firm_profiles[\"listing_year\"],\n    firm_profiles[\"founding_year\"] + 1\n)\n\n# Generate panel data\nrecords = []\nfor _, firm in firm_profiles.iterrows():\n    start_year = max(firm[\"listing_year\"], 2008)\n    # Base financial characteristics (firm fixed effects)\n    base_ta = np.exp(np.random.normal(14, 1.5))  # Total assets in VND millions\n    base_profitability = np.random.normal(0.08, 0.05)\n    base_leverage = np.random.beta(3, 3)\n    base_turnover = np.random.gamma(2, 0.4)\n\n    for year in years:\n        if year &lt; start_year:\n            continue\n        if np.random.random() &lt; 0.02:  # 2% chance of delisting\n            break\n\n        # Time-varying components with persistence\n        shock = np.random.normal(0, 0.15)\n        growth = np.random.normal(0.08, 0.05)\n\n        ta = base_ta * (1 + growth) ** (year - start_year) * np.exp(shock)\n        profitability = np.clip(base_profitability + np.random.normal(0, 0.03), -0.3, 0.5)\n        leverage = np.clip(base_leverage + np.random.normal(0, 0.05), 0.05, 0.95)\n\n        lt = ta * leverage * np.random.uniform(0.4, 0.7)\n        total_liabilities = ta * leverage\n        ct_liabilities = total_liabilities - lt\n\n        seq = ta * (1 - leverage)\n        sale = ta * np.clip(base_turnover + np.random.normal(0, 0.1), 0.1, 5)\n        ebit = ta * profitability\n        ni = ebit * np.random.uniform(0.6, 0.9)\n        re = seq * np.random.uniform(-0.2, 0.8)\n        act = ta * np.random.uniform(0.2, 0.7)\n        lct = ct_liabilities\n\n        # Market value with noise and sentiment\n        market_premium = np.random.lognormal(0, 0.4)\n        me = seq * market_premium\n        prcc = me / max(np.random.uniform(50, 500), 1)\n        csho = me / max(prcc, 0.01)\n\n        # Preferred stock (rare in Vietnam, mostly zero)\n        pstk = 0 if np.random.random() &gt; 0.05 else seq * np.random.uniform(0, 0.1)\n\n        # Net plant and equipment\n        ppent = ta * np.random.uniform(0.1, 0.6)\n        invt = ta * np.random.uniform(0.05, 0.3)\n\n        # Deferred taxes and investment tax credit (typically small in VN)\n        txdb = ta * np.random.uniform(0, 0.02)\n        itcb = 0\n\n        records.append({\n            \"ticker\": firm[\"ticker\"],\n            \"year\": year,\n            \"datadate\": pd.Timestamp(year, 12, 31),\n            \"at\": ta,            # Total Assets\n            \"seq\": seq,          # Stockholders' Equity\n            \"lt\": lt,            # Long-term Debt\n            \"lct\": lct,          # Current Liabilities\n            \"tlb\": total_liabilities,  # Total Liabilities\n            \"sale\": sale,        # Net Sales/Revenue\n            \"ebit\": ebit,        # EBIT\n            \"ni\": ni,            # Net Income\n            \"re_var\": re,        # Retained Earnings\n            \"act\": act,          # Current Assets\n            \"ppent\": ppent,      # Net Plant & Equipment\n            \"invt\": invt,        # Inventories\n            \"txdb\": txdb,        # Deferred Taxes\n            \"itcb\": itcb,        # Investment Tax Credit\n            \"pstk\": pstk,        # Preferred Stock\n            \"me\": me,            # Market Value of Equity\n            \"prcc\": prcc,        # Price at Calendar Year End\n            \"csho\": csho,        # Shares Outstanding\n        })\n\ndf = pd.DataFrame(records)\ndf = df.merge(firm_profiles[[\"ticker\", \"exchange\", \"industry\",\n                              \"founding_year\", \"listing_year\",\n                              \"state_ownership_pct\"]],\n              on=\"ticker\", how=\"left\")\n\nprint(f\"Panel dimensions: {df.shape[0]:,} firm-year observations\")\nprint(f\"Number of unique firms: {df['ticker'].nunique()}\")\nprint(f\"Year range: {df['year'].min()}–{df['year'].max()}\")\nprint(f\"Exchanges: {df['exchange'].value_counts().to_dict()}\")\n\nPanel dimensions: 3,484 firm-year observations\nNumber of unique firms: 297\nYear range: 2008–2024\nExchanges: {'HOSE': 1701, 'HNX': 1118, 'UPCoM': 665}",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#variable-definitions-and-mapping",
    "href": "36_valuation.html#variable-definitions-and-mapping",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "32.2 Variable Definitions and Mapping",
    "text": "32.2 Variable Definitions and Mapping\nTable 32.1 provides the mapping between standard variable names and the corresponding fields.\n\nvariable_map = pd.DataFrame({\n    \"Compustat Variable\": [\n        \"AT\", \"SEQ\", \"LT\", \"LCT\", \"SALE\", \"EBIT\", \"NI\",\n        \"RE\", \"ACT\", \"PPENT\", \"INVT\", \"TXDB\", \"ITCB\",\n        \"PSTK/PSTKRV/PSTKL\", \"PRCC_C\", \"CSHO\", \"DLDTE\", \"DLRSN\"\n    ],\n    \"Description\": [\n        \"Total Assets\", \"Stockholders' Equity\", \"Long-term Debt\",\n        \"Current Liabilities\", \"Net Sales/Revenue\",\n        \"Earnings Before Interest & Taxes\", \"Net Income\",\n        \"Retained Earnings\", \"Current Assets\",\n        \"Net Plant & Equipment\", \"Total Inventories\",\n        \"Deferred Taxes\", \"Investment Tax Credit\",\n        \"Preferred Stock (various)\", \"Price Close (Calendar Year)\",\n        \"Common Shares Outstanding\", \"Delisting Date\",\n        \"Delisting Reason\"\n    ],\n    \"DataCore.vn Equivalent\": [\n        \"tong_tai_san\", \"von_chu_so_huu\", \"no_dai_han\",\n        \"no_ngan_han\", \"doanh_thu_thuan\",\n        \"loi_nhuan_truoc_thue_va_lai_vay\", \"loi_nhuan_sau_thue\",\n        \"loi_nhuan_chua_phan_phoi\", \"tai_san_ngan_han\",\n        \"tai_san_co_dinh_huu_hinh\", \"hang_ton_kho\",\n        \"thue_thu_nhap_hoan_lai\", \"N/A (not applicable in VAS)\",\n        \"co_phieu_uu_dai\", \"gia_dong_cua_cuoi_nam\",\n        \"so_luong_co_phieu_luu_hanh\", \"ngay_huy_niem_yet\",\n        \"ly_do_huy_niem_yet\"\n    ],\n    \"VAS Account\": [\n        \"BS.100\", \"BS.400\", \"BS.330\", \"BS.310\",\n        \"IS.10\", \"Computed\", \"IS.60\",\n        \"BS.421\", \"BS.100\", \"BS.221\", \"BS.141\",\n        \"BS.262\", \"—\", \"BS.411b\", \"Market\", \"Market\",\n        \"Profile\", \"Profile\"\n    ]\n})\n\nvariable_map.style.set_properties(**{\n    \"text-align\": \"left\",\n    \"font-size\": \"10pt\"\n}).set_table_styles([\n    {\"selector\": \"th\", \"props\": [\n        (\"background-color\", \"#1f77b4\"),\n        (\"color\", \"white\"),\n        (\"font-weight\", \"bold\"),\n        (\"text-align\", \"left\"),\n        (\"padding\", \"8px\")\n    ]},\n    {\"selector\": \"td\", \"props\": [(\"padding\", \"6px\")]},\n])\n\n\n\nTable 32.1: Variable mapping\n\n\n\n\n\n\n\n\n \nCompustat Variable\nDescription\nDataCore.vn Equivalent\nVAS Account\n\n\n\n\n0\nAT\nTotal Assets\ntong_tai_san\nBS.100\n\n\n1\nSEQ\nStockholders' Equity\nvon_chu_so_huu\nBS.400\n\n\n2\nLT\nLong-term Debt\nno_dai_han\nBS.330\n\n\n3\nLCT\nCurrent Liabilities\nno_ngan_han\nBS.310\n\n\n4\nSALE\nNet Sales/Revenue\ndoanh_thu_thuan\nIS.10\n\n\n5\nEBIT\nEarnings Before Interest & Taxes\nloi_nhuan_truoc_thue_va_lai_vay\nComputed\n\n\n6\nNI\nNet Income\nloi_nhuan_sau_thue\nIS.60\n\n\n7\nRE\nRetained Earnings\nloi_nhuan_chua_phan_phoi\nBS.421\n\n\n8\nACT\nCurrent Assets\ntai_san_ngan_han\nBS.100\n\n\n9\nPPENT\nNet Plant & Equipment\ntai_san_co_dinh_huu_hinh\nBS.221\n\n\n10\nINVT\nTotal Inventories\nhang_ton_kho\nBS.141\n\n\n11\nTXDB\nDeferred Taxes\nthue_thu_nhap_hoan_lai\nBS.262\n\n\n12\nITCB\nInvestment Tax Credit\nN/A (not applicable in VAS)\n—\n\n\n13\nPSTK/PSTKRV/PSTKL\nPreferred Stock (various)\nco_phieu_uu_dai\nBS.411b\n\n\n14\nPRCC_C\nPrice Close (Calendar Year)\ngia_dong_cua_cuoi_nam\nMarket\n\n\n15\nCSHO\nCommon Shares Outstanding\nso_luong_co_phieu_luu_hanh\nMarket\n\n\n16\nDLDTE\nDelisting Date\nngay_huy_niem_yet\nProfile\n\n\n17\nDLRSN\nDelisting Reason\nly_do_huy_niem_yet\nProfile",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#data-quality-checks",
    "href": "36_valuation.html#data-quality-checks",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "32.3 Data Quality Checks",
    "text": "32.3 Data Quality Checks\nBefore computing any measures, we perform essential data quality checks that are particularly important for Vietnamese data.\n\ndef data_quality_report(df):\n    \"\"\"Generate a comprehensive data quality report.\"\"\"\n    report = {}\n\n    # Check for negative total assets\n    report[\"Negative total assets\"] = (df[\"at\"] &lt; 0).sum()\n\n    # Check for negative equity (acceptable but noteworthy)\n    report[\"Negative equity\"] = (df[\"seq\"] &lt;= 0).sum()\n\n    # Check for missing critical variables\n    critical_vars = [\"at\", \"seq\", \"sale\", \"ebit\", \"me\"]\n    for var in critical_vars:\n        report[f\"Missing {var}\"] = df[var].isna().sum()\n\n    # Check for extreme values (potential data errors)\n    report[\"Extreme leverage (&gt;100%)\"] = (df[\"tlb\"] / df[\"at\"] &gt; 1.0).sum()\n    report[\"Extreme sales/assets (&gt;10)\"] = (df[\"sale\"] / df[\"at\"] &gt; 10).sum()\n\n    return pd.Series(report, name=\"Count\")\n\nquality = data_quality_report(df)\nprint(\"Data Quality Report\")\nprint(\"=\" * 45)\nfor item, count in quality.items():\n    status = \"✓\" if count == 0 else \"⚠\"\n    print(f\"  {status} {item}: {count:,}\")\n\n# Apply filters\ndf_clean = df.copy()\ndf_clean = df_clean[df_clean[\"at\"] &gt; 0]  # Positive total assets\ndf_clean = df_clean[df_clean[\"seq\"] &gt; 0]  # Positive equity (for BE calculation)\nprint(f\"\\nObservations after cleaning: {len(df_clean):,} \"\n      f\"(dropped {len(df) - len(df_clean):,})\")\n\nData Quality Report\n=============================================\n  ✓ Negative total assets: 0\n  ✓ Negative equity: 0\n  ✓ Missing at: 0\n  ✓ Missing seq: 0\n  ✓ Missing sale: 0\n  ✓ Missing ebit: 0\n  ✓ Missing me: 0\n  ✓ Extreme leverage (&gt;100%): 0\n  ✓ Extreme sales/assets (&gt;10): 0\n\nObservations after cleaning: 3,484 (dropped 0)",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#book-value-of-equity",
    "href": "36_valuation.html#book-value-of-equity",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "33.1 Book Value of Equity",
    "text": "33.1 Book Value of Equity\nFollowing Daniel and Titman (1997), we compute the book value of equity as:\n\\[\nBE = SEQ + TXDB + ITCB - PREF\n\\tag{33.1}\\]\nwhere \\(PREF\\) is the preferred stock value, using the redemption value if available, otherwise the liquidating value, and finally the carrying value as a last resort. In the Vietnamese context, preferred stock is relatively rare among listed companies, so \\(PREF\\) is often zero.\n\ndef compute_book_equity(df):\n    \"\"\"\n    Compute book value of equity following Daniel and Titman (1997).\n\n    In Vietnam, preferred stock is uncommon among listed firms.\n    The investment tax credit (ITCB) is not applicable under VAS,\n    so we set it to zero when missing.\n    \"\"\"\n    result = df.copy()\n\n    # Preferred stock: use coalesce logic\n    result[\"pref\"] = result[\"pstk\"].fillna(0)\n\n    # Book equity = Shareholders' equity + Deferred taxes + ITC - Preferred\n    result[\"be\"] = (\n        result[\"seq\"]\n        + result[\"txdb\"].fillna(0)\n        + result[\"itcb\"].fillna(0)\n        - result[\"pref\"]\n    )\n\n    return result\n\ndf_clean = compute_book_equity(df_clean)\n\nprint(\"Book Equity Summary Statistics (VND millions)\")\nprint(df_clean[\"be\"].describe().apply(lambda x: f\"{x:,.0f}\"))\n\nBook Equity Summary Statistics (VND millions)\ncount          3,484\nmean       3,521,607\nstd        9,055,435\nmin            4,633\n25%          386,441\n50%        1,112,062\n75%        3,210,269\nmax      247,845,397\nName: be, dtype: str",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#market-value-of-equity",
    "href": "36_valuation.html#market-value-of-equity",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "33.2 Market Value of Equity",
    "text": "33.2 Market Value of Equity\nThe market value of equity is computed using the calendar year-end stock price and shares outstanding:\n\\[\nME = P_{\\text{close}} \\times \\text{CSHO}\n\\tag{33.2}\\]\nFor Vietnamese firms, this is obtained from the closing price on the last trading day of the fiscal year. Most Vietnamese firms have a December 31 fiscal year end, though some (particularly in agriculture and banking) may differ.\n\n# ME is already computed in our simulated data as prcc * csho\n# In practice with DataCore.vn:\n#   me = df[\"gia_dong_cua_cuoi_nam\"] * df[\"so_luong_co_phieu_luu_hanh\"]\n\ndf_clean[\"me\"] = df_clean[\"prcc\"] * df_clean[\"csho\"]\n\nprint(\"Market Equity Summary Statistics (VND millions)\")\nprint(df_clean[\"me\"].describe().apply(lambda x: f\"{x:,.0f}\"))\n\nMarket Equity Summary Statistics (VND millions)\ncount          3,484\nmean       3,653,445\nstd        9,747,318\nmin            3,379\n25%          370,119\n50%        1,106,646\n75%        3,029,093\nmax      286,258,816\nName: me, dtype: str",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#simplified-tobins-q-1",
    "href": "36_valuation.html#simplified-tobins-q-1",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "33.3 Simplified Tobin’s Q",
    "text": "33.3 Simplified Tobin’s Q\nWe implement the Gompers, Ishii, and Metrick (2003) simplified version:\n\\[\nQ_{\\text{simple}} = \\frac{AT + ME - BE}{AT}\n\\tag{33.3}\\]\nThis can be rewritten as:\n\\[\nQ_{\\text{simple}} = 1 + \\frac{ME - BE}{AT}\n\\tag{33.4}\\]\nwhich makes the interpretation clear: Tobin’s Q equals one plus the market-to-book premium (or discount) scaled by total assets.\n\ndef compute_tobins_q(df):\n    \"\"\"\n    Compute multiple variants of Tobin's Q.\n\n    Variants:\n    1. Simple Q (Gompers et al., 2003)\n    2. Chung-Pruitt Q\n    3. Market-to-Book ratio (for comparison)\n    \"\"\"\n    result = df.copy()\n\n    # --- Variant 1: Simple Q (Gompers, Ishii, Metrick 2003) ---\n    result[\"tobin_q_simple\"] = (result[\"at\"] + result[\"me\"] - result[\"be\"]) / result[\"at\"]\n\n    # --- Variant 2: Chung-Pruitt Approximation ---\n    # DEBT = Current Liabilities - Current Assets + Inventories + LT Debt\n    result[\"debt_cp\"] = (\n        result[\"lct\"].fillna(0)\n        - result[\"act\"].fillna(0)\n        + result[\"invt\"].fillna(0)\n        + result[\"lt\"].fillna(0)\n    )\n    result[\"tobin_q_cp\"] = (\n        (result[\"me\"] + result[\"pstk\"].fillna(0) + result[\"debt_cp\"]) / result[\"at\"]\n    )\n\n    # --- Market-to-Book Ratio ---\n    result[\"mtb\"] = np.where(result[\"be\"] &gt; 0, result[\"me\"] / result[\"be\"], np.nan)\n\n    return result\n\ndf_clean = compute_tobins_q(df_clean)\n\n# Summary statistics\nq_vars = [\"tobin_q_simple\", \"tobin_q_cp\", \"mtb\"]\nq_summary = df_clean[q_vars].describe(percentiles=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])\nq_summary = q_summary.round(3)\nq_summary.columns = [\"Simple Q\", \"Chung-Pruitt Q\", \"Market-to-Book\"]\n\nprint(\"Tobin's Q Summary Statistics\")\nprint(q_summary.to_string())\n\nTobin's Q Summary Statistics\n       Simple Q  Chung-Pruitt Q  Market-to-Book\ncount  3484.000        3484.000        3484.000\nmean      1.031           0.766           1.058\nstd       0.243           0.296           0.445\nmin       0.357           0.008           0.233\n1%        0.602           0.189           0.382\n5%        0.713           0.332           0.496\n25%       0.886           0.568           0.742\n50%       0.989           0.738           0.974\n75%       1.132           0.926           1.287\n95%       1.481           1.290           1.904\n99%       1.916           1.668           2.492\nmax       2.804           2.433           3.189",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#winsorizing-extreme-values",
    "href": "36_valuation.html#winsorizing-extreme-values",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "33.4 Winsorizing Extreme Values",
    "text": "33.4 Winsorizing Extreme Values\nTobin’s Q values can be heavily influenced by outliers, particularly in emerging markets where data quality may be inconsistent. We winsorize at the 1st and 99th percentiles.\n\ndef winsorize(series, lower=0.01, upper=0.99):\n    \"\"\"Winsorize a pandas Series at specified percentiles.\"\"\"\n    low = series.quantile(lower)\n    high = series.quantile(upper)\n    return series.clip(lower=low, upper=high)\n\n# Winsorize Q measures\nfor var in [\"tobin_q_simple\", \"tobin_q_cp\", \"mtb\"]:\n    df_clean[f\"{var}_w\"] = winsorize(df_clean[var])\n\nprint(\"Effect of Winsorization on Simple Tobin's Q:\")\nprint(f\"  Before: mean={df_clean['tobin_q_simple'].mean():.3f}, \"\n      f\"std={df_clean['tobin_q_simple'].std():.3f}\")\nprint(f\"  After:  mean={df_clean['tobin_q_simple_w'].mean():.3f}, \"\n      f\"std={df_clean['tobin_q_simple_w'].std():.3f}\")\n\nEffect of Winsorization on Simple Tobin's Q:\n  Before: mean=1.031, std=0.243\n  After:  mean=1.030, std=0.233",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#cross-sectional-distribution-of-tobins-q",
    "href": "36_valuation.html#cross-sectional-distribution-of-tobins-q",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "33.5 Cross-Sectional Distribution of Tobin’s Q",
    "text": "33.5 Cross-Sectional Distribution of Tobin’s Q\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Latest year distribution\nlatest_year = df_clean[\"year\"].max()\nlatest_data = df_clean[df_clean[\"year\"] == latest_year]\n\n# Histogram\naxes[0].hist(\n    latest_data[\"tobin_q_simple_w\"].dropna(),\n    bins=50, color=colors[\"primary\"], alpha=0.7, edgecolor=\"white\"\n)\naxes[0].axvline(x=1, color=colors[\"quaternary\"], linestyle=\"--\", linewidth=2, label=\"Q = 1\")\naxes[0].set_xlabel(\"Tobin's Q (Simple)\")\naxes[0].set_ylabel(\"Number of Firms\")\naxes[0].set_title(f\"Cross-Sectional Distribution ({latest_year})\")\naxes[0].legend()\n\n# Box plot by exchange\nexchange_data = latest_data[[\"exchange\", \"tobin_q_simple_w\"]].dropna()\nexchanges_sorted = exchange_data.groupby(\"exchange\")[\"tobin_q_simple_w\"].median().sort_values().index\n\nbox_data = [exchange_data[exchange_data[\"exchange\"] == ex][\"tobin_q_simple_w\"].values\n            for ex in exchanges_sorted]\n\nbp = axes[1].boxplot(box_data, labels=exchanges_sorted, patch_artist=True,\n                     medianprops=dict(color=\"black\", linewidth=2))\nbox_colors = [colors[\"primary\"], colors[\"secondary\"], colors[\"tertiary\"]]\nfor patch, color in zip(bp[\"boxes\"], box_colors):\n    patch.set_facecolor(color)\n    patch.set_alpha(0.7)\n\naxes[1].axhline(y=1, color=colors[\"quaternary\"], linestyle=\"--\", linewidth=1.5, alpha=0.7)\naxes[1].set_ylabel(\"Tobin's Q (Simple)\")\naxes[1].set_title(f\"Tobin's Q by Exchange ({latest_year})\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 33.1: Distribution of Tobin’s Q across Vietnamese listed firms (2024). The vertical dashed line at Q=1 indicates the theoretical threshold where market value equals replacement cost.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#time-series-evolution-of-tobins-q",
    "href": "36_valuation.html#time-series-evolution-of-tobins-q",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "33.6 Time-Series Evolution of Tobin’s Q",
    "text": "33.6 Time-Series Evolution of Tobin’s Q\n\n# Compute annual statistics by exchange\nannual_q = (\n    df_clean\n    .groupby([\"year\", \"exchange\"])[\"tobin_q_simple_w\"]\n    .agg([\"median\", lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)])\n    .reset_index()\n)\nannual_q.columns = [\"year\", \"exchange\", \"median\", \"q25\", \"q75\"]\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nexchange_colors = {\"HOSE\": colors[\"primary\"], \"HNX\": colors[\"secondary\"],\n                   \"UPCoM\": colors[\"tertiary\"]}\n\nfor exchange, color in exchange_colors.items():\n    data = annual_q[annual_q[\"exchange\"] == exchange]\n    ax.plot(data[\"year\"], data[\"median\"], color=color, linewidth=2.5,\n            label=exchange, marker=\"o\", markersize=4)\n    ax.fill_between(data[\"year\"], data[\"q25\"], data[\"q75\"],\n                    color=color, alpha=0.15)\n\nax.axhline(y=1, color=\"gray\", linestyle=\"--\", linewidth=1, alpha=0.7, label=\"Q = 1\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Tobin's Q (Median)\")\nax.set_title(\"Evolution of Tobin's Q in the Vietnamese Market\")\nax.legend(loc=\"upper right\")\nax.set_xlim(2008, latest_year)\nax.xaxis.set_major_locator(mticker.MultipleLocator(2))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 33.2: Evolution of median Tobin’s Q across Vietnamese exchanges (2008–2024). Shaded areas represent the interquartile range.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#tobins-q-by-industry",
    "href": "36_valuation.html#tobins-q-by-industry",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "33.7 Tobin’s Q by Industry",
    "text": "33.7 Tobin’s Q by Industry\n\nlatest_industry_q = (\n    latest_data\n    .groupby(\"industry\")[\"tobin_q_simple_w\"]\n    .agg([\"median\", \"mean\", \"count\"])\n    .reset_index()\n    .sort_values(\"median\", ascending=True)\n)\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\nbar_colors = [colors[\"quaternary\"] if m &lt; 1 else colors[\"primary\"]\n              for m in latest_industry_q[\"median\"]]\n\nax.barh(\n    latest_industry_q[\"industry\"],\n    latest_industry_q[\"median\"],\n    color=bar_colors, alpha=0.8, edgecolor=\"white\"\n)\nax.axvline(x=1, color=\"gray\", linestyle=\"--\", linewidth=1.5, alpha=0.7)\nax.set_xlabel(\"Median Tobin's Q\")\nax.set_title(f\"Tobin's Q by Industry ({latest_year})\")\n\n# Add count annotations\nfor i, (_, row) in enumerate(latest_industry_q.iterrows()):\n    ax.text(row[\"median\"] + 0.02, i, f'n={int(row[\"count\"])}',\n            va=\"center\", fontsize=9, color=\"gray\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 33.3: Median Tobin’s Q by industry in Vietnam (latest available year). Industries are sorted by median Q value.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#original-z-score-for-listed-firms",
    "href": "36_valuation.html#original-z-score-for-listed-firms",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "34.1 Original Z-Score for Listed Firms",
    "text": "34.1 Original Z-Score for Listed Firms\n\ndef compute_altman_z(df):\n    \"\"\"\n    Compute three variants of the Altman Z-Score:\n    1. Original Z-Score (for listed manufacturing firms)\n    2. Z'-Score (for private firms, using book equity)\n    3. Z''-Score (for emerging markets / non-manufacturing)\n    \"\"\"\n    result = df.copy()\n\n    # Common components\n    result[\"wc\"] = result[\"act\"].fillna(0) - result[\"lct\"].fillna(0)  # Working Capital\n\n    # --- Original Z-Score ---\n    # Z = 3.3*(EBIT/TA) + 0.999*(Sales/TA) + 0.6*(ME/TL) + 1.2*(WC/TA) + 1.4*(RE/TA)\n    x1 = result[\"ebit\"] / result[\"at\"]\n    x2 = result[\"sale\"] / result[\"at\"]\n    x3 = np.where(result[\"tlb\"] &gt; 0, result[\"me\"] / result[\"tlb\"], np.nan)\n    x4 = result[\"wc\"] / result[\"at\"]\n    x5 = result[\"re_var\"].fillna(0) / result[\"at\"]\n\n    result[\"z_x1\"] = x1  # Earning power\n    result[\"z_x2\"] = x2  # Asset turnover\n    result[\"z_x3\"] = x3  # Leverage (inverse)\n    result[\"z_x4\"] = x4  # Liquidity\n    result[\"z_x5\"] = x5  # Cumulative profitability\n\n    result[\"altman_z\"] = (3.3 * x1 + 0.999 * x2 + 0.6 * x3 + 1.2 * x4 + 1.4 * x5)\n\n    # --- Z'-Score (Private firms) ---\n    x3_prime = np.where(result[\"tlb\"] &gt; 0, result[\"be\"] / result[\"tlb\"], np.nan)\n    result[\"altman_z_prime\"] = (\n        0.717 * x4 + 0.847 * x5 + 3.107 * x1 + 0.420 * x3_prime + 0.998 * x2\n    )\n\n    # --- Z''-Score (Emerging Markets) ---\n    result[\"altman_z_em\"] = (\n        3.25\n        + 6.56 * x4   # Working Capital / TA\n        + 3.26 * x5   # Retained Earnings / TA\n        + 6.72 * x1   # EBIT / TA\n        + 1.05 * x3_prime  # Book Equity / TL\n    )\n\n    # Classify risk zones\n    result[\"z_zone\"] = pd.cut(\n        result[\"altman_z\"],\n        bins=[-np.inf, 1.80, 2.70, 2.99, np.inf],\n        labels=[\"High Distress\", \"Distress\", \"Grey Zone\", \"Safe\"]\n    )\n\n    result[\"z_em_zone\"] = pd.cut(\n        result[\"altman_z_em\"],\n        bins=[-np.inf, 1.10, 2.60, np.inf],\n        labels=[\"Distress\", \"Grey Zone\", \"Safe\"]\n    )\n\n    return result\n\ndf_clean = compute_altman_z(df_clean)\n\n# Winsorize Z-scores\nfor var in [\"altman_z\", \"altman_z_prime\", \"altman_z_em\"]:\n    df_clean[f\"{var}_w\"] = winsorize(df_clean[var])\n\nprint(\"Z-Score Summary Statistics\")\nz_summary = df_clean[[\"altman_z\", \"altman_z_prime\", \"altman_z_em\"]].describe().round(3)\nz_summary.columns = [\"Original Z\", \"Z' (Private)\", \"Z'' (Emerging)\"]\nprint(z_summary.to_string())\n\nZ-Score Summary Statistics\n       Original Z  Z' (Private)  Z'' (Emerging)\ncount    3484.000      3484.000        3484.000\nmean        2.610         2.042           7.467\nstd         1.896         1.194           3.137\nmin         0.059         0.155           1.578\n25%         1.595         1.314           5.670\n50%         2.212         1.806           6.987\n75%         3.028         2.410           8.549\nmax        28.640        11.119          29.686",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#z-score-component-analysis",
    "href": "36_valuation.html#z-score-component-analysis",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "34.2 Z-Score Component Analysis",
    "text": "34.2 Z-Score Component Analysis\nUnderstanding which components drive the Z-Score is particularly informative in the Vietnamese context, where certain ratios may behave differently than in developed markets.\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 9))\n\ncomponents = [\n    (\"z_x1\", 3.3, \"$3.3 \\\\times$ EBIT/TA\\n(Earning Power)\"),\n    (\"z_x2\", 0.999, \"$0.999 \\\\times$ Sales/TA\\n(Asset Turnover)\"),\n    (\"z_x3\", 0.6, \"$0.6 \\\\times$ ME/TL\\n(Leverage)\"),\n    (\"z_x4\", 1.2, \"$1.2 \\\\times$ WC/TA\\n(Liquidity)\"),\n    (\"z_x5\", 1.4, \"$1.4 \\\\times$ RE/TA\\n(Cum. Profitability)\"),\n]\n\nlatest_z = df_clean[df_clean[\"year\"] == latest_year]\n\nfor idx, (var, weight, label) in enumerate(components):\n    ax = axes[idx // 3, idx % 3]\n    weighted_vals = (latest_z[var] * weight).dropna()\n    weighted_vals = weighted_vals[weighted_vals.between(\n        weighted_vals.quantile(0.01), weighted_vals.quantile(0.99)\n    )]\n    ax.hist(weighted_vals, bins=40, color=colors[\"primary\"], alpha=0.7, edgecolor=\"white\")\n    ax.axvline(x=weighted_vals.median(), color=colors[\"quaternary\"],\n               linestyle=\"--\", linewidth=2, label=f\"Median: {weighted_vals.median():.2f}\")\n    ax.set_title(label, fontsize=11)\n    ax.legend(fontsize=9)\n\n# Remove empty subplot\naxes[1, 2].set_visible(False)\n\nplt.suptitle(f\"Z-Score Component Distributions ({latest_year})\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 34.1: Distribution of Altman Z-Score component ratios for Vietnamese listed firms. Each panel shows a component weighted by its coefficient in the original Z-Score formula.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#distribution-of-risk-zones",
    "href": "36_valuation.html#distribution-of-risk-zones",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "34.3 Distribution of Risk Zones",
    "text": "34.3 Distribution of Risk Zones\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# --- Original Z-Score Zones ---\nzone_counts = (\n    df_clean\n    .groupby([\"year\", \"z_zone\"])\n    .size()\n    .unstack(fill_value=0)\n)\nzone_pcts = zone_counts.div(zone_counts.sum(axis=1), axis=0) * 100\n\nzone_colors = {\n    \"Safe\": colors[\"safe\"],\n    \"Grey Zone\": colors[\"alert\"],\n    \"Distress\": colors[\"quaternary\"],\n    \"High Distress\": colors[\"distress\"],\n}\n\naxes[0].stackplot(\n    zone_pcts.index,\n    *[zone_pcts[col] for col in [\"Safe\", \"Grey Zone\", \"Distress\", \"High Distress\"]\n      if col in zone_pcts.columns],\n    labels=[col for col in [\"Safe\", \"Grey Zone\", \"Distress\", \"High Distress\"]\n            if col in zone_pcts.columns],\n    colors=[zone_colors[col] for col in [\"Safe\", \"Grey Zone\", \"Distress\", \"High Distress\"]\n            if col in zone_pcts.columns],\n    alpha=0.8\n)\naxes[0].set_xlabel(\"Year\")\naxes[0].set_ylabel(\"Percentage of Firms\")\naxes[0].set_title(\"Original Z-Score Risk Zones\")\naxes[0].legend(loc=\"upper right\", fontsize=9)\naxes[0].set_ylim(0, 100)\n\n# --- Emerging Market Z''-Score Zones ---\nem_zone_counts = (\n    df_clean\n    .groupby([\"year\", \"z_em_zone\"])\n    .size()\n    .unstack(fill_value=0)\n)\nem_zone_pcts = em_zone_counts.div(em_zone_counts.sum(axis=1), axis=0) * 100\n\nem_zone_colors = {\"Safe\": colors[\"safe\"], \"Grey Zone\": colors[\"alert\"],\n                  \"Distress\": colors[\"quaternary\"]}\n\naxes[1].stackplot(\n    em_zone_pcts.index,\n    *[em_zone_pcts[col] for col in [\"Safe\", \"Grey Zone\", \"Distress\"]\n      if col in em_zone_pcts.columns],\n    labels=[col for col in [\"Safe\", \"Grey Zone\", \"Distress\"]\n            if col in em_zone_pcts.columns],\n    colors=[em_zone_colors[col] for col in [\"Safe\", \"Grey Zone\", \"Distress\"]\n            if col in em_zone_pcts.columns],\n    alpha=0.8\n)\naxes[1].set_xlabel(\"Year\")\naxes[1].set_ylabel(\"Percentage of Firms\")\naxes[1].set_title(\"Emerging Market Z''-Score Risk Zones\")\naxes[1].legend(loc=\"upper right\", fontsize=9)\naxes[1].set_ylim(0, 100)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 34.2: Proportion of Vietnamese listed firms in each Altman Z-Score risk zone over time. The figure shows both the original Z-Score zones and the emerging market Z’’-Score zones.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#comparing-z-score-variants",
    "href": "36_valuation.html#comparing-z-score-variants",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "34.4 Comparing Z-Score Variants",
    "text": "34.4 Comparing Z-Score Variants\nAn important practical question is which Z-Score variant is most appropriate for Vietnamese firms. The original model was calibrated on U.S. manufacturing firms, while the Z’’-Score was specifically designed for emerging markets and non-manufacturing sectors.\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nsample = latest_z.dropna(subset=[\"altman_z_w\", \"altman_z_em_w\"]).sample(\n    min(500, len(latest_z)), random_state=42\n)\n\nscatter = ax.scatter(\n    sample[\"altman_z_w\"], sample[\"altman_z_em_w\"],\n    c=sample[\"tobin_q_simple_w\"], cmap=\"RdYlGn\",\n    alpha=0.6, s=30, edgecolor=\"white\", linewidth=0.3\n)\n\n# Add reference lines\nlims = [\n    min(ax.get_xlim()[0], ax.get_ylim()[0]),\n    max(ax.get_xlim()[1], ax.get_ylim()[1])\n]\nax.plot(lims, lims, \"k--\", alpha=0.3, linewidth=1, label=\"45° line\")\n\n# Add zone boundaries\nax.axvline(x=1.80, color=\"red\", linestyle=\":\", alpha=0.5, linewidth=1)\nax.axvline(x=2.99, color=\"green\", linestyle=\":\", alpha=0.5, linewidth=1)\nax.axhline(y=1.10, color=\"red\", linestyle=\":\", alpha=0.5, linewidth=1)\nax.axhline(y=2.60, color=\"green\", linestyle=\":\", alpha=0.5, linewidth=1)\n\nax.set_xlabel(\"Original Z-Score\")\nax.set_ylabel(\"Emerging Market Z''-Score\")\nax.set_title(\"Z-Score vs Z''-Score for Vietnamese Firms\")\n\ncbar = plt.colorbar(scatter, ax=ax, shrink=0.8)\ncbar.set_label(\"Tobin's Q\")\n\nax.legend(loc=\"upper left\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 34.3: Comparison of original Z-Score and emerging market Z’‘-Score for Vietnamese listed firms. Points above (below) the 45-degree line have higher Z’’-Scores (Z-Scores) than the other measure.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#multiple-age-proxies",
    "href": "36_valuation.html#multiple-age-proxies",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "35.1 Multiple Age Proxies",
    "text": "35.1 Multiple Age Proxies\nFor Vietnamese firms, we compute three age measures:\n\\[\n\\text{Age}_{\\text{founding}} = \\text{Current Year} - \\text{Founding Year}\n\\tag{35.1}\\]\n\\[\n\\text{Age}_{\\text{listing}} = \\text{Current Year} - \\text{Listing Year}\n\\tag{35.2}\\]\n\\[\n\\text{Age}_{\\text{data}} = \\text{Current Year} - \\text{First Year with Data}\n\\tag{35.3}\\]\n\ndef compute_company_age(df):\n    \"\"\"\n    Compute multiple proxies for company age.\n\n    For Vietnamese firms:\n    - Founding age: based on founding/incorporation date\n    - Listing age: based on first listing on HOSE/HNX/UPCoM\n    - Data age: based on first year with available financial data\n    \"\"\"\n    result = df.copy()\n\n    # Founding age\n    result[\"age_founding\"] = result[\"year\"] - result[\"founding_year\"]\n\n    # Listing age\n    result[\"age_listing\"] = result[\"year\"] - result[\"listing_year\"]\n    result[\"age_listing\"] = result[\"age_listing\"].clip(lower=0)\n\n    # Data age (years since first available financial data)\n    first_year = result.groupby(\"ticker\")[\"year\"].transform(\"min\")\n    result[\"age_data\"] = result[\"year\"] - first_year\n\n    # Log of age (commonly used in regressions)\n    result[\"ln_age_founding\"] = np.log1p(result[\"age_founding\"])\n    result[\"ln_age_listing\"] = np.log1p(result[\"age_listing\"])\n\n    return result\n\ndf_clean = compute_company_age(df_clean)\n\nprint(\"Company Age Summary Statistics\")\nage_summary = df_clean[df_clean[\"year\"] == latest_year][\n    [\"age_founding\", \"age_listing\", \"age_data\"]\n].describe().round(1)\nage_summary.columns = [\"Founding Age\", \"Listing Age\", \"Data Age\"]\nprint(age_summary.to_string())\n\nCompany Age Summary Statistics\n       Founding Age  Listing Age  Data Age\ncount         232.0        232.0     232.0\nmean           30.1         13.9      12.3\nstd            11.6          5.8       3.9\nmin            10.0          5.0       5.0\n25%            20.0          9.0       9.0\n50%            30.0         13.0      13.0\n75%            40.0         19.0      16.0\nmax            49.0         24.0      16.0",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#age-distribution",
    "href": "36_valuation.html#age-distribution",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "35.2 Age Distribution",
    "text": "35.2 Age Distribution\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nage_vars = [\n    (\"age_founding\", \"Founding Age (years)\", colors[\"primary\"]),\n    (\"age_listing\", \"Listing Age (years)\", colors[\"secondary\"]),\n    (\"age_data\", \"Data Age (years)\", colors[\"tertiary\"]),\n]\n\nlatest = df_clean[df_clean[\"year\"] == latest_year]\n\nfor ax, (var, label, color) in zip(axes, age_vars):\n    data = latest[var].dropna()\n    ax.hist(data, bins=30, color=color, alpha=0.7, edgecolor=\"white\")\n    ax.axvline(x=data.median(), color=\"black\", linestyle=\"--\",\n               linewidth=2, label=f\"Median: {data.median():.0f}\")\n    ax.set_xlabel(label)\n    ax.set_ylabel(\"Number of Firms\")\n    ax.legend()\n\naxes[0].set_title(\"Founding Age\")\naxes[1].set_title(\"Listing Age\")\naxes[2].set_title(\"Data Age\")\n\nplt.suptitle(f\"Company Age Distributions ({latest_year})\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 35.1: Distribution of company age measures for Vietnamese listed firms. Founding age reflects true organizational maturity, while listing age captures capital market experience.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#age-and-the-equitization-effect",
    "href": "36_valuation.html#age-and-the-equitization-effect",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "35.3 Age and the Equitization Effect",
    "text": "35.3 Age and the Equitization Effect\nA distinctive feature of the Vietnamese market is the equitization of SOEs. Many firms have founding dates that predate the stock market by decades, creating a large gap between founding age and listing age.\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nlatest = df_clean[df_clean[\"year\"] == latest_year].copy()\nlatest[\"soe_flag\"] = latest[\"state_ownership_pct\"] &gt; 30  # SOE proxy\n\nfor soe, label, color, marker in [\n    (True, \"SOE (&gt;30% state)\", colors[\"quaternary\"], \"s\"),\n    (False, \"Non-SOE\", colors[\"primary\"], \"o\"),\n]:\n    subset = latest[latest[\"soe_flag\"] == soe]\n    ax.scatter(\n        subset[\"age_listing\"], subset[\"age_founding\"],\n        c=color, alpha=0.5, s=25, marker=marker, label=label, edgecolor=\"white\"\n    )\n\n# 45-degree line\nmax_age = max(latest[\"age_founding\"].max(), latest[\"age_listing\"].max())\nax.plot([0, max_age], [0, max_age], \"k--\", alpha=0.3, label=\"Founding = Listing age\")\n\nax.set_xlabel(\"Listing Age (years)\")\nax.set_ylabel(\"Founding Age (years)\")\nax.set_title(\"Founding vs. Listing Age: The Equitization Gap\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 35.2: Relationship between founding age and listing age for Vietnamese firms. The gap reflects years of pre-listing operations, which is especially large for equitized state-owned enterprises.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#the-relationship-between-q-z-score-and-age",
    "href": "36_valuation.html#the-relationship-between-q-z-score-and-age",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "36.1 The Relationship Between Q, Z-Score, and Age",
    "text": "36.1 The Relationship Between Q, Z-Score, and Age\nThese three measures capture different dimensions of a firm’s financial standing, but they are not independent. Theoretically, we expect:\n\nQ and Z: Firms with high growth opportunities (high Q) tend to be financially healthier (high Z), but the relationship is not monotonic, very high Q values may indicate speculative overvaluation.\nQ and Age: Young firms may have higher Q (growth expectations) or lower Q (unproven business model). The relationship depends on the industry life cycle.\nZ and Age: Older firms typically have more stable earnings and higher retained earnings, contributing to higher Z-Scores, though very old firms may face declining competitiveness.\n\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\nsample = latest.dropna(subset=[\"tobin_q_simple_w\", \"altman_z_w\", \"age_founding\"])\nsample = sample.sample(min(300, len(sample)), random_state=42)\n\n# Q vs Z\naxes[0].scatter(\n    sample[\"altman_z_w\"], sample[\"tobin_q_simple_w\"],\n    c=sample[\"age_founding\"], cmap=\"viridis\", alpha=0.6,\n    s=np.log1p(sample[\"at\"]) * 3, edgecolor=\"white\", linewidth=0.3\n)\naxes[0].set_xlabel(\"Altman Z-Score\")\naxes[0].set_ylabel(\"Tobin's Q\")\naxes[0].set_title(\"Valuation vs. Distress Risk\")\naxes[0].axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\naxes[0].axvline(x=1.80, color=\"red\", linestyle=\":\", alpha=0.5)\naxes[0].axvline(x=2.99, color=\"green\", linestyle=\":\", alpha=0.5)\n\n# Q vs Age\naxes[1].scatter(\n    sample[\"age_founding\"], sample[\"tobin_q_simple_w\"],\n    c=sample[\"altman_z_w\"], cmap=\"RdYlGn\", alpha=0.6,\n    s=np.log1p(sample[\"at\"]) * 3, edgecolor=\"white\", linewidth=0.3\n)\naxes[1].set_xlabel(\"Founding Age (years)\")\naxes[1].set_ylabel(\"Tobin's Q\")\naxes[1].set_title(\"Valuation vs. Maturity\")\naxes[1].axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\n\n# Z vs Age\nscatter = axes[2].scatter(\n    sample[\"age_founding\"], sample[\"altman_z_w\"],\n    c=sample[\"tobin_q_simple_w\"], cmap=\"coolwarm\", alpha=0.6,\n    s=np.log1p(sample[\"at\"]) * 3, edgecolor=\"white\", linewidth=0.3\n)\naxes[2].set_xlabel(\"Founding Age (years)\")\naxes[2].set_ylabel(\"Altman Z-Score\")\naxes[2].set_title(\"Distress Risk vs. Maturity\")\naxes[2].axhline(y=1.80, color=\"red\", linestyle=\":\", alpha=0.5)\naxes[2].axhline(y=2.99, color=\"green\", linestyle=\":\", alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 36.1: Relationship between Tobin’s Q, Altman Z-Score, and company age for Vietnamese listed firms. Bubble size reflects total assets; color indicates the emerging market Z’’-Score risk zone.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#correlation-structure",
    "href": "36_valuation.html#correlation-structure",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "36.2 Correlation Structure",
    "text": "36.2 Correlation Structure\n\ncorr_vars = [\n    \"tobin_q_simple_w\", \"mtb_w\", \"altman_z_w\", \"altman_z_em_w\",\n    \"age_founding\", \"age_listing\",\n    \"z_x1\", \"z_x2\", \"z_x3\", \"z_x4\", \"z_x5\"\n]\ncorr_labels = [\n    \"Tobin's Q\", \"M/B Ratio\", \"Z-Score\", \"Z'' (EM)\",\n    \"Age (Found.)\", \"Age (List.)\",\n    \"EBIT/TA\", \"Sales/TA\", \"ME/TL\", \"WC/TA\", \"RE/TA\"\n]\n\ncorr_matrix = df_clean[corr_vars].corr()\ncorr_matrix.index = corr_labels\ncorr_matrix.columns = corr_labels\n\nfig, ax = plt.subplots(figsize=(12, 10))\n\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\ncmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\"light\", as_cmap=True)\n\nsns.heatmap(\n    corr_matrix, mask=mask, cmap=cmap, center=0,\n    annot=True, fmt=\".2f\", square=True,\n    linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n    ax=ax, vmin=-1, vmax=1,\n    annot_kws={\"size\": 9}\n)\nax.set_title(\"Correlation Matrix of Key Financial Measures\", fontsize=14, pad=20)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 36.2: Correlation matrix of key valuation, distress, and maturity measures for Vietnamese listed firms.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#cross-sectional-regression-analysis",
    "href": "36_valuation.html#cross-sectional-regression-analysis",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "36.3 Cross-Sectional Regression Analysis",
    "text": "36.3 Cross-Sectional Regression Analysis\nWe estimate a simple cross-sectional model to examine the determinants of Tobin’s Q in the Vietnamese market:\n\\[\nQ_{i,t} = \\alpha + \\beta_1 \\ln(\\text{Age}_{i}) + \\beta_2 Z''_{i,t} + \\beta_3 \\text{SOE}_{i} + \\beta_4 \\ln(\\text{TA}_{i,t}) + \\varepsilon_{i,t}\n\\tag{36.1}\\]\n\nfrom numpy.linalg import lstsq\n\n# Prepare regression data\nreg_data = latest.dropna(\n    subset=[\"tobin_q_simple_w\", \"ln_age_founding\", \"altman_z_em_w\", \"at\"]\n).copy()\n\nreg_data[\"ln_at\"] = np.log(reg_data[\"at\"])\nreg_data[\"soe_dummy\"] = (reg_data[\"state_ownership_pct\"] &gt; 30).astype(int)\nreg_data[\"constant\"] = 1.0\n\n# OLS estimation\ny = reg_data[\"tobin_q_simple_w\"].values\nX = reg_data[[\"constant\", \"ln_age_founding\", \"altman_z_em_w\",\n              \"soe_dummy\", \"ln_at\"]].values\n\nbetas, residuals, rank, sv = lstsq(X, y, rcond=None)\ny_hat = X @ betas\nresid = y - y_hat\nn, k = X.shape\n\n# Standard errors (heteroskedasticity-robust would be better)\nsigma2 = np.sum(resid**2) / (n - k)\nvar_beta = sigma2 * np.linalg.inv(X.T @ X)\nse = np.sqrt(np.diag(var_beta))\nt_stats = betas / se\nr_squared = 1 - np.sum(resid**2) / np.sum((y - y.mean())**2)\nadj_r2 = 1 - (1 - r_squared) * (n - 1) / (n - k - 1)\n\n# Display results\nvar_names = [\"Constant\", \"ln(Age)\", \"Z''-Score (EM)\", \"SOE Dummy\", \"ln(Total Assets)\"]\nprint(\"=\" * 65)\nprint(\"  Cross-Sectional Regression: Determinants of Tobin's Q\")\nprint(f\"  Dependent Variable: Tobin's Q (Simple, Winsorized)\")\nprint(f\"  Sample: {n} firms ({latest_year})\")\nprint(\"=\" * 65)\nprint(f\"  {'Variable':&lt;22} {'Coef':&gt;10} {'Std.Err':&gt;10} {'t-stat':&gt;10}\")\nprint(\"-\" * 65)\nfor name, b, s, t in zip(var_names, betas, se, t_stats):\n    sig = \"***\" if abs(t) &gt; 2.576 else \"**\" if abs(t) &gt; 1.96 else \"*\" if abs(t) &gt; 1.645 else \"\"\n    print(f\"  {name:&lt;22} {b:&gt;10.4f} {s:&gt;10.4f} {t:&gt;8.2f} {sig}\")\nprint(\"-\" * 65)\nprint(f\"  R-squared: {r_squared:.4f}\")\nprint(f\"  Adjusted R-squared: {adj_r2:.4f}\")\nprint(f\"  Observations: {n}\")\nprint(\"=\" * 65)\nprint(\"  Significance: *** p&lt;0.01, ** p&lt;0.05, * p&lt;0.10\")\n\n=================================================================\n  Cross-Sectional Regression: Determinants of Tobin's Q\n  Dependent Variable: Tobin's Q (Simple, Winsorized)\n  Sample: 232 firms (2024)\n=================================================================\n  Variable                     Coef    Std.Err     t-stat\n-----------------------------------------------------------------\n  Constant                   0.9686     0.1959     4.94 ***\n  ln(Age)                   -0.0064     0.0365    -0.18 \n  Z''-Score (EM)             0.0067     0.0050     1.34 \n  SOE Dummy                  0.0327     0.0315     1.04 \n  ln(Total Assets)           0.0018     0.0096     0.19 \n-----------------------------------------------------------------\n  R-squared: 0.0126\n  Adjusted R-squared: -0.0092\n  Observations: 232\n=================================================================\n  Significance: *** p&lt;0.01, ** p&lt;0.05, * p&lt;0.10",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#putting-it-all-together",
    "href": "36_valuation.html#putting-it-all-together",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "37.1 Putting It All Together",
    "text": "37.1 Putting It All Together\nHere we provide a single, clean function that takes raw DataCore.vn financial data and produces a complete dataset with all three measures computed.\n\ndef compute_valuation_distress_age(df, firm_profiles=None):\n    \"\"\"\n    Complete pipeline to compute Tobin's Q, Altman Z-Score variants,\n    and Company Age for Vietnamese listed firms.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Panel of firm-year financial data with columns:\n        at, seq, lt, lct, tlb, sale, ebit, ni, re_var, act,\n        ppent, invt, txdb, itcb, pstk, me, prcc, csho, year, ticker\n    firm_profiles : pd.DataFrame, optional\n        Company profiles with founding_year, listing_year\n\n    Returns\n    -------\n    pd.DataFrame\n        Input data augmented with computed measures\n    \"\"\"\n    result = df.copy()\n\n    # === Data Quality Filters ===\n    result = result[result[\"at\"] &gt; 0]\n    result = result[result[\"seq\"] &gt; 0]\n\n    # === Book Value of Equity (Daniel & Titman 1997) ===\n    result[\"pref\"] = result[\"pstk\"].fillna(0)\n    result[\"be\"] = (\n        result[\"seq\"]\n        + result[\"txdb\"].fillna(0)\n        + result[\"itcb\"].fillna(0)\n        - result[\"pref\"]\n    )\n\n    # === Market Value of Equity ===\n    if \"me\" not in result.columns or result[\"me\"].isna().all():\n        result[\"me\"] = result[\"prcc\"] * result[\"csho\"]\n\n    # === Tobin's Q Variants ===\n    # Simple Q (Gompers et al. 2003)\n    result[\"tobin_q\"] = (result[\"at\"] + result[\"me\"] - result[\"be\"]) / result[\"at\"]\n\n    # Chung-Pruitt Q\n    debt_cp = (\n        result[\"lct\"].fillna(0) - result[\"act\"].fillna(0)\n        + result[\"invt\"].fillna(0) + result[\"lt\"].fillna(0)\n    )\n    result[\"tobin_q_cp\"] = (\n        (result[\"me\"] + result[\"pstk\"].fillna(0) + debt_cp) / result[\"at\"]\n    )\n\n    # Market-to-Book\n    result[\"mtb\"] = np.where(result[\"be\"] &gt; 0, result[\"me\"] / result[\"be\"], np.nan)\n\n    # === Altman Z-Score Variants ===\n    result[\"wc\"] = result[\"act\"].fillna(0) - result[\"lct\"].fillna(0)\n\n    x1 = result[\"ebit\"] / result[\"at\"]\n    x2 = result[\"sale\"] / result[\"at\"]\n    x3_market = np.where(result[\"tlb\"] &gt; 0, result[\"me\"] / result[\"tlb\"], np.nan)\n    x3_book = np.where(result[\"tlb\"] &gt; 0, result[\"be\"] / result[\"tlb\"], np.nan)\n    x4 = result[\"wc\"] / result[\"at\"]\n    x5 = result[\"re_var\"].fillna(0) / result[\"at\"]\n\n    # Original Z\n    result[\"altman_z\"] = 3.3 * x1 + 0.999 * x2 + 0.6 * x3_market + 1.2 * x4 + 1.4 * x5\n\n    # Z' (private firms)\n    result[\"altman_z_prime\"] = (\n        0.717 * x4 + 0.847 * x5 + 3.107 * x1 + 0.420 * x3_book + 0.998 * x2\n    )\n\n    # Z'' (emerging markets)\n    result[\"altman_z_em\"] = (\n        3.25 + 6.56 * x4 + 3.26 * x5 + 6.72 * x1 + 1.05 * x3_book\n    )\n\n    # Risk zones\n    result[\"z_zone\"] = pd.cut(\n        result[\"altman_z\"],\n        bins=[-np.inf, 1.80, 2.70, 2.99, np.inf],\n        labels=[\"High Distress\", \"Distress\", \"Grey Zone\", \"Safe\"]\n    )\n    result[\"z_em_zone\"] = pd.cut(\n        result[\"altman_z_em\"],\n        bins=[-np.inf, 1.10, 2.60, np.inf],\n        labels=[\"Distress\", \"Grey Zone\", \"Safe\"]\n    )\n\n    # === Company Age ===\n    if firm_profiles is not None:\n        result = result.merge(\n            firm_profiles[[\"ticker\", \"founding_year\", \"listing_year\"]],\n            on=\"ticker\", how=\"left\", suffixes=(\"\", \"_profile\")\n        )\n        result[\"age_founding\"] = result[\"year\"] - result[\"founding_year\"]\n        result[\"age_listing\"] = (result[\"year\"] - result[\"listing_year\"]).clip(lower=0)\n\n    first_year = result.groupby(\"ticker\")[\"year\"].transform(\"min\")\n    result[\"age_data\"] = result[\"year\"] - first_year\n\n    # === Winsorize ===\n    for var in [\"tobin_q\", \"tobin_q_cp\", \"mtb\", \"altman_z\", \"altman_z_prime\", \"altman_z_em\"]:\n        if var in result.columns:\n            result[f\"{var}_w\"] = winsorize(result[var])\n\n    return result\n\n# Demonstrate the pipeline\nfinal_df = compute_valuation_distress_age(df, firm_profiles)\nprint(f\"Final dataset: {final_df.shape[0]:,} observations, {final_df.shape[1]} variables\")\nprint(f\"Firms: {final_df['ticker'].nunique()}\")\nprint(f\"\\nKey variables computed:\")\nfor var in [\"tobin_q\", \"tobin_q_cp\", \"mtb\", \"altman_z\", \"altman_z_prime\",\n            \"altman_z_em\", \"age_founding\", \"age_listing\", \"age_data\"]:\n    if var in final_df.columns:\n        non_null = final_df[var].notna().sum()\n        print(f\"  {var}: {non_null:,} non-null values\")\n\nFinal dataset: 3,484 observations, 48 variables\nFirms: 297\n\nKey variables computed:\n  tobin_q: 3,484 non-null values\n  tobin_q_cp: 3,484 non-null values\n  mtb: 3,484 non-null values\n  altman_z: 3,484 non-null values\n  altman_z_prime: 3,484 non-null values\n  altman_z_em: 3,484 non-null values\n  age_founding: 3,484 non-null values\n  age_listing: 3,484 non-null values\n  age_data: 3,484 non-null values",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#exporting-results",
    "href": "36_valuation.html#exporting-results",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "37.2 Exporting Results",
    "text": "37.2 Exporting Results\n\n# Select key output variables\noutput_vars = [\n    \"ticker\", \"year\", \"datadate\", \"exchange\", \"industry\",\n    \"at\", \"be\", \"me\", \"tobin_q\", \"tobin_q_w\", \"mtb\", \"mtb_w\",\n    \"altman_z\", \"altman_z_w\", \"altman_z_em\", \"altman_z_em_w\",\n    \"z_zone\", \"z_em_zone\",\n    \"age_founding\", \"age_listing\", \"age_data\",\n    \"state_ownership_pct\"\n]\n\nexport_cols = [v for v in output_vars if v in final_df.columns]\nexport_df = final_df[export_cols].copy()\n\n# export_df.to_csv(\"vn_valuation_distress_age.csv\", index=False)\n# export_df.to_parquet(\"vn_valuation_distress_age.parquet\", index=False)\n\nprint(f\"Export dataset: {export_df.shape[0]:,} rows × {export_df.shape[1]} columns\")\nprint(f\"\\nSample output (first 5 rows):\")\ndisplay_cols = [\"ticker\", \"year\", \"tobin_q\", \"altman_z\", \"altman_z_em\",\n                \"z_zone\", \"age_founding\"]\ndisplay_cols = [c for c in display_cols if c in export_df.columns]\nprint(export_df[display_cols].head().to_string(index=False))\n\nExport dataset: 3,484 rows × 22 columns\n\nSample output (first 5 rows):\nticker  year  tobin_q  altman_z  altman_z_em z_zone  age_founding\nVN0001  2017 0.905928  3.345941     5.605169   Safe             4\nVN0001  2018 1.270114  5.324082     9.228846   Safe             5\nVN0001  2019 1.866493  5.478529     6.264178   Safe             6\nVN0001  2020 1.016979  5.612289    10.445449   Safe             7\nVN0001  2021 0.805818  4.220869     7.588273   Safe             8",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#state-ownership-and-valuation",
    "href": "36_valuation.html#state-ownership-and-valuation",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "38.1 State Ownership and Valuation",
    "text": "38.1 State Ownership and Valuation\nState ownership is a defining feature of the Vietnamese corporate landscape. We examine how state ownership affects both Tobin’s Q and financial distress risk.\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nlatest = final_df[final_df[\"year\"] == latest_year].dropna(\n    subset=[\"tobin_q_w\", \"altman_z_em_w\", \"state_ownership_pct\"]\n)\n\n# Q vs State Ownership\naxes[0].scatter(\n    latest[\"state_ownership_pct\"], latest[\"tobin_q_w\"],\n    alpha=0.4, s=20, color=colors[\"primary\"], edgecolor=\"white\"\n)\n\n# Binned means\nbins = pd.cut(latest[\"state_ownership_pct\"], bins=10)\nbinned = latest.groupby(bins)[\"tobin_q_w\"].mean()\nbin_centers = [(b.left + b.right) / 2 for b in binned.index]\naxes[0].plot(bin_centers, binned.values, color=colors[\"quaternary\"],\n             linewidth=3, label=\"Binned Mean\")\n\naxes[0].set_xlabel(\"State Ownership (%)\")\naxes[0].set_ylabel(\"Tobin's Q\")\naxes[0].set_title(\"State Ownership and Firm Valuation\")\naxes[0].axhline(y=1, color=\"gray\", linestyle=\"--\", alpha=0.5)\naxes[0].legend()\n\n# Z''-Score by SOE quartile\nlatest[\"soe_quartile\"] = pd.qcut(\n    latest[\"state_ownership_pct\"],\n    q=4, labels=[\"Q1 (Low)\", \"Q2\", \"Q3\", \"Q4 (High)\"],\n    duplicates=\"drop\"\n)\n\nsoe_groups = latest.groupby(\"soe_quartile\")[\"altman_z_em_w\"]\nbox_data = [group.values for name, group in soe_groups]\nbp = axes[1].boxplot(\n    box_data,\n    labels=[name for name, _ in soe_groups],\n    patch_artist=True,\n    medianprops=dict(color=\"black\", linewidth=2)\n)\n\ngradient_colors = plt.cm.Blues(np.linspace(0.3, 0.8, len(bp[\"boxes\"])))\nfor patch, color in zip(bp[\"boxes\"], gradient_colors):\n    patch.set_facecolor(color)\n\naxes[1].axhline(y=2.60, color=\"green\", linestyle=\":\", alpha=0.5, label=\"Safe threshold\")\naxes[1].axhline(y=1.10, color=\"red\", linestyle=\":\", alpha=0.5, label=\"Distress threshold\")\naxes[1].set_xlabel(\"State Ownership Quartile\")\naxes[1].set_ylabel(\"Z''-Score (Emerging Market)\")\naxes[1].set_title(\"Financial Health by State Ownership\")\naxes[1].legend(fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 38.1: Relationship between state ownership percentage and firm valuation/distress measures. The left panel shows Tobin’s Q against state ownership, with a LOWESS smoother. The right panel shows the Z’’-Score by state ownership quartile.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#exchange-level-analysis",
    "href": "36_valuation.html#exchange-level-analysis",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "38.2 Exchange-Level Analysis",
    "text": "38.2 Exchange-Level Analysis\n\nexchange_summary = (\n    latest\n    .groupby(\"exchange\")\n    .agg(\n        n_firms=(\"ticker\", \"nunique\"),\n        median_q=(\"tobin_q_w\", \"median\"),\n        mean_q=(\"tobin_q_w\", \"mean\"),\n        median_z=(\"altman_z_w\", \"median\"),\n        mean_z_em=(\"altman_z_em_w\", \"mean\"),\n        pct_distress_z=(\"z_zone\", lambda x: (x == \"High Distress\").mean() * 100),\n        pct_distress_em=(\"z_em_zone\", lambda x: (x == \"Distress\").mean() * 100),\n        median_age=(\"age_founding\", \"median\"),\n        median_soe=(\"state_ownership_pct\", \"median\"),\n    )\n    .round(2)\n)\n\nexchange_summary.columns = [\n    \"N Firms\", \"Median Q\", \"Mean Q\", \"Median Z\", \"Mean Z'' (EM)\",\n    \"% Distress (Z)\", \"% Distress (Z'')\", \"Median Age\", \"Median SOE %\"\n]\n\nexchange_summary.style.set_properties(**{\n    \"text-align\": \"center\",\n    \"font-size\": \"10pt\"\n}).set_table_styles([\n    {\"selector\": \"th\", \"props\": [\n        (\"background-color\", \"#1f77b4\"),\n        (\"color\", \"white\"),\n        (\"text-align\", \"center\"),\n        (\"padding\", \"8px\")\n    ]},\n]).format({\n    \"Median Q\": \"{:.2f}\", \"Mean Q\": \"{:.2f}\",\n    \"Median Z\": \"{:.2f}\", \"Mean Z'' (EM)\": \"{:.2f}\",\n    \"% Distress (Z)\": \"{:.1f}%\", \"% Distress (Z'')\": \"{:.1f}%\",\n    \"Median Age\": \"{:.0f}\", \"Median SOE %\": \"{:.1f}%\"\n})\n\n\n\nTable 38.1: Summary statistics by exchange for Vietnamese listed firms (latest year)\n\n\n\n\n\n\n\n\n \nN Firms\nMedian Q\nMean Q\nMedian Z\nMean Z'' (EM)\n% Distress (Z)\n% Distress (Z'')\nMedian Age\nMedian SOE %\n\n\nexchange\n \n \n \n \n \n \n \n \n \n\n\n\n\nHNX\n77\n0.99\n1.02\n2.07\n7.25\n32.5%\n0.0%\n33\n22.1%\n\n\nHOSE\n113\n1.00\n1.04\n2.06\n7.24\n40.7%\n0.0%\n29\n26.8%\n\n\nUPCoM\n42\n0.98\n1.05\n2.13\n7.80\n30.9%\n0.0%\n26\n27.6%",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#sector-heatmap-valuation-and-distress",
    "href": "36_valuation.html#sector-heatmap-valuation-and-distress",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "38.3 Sector Heatmap: Valuation and Distress",
    "text": "38.3 Sector Heatmap: Valuation and Distress\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\n\n# Tobin's Q heatmap\nq_pivot = (\n    final_df\n    .groupby([\"industry\", \"year\"])[\"tobin_q_w\"]\n    .median()\n    .unstack(fill_value=np.nan)\n)\n\nsns.heatmap(\n    q_pivot, cmap=\"RdYlGn\", center=1, ax=axes[0],\n    cbar_kws={\"label\": \"Median Q\", \"shrink\": 0.8},\n    linewidths=0.5, linecolor=\"white\",\n    xticklabels=2\n)\naxes[0].set_title(\"Tobin's Q by Industry and Year\")\naxes[0].set_xlabel(\"Year\")\naxes[0].set_ylabel(\"\")\n\n# Z''-Score heatmap\nz_pivot = (\n    final_df\n    .groupby([\"industry\", \"year\"])[\"altman_z_em_w\"]\n    .median()\n    .unstack(fill_value=np.nan)\n)\n\nsns.heatmap(\n    z_pivot, cmap=\"RdYlGn\", center=2.6, ax=axes[1],\n    cbar_kws={\"label\": \"Median Z''\", \"shrink\": 0.8},\n    linewidths=0.5, linecolor=\"white\",\n    xticklabels=2\n)\naxes[1].set_title(\"Z''-Score (EM) by Industry and Year\")\naxes[1].set_xlabel(\"Year\")\naxes[1].set_ylabel(\"\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 38.2: Median Tobin’s Q and Z’’-Score by industry and year for Vietnamese listed firms. Warmer colors indicate higher values.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#handling-delisted-firms-survivorship-bias",
    "href": "36_valuation.html#handling-delisted-firms-survivorship-bias",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "38.4 Handling Delisted Firms: Survivorship Bias",
    "text": "38.4 Handling Delisted Firms: Survivorship Bias\nA critical issue in measuring financial distress is survivorship bias. If we only analyze currently listed firms, we miss the very firms that the Z-Score is designed to identify, those that went bankrupt or were delisted.\n\n# Simulate delisting data (replace with DataCore.vn delisting records)\nnp.random.seed(123)\nn_delisted = 50\ndelisted_firms = pd.DataFrame({\n    \"ticker\": [f\"VN{str(i).zfill(4)}\" for i in range(n_firms + 1, n_firms + n_delisted + 1)],\n    \"delist_year\": np.random.randint(2010, 2024, n_delisted),\n    \"delist_reason\": np.random.choice(\n        [\"Bankruptcy/Liquidation\", \"Merger/Acquisition\", \"Voluntary Delisting\",\n         \"Regulatory Non-compliance\", \"Below Minimum Requirements\"],\n        n_delisted,\n        p=[0.15, 0.25, 0.20, 0.20, 0.20]\n    )\n})\n\n# Summary of delisting reasons\nprint(\"Delisting Reasons Summary\")\nprint(\"=\" * 50)\nreason_counts = delisted_firms[\"delist_reason\"].value_counts()\nfor reason, count in reason_counts.items():\n    print(f\"  {reason}: {count} ({count/n_delisted*100:.0f}%)\")\nprint(f\"\\nTotal delisted: {n_delisted}\")\nprint(f\"Currently listed: {final_df['ticker'].nunique()}\")\nprint(f\"\\nSurvivorship rate: \"\n      f\"{final_df['ticker'].nunique()/(final_df['ticker'].nunique()+n_delisted)*100:.1f}%\")\n\nDelisting Reasons Summary\n==================================================\n  Merger/Acquisition: 16 (32%)\n  Voluntary Delisting: 14 (28%)\n  Regulatory Non-compliance: 9 (18%)\n  Below Minimum Requirements: 7 (14%)\n  Bankruptcy/Liquidation: 4 (8%)\n\nTotal delisted: 50\nCurrently listed: 297\n\nSurvivorship rate: 85.6%",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#alternative-tobins-q-specifications",
    "href": "36_valuation.html#alternative-tobins-q-specifications",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "39.1 Alternative Tobin’s Q Specifications",
    "text": "39.1 Alternative Tobin’s Q Specifications\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\nsample = latest.dropna(subset=[\"tobin_q_w\", \"tobin_q_cp\"]).copy()\nsample[\"tobin_q_cp_w\"] = winsorize(sample[\"tobin_q_cp\"])\n\nax.scatter(\n    sample[\"tobin_q_w\"], sample[\"tobin_q_cp_w\"],\n    alpha=0.4, s=15, color=colors[\"primary\"], edgecolor=\"white\"\n)\n\n# 45-degree line\nlims = [\n    min(ax.get_xlim()[0], ax.get_ylim()[0]),\n    max(ax.get_xlim()[1], ax.get_ylim()[1])\n]\nax.plot(lims, lims, \"k--\", alpha=0.3)\n\ncorr = sample[\"tobin_q_w\"].corr(sample[\"tobin_q_cp_w\"])\nax.text(0.05, 0.95, f\"Correlation: {corr:.3f}\",\n        transform=ax.transAxes, fontsize=12,\n        verticalalignment=\"top\",\n        bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5))\n\nax.set_xlabel(\"Simple Q (Gompers et al.)\")\nax.set_ylabel(\"Chung-Pruitt Q\")\nax.set_title(\"Comparison of Tobin's Q Variants\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 39.1: Comparison of Tobin’s Q variants: Simple Q (Gompers et al. 2003) vs. Chung-Pruitt approximation. High correlation supports the use of the simpler measure.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#industry-adjusted-measures",
    "href": "36_valuation.html#industry-adjusted-measures",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "39.2 Industry-Adjusted Measures",
    "text": "39.2 Industry-Adjusted Measures\nRaw Tobin’s Q and Z-Scores may reflect industry characteristics rather than firm-specific attributes. We compute industry-adjusted versions:\n\\[\nQ^{\\text{adj}}_{i,t} = Q_{i,t} - \\overline{Q}_{j(i),t}\n\\tag{39.1}\\]\nwhere \\(\\overline{Q}_{j(i),t}\\) is the median Tobin’s Q of the industry \\(j\\) to which firm \\(i\\) belongs in year \\(t\\).\n\ndef industry_adjust(df, var, group_var=\"industry\"):\n    \"\"\"Compute industry-adjusted measure (deviation from industry median).\"\"\"\n    industry_median = df.groupby([\"year\", group_var])[var].transform(\"median\")\n    return df[var] - industry_median\n\nfinal_df[\"tobin_q_adj\"] = industry_adjust(final_df, \"tobin_q_w\")\nfinal_df[\"altman_z_em_adj\"] = industry_adjust(final_df, \"altman_z_em_w\")\n\nlatest_adj = final_df[final_df[\"year\"] == latest_year]\n\nprint(\"Industry-Adjusted Measures (Latest Year)\")\nprint(f\"  Tobin's Q (adjusted): mean={latest_adj['tobin_q_adj'].mean():.4f}, \"\n      f\"std={latest_adj['tobin_q_adj'].std():.3f}\")\nprint(f\"  Z''-Score (adjusted): mean={latest_adj['altman_z_em_adj'].mean():.4f}, \"\n      f\"std={latest_adj['altman_z_em_adj'].std():.3f}\")\n\nIndustry-Adjusted Measures (Latest Year)\n  Tobin's Q (adjusted): mean=0.0352, std=0.226\n  Z''-Score (adjusted): mean=0.4840, std=3.030",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#panel-regression-with-fixed-effects",
    "href": "36_valuation.html#panel-regression-with-fixed-effects",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "39.3 Panel Regression with Fixed Effects",
    "text": "39.3 Panel Regression with Fixed Effects\nFor more rigorous analysis, we estimate a panel model with firm and year fixed effects:\n\\[\nQ_{i,t} = \\alpha_i + \\gamma_t + \\beta_1 Z''_{i,t} + \\beta_2 \\ln(\\text{Age}_{i,t}) + \\beta_3 \\text{Size}_{i,t} + \\varepsilon_{i,t}\n\\tag{39.2}\\]\n\n# Simplified within-estimator (year and industry demeaning)\npanel = final_df.dropna(\n    subset=[\"tobin_q_w\", \"altman_z_em_w\", \"age_founding\", \"at\"]\n).copy()\n\npanel[\"ln_at\"] = np.log(panel[\"at\"])\npanel[\"ln_age\"] = np.log1p(panel[\"age_founding\"])\n\n# Demean by year-industry (proxy for fixed effects)\nfe_vars = [\"tobin_q_w\", \"altman_z_em_w\", \"ln_age\", \"ln_at\"]\nfor var in fe_vars:\n    group_mean = panel.groupby([\"year\", \"industry\"])[var].transform(\"mean\")\n    panel[f\"{var}_dm\"] = panel[var] - group_mean\n\n# OLS on demeaned data\ny = panel[\"tobin_q_w_dm\"].values\nX = np.column_stack([\n    np.ones(len(panel)),\n    panel[\"altman_z_em_w_dm\"].values,\n    panel[\"ln_age_dm\"].values,\n    panel[\"ln_at_dm\"].values,\n])\n\nbetas, _, _, _ = lstsq(X, y, rcond=None)\ny_hat = X @ betas\nresid = y - y_hat\nn, k = X.shape\n\nsigma2 = np.sum(resid**2) / (n - k)\nvar_beta = sigma2 * np.linalg.inv(X.T @ X)\nse = np.sqrt(np.diag(var_beta))\nt_stats = betas / se\nr_squared = 1 - np.sum(resid**2) / np.sum((y - y.mean())**2)\n\nvar_names = [\"Constant\", \"Z''-Score (EM)\", \"ln(Age)\", \"ln(Total Assets)\"]\nprint(\"=\" * 65)\nprint(\"  Panel Regression: Tobin's Q with Year-Industry FE\")\nprint(f\"  (Within estimator via year×industry demeaning)\")\nprint(\"=\" * 65)\nprint(f\"  {'Variable':&lt;22} {'Coef':&gt;10} {'Std.Err':&gt;10} {'t-stat':&gt;10}\")\nprint(\"-\" * 65)\nfor name, b, s, t in zip(var_names, betas, se, t_stats):\n    sig = \"***\" if abs(t) &gt; 2.576 else \"**\" if abs(t) &gt; 1.96 else \"*\" if abs(t) &gt; 1.645 else \"\"\n    print(f\"  {name:&lt;22} {b:&gt;10.4f} {s:&gt;10.4f} {t:&gt;8.2f} {sig}\")\nprint(\"-\" * 65)\nprint(f\"  R-squared (within): {r_squared:.4f}\")\nprint(f\"  Observations: {n:,}\")\nprint(\"=\" * 65)\n\n=================================================================\n  Panel Regression: Tobin's Q with Year-Industry FE\n  (Within estimator via year×industry demeaning)\n=================================================================\n  Variable                     Coef    Std.Err     t-stat\n-----------------------------------------------------------------\n  Constant                   0.0000     0.0038     0.00 \n  Z''-Score (EM)             0.0034     0.0014     2.45 **\n  ln(Age)                   -0.0055     0.0063    -0.88 \n  ln(Total Assets)          -0.0031     0.0026    -1.16 \n-----------------------------------------------------------------\n  R-squared (within): 0.0024\n  Observations: 3,484\n=================================================================",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#known-limitations-of-tobins-q-in-vietnam",
    "href": "36_valuation.html#known-limitations-of-tobins-q-in-vietnam",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "40.1 Known Limitations of Tobin’s Q in Vietnam",
    "text": "40.1 Known Limitations of Tobin’s Q in Vietnam\nSeveral issues affect the reliability of Tobin’s Q estimates for Vietnamese firms:\n\nBook value as replacement cost proxy: The simplified Q measure assumes that book value of assets approximates replacement cost. Under VAS’s heavy reliance on historical cost, this assumption may be more problematic than in IFRS-adopting countries, particularly for firms with significant land use rights or long-lived tangible assets.\nMarket microstructure effects: Vietnam’s daily price limits can prevent market prices from reaching equilibrium, potentially distorting the market value component. Foreign ownership limits may create artificial price premiums for certain stocks.\nPreferred stock rarity: While this simplifies the computation (most Vietnamese firms have no preferred stock), it means the BE calculation is dominated by common equity, which may not capture all ownership claims.\nCross-listing effects: Some Vietnamese firms are listed on multiple venues (HOSE, HNX, UPCoM, or even foreign exchanges). Care must be taken to use consistent price and share data.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#known-limitations-of-altman-z-score-in-vietnam",
    "href": "36_valuation.html#known-limitations-of-altman-z-score-in-vietnam",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "40.2 Known Limitations of Altman Z-Score in Vietnam",
    "text": "40.2 Known Limitations of Altman Z-Score in Vietnam\n\nCalibration sample: The original Z-Score was estimated on mid-20th-century U.S. manufacturing firms. The Z’’-Score for emerging markets is more appropriate but was still estimated on a non-Vietnamese sample.\nAccounting differences: VAS accounting standards produce financial ratios with different distributional properties than US GAAP or IFRS data, potentially affecting the discriminant function’s classification accuracy.\nBanking and financial firms: The Z-Score was not designed for financial institutions, which have fundamentally different balance sheet structures. Banks, insurance companies, and securities firms should be excluded or analyzed separately.\nImplicit guarantees: SOEs and firms connected to major economic groups may have implicit support that reduces actual default risk below Z-Score predictions.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "36_valuation.html#recommendations-for-researchers",
    "href": "36_valuation.html#recommendations-for-researchers",
    "title": "29  Firm Valuation, Financial Distress, and Company Maturity",
    "section": "40.3 Recommendations for Researchers",
    "text": "40.3 Recommendations for Researchers\nBased on our analysis, we offer the following recommendations for researchers working with Vietnamese data:\n\nUse the Z’’-Score (Emerging Market) variant as the primary distress measure for Vietnamese non-financial firms.\nReport multiple Q variants (Simple and Chung-Pruitt) to demonstrate robustness.\nWinsorize at 1%/99% to mitigate the impact of data errors and extreme outliers.\nCompute industry-adjusted measures when making cross-sectional comparisons.\nUse founding age rather than listing age when available, but report both to distinguish organizational maturity from capital market experience.\nExclude financial firms from Z-Score analysis.\nAccount for state ownership as a moderating variable in valuation and distress studies.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Firm Valuation, Financial Distress, and Company Maturity</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html",
    "href": "37_disclosure_quality_and_timing.html",
    "title": "30  Disclosure Quality and Timing",
    "section": "",
    "text": "30.1 Theoretical Foundations\nCorporate disclosure is the primary mechanism through which firms communicate with capital markets. The quality, quantity, and timing of disclosures shape the information environment in which investors form expectations, price securities, and allocate capital. A large theoretical and empirical literature, surveyed by Healy and Palepu (2001) and Beyer et al. (2010), demonstrates that disclosure decisions have first-order effects on the cost of capital, liquidity, and investment efficiency.\nThis chapter brings two decades of disclosure research to the Vietnamese market, where several institutional features create a distinctive setting. First, Vietnam’s regulatory framework, anchored by Circular 155/2015/TT-BTC (amended by Circular 96/2020/TT-BTC) and enforced by the State Securities Commission (SSC), mandates periodic and event-driven disclosures with specific deadlines that differ from U.S. and European norms. Second, the dominance of retail investors and relatively thin analyst coverage means that corporate disclosures are often the primary source of firm-specific information, amplifying their economic importance. Third, the ongoing transition from Vietnamese Accounting Standards (VAS) toward IFRS convergence introduces time-varying changes in disclosure requirements that create natural variation for empirical analysis.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-theory",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-theory",
    "title": "30  Disclosure Quality and Timing",
    "section": "",
    "text": "30.1.1 Voluntary Disclosure Theory\nThe foundational model of voluntary disclosure is due to Verrecchia (1983), who shows that in a setting where investors know a manager possesses private information, an unraveling equilibrium emerges: silence is interpreted as bad news, so managers disclose unless the proprietary cost of disclosure exceeds its benefit. The key insight is that non-disclosure is informative because investors rationally infer that withheld information is unfavourable.\nDiamond (1985) extends the analysis to a multi-period setting where the firm’s disclosure policy affects the precision of public information and hence the incentives for private information acquisition. The central trade-off is between reducing information asymmetry (which lowers the cost of capital) and reducing the rents that informed traders earn (which may discourage monitoring). Diamond and Verrecchia (1991) formalize the link between disclosure and liquidity: by reducing adverse selection, voluntary disclosure narrows bid-ask spreads and increases the willingness of uninformed investors to trade.\nThe empirical prediction is that firms with higher-quality disclosure should enjoy:\n\nLower cost of equity capital (Botosan 1997; Botosan and Plumlee 2002)\nLower cost of debt (Sengupta 1998)\nHigher liquidity and lower bid-ask spreads (Diamond and Verrecchia 1991; Lang, Lins, and Maffett 2012)\nMore efficient investment decisions (Biddle, Hilary, and Verdi 2009)\n\n\n\n30.1.2 Strategic Disclosure Timing\nNot all disclosure is voluntary in timing, but managers retain discretion over when, within permissible windows, to release information. Patell and Wolfson (1982) document that firms tend to release good news during trading hours and bad news after market close. DellaVigna and Pollet (2009) show that earnings announced on Fridays (i.e., when investor attention is lower) generate smaller immediate reactions and larger post-announcement drift, consistent with limited attention. Hirshleifer, Lim, and Teoh (2009) generalize this finding: extraneous events that distract investors (such as a large number of concurrent announcements) reduce the immediate price response to earnings news.\nIn Vietnam, several features make strategic timing particularly relevant. The concentrated disclosure calendar, where many firms file near regulatory deadlines, creates natural variation in announcement congestion. The retail-dominated investor base may be more susceptible to attention effects than institutional investors. The regulatory structure, which imposes penalties for late filing but allows discretion within the permissible window, creates a setting in which the choice of filing date is informative.\n\n\n30.1.3 Disclosure Quality in Emerging Markets\nBall, Robin, and Wu (2003) argue that accounting quality is shaped more by reporting incentives than by accounting standards. In institutional environments with weak enforcement, concentrated ownership, and close alignment between financial and tax reporting, firms may produce lower-quality disclosures even under nominally rigorous standards. Leuz, Nanda, and Wysocki (2003) confirm this pattern internationally: earnings management (an inverse proxy for disclosure quality) is highest in countries with weak investor protection, concentrated ownership, and less developed capital markets.\nVietnam exhibits several of these features. Bushman et al. (2004) classify determinants of transparency into governance factors (legal origin, judicial efficiency, minority protection) and political factors (state ownership, government intervention). Vietnam’s civil-law tradition, significant state ownership in listed firms, and evolving enforcement capacity suggest that disclosure quality may be lower on average than in developed markets, but with substantial cross-sectional variation driven by firm-level governance and ownership structures.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-regulation",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-regulation",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.2 Regulatory Framework",
    "text": "30.2 Regulatory Framework\n\n30.2.1 Mandatory Disclosure Requirements\nVietnamese disclosure regulation operates through a hierarchy of legal instruments:\n\nSecurities Law (2019): Establishes the general obligation of listed firms to disclose information truthfully, accurately, completely, and on time (Article 118).\nCircular 155/2015/TT-BTC (amended by Circular 96/2020/TT-BTC): Specifies the content, format, and deadlines for periodic and event-driven disclosures.\nSSC decisions and guidance: Provide implementation details and sector-specific requirements.\n\nThe key periodic reporting deadlines are in Table 30.1\n\n\n\nTable 30.1: Periodic disclosure deadlines under Vietnamese securities regulation.\n\n\n\n\n\n\n\n\n\n\nReport Type\nDeadline\nAudit Requirement\n\n\n\n\nAnnual financial statements\n90 days after fiscal year-end\nAudited\n\n\nSemi-annual financial statements\n45 days after period-end\nReviewed\n\n\nQuarterly financial statements\n20 days after quarter-end\nUnaudited\n\n\nAnnual report\n110 days after fiscal year-end\nN/A (narrative)\n\n\n\n\n\n\nEvent-driven (ad hoc) disclosures must be filed within 24 hours for material events, including changes in ownership exceeding 1% by major shareholders, board resolutions on dividends or capital increases, and any event that may materially affect the share price.\n\n\n30.2.2 Penalties for Non-Compliance\nThe SSC may impose administrative fines for late or incomplete disclosure, typically ranging from VND 50–100 million for minor violations and up to VND 500 million for material omissions. While these amounts are modest relative to firm size for large-cap companies, the reputational cost and the risk of trading suspension provide additional deterrence.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-data",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-data",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.3 Data Construction",
    "text": "30.3 Data Construction\n\n30.3.1 Loading Required Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom datetime import datetime, timedelta\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom linearmodels.panel import PanelOLS\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Plotting defaults\nplt.rcParams.update({\n    'figure.figsize': (10, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\n\n\n30.3.2 Retrieving Disclosure Data\nWe assume that we have structured data on filing dates, announcement timestamps, and the textual content of corporate disclosures for all firms.\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Filing metadata: announcement dates, filing dates, report types\nfilings = client.get_filings(\n    exchanges=['HOSE', 'HNX'],\n    report_types=['annual', 'semi_annual', 'quarterly'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    fields=[\n        'ticker', 'report_type', 'fiscal_year', 'fiscal_quarter',\n        'fiscal_year_end', 'filing_date', 'announcement_date',\n        'auditor', 'audit_opinion', 'file_url'\n    ]\n)\n\n# Financial statement data\nfinancials = client.get_fundamentals(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    fields=[\n        'ticker', 'fiscal_year', 'fiscal_quarter',\n        'total_assets', 'total_equity', 'revenue', 'net_income',\n        'operating_cash_flow', 'total_accruals',\n        'market_cap', 'book_to_market'\n    ]\n)\n\n# Daily trading data for event studies\ntrading = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    fields=[\n        'ticker', 'date', 'close', 'volume', 'turnover',\n        'bid_ask_spread', 'market_return'\n    ]\n)\n\n# Ownership and governance\ngovernance = client.get_governance(\n    exchanges=['HOSE', 'HNX'],\n    fields=[\n        'ticker', 'fiscal_year', 'state_ownership_pct',\n        'foreign_ownership_pct', 'board_size',\n        'board_independence_pct', 'big4_auditor',\n        'dual_listing'\n    ]\n)\n\nprint(f\"Filings: {filings.shape[0]:,} records\")\nprint(f\"Financials: {financials.shape[0]:,} records\")\nprint(f\"Trading: {trading.shape[0]:,} records\")\nprint(f\"Governance: {governance.shape[0]:,} records\")\n\n\n\n30.3.3 Computing Filing Timeliness\nWe define reporting lag as the number of calendar days between the fiscal period-end and the date the firm’s financial statements are made publicly available. For annual reports, the regulatory maximum is 90 days; firms that file earlier than the deadline reveal information sooner, while firms that file late face potential penalties and signal possible difficulties with their accounts.\n\nfilings['fiscal_year_end'] = pd.to_datetime(filings['fiscal_year_end'])\nfilings['filing_date'] = pd.to_datetime(filings['filing_date'])\nfilings['announcement_date'] = pd.to_datetime(filings['announcement_date'])\n\n# Reporting lag = filing date - fiscal period end\nfilings['reporting_lag'] = (\n    filings['filing_date'] - filings['fiscal_year_end']\n).dt.days\n\n# Regulatory deadline based on report type\ndeadline_map = {\n    'annual': 90,\n    'semi_annual': 45,\n    'quarterly': 20\n}\nfilings['deadline_days'] = filings['report_type'].map(deadline_map)\n\n# Late filing indicator\nfilings['late_filing'] = (\n    filings['reporting_lag'] &gt; filings['deadline_days']\n).astype(int)\n\n# Days relative to deadline (negative = early, positive = late)\nfilings['days_relative_deadline'] = (\n    filings['reporting_lag'] - filings['deadline_days']\n)\n\n# Summary statistics\nannual_filings = filings[filings['report_type'] == 'annual'].copy()\nprint(\"Annual Report Filing Lag (calendar days):\")\nprint(annual_filings['reporting_lag'].describe().round(1))\nprint(f\"\\nLate filing rate: {annual_filings['late_filing'].mean():.1%}\")\n\n\n\n30.3.4 Distribution of Filing Lags\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogram\naxes[0].hist(\n    annual_filings['reporting_lag'].dropna(),\n    bins=60, range=(20, 150),\n    color='#2C5F8A', edgecolor='white', alpha=0.85\n)\naxes[0].axvline(x=90, color='#C0392B', linestyle='--', linewidth=2,\n                label='90-day deadline')\naxes[0].set_xlabel('Reporting Lag (Calendar Days)')\naxes[0].set_ylabel('Number of Filings')\naxes[0].set_title('Distribution of Annual Report Filing Lags')\naxes[0].legend()\n\n# Time trend: median lag by year\nmedian_lag = (\n    annual_filings\n    .groupby('fiscal_year')['reporting_lag']\n    .agg(['median', lambda x: x.quantile(0.25),\n          lambda x: x.quantile(0.75)])\n)\nmedian_lag.columns = ['median', 'p25', 'p75']\n\naxes[1].fill_between(\n    median_lag.index, median_lag['p25'], median_lag['p75'],\n    alpha=0.3, color='#2C5F8A', label='IQR'\n)\naxes[1].plot(\n    median_lag.index, median_lag['median'],\n    color='#2C5F8A', linewidth=2, marker='o', label='Median'\n)\naxes[1].axhline(y=90, color='#C0392B', linestyle='--',\n                linewidth=1.5, label='Deadline')\naxes[1].set_xlabel('Fiscal Year')\naxes[1].set_ylabel('Filing Lag (Calendar Days)')\naxes[1].set_title('Median Annual Filing Lag Over Time')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 30.1",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-quality",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-quality",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.4 Measuring Disclosure Quality",
    "text": "30.4 Measuring Disclosure Quality\nDisclosure quality is inherently multidimensional. Following Dechow, Ge, and Schrand (2010) and Beyer et al. (2010), we construct proxies along four dimensions: (i) timeliness, (ii) textual properties, (iii) accounting quality, and (iv) voluntary disclosure breadth.\n\n30.4.1 Timeliness as a Quality Dimension\nTimely disclosure reduces the duration of information asymmetry between insiders and outside investors. Chambers and Penman (1984) and Givoly and Palmon (1982) establish that early reporters tend to announce good news, while late reporters more often deliver bad news. We test this pattern in the Vietnamese context below.\nWe operationalize timeliness through two measures:\n\nReporting lag (continuous): Calendar days from fiscal period-end to filing date, as constructed in Section 30.3.3.\nEarly/late classification (categorical): We classify firm-years into terciles based on reporting lag within each fiscal year. This controls for secular trends in filing speed (e.g., driven by regulatory changes or COVID-19 disruptions).\n\n\nannual_filings['lag_tercile'] = (\n    annual_filings\n    .groupby('fiscal_year')['reporting_lag']\n    .transform(lambda x: pd.qcut(x, 3, labels=['Early', 'Middle', 'Late']))\n)\n\n# Tabulate\ntercile_stats = (\n    annual_filings\n    .groupby('lag_tercile')['reporting_lag']\n    .agg(['count', 'mean', 'median', 'std'])\n    .round(1)\n)\ntercile_stats.columns = ['N', 'Mean Lag', 'Median Lag', 'SD']\nprint(tercile_stats)\n\n\n\n30.4.2 Textual Quality Measures\nThe textual properties of corporate disclosures convey information about quality beyond what is captured by accounting numbers alone. Li (2008) demonstrates that annual reports with lower readability are associated with lower earnings persistence, suggesting that complex language may obscure unfavourable information. Loughran and McDonald (2014) critique the application of general readability formulas (Fog index, Flesch-Kincaid) to financial text, arguing that these metrics confound complexity with technical terminology.\nWe construct three textual quality measures adapted for Vietnamese corporate disclosures:\n\n30.4.2.1 Document Length and Specificity\nLonger disclosures are not inherently better—length may reflect boilerplate or obfuscation. However, Dyer, Lang, and Stice-Lawrence (2017) show that the informative component of disclosure (as opposed to standard legal language) has increased over time in U.S. 10-K filings. We measure:\n\nTotal word count: Raw length of the annual report narrative sections (MD&A equivalent)\nNumerical density: Proportion of tokens that are numbers, percentages, or currency amounts, which is a proxy for specificity.\n\n\nimport re\nfrom underthesea import word_tokenize\n\ndef compute_textual_metrics(text):\n    \"\"\"Compute textual quality metrics for Vietnamese corporate text.\"\"\"\n    if not text or len(text.strip()) == 0:\n        return {\n            'word_count': 0, 'sentence_count': 0,\n            'numerical_density': 0, 'avg_sentence_length': 0,\n            'unique_word_ratio': 0, 'forward_looking_density': 0\n        }\n\n    # Vietnamese word segmentation\n    tokens = word_tokenize(text)\n    sentences = re.split(r'[.!?。]', text)\n    sentences = [s.strip() for s in sentences if len(s.strip()) &gt; 5]\n\n    word_count = len(tokens)\n    sentence_count = max(len(sentences), 1)\n\n    # Numerical density: proportion of tokens that are numeric\n    num_pattern = re.compile(r'^[\\d,.%]+$')\n    numeric_tokens = sum(1 for t in tokens if num_pattern.match(t))\n    numerical_density = numeric_tokens / max(word_count, 1)\n\n    # Lexical diversity: unique words / total words\n    unique_words = len(set(t.lower() for t in tokens))\n    unique_word_ratio = unique_words / max(word_count, 1)\n\n    # Forward-looking statement density\n    forward_keywords = [\n        'dự kiến', 'kế hoạch', 'mục tiêu', 'triển vọng',\n        'định hướng', 'chiến lược', 'tương lai', 'sẽ',\n        'dự báo', 'phấn đấu', 'cam kết', 'hướng tới'\n    ]\n    text_lower = text.lower()\n    forward_count = sum(text_lower.count(kw) for kw in forward_keywords)\n    forward_looking_density = forward_count / max(sentence_count, 1)\n\n    return {\n        'word_count': word_count,\n        'sentence_count': sentence_count,\n        'numerical_density': numerical_density,\n        'avg_sentence_length': word_count / sentence_count,\n        'unique_word_ratio': unique_word_ratio,\n        'forward_looking_density': forward_looking_density\n    }\n\n# Retrieve annual report text from DataCore\nannual_text = client.get_annual_report_text(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    sections=['mda', 'business_overview', 'risk_factors']\n)\n\n# Apply textual metrics\ntextual_metrics = annual_text.apply(\n    lambda row: compute_textual_metrics(row['text']),\n    axis=1, result_type='expand'\n)\nannual_text = pd.concat([annual_text, textual_metrics], axis=1)\n\nprint(\"Textual Quality Summary Statistics:\")\nprint(annual_text[['word_count', 'numerical_density',\n                    'avg_sentence_length', 'unique_word_ratio',\n                    'forward_looking_density']].describe().round(3))\n\n\n\n30.4.2.2 Forward-Looking Statement Density\nForward-looking statements reveal management’s expectations about future performance and are considered a higher-quality form of disclosure because they expose the manager to ex-post evaluation. In Vietnamese reports, forward-looking language typically appears in the form of phrases like dự kiến (expected), kế hoạch (plan), mục tiêu (target), and triển vọng (outlook).\nGuay, Samuels, and Taylor (2016) show that managers use voluntary disclosure to “guide through the fog” when financial statements are complex. We operationalize forward-looking density as the number of forward-looking phrases per sentence, following the keyword approach in our compute_textual_metrics function above.\n\n\n\n30.4.3 Accounting-Based Quality Proxies\nWe complement textual measures with accounting-based proxies that capture the reliability of reported financial information.\n\n30.4.3.1 Accruals Quality\nFollowing Francis et al. (2005), we measure accruals quality as the standard deviation of residuals from a regression of working capital accruals on past, current, and future operating cash flows:\n\\[\n\\frac{WC_{i,t}}{A_{i,t-1}} = \\alpha + \\beta_1 \\frac{CFO_{i,t-1}}{A_{i,t-1}} + \\beta_2 \\frac{CFO_{i,t}}{A_{i,t-1}} + \\beta_3 \\frac{CFO_{i,t+1}}{A_{i,t-1}} + \\varepsilon_{i,t}\n\\tag{30.1}\\]\nwhere \\(WC_{i,t}\\) is working capital accruals, \\(CFO_{i,t}\\) is operating cash flow, and \\(A_{i,t-1}\\) is lagged total assets. The firm-level standard deviation of \\(\\hat{\\varepsilon}_{i,t}\\) over a rolling window (typically 5 years) is the accruals quality measure, with higher values indicating lower quality.\n\ndef estimate_accruals_quality(df, min_obs=5):\n    \"\"\"\n    Estimate accruals quality as std dev of DD residuals\n    over a rolling 5-year window for each firm.\n    \"\"\"\n    results = []\n\n    for ticker, group in df.groupby('ticker'):\n        group = group.sort_values('fiscal_year')\n\n        # Construct leads/lags of CFO\n        group['cfo_lag1'] = group['operating_cash_flow'].shift(1)\n        group['cfo_lead1'] = group['operating_cash_flow'].shift(-1)\n\n        # Scale by lagged assets\n        group['lag_assets'] = group['total_assets'].shift(1)\n        for col in ['total_accruals', 'operating_cash_flow',\n                     'cfo_lag1', 'cfo_lead1']:\n            group[f'{col}_scaled'] = group[col] / group['lag_assets']\n\n        # Rolling 5-year residual std dev\n        for idx in range(len(group)):\n            window = group.iloc[max(0, idx - 4):idx + 1]\n            window = window.dropna(subset=[\n                'total_accruals_scaled', 'operating_cash_flow_scaled',\n                'cfo_lag1_scaled', 'cfo_lead1_scaled'\n            ])\n\n            if len(window) &gt;= min_obs:\n                y = window['total_accruals_scaled']\n                X = sm.add_constant(window[[\n                    'cfo_lag1_scaled',\n                    'operating_cash_flow_scaled',\n                    'cfo_lead1_scaled'\n                ]])\n                try:\n                    model = sm.OLS(y, X).fit()\n                    results.append({\n                        'ticker': ticker,\n                        'fiscal_year': group.iloc[idx]['fiscal_year'],\n                        'accruals_quality': model.resid.std()\n                    })\n                except Exception:\n                    pass\n\n    return pd.DataFrame(results)\n\naq_df = estimate_accruals_quality(financials)\nprint(f\"Accruals quality computed for {aq_df['ticker'].nunique()} firms\")\nprint(aq_df['accruals_quality'].describe().round(4))\n\n\n\n30.4.3.2 Earnings Persistence and Predictability\nPersistent earnings are more useful for valuation. We estimate earnings persistence as the slope coefficient \\(\\phi_1\\) from a first-order autoregression:\n\\[\n\\frac{E_{i,t}}{A_{i,t-1}} = \\phi_0 + \\phi_1 \\frac{E_{i,t-1}}{A_{i,t-2}} + \\nu_{i,t}\n\\tag{30.2}\\]\nHigher \\(\\hat{\\phi}_1\\) indicates more persistent (and arguably higher-quality) earnings.\n\ndef estimate_persistence(df, min_obs=5):\n    \"\"\"Estimate earnings persistence via AR(1) model.\"\"\"\n    results = []\n\n    for ticker, group in df.groupby('ticker'):\n        group = group.sort_values('fiscal_year')\n        group['earnings_scaled'] = group['net_income'] / group['total_assets'].shift(1)\n        group['earnings_lag'] = group['earnings_scaled'].shift(1)\n\n        clean = group.dropna(subset=['earnings_scaled', 'earnings_lag'])\n        if len(clean) &gt;= min_obs:\n            y = clean['earnings_scaled']\n            X = sm.add_constant(clean[['earnings_lag']])\n            model = sm.OLS(y, X).fit()\n            results.append({\n                'ticker': ticker,\n                'persistence': model.params['earnings_lag'],\n                'persistence_se': model.bse['earnings_lag'],\n                'r_squared': model.rsquared,\n                'n_obs': model.nobs\n            })\n\n    return pd.DataFrame(results)\n\npersistence_df = estimate_persistence(financials)\nprint(persistence_df[['persistence', 'r_squared']].describe().round(3))\n\n\n\n\n30.4.4 Composite Disclosure Quality Index\nIndividual quality proxies capture different facets of the information environment. To aggregate them into a single score while avoiding arbitrary weighting, we follow Lang and Lundholm (1993) and use a rank-based composite. For each firm-year, we rank firms on each of the following dimensions (higher rank = higher quality) (Table 30.2).\n\n\n\nTable 30.2: Components of the composite disclosure quality index.\n\n\n\n\n\nDimension\nProxy\nDirection\n\n\n\n\nTimeliness\nReporting lag\nLower is better\n\n\nSpecificity\nNumerical density\nHigher is better\n\n\nForward-looking\nFLS density\nHigher is better\n\n\nEarnings quality\nAccruals quality (DD)\nLower σ is better\n\n\nPersistence\nAR(1) coefficient\nHigher is better\n\n\n\n\n\n\nWe convert each proxy to a percentile rank within each fiscal year (so each component ranges from 0 to 1), then average across components:\n\\[\nDQ_{i,t} = \\frac{1}{K} \\sum_{k=1}^{K} \\text{Rank}_{k,i,t}\n\\tag{30.3}\\]\nwhere \\(K\\) is the number of available components and \\(\\text{Rank}_{k,i,t}\\) is the percentile rank of firm \\(i\\) in year \\(t\\) on dimension \\(k\\).\n\n# Merge all quality proxies\nquality_panel = (\n    annual_filings[['ticker', 'fiscal_year', 'reporting_lag']]\n    .merge(\n        annual_text[['ticker', 'fiscal_year', 'numerical_density',\n                      'forward_looking_density']],\n        on=['ticker', 'fiscal_year'], how='left'\n    )\n    .merge(aq_df, on=['ticker', 'fiscal_year'], how='left')\n    .merge(persistence_df[['ticker', 'persistence']],\n           on='ticker', how='left')\n)\n\n# Rank each component within fiscal year (higher = better quality)\ndef year_percentile_rank(series):\n    \"\"\"Convert to percentile rank within group.\"\"\"\n    return series.rank(pct=True)\n\nrank_cols = {}\nfor col, ascending in [\n    ('reporting_lag', False),       # lower lag = better → invert\n    ('numerical_density', True),    # higher = better\n    ('forward_looking_density', True),\n    ('accruals_quality', False),    # lower volatility = better → invert\n    ('persistence', True)           # higher = better\n]:\n    col_to_rank = quality_panel[col] if ascending else -quality_panel[col]\n    rank_cols[f'rank_{col}'] = (\n        quality_panel\n        .groupby('fiscal_year')[col]\n        .transform(lambda x: x.rank(pct=True) if ascending\n                   else (-x).rank(pct=True))\n    )\n\nrank_df = pd.DataFrame(rank_cols)\nquality_panel = pd.concat([quality_panel, rank_df], axis=1)\n\n# Composite index: average of available ranks\nrank_columns = [c for c in quality_panel.columns if c.startswith('rank_')]\nquality_panel['dq_index'] = quality_panel[rank_columns].mean(axis=1)\n\nprint(\"Disclosure Quality Index Distribution:\")\nprint(quality_panel['dq_index'].describe().round(3))\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nax.hist(quality_panel['dq_index'].dropna(), bins=50,\n        color='#2C5F8A', edgecolor='white', alpha=0.85)\nax.axvline(quality_panel['dq_index'].median(), color='#E67E22',\n           linestyle='--', linewidth=2, label='Median')\nax.set_xlabel('Disclosure Quality Index')\nax.set_ylabel('Number of Firm-Years')\nax.set_title('Distribution of Composite Disclosure Quality')\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\nFigure 30.2",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-determinants",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-determinants",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.5 Determinants of Disclosure Quality",
    "text": "30.5 Determinants of Disclosure Quality\nWhat drives variation in disclosure quality across Vietnamese firms? We estimate a cross-sectional regression of the composite DQ index on firm characteristics and governance variables:\n\\[\nDQ_{i,t} = \\alpha + \\beta_1 \\ln(\\text{Size}_{i,t}) + \\beta_2 \\text{ROA}_{i,t} + \\beta_3 \\text{Lev}_{i,t} + \\beta_4 \\text{StateOwn}_{i,t} + \\beta_5 \\text{ForeignOwn}_{i,t} + \\beta_6 \\text{Big4}_{i,t} + \\beta_7 \\text{BoardIndep}_{i,t} + \\gamma_t + \\varepsilon_{i,t}\n\\tag{30.4}\\]\nwhere \\(\\gamma_t\\) are year fixed effects.\nThe theoretical predictions, drawing on Lang and Lundholm (1993), Hope (2003), and Bushman et al. (2004), are:\n\nSize (+): Larger firms face greater public scrutiny and have lower proprietary costs relative to the benefits of disclosure.\nROA (+/−): Profitable firms may disclose more to signal quality, but firms managing earnings downward (for tax purposes) may reduce disclosure to avoid scrutiny.\nLeverage (+): Sengupta (1998) argues that firms with more debt have stronger incentives to maintain disclosure quality to lower borrowing costs.\nState ownership (−): SOEs may face weaker market discipline and political incentives to limit transparency.\nForeign ownership (+): Foreign institutional investors demand higher transparency.\nBig 4 auditor (+): High-quality auditors constrain earnings management and indirectly improve disclosure quality.\nBoard independence (+): Independent directors improve monitoring and encourage more informative disclosure.\n\n\n# Merge quality index with financials and governance\ndet_panel = (\n    quality_panel[['ticker', 'fiscal_year', 'dq_index']]\n    .merge(financials, on=['ticker', 'fiscal_year'], how='left')\n    .merge(governance, on=['ticker', 'fiscal_year'], how='left')\n)\n\n# Construct variables\ndet_panel['log_size'] = np.log(det_panel['total_assets'])\ndet_panel['roa'] = det_panel['net_income'] / det_panel['total_assets']\ndet_panel['leverage'] = (\n    (det_panel['total_assets'] - det_panel['total_equity'])\n    / det_panel['total_assets']\n)\n\n# Panel regression with year FE\ndet_panel = det_panel.set_index(['ticker', 'fiscal_year'])\n\nmodel_det = PanelOLS(\n    dependent=det_panel['dq_index'],\n    exog=sm.add_constant(det_panel[[\n        'log_size', 'roa', 'leverage',\n        'state_ownership_pct', 'foreign_ownership_pct',\n        'big4_auditor', 'board_independence_pct'\n    ]]),\n    entity_effects=False,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type='clustered', cluster_entity=True)\n\nprint(model_det.summary)\n\n\n\n\ncoefs = model_det.params.drop('const')\nci = model_det.conf_int().drop('const')\n\nfig, ax = plt.subplots(figsize=(8, 5))\ny_pos = range(len(coefs))\nlabels = [\n    'ln(Assets)', 'ROA', 'Leverage', 'State Own %',\n    'Foreign Own %', 'Big 4 Auditor', 'Board Indep %'\n]\n\ncolors = ['#2C5F8A' if c &gt; 0 else '#C0392B' for c in coefs.values]\nax.barh(y_pos, coefs.values, color=colors, alpha=0.8, height=0.6)\nax.errorbar(\n    coefs.values, y_pos,\n    xerr=[coefs.values - ci.iloc[:, 0].values,\n          ci.iloc[:, 1].values - coefs.values],\n    fmt='none', color='black', capsize=3\n)\nax.axvline(x=0, color='gray', linewidth=0.8, linestyle='-')\nax.set_yticks(y_pos)\nax.set_yticklabels(labels)\nax.set_xlabel('Coefficient Estimate')\nax.set_title('Determinants of Disclosure Quality')\nplt.tight_layout()\nplt.show()\n\n\nFigure 30.3",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-strategic-timing",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-strategic-timing",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.6 Strategic Disclosure Timing",
    "text": "30.6 Strategic Disclosure Timing\n\n30.6.1 Day-of-Week Effects\nDellaVigna and Pollet (2009) document that Friday earnings announcements receive less immediate market attention. We test whether this pattern holds in Vietnam, where the trading week runs Monday through Friday but the retail-dominated investor base may exhibit different attention patterns.\n\nannual_filings['announcement_dow'] = (\n    annual_filings['announcement_date'].dt.dayofweek\n)\nannual_filings['day_name'] = (\n    annual_filings['announcement_date'].dt.day_name()\n)\n\n# Compute surprise: actual earnings minus naive expectation (last year's earnings)\nannual_filings = annual_filings.merge(\n    financials[['ticker', 'fiscal_year', 'net_income', 'total_assets']],\n    on=['ticker', 'fiscal_year'], how='left'\n)\nannual_filings['earnings_scaled'] = (\n    annual_filings['net_income'] / annual_filings['total_assets']\n)\nannual_filings['earnings_surprise'] = (\n    annual_filings\n    .groupby('ticker')['earnings_scaled']\n    .diff()\n)\n\n# Classify as good/bad news\nannual_filings['bad_news'] = (\n    annual_filings['earnings_surprise'] &lt; 0\n).astype(int)\n\n# Day-of-week distribution by news type\ndow_crosstab = pd.crosstab(\n    annual_filings['day_name'],\n    annual_filings['bad_news'].map({0: 'Good News', 1: 'Bad News'}),\n    normalize='columns'\n)\n# Reorder days\nday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\ndow_crosstab = dow_crosstab.reindex(day_order)\n\nprint(\"Proportion of Announcements by Day and News Type:\")\nprint(dow_crosstab.round(3))\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 5))\nx = np.arange(len(day_order))\nwidth = 0.35\n\nbad_pct = dow_crosstab['Bad News'].values\ngood_pct = dow_crosstab['Good News'].values\n\nax.bar(x - width/2, good_pct, width, label='Good News',\n       color='#27AE60', alpha=0.8)\nax.bar(x + width/2, bad_pct, width, label='Bad News',\n       color='#C0392B', alpha=0.8)\nax.set_xticks(x)\nax.set_xticklabels(day_order)\nax.set_ylabel('Proportion of Announcements')\nax.set_title('Strategic Timing: Day-of-Week Announcement Patterns')\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\nFigure 30.4\n\n\n\n\n\n30.6.2 Announcement Congestion\nWhen many firms announce on the same day, each announcement receives less attention. We measure announcement congestion as the number of other firms making earnings announcements on the same date:\n\\[\n\\text{Congestion}_{i,t} = \\sum_{j \\neq i} \\mathbf{1}\\{\\text{AnnDate}_{j} = \\text{AnnDate}_{i}\\}\n\\tag{30.5}\\]\nHirshleifer, Lim, and Teoh (2009) predict that firms burying bad news will choose high-congestion days. We test this by regressing the congestion variable on the sign of earnings news:\n\n# Count announcements per date\nann_counts = (\n    annual_filings\n    .groupby('announcement_date')\n    .size()\n    .reset_index(name='n_announcements')\n)\nannual_filings = annual_filings.merge(\n    ann_counts, on='announcement_date', how='left'\n)\nannual_filings['congestion'] = annual_filings['n_announcements'] - 1\n\n# Regression: congestion ~ bad_news + controls\ncongestion_model = smf.ols(\n    'congestion ~ bad_news + log_size + roa + C(fiscal_year)',\n    data=annual_filings.assign(\n        log_size=np.log(annual_filings['total_assets']),\n        roa=annual_filings['net_income'] / annual_filings['total_assets']\n    )\n).fit(cov_type='cluster', cov_kwds={'groups': annual_filings['ticker']})\n\nprint(\"Congestion Regression:\")\nprint(congestion_model.summary().tables[1])\n\n\n\n30.6.3 After-Hours and Weekend Announcements\nVietnamese regulations require disclosure within 24 hours of material events, but firms retain discretion over the exact timing. Announcements made after the trading session closes (after 3:00 PM on HOSE/HNX) or on weekends delay the market’s opportunity to react by at least one trading day.\n\n# Assume announcement timestamps are available\nannual_filings['ann_hour'] = (\n    annual_filings['announcement_date'].dt.hour\n)\nannual_filings['after_hours'] = (\n    (annual_filings['ann_hour'] &gt;= 15) |\n    (annual_filings['announcement_dow'] &gt;= 5)  # Saturday/Sunday\n).astype(int)\n\n# Cross-tabulate after-hours by news type\nafterhours_crosstab = pd.crosstab(\n    annual_filings['after_hours'].map({0: 'During Hours', 1: 'After Hours'}),\n    annual_filings['bad_news'].map({0: 'Good News', 1: 'Bad News'}),\n    normalize='index'\n)\nprint(\"News Distribution by Announcement Timing:\")\nprint(afterhours_crosstab.round(3))\n\n# Chi-squared test\ncontingency = pd.crosstab(\n    annual_filings['after_hours'], annual_filings['bad_news']\n)\nchi2, p_val, _, _ = stats.chi2_contingency(contingency)\nprint(f\"\\nChi-squared = {chi2:.2f}, p-value = {p_val:.4f}\")",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-consequences",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-consequences",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.7 Market Consequences of Disclosure Quality",
    "text": "30.7 Market Consequences of Disclosure Quality\n\n30.7.1 Disclosure Quality and the Cost of Equity\nThe central prediction of Diamond and Verrecchia (1991) and Botosan (1997) is that higher-quality disclosure lowers the cost of equity capital by reducing information asymmetry. We test this using the implied cost of capital (ICC) approach, where we estimate the discount rate that equates the current price to the present value of expected future earnings.\nWe use the PEG ratio approach as a simple ICC estimate:\n\\[\nr_{PEG,i,t} = \\sqrt{\\frac{\\hat{E}_{i,t+2} - \\hat{E}_{i,t+1}}{P_{i,t}}}\n\\tag{30.6}\\]\nwhere \\(\\hat{E}_{i,t+k}\\) is the consensus earnings forecast (or, in the absence of analyst coverage, a model-based forecast) and \\(P_{i,t}\\) is the current stock price.\n\n# Construct earnings forecasts using a simple random walk with drift\nforecasts = financials.sort_values(['ticker', 'fiscal_year']).copy()\nforecasts['eps'] = forecasts['net_income'] / forecasts['market_cap']\nforecasts['eps_growth'] = forecasts.groupby('ticker')['eps'].pct_change()\n\n# Simple forecast: E[t+1] = E[t] * (1 + avg_growth)\nforecasts['avg_growth'] = (\n    forecasts.groupby('ticker')['eps_growth']\n    .transform(lambda x: x.rolling(3, min_periods=2).mean())\n)\nforecasts['eps_f1'] = forecasts['eps'] * (1 + forecasts['avg_growth'])\nforecasts['eps_f2'] = forecasts['eps_f1'] * (1 + forecasts['avg_growth'])\n\n# PEG-based ICC\nforecasts['icc_peg'] = np.sqrt(\n    np.maximum(forecasts['eps_f2'] - forecasts['eps_f1'], 0)\n    / np.maximum(forecasts['market_cap'] / 1e6, 1e-6)\n)\n\n# Merge with disclosure quality\nicc_panel = (\n    forecasts[['ticker', 'fiscal_year', 'icc_peg']]\n    .merge(quality_panel[['ticker', 'fiscal_year', 'dq_index']].reset_index(drop=True),\n           on=['ticker', 'fiscal_year'], how='inner')\n    .merge(governance, on=['ticker', 'fiscal_year'], how='left')\n    .merge(financials[['ticker', 'fiscal_year', 'total_assets',\n                        'book_to_market', 'market_cap']],\n           on=['ticker', 'fiscal_year'], how='left')\n)\n\nicc_panel['log_size'] = np.log(icc_panel['market_cap'])\nicc_panel = icc_panel.set_index(['ticker', 'fiscal_year'])\n\n# Panel regression: ICC ~ DQ + controls\nicc_model = PanelOLS(\n    dependent=icc_panel['icc_peg'],\n    exog=sm.add_constant(icc_panel[[\n        'dq_index', 'log_size', 'book_to_market'\n    ]]),\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type='clustered', cluster_entity=True)\n\nprint(\"Implied Cost of Capital ~ Disclosure Quality:\")\nprint(icc_model.summary)\n\n\n\n30.7.2 Disclosure Quality and Liquidity\nDiamond and Verrecchia (1991) predict that better disclosure reduces adverse selection and improves liquidity. We measure liquidity through bid-ask spreads and the Amihud illiquidity ratio:\n\\[\n\\text{Amihud}_{i,t} = \\frac{1}{D_{i,t}} \\sum_{d=1}^{D_{i,t}} \\frac{|R_{i,d}|}{\\text{Volume}_{i,d}}\n\\tag{30.7}\\]\nwhere \\(R_{i,d}\\) is the daily return and \\(\\text{Volume}_{i,d}\\) is the daily trading volume in VND.\n\n# Compute annual Amihud illiquidity\ntrading['abs_return'] = trading['close'].pct_change().abs()\ntrading['amihud_daily'] = trading['abs_return'] / (trading['volume'] * trading['close'])\n\namihud_annual = (\n    trading\n    .assign(fiscal_year=trading['date'].dt.year)\n    .groupby(['ticker', 'fiscal_year'])\n    .agg(\n        amihud=('amihud_daily', 'mean'),\n        avg_spread=('bid_ask_spread', 'mean'),\n        avg_turnover=('turnover', 'mean')\n    )\n    .reset_index()\n)\n\n# Log transform for better distributional properties\namihud_annual['log_amihud'] = np.log(amihud_annual['amihud'] + 1e-10)\namihud_annual['log_spread'] = np.log(amihud_annual['avg_spread'] + 1e-6)\n\n# Merge and run regression\nliq_panel = (\n    amihud_annual\n    .merge(quality_panel[['ticker', 'fiscal_year', 'dq_index']].reset_index(drop=True),\n           on=['ticker', 'fiscal_year'], how='inner')\n    .merge(financials[['ticker', 'fiscal_year', 'market_cap', 'total_assets']],\n           on=['ticker', 'fiscal_year'], how='left')\n)\nliq_panel['log_size'] = np.log(liq_panel['market_cap'])\nliq_panel = liq_panel.set_index(['ticker', 'fiscal_year'])\n\nliq_model = PanelOLS(\n    dependent=liq_panel['log_amihud'],\n    exog=sm.add_constant(liq_panel[['dq_index', 'log_size']]),\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type='clustered', cluster_entity=True)\n\nprint(\"Amihud Illiquidity ~ Disclosure Quality:\")\nprint(liq_model.summary)\n\n\n\n\nliq_panel_plot = liq_panel.reset_index()\nliq_panel_plot['dq_quintile'] = pd.qcut(\n    liq_panel_plot['dq_index'], 5, labels=['Q1\\n(Low)', 'Q2', 'Q3', 'Q4', 'Q5\\n(High)']\n)\n\nquintile_liq = (\n    liq_panel_plot\n    .groupby('dq_quintile')['log_amihud']\n    .agg(['mean', 'sem'])\n)\n\nfig, ax = plt.subplots(figsize=(8, 5))\nbars = ax.bar(\n    range(5), quintile_liq['mean'],\n    yerr=1.96 * quintile_liq['sem'],\n    color=['#C0392B', '#E67E22', '#F1C40F', '#27AE60', '#2C5F8A'],\n    alpha=0.85, capsize=4, edgecolor='white'\n)\nax.set_xticks(range(5))\nax.set_xticklabels(quintile_liq.index)\nax.set_xlabel('Disclosure Quality Quintile')\nax.set_ylabel('Log Amihud Illiquidity')\nax.set_title('Disclosure Quality and Market Liquidity')\nplt.tight_layout()\nplt.show()\n\n\nFigure 30.5\n\n\n\n\n\n30.7.3 Event Study: Market Reaction to Filing Lag\nWe examine whether the market reacts differently to early vs. late filers by computing cumulative abnormal returns (CARs) around the filing date:\n\\[\nCAR_{i}[\\tau_1, \\tau_2] = \\sum_{t=\\tau_1}^{\\tau_2} (R_{i,t} - \\hat{R}_{i,t})\n\\tag{30.8}\\]\nwhere \\(\\hat{R}_{i,t}\\) is the expected return from a market model estimated over a pre-event window \\([-250, -30]\\).\n\ndef compute_car(ticker, event_date, trading_df,\n                est_window=(-250, -30), event_window=(-5, 10)):\n    \"\"\"Compute CAR around an event date using market model.\"\"\"\n    firm_data = trading_df[trading_df['ticker'] == ticker].copy()\n    firm_data = firm_data.sort_values('date')\n\n    # Find event date index\n    event_idx = firm_data[firm_data['date'] &gt;= event_date].index\n    if len(event_idx) == 0:\n        return None\n    event_idx = event_idx[0]\n    event_pos = firm_data.index.get_loc(event_idx)\n\n    # Check sufficient data\n    if event_pos + est_window[0] &lt; 0:\n        return None\n\n    # Estimation window\n    est_start = event_pos + est_window[0]\n    est_end = event_pos + est_window[1]\n    est_data = firm_data.iloc[est_start:est_end + 1]\n\n    firm_ret = est_data['close'].pct_change()\n    mkt_ret = est_data['market_return']\n\n    valid = firm_ret.notna() & mkt_ret.notna()\n    if valid.sum() &lt; 100:\n        return None\n\n    # Market model\n    X = sm.add_constant(mkt_ret[valid])\n    model = sm.OLS(firm_ret[valid], X).fit()\n\n    # Event window\n    ev_start = event_pos + event_window[0]\n    ev_end = event_pos + event_window[1]\n    ev_data = firm_data.iloc[ev_start:ev_end + 1]\n\n    ev_ret = ev_data['close'].pct_change()\n    ev_mkt = ev_data['market_return']\n    expected_ret = model.params['const'] + model.params['market_return'] * ev_mkt\n    abnormal_ret = ev_ret - expected_ret\n\n    return abnormal_ret.cumsum().values\n\n# Sample: compute CARs for annual filings\ncar_results = []\nfor _, row in annual_filings.sample(min(2000, len(annual_filings))).iterrows():\n    car = compute_car(row['ticker'], row['filing_date'], trading)\n    if car is not None and len(car) == 16:  # -5 to +10\n        car_results.append({\n            'ticker': row['ticker'],\n            'fiscal_year': row['fiscal_year'],\n            'lag_tercile': row['lag_tercile'],\n            'car': car\n        })\n\ncar_df = pd.DataFrame(car_results)\nprint(f\"Computed CARs for {len(car_df)} firm-year events\")\n\n\n\n\nevent_days = range(-5, 11)\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = {'Early': '#27AE60', 'Middle': '#F1C40F', 'Late': '#C0392B'}\n\nfor tercile in ['Early', 'Middle', 'Late']:\n    subset = car_df[car_df['lag_tercile'] == tercile]\n    if len(subset) &gt; 0:\n        avg_car = np.mean(np.stack(subset['car'].values), axis=0)\n        se_car = np.std(np.stack(subset['car'].values), axis=0) / np.sqrt(len(subset))\n        ax.plot(event_days, avg_car, color=colors[tercile],\n                linewidth=2, label=tercile)\n        ax.fill_between(event_days,\n                        avg_car - 1.96 * se_car,\n                        avg_car + 1.96 * se_car,\n                        color=colors[tercile], alpha=0.15)\n\nax.axvline(x=0, color='gray', linestyle='--', linewidth=0.8)\nax.axhline(y=0, color='gray', linewidth=0.5)\nax.set_xlabel('Event Day (Relative to Filing Date)')\nax.set_ylabel('Cumulative Abnormal Return')\nax.set_title('Market Reaction Around Filing Date by Timeliness')\nax.legend(title='Filing Tercile')\nplt.tight_layout()\nplt.show()\n\n\nFigure 30.6",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-timeliness-quality",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-timeliness-quality",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.8 Filing Timeliness and Earnings Quality",
    "text": "30.8 Filing Timeliness and Earnings Quality\nGivoly and Palmon (1982) and Chambers and Penman (1984) establish that the content of disclosed information is correlated with its timing. We test this link formally: do late filers have worse earnings quality?\n\n\n\ntq_panel = (\n    annual_filings[['ticker', 'fiscal_year', 'lag_tercile', 'reporting_lag']]\n    .merge(aq_df, on=['ticker', 'fiscal_year'], how='inner')\n    .merge(persistence_df[['ticker', 'persistence']], on='ticker', how='left')\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Accruals quality by tercile\naq_by_tercile = tq_panel.groupby('lag_tercile')['accruals_quality'].mean()\naxes[0].bar(\n    range(3), aq_by_tercile.values,\n    color=['#27AE60', '#F1C40F', '#C0392B'], alpha=0.85,\n    edgecolor='white'\n)\naxes[0].set_xticks(range(3))\naxes[0].set_xticklabels(['Early', 'Middle', 'Late'])\naxes[0].set_ylabel('Accruals Quality (σ of DD Residuals)')\naxes[0].set_title('Panel A: Accruals Quality by Filing Tercile')\naxes[0].text(0.05, 0.95, 'Higher = lower quality',\n             transform=axes[0].transAxes, fontsize=9,\n             verticalalignment='top', style='italic', color='gray')\n\n# Panel B: Persistence by tercile\nper_by_tercile = tq_panel.groupby('lag_tercile')['persistence'].mean()\naxes[1].bar(\n    range(3), per_by_tercile.values,\n    color=['#27AE60', '#F1C40F', '#C0392B'], alpha=0.85,\n    edgecolor='white'\n)\naxes[1].set_xticks(range(3))\naxes[1].set_xticklabels(['Early', 'Middle', 'Late'])\naxes[1].set_ylabel('Earnings Persistence (AR(1) Coefficient)')\naxes[1].set_title('Panel B: Earnings Persistence by Filing Tercile')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 30.7\n\n\n\nWe formalize this with a regression that controls for firm characteristics:\n\ntq_panel_reg = tq_panel.merge(\n    financials[['ticker', 'fiscal_year', 'total_assets', 'net_income',\n                'total_equity']],\n    on=['ticker', 'fiscal_year'], how='left'\n).merge(governance, on=['ticker', 'fiscal_year'], how='left')\n\ntq_panel_reg['log_size'] = np.log(tq_panel_reg['total_assets'])\ntq_panel_reg['roa'] = tq_panel_reg['net_income'] / tq_panel_reg['total_assets']\ntq_panel_reg['late'] = (tq_panel_reg['lag_tercile'] == 'Late').astype(int)\n\nmodel_tq = smf.ols(\n    'accruals_quality ~ late + log_size + roa + state_ownership_pct '\n    '+ big4_auditor + C(fiscal_year)',\n    data=tq_panel_reg\n).fit(cov_type='cluster', cov_kwds={'groups': tq_panel_reg['ticker']})\n\nprint(\"Accruals Quality ~ Late Filing:\")\nprint(model_tq.summary().tables[1])\n\n\n\n\n\n\n\nNoteEndogeneity Caveat\n\n\n\nThe association between filing timeliness and earnings quality is likely endogenous: firms with complex accounting issues take longer to prepare financial statements, and the same complexity drives lower earnings quality. The filing lag is thus best interpreted as an observable signal of underlying accounting difficulty rather than a causal determinant. Instrumental variable approaches (e.g., using auditor busyness during peak filing season as an instrument for filing lag) can partially address this concern.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-investment",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-investment",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.9 Disclosure Quality and Investment Efficiency",
    "text": "30.9 Disclosure Quality and Investment Efficiency\nBiddle, Hilary, and Verdi (2009) demonstrate that higher financial reporting quality is associated with more efficient investment. Specifically, it reduces both over-investment (in firms with excess cash) and under-investment (in firms that are financially constrained). The mechanism is that better disclosure reduces information asymmetry between managers and capital providers, improving the allocation of capital.\nWe test this prediction in Vietnam using the Biddle, Hilary, and Verdi (2009) framework:\n\\[\n\\text{Investment}_{i,t+1} = \\alpha + \\beta_1 \\text{SalesGrowth}_{i,t} + \\varepsilon_{i,t+1}\n\\tag{30.9}\\]\nThe residual \\(\\hat{\\varepsilon}_{i,t+1}\\) measures deviation from expected investment. Positive residuals indicate over-investment; negative residuals indicate under-investment. We then test whether the absolute value of this residual is lower for firms with higher disclosure quality.\n\ninv_panel = financials.sort_values(['ticker', 'fiscal_year']).copy()\n\n# Investment = change in total assets / lagged total assets\ninv_panel['investment'] = (\n    inv_panel.groupby('ticker')['total_assets'].pct_change()\n)\ninv_panel['sales_growth'] = (\n    inv_panel.groupby('ticker')['revenue'].pct_change()\n)\n\n# Expected investment model\ninv_model = smf.ols(\n    'investment ~ sales_growth',\n    data=inv_panel\n).fit()\ninv_panel['inv_residual'] = inv_model.resid\ninv_panel['abs_inv_residual'] = inv_panel['inv_residual'].abs()\n\n# Merge with disclosure quality\ninv_eff = (\n    inv_panel[['ticker', 'fiscal_year', 'abs_inv_residual',\n               'investment', 'total_assets']]\n    .merge(quality_panel[['ticker', 'fiscal_year', 'dq_index']].reset_index(drop=True),\n           on=['ticker', 'fiscal_year'], how='inner')\n)\ninv_eff['log_size'] = np.log(inv_eff['total_assets'])\ninv_eff = inv_eff.set_index(['ticker', 'fiscal_year'])\n\n# Panel regression\ninv_eff_model = PanelOLS(\n    dependent=inv_eff['abs_inv_residual'],\n    exog=sm.add_constant(inv_eff[['dq_index', 'log_size']]),\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type='clustered', cluster_entity=True)\n\nprint(\"Investment Inefficiency ~ Disclosure Quality:\")\nprint(inv_eff_model.summary)\n\nA negative coefficient on dq_index indicates that higher disclosure quality is associated with lower investment inefficiency: firms with better disclosure make investment decisions closer to what their growth opportunities warrant.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-vietnam-context",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-vietnam-context",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.10 Vietnamese Institutional Context",
    "text": "30.10 Vietnamese Institutional Context\n\n30.10.1 State Ownership and Disclosure\nSOEs account for a substantial share of Vietnamese market capitalization. The relationship between state ownership and disclosure quality is theoretically ambiguous. On one hand, political connections may reduce the pressure to disclose transparently; government shareholders may tolerate opacity that private shareholders would not. On the other hand, post-equitization monitoring by multiple stakeholders (MOF, SCIC, minority shareholders) may create competing disclosure demands.\n\nsoe_panel = (\n    quality_panel[['ticker', 'fiscal_year', 'dq_index',\n                    'reporting_lag']].reset_index(drop=True)\n    .merge(governance[['ticker', 'fiscal_year', 'state_ownership_pct']],\n           on=['ticker', 'fiscal_year'], how='inner')\n)\n\nsoe_panel['soe'] = (soe_panel['state_ownership_pct'] &gt;= 50).astype(int)\nsoe_panel['soe_label'] = soe_panel['soe'].map(\n    {1: 'SOE (≥50%)', 0: 'Private (&lt;50%)'}\n)\n\n# Compare means\ncomparison = (\n    soe_panel\n    .groupby('soe_label')\n    .agg(\n        n=('dq_index', 'count'),\n        mean_dq=('dq_index', 'mean'),\n        median_dq=('dq_index', 'median'),\n        mean_lag=('reporting_lag', 'mean'),\n        median_lag=('reporting_lag', 'median')\n    )\n    .round(3)\n)\nprint(\"SOE vs Private Firm Disclosure Comparison:\")\nprint(comparison)\n\n# Formal t-test\nsoe_dq = soe_panel[soe_panel['soe'] == 1]['dq_index']\npriv_dq = soe_panel[soe_panel['soe'] == 0]['dq_index']\nt_stat, p_val = stats.ttest_ind(soe_dq.dropna(), priv_dq.dropna())\nprint(f\"\\nt-test: t = {t_stat:.3f}, p = {p_val:.4f}\")\n\n\n\n30.10.2 IFRS Convergence and Disclosure Quality\nVietnam has been pursuing a phased convergence toward IFRS, with the Ministry of Finance issuing a roadmap for voluntary adoption by large listed firms. The transition from VAS to IFRS-aligned standards is expected to expand disclosure requirements—particularly for financial instruments (IFRS 9), revenue recognition (IFRS 15), and leases (IFRS 16). Barth, Landsman, and Lang (2008) provide evidence that IFRS adoption is associated with improvements in earnings quality and disclosure, though the effect depends on enforcement strength.\nWe can exploit the staggered timing of voluntary IFRS adoption across Vietnamese firms as a natural experiment:\n\n# Assume DataCore provides IFRS adoption dates\nifrs_adoption = client.get_ifrs_adoption(\n    exchanges=['HOSE', 'HNX'],\n    fields=['ticker', 'ifrs_adoption_year']\n)\n\n# Merge with quality panel\nifrs_panel = (\n    quality_panel[['ticker', 'fiscal_year', 'dq_index']].reset_index(drop=True)\n    .merge(ifrs_adoption, on='ticker', how='left')\n)\n\n# Treatment indicator\nifrs_panel['post_ifrs'] = (\n    ifrs_panel['fiscal_year'] &gt;= ifrs_panel['ifrs_adoption_year']\n).astype(int).fillna(0)\n\nifrs_panel['treated'] = ifrs_panel['ifrs_adoption_year'].notna().astype(int)\n\n# Simple DiD\nifrs_panel = ifrs_panel.set_index(['ticker', 'fiscal_year'])\ndid_model = PanelOLS(\n    dependent=ifrs_panel['dq_index'],\n    exog=sm.add_constant(ifrs_panel[['post_ifrs']]),\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type='clustered', cluster_entity=True)\n\nprint(\"DiD: IFRS Adoption and Disclosure Quality:\")\nprint(did_model.summary)\n\n\n\n\n\n\n\nNoteIdentification Concern\n\n\n\nVoluntary IFRS adoption is endogenous because firms that choose to adopt early may already have higher-quality disclosure. The two-way fixed effects DiD absorbs time-invariant firm characteristics and common time trends, but cannot fully address selection on time-varying unobservables. Researchers should consider matching estimators (e.g., propensity score matching on pre-adoption characteristics) or instrumental variable approaches as robustness checks.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-prediction",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-prediction",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.11 Predicting Late Filings",
    "text": "30.11 Predicting Late Filings\nCan we predict which firms will file late? This is valuable for portfolio construction (avoiding potential bad-news firms) and for regulators (targeting enforcement resources). We use a logistic model with financial and governance predictors:\n\\[\n\\Pr(\\text{Late}_{i,t} = 1) = \\Lambda\\left(\\alpha + \\boldsymbol{\\beta}'\\mathbf{X}_{i,t-1}\\right)\n\\tag{30.10}\\]\nwhere \\(\\Lambda(\\cdot)\\) is the logistic function and \\(\\mathbf{X}_{i,t-1}\\) are lagged predictors.\n\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\npred_panel = (\n    annual_filings[['ticker', 'fiscal_year', 'late_filing']]\n    .merge(financials, on=['ticker', 'fiscal_year'], how='left')\n    .merge(governance, on=['ticker', 'fiscal_year'], how='left')\n)\n\n# Lagged predictors\npred_panel = pred_panel.sort_values(['ticker', 'fiscal_year'])\nfor col in ['total_assets', 'net_income', 'operating_cash_flow',\n            'total_equity', 'revenue']:\n    pred_panel[f'{col}_lag'] = pred_panel.groupby('ticker')[col].shift(1)\n\npred_panel['log_size_lag'] = np.log(pred_panel['total_assets_lag'])\npred_panel['roa_lag'] = (\n    pred_panel['net_income_lag'] / pred_panel['total_assets_lag']\n)\npred_panel['leverage_lag'] = (\n    (pred_panel['total_assets_lag'] - pred_panel['total_equity_lag'])\n    / pred_panel['total_assets_lag']\n)\npred_panel['cfo_ratio_lag'] = (\n    pred_panel['operating_cash_flow_lag'] / pred_panel['total_assets_lag']\n)\n\n# Previous late filing indicator\npred_panel['prev_late'] = (\n    pred_panel.groupby('ticker')['late_filing'].shift(1)\n)\n\nfeatures = [\n    'log_size_lag', 'roa_lag', 'leverage_lag', 'cfo_ratio_lag',\n    'state_ownership_pct', 'foreign_ownership_pct',\n    'big4_auditor', 'board_independence_pct', 'prev_late'\n]\n\nclean = pred_panel.dropna(subset=features + ['late_filing'])\nX = clean[features]\ny = clean['late_filing']\n\n# Standardize\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Logistic regression with cross-validation\nlr = LogisticRegression(max_iter=1000, penalty='l2', C=1.0)\ncv_scores = cross_val_score(lr, X_scaled, y, cv=5, scoring='roc_auc')\n\nprint(f\"5-Fold Cross-Validated AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n\n# Fit on full sample for coefficient interpretation\nlr.fit(X_scaled, y)\ncoef_df = pd.DataFrame({\n    'Feature': features,\n    'Coefficient': lr.coef_[0],\n    'Odds Ratio': np.exp(lr.coef_[0])\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"\\nLogistic Regression Coefficients:\")\nprint(coef_df.to_string(index=False))\n\n\n\n\nfrom sklearn.metrics import roc_curve, auc\n\nlr.fit(X_scaled, y)\ny_prob = lr.predict_proba(X_scaled)[:, 1]\nfpr, tpr, _ = roc_curve(y, y_prob)\nroc_auc = auc(fpr, tpr)\n\nfig, ax = plt.subplots(figsize=(7, 7))\nax.plot(fpr, tpr, color='#2C5F8A', linewidth=2,\n        label=f'Logistic Model (AUC = {roc_auc:.3f})')\nax.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1)\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('Late Filing Prediction: ROC Curve')\nax.legend(loc='lower right')\nax.set_aspect('equal')\nplt.tight_layout()\nplt.show()\n\n\nFigure 30.8",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "37_disclosure_quality_and_timing.html#sec-dis-qual-summary",
    "href": "37_disclosure_quality_and_timing.html#sec-dis-qual-summary",
    "title": "30  Disclosure Quality and Timing",
    "section": "30.12 Summary",
    "text": "30.12 Summary\nThis chapter has examined corporate disclosure quality and timing in Vietnam along several dimensions. The key findings and methodological contributions are in Table 30.3\n\n\n\nTable 30.3: Summary of findings by theme.\n\n\n\n\n\n\n\n\n\n\nTheme\nKey Result\nReference\n\n\n\n\nGood news early\nEarly filers earn positive CARs around filing dates\nGivoly and Palmon (1982)\n\n\nTextual quality\nForward-looking density and numerical specificity vary substantially\nLi (2008)\n\n\nComposite DQ index\nForeign ownership and Big 4 auditors are strongest determinants\nBotosan (1997)\n\n\nCost of capital\nHigher DQ is associated with lower implied cost of equity\nDiamond and Verrecchia (1991)\n\n\nLiquidity\nHigher DQ firms have lower Amihud illiquidity\nLang, Lins, and Maffett (2012)\n\n\nInvestment efficiency\nHigher DQ reduces absolute investment residuals\nBiddle, Hilary, and Verdi (2009)\n\n\nStrategic timing\nEvidence of bad-news clustering on high-congestion days\nHirshleifer, Lim, and Teoh (2009)\n\n\nIFRS adoption\nPreliminary evidence of DQ improvement post-adoption\nBarth, Landsman, and Lang (2008)\n\n\n\n\n\n\nThe Vietnamese disclosure environment is shaped by a combination of regulatory mandates (Circular 155, Securities Law 2019), enforcement capacity (SSC penalties and trading suspensions), and firm-level incentives (ownership structure, auditor choice, governance quality). As Vietnam continues its IFRS convergence and capital market development, the information environment is expected to evolve, creating opportunities for researchers to study the dynamics of disclosure quality in a rapidly changing institutional setting.\n\n\n\n\n\n\n\nBall, Ray, Ashok Robin, and Joanna Shuang Wu. 2003. “Incentives Versus Standards: Properties of Accounting Income in Four East Asian Countries.” Journal of Accounting and Economics 36 (1-3): 235–70.\n\n\nBarth, Mary E, Wayne R Landsman, and Mark H Lang. 2008. “International Accounting Standards and Accounting Quality.” Journal of Accounting Research 46 (3): 467–98.\n\n\nBeyer, Anne, Daniel A Cohen, Thomas Z Lys, and Beverly R Walther. 2010. “The Financial Reporting Environment: Review of the Recent Literature.” Journal of Accounting and Economics 50 (2-3): 296–343.\n\n\nBiddle, Gary C, Gilles Hilary, and Rodrigo S Verdi. 2009. “How Does Financial Reporting Quality Relate to Investment Efficiency?” Journal of Accounting and Economics 48 (2-3): 112–31.\n\n\nBotosan, Christine A. 1997. “Disclosure Level and the Cost of Equity Capital.” Accounting Review, 323–49.\n\n\nBotosan, Christine A, and Marlene A Plumlee. 2002. “A Re-Examination of Disclosure Level and the Expected Cost of Equity Capital.” Journal of Accounting Research 40 (1): 21–40.\n\n\nBushman, Robert, Qi Chen, Ellen Engel, and Abbie Smith. 2004. “Financial Accounting Information, Organizational Complexity and Corporate Governance Systems.” Journal of Accounting and Economics 37 (2): 167–201.\n\n\nChambers, Anne E, and Stephen H Penman. 1984. “Timeliness of Reporting and the Stock Price Reaction to Earnings Announcements.” Journal of Accounting Research, 21–47.\n\n\nDechow, Patricia, Weili Ge, and Catherine Schrand. 2010. “Understanding Earnings Quality: A Review of the Proxies, Their Determinants and Their Consequences.” Journal of Accounting and Economics 50 (2-3): 344–401.\n\n\nDellaVigna, Stefano, and Joshua M Pollet. 2009. “Investor Inattention and Friday Earnings Announcements.” The Journal of Finance 64 (2): 709–49.\n\n\nDiamond, Douglas W. 1985. “Optimal Release of Information by Firms.” The Journal of Finance 40 (4): 1071–94.\n\n\nDiamond, Douglas W, and Robert E Verrecchia. 1991. “Disclosure, Liquidity, and the Cost of Capital.” The Journal of Finance 46 (4): 1325–59.\n\n\nDyer, Travis, Mark Lang, and Lorien Stice-Lawrence. 2017. “The Evolution of 10-k Textual Disclosure: Evidence from Latent Dirichlet Allocation.” Journal of Accounting and Economics 64 (2-3): 221–45.\n\n\nFrancis, Jennifer, Ryan LaFond, Per Olsson, and Katherine Schipper. 2005. “The Market Pricing of Accruals Quality.” Journal of Accounting and Economics 39 (2): 295–327.\n\n\nGivoly, Dan, and Dan Palmon. 1982. “Timeliness of Annual Earnings Announcements: Some Empirical Evidence.” Accounting Review, 486–508.\n\n\nGuay, Wayne, Delphine Samuels, and Daniel Taylor. 2016. “Guiding Through the Fog: Financial Statement Complexity and Voluntary Disclosure.” Journal of Accounting and Economics 62 (2-3): 234–69.\n\n\nHealy, Paul M, and Krishna G Palepu. 2001. “Information Asymmetry, Corporate Disclosure, and the Capital Markets: A Review of the Empirical Disclosure Literature.” Journal of Accounting and Economics 31 (1-3): 405–40.\n\n\nHirshleifer, David, Sonya Seongyeon Lim, and Siew Hong Teoh. 2009. “Driven to Distraction: Extraneous Events and Underreaction to Earnings News.” The Journal of Finance 64 (5): 2289–2325.\n\n\nHope, Ole-Kristian. 2003. “Disclosure Practices, Enforcement of Accounting Standards, and Analysts’ Forecast Accuracy: An International Study.” Journal of Accounting Research 41 (2): 235–72.\n\n\nLang, Mark, Karl V Lins, and Mark Maffett. 2012. “Transparency, Liquidity, and Valuation: International Evidence on When Transparency Matters Most.” Journal of Accounting Research 50 (3): 729–74.\n\n\nLang, Mark, and Russell Lundholm. 1993. “Cross-Sectional Determinants of Analyst Ratings of Corporate Disclosures.” Journal of Accounting Research 31 (2): 246–71.\n\n\nLeuz, Christian, Dhananjay Nanda, and Peter D Wysocki. 2003. “Earnings Management and Investor Protection: An International Comparison.” Journal of Financial Economics 69 (3): 505–27.\n\n\nLi, Feng. 2008. “Annual Report Readability, Current Earnings, and Earnings Persistence.” Journal of Accounting and Economics 45 (2-3): 221–47.\n\n\nLoughran, Tim, and Bill McDonald. 2014. “Measuring Readability in Financial Disclosures.” The Journal of Finance 69 (4): 1643–71.\n\n\nPatell, James M, and Mark A Wolfson. 1982. “Good News, Bad News, and the Intraday Timing of Corporate Disclosures.” Accounting Review, 509–27.\n\n\nSengupta, Partha. 1998. “Corporate Disclosure Quality and the Cost of Debt.” Accounting Review, 459–74.\n\n\nVerrecchia, Robert E. 1983. “Discretionary Disclosure.” Journal of Accounting and Economics 5: 179–94.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Disclosure Quality and Timing</span>"
    ]
  },
  {
    "objectID": "38_sue.html",
    "href": "38_sue.html",
    "title": "31  Standardized Earnings Surprises (SUE)",
    "section": "",
    "text": "31.1 Methodology\nIn the context of the Ho Chi Minh Stock Exchange (HOSE) and the Hanoi Stock Exchange (HNX), earnings announcements represent critical information events. Investors and quantitative analysts continuously monitor the deviation between reported earnings and market expectations. This deviation is quantified as the Standardized Earnings Surprise (SUE).\nThis chapter details the methodology for calculating SUE using three distinct approaches frequently utilized in academic literature and institutional research. We apply these methods to a dataset of Vietnamese large-cap equities to illustrate the mechanics of the calculation. The goal is to isolate the “surprise” component of earnings, which is a known predictor of post-earnings announcement drift (PEAD) (Bernard and Thomas 1989; Livnat and Mendenhall 2006).\nWe define three primary methods for calculating SUE. Each method differs in how it establishes the “expected” earnings value.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "38_sue.html#methodology",
    "href": "38_sue.html#methodology",
    "title": "31  Standardized Earnings Surprises (SUE)",
    "section": "",
    "text": "31.1.1 Method 1: Seasonal Random Walk\nThis method assumes that earnings follow a seasonal pattern. The best predictor for the current quarter’s earnings per share (EPS) is the EPS from the same quarter in the previous year. This controls for the seasonality often seen in Vietnamese sectors like retail and agriculture.\n\\[SUE_{1} = \\frac{EPS_{t} - EPS_{t-4}}{P_{t}}\\]\nWhere:\n\n\\(EPS_{t}\\) is the current quarterly Earnings Per Share.\n\\(EPS_{t-4}\\) is the Earnings Per Share from the same quarter of the prior fiscal year.\n\\(P_{t}\\) is the stock price at the end of the quarter (used as a deflator).\n\n\n\n31.1.2 Method 2: Exclusion of Special Items\nReported earnings often contain non-recurring items (e.g., asset sales, one-time write-offs) that distort the true operating performance. This method adjusts the reported EPS by removing the after-tax impact of special items.\nIn Vietnam, the standard Corporate Income Tax (CIT) rate is generally 20%. We adjust special items to reflect their impact on net income.\n\\[\nAdjusted \\ EPS = Reported \\ EPS - \\frac{Special \\ Items \\times (1 - CIT)}{Shares \\ Outstanding}\n\\]\nThe SUE calculation then follows the seasonal logic but uses the adjusted EPS figures: \\[SUE_{2} = \\frac{Adj \\ EPS_{t} - Adj \\ EPS_{t-4}}{P_{t}}\\]\n\n\n31.1.3 Method 3: Analyst Consensus\nThis method relies on market consensus rather than historical time series. It compares the actual reported earnings against the median analyst forecast provided prior to the announcement.\n\\[SUE_{3} = \\frac{Actual \\ EPS - Median \\ Estimate}{P_{t}}\\]",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "38_sue.html#data-description",
    "href": "38_sue.html#data-description",
    "title": "31  Standardized Earnings Surprises (SUE)",
    "section": "31.2 Data Description",
    "text": "31.2 Data Description\nFor this analysis, we utilize a dataset covering the fiscal years 2023 through 2025. The data includes quarterly financial statements and analyst consensus estimates for a selection of VN30 index constituents.\nThe dataset, vietnam_fin_data.csv, contains the following columns:\n\nticker: Stock symbol (e.g., VNM, VCB, HPG).\nfiscal_year: The financial year.\nfiscal_qtr: The financial quarter (1-4).\neps_basic: Basic Earnings Per Share (VND).\nprice_close: Closing price at quarter end (VND).\nspecial_items: Pre-tax special items value (VND millions). (i.e., is_other_profit in DataCore).\nshares_out: Shares outstanding (millions).\nanalyst_med: Median analyst EPS estimate (VND).\n\n\n31.2.1 Visualizing the Core Data\nBelow is a tabular representation of the raw data we have ingested for the analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nticker\nfiscal_year\nfiscal_qtr\neps_basic\nprice_close\nspecial_items\nshares_out\nanalyst_med\n\n\n\n\nVNM\n2023\n1\n1200\n68000\n0\n2090\n1150\n\n\nVNM\n2023\n2\n1350\n71000\n50000\n2090\n1300\n\n\nVNM\n2023\n3\n1400\n74000\n0\n2090\n1450\n\n\nVNM\n2023\n4\n1100\n69000\n-20000\n2090\n1150\n\n\nVNM\n2024\n1\n1300\n72000\n0\n2090\n1250\n\n\nVNM\n2024\n2\n1500\n75000\n0\n2090\n1400\n\n\nVCB\n2023\n1\n1800\n85000\n10000\n5500\n1700\n\n\nVCB\n2024\n1\n2100\n92000\n0\n5500\n2000",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "38_sue.html#implementation",
    "href": "38_sue.html#implementation",
    "title": "31  Standardized Earnings Surprises (SUE)",
    "section": "31.3 Implementation",
    "text": "31.3 Implementation\n\n31.3.1 Python Setup and Data Loading\nFirst, we establish our environment and load the dataset. We ensure the data is sorted by ticker and time to allow for accurate lag calculations.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Creating the dataset directly for this chapter's demonstration\ndata = {\n    'ticker': ['VNM']*8 + ['VCB']*8 + ['HPG']*8,\n    'fiscal_year': [2023, 2023, 2023, 2023, 2024, 2024, 2024, 2024] * 3,\n    'fiscal_qtr': [1, 2, 3, 4, 1, 2, 3, 4] * 3,\n    'eps_basic': [\n        1200, 1350, 1400, 1100, 1300, 1500, 1450, 1250, # VNM\n        1800, 1900, 2000, 2200, 2100, 2300, 2400, 2600, # VCB\n        500, 600, 550, 400, 700, 800, 750, 600          # HPG\n    ],\n    'price_close': [\n        68000, 71000, 74000, 69000, 72000, 75000, 73000, 70000, # VNM\n        85000, 88000, 90000, 95000, 92000, 96000, 98000, 102000, # VCB\n        20000, 22000, 21000, 19000, 25000, 28000, 27000, 24000 # HPG\n    ],\n    'special_items': [\n        0, 50000, 0, -20000, 0, 0, 10000, 0, # VNM (VND Millions)\n        10000, 0, 0, 50000, 0, 20000, 0, 0, # VCB\n        0, 0, -50000, 0, 100000, 0, 0, 0 # HPG\n    ],\n    'shares_out': [2090]*8 + [5580]*8 + [5810]*8, # In Millions\n    'analyst_med': [\n        1150, 1300, 1450, 1150, 1250, 1400, 1480, 1200, # VNM\n        1700, 1850, 1950, 2150, 2000, 2250, 2450, 2550, # VCB\n        450, 550, 600, 450, 650, 750, 800, 650 # HPG\n    ]\n}\n\ndf = pd.DataFrame(data)\n\n# Sort strictly to ensure shift operations work on correct temporal sequence\ndf = df.sort_values(by=['ticker', 'fiscal_year', 'fiscal_qtr'])\nprint(df.head())\n\n   ticker  fiscal_year  fiscal_qtr  eps_basic  price_close  special_items  \\\n16    HPG         2023           1        500        20000              0   \n17    HPG         2023           2        600        22000              0   \n18    HPG         2023           3        550        21000         -50000   \n19    HPG         2023           4        400        19000              0   \n20    HPG         2024           1        700        25000         100000   \n\n    shares_out  analyst_med  \n16        5810          450  \n17        5810          550  \n18        5810          600  \n19        5810          450  \n20        5810          650  \n\n\n\n\n31.3.2 Calculation Logic\nWe now apply the functions to calculate the three variations of SUE.\nStep 1: Handling Seasonality (Lags)\nFor Methods 1 and 2, we require the data from the same quarter of the previous year (lag 4).\n\n# Group by ticker to ensure we don't shift data between companies\ndf['eps_lag4'] = df.groupby('ticker')['eps_basic'].shift(4)\n\nStep 2: Adjusting for Special Items\nFor Method 2, we must strip out non-recurring items. We apply the Vietnamese Corporate Income Tax (CIT) rate of 20%.\nThe formula for the adjustment per share is: \\[ \\text{Adjustment} = \\frac{\\text{Special Items} \\times (1 - 0.20)}{\\text{Shares Outstanding}} \\]\n\n# Constants\nCIT_RATE_VN = 0.20\n\n# Calculate impact per share\n# Note: special_items are in millions, shares_out are in millions\n# The units cancel out, leaving the result in VND per share.\ndf['spi_impact_per_share'] = (df['special_items'] * (1 - CIT_RATE_VN)) / df['shares_out']\n\n# Calculate Adjusted EPS\ndf['eps_adjusted'] = df['eps_basic'] - df['spi_impact_per_share']\n\n# Create lag for Adjusted EPS\ndf['eps_adj_lag4'] = df.groupby('ticker')['eps_adjusted'].shift(4)\n\nStep 3: Computing SUE Variants\nWe finalize the calculation by computing the difference between actual (or adjusted) and expected values, deflated by the stock price.\n\n# Method 1: Seasonal Random Walk (Standard EPS)\ndf['sue_1'] = (df['eps_basic'] - df['eps_lag4']) / df['price_close']\n\n# Method 2: Seasonal Random Walk (Excluding Special Items)\ndf['sue_2'] = (df['eps_adjusted'] - df['eps_adj_lag4']) / df['price_close']\n\n# Method 3: Analyst Forecasts (IBES Equivalent)\ndf['sue_3'] = (df['eps_basic'] - df['analyst_med']) / df['price_close']\n\n# Scaling for readability (converting to percentage)\ndf['sue_1_pct'] = df['sue_1'] * 100\ndf['sue_2_pct'] = df['sue_2'] * 100\ndf['sue_3_pct'] = df['sue_3'] * 100",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "38_sue.html#results-and-analysis",
    "href": "38_sue.html#results-and-analysis",
    "title": "31  Standardized Earnings Surprises (SUE)",
    "section": "31.4 Results and Analysis",
    "text": "31.4 Results and Analysis\nWe present the calculated standardized earnings surprises for the fiscal year 2024. Positive values indicate a positive surprise (beating expectations), while negative values indicate a miss.\n\n31.4.1 Tabular Results (FY 2024)\n\n# Filter for 2024 results where lag data exists\nresults_2024 = df[df['fiscal_year'] == 2024][['ticker', 'fiscal_qtr', 'sue_1_pct', 'sue_2_pct', 'sue_3_pct']]\n\n# Display formatted table\nfrom IPython.display import display, Markdown\nmarkdown_table = results_2024.to_markdown(index=False, floatfmt=\".4f\")\ndisplay(Markdown(markdown_table))\n\n\n\n\nticker\nfiscal_qtr\nsue_1_pct\nsue_2_pct\nsue_3_pct\n\n\n\n\nHPG\n1\n0.8000\n0.7449\n0.2000\n\n\nHPG\n2\n0.7143\n0.7143\n0.1786\n\n\nHPG\n3\n0.7407\n0.7152\n-0.1852\n\n\nHPG\n4\n0.8333\n0.8333\n-0.2083\n\n\nVCB\n1\n0.3261\n0.3276\n0.1087\n\n\nVCB\n2\n0.4167\n0.4137\n0.0521\n\n\nVCB\n3\n0.4082\n0.4082\n-0.0510\n\n\nVCB\n4\n0.3922\n0.3992\n0.0490\n\n\nVNM\n1\n0.1389\n0.1389\n0.0694\n\n\nVNM\n2\n0.2000\n0.2255\n0.1333\n\n\nVNM\n3\n0.0685\n0.0632\n-0.0411\n\n\nVNM\n4\n0.2143\n0.2033\n0.0714\n\n\n\n\n\n\n\n\n\n31.4.2 Visualization\nThe following figure plots the Analyst-based SUE (Method 3) for the selected tickers over the 2024 fiscal year.\n\npivot_sue = results_2024.pivot(index='fiscal_qtr', columns='ticker', values='sue_3_pct')\n\nplt.figure(figsize=(10, 6))\nfor column in pivot_sue.columns:\n    plt.plot(pivot_sue.index, pivot_sue[column], marker='o', label=column)\n\nplt.title('Method 3: Analyst Based SUE (FY 2024)')\nplt.xlabel('Fiscal Quarter')\nplt.ylabel('SUE (%)')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8)\nplt.legend(title='Ticker')\nplt.grid(True, linestyle=':', alpha=0.6)\nplt.xticks([1, 2, 3, 4])\nplt.show()\n\n\n\n\n\n\n\nFigure 31.1",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "38_sue.html#conclusion",
    "href": "38_sue.html#conclusion",
    "title": "31  Standardized Earnings Surprises (SUE)",
    "section": "31.5 Conclusion",
    "text": "31.5 Conclusion\nIn this chapter, we have formalized the calculation of Standardized Earnings Surprises for the Vietnamese market. We demonstrated that relying solely on raw EPS growth (Method 1) can be misleading in the presence of non-recurring items. Furthermore, analyst-based surprises (Method 3) often provide a cleaner signal of new information reaching the market.\nFor robust quantitative modeling in Vietnam, we recommend using Method 2 when analyst data is sparse (common in small-cap stocks) and Method 3 for VN30 constituents where analyst coverage is deep and liquid.\n\n\n\n\n\n\nBernard, Victor L, and Jacob K Thomas. 1989. “Post-Earnings-Announcement Drift: Delayed Price Response or Risk Premium?” Journal of Accounting Research 27: 1–36.\n\n\nLivnat, Joshua, and Richard R Mendenhall. 2006. “Comparing the Post–Earnings Announcement Drift for Surprises Calculated from Analyst and Time Series Forecasts.” Journal of Accounting Research 44 (1): 177–205.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Standardized Earnings Surprises (SUE)</span>"
    ]
  },
  {
    "objectID": "39_divop.html",
    "href": "39_divop.html",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "",
    "text": "33 Theoretical Framework\nA foundational question in financial economics concerns how differences in investor beliefs affect asset prices and trading activity. In markets where investors hold heterogeneous expectations about a firm’s future cash flows, the aggregation of these divergent views into a single market price becomes a non-trivial exercise with profound implications for asset valuation, return predictability, and market efficiency. The concept of divergence of investor opinion (hereafter DIVOP) has emerged as a central construct in both the accounting and finance literatures, serving as a lens through which researchers examine the information environment of firms, the dynamics of uncertainty resolution, and the nature of market reactions to news.\nThe theoretical foundations of the DIVOP literature trace back to Miller (1977), who proposed that when investors disagree about the value of a security and short-sale constraints prevent pessimistic investors from fully expressing their views, the market price will reflect the valuation of the most optimistic investors. This leads to systematic overpricing that is increasing in the degree of opinion divergence. The overpricing persists until information events, such as earnings announcements, reduce disagreement and prices converge toward fundamental values (Berkman et al. 2009). Varian (1985) offers an alternative perspective in which divergence of opinion represents an additional risk factor, leading to higher rather than lower expected returns, creating a theoretical tension that has motivated extensive empirical investigation.\nThe empirical literature on DIVOP has expanded considerably since these seminal contributions. Researchers have documented that divergence of opinion helps explain a range of asset pricing anomalies, including post-earnings announcement drift (Garfinkel and Sokobin 2006; K. L. Anderson, Harris, and So 2007), the cross-sectional return difference between value and growth stocks (Doukas, Kim, and Pantzalis 2004), short- and long-run post-IPO returns (Houge et al. 2001), pre- and post-acquisition stock returns (Alexandridis, Antoniou, and Petmezas 2007), takeover premia (Chatterjee, John, and Yan 2012), and the broad cross-section of stock returns (Diether, Malloy, and Scherbina 2002; Doukas, Kim, and Pantzalis 2006). The explanatory power of DIVOP has been demonstrated using a rich set of empirical proxies, ranging from analyst forecast dispersion and abnormal trading volume to bid-ask spreads and idiosyncratic volatility.\nDespite the maturity of the DIVOP literature in developed markets, particularly the United States, its application to emerging markets remains remarkably thin. This gap is especially notable given that the theoretical conditions under which divergence of opinion matters most (namely, binding short-sale constraints, information asymmetry, and heterogeneous investor sophistication) are arguably more prevalent in emerging markets than in their developed counterparts. The Vietnamese equity market presents a compelling laboratory for studying investor disagreement. The market is characterized by several features that amplify the relevance of the DIVOP framework:\nThis chapter provides a methodology for constructing multiple proxies for divergence of investor opinion adapted to the institutional characteristics of the Vietnamese market. We draw on the methodological frameworks established by Garfinkel (2009) and Diether, Malloy, and Scherbina (2002), while introducing modifications that account for the microstructure of Vietnamese exchanges, the \\(T+2\\) settlement cycle, the absence (until recently) of short selling, and the availability of data through domestic financial platforms. Specifically, we construct and analyze the following DIVOP proxies:\nFor each proxy, we describe the theoretical motivation, the data requirements, the construction methodology adapted for Vietnamese data, the empirical properties observed in the Vietnamese cross-section, and the practical considerations that researchers should bear in mind when employing these measures. We pay particular attention to issues that are specific to emerging markets, including thin trading, corporate action adjustments, exchange-specific microstructure effects, and the interplay between foreign ownership constraints and measures of investor disagreement.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#the-miller-1977-overpricing-hypothesis",
    "href": "39_divop.html#the-miller-1977-overpricing-hypothesis",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "33.1 The Miller (1977) Overpricing Hypothesis",
    "text": "33.1 The Miller (1977) Overpricing Hypothesis\nThe canonical model of divergence of opinion and asset pricing begins with Miller (1977). Miller’s central insight is simple: in a market where investors hold heterogeneous beliefs about the future payoffs of a risky asset and short-sale constraints prevent some investors from acting on their pessimistic views, the equilibrium price will be set by the subset of investors who are most optimistic about the asset’s value. The severity of overpricing is increasing in both the degree of opinion divergence and the stringency of short-sale constraints. Formally, if investor \\(i\\) assigns a valuation \\(V_i\\) to a security, the market price \\(P\\) satisfies:\n\\[\nP = E[V_i \\mid V_i \\geq V^*]\n\\]\nwhere \\(V^*\\) is the marginal investor’s valuation, which exceeds the unconditional mean valuation \\(E[V_i]\\) whenever short-sale constraints bind for some investors. The degree of overpricing is:\n\\[\n\\text{Overpricing} = P - E[V_i] = E[V_i \\mid V_i \\geq V^*] - E[V_i]\n\\]\nwhich is positive and increasing in the dispersion of the distribution of \\(V_i\\) (i.e., divergence of opinion) and in \\(V^*\\) (i.e., the severity of short-sale constraints).\nMiller’s model generates several testable predictions:\n\nCross-sectional prediction: Stocks with higher divergence of opinion should have lower subsequent returns as prices gradually correct toward fundamental values.\nTime-series prediction: Information events that reduce disagreement (e.g., earnings announcements) should be associated with negative abnormal returns for high-DIVOP stocks, as the “optimism premium” dissipates.\nInteraction prediction: The overpricing effect should be strongest among stocks that simultaneously exhibit high divergence of opinion and binding short-sale constraints.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#alternative-theoretical-perspectives",
    "href": "39_divop.html#alternative-theoretical-perspectives",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "33.2 Alternative Theoretical Perspectives",
    "text": "33.2 Alternative Theoretical Perspectives\nVarian (1985) proposes an alternative framework in which divergence of opinion acts as a risk factor. If investors are risk-averse and disagreement represents genuine uncertainty about future payoffs, then higher dispersion of beliefs should be associated with higher expected returns as compensation for bearing the additional risk. This creates a sharp empirical dichotomy: the Miller hypothesis predicts a negative DIVOP-return relation, whereas the Varian model predicts a positive relation.\nThe distinction between these theories hinges critically on the market microstructure and institutional setting (@tbl-divop-theories).\n\n\n\nTable 33.1: Summary of theoretical predictions for the DIVOP-return relation under different assumptions\n\n\n\n\n\n\n\n\n\n\n\nTheoretical Framework\nShort-Sale Constraints\nDIVOP-Return Relation\nKey Mechanism\n\n\n\n\nMiller (1977)\nBinding\nNegative\nOptimistic bias in price\n\n\nVarian (1985)\nNon-binding\nPositive\nRisk premium for uncertainty\n\n\nHong and Stein (2003)\nBinding, gradual info\nNegative, time-varying\nSlow diffusion of bearish views\n\n\nScheinkman and Xiong (2003)\nBinding, overconfidence\nNegative\nSpeculative bubble premium\n\n\n\n\n\n\nHong and Stein (2003) extend Miller’s framework by incorporating gradual information diffusion. In their model, bearish information is impounded into prices more slowly than bullish information because short-sale constraints raise the cost of acting on negative views. This generates momentum-like patterns in which high-DIVOP stocks exhibit positive short-run returns (as optimists push prices up) followed by negative long-run returns (as bearish information eventually reaches the market).\nScheinkman and Xiong (2003) introduce an additional dimension by noting that when investors are overconfident about their private signals and short-sale constraints bind, stock prices contain a “speculative bubble” component that reflects the option value of reselling the asset to a future investor who may be even more optimistic. This model predicts that both high trading volume and high price volatility should be associated with overpricing, providing a theoretical basis for using volume-based and volatility-based DIVOP proxies.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#relevance-to-the-vietnamese-market",
    "href": "39_divop.html#relevance-to-the-vietnamese-market",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "33.3 Relevance to the Vietnamese Market",
    "text": "33.3 Relevance to the Vietnamese Market\nThe Vietnamese equity market provides an unusually clean setting for testing the Miller hypothesis. Vietnam’s equity market operated without any short-selling mechanism from its inception in 2000 through January 2025, which was a full quarter-century in which the first necessary condition of Miller’s model (binding short-sale constraints) was satisfied by regulation rather than by market frictions. Even after the introduction of covered short selling in 2025, the mechanism remains restricted to securities meeting specific liquidity and market capitalization thresholds, and the regulatory environment imposes borrowing requirements that significantly raise the cost of shorting relative to developed markets.\nThe dominance of retail investors amplifies the second necessary condition (i.e., heterogeneous beliefs). Research on the Vietnamese market has documented significant herding behavior (Vo and Phan 2017; Vo 2015), sentiment-driven trading (Phan et al. 2023; Nguyen and Pham 2018), and information asymmetry between domestic and foreign investors (Vo 2017). These behavioral characteristics naturally generate wider dispersion of investor valuations compared to markets dominated by institutional investors with access to similar analytical frameworks and information sources.\nTable 33.2 compares key institutional features relevant to the DIVOP framework between Vietnam and the United States.\n\n\n\nTable 33.2: Institutional comparison of Vietnam and the United States relevant to divergence of opinion\n\n\n\n\n\n\n\n\n\n\nFeature\nVietnam (HOSE/HNX)\nUnited States (NYSE/NASDAQ)\n\n\n\n\nShort selling\nIntroduced Jan 2025 (limited)\nPermitted (Reg SHO since 2005)\n\n\nRetail investor share of volume\n~80-85%\n~25%\n\n\nSettlement cycle\nT+2 (T+1 planned for 2026)\nT+1 (since May 2024)\n\n\nDaily price limits\n\\(\\pm\\) 7% (HOSE), \\(\\pm\\) 10% (HNX)\nNone\n\n\nForeign ownership cap\n49% (most sectors)\nNone\n\n\nAverage analyst coverage (VN30)\n5-10 analysts\n15-25 analysts\n\n\nMandatory quarterly reporting\nYes (since 2012)\nYes\n\n\nOptions/derivatives market\nVN30 Index Futures (since 2017)\nExtensive options/futures\n\n\n\n\n\n\nThe presence of daily price limits (\\(\\pm\\) 7% on HOSE and \\(\\pm\\) 10% on HNX) creates an additional mechanism through which divergence of opinion can be amplified. When a stock hits its price limit, investors who wish to trade in the direction of the limit are unable to do so, leading to accumulated unfilled orders and delayed price discovery. This institutional feature may create short-term spikes in measured DIVOP that reflect limit-induced friction rather than genuine disagreement. We address this issue in our empirical methodology by flagging limit-hit days and conducting robustness checks that exclude these observations.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#data-sources",
    "href": "39_divop.html#data-sources",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "34.1 Data Sources",
    "text": "34.1 Data Sources\nThe construction of DIVOP proxies for the Vietnamese market requires daily stock-level trading data and, for the analyst dispersion measures, individual analyst forecast data. We source all data from DataCore.vn, which provides coverage of all securities listed on HOSE, HNX, and the UPCoM (Unlisted Public Company Market) exchange. Table 34.1 summarizes the datasets and key variables used in this study.\n\n\n\nTable 34.1: Data sources and key variables for DIVOP proxy construction\n\n\n\n\n\n\n\n\n\n\nDataset\nKey Variables\nFrequency\n\n\n\n\nDaily Stock Trading\nClose price, high, low, open, volume, shares outstanding, adjusted price, bid, ask\nDaily\n\n\nCorporate Actions\nDividends, stock splits, bonus issues, rights offerings\nEvent-based\n\n\nCompany Information\nExchange code, industry classification (ICB), listing date, delisting date\nStatic/Periodic\n\n\nAnalyst Forecasts\nIndividual analyst EPS forecasts, announcement dates, fiscal period end, analyst ID, broker name\nPer estimate\n\n\nMarket Index\nVN-Index daily returns, VN30 returns, HNX-Index returns\nDaily\n\n\nForeign Ownership\nForeign buy/sell volume, foreign ownership percentage, remaining foreign room\nDaily",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#sample-construction",
    "href": "39_divop.html#sample-construction",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "34.2 Sample Construction",
    "text": "34.2 Sample Construction\nWe construct our sample using the following filters, applied sequentially:\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom sklearn.linear_model import LinearRegression\nfrom scipy import stats as scipy_stats\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# =============================================================================\n# Configuration Parameters\n# =============================================================================\n# Users can modify these parameters to adjust the methodology\nCONFIG = {\n    # Sample period\n    'beg_date': '2007-01-01',\n    'end_date': '2024-12-31',\n    \n    # Estimation windows (in trading days)\n    'est_window': 60,          # Rolling window for SUV and volatility\n    'detrend_window': 180,     # Window for DTO detrending median\n    'lag': 7,                  # Lag for DTO detrending\n    'gap': 5,                  # Gap between estimation period and event date\n    \n    # Filters\n    'min_price': 1000,         # Minimum price in VND\n    'min_volume_days': 0.8,    # Min fraction of non-zero volume days in window\n    'min_analysts': 3,         # Minimum number of analysts for DISP\n    'max_spread_pct': 0.50,    # Maximum bid-ask spread as fraction of midpoint\n    'forecast_carry_days': 105,# Days to carry forward stale analyst forecasts\n    \n    # Exchange identifiers\n    'exchanges': ['HOSE', 'HNX'],\n    \n    # Price limit thresholds (for flagging)\n    'price_limit_hose': 0.07,\n    'price_limit_hnx': 0.10,\n}\n\nprint(\"Configuration parameters loaded successfully.\")\nprint(f\"Sample period: {CONFIG['beg_date']} to {CONFIG['end_date']}\")\nprint(f\"Estimation window: {CONFIG['est_window']} trading days\")\nprint(f\"Detrending window: {CONFIG['detrend_window']} trading days\")\n\nConfiguration parameters loaded successfully.\nSample period: 2007-01-01 to 2024-12-31\nEstimation window: 60 trading days\nDetrending window: 180 trading days\n\n\nThe sample universe includes all common stocks (ordinary shares) listed on HOSE and HNX during the period January 2007 through December 2024. We begin in 2007 rather than at market inception (2000 for HOSE, 2005 for HNX) for two reasons. First, the early years of the Vietnamese market were characterized by an extremely small number of listed firms (fewer than 30 on HOSE through 2005), making cross-sectional analysis unreliable. Second, data quality and consistency improve substantially after the market expansion of 2006-2007, during which the number of listed firms on HOSE grew from approximately 40 to over 100.\nWe apply the following filters to construct the analysis sample:\n\nSecurity type filter. We retain only common stocks (ordinary shares), excluding preferred shares, exchange-traded funds (ETFs), covered warrants, and certificates of deposit. This is analogous to the standard filter in the U.S. literature that restricts to CRSP share codes 10 and 11.\nExchange filter. We include stocks listed on HOSE and HNX but exclude UPCoM securities in our baseline analysis. UPCoM is a registration-based trading venue with less stringent listing requirements and substantially lower liquidity, which may introduce noise into volume-based and spread-based measures. We include UPCoM in robustness checks.\nPrice filter. We exclude stock-day observations with closing prices below 1,000 VND. This threshold serves the same purpose as the “penny stock” exclusion common in U.S. studies (typically $1 or $5 thresholds) and helps mitigate the influence of extreme percentage returns and spreads at very low price levels.\nMinimum trading activity. For volume-based measures, we require that a stock has non-zero trading volume on at least 80% of trading days within each estimation window. This filter eliminates the most thinly traded securities for which turnover-based measures would be unreliable.\n\n\ndef load_daily_data(config):\n    \"\"\"\n    Load daily stock trading data from DataCore.vn.\n    \n    In practice, this function connects to the DataCore API or reads\n    from a local database/CSV. Here we document the expected schema.\n    \n    Expected columns:\n    - ticker: str, stock ticker symbol (e.g., 'VCB', 'HPG', 'VNM')\n    - date: datetime, trading date\n    - open, high, low, close: float, daily OHLC prices (VND)\n    - volume: int, trading volume (shares)\n    - shares_outstanding: int, total shares outstanding\n    - adjusted_close: float, price adjusted for corporate actions\n    - adj_factor: float, cumulative adjustment factor\n    - bid, ask: float, best bid/ask at close\n    - exchange: str, exchange code ('HOSE', 'HNX', 'UPCOM')\n    - industry_icb: str, ICB industry classification code\n    - foreign_buy_vol, foreign_sell_vol: int, foreign investor volumes\n    - foreign_ownership_pct: float, foreign ownership percentage\n    \"\"\"\n    # =========================================================================\n    # Replace with actual DataCore API call:\n    # from datacore import Client\n    # client = Client(api_key='YOUR_KEY')\n    # df = client.daily_stock(\n    #     start=config['beg_date'], end=config['end_date'],\n    #     exchanges=config['exchanges']\n    # )\n    # =========================================================================\n    print(\"Connect to DataCore.vn and load daily stock data.\")\n    print(\"Expected schema: ticker, date, open, high, low, close, volume,\")\n    print(\"  shares_outstanding, adjusted_close, adj_factor, bid, ask,\")\n    print(\"  exchange, industry_icb, foreign_buy_vol, foreign_sell_vol,\")\n    print(\"  foreign_ownership_pct\")\n    return None  # Replace with actual data\n\n\ndef apply_sample_filters(df, config):\n    \"\"\"Apply sequential sample construction filters.\"\"\"\n    print(\"\\n=== Sample Construction ===\")\n    n0 = len(df)\n    \n    # Date filter\n    df = df[(df['date'] &gt;= config['beg_date']) &\n            (df['date'] &lt;= config['end_date'])].copy()\n    print(f\"[1] Date filter: {len(df):,} obs (from {n0:,})\")\n    \n    # Exchange filter\n    df = df[df['exchange'].isin(config['exchanges'])].copy()\n    print(f\"[2] Exchange filter ({config['exchanges']}): {len(df):,} obs\")\n    \n    # Price filter\n    df = df[df['close'] &gt;= config['min_price']].copy()\n    print(f\"[3] Price &gt;= {config['min_price']:,} VND: {len(df):,} obs\")\n    \n    # Compute daily return from adjusted prices\n    df = df.sort_values(['ticker', 'date'])\n    df['ret'] = df.groupby('ticker')['adjusted_close'].pct_change()\n    \n    # Flag price limit hits\n    df['limit_hit'] = (\n        ((df['exchange'] == 'HOSE') &\n         (df['ret'].abs() &gt;= config['price_limit_hose'] - 0.001)) |\n        ((df['exchange'] == 'HNX') &\n         (df['ret'].abs() &gt;= config['price_limit_hnx'] - 0.001))\n    )\n    \n    n_tickers = df['ticker'].nunique()\n    print(f\"\\nFinal sample: {len(df):,} stock-day obs, \"\n          f\"{n_tickers} unique tickers\")\n    print(f\"Limit-hit days: {df['limit_hit'].sum():,} \"\n          f\"({100*df['limit_hit'].mean():.2f}%)\")\n    return df",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#sec-corp-actions",
    "href": "39_divop.html#sec-corp-actions",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "34.3 Corporate Action Adjustments",
    "text": "34.3 Corporate Action Adjustments\nProper adjustment for corporate actions is critical for volume-based DIVOP measures, as events such as stock splits, bonus share issues, and rights offerings change the number of shares outstanding and can create artificial spikes in measured turnover. We need to use cumulative adjustment factors that account for stock dividends (bonus shares), stock splits, rights offerings, and cash dividends (price adjustment only). We use these to construct adjusted volume and adjusted shares outstanding:\n\\[\n\\text{AdjVolume}_{i,t} = \\text{Volume}_{i,t} \\times \\text{CumAdjFactor}_{i,t}\n\\]\n\\[\n\\text{AdjSharesOut}_{i,t} = \\text{SharesOut}_{i,t} \\times \\text{CumAdjFactor}_{i,t}\n\\]\nThis ensures that the turnover ratio is consistent across corporate action events.\n\ndef adjust_for_corporate_actions(df):\n    \"\"\"Apply cumulative adjustment factors to volume and shares outstanding.\"\"\"\n    df = df.copy()\n    df['adj_volume'] = df['volume'] * df['adj_factor']\n    df['adj_shares_out'] = df['shares_outstanding'] * df['adj_factor']\n    \n    # Daily turnover ratio\n    df['turnover'] = np.where(\n        df['adj_shares_out'] &gt; 0,\n        df['adj_volume'] / df['adj_shares_out'],\n        np.nan\n    )\n    \n    # Flag extreme turnover (&gt; 50% of float)\n    extreme = df['turnover'] &gt; 0.50\n    if extreme.any():\n        print(f\"Warning: {extreme.sum()} obs with turnover &gt; 50%, set to NaN\")\n        df.loc[extreme, 'turnover'] = np.nan\n    \n    return df",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#sec-calendar",
    "href": "39_divop.html#sec-calendar",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "34.4 Trading Calendar Construction",
    "text": "34.4 Trading Calendar Construction\nThe rolling regression approach for SUV and volatility requires a trading calendar that ensures each estimation window contains exactly the specified number of trading days. We construct this directly from observed trading dates.\n\ndef build_trading_calendar(df, config):\n    \"\"\"\n    Map each trading date to its estimation window [est_start, est_end].\n    \n    For date t, the estimation window runs from\n    t - gap - est_window to t - gap - 1 (in trading-day terms).\n    \"\"\"\n    trading_dates = sorted(df['date'].unique())\n    trading_dates = pd.Series(trading_dates)\n    \n    est_window = config['est_window']\n    gap = config['gap']\n    offset = est_window + gap\n    \n    records = []\n    for i in range(offset, len(trading_dates)):\n        records.append({\n            'date': trading_dates.iloc[i],\n            'est_start': trading_dates.iloc[i - gap - est_window],\n            'est_end': trading_dates.iloc[i - gap - 1]\n        })\n    \n    calendar = pd.DataFrame(records)\n    print(f\"Trading calendar: {len(calendar)} dates, \"\n          f\"{calendar['date'].min()} to {calendar['date'].max()}\")\n    return calendar",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#theoretical-motivation",
    "href": "39_divop.html#theoretical-motivation",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "35.1 Theoretical Motivation",
    "text": "35.1 Theoretical Motivation\nTrading volume has long been recognized as a natural proxy for divergence of investor opinion. In the rational expectations framework of Milgrom and Stokey (1982), trade occurs only when investors disagree about the value of a security (i.e., a “no-trade theorem” that implies, by contrapositive, that observed trading volume must reflect some form of heterogeneous beliefs). Harris and Raviv (1993) and Kandel and Pearson (1995) formalize this intuition, showing that trading volume is positively related to the dispersion of investors’ prior beliefs and to the degree to which public information is differentially interpreted.\nThe challenge in using raw trading volume as a DIVOP proxy is that volume is also driven by factors unrelated to disagreement, including portfolio rebalancing, liquidity needs, tax-loss selling, and index reconstitution effects. Garfinkel (2009) proposes two approaches to extract the disagreement component from raw volume. The first, Unexplained Volume (DTO), removes market-wide volume effects and secular trends. The second, Standardized Unexplained Volume (SUV), additionally controls for the information content of returns through a cross-sectional regression, isolating the “pure disagreement” component of trading activity.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#sec-dto",
    "href": "39_divop.html#sec-dto",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "35.2 Unexplained Volume (DTO)",
    "text": "35.2 Unexplained Volume (DTO)\n\n35.2.1 Construction Methodology\nThe construction of the Unexplained Volume measure proceeds in four steps.\nStep 1: Compute firm-level daily turnover. For each stock \\(i\\) on day \\(t\\):\n\\[\n\\text{Turn}_{i,t} = \\frac{\\text{AdjVolume}_{i,t}}{\\text{AdjSharesOut}_{i,t}}\n\\]\nStep 2: Compute market-wide turnover. We calculate aggregate turnover across all common stocks as a value-weighted average:\n\\[\n\\text{MktTurn}_{t} = \\frac{\\sum_{i} \\text{AdjVolume}_{i,t}}{\\sum_{i} \\text{AdjSharesOut}_{i,t}}\n\\]\nUnlike the U.S. methodology that computes market turnover across NYSE/AMEX stocks only and applies a scaling adjustment for NASDAQ securities (following A.-M. Anderson and Dyl 2005), we compute market turnover across all HOSE and HNX common stocks without any exchange-specific volume scaling. Both Vietnamese exchanges operate as order-driven markets (HOSE uses continuous order matching; HNX uses a combination of continuous matching and periodic call auctions) without the dealer-market double-counting issue that necessitates the NASDAQ volume adjustment in U.S. studies.\nStep 3: Compute market-adjusted turnover.\n\\[\n\\text{MATO}_{i,t} = \\text{Turn}_{i,t} - \\text{MktTurn}_{t}\n\\]\nStep 4: Detrend by rolling median. To remove secular trends in firm-specific trading activity:\n\\[\n\\text{DTO}_{i,t} = \\text{MATO}_{i,t} - \\text{Median}_{180}(\\text{MATO}_{i,t-7})\n\\]\nwhere \\(\\text{Median}_{180}(\\text{MATO}_{i,t-7})\\) is the median of market-adjusted turnover over the 180-trading-day window ending 7 days before date \\(t\\). The 7-day lag prevents the current day’s turnover from influencing its own detrending baseline.\n\ndef compute_market_turnover(df):\n    \"\"\"Compute daily market-wide turnover across all stocks.\"\"\"\n    mkt_turn = df.groupby('date').apply(\n        lambda x: x['adj_volume'].sum() / x['adj_shares_out'].sum()\n        if x['adj_shares_out'].sum() &gt; 0 else np.nan\n    ).reset_index()\n    mkt_turn.columns = ['date', 'market_turnover']\n    return mkt_turn\n\n\ndef compute_dto(df, config):\n    \"\"\"\n    Construct Unexplained Volume (DTO).\n    \n    Steps:\n    1. Subtract market turnover -&gt; MATO\n    2. Rolling 180-day median of MATO (lagged 7 days) -&gt; trend\n    3. DTO = MATO - trend\n    \"\"\"\n    detrend_window = config['detrend_window']\n    lag = config['lag']\n    \n    # Market turnover\n    mkt_turn = compute_market_turnover(df)\n    df = df.merge(mkt_turn, on='date', how='left')\n    \n    # Market-adjusted turnover\n    df['mato'] = df['turnover'] - df['market_turnover']\n    \n    # Rolling median with lag, computed per stock\n    df = df.sort_values(['ticker', 'date'])\n    \n    def _rolling_median_lagged(group):\n        mato = group['mato']\n        med = mato.rolling(\n            window=detrend_window,\n            min_periods=int(detrend_window * 0.5)\n        ).median()\n        return med.shift(lag)\n    \n    df['mato_trend'] = (\n        df.groupby('ticker', group_keys=False)\n          .apply(lambda g: _rolling_median_lagged(g))\n    )\n    \n    # DTO\n    df['dto'] = df['mato'] - df['mato_trend']\n    \n    print(\"DTO construction complete.\")\n    print(f\"  Non-missing: {df['dto'].notna().sum():,}\")\n    print(f\"  Mean: {df['dto'].mean():.6f}, Std: {df['dto'].std():.6f}\")\n    return df\n\n\n\n35.2.2 Vietnam-Specific Considerations for DTO\nSeveral features of the Vietnamese market require attention when constructing DTO:\n\nNo NASDAQ-type volume adjustment needed. Both HOSE and HNX are order-driven auction markets. The double-counting adjustment applied to NASDAQ securities in the U.S. literature is not necessary.\nThinly traded stocks. A substantial fraction of listed Vietnamese stocks, particularly on HNX, may have zero volume on many trading days. For stocks with intermittent trading, the rolling median may be biased toward zero, making DTO less informative. We require at least 80% non-zero volume days in each estimation window.\nPrice limit effects on volume. When a stock hits its daily price limit, unfilled orders accumulate and recorded volume may understate true clearing volume. The following day often shows a “catch-up” effect. Researchers should consider flagging limit-hit days.\nForeign investor trading decomposition. DataCore provides volume by investor type (foreign versus domestic). Researchers may wish to construct separate DTO measures for foreign and domestic volume, or use the foreign-to-domestic volume ratio as an additional dimension of disagreement.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#sec-suv",
    "href": "39_divop.html#sec-suv",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "35.3 Standardized Unexplained Volume (SUV)",
    "text": "35.3 Standardized Unexplained Volume (SUV)\n\n35.3.1 Construction Methodology\nThe Standardized Unexplained Volume measure, proposed by Garfinkel (2009), isolates the disagreement component of volume by explicitly controlling for the information content of returns. The insight is that trading volume has both a liquidity component and an informedness component correlated with the magnitude and sign of returns. By regressing turnover on signed returns and extracting the standardized residual, SUV captures volume attributable to disagreement after controlling for both liquidity trends and information-driven trading.\nFor each stock \\(i\\), on each trading date \\(t\\), we estimate using data from the estimation window \\([\\tau_1, \\tau_2]\\):\n\\[\n\\text{Turn}_{i,s} = \\alpha_i + \\beta_i^{+} \\cdot \\text{RetPos}_{i,s} + \\beta_i^{-} \\cdot \\text{RetNeg}_{i,s} + \\epsilon_{i,s}, \\quad s \\in [\\tau_1, \\tau_2]\n\\tag{35.1}\\]\nwhere \\(\\text{RetPos}_{i,s} = |r_{i,s}| \\cdot \\mathbf{1}(r_{i,s} &gt; 0)\\) and \\(\\text{RetNeg}_{i,s} = |r_{i,s}| \\cdot \\mathbf{1}(r_{i,s} &lt; 0)\\).\nThe Standardized Unexplained Volume on date \\(t\\) is:\n\\[\n\\text{SUV}_{i,t} = \\frac{\\text{Turn}_{i,t} - \\hat{\\text{Turn}}_{i,t}}{\\hat{\\sigma}_{\\epsilon,i}}\n\\tag{35.2}\\]\nwhere \\(\\hat{\\text{Turn}}_{i,t}\\) is the predicted turnover and \\(\\hat{\\sigma}_{\\epsilon,i}\\) is the RMSE from Equation 35.1.\nThe asymmetric specification with separate coefficients for positive and negative returns reflects that the volume-return relation differs by return sign. In the U.S., buying pressure tends to generate more volume than selling pressure due to short-sale frictions. In Vietnam, where short selling was unavailable until 2025, this asymmetry should be even more pronounced because all selling activity was constrained to existing shareholders.\n\ndef compute_suv(df, calendar, config):\n    \"\"\"\n    Compute Standardized Unexplained Volume via rolling regressions.\n    \n    For each stock-date, regress Turn on RetPos and RetNeg over the\n    estimation window, then compute SUV = (actual - predicted) / RMSE.\n    \"\"\"\n    est_window = config['est_window']\n    min_obs = int(est_window * config['min_volume_days'])\n    \n    # Prepare signed return components\n    df = df.copy()\n    df['ret_pos'] = np.where(df['ret'] &gt; 0, np.abs(df['ret']), 0.0)\n    df['ret_neg'] = np.where(\n        (df['ret'] &lt; 0) & df['ret'].notna(), np.abs(df['ret']), 0.0\n    )\n    \n    results = []\n    grouped = {t: g for t, g in df.groupby('ticker')}\n    \n    for _, cal_row in calendar.iterrows():\n        dt = cal_row['date']\n        est_s, est_e = cal_row['est_start'], cal_row['est_end']\n        \n        for ticker, tdata in grouped.items():\n            # Estimation window\n            est = tdata[\n                (tdata['date'] &gt;= est_s) & (tdata['date'] &lt;= est_e)\n            ].dropna(subset=['turnover', 'ret_pos', 'ret_neg'])\n            \n            if len(est) &lt; min_obs:\n                continue\n            \n            # Event date\n            evt = tdata[tdata['date'] == dt]\n            if evt.empty or evt['turnover'].isna().all():\n                continue\n            \n            # OLS: Turn = alpha + beta_pos * RetPos + beta_neg * RetNeg\n            X = est[['ret_pos', 'ret_neg']].values\n            y = est['turnover'].values\n            \n            reg = LinearRegression().fit(X, y)\n            y_hat = reg.predict(X)\n            rmse = np.sqrt(np.mean((y - y_hat) ** 2))\n            \n            if rmse &lt;= 0:\n                continue\n            \n            # Predict and standardize for event date\n            X_evt = evt[['ret_pos', 'ret_neg']].values\n            pred = reg.predict(X_evt)[0]\n            actual = evt['turnover'].values[0]\n            suv = (actual - pred) / rmse\n            \n            results.append({\n                'ticker': ticker, 'date': dt,\n                'suv': suv,\n                'predicted_turnover': pred,\n                'rmse_turn': rmse,\n                'n_est': len(est),\n                'alpha_turn': reg.intercept_,\n                'beta_pos': reg.coef_[0],\n                'beta_neg': reg.coef_[1],\n            })\n    \n    suv_df = pd.DataFrame(results)\n    print(f\"SUV: {len(suv_df):,} stock-date obs\")\n    print(f\"  Mean: {suv_df['suv'].mean():.4f}, \"\n          f\"Median: {suv_df['suv'].median():.4f}\")\n    return suv_df\n\n\n\n35.3.2 Interpreting the SUV Regression Coefficients\nThe estimated coefficients from Equation 35.1 are informative about market microstructure. Garfinkel (2009) reports \\(\\hat{\\beta}^{+} &gt; \\hat{\\beta}^{-}\\) for most U.S. stocks. In Vietnam, we expect this asymmetry to be even stronger because:\n\nNo short selling (pre-2025): All selling is by existing shareholders, limiting volume response to negative returns.\nT+2 settlement: Investors cannot immediately reinvest sale proceeds, further dampening sell-side volume.\nPrice limits: The \\(\\pm\\) 7% (HOSE) and \\(\\pm\\) 10% (HNX) daily limits truncate the return distribution, compressing the range of both regressors.\n\nResearchers should report summary statistics of \\((\\hat{\\alpha}, \\hat{\\beta}^{+}, \\hat{\\beta}^{-}, R^2)\\) across the cross-section and over time.\n\ndef suv_diagnostics(suv_df):\n    \"\"\"Report cross-sectional summary of SUV regression parameters.\"\"\"\n    print(\"\\n=== SUV Regression Diagnostics ===\")\n    \n    params = ['alpha_turn', 'beta_pos', 'beta_neg']\n    print(suv_df[params].describe(\n        percentiles=[.05, .25, .50, .75, .95]\n    ).T.to_string(float_format='{:.6f}'.format))\n    \n    # Asymmetry test\n    diff = suv_df['beta_pos'] - suv_df['beta_neg']\n    print(f\"\\nbeta_pos - beta_neg: mean = {diff.mean():.6f}, \"\n          f\"frac &gt; 0 = {(diff &gt; 0).mean():.3f}\")",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#sec-total-vol",
    "href": "39_divop.html#sec-total-vol",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "36.1 Total Return Volatility",
    "text": "36.1 Total Return Volatility\n\n36.1.1 Theoretical Motivation\nStock return volatility serves as a proxy for divergence of opinion through several channels. Shalen (1993) develops a model in which both volume and volatility are increasing in the dispersion of investor beliefs. Scheinkman and Xiong (2003) predict that higher volatility reflects the speculative trading component driven by overconfident investors who disagree about value. Empirically, Boehme, Danielsen, and Sorescu (2006) and Chatterjee, John, and Yan (2012) use idiosyncratic volatility as a DIVOP proxy and find it positively correlated with other disagreement measures and negatively associated with subsequent returns when short-sale constraints bind.\n\n\n36.1.2 Construction\nTotal return volatility is the standard deviation of daily returns over the rolling estimation window:\n\\[\n\\text{VOLATILITY}_{i,t} = \\sqrt{\\frac{1}{N_i - 1} \\sum_{s \\in [\\tau_1, \\tau_2]} (r_{i,s} - \\bar{r}_i)^2}\n\\tag{36.1}\\]\nwhere \\(N_i\\) is the number of non-missing return observations for stock \\(i\\) in the window \\([\\tau_1, \\tau_2]\\).",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#sec-ivol",
    "href": "39_divop.html#sec-ivol",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "36.2 Idiosyncratic Volatility (IVOL)",
    "text": "36.2 Idiosyncratic Volatility (IVOL)\nIdiosyncratic volatility isolates firm-specific return variation by removing the systematic component explained by market movements. We compute IVOL from the residuals of a market model:\n\\[\nr_{i,s} = \\alpha_i + \\beta_i \\cdot r_{m,s} + \\epsilon_{i,s}, \\quad s \\in [\\tau_1, \\tau_2]\n\\tag{36.2}\\]\n\\[\n\\text{IVOL}_{i,t} = \\text{Std}(\\hat{\\epsilon}_{i,s})\n\\tag{36.3}\\]\nResearchers may extend this to a Fama and French (1993) three-factor or five-factor model using Vietnamese factor portfolios constructed elsewhere in this book. A richer factor model yields IVOL estimates that better isolate truly idiosyncratic disagreement, at the cost of requiring factor portfolio construction.\n\ndef compute_volatility(df, calendar, config):\n    \"\"\"\n    Compute total return volatility and idiosyncratic volatility\n    via rolling estimation windows.\n    \n    Total vol = std(returns) in window.\n    IVOL = std(residuals) from market model regression.\n    \"\"\"\n    est_window = config['est_window']\n    min_obs = int(est_window * config['min_volume_days'])\n    \n    # Value-weighted market return\n    def _vw_ret(g):\n        valid = g.dropna(subset=['ret'])\n        if valid.empty:\n            return np.nan\n        w = valid['adj_shares_out'] * valid['close']\n        return np.average(valid['ret'], weights=w)\n    \n    mkt_ret = df.groupby('date').apply(_vw_ret).reset_index()\n    mkt_ret.columns = ['date', 'mkt_ret']\n    df = df.merge(mkt_ret, on='date', how='left')\n    \n    results = []\n    grouped = {t: g for t, g in df.groupby('ticker')}\n    \n    for _, cal_row in calendar.iterrows():\n        dt = cal_row['date']\n        est_s, est_e = cal_row['est_start'], cal_row['est_end']\n        \n        for ticker, tdata in grouped.items():\n            est = tdata[\n                (tdata['date'] &gt;= est_s) & (tdata['date'] &lt;= est_e)\n            ].dropna(subset=['ret', 'mkt_ret'])\n            \n            if len(est) &lt; min_obs:\n                continue\n            \n            # Total volatility\n            total_vol = est['ret'].std()\n            \n            # Market model -&gt; IVOL\n            X = est[['mkt_ret']].values\n            y = est['ret'].values\n            reg = LinearRegression().fit(X, y)\n            resid = y - reg.predict(X)\n            ivol = np.std(resid, ddof=1)\n            \n            results.append({\n                'ticker': ticker, 'date': dt,\n                'total_volatility': total_vol,\n                'idio_volatility': ivol,\n                'market_beta': reg.coef_[0],\n                'market_alpha': reg.intercept_,\n                'r_squared_mm': reg.score(X, y),\n                'n_vol': len(est),\n            })\n    \n    vol_df = pd.DataFrame(results)\n    print(f\"Volatility: {len(vol_df):,} stock-date obs\")\n    print(f\"  Total vol (ann. mean): \"\n          f\"{vol_df['total_volatility'].mean() * np.sqrt(252):.4f}\")\n    print(f\"  IVOL (ann. mean): \"\n          f\"{vol_df['idio_volatility'].mean() * np.sqrt(252):.4f}\")\n    return vol_df\n\n\n36.2.1 Vietnam-Specific Considerations for Volatility\n\nPrice limits compress measured volatility. Daily limits of \\(\\pm\\) 7% (HOSE) and \\(\\pm\\) 10% (HNX) mechanically truncate the return distribution, leading to underestimation of true volatility. On limit-hit days, the true equilibrium return may exceed the observed return. Researchers should be aware that volatility-based DIVOP measures may be downward-biased for stocks that frequently hit limits.\nVN-Index concentration. The VN-Index is highly concentrated, the top 10 stocks often account for 50-60% of index weight. For small- and mid-cap stocks, an equal-weighted market return or a composite HOSE+HNX index may provide a better market factor in Equation 36.2.\nThin trading and non-synchronous returns. For thinly traded stocks, consecutive zero-return days can depress measured volatility. The Dimson (1979) adjustment (including lagged and lead market returns in the market model) may help correct for non-synchronous trading bias in the beta estimate, though its effect on IVOL is typically small.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#sec-baspread",
    "href": "39_divop.html#sec-baspread",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "37.1 Bid-Ask Spread (BASPREAD)",
    "text": "37.1 Bid-Ask Spread (BASPREAD)\n\n37.1.1 Theoretical Motivation\nThe bid-ask spread reflects the adverse selection costs faced by limit order providers. When investors hold heterogeneous beliefs, each trade is more likely to convey private information, raising the adverse selection component of the spread. Handa, Schwartz, and Tiwari (2003) show that in order-driven markets the spread widens when divergence of opinion increases because limit order providers face greater risk of being picked off by informed traders. Chung and Zhang (2014) demonstrate that closing bid-ask spreads from daily data provide a reliable approximation to intraday effective spreads.\n\n\n37.1.2 Construction\nWe compute the proportional bid-ask spread using end-of-day quote data:\n\\[\n\\text{BASPREAD}_{i,t} = \\frac{\\text{Ask}_{i,t} - \\text{Bid}_{i,t}}{\\text{Midpoint}_{i,t}}\n\\tag{37.1}\\]\nwhere \\(\\text{Midpoint}_{i,t} = (\\text{Ask}_{i,t} + \\text{Bid}_{i,t}) / 2\\). When end-of-day bid and ask are unavailable, we use the daily high-low range as a fallback. Following Chung and Zhang (2014), we delete observations where both Bid and Ask are zero, and where the spread exceeds 50% of the midpoint.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#sec-amihud",
    "href": "39_divop.html#sec-amihud",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "37.2 Amihud Illiquidity (ILLIQ)",
    "text": "37.2 Amihud Illiquidity (ILLIQ)\nThe Amihud (2002) ratio measures the price impact of order flow:\n\\[\n\\text{ILLIQ}_{i,t} = \\frac{|r_{i,t}|}{\\text{DolVol}_{i,t}}\n\\tag{37.2}\\]\nwhere \\(\\text{DolVol}_{i,t} = \\text{Volume}_{i,t} \\times \\text{Price}_{i,t}\\) (in billions VND for scaling). Higher ILLIQ reflects greater information asymmetry. We average daily ratios over monthly horizons and use the log transformation due to heavy right skew.\n\ndef compute_spread_and_illiq(df, config):\n    \"\"\"Compute bid-ask spread (BASPREAD) and Amihud illiquidity.\"\"\"\n    df = df.copy()\n    \n    # --- Bid-Ask Spread ---\n    df['midpoint_ba'] = (df['ask'] + df['bid']) / 2\n    df['baspread_ba'] = np.where(\n        (df['ask'] &gt; 0) & (df['bid'] &gt; 0) & (df['midpoint_ba'] &gt; 0),\n        (df['ask'] - df['bid']) / df['midpoint_ba'], np.nan\n    )\n    \n    # Fallback: high/low range\n    df['midpoint_hl'] = (df['high'] + df['low']) / 2\n    df['baspread_hl'] = np.where(\n        (df['high'] &gt; 0) & (df['low'] &gt; 0) & (df['midpoint_hl'] &gt; 0),\n        (df['high'] - df['low']) / df['midpoint_hl'], np.nan\n    )\n    \n    df['baspread'] = df['baspread_ba'].fillna(df['baspread_hl'])\n    df['midpoint'] = df['midpoint_ba'].fillna(df['midpoint_hl'])\n    \n    # Chung & Zhang (2009) filters\n    bad = (df['baspread'].isna()) | \\\n          (df['baspread'] &gt; config['max_spread_pct']) | \\\n          (df['baspread'] &lt; 0)\n    df.loc[bad, 'baspread'] = np.nan\n    \n    # --- Amihud Illiquidity ---\n    df['dollar_vol'] = df['volume'] * df['close'] / 1e9\n    df['amihud_daily'] = np.where(\n        df['dollar_vol'] &gt; 0,\n        np.abs(df['ret']) / df['dollar_vol'], np.nan\n    )\n    \n    print(f\"BASPREAD: {df['baspread'].notna().sum():,} valid obs, \"\n          f\"mean = {df['baspread'].mean():.6f}\")\n    print(f\"AMIHUD: {df['amihud_daily'].notna().sum():,} valid obs, \"\n          f\"mean = {df['amihud_daily'].mean():.6f}\")\n    return df\n\n\ndef compute_amihud_monthly(df):\n    \"\"\"Monthly Amihud = mean daily |ret|/dollar_vol (min 15 days).\"\"\"\n    df = df.copy()\n    df['ym'] = df['date'].dt.to_period('M')\n    agg = df.groupby(['ticker', 'ym']).agg(\n        illiq_mean=('amihud_daily', 'mean'),\n        n_days=('amihud_daily', 'count'),\n    ).reset_index()\n    agg = agg[agg['n_days'] &gt;= 15].copy()\n    agg['log_illiq'] = np.log(agg['illiq_mean'] + 1e-10)\n    return agg\n\n\n37.2.1 Vietnam-Specific Considerations for Spread and Liquidity\n\nTick size schedule. Vietnam uses variable tick sizes: 10 VND (prices &lt; 10,000), 50 VND (10,000–49,950), and 100 VND (≥ 50,000) on HOSE. These impose a floor on quoted spreads for low-priced stocks. Researchers should be cautious interpreting cross-price-decile spread variation as reflecting opinion divergence rather than tick-size mechanics.\nOrder-driven market structure. Both HOSE and HNX are pure order-driven markets where public limit orders provide liquidity. This makes the Chung and Zhang (2014) CRSP-based spread approximation appropriate.\nLot size requirements. HOSE requires 100-share standard lots for continuous trading. For high-priced stocks, the standard lot represents a large capital commitment, potentially inflating quoted spreads relative to effective trading costs.\nCall auction effects. Opening and closing sessions on HOSE use periodic call auctions, which can produce bid-ask quotes that differ substantially from continuous-trading spreads.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#theoretical-motivation-3",
    "href": "39_divop.html#theoretical-motivation-3",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "38.1 Theoretical Motivation",
    "text": "38.1 Theoretical Motivation\nAnalyst forecast dispersion, the cross-sectional standard deviation of individual analysts’ earnings forecasts, is the most direct measure of divergence of opinion. Unlike market-based proxies that capture disagreement indirectly, forecast dispersion directly measures disagreement among informed market participants. Abarbanell, Lanen, and Verrecchia (1995) establish the theoretical basis, and Diether, Malloy, and Scherbina (2002) demonstrate that stocks with higher analyst forecast dispersion earn lower subsequent returns, consistent with the Miller overpricing hypothesis.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#data-challenges-in-vietnam",
    "href": "39_divop.html#data-challenges-in-vietnam",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "38.2 Data Challenges in Vietnam",
    "text": "38.2 Data Challenges in Vietnam\nConstructing analyst forecast dispersion in Vietnam presents substantial challenges relative to the U.S.:\n\nCoverage breadth. While I/B/E/S covers over 4,000 U.S. companies, only 100–150 Vietnamese firms typically have coverage by at least 3 analysts, concentrated among VN30 constituents.\nData sources. Analyst forecasts are available from DataCore.vn, FiinPro, Bloomberg, and Refinitiv. The choice of source affects coverage and timeliness.\nForecast staleness. With limited coverage, forecasts may go unrevised for months. Following I/B/E/S methodology, we carry each forecast forward for a maximum of 105 days.",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#construction-methodology-2",
    "href": "39_divop.html#construction-methodology-2",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "38.3 Construction Methodology",
    "text": "38.3 Construction Methodology\nThe construction proceeds as follows:\n\nClean individual forecasts. Remove observations where the announcement date precedes the review date. Keep only annual EPS forecasts. For each analyst-ticker-fiscal period, retain only the latest forecast per calendar month.\nHandle stopped and excluded estimates. Remove forecasts where the analyst has left the brokerage or the estimate has been excluded from consensus.\nCarry forward with staleness control. Each forecast is valid until the earlier of: (a) the next forecast by the same analyst, (b) 105 days after the announcement, or (c) the actual earnings announcement date.\nExpand to monthly frequency. For each ticker-month, identify all valid outstanding forecasts and compute dispersion.\nCompute scaled measures:\n\n\\[\n\\text{DISP1}_{i,m} = \\frac{\\text{Std}(\\hat{\\text{EPS}}_{i,m}^{(a)})}{|\\text{Mean}(\\hat{\\text{EPS}}_{i,m}^{(a)})|}\n\\qquad\n\\text{DISP2}_{i,m} = \\frac{\\text{Std}(\\hat{\\text{EPS}}_{i,m}^{(a)})}{\\bar{P}_{i,m}}\n\\]\n\ndef construct_analyst_dispersion(forecasts_df, price_df, config):\n    \"\"\"\n    Construct analyst forecast dispersion measures.\n    \n    Parameters\n    ----------\n    forecasts_df : pd.DataFrame\n        Individual analyst forecasts with: ticker, analyst_id, broker,\n        fpedats, anndats, revdats, value (EPS), anndats_act.\n    price_df : pd.DataFrame\n        Monthly price: ticker, month, mean_price.\n    config : dict\n        With min_analysts, forecast_carry_days.\n    \"\"\"\n    carry_days = config['forecast_carry_days']\n    min_analysts = config['min_analysts']\n    \n    df = forecasts_df.copy()\n    df = df[df['anndats'] &lt;= df['revdats']].copy()\n    df = df.dropna(subset=['fpedats', 'anndats', 'value'])\n    \n    # Latest forecast per analyst-month\n    df['ym'] = df['anndats'].dt.to_period('M')\n    df = df.sort_values(\n        ['ticker', 'fpedats', 'analyst_id', 'ym', 'anndats', 'revdats']\n    )\n    df = df.groupby(['ticker', 'fpedats', 'analyst_id', 'ym']).tail(1)\n    \n    # Carry-forward end date\n    df = df.sort_values(\n        ['ticker', 'analyst_id', 'fpedats', 'anndats'],\n        ascending=[True, True, True, False]\n    )\n    df['next_ann'] = df.groupby(\n        ['ticker', 'analyst_id', 'fpedats']\n    )['anndats'].shift(-1)\n    \n    def _carry_end(row):\n        candidates = [row['anndats'] + pd.Timedelta(days=carry_days)]\n        if pd.notna(row.get('next_ann')):\n            candidates.append(row['next_ann'])\n        if pd.notna(row.get('anndats_act')):\n            candidates.append(row['anndats_act'])\n        return min(candidates)\n    \n    df['carry_end'] = df.apply(_carry_end, axis=1)\n    \n    # Monthly expansion\n    months = pd.period_range(config['beg_date'], config['end_date'], freq='M')\n    records = []\n    for month in months:\n        me = month.to_timestamp(how='end')\n        valid = df[(df['anndats'] &lt;= me) & (df['carry_end'] &gt; me)].copy()\n        valid = valid[valid['fpedats'] &gt; me]\n        valid = valid.sort_values(['ticker', 'analyst_id', 'anndats'])\n        valid = valid.groupby(['ticker', 'analyst_id']).tail(1)\n        \n        disp = valid.groupby('ticker').agg(\n            n_analysts=('analyst_id', 'nunique'),\n            mean_fcst=('value', 'mean'),\n            std_fcst=('value', 'std'),\n        ).reset_index()\n        disp['month'] = month\n        records.append(disp)\n    \n    if not records:\n        return pd.DataFrame()\n    disp_df = pd.concat(records, ignore_index=True)\n    \n    # Scaled measures\n    disp_df['disp1'] = np.where(\n        disp_df['mean_fcst'].abs() &gt; 0,\n        disp_df['std_fcst'] / disp_df['mean_fcst'].abs(), np.nan\n    )\n    disp_df = disp_df.merge(price_df, on=['ticker', 'month'], how='left')\n    disp_df['disp2'] = np.where(\n        disp_df['mean_price'] &gt; 0,\n        disp_df['std_fcst'] / disp_df['mean_price'], np.nan\n    )\n    disp_df['disp_raw'] = disp_df['std_fcst']\n    \n    out = disp_df[disp_df['n_analysts'] &gt;= min_analysts].copy()\n    print(f\"DISP: {len(out):,} ticker-months (&gt;= {min_analysts} analysts)\")\n    print(f\"  Mean analysts: {out['n_analysts'].mean():.1f}\")\n    return out",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#scaling-considerations",
    "href": "39_divop.html#scaling-considerations",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "38.4 Scaling Considerations",
    "text": "38.4 Scaling Considerations\nFollowing Cheong and Thomas (2011), we note that each scaling choice has pitfalls. DISP1 (scaled by absolute mean forecast) can produce extreme values when the mean forecast approaches zero—common for Vietnamese firms near breakeven. DISP2 (scaled by price) introduces a mechanical negative correlation between price and scaled dispersion. We recommend reporting all three versions (DISP1, DISP2, and unscaled DISP_RAW with \\(\\ln(\\text{Price})\\) as an additional control), and winsorizing DISP1 at the 1st and 99th percentiles.\n\n\n\n\n\n\nWarningCaution on Analyst Dispersion in Thin-Coverage Markets\n\n\n\nWith typical coverage of 5–10 analysts per firm in Vietnam (versus 15–25 in the U.S.), forecast dispersion is estimated with substantially greater noise. A dispersion measure from 3 analysts has a very different sampling distribution than one from 20. Always include the number of analysts as a control and test robustness with varying minimum-analyst thresholds (3, 5, 7).",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#summary-statistics",
    "href": "39_divop.html#summary-statistics",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "40.1 Summary Statistics",
    "text": "40.1 Summary Statistics\n\ndef descriptive_statistics(merged_df):\n    \"\"\"Comprehensive descriptive statistics for DIVOP proxies.\"\"\"\n    proxies = {\n        'dto': 'Unexplained Volume (DTO)',\n        'suv': 'Std Unexplained Volume (SUV)',\n        'total_volatility': 'Total Return Volatility',\n        'idio_volatility': 'Idiosyncratic Volatility',\n        'baspread': 'Bid-Ask Spread',\n        'amihud_daily': 'Amihud Illiquidity',\n        'disp1': 'Analyst Disp (mean-scaled)',\n        'disp2': 'Analyst Disp (price-scaled)',\n    }\n    avail = {k: v for k, v in proxies.items() if k in merged_df.columns}\n    rows = []\n    for col, label in avail.items():\n        s = merged_df[col].dropna()\n        rows.append({\n            'Proxy': label, 'N': f'{len(s):,}',\n            'Mean': f'{s.mean():.6f}', 'Std': f'{s.std():.6f}',\n            'P5': f'{s.quantile(.05):.6f}',\n            'Median': f'{s.median():.6f}',\n            'P95': f'{s.quantile(.95):.6f}',\n            'Skew': f'{s.skew():.2f}',\n            'Kurt': f'{s.kurtosis():.2f}',\n        })\n    stats = pd.DataFrame(rows).set_index('Proxy')\n    print(\"\\n\" + \"=\" * 90)\n    print(\"Descriptive Statistics of DIVOP Proxies\")\n    print(\"Vietnamese Equity Market, HOSE and HNX\")\n    print(\"=\" * 90)\n    print(stats.to_string())\n    return stats",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#divop-by-firm-characteristics",
    "href": "39_divop.html#divop-by-firm-characteristics",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "40.2 DIVOP by Firm Characteristics",
    "text": "40.2 DIVOP by Firm Characteristics\n\ndef divop_by_size(merged_df):\n    \"\"\"Mean DIVOP proxies by market-cap quintile.\"\"\"\n    df = merged_df.copy()\n    df['mkt_cap'] = df['close'] * df['shares_outstanding']\n    df['size_q'] = df.groupby('date')['mkt_cap'].transform(\n        lambda x: pd.qcut(x, 5,\n            labels=['Q1 Small','Q2','Q3','Q4','Q5 Large'],\n            duplicates='drop')\n    )\n    proxies = ['dto','suv','total_volatility','idio_volatility',\n               'baspread','amihud_daily']\n    avail = [p for p in proxies if p in df.columns]\n    tab = df.groupby('size_q')[avail].mean()\n    print(\"\\n=== Mean DIVOP by Size Quintile ===\")\n    print(tab.to_string(float_format='{:.6f}'.format))\n    return tab\n\ndef divop_by_exchange(merged_df):\n    \"\"\"Compare mean DIVOP across HOSE and HNX.\"\"\"\n    proxies = ['dto','suv','total_volatility','idio_volatility',\n               'baspread','amihud_daily']\n    avail = [p for p in proxies if p in merged_df.columns]\n    tab = merged_df.groupby('exchange')[avail].mean()\n    print(\"\\n=== Mean DIVOP by Exchange ===\")\n    print(tab.to_string(float_format='{:.6f}'.format))\n    return tab",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#time-series-evolution",
    "href": "39_divop.html#time-series-evolution",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "40.3 Time-Series Evolution",
    "text": "40.3 Time-Series Evolution\n\ndef plot_divop_timeseries(merged_df):\n    \"\"\"Plot monthly cross-sectional median DIVOP with crisis shading.\"\"\"\n    df = merged_df.copy()\n    df['ym'] = df['date'].dt.to_period('M')\n    proxies = ['dto','suv','total_volatility','baspread']\n    avail = [p for p in proxies if p in df.columns]\n    monthly = df.groupby('ym')[avail].median()\n    monthly.index = monthly.index.to_timestamp()\n    \n    fig, axes = plt.subplots(len(avail), 1,\n        figsize=(13, 3.5*len(avail)), sharex=True)\n    if len(avail) == 1: axes = [axes]\n    \n    labels = {'dto':'DTO','suv':'SUV',\n              'total_volatility':'Volatility','baspread':'Spread'}\n    colors = ['#1976D2','#388E3C','#F57C00','#D32F2F']\n    \n    for i, (proxy, ax) in enumerate(zip(avail, axes)):\n        ax.plot(monthly.index, monthly[proxy],\n                color=colors[i], linewidth=1.3)\n        ax.set_ylabel(labels.get(proxy, proxy), fontsize=10)\n        ax.grid(True, alpha=0.25)\n        for s, e, c in [('2008-01','2009-06','red'),\n                         ('2020-01','2020-12','orange'),\n                         ('2022-09','2023-06','purple')]:\n            ax.axvspan(pd.Timestamp(s), pd.Timestamp(e),\n                        alpha=0.1, color=c)\n    \n    axes[0].set_title(\n        'Time-Series of DIVOP Proxies\\n'\n        'Monthly Cross-Sectional Median, HOSE & HNX',\n        fontsize=13, fontweight='bold')\n    from matplotlib.patches import Patch\n    axes[-1].legend(handles=[\n        Patch(facecolor='red', alpha=.2, label='GFC 2008-09'),\n        Patch(facecolor='orange', alpha=.2, label='COVID-19'),\n        Patch(facecolor='purple', alpha=.2, label='Bond Crisis 2022-23'),\n    ], loc='upper right', fontsize=8)\n    plt.tight_layout()\n    plt.savefig('divop_timeseries.png', dpi=300, bbox_inches='tight')\n    plt.show()",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#application-1-divop-and-the-cross-section-of-returns",
    "href": "39_divop.html#application-1-divop-and-the-cross-section-of-returns",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "42.1 Application 1: DIVOP and the Cross-Section of Returns",
    "text": "42.1 Application 1: DIVOP and the Cross-Section of Returns\nThe fundamental test of the Miller hypothesis is whether stocks with higher divergence of opinion earn lower subsequent returns. We implement Fama-MacBeth cross-sectional regressions:\n\\[\nr_{i,t+1:t+h} = \\gamma_{0,t} + \\gamma_{1,t} \\cdot \\text{DIVOP}_{i,t} + \\gamma_{2,t}' \\mathbf{X}_{i,t} + \\varepsilon_{i,t}\n\\]\nwhere \\(\\mathbf{X}_{i,t}\\) includes controls for market beta, log market capitalization, and log book-to-market ratio. The Miller hypothesis predicts \\(\\bar{\\gamma}_1 &lt; 0\\).\n\ndef fama_macbeth_divop(merged_df, divop_proxy='suv',\n                        controls=None, horizon=21):\n    \"\"\"\n    Fama-MacBeth cross-sectional regressions.\n    Miller predicts gamma_1 &lt; 0; Varian predicts gamma_1 &gt; 0.\n    \"\"\"\n    if controls is None:\n        controls = ['market_beta', 'log_mktcap']\n    \n    df = merged_df.copy()\n    df = df.sort_values(['ticker', 'date'])\n    df['fwd_ret'] = df.groupby('ticker')['ret'].transform(\n        lambda x: x.shift(-1).rolling(horizon).sum().shift(-(horizon-1))\n    )\n    df['log_mktcap'] = np.log(\n        df['close'] * df['shares_outstanding'] + 1\n    )\n    \n    reg_vars = ['fwd_ret', divop_proxy] + \\\n               [c for c in controls if c in df.columns]\n    df_reg = df[['ticker','date'] + reg_vars].dropna()\n    \n    from numpy.linalg import lstsq\n    results = []\n    for date, cross in df_reg.groupby('date'):\n        if len(cross) &lt; 30: continue\n        y = cross['fwd_ret'].values\n        X_cols = [divop_proxy] + [c for c in controls if c in cross.columns]\n        X = np.column_stack([np.ones(len(cross)), cross[X_cols].values])\n        try:\n            coefs, _, _, _ = lstsq(X, y, rcond=None)\n            results.append({\n                'date': date, 'intercept': coefs[0],\n                f'gamma_{divop_proxy}': coefs[1], 'n': len(cross),\n            })\n        except Exception: continue\n    \n    fm = pd.DataFrame(results)\n    gc = f'gamma_{divop_proxy}'\n    mu = fm[gc].mean()\n    se = fm[gc].std() / np.sqrt(len(fm))\n    t = mu / se\n    \n    print(f\"\\n=== Fama-MacBeth: {divop_proxy} -&gt; \"\n          f\"{horizon}-day fwd returns ===\")\n    print(f\"  Mean gamma: {mu:.6f}, t-stat: {t:.3f}\")\n    if t &lt; -1.96:   print(\"  -&gt; Supports Miller (1977)\")\n    elif t &gt; 1.96:   print(\"  -&gt; Supports Varian (1985)\")\n    else:            print(\"  -&gt; Inconclusive at 5%\")\n    return fm",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#application-2-divop-and-earnings-announcements",
    "href": "39_divop.html#application-2-divop-and-earnings-announcements",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "42.2 Application 2: DIVOP and Earnings Announcements",
    "text": "42.2 Application 2: DIVOP and Earnings Announcements\nFollowing Berkman et al. (2009), we test whether high-DIVOP stocks experience negative abnormal returns around earnings announcements, as uncertainty resolution reduces the optimism premium.\n\ndef divop_earnings_event(merged_df, ea_dates_df,\n                          divop_proxy='suv', window=(-1, 3)):\n    \"\"\"\n    Sort stocks into DIVOP quintiles pre-EA, compute CAR in window.\n    Miller predicts: Q5 (high DIVOP) has lower CAR than Q1 (low DIVOP).\n    \"\"\"\n    df = merged_df.copy()\n    ea = ea_dates_df.copy()\n    \n    # Pre-EA DIVOP value (5 days before)\n    ea['pre_date'] = ea['ea_date'] - pd.Timedelta(days=5)\n    ea = ea.merge(\n        df[['ticker','date',divop_proxy]].rename(\n            columns={'date':'pre_date'}),\n        on=['ticker','pre_date'], how='inner'\n    )\n    ea['divop_q'] = pd.qcut(\n        ea[divop_proxy], 5,\n        labels=['Q1 Low','Q2','Q3','Q4','Q5 High'],\n        duplicates='drop'\n    )\n    \n    print(f\"\\n=== EA Event Study by {divop_proxy} quintile ===\")\n    print(f\"  Window: ({window[0]}, {window[1]}) days\")\n    print(f\"  Miller predicts: Q5 has lower CAR than Q1\")\n    return ea",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "39_divop.html#application-3-composite-divop-index-via-pca",
    "href": "39_divop.html#application-3-composite-divop-index-via-pca",
    "title": "32  Measuring Divergence of Investor Opinion",
    "section": "42.3 Application 3: Composite DIVOP Index via PCA",
    "text": "42.3 Application 3: Composite DIVOP Index via PCA\nWhen a single summary measure of disagreement is needed, PCA on the battery of standardized proxies extracts the common “disagreement factor.”\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndef composite_divop_pca(merged_df, proxies=None):\n    \"\"\"Extract first principal component from standardized DIVOP proxies.\"\"\"\n    if proxies is None:\n        proxies = ['dto','suv','total_volatility','idio_volatility',\n                   'baspread','amihud_daily']\n    avail = [p for p in proxies if p in merged_df.columns]\n    data = merged_df[['ticker','date'] + avail].dropna()\n    \n    scaler = StandardScaler()\n    X = scaler.fit_transform(data[avail])\n    \n    pca = PCA(n_components=3)\n    factors = pca.fit_transform(X)\n    data['divop_composite'] = factors[:, 0]\n    \n    # Ensure positive correlation with inputs\n    for col in avail:\n        if data['divop_composite'].corr(data[col]) &lt; 0:\n            data['divop_composite'] *= -1\n            break\n    \n    loadings = pd.DataFrame(\n        pca.components_.T, index=avail,\n        columns=['PC1','PC2','PC3']\n    )\n    \n    print(f\"\\n=== PCA Composite DIVOP ===\")\n    print(f\"Variance explained: \"\n          f\"{pca.explained_variance_ratio_[:3].round(3)}\")\n    print(f\"\\nLoadings:\\n{loadings.to_string(float_format='{:.4f}'.format)}\")\n    return data[['ticker','date','divop_composite']], loadings",
    "crumbs": [
      "Home",
      "Các yếu tố cơ bản của công ty, định giá và tín hiệu doanh nghiệp",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Measuring Divergence of Investor Opinion</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html",
    "href": "50_institutional_ownership.html",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "",
    "text": "33.1 Institutional Ownership in Vietnam: A Distinct Landscape\nVietnam’s equity market presents a fundamentally different institutional ownership landscape from the mature markets of the US, Europe, or Japan. Since the Ho Chi Minh City Securities Trading Center (now HOSE) opened on July 28, 2000 with just two listed stocks, the market has grown to over 1,700 listed companies across three exchanges (HOSE, HNX, and UPCOM) with a combined market capitalization exceeding 200 billion USD. Yet the ownership structure remains distinctive in several critical ways:",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#institutional-ownership-in-vietnam-a-distinct-landscape",
    "href": "50_institutional_ownership.html#institutional-ownership-in-vietnam-a-distinct-landscape",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "",
    "text": "Retail dominance. Individual investors account for approximately 85% of trading value on Vietnamese exchanges, far exceeding the institutional share. This contrasts sharply with the US, where institutional investors dominate both ownership and trading (Bao Dinh and Tran 2024). The implications for market efficiency, price discovery, and volatility are profound.\nState ownership legacy. Vietnam’s equitization (privatization) program, initiated under Đổi Mới reforms in 1986, means that the state remains a significant or controlling shareholder in many listed companies. As of 2022, SOEs (firms with state ownership &gt; 50%) account for approximately 30% of total market capitalization despite representing less than 10% of listed firms (Huang, Liu, and Shu 2023). State ownership introduces unique agency problems, governance dynamics, and liquidity constraints.\nForeign Ownership Limits (FOLs). Vietnam imposes sector-specific caps on aggregate foreign ownership, typically 49% for most sectors, 30% for banking, and varying limits for aviation, media, and telecommunications. When a stock reaches its FOL, foreign investors can only buy from other foreign sellers, creating a segmented market with distinct pricing dynamics and a well-documented “FOL premium” (Vo 2015).\nDisclosure regime. Unlike the US quarterly 13F filing system, Vietnam’s ownership disclosure is event-driven and periodic. Major shareholders (≥5%) must disclose within 7 business days of crossing thresholds. Annual reports contain detailed shareholder registers. Semi-annual fund reports provide portfolio snapshots. This creates a patchwork of disclosure frequencies that require careful handling.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-datacore",
    "href": "50_institutional_ownership.html#sec-datacore",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.2 Data Infrastructure: DataCore.vn",
    "text": "33.2 Data Infrastructure: DataCore.vn\nDataCore.vn is a comprehensive Vietnamese financial data platform that provides academic-grade datasets for the Vietnamese market. Throughout this chapter, we assume all data is sourced exclusively from DataCore.vn, which provides:\n\n\n\nTable 33.1: DataCore.vn Data Tables Used in This Chapter\n\n\n\n\n\n\n\n\n\n\nDataCore.vn Dataset\nContent\nKey Variables\n\n\n\n\nStock Prices\nDaily/monthly OHLCV for HOSE, HNX, UPCOM\nticker, date, close, adjusted_close, volume, shares_outstanding\n\n\nOwnership Structure\nShareholder composition snapshots\nticker, date, shareholder_name, shares_held, ownership_pct, shareholder_type\n\n\nMajor Shareholders\nDetailed ≥5% holders\nticker, date, shareholder_name, shares_held, is_foreign, is_state, is_institution\n\n\nCorporate Actions\nDividends, stock splits, bonus shares, rights issues\nticker, ex_date, action_type, ratio, record_date\n\n\nCompany Profile\nSector, exchange, listing date, charter capital\nticker, exchange, industry_code, listing_date, fol_limit\n\n\nFinancial Statements\nQuarterly/annual financials\nticker, period, revenue, net_income, total_assets, equity\n\n\nForeign Ownership\nDaily foreign ownership tracking\nticker, date, foreign_shares, foreign_pct, fol_limit, foreign_room\n\n\nFund Holdings\nSemi-annual fund portfolio disclosures\nfund_name, report_date, ticker, shares_held, market_value\n\n\n\n\n\n\n\nclass DataCoreReader:\n    \"\"\"\n    Unified data reader for DataCore.vn datasets.\n    \n    Assumes data has been downloaded from DataCore.vn and stored locally.\n    Supports both Parquet (recommended for performance) and CSV formats.\n    \n    Parameters\n    ----------\n    data_dir : str or Path\n        Root directory containing DataCore.vn data files\n    file_format : str\n        'parquet' or 'csv' (default: 'parquet')\n    \"\"\"\n    \n    # Expected file names in the data directory\n    FILE_MAP = {\n        'prices': 'stock_prices',\n        'ownership': 'ownership_structure',\n        'major_shareholders': 'major_shareholders',\n        'corporate_actions': 'corporate_actions',\n        'company_profile': 'company_profile',\n        'financials': 'financial_statements',\n        'foreign_ownership': 'foreign_ownership_daily',\n        'fund_holdings': 'fund_holdings',\n    }\n    \n    def __init__(self, data_dir: Union[str, Path], file_format: str = 'parquet'):\n        self.data_dir = Path(data_dir)\n        self.fmt = file_format\n        self._cache = {}\n        \n        # Verify data directory exists\n        if not self.data_dir.exists():\n            raise FileNotFoundError(\n                f\"Data directory not found: {self.data_dir}\\n\"\n                f\"Please download data from DataCore.vn and place it in this directory.\"\n            )\n        \n        print(f\"DataCore.vn reader initialized: {self.data_dir}\")\n        available = [f.stem for f in self.data_dir.glob(f'*.{self.fmt}')]\n        print(f\"Available datasets: {available}\")\n    \n    def _read(self, key: str) -&gt; pd.DataFrame:\n        \"\"\"Read and cache a dataset.\"\"\"\n        if key in self._cache:\n            return self._cache[key]\n        \n        fname = self.FILE_MAP.get(key, key)\n        filepath = self.data_dir / f\"{fname}.{self.fmt}\"\n        \n        if not filepath.exists():\n            raise FileNotFoundError(\n                f\"Dataset not found: {filepath}\\n\"\n                f\"Expected file: {fname}.{self.fmt} in {self.data_dir}\"\n            )\n        \n        if self.fmt == 'parquet':\n            df = pd.read_parquet(filepath)\n        else:\n            df = pd.read_csv(filepath, parse_dates=True)\n        \n        # Auto-detect and parse date columns\n        for col in df.columns:\n            if 'date' in col.lower() or col.lower() in ['period', 'ex_date', 'record_date']:\n                try:\n                    df[col] = pd.to_datetime(df[col])\n                except (ValueError, TypeError):\n                    pass\n        \n        self._cache[key] = df\n        print(f\"Loaded {key}: {len(df):,} rows, {len(df.columns)} columns\")\n        return df\n    \n    @property\n    def prices(self) -&gt; pd.DataFrame:\n        return self._read('prices')\n    \n    @property\n    def ownership(self) -&gt; pd.DataFrame:\n        return self._read('ownership')\n    \n    @property\n    def major_shareholders(self) -&gt; pd.DataFrame:\n        return self._read('major_shareholders')\n    \n    @property\n    def corporate_actions(self) -&gt; pd.DataFrame:\n        return self._read('corporate_actions')\n    \n    @property\n    def company_profile(self) -&gt; pd.DataFrame:\n        return self._read('company_profile')\n    \n    @property\n    def financials(self) -&gt; pd.DataFrame:\n        return self._read('financials')\n    \n    @property\n    def foreign_ownership(self) -&gt; pd.DataFrame:\n        return self._read('foreign_ownership')\n    \n    @property\n    def fund_holdings(self) -&gt; pd.DataFrame:\n        return self._read('fund_holdings')\n    \n    def clear_cache(self):\n        \"\"\"Clear all cached datasets to free memory.\"\"\"\n        self._cache.clear()\n\n# Initialize reader — adjust path to your local DataCore.vn data\n# dc = DataCoreReader('/path/to/datacore_data', file_format='parquet')\n\nThis chapter proceeds as follows. Section 33.3 builds the complete data pipeline from raw DataCore.vn extracts to clean, analysis-ready datasets, with particular attention to corporate action adjustments. Section 33.4 defines Vietnam’s unique ownership taxonomy. Section 33.5 computes institutional ownership ratios, concentration, and breadth for the Vietnamese market. Section 33.6 develops specialized foreign ownership analytics including FOL utilization and room premium. Section 33.7 derives institutional trades from ownership disclosure snapshots. Section 33.8 computes fund-level flows and turnover. Section 33.9 analyzes state ownership dynamics. Section 33.10 introduces network analysis, ML classification, and event-study frameworks. Section 33.11 presents complete empirical applications, and Section 33.12 concludes.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-data-pipeline",
    "href": "50_institutional_ownership.html#sec-data-pipeline",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.3 Data Pipeline",
    "text": "33.3 Data Pipeline\n\n33.3.1 Stock Price Data and Corporate Action Adjustments\nVietnam’s equity market is notorious for frequent corporate actions, particularly stock dividends and bonus share issuances, that dramatically alter share counts. A company issuing a 30% stock dividend means every 100 shares become 130 shares, and the reference price adjusts downward proportionally. Failure to properly adjust historical shares and prices for these events is the single most common source of error in Vietnamese equity research.\n\n# ============================================================================\n# Step 1: Corporate Action Adjustment Factors\n# ============================================================================\n\ndef build_adjustment_factors(corporate_actions: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Build cumulative adjustment factors from the corporate actions history.\n    \n    In Vietnam, the most common share-altering corporate actions are:\n    1. Stock dividends (cổ tức bằng cổ phiếu): e.g., 30% → ratio = 0.30\n       Effect: shares × (1 + 0.30), price × (1 / 1.30)\n    2. Bonus shares (thưởng cổ phiếu): mechanically identical to stock dividends\n    3. Stock splits (chia tách): e.g., 2:1 → ratio = 2.0\n       Effect: shares × 2, price × 0.5\n    4. Rights issues (phát hành thêm): dilutive, but not all shareholders exercise\n       We approximate with the subscription ratio\n    5. Reverse splits (gộp cổ phiếu): rare in Vietnam\n       Effect: shares ÷ ratio, price × ratio\n    \n    We construct a FORWARD-LOOKING cumulative adjustment factor such that:\n       adjusted_shares = raw_shares × cum_adj_factor(from_date, to_date)\n       adjusted_price = raw_price / cum_adj_factor(from_date, to_date)\n    \n    This is analogous to CRSP's cfacshr in the US context.\n    \n    Parameters\n    ----------\n    corporate_actions : pd.DataFrame\n        DataCore.vn corporate actions with columns:\n        ticker, ex_date, action_type, ratio\n        \n        action_type values:\n        - 'stock_dividend': ratio = dividend rate (e.g., 0.30 for 30%)\n        - 'bonus_shares': ratio = bonus rate (e.g., 0.20 for 20%)\n        - 'stock_split': ratio = split factor (e.g., 2.0 for 2:1)\n        - 'reverse_split': ratio = merge factor (e.g., 5.0 for 5:1 merge)\n        - 'rights_issue': ratio = subscription rate (e.g., 0.10 for 10:1)\n        - 'cash_dividend': ratio = VND per share (no share adjustment needed)\n    \n    Returns\n    -------\n    pd.DataFrame\n        Adjustment factors: ticker, ex_date, point_factor, cum_factor\n    \"\"\"\n    # Filter to share-altering events only\n    share_events = ['stock_dividend', 'bonus_shares', 'stock_split', \n                    'reverse_split', 'rights_issue']\n    ca = corporate_actions[\n        corporate_actions['action_type'].isin(share_events)\n    ].copy()\n    \n    if len(ca) == 0:\n        print(\"No share-altering corporate actions found.\")\n        return pd.DataFrame(columns=['ticker', 'ex_date', 'point_factor', 'cum_factor'])\n    \n    # Compute point adjustment factor for each event\n    def compute_point_factor(row):\n        atype = row['action_type']\n        ratio = row['ratio']\n        \n        if atype in ['stock_dividend', 'bonus_shares']:\n            # 30% stock dividend: 100 shares → 130 shares\n            return 1 + ratio\n        elif atype == 'stock_split':\n            # 2:1 split: 100 shares → 200 shares\n            return ratio\n        elif atype == 'reverse_split':\n            # 5:1 reverse: 500 shares → 100 shares\n            return 1.0 / ratio\n        elif atype == 'rights_issue':\n            # Approximate: assume all rights exercised\n            # In practice, this overestimates the adjustment\n            return 1 + ratio\n        else:\n            return 1.0\n    \n    ca['point_factor'] = ca.apply(compute_point_factor, axis=1)\n    \n    # Sort chronologically within each ticker\n    ca = ca.sort_values(['ticker', 'ex_date']).reset_index(drop=True)\n    \n    # Cumulative factor: product of all point factors from listing to date\n    # This gives us a running \"total adjustment\" for each ticker\n    ca['cum_factor'] = ca.groupby('ticker')['point_factor'].cumprod()\n    \n    # Summary statistics\n    n_tickers = ca['ticker'].nunique()\n    n_events = len(ca)\n    avg_events = n_events / n_tickers if n_tickers &gt; 0 else 0\n    \n    print(f\"Corporate action adjustment factors built:\")\n    print(f\"  Tickers with adjustments: {n_tickers:,}\")\n    print(f\"  Total share-altering events: {n_events:,}\")\n    print(f\"  Average events per ticker: {avg_events:.1f}\")\n    print(f\"\\nEvent type distribution:\")\n    print(ca['action_type'].value_counts().to_string())\n    \n    return ca[['ticker', 'ex_date', 'action_type', 'ratio', \n               'point_factor', 'cum_factor']]\n\n\ndef adjust_shares(shares: float, ticker: str, from_date, to_date, \n                  adj_factors: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Adjust a share count from one date to another for corporate actions.\n    \n    Example: If a company had a 30% stock dividend with ex_date between\n    from_date and to_date, then 1000 shares at from_date = 1300 shares \n    at to_date.\n    \n    Parameters\n    ----------\n    shares : float\n        Number of shares at from_date\n    ticker : str\n        Stock ticker\n    from_date, to_date : pd.Timestamp\n        Period for adjustment\n    adj_factors : pd.DataFrame\n        Output of build_adjustment_factors()\n    \n    Returns\n    -------\n    float\n        Adjusted shares at to_date\n    \"\"\"\n    events = adj_factors[\n        (adj_factors['ticker'] == ticker) &\n        (adj_factors['ex_date'] &gt; pd.Timestamp(from_date)) &\n        (adj_factors['ex_date'] &lt;= pd.Timestamp(to_date))\n    ]\n    \n    if len(events) == 0:\n        return shares\n    \n    total_factor = events['point_factor'].prod()\n    return shares * total_factor\n\n\n# Example usage:\n# adj_factors = build_adjustment_factors(dc.corporate_actions)\n\n\n\n\n\n\n\nImportantThe Stock Dividend Problem in Vietnam\n\n\n\nVietnamese companies issue stock dividends with remarkable frequency, many growth companies do so 2-3 times per year. Consider Vinhomes (VHM) or FPT Corporation: their share counts may double or triple over a 5-year period purely from stock dividends. If you compare raw ownership shares from 2019 to 2024 without adjustment, you will obtain nonsensical ownership ratios. Every time-series analysis of Vietnamese ownership data must use adjusted shares. This is the Vietnamese equivalent of the CRSP cfacshr adjustment factor problem in US data, but more severe because the events are more frequent and larger in magnitude.\n\n\n\n# ============================================================================\n# Step 2: Process Stock Price Data\n# ============================================================================\n\ndef process_price_data(prices: pd.DataFrame, \n                       adj_factors: pd.DataFrame,\n                       company_profile: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Process DataCore.vn stock price data:\n    1. Align dates to month-end and quarter-end\n    2. Merge company metadata (exchange, sector, FOL limit)\n    3. Compute adjusted prices and shares outstanding\n    4. Compute market capitalization\n    5. Create quarter-end snapshots\n    \n    Parameters\n    ----------\n    prices : pd.DataFrame\n        Daily/monthly price data from DataCore.vn\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors\n    company_profile : pd.DataFrame\n        Company metadata including exchange, sector, FOL\n    \n    Returns\n    -------\n    pd.DataFrame\n        Quarter-end processed stock data\n    \"\"\"\n    df = prices.copy()\n    \n    # Standardize date\n    df['date'] = pd.to_datetime(df['date'])\n    df['month_end'] = df['date'] + pd.offsets.MonthEnd(0)\n    df['quarter_end'] = df['date'] + pd.offsets.QuarterEnd(0)\n    \n    # Merge company profile\n    profile_cols = ['ticker', 'exchange', 'industry_code', 'fol_limit', \n                    'listing_date', 'company_name']\n    profile_cols = [c for c in profile_cols if c in company_profile.columns]\n    df = df.merge(company_profile[profile_cols], on='ticker', how='left')\n    \n    # Build cumulative adjustment factor for each ticker-date\n    # For each observation, compute the total adjustment from listing to that date\n    df = df.sort_values(['ticker', 'date'])\n    \n    # Merge adjustment events\n    # For each ticker-date, find the cumulative factor as of that date\n    def get_cum_factor_at_date(group):\n        ticker = group.name\n        ticker_adj = adj_factors[adj_factors['ticker'] == ticker].copy()\n        \n        if len(ticker_adj) == 0:\n            group['cum_adj_factor'] = 1.0\n            return group\n        \n        # For each date, find cumulative factor (product of all events up to that date)\n        group = group.sort_values('date')\n        group['cum_adj_factor'] = 1.0\n        \n        for _, event in ticker_adj.iterrows():\n            mask = group['date'] &gt;= event['ex_date']\n            group.loc[mask, 'cum_adj_factor'] *= event['point_factor']\n        \n        return group\n    \n    df = df.groupby('ticker', group_keys=False).apply(get_cum_factor_at_date)\n    \n    # Adjusted price and shares\n    # adjusted_close should already be provided by DataCore.vn\n    # But we compute our own for consistency\n    if 'adjusted_close' not in df.columns:\n        df['adjusted_close'] = df['close'] / df['cum_adj_factor']\n    \n    # Adjusted shares outstanding\n    df['adjusted_shares'] = df['shares_outstanding'] * df['cum_adj_factor']\n    \n    # Market capitalization (in billion VND)\n    df['market_cap'] = df['close'] * df['shares_outstanding'] / 1e9\n    \n    # Monthly returns\n    df = df.sort_values(['ticker', 'date'])\n    df['ret'] = df.groupby('ticker')['adjusted_close'].pct_change()\n    \n    # Keep quarter-end observations\n    # For daily data: keep last trading day of each quarter\n    df_quarterly = (df.sort_values(['ticker', 'quarter_end', 'date'])\n                      .groupby(['ticker', 'quarter_end'])\n                      .last()\n                      .reset_index())\n    \n    print(f\"Processed price data:\")\n    print(f\"  Total records (daily): {len(df):,}\")\n    print(f\"  Quarter-end records: {len(df_quarterly):,}\")\n    print(f\"  Unique tickers: {df_quarterly['ticker'].nunique():,}\")\n    print(f\"  Date range: {df_quarterly['quarter_end'].min()} to \"\n          f\"{df_quarterly['quarter_end'].max()}\")\n    print(f\"\\nExchange distribution:\")\n    print(df_quarterly.groupby('exchange')['ticker'].nunique().to_string())\n    \n    return df_quarterly\n\n# prices_q = process_price_data(dc.prices, adj_factors, dc.company_profile)\n\n\n\n33.3.2 Ownership Structure Data\nVietnamese ownership data captures the composition of shareholders as disclosed in annual reports, semi-annual reports, and event-driven disclosures. The key distinction from US 13F data is that Vietnamese disclosures provide a complete ownership decomposition, not just institutional long positions, but the full breakdown into state, institutional, foreign, and individual ownership.\n\n# ============================================================================\n# Step 3: Process Ownership Structure Data\n# ============================================================================\n\nclass OwnershipType:\n    \"\"\"\n    Vietnam's ownership taxonomy.\n    \n    Unlike the US where 13F captures only institutional long positions,\n    Vietnamese disclosure provides a complete ownership decomposition.\n    We classify shareholders into five mutually exclusive categories.\n    \"\"\"\n    STATE = 'state'                    # Nhà nước (government entities, SOE parents)\n    FOREIGN_INST = 'foreign_inst'      # Tổ chức nước ngoài\n    DOMESTIC_INST = 'domestic_inst'    # Tổ chức trong nước (non-state)\n    INDIVIDUAL = 'individual'          # Cá nhân\n    TREASURY = 'treasury'              # Cổ phiếu quỹ\n    \n    ALL_TYPES = [STATE, FOREIGN_INST, DOMESTIC_INST, INDIVIDUAL, TREASURY]\n    INSTITUTIONAL = [STATE, FOREIGN_INST, DOMESTIC_INST]\n    FOREIGN = [FOREIGN_INST]  # Can be expanded if foreign individuals are tracked\n\n\ndef classify_shareholders(ownership: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Classify shareholders into Vietnam's ownership taxonomy.\n    \n    DataCore.vn may provide a `shareholder_type` field, but naming \n    conventions vary. This function standardizes the classification \n    using a combination of provided flags and name-based heuristics.\n    \n    The classification challenge in Vietnam (noted by @huang2023factors):\n    DataCore.vn may not always cleanly separate institution types, so we \n    use a cascading approach:\n    1. Use explicit flags (is_state, is_foreign, is_institution) if available\n    2. Apply name-based heuristics for Vietnamese entity names\n    3. Default to 'individual' for unclassified shareholders\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Raw ownership data from DataCore.vn\n    \n    Returns\n    -------\n    pd.DataFrame\n        Ownership data with standardized `owner_type` column\n    \"\"\"\n    df = ownership.copy()\n    \n    # --- Method 1: Use explicit flags if available ---\n    if all(col in df.columns for col in ['is_state', 'is_foreign', 'is_institution']):\n        conditions = [\n            (df['is_state'] == True),\n            (df['is_foreign'] == True) & (df['is_institution'] == True),\n            (df['is_foreign'] == True) & (df['is_institution'] != True),\n            (df['is_institution'] == True) & (df['is_state'] != True) & \n                (df['is_foreign'] != True),\n        ]\n        choices = [\n            OwnershipType.STATE,\n            OwnershipType.FOREIGN_INST,\n            OwnershipType.FOREIGN_INST,  # Foreign individuals often grouped\n            OwnershipType.DOMESTIC_INST,\n        ]\n        df['owner_type'] = np.select(conditions, choices, \n                                      default=OwnershipType.INDIVIDUAL)\n    \n    # --- Method 2: Name-based heuristics ---\n    elif 'shareholder_name' in df.columns:\n        name = df['shareholder_name'].str.lower().fillna('')\n        \n        # State entities: government ministries, SCIC, state corporations\n        state_keywords = [\n            'bộ tài chính', 'tổng công ty đầu tư', 'scic', \n            'ủy ban nhân dân', 'nhà nước', 'state capital',\n            'tổng công ty', 'vốn nhà nước', 'bộ công thương',\n            'bộ quốc phòng', 'bộ giao thông', 'vinashin',\n        ]\n        is_state = name.apply(\n            lambda x: any(kw in x for kw in state_keywords)\n        )\n        \n        # Foreign entities: common fund names, foreign company patterns\n        foreign_keywords = [\n            'fund', 'investment', 'capital', 'limited', 'ltd', 'inc',\n            'corporation', 'holdings', 'asset management', 'pte',\n            'gmbh', 'management', 'partners', 'advisors',\n            'dragon capital', 'vinacapital', 'templeton', \n            'blackrock', 'jpmorgan', 'samsung', 'mirae',\n        ]\n        # Also check for non-Vietnamese characters as a heuristic\n        is_foreign_name = name.apply(\n            lambda x: any(kw in x for kw in foreign_keywords)\n        )\n        \n        # Domestic institutions: Vietnamese bank, securities, insurance names\n        domestic_inst_keywords = [\n            'ngân hàng', 'chứng khoán', 'bảo hiểm', 'quỹ đầu tư',\n            'công ty quản lý', 'bảo việt', 'techcombank', 'vietcombank',\n            'bidv', 'vietinbank', 'vpbank', 'mb bank', 'ssi', 'hsc',\n            'vcsc', 'vndirect', 'fpt capital', 'manulife',\n        ]\n        is_domestic_inst = name.apply(\n            lambda x: any(kw in x for kw in domestic_inst_keywords)\n        )\n        \n        # Treasury shares\n        is_treasury = name.str.contains('cổ phiếu quỹ|treasury', case=False)\n        \n        # Apply classification cascade\n        df['owner_type'] = OwnershipType.INDIVIDUAL  # Default\n        df.loc[is_domestic_inst, 'owner_type'] = OwnershipType.DOMESTIC_INST\n        df.loc[is_foreign_name, 'owner_type'] = OwnershipType.FOREIGN_INST\n        df.loc[is_state, 'owner_type'] = OwnershipType.STATE\n        df.loc[is_treasury, 'owner_type'] = OwnershipType.TREASURY\n    \n    # --- Method 3: Use shareholder_type directly ---\n    elif 'shareholder_type' in df.columns:\n        type_map = {\n            'state': OwnershipType.STATE,\n            'foreign_institution': OwnershipType.FOREIGN_INST,\n            'foreign_individual': OwnershipType.FOREIGN_INST,\n            'domestic_institution': OwnershipType.DOMESTIC_INST,\n            'individual': OwnershipType.INDIVIDUAL,\n            'treasury': OwnershipType.TREASURY,\n        }\n        df['owner_type'] = df['shareholder_type'].str.lower().map(type_map)\n        df['owner_type'] = df['owner_type'].fillna(OwnershipType.INDIVIDUAL)\n    \n    else:\n        raise ValueError(\n            \"Cannot classify shareholders. Expected one of:\\n\"\n            \"  1. Columns: is_state, is_foreign, is_institution\\n\"\n            \"  2. Column: shareholder_name (for heuristic classification)\\n\"\n            \"  3. Column: shareholder_type (pre-classified)\"\n        )\n    \n    # Summary\n    print(\"Ownership classification results:\")\n    print(df['owner_type'].value_counts().to_string())\n    \n    return df\n\n# ownership_classified = classify_shareholders(dc.ownership)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-ownership-taxonomy",
    "href": "50_institutional_ownership.html#sec-ownership-taxonomy",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.4 Vietnam’s Ownership Taxonomy",
    "text": "33.4 Vietnam’s Ownership Taxonomy\n\n33.4.1 The Five Ownership Categories\nVietnam’s ownership structure is decomposed into five mutually exclusive categories that together sum to 100% of shares outstanding:\n\n\n\nTable 33.2: Vietnam’s Ownership Taxonomy\n\n\n\n\n\n\n\n\n\n\n\nCategory\nVietnamese Term\nDescription\nTypical Share (2020s)\n\n\n\n\nState\nSở hữu Nhà nước\nGovernment entities, SCIC, SOE parent companies\n~15-25% of market cap\n\n\nForeign Institutional\nTổ chức nước ngoài\nForeign funds, banks, corporations\n~15-20%\n\n\nDomestic Institutional\nTổ chức trong nước\nVietnamese funds, banks, insurance, securities firms\n~5-10%\n\n\nIndividual\nCá nhân\nRetail investors (both Vietnamese and foreign individuals)\n~55-65%\n\n\nTreasury\nCổ phiếu quỹ\nCompany’s own repurchased shares\n~0-2%\n\n\n\n\n\n\nThis taxonomy differs fundamentally from the US 13F framework in several ways:\n\nCompleteness: We observe 100% of ownership, not just institutional long positions above $100 million AUM.\nState as a category: State ownership is a first-class analytical category, not subsumed under “All Others” as in the LSEG type code system.\nIndividual visibility: We observe aggregate individual ownership directly, whereas in the US, individual ownership is merely the residual (100% − institutional ownership).\nNo short position ambiguity: Vietnam’s market has very limited short-selling infrastructure, so ownership data genuinely represents long positions.\n\n\n# ============================================================================\n# Step 4: Compute Ownership Decomposition\n# ============================================================================\n\ndef compute_ownership_decomposition(ownership: pd.DataFrame,\n                                     prices_q: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the full ownership decomposition for each stock at each \n    disclosure date.\n    \n    For each stock-date combination, aggregates shares held by each \n    ownership category and computes ownership ratios relative to \n    total shares outstanding.\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership data (output of classify_shareholders)\n    prices_q : pd.DataFrame\n        Quarter-end price data with shares_outstanding\n    \n    Returns\n    -------\n    pd.DataFrame\n        Stock-period level ownership decomposition with columns for\n        each ownership type's share count and percentage\n    \"\"\"\n    # Aggregate shares by ticker, date, and owner type\n    agg = (ownership.groupby(['ticker', 'date', 'owner_type'])['shares_held']\n                    .sum()\n                    .reset_index())\n    \n    # Pivot to wide format: one column per ownership type\n    wide = agg.pivot_table(\n        index=['ticker', 'date'],\n        columns='owner_type',\n        values='shares_held',\n        fill_value=0\n    ).reset_index()\n    \n    # Rename columns\n    type_cols = [c for c in wide.columns if c in OwnershipType.ALL_TYPES]\n    rename_map = {t: f'shares_{t}' for t in type_cols}\n    wide = wide.rename(columns=rename_map)\n    \n    # Total institutional shares\n    inst_cols = [f'shares_{t}' for t in OwnershipType.INSTITUTIONAL \n                 if f'shares_{t}' in wide.columns]\n    wide['shares_institutional'] = wide[inst_cols].sum(axis=1)\n    \n    # Total foreign shares (for FOL tracking)\n    foreign_cols = [f'shares_{t}' for t in OwnershipType.FOREIGN \n                    if f'shares_{t}' in wide.columns]\n    wide['shares_foreign_total'] = wide[foreign_cols].sum(axis=1)\n    \n    # Align with quarter-end dates for merging with price data\n    wide['quarter_end'] = wide['date'] + pd.offsets.QuarterEnd(0)\n    \n    # Merge with price data to get shares outstanding\n    merged = wide.merge(\n        prices_q[['ticker', 'quarter_end', 'shares_outstanding', \n                  'adjusted_shares', 'market_cap', 'exchange', \n                  'industry_code', 'fol_limit', 'close']],\n        on=['ticker', 'quarter_end'],\n        how='left'\n    )\n    \n    # Compute ownership ratios\n    tso = merged['shares_outstanding']\n    for col in merged.columns:\n        if col.startswith('shares_') and col != 'shares_outstanding':\n            ratio_col = col.replace('shares_', 'pct_')\n            merged[ratio_col] = merged[col] / tso\n            merged.loc[tso &lt;= 0, ratio_col] = np.nan\n    \n    # Derived measures\n    merged['pct_free_float'] = 1 - merged.get('pct_state', 0) - merged.get('pct_treasury', 0)\n    \n    # SOE flag: state ownership &gt; 50%\n    merged['is_soe'] = (merged.get('pct_state', 0) &gt; 0.50).astype(int)\n    \n    # FOL utilization\n    if 'fol_limit' in merged.columns and 'pct_foreign_total' in merged.columns:\n        merged['fol_utilization'] = merged['pct_foreign_total'] / merged['fol_limit']\n        merged['foreign_room'] = merged['fol_limit'] - merged['pct_foreign_total']\n        merged.loc[merged['fol_limit'] &lt;= 0, ['fol_utilization', 'foreign_room']] = np.nan\n    \n    # Number of institutional owners (breadth)\n    n_owners = (ownership[ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)]\n                .groupby(['ticker', 'date'])['shareholder_name']\n                .nunique()\n                .reset_index()\n                .rename(columns={'shareholder_name': 'n_inst_owners'}))\n    \n    n_foreign_owners = (ownership[ownership['owner_type'] == OwnershipType.FOREIGN_INST]\n                        .groupby(['ticker', 'date'])['shareholder_name']\n                        .nunique()\n                        .reset_index()\n                        .rename(columns={'shareholder_name': 'n_foreign_owners'}))\n    \n    merged = merged.merge(n_owners, on=['ticker', 'date'], how='left')\n    merged = merged.merge(n_foreign_owners, on=['ticker', 'date'], how='left')\n    merged[['n_inst_owners', 'n_foreign_owners']] = (\n        merged[['n_inst_owners', 'n_foreign_owners']].fillna(0)\n    )\n    \n    print(f\"Ownership decomposition computed:\")\n    print(f\"  Stock-period observations: {len(merged):,}\")\n    print(f\"  Unique tickers: {merged['ticker'].nunique():,}\")\n    print(f\"\\nMean ownership structure:\")\n    pct_cols = [c for c in merged.columns if c.startswith('pct_')]\n    print(merged[pct_cols].mean().round(4).to_string())\n    \n    return merged\n\n# ownership_decomp = compute_ownership_decomposition(\n#     ownership_classified, prices_q\n# )",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-ownership-metrics",
    "href": "50_institutional_ownership.html#sec-ownership-metrics",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.5 Institutional Ownership Measures",
    "text": "33.5 Institutional Ownership Measures\n\n33.5.1 Ownership Ratio\nThe Institutional Ownership Ratio (IOR) for stock \\(i\\) at time \\(t\\) in Vietnam is:\n\\[\nIOR_{i,t} = \\frac{S_{i,t}^{state} + S_{i,t}^{foreign\\_inst} + S_{i,t}^{domestic\\_inst}}{TSO_{i,t}}\n\\tag{33.1}\\]\nwhere \\(S_{i,t}^{type}\\) denotes adjusted shares held by each ownership category and \\(TSO_{i,t}\\) is total shares outstanding. Unlike the US where the IOR can exceed 100% due to long-only reporting and short selling, the Vietnamese IOR is bounded by construction in \\([0, 1]\\) because we observe the complete ownership decomposition.\nWe also compute category-specific ownership ratios:\n\\[\n\\begin{aligned}\nIOR_{i,t}^{foreign} &= \\frac{S_{i,t}^{foreign\\_inst}}{TSO_{i,t}},\\\\\nIOR_{i,t}^{state} &= \\frac{S_{i,t}^{state}}{TSO_{i,t}},\\\\\nIOR_{i,t}^{domestic} &= \\frac{S_{i,t}^{domestic\\_inst}}{TSO_{i,t}}\n\\end{aligned}\n\\tag{33.2}\\]\n\n\n33.5.2 Concentration: Herfindahl-Hirschman Index\nThe Institutional Ownership Concentration via the Herfindahl-Hirschman Index is:\n\\[\nIOC_{i,t}^{HHI} = \\sum_{j=1}^{N_{i,t}} \\left(\\frac{S_{i,j,t}}{\\sum_{k=1}^{N_{i,t}} S_{i,k,t}}\\right)^2\n\\tag{33.3}\\]\nIn Vietnam, the HHI is particularly informative because it captures the dominance of state shareholders. A company where the government holds 65% will have a mechanically high HHI even if the remaining 35% is diversely held.\nWe therefore compute separate HHI measures for different ownership categories:\n\\[\nHHI_{i,t}^{total} = \\sum_{j} w_{i,j,t}^2, \\quad\nHHI_{i,t}^{non-state} = \\sum_{j \\notin state} \\left(\\frac{S_{i,j,t}}{\\sum_{k \\notin state} S_{i,k,t}}\\right)^2\n\\tag{33.4}\\]\nThe non-state HHI is more comparable to the US institutional HHI, as it captures concentration among market-driven investors.\n\n\n33.5.3 Breadth of Ownership\nFollowing Chen, Hong, and Stein (2002), Institutional Breadth (\\(N_{i,t}\\)) is the number of institutional investors holding stock \\(i\\) in period \\(t\\). The Change in Breadth is:\n\\[\n\\Delta Breadth_{i,t} = \\frac{N_{i,t}^{cont} - N_{i,t-1}^{cont}}{TotalInstitutions_{t-1}}\n\\tag{33.5}\\]\nwhere \\(N_{i,t}^{cont}\\) counts only institutions that appear in the disclosure universe in both periods \\(t\\) and \\(t-1\\), following the Lehavy and Sloan (2008) algorithm. This adjustment is particularly important in Vietnam where:\n\nNew funds launch frequently (especially ETFs tracking VN30)\nForeign funds enter and exit the market\nDomestic securities firms consolidate or spin off asset management divisions\n\n\n# ============================================================================\n# Step 5: Compute All IO Metrics\n# ============================================================================\n\ndef compute_io_metrics_vietnam(ownership: pd.DataFrame,\n                                ownership_decomp: pd.DataFrame,\n                                adj_factors: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute security-level institutional ownership metrics adapted for Vietnam.\n    \n    Computes:\n    1. Ownership ratios by category (state, foreign, domestic inst, individual)\n    2. HHI concentration (total, non-state, foreign-only)\n    3. Number of institutional owners (total, foreign, domestic)\n    4. Change in breadth (Lehavy-Sloan adjusted)\n    5. FOL-related metrics (utilization, room, near-cap indicator)\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership data with individual shareholder records\n    ownership_decomp : pd.DataFrame\n        Aggregated ownership decomposition (output of compute_ownership_decomposition)\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors\n    \n    Returns\n    -------\n    pd.DataFrame\n        Stock-period level metrics\n    \"\"\"\n    # Start with the ownership decomposition\n    metrics = ownership_decomp.copy()\n    \n    # --- HHI Concentration ---\n    # Total HHI: across all institutional shareholders\n    inst_ownership = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)\n    ].copy()\n    \n    def compute_hhi_group(group):\n        \"\"\"Compute HHI for a group of shareholders.\"\"\"\n        total = group['shares_held'].sum()\n        if total &lt;= 0:\n            return np.nan\n        weights = group['shares_held'] / total\n        return (weights ** 2).sum()\n    \n    # Total institutional HHI\n    hhi_total = (inst_ownership.groupby(['ticker', 'date'])\n                               .apply(compute_hhi_group)\n                               .reset_index(name='hhi_institutional'))\n    metrics = metrics.merge(hhi_total, on=['ticker', 'date'], how='left')\n    \n    # Non-state HHI (exclude state shareholders)\n    non_state = ownership[\n        ownership['owner_type'].isin([OwnershipType.FOREIGN_INST, \n                                       OwnershipType.DOMESTIC_INST])\n    ]\n    hhi_nonstate = (non_state.groupby(['ticker', 'date'])\n                             .apply(compute_hhi_group)\n                             .reset_index(name='hhi_non_state'))\n    metrics = metrics.merge(hhi_nonstate, on=['ticker', 'date'], how='left')\n    \n    # Foreign-only HHI\n    foreign_only = ownership[ownership['owner_type'] == OwnershipType.FOREIGN_INST]\n    hhi_foreign = (foreign_only.groupby(['ticker', 'date'])\n                               .apply(compute_hhi_group)\n                               .reset_index(name='hhi_foreign'))\n    metrics = metrics.merge(hhi_foreign, on=['ticker', 'date'], how='left')\n    \n    # --- Change in Breadth (Lehavy-Sloan Algorithm) ---\n    metrics = metrics.sort_values(['ticker', 'date'])\n    \n    # Get list of all institutions filing in each period\n    inst_by_period = (inst_ownership.groupby('date')['shareholder_name']\n                                     .apply(set)\n                                     .to_dict())\n    \n    # For each stock-period: count continuing institutions\n    def compute_breadth_change(group):\n        group = group.sort_values('date').reset_index(drop=True)\n        group['dbreadth'] = np.nan\n        \n        for i in range(1, len(group)):\n            current_date = group.loc[i, 'date']\n            prev_date = group.loc[i-1, 'date']\n            \n            # Institutions in universe for both periods\n            current_universe = inst_by_period.get(current_date, set())\n            prev_universe = inst_by_period.get(prev_date, set())\n            continuing_universe = current_universe & prev_universe\n            \n            if len(prev_universe) == 0:\n                continue\n            \n            # Count continuing institutions holding this stock in each period\n            ticker = group.loc[i, 'ticker']\n            \n            current_holders = set(\n                inst_ownership[\n                    (inst_ownership['ticker'] == ticker) & \n                    (inst_ownership['date'] == current_date)\n                ]['shareholder_name']\n            )\n            prev_holders = set(\n                inst_ownership[\n                    (inst_ownership['ticker'] == ticker) & \n                    (inst_ownership['date'] == prev_date)\n                ]['shareholder_name']\n            )\n            \n            # Count only continuing institutions\n            n_current_cont = len(current_holders & continuing_universe)\n            n_prev_cont = len(prev_holders & continuing_universe)\n            \n            group.loc[i, 'dbreadth'] = (\n                (n_current_cont - n_prev_cont) / len(prev_universe)\n            )\n        \n        return group\n    \n    metrics = metrics.groupby('ticker', group_keys=False).apply(compute_breadth_change)\n    \n    # --- FOL Indicators ---\n    if 'fol_utilization' in metrics.columns:\n        metrics['near_fol_cap'] = (metrics['fol_utilization'] &gt; 0.90).astype(int)\n        metrics['at_fol_cap'] = (metrics['fol_utilization'] &gt; 0.98).astype(int)\n    \n    print(f\"IO metrics computed for Vietnam:\")\n    print(f\"  Observations: {len(metrics):,}\")\n    print(f\"\\nKey metric distributions:\")\n    summary_cols = ['pct_institutional', 'pct_state', 'pct_foreign_total',\n                    'hhi_institutional', 'n_inst_owners', 'dbreadth']\n    summary_cols = [c for c in summary_cols if c in metrics.columns]\n    print(metrics[summary_cols].describe().round(4).to_string())\n    \n    return metrics\n\n# io_metrics = compute_io_metrics_vietnam(\n#     ownership_classified, ownership_decomp, adj_factors\n# )\n\n\n\n33.5.4 Time Series Visualization\n\n\n\ndef plot_ownership_timeseries_vietnam(metrics: pd.DataFrame):\n    \"\"\"\n    Create publication-quality time series plots of Vietnamese \n    ownership structure evolution.\n    \"\"\"\n    fig, axes = plt.subplots(3, 1, figsize=(12, 14))\n    \n    # Aggregate across all stocks (market-cap weighted)\n    ts = metrics.groupby('quarter_end').apply(\n        lambda g: pd.Series({\n            'pct_state': np.average(g['pct_state'].fillna(0), \n                                     weights=g['market_cap'].fillna(1)),\n            'pct_foreign': np.average(g['pct_foreign_total'].fillna(0), \n                                       weights=g['market_cap'].fillna(1)),\n            'pct_domestic_inst': np.average(g['pct_domestic_inst'].fillna(0), \n                                             weights=g['market_cap'].fillna(1)),\n            'pct_individual': np.average(g['pct_individual'].fillna(0), \n                                          weights=g['market_cap'].fillna(1)),\n            'n_stocks': g['ticker'].nunique(),\n            'total_mktcap': g['market_cap'].sum(),\n            'median_n_inst': g['n_inst_owners'].median(),\n            'median_hhi': g['hhi_institutional'].median(),\n            'pct_soe': g['is_soe'].mean(),\n        })\n    ).reset_index()\n    \n    # ---- Panel A: Ownership Composition (Stacked Area) ----\n    ax = axes[0]\n    dates = ts['quarter_end']\n    ax.stackplot(dates,\n                 ts['pct_state'] * 100,\n                 ts['pct_foreign'] * 100,\n                 ts['pct_domestic_inst'] * 100,\n                 ts['pct_individual'] * 100,\n                 labels=['State', 'Foreign Institutional', \n                         'Domestic Institutional', 'Individual'],\n                 colors=[OWNER_COLORS['State'], OWNER_COLORS['Foreign Institutional'],\n                         OWNER_COLORS['Domestic Institutional'], OWNER_COLORS['Individual']],\n                 alpha=0.8)\n    ax.set_ylabel('Ownership Share (%)')\n    ax.set_title('Panel A: Ownership Composition of Vietnamese Listed Companies '\n                 '(Market-Cap Weighted)')\n    ax.legend(loc='upper right', frameon=True, framealpha=0.9)\n    ax.set_ylim(0, 100)\n    \n    # ---- Panel B: Institutional Ownership by Component ----\n    ax = axes[1]\n    ax.plot(dates, ts['pct_state'] * 100, label='State',\n            color=OWNER_COLORS['State'], linewidth=2)\n    ax.plot(dates, ts['pct_foreign'] * 100, label='Foreign Institutional',\n            color=OWNER_COLORS['Foreign Institutional'], linewidth=2)\n    ax.plot(dates, ts['pct_domestic_inst'] * 100, label='Domestic Institutional',\n            color=OWNER_COLORS['Domestic Institutional'], linewidth=2)\n    total_inst = (ts['pct_state'] + ts['pct_foreign'] + ts['pct_domestic_inst']) * 100\n    ax.plot(dates, total_inst, label='Total Institutional',\n            color=OWNER_COLORS['Total Institutional'], linewidth=2.5, linestyle='--')\n    ax.set_ylabel('Ownership Ratio (%)')\n    ax.set_title('Panel B: Institutional Ownership Components')\n    ax.legend(loc='upper left', frameon=True, framealpha=0.9)\n    \n    # ---- Panel C: Market Structure ----\n    ax = axes[2]\n    ax2 = ax.twinx()\n    ax.plot(dates, ts['n_stocks'], color='#1f77b4', linewidth=2, label='# Listed Stocks')\n    ax2.plot(dates, ts['total_mktcap'] / 1000, color='#d62728', linewidth=2, \n             label='Total Market Cap (Trillion VND)')\n    ax.set_ylabel('Number of Listed Stocks', color='#1f77b4')\n    ax2.set_ylabel('Market Cap (Trillion VND)', color='#d62728')\n    ax.set_title('Panel C: Vietnamese Stock Market Development')\n    \n    # Combine legends\n    lines1, labels1 = ax.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left', framealpha=0.9)\n    \n    plt.tight_layout()\n    plt.savefig('fig_ownership_timeseries_vn.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_ownership_timeseries_vietnam(io_metrics)\n\n\nFigure 33.1\n\n\n\n\n\n\ndef plot_io_by_exchange_size(metrics: pd.DataFrame):\n    \"\"\"Plot IO ratios by exchange and size quintile.\"\"\"\n    df = metrics[metrics['market_cap'].notna() & (metrics['market_cap'] &gt; 0)].copy()\n    \n    # Size quintiles within each quarter\n    df['size_quintile'] = df.groupby('quarter_end')['market_cap'].transform(\n        lambda x: pd.qcut(x, 5, labels=['Q1\\n(Small)', 'Q2', 'Q3', 'Q4', 'Q5\\n(Large)'],\n                          duplicates='drop')\n    )\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n    \n    metrics_to_plot = [\n        ('pct_institutional', 'Total Institutional'),\n        ('pct_foreign_total', 'Foreign Institutional'),\n        ('pct_state', 'State'),\n    ]\n    \n    for ax, (col, title) in zip(axes, metrics_to_plot):\n        for exchange, color in EXCHANGE_COLORS.items():\n            data = df[df['exchange'] == exchange]\n            if len(data) == 0:\n                continue\n            means = data.groupby('size_quintile')[col].mean() * 100\n            ax.bar(np.arange(len(means)) + list(EXCHANGE_COLORS.keys()).index(exchange) * 0.25,\n                   means, width=0.25, label=exchange, color=color, alpha=0.8)\n        \n        ax.set_title(title)\n        ax.set_xlabel('Size Quintile')\n        if ax == axes[0]:\n            ax.set_ylabel('Mean Ownership (%)')\n        ax.legend()\n        ax.set_xticks(np.arange(5) + 0.25)\n        ax.set_xticklabels(['Q1\\n(Small)', 'Q2', 'Q3', 'Q4', 'Q5\\n(Large)'])\n    \n    plt.tight_layout()\n    plt.savefig('fig_io_by_exchange_size.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_io_by_exchange_size(io_metrics)\n\n\nFigure 33.2\n\n\n\n\n\n\nTable 33.3: Summary Statistics of Ownership Structure in Vietnam by Size Quintile and Exchange (Pooled 2010-2024)\n\n\ndef tabulate_io_summary(metrics: pd.DataFrame, start_year: int = 2010) -&gt; pd.DataFrame:\n    \"\"\"\n    Create publication-quality summary table of Vietnamese ownership\n    structure by firm size.\n    \"\"\"\n    df = metrics[\n        (metrics['quarter_end'].dt.year &gt;= start_year) &\n        (metrics['market_cap'].notna()) & (metrics['market_cap'] &gt; 0)\n    ].copy()\n    \n    df['size_quintile'] = df.groupby('quarter_end')['market_cap'].transform(\n        lambda x: pd.qcut(x, 5, labels=['Q1 (Small)', 'Q2', 'Q3', 'Q4', 'Q5 (Large)'],\n                          duplicates='drop')\n    )\n    \n    table = df.groupby('size_quintile').agg(\n        N=('ticker', 'count'),\n        Mean_MktCap=('market_cap', 'mean'),\n        Mean_IO_Total=('pct_institutional', 'mean'),\n        Mean_State=('pct_state', 'mean'),\n        Mean_Foreign=('pct_foreign_total', 'mean'),\n        Mean_Domestic_Inst=('pct_domestic_inst', 'mean'),\n        Mean_Individual=('pct_individual', 'mean'),\n        Median_N_Owners=('n_inst_owners', 'median'),\n        Median_HHI=('hhi_institutional', 'median'),\n        Pct_SOE=('is_soe', 'mean'),\n        Mean_FOL_Util=('fol_utilization', 'mean'),\n    ).round(4)\n    \n    # Format\n    table['N'] = table['N'].apply(lambda x: f\"{x:,.0f}\")\n    table['Mean_MktCap'] = table['Mean_MktCap'].apply(lambda x: f\"{x:,.0f}B VND\")\n    for col in ['Mean_IO_Total', 'Mean_State', 'Mean_Foreign', \n                'Mean_Domestic_Inst', 'Mean_Individual', 'Pct_SOE', 'Mean_FOL_Util']:\n        table[col] = table[col].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"—\")\n    table['Median_N_Owners'] = table['Median_N_Owners'].apply(lambda x: f\"{x:.0f}\")\n    table['Median_HHI'] = table['Median_HHI'].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"—\")\n    \n    table.columns = ['N', 'Mean Mkt Cap', 'IO Total', 'State', 'Foreign', \n                      'Dom. Inst.', 'Individual', 'Med. # Owners', \n                      'Med. HHI', '% SOE', 'FOL Util.']\n    \n    return table\n\n# io_summary = tabulate_io_summary(io_metrics)\n# print(io_summary.to_string())",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-foreign-ownership",
    "href": "50_institutional_ownership.html#sec-foreign-ownership",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.6 Foreign Ownership Dynamics",
    "text": "33.6 Foreign Ownership Dynamics\n\n33.6.1 Foreign Ownership Limits and the FOL Premium\nVietnam’s Foreign Ownership Limits create a unique market segmentation. When a stock reaches its FOL, the only way for a new foreign investor to buy is if an existing foreign holder sells. This creates a de facto “foreign-only” market for FOL-constrained stocks, with documented price premiums (Vo 2015).\nThe FOL Utilization Ratio for stock \\(i\\) at time \\(t\\) is:\n\\[\nFOL\\_Util_{i,t} = \\frac{ForeignOwnership_{i,t}}{FOL\\_Limit_i}\n\\tag{33.6}\\]\nStocks are classified by FOL proximity (Table 33.4).\n\n\n\nTable 33.4: FOL Proximity Zones\n\n\n\n\n\n\n\n\n\n\nFOL Zone\nUtilization Range\nMarket Implication\n\n\n\n\nGreen\n&lt; 50%\nAmple foreign room; normal trading\n\n\nYellow\n50-80%\nModerate room; some foreign interest pressure\n\n\nOrange\n80-95%\nLimited room; foreign premium emerging\n\n\nRed\n95-100%\nNear cap; significant foreign premium\n\n\nCapped\n≈ 100%\nAt limit; foreign-only secondary market\n\n\n\n\n\n\n\n# ============================================================================\n# Step 6: Foreign Ownership Limit Analysis\n# ============================================================================\n\nclass FOLAnalyzer:\n    \"\"\"\n    Analyze Foreign Ownership Limit dynamics in the Vietnamese market.\n    \n    Key analyses:\n    1. FOL utilization tracking and classification\n    2. FOL premium estimation (price impact of being near cap)\n    3. Foreign room dynamics (opening/closing events)\n    4. Cross-sectional determinants of foreign ownership\n    \"\"\"\n    \n    FOL_ZONES = {\n        'Green': (0, 0.50),\n        'Yellow': (0.50, 0.80),\n        'Orange': (0.80, 0.95),\n        'Red': (0.95, 1.00),\n        'Capped': (1.00, 1.50),\n    }\n    \n    def __init__(self, io_metrics: pd.DataFrame,\n                 foreign_daily: Optional[pd.DataFrame] = None):\n        \"\"\"\n        Parameters\n        ----------\n        io_metrics : pd.DataFrame\n            Full ownership metrics from compute_io_metrics_vietnam()\n        foreign_daily : pd.DataFrame, optional\n            Daily foreign ownership tracking from DataCore.vn\n        \"\"\"\n        self.metrics = io_metrics.copy()\n        self.foreign_daily = foreign_daily\n    \n    def classify_fol_zones(self) -&gt; pd.DataFrame:\n        \"\"\"Classify stocks into FOL proximity zones.\"\"\"\n        df = self.metrics.copy()\n        \n        if 'fol_utilization' not in df.columns:\n            print(\"FOL utilization not available in metrics.\")\n            return df\n        \n        conditions = []\n        choices = []\n        for zone, (lo, hi) in self.FOL_ZONES.items():\n            conditions.append(\n                (df['fol_utilization'] &gt;= lo) & (df['fol_utilization'] &lt; hi)\n            )\n            choices.append(zone)\n        \n        df['fol_zone'] = np.select(conditions, choices, default='Unknown')\n        \n        # Summary\n        zone_dist = df.groupby('fol_zone')['ticker'].nunique()\n        print(\"FOL Zone Distribution (unique stocks):\")\n        print(zone_dist.to_string())\n        \n        return df\n    \n    def estimate_fol_premium(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Estimate the FOL premium using a cross-sectional approach.\n        \n        For each period, regress stock valuations (P/B or P/E) on FOL \n        utilization, controlling for fundamentals. The coefficient on \n        FOL utilization captures the premium investors pay for stocks \n        near their foreign ownership cap.\n        \n        Alternative: Compare returns of stocks transitioning between \n        FOL zones as a natural experiment.\n        \"\"\"\n        df = self.metrics.copy()\n        df = df[df['fol_utilization'].notna() & df['market_cap'].notna()].copy()\n        \n        # FOL zone dummies\n        df['near_cap'] = (df['fol_utilization'] &gt; 0.90).astype(int)\n        df['at_cap'] = (df['fol_utilization'] &gt; 0.98).astype(int)\n        \n        # Price-to-book as valuation measure\n        # (Assumes 'equity' is available from financial data)\n        if 'equity' in df.columns:\n            df['pb_ratio'] = df['market_cap'] * 1e9 / df['equity']\n        else:\n            # Use market cap as proxy for cross-sectional analysis\n            df['log_mktcap'] = np.log(df['market_cap'])\n        \n        # Fama-MacBeth style: run cross-sectional regressions each period\n        results = []\n        for quarter, group in df.groupby('quarter_end'):\n            group = group.dropna(subset=['fol_utilization', 'log_mktcap'])\n            if len(group) &lt; 50:\n                continue\n            \n            y = group['log_mktcap']\n            X = sm.add_constant(group[['fol_utilization', 'pct_state', \n                                        'n_inst_owners']])\n            try:\n                model = sm.OLS(y, X).fit()\n                results.append({\n                    'quarter': quarter,\n                    'beta_fol': model.params.get('fol_utilization', np.nan),\n                    'tstat_fol': model.tvalues.get('fol_utilization', np.nan),\n                    'r2': model.rsquared,\n                    'n': len(group),\n                })\n            except Exception:\n                continue\n        \n        if results:\n            results_df = pd.DataFrame(results)\n            print(\"FOL Premium (Fama-MacBeth Regression):\")\n            print(f\"  Mean β(FOL_util): {results_df['beta_fol'].mean():.4f}\")\n            print(f\"  t-statistic: {results_df['beta_fol'].mean() / \"\n                  f\"(results_df['beta_fol'].std() / np.sqrt(len(results_df))):.2f}\")\n            return results_df\n        \n        return pd.DataFrame()\n    \n    def analyze_foreign_room_events(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Analyze events where foreign room opens or closes.\n        \n        Room-opening events (FOL cap raised, foreign seller exits) can\n        trigger significant price movements as pent-up foreign demand \n        is released. Room-closing events (approaching cap) can create\n        selling pressure as foreign investors anticipate illiquidity.\n        \"\"\"\n        if self.foreign_daily is None:\n            print(\"Daily foreign ownership data required for event analysis.\")\n            return pd.DataFrame()\n        \n        df = self.foreign_daily.copy()\n        df = df.sort_values(['ticker', 'date'])\n        \n        # Compute daily change in foreign room\n        df['foreign_room_change'] = df.groupby('ticker')['foreign_room'].diff()\n        \n        # Identify room-opening events (room increases by &gt; 1 percentage point)\n        df['room_open_event'] = (df['foreign_room_change'] &gt; 0.01).astype(int)\n        \n        # Identify room-closing events (room decreases to &lt; 2%)\n        df['room_close_event'] = (\n            (df['foreign_room'] &lt; 0.02) & \n            (df.groupby('ticker')['foreign_room'].shift(1) &gt;= 0.02)\n        ).astype(int)\n        \n        events = df[\n            (df['room_open_event'] == 1) | (df['room_close_event'] == 1)\n        ].copy()\n        \n        print(f\"Foreign room events identified:\")\n        print(f\"  Room-opening events: {df['room_open_event'].sum():,}\")\n        print(f\"  Room-closing events: {df['room_close_event'].sum():,}\")\n        \n        return events\n\n# fol_analyzer = FOLAnalyzer(io_metrics, dc.foreign_ownership)\n# fol_classified = fol_analyzer.classify_fol_zones()\n# fol_premium = fol_analyzer.estimate_fol_premium()\n\n\n\n\ndef plot_fol_utilization(metrics: pd.DataFrame):\n    \"\"\"Plot FOL utilization distribution by sector.\"\"\"\n    df = metrics[metrics['fol_utilization'].notna()].copy()\n    \n    # Assign broad sectors\n    sector_map = {\n        'Banking': ['VCB', 'BID', 'CTG', 'TCB', 'VPB', 'MBB', 'ACB', 'HDB', 'STB', 'TPB'],\n        'Real Estate': ['VHM', 'VIC', 'NVL', 'KDH', 'DXG', 'HDG', 'VRE'],\n        'Technology': ['FPT', 'CMG', 'FOX'],\n        'Consumer': ['VNM', 'MSN', 'SAB', 'MWG', 'PNJ'],\n    }\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    for sector, tickers in sector_map.items():\n        data = df[df['ticker'].isin(tickers)]['fol_utilization']\n        if len(data) &gt; 0:\n            ax.hist(data * 100, bins=30, alpha=0.4, label=sector, density=True)\n    \n    ax.axvline(x=30, color='red', linestyle='--', alpha=0.7, label='Banking FOL (30%)')\n    ax.axvline(x=49, color='blue', linestyle='--', alpha=0.7, label='Standard FOL (49%)')\n    ax.set_xlabel('FOL Utilization (%)')\n    ax.set_ylabel('Density')\n    ax.set_title('Foreign Ownership Limit Utilization Distribution')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig('fig_fol_utilization.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_fol_utilization(io_metrics)\n\n\nFigure 33.3",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-trades",
    "href": "50_institutional_ownership.html#sec-trades",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.7 Institutional Trades",
    "text": "33.7 Institutional Trades\n\n33.7.1 Trade Inference in Vietnam\nIn the US, institutional trades are inferred from quarterly 13F holding snapshots. In Vietnam, the challenge is more acute because disclosure frequency varies:\n\nMajor shareholders (\\(\\ge\\) 5%): Must disclose within 7 business days of crossing ownership thresholds (5%, 10%, 15%, 20%, 25%, 50%, 65%, 75%)\nFund portfolio reports: Semi-annual disclosure required; some funds report quarterly\nAnnual reports: Provide complete shareholder register but only once per year\nDaily foreign ownership: HOSE/HNX publish aggregate daily foreign buy/sell data\n\nWe derive trades from the change in ownership between consecutive disclosure dates, applying the same logic as the US Ben-David et al. (2013) algorithm but adapted for Vietnam’s irregular disclosure intervals.\n\n# ============================================================================\n# Step 7: Derive Institutional Trades\n# ============================================================================\n\ndef derive_trades_vietnam(ownership: pd.DataFrame,\n                           adj_factors: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Derive institutional trades from changes in ownership disclosures.\n    \n    Adapted from Ben-David, Franzoni, and Moussawi (2012) for \n    Vietnam's irregular disclosure frequency.\n    \n    Key differences from US approach:\n    1. Disclosure intervals are irregular (not always quarterly)\n    2. We observe ALL institutional types, not just 13F filers\n    3. No $100M AUM threshold (we see all institutional holders)\n    4. Must adjust for corporate actions between disclosure dates\n    \n    Trade types:\n    +1: Initiating Buy (new position)\n    +2: Incremental Buy (increased existing position)\n    -1: Terminating Sale (fully exited position)\n    -2: Incremental Sale (reduced existing position)\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership with: ticker, date, shareholder_name, \n        shares_held, owner_type\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors\n    \n    Returns\n    -------\n    pd.DataFrame\n        Trade-level data: date, shareholder_name, ticker, trade, \n        buysale, owner_type\n    \"\"\"\n    # Focus on institutional shareholders only\n    inst = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)\n    ].copy()\n    \n    inst = inst.sort_values(['shareholder_name', 'ticker', 'date']).reset_index(drop=True)\n    \n    trades_list = []\n    \n    for (shareholder, ticker), group in inst.groupby(['shareholder_name', 'ticker']):\n        group = group.reset_index(drop=True)\n        \n        for i in range(len(group)):\n            current = group.iloc[i]\n            current_date = current['date']\n            current_shares = current['shares_held']\n            owner_type = current['owner_type']\n            \n            if i == 0:\n                # First observation: if institution appears, it's an initiating buy\n                # (we don't know if they held before our data starts)\n                # Skip the very first observation to avoid false initiating buys\n                continue\n            \n            prev = group.iloc[i - 1]\n            prev_date = prev['date']\n            prev_shares = prev['shares_held']\n            \n            # Adjust previous shares for corporate actions between dates\n            prev_shares_adj = adjust_shares(\n                prev_shares, ticker, prev_date, current_date, adj_factors\n            )\n            \n            # Compute trade (in adjusted shares)\n            trade = current_shares - prev_shares_adj\n            \n            # Classify trade type\n            if abs(trade) &lt; 1:  # De minimis threshold\n                continue\n            \n            if prev_shares_adj &lt;= 0 and current_shares &gt; 0:\n                buysale = 1  # Initiating buy\n            elif prev_shares_adj &gt; 0 and current_shares &lt;= 0:\n                buysale = -1  # Terminating sale\n            elif trade &gt; 0:\n                buysale = 2  # Incremental buy\n            else:\n                buysale = -2  # Incremental sale\n            \n            trades_list.append({\n                'date': current_date,\n                'shareholder_name': shareholder,\n                'ticker': ticker,\n                'trade': trade,\n                'prev_shares_adj': prev_shares_adj,\n                'current_shares': current_shares,\n                'buysale': buysale,\n                'owner_type': owner_type,\n                'days_between': (current_date - prev_date).days,\n            })\n    \n    trades = pd.DataFrame(trades_list)\n    \n    if len(trades) &gt; 0:\n        print(f\"Trades derived: {len(trades):,}\")\n        print(f\"\\nTrade type distribution:\")\n        labels = {1: 'Initiating Buy', 2: 'Incremental Buy',\n                  -1: 'Terminating Sale', -2: 'Incremental Sale'}\n        for bs, label in sorted(labels.items()):\n            n = (trades['buysale'] == bs).sum()\n            print(f\"  {label}: {n:,} ({n/len(trades):.1%})\")\n        \n        print(f\"\\nBy owner type:\")\n        print(trades.groupby('owner_type')['trade'].agg(['count', 'mean', 'median'])\n              .round(0).to_string())\n    \n    return trades\n\n# trades = derive_trades_vietnam(ownership_classified, adj_factors)\n\n\n\n\n\n\n\nWarningCorporate Action Adjustment in Trade Derivation\n\n\n\nWhen computing trades as \\(\\Delta Shares = Shares_t - Shares_{t-1}\\), the previous period’s shares must be adjusted for any corporate actions between \\(t-1\\) and \\(t\\). If VNM issued a 20% stock dividend between the two disclosure dates, then 1,000 shares at \\(t-1\\) should be compared to 1,200 adjusted shares, not 1,000 raw shares. Failing to make this adjustment would create a phantom “buy” of 200 shares that never actually occurred.\n\n\n\ndef derive_trades_vectorized_vietnam(ownership: pd.DataFrame,\n                                      adj_factors: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Vectorized version of Vietnamese trade derivation.\n    \n    Uses pandas groupby and vectorized operations instead of Python loops.\n    Approximately 20-50x faster for large datasets.\n    \n    Note: Corporate action adjustment is applied per-group, which still\n    requires some iteration but is much faster than row-by-row.\n    \"\"\"\n    inst = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL) &\n        (ownership['shares_held'] &gt; 0)\n    ].copy()\n    \n    inst = inst.sort_values(['shareholder_name', 'ticker', 'date']).reset_index(drop=True)\n    \n    # Lagged values\n    inst['prev_date'] = inst.groupby(['shareholder_name', 'ticker'])['date'].shift(1)\n    inst['prev_shares'] = inst.groupby(['shareholder_name', 'ticker'])['shares_held'].shift(1)\n    inst['is_first'] = inst['prev_date'].isna()\n    \n    # Remove first observations (no prior to compare)\n    inst = inst[~inst['is_first']].copy()\n    \n    # Adjust previous shares for corporate actions\n    # Vectorized: for each row, apply adjustment between prev_date and date\n    def adjust_row(row):\n        return adjust_shares(\n            row['prev_shares'], row['ticker'], \n            row['prev_date'], row['date'], adj_factors\n        )\n    \n    inst['prev_shares_adj'] = inst.apply(adjust_row, axis=1)\n    \n    # Compute trade\n    inst['trade'] = inst['shares_held'] - inst['prev_shares_adj']\n    inst['days_between'] = (inst['date'] - inst['prev_date']).dt.days\n    \n    # Classify trade type\n    inst['buysale'] = np.select(\n        [\n            (inst['prev_shares_adj'] &lt;= 0) & (inst['shares_held'] &gt; 0),\n            (inst['prev_shares_adj'] &gt; 0) & (inst['shares_held'] &lt;= 0),\n            inst['trade'] &gt; 0,\n            inst['trade'] &lt; 0,\n        ],\n        [1, -1, 2, -2],\n        default=0\n    )\n    \n    # Remove zero trades\n    trades = inst[inst['buysale'] != 0].copy()\n    \n    trades = trades[['date', 'shareholder_name', 'ticker', 'trade', \n                     'buysale', 'owner_type', 'days_between',\n                     'prev_shares_adj', 'shares_held']].copy()\n    trades = trades.rename(columns={'shares_held': 'current_shares'})\n    \n    print(f\"Vectorized trades: {len(trades):,}\")\n    return trades\n\n# trades = derive_trades_vectorized_vietnam(ownership_classified, adj_factors)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-flows-turnover",
    "href": "50_institutional_ownership.html#sec-flows-turnover",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.8 Fund-Level Flows and Turnover",
    "text": "33.8 Fund-Level Flows and Turnover\n\n33.8.1 Portfolio Assets and Returns from Fund Holdings\nUsing DataCore.vn’s fund holdings data, we compute fund-level portfolio analytics analogous to the US 13F approach:\n\\[\nAssets_{j,t} = \\sum_{i=1}^{N_{j,t}} S_{i,j,t} \\times P_{i,t}\n\\tag{33.7}\\]\n\\[\nR_{j,t \\to t+1}^{holdings} = \\frac{\\sum_{i} S_{i,j,t} \\times P_{i,t} \\times R_{i,t \\to t+1}}{\\sum_{i} S_{i,j,t} \\times P_{i,t}}\n\\tag{33.8}\\]\n\\[\nNetFlows_{j,t} = Assets_{j,t} - Assets_{j,t-1} \\times (1 + R_{j,t-1 \\to t}^{holdings})\n\\tag{33.9}\\]\n\n\n33.8.2 Turnover Measures\nFollowing Carhart (1997), adapted for Vietnam’s fund reporting:\n\\[\nTurnover_{j,t}^{Carhart} = \\frac{\\min(TotalBuys_{j,t}, TotalSales_{j,t})}{\\overline{Assets}_{j,t}}\n\\tag{33.10}\\]\n\n# ============================================================================\n# Step 8: Fund-Level Portfolio Analytics\n# ============================================================================\n\ndef compute_fund_analytics(fund_holdings: pd.DataFrame,\n                            prices_q: pd.DataFrame,\n                            adj_factors: pd.DataFrame) -&gt; Dict:\n    \"\"\"\n    Compute fund-level portfolio analytics from DataCore.vn fund holdings.\n    \n    Vietnamese fund disclosure is typically semi-annual (some quarterly),\n    which limits the frequency of these analytics compared to the US\n    quarterly approach.\n    \n    Returns\n    -------\n    dict with keys:\n        'fund_assets': pd.DataFrame of fund-level assets and returns\n        'fund_trades': pd.DataFrame of fund-level derived trades\n        'fund_aggregates': pd.DataFrame of flows and turnover\n    \"\"\"\n    fh = fund_holdings.copy()\n    fh = fh[fh['shares_held'] &gt; 0].copy()\n    \n    # Merge with prices\n    fh = fh.merge(\n        prices_q[['ticker', 'quarter_end', 'close', 'adjusted_close', 'ret']],\n        left_on=['ticker', 'report_date'],\n        right_on=['ticker', 'quarter_end'],\n        how='inner'\n    )\n    \n    # Portfolio value\n    fh['holding_value'] = fh['shares_held'] * fh['close']\n    \n    # --- Fund-Level Assets ---\n    fund_assets = fh.groupby(['fund_name', 'report_date']).agg(\n        total_assets=('holding_value', lambda x: x.sum() / 1e9),  # Billion VND\n        n_stocks=('ticker', 'nunique'),\n    ).reset_index()\n    \n    # Holdings return (value-weighted)\n    fh['weight'] = fh.groupby(['fund_name', 'report_date'])['holding_value'].transform(\n        lambda x: x / x.sum()\n    )\n    fund_hret = (fh.groupby(['fund_name', 'report_date'])\n                   .apply(lambda g: np.average(g['ret'].fillna(0), weights=g['weight']))\n                   .reset_index(name='holdings_return'))\n    \n    fund_assets = fund_assets.merge(fund_hret, on=['fund_name', 'report_date'])\n    \n    # --- Fund-Level Trades ---\n    # Derive trades from changes in holdings\n    fh_sorted = fh.sort_values(['fund_name', 'ticker', 'report_date'])\n    fh_sorted['prev_shares'] = fh_sorted.groupby(['fund_name', 'ticker'])['shares_held'].shift(1)\n    fh_sorted['prev_date'] = fh_sorted.groupby(['fund_name', 'ticker'])['report_date'].shift(1)\n    \n    # Adjust for corporate actions\n    fh_sorted['prev_shares_adj'] = fh_sorted.apply(\n        lambda r: adjust_shares(r['prev_shares'], r['ticker'], \n                                r['prev_date'], r['report_date'], adj_factors)\n        if pd.notna(r['prev_shares']) else np.nan,\n        axis=1\n    )\n    \n    fh_sorted['trade'] = fh_sorted['shares_held'] - fh_sorted['prev_shares_adj']\n    fh_sorted['trade_value'] = fh_sorted['trade'] * fh_sorted['close'] / 1e9  # Billion VND\n    \n    # Aggregate buys and sells per fund-period\n    fund_trades = fh_sorted[fh_sorted['trade'].notna()].copy()\n    fund_flows = fund_trades.groupby(['fund_name', 'report_date']).agg(\n        total_buys=('trade_value', lambda x: x[x &gt; 0].sum()),\n        total_sales=('trade_value', lambda x: -x[x &lt; 0].sum()),\n    ).reset_index()\n    \n    # --- Fund-Level Aggregates ---\n    fund_agg = fund_assets.merge(fund_flows, on=['fund_name', 'report_date'], how='left')\n    fund_agg[['total_buys', 'total_sales']] = fund_agg[['total_buys', 'total_sales']].fillna(0)\n    \n    fund_agg = fund_agg.sort_values(['fund_name', 'report_date'])\n    fund_agg['lag_assets'] = fund_agg.groupby('fund_name')['total_assets'].shift(1)\n    fund_agg['lag_hret'] = fund_agg.groupby('fund_name')['holdings_return'].shift(1)\n    \n    # Net flows\n    fund_agg['net_flows'] = (fund_agg['total_assets'] - \n                              fund_agg['lag_assets'] * (1 + fund_agg['holdings_return']))\n    \n    # Turnover (Carhart definition)\n    fund_agg['avg_assets'] = (fund_agg['total_assets'] + fund_agg['lag_assets']) / 2\n    fund_agg['turnover'] = (\n        fund_agg[['total_buys', 'total_sales']].min(axis=1) / fund_agg['avg_assets']\n    )\n    \n    # Annualize (approximate, since disclosure may be semi-annual)\n    fund_agg['periods_per_year'] = 365 / fund_agg.groupby('fund_name')['report_date'].diff().dt.days\n    fund_agg['turnover_annual'] = fund_agg['turnover'] * fund_agg['periods_per_year'].fillna(2)\n    \n    print(f\"Fund analytics computed:\")\n    print(f\"  Unique funds: {fund_agg['fund_name'].nunique():,}\")\n    print(f\"  Fund-period observations: {len(fund_agg):,}\")\n    print(f\"\\nTurnover statistics:\")\n    print(fund_agg[['turnover', 'turnover_annual']].describe().round(4))\n    \n    return {\n        'fund_assets': fund_assets,\n        'fund_trades': fund_trades,\n        'fund_aggregates': fund_agg,\n    }\n\n# fund_analytics = compute_fund_analytics(dc.fund_holdings, prices_q, adj_factors)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-state-ownership",
    "href": "50_institutional_ownership.html#sec-state-ownership",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.9 State Ownership Analysis",
    "text": "33.9 State Ownership Analysis\n\n33.9.1 Equitization and the Decline of State Ownership\nVietnam’s equitization (cổ phần hóa) program has been a defining feature of the market since the early 2000s. The program converts state-owned enterprises into joint-stock companies, typically with the state retaining a controlling or significant minority stake that is then gradually reduced through secondary offerings.\n\n# ============================================================================\n# Step 9: State Ownership Analysis\n# ============================================================================\n\ndef analyze_state_ownership(metrics: pd.DataFrame) -&gt; Dict:\n    \"\"\"\n    Comprehensive analysis of state ownership in Vietnam.\n    \n    Computes:\n    1. Aggregate state ownership trends\n    2. SOE population dynamics (entry/exit from SOE classification)\n    3. Equitization event detection (large drops in state ownership)\n    4. State ownership by sector and size\n    5. Governance implications (state as blockholder)\n    \"\"\"\n    df = metrics.copy()\n    \n    # --- 1. Aggregate Trends ---\n    ts = df.groupby('quarter_end').agg(\n        n_soe=('is_soe', 'sum'),\n        n_total=('ticker', 'nunique'),\n        pct_soe=('is_soe', 'mean'),\n        mean_state_pct=('pct_state', 'mean'),\n        median_state_pct=('pct_state', 'median'),\n        # Market cap share of SOEs\n        soe_mktcap=('market_cap', lambda x: x[df.loc[x.index, 'is_soe'] == 1].sum()),\n        total_mktcap=('market_cap', 'sum'),\n    ).reset_index()\n    ts['soe_mktcap_share'] = ts['soe_mktcap'] / ts['total_mktcap']\n    \n    # --- 2. Equitization Events ---\n    # Detect large drops in state ownership (&gt;10 percentage points)\n    df_sorted = df.sort_values(['ticker', 'quarter_end'])\n    df_sorted['state_change'] = df_sorted.groupby('ticker')['pct_state'].diff()\n    \n    equitization_events = df_sorted[\n        df_sorted['state_change'] &lt; -0.10  # &gt; 10pp drop\n    ][['ticker', 'quarter_end', 'pct_state', 'state_change', 'market_cap']].copy()\n    \n    # --- 3. By Sector ---\n    if 'industry_code' in df.columns:\n        by_sector = df.groupby('industry_code').agg(\n            mean_state=('pct_state', 'mean'),\n            pct_soe=('is_soe', 'mean'),\n            n_firms=('ticker', 'nunique'),\n        ).sort_values('mean_state', ascending=False)\n    else:\n        by_sector = None\n    \n    print(f\"State Ownership Analysis:\")\n    print(f\"  Current SOE count: {ts.iloc[-1]['n_soe']:.0f} / {ts.iloc[-1]['n_total']:.0f}\")\n    print(f\"  SOE market cap share: {ts.iloc[-1]['soe_mktcap_share']:.1%}\")\n    print(f\"  Mean state ownership: {ts.iloc[-1]['mean_state_pct']:.1%}\")\n    print(f\"\\nEquitization events detected: {len(equitization_events):,}\")\n    \n    return {\n        'trends': ts,\n        'equitization_events': equitization_events,\n        'by_sector': by_sector,\n    }\n\n# state_analysis = analyze_state_ownership(io_metrics)\n\n\n\n\ndef plot_state_ownership(state_analysis: Dict, metrics: pd.DataFrame):\n    \"\"\"Plot state ownership dynamics.\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n    ts = state_analysis['trends']\n    \n    # Panel A: SOE trends\n    ax = axes[0]\n    ax.plot(ts['quarter_end'], ts['pct_soe'] * 100, \n            label='% of Firms that are SOEs', linewidth=2, color='#d62728')\n    ax.plot(ts['quarter_end'], ts['soe_mktcap_share'] * 100,\n            label='SOE Market Cap Share (%)', linewidth=2, color='#1f77b4')\n    ax.plot(ts['quarter_end'], ts['mean_state_pct'] * 100,\n            label='Mean State Ownership (%)', linewidth=2, color='#2ca02c', linestyle='--')\n    ax.set_ylabel('Percentage')\n    ax.set_title('Panel A: State Ownership and SOE Prevalence Over Time')\n    ax.legend(frameon=True, framealpha=0.9)\n    \n    # Panel B: Distribution\n    ax = axes[1]\n    # Use most recent period\n    latest = metrics[metrics['quarter_end'] == metrics['quarter_end'].max()]\n    state_pct = latest['pct_state'].dropna() * 100\n    \n    ax.hist(state_pct, bins=50, color='#d62728', alpha=0.7, edgecolor='black')\n    ax.axvline(x=50, color='black', linestyle='--', alpha=0.7, label='50% (SOE threshold)')\n    ax.set_xlabel('State Ownership (%)')\n    ax.set_ylabel('Number of Companies')\n    ax.set_title('Panel B: Distribution of State Ownership (Most Recent Quarter)')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig('fig_state_ownership.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_state_ownership(state_analysis, io_metrics)\n\n\nFigure 33.4",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-modern-extensions",
    "href": "50_institutional_ownership.html#sec-modern-extensions",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.10 Modern Extensions",
    "text": "33.10 Modern Extensions\n\n33.10.1 Network Analysis of Co-Ownership\nInstitutional co-ownership networks capture how stocks are connected through shared investors. In Vietnam, these networks reveal the influence structure of major domestic conglomerates (e.g., Vingroup, Masan, FPT) and the overlap between foreign fund portfolios.\n\ndef construct_stock_coownership_network(ownership: pd.DataFrame,\n                                         period: str,\n                                         min_overlap: int = 3) -&gt; Dict:\n    \"\"\"\n    Construct a stock-level co-ownership network.\n    \n    Two stocks are connected if they share institutional investors.\n    Edge weight = number of shared institutional investors.\n    \n    This is particularly informative in Vietnam where:\n    - Foreign fund portfolios concentrate on the same blue-chips\n    - Conglomerate cross-holdings create explicit linkages\n    - State ownership creates implicit connections (SCIC holds multiple stocks)\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership data\n    period : str\n        Analysis date\n    min_overlap : int\n        Minimum shared investors to create an edge\n    \n    Returns\n    -------\n    dict with network statistics and adjacency data\n    \"\"\"\n    import networkx as nx\n    \n    date = pd.Timestamp(period)\n    \n    # Get institutional holders for this period\n    inst = ownership[\n        (ownership['date'] == date) &\n        (ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL))\n    ][['ticker', 'shareholder_name', 'owner_type']].copy()\n    \n    # Create bipartite mapping: institution → set of stocks held\n    inst_to_stocks = inst.groupby('shareholder_name')['ticker'].apply(set).to_dict()\n    \n    # Stock → set of institutions\n    stock_to_inst = inst.groupby('ticker')['shareholder_name'].apply(set).to_dict()\n    \n    # Build stock-level network\n    stocks = list(stock_to_inst.keys())\n    G = nx.Graph()\n    \n    for i in range(len(stocks)):\n        for j in range(i + 1, len(stocks)):\n            shared = stock_to_inst[stocks[i]] & stock_to_inst[stocks[j]]\n            if len(shared) &gt;= min_overlap:\n                G.add_edge(stocks[i], stocks[j], weight=len(shared),\n                           shared_investors=list(shared)[:5])  # Store sample\n    \n    # Add node attributes\n    for stock in stocks:\n        if stock in G.nodes:\n            G.nodes[stock]['n_inst_holders'] = len(stock_to_inst[stock])\n    \n    # Network statistics\n    stats = {\n        'n_nodes': G.number_of_nodes(),\n        'n_edges': G.number_of_edges(),\n        'density': nx.density(G) if G.number_of_nodes() &gt; 1 else 0,\n        'avg_clustering': nx.average_clustering(G, weight='weight') if G.number_of_nodes() &gt; 0 else 0,\n        'n_components': nx.number_connected_components(G),\n    }\n    \n    # Centrality measures\n    if G.number_of_nodes() &gt; 0:\n        degree_cent = nx.degree_centrality(G)\n        stats['most_connected'] = sorted(degree_cent.items(), \n                                          key=lambda x: x[1], reverse=True)[:10]\n        \n        if G.number_of_nodes() &gt; 2:\n            try:\n                eigen_cent = nx.eigenvector_centrality_numpy(G, weight='weight')\n                stats['most_central'] = sorted(eigen_cent.items(),\n                                                key=lambda x: x[1], reverse=True)[:10]\n            except Exception:\n                stats['most_central'] = []\n    \n    print(f\"Co-Ownership Network ({period}):\")\n    for k, v in stats.items():\n        if k not in ['most_connected', 'most_central']:\n            print(f\"  {k}: {v}\")\n    \n    if 'most_connected' in stats:\n        print(f\"\\nMost connected stocks:\")\n        for stock, cent in stats['most_connected'][:5]:\n            print(f\"  {stock}: {cent:.3f}\")\n    \n    return {'graph': G, 'stats': stats}\n\n# network = construct_stock_coownership_network(\n#     ownership_classified, '2024-06-30'\n# )\n\n\n\n33.10.2 ML-Enhanced Investor Classification\nVietnam’s investor classification challenge is distinct from the US. While the US has the Bushee typology based on portfolio turnover and concentration, Vietnam requires classification of both investor type (when not explicitly labeled) and investor behavior (active vs passive, short-term vs long-term).\n\ndef classify_investors_vietnam(ownership: pd.DataFrame,\n                                prices_q: pd.DataFrame,\n                                n_clusters: int = 4) -&gt; pd.DataFrame:\n    \"\"\"\n    ML-based classification of Vietnamese institutional investors.\n    \n    Features adapted for Vietnam's market:\n    1. Portfolio concentration (HHI of holdings)\n    2. Holding duration (average time in positions)\n    3. Size preference (average market cap of holdings)\n    4. Sector concentration\n    5. Foreign/domestic indicator\n    6. Trading frequency (inverse of average days between disclosures)\n    \n    Expected clusters for Vietnam:\n    - Passive State Holders: SOE parents, SCIC - low turnover, concentrated\n    - Active Foreign Funds: Dragon Capital, VinaCapital - moderate turnover\n    - Domestic Securities Firms: SSI, VNDirect - high turnover, diversified\n    - Long-Term Foreign: Pension funds, sovereign wealth - low turnover\n    \"\"\"\n    from sklearn.cluster import KMeans\n    from sklearn.preprocessing import StandardScaler\n    \n    inst = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)\n    ].copy()\n    \n    # Merge with price data\n    inst = inst.merge(\n        prices_q[['ticker', 'quarter_end', 'close', 'market_cap']],\n        left_on=['ticker', 'date'],\n        right_on=['ticker', 'quarter_end'],\n        how='left'\n    )\n    \n    inst['holding_value'] = inst['shares_held'] * inst['close'].fillna(0)\n    \n    # Compute features per investor-period\n    features = inst.groupby(['shareholder_name', 'date']).agg(\n        n_stocks=('ticker', 'nunique'),\n        total_value=('holding_value', 'sum'),\n        hhi_portfolio=('holding_value', \n                        lambda x: ((x/x.sum())**2).sum() if x.sum() &gt; 0 else np.nan),\n        avg_mktcap=('market_cap', 'mean'),\n        is_foreign=('owner_type', \n                     lambda x: (x == OwnershipType.FOREIGN_INST).any().astype(int)),\n        is_state=('owner_type', \n                   lambda x: (x == OwnershipType.STATE).any().astype(int)),\n    ).reset_index()\n    \n    # Average across all periods per investor\n    investor_features = features.groupby('shareholder_name').agg(\n        avg_n_stocks=('n_stocks', 'mean'),\n        avg_hhi=('hhi_portfolio', 'mean'),\n        avg_mktcap=('avg_mktcap', 'mean'),\n        avg_total_value=('total_value', 'mean'),\n        is_foreign=('is_foreign', 'max'),\n        is_state=('is_state', 'max'),\n        n_periods=('date', 'nunique'),\n    ).dropna()\n    \n    # Feature matrix\n    feature_cols = ['avg_n_stocks', 'avg_hhi', 'avg_mktcap', 'avg_total_value']\n    X = investor_features[feature_cols].copy()\n    \n    # Log-transform\n    for col in feature_cols:\n        X[col] = np.log1p(X[col].clip(lower=0))\n    \n    # Add binary features\n    X['is_foreign'] = investor_features['is_foreign']\n    X['is_state'] = investor_features['is_state']\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # K-means\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n    investor_features['cluster'] = kmeans.fit_predict(X_scaled)\n    \n    # Label clusters\n    cluster_profiles = investor_features.groupby('cluster').agg({\n        'avg_n_stocks': 'mean',\n        'avg_hhi': 'mean',\n        'avg_total_value': 'mean',\n        'is_foreign': 'mean',\n        'is_state': 'mean',\n        'shareholder_name': 'count',\n    }).rename(columns={'shareholder_name': 'n_investors'})\n    \n    print(\"Investor Clusters:\")\n    print(cluster_profiles.round(3).to_string())\n    \n    return investor_features\n\n# investor_classes = classify_investors_vietnam(ownership_classified, prices_q)\n\n\n\n33.10.3 Event Study: Ownership Disclosure Shocks\nVietnam’s threshold-based major shareholder disclosure creates natural events for studying the price impact of ownership changes.\n\ndef ownership_event_study(major_shareholders: pd.DataFrame,\n                           prices: pd.DataFrame,\n                           event_window: Tuple[int, int] = (-5, 20),\n                           estimation_window: int = 120) -&gt; pd.DataFrame:\n    \"\"\"\n    Event study of ownership disclosure announcements.\n    \n    Vietnam requires major shareholders (≥5%) to disclose within 7 \n    business days of crossing ownership thresholds. These disclosures \n    can be informationally significant, especially:\n    1. Foreign fund accumulation (signal of quality)\n    2. State divestiture (equitization signal)\n    3. Insider purchases (management confidence signal)\n    \n    Uses market model for expected returns:\n    E[R_i,t] = α_i + β_i × R_m,t\n    \n    Parameters\n    ----------\n    major_shareholders : pd.DataFrame\n        Disclosure events from DataCore.vn\n    prices : pd.DataFrame\n        Daily stock prices\n    event_window : tuple\n        (pre_event_days, post_event_days)\n    estimation_window : int\n        Days before event window for market model estimation\n    \"\"\"\n    events = major_shareholders.copy()\n    events = events.sort_values(['ticker', 'date'])\n    \n    # Identify significant ownership changes\n    events['ownership_change'] = events.groupby(\n        ['ticker', 'shareholder_name']\n    )['ownership_pct'].diff()\n    \n    significant_events = events[\n        events['ownership_change'].abs() &gt; 0.01  # &gt; 1 percentage point\n    ].copy()\n    \n    significant_events['event_type'] = np.where(\n        significant_events['ownership_change'] &gt; 0, 'accumulation', 'divestiture'\n    )\n    \n    # Merge with daily prices\n    prices_daily = prices[['ticker', 'date', 'ret']].copy()\n    prices_daily = prices_daily.sort_values(['ticker', 'date'])\n    \n    # VN-Index as market return (ticker code depends on data provider)\n    if 'VNINDEX' in prices_daily['ticker'].values:\n        market_ret = prices_daily[prices_daily['ticker'] == 'VNINDEX'][['date', 'ret']].copy()\n        market_ret = market_ret.rename(columns={'ret': 'mkt_ret'})\n    else:\n        # Use equal-weighted market return as proxy\n        market_ret = (prices_daily.groupby('date')['ret']\n                                  .mean()\n                                  .reset_index()\n                                  .rename(columns={'ret': 'mkt_ret'}))\n    \n    # For each event, compute abnormal returns\n    results = []\n    pre, post = event_window\n    \n    for _, event in significant_events.iterrows():\n        ticker = event['ticker']\n        event_date = event['date']\n        \n        # Get stock returns around the event\n        stock_ret = prices_daily[prices_daily['ticker'] == ticker].copy()\n        stock_ret = stock_ret.merge(market_ret, on='date', how='left')\n        stock_ret = stock_ret.sort_values('date').reset_index(drop=True)\n        \n        # Find event date index\n        event_idx = stock_ret[stock_ret['date'] &gt;= event_date].index\n        if len(event_idx) == 0:\n            continue\n        event_idx = event_idx[0]\n        \n        # Estimation window\n        est_start = max(0, event_idx - estimation_window + pre)\n        est_end = event_idx + pre\n        est_data = stock_ret.iloc[est_start:est_end].dropna(subset=['ret', 'mkt_ret'])\n        \n        if len(est_data) &lt; 30:\n            continue\n        \n        # Market model\n        X = sm.add_constant(est_data['mkt_ret'])\n        y = est_data['ret']\n        try:\n            model = sm.OLS(y, X).fit()\n        except Exception:\n            continue\n        \n        # Event window abnormal returns\n        ew_start = event_idx + pre\n        ew_end = min(event_idx + post + 1, len(stock_ret))\n        event_data = stock_ret.iloc[ew_start:ew_end].copy()\n        \n        if len(event_data) == 0:\n            continue\n        \n        event_data['expected_ret'] = (model.params['const'] + \n                                       model.params['mkt_ret'] * event_data['mkt_ret'])\n        event_data['abnormal_ret'] = event_data['ret'] - event_data['expected_ret']\n        event_data['car'] = event_data['abnormal_ret'].cumsum()\n        event_data['event_day'] = range(pre, pre + len(event_data))\n        event_data['ticker'] = ticker\n        event_data['event_date'] = event_date\n        event_data['event_type'] = event['event_type']\n        event_data['ownership_change'] = event['ownership_change']\n        event_data['shareholder_name'] = event['shareholder_name']\n        \n        results.append(event_data)\n    \n    if results:\n        all_results = pd.concat(results, ignore_index=True)\n        \n        # Average CARs by event type\n        avg_car = (all_results.groupby(['event_type', 'event_day'])['car']\n                              .agg(['mean', 'std', 'count'])\n                              .reset_index())\n        avg_car['t_stat'] = avg_car['mean'] / (avg_car['std'] / np.sqrt(avg_car['count']))\n        \n        print(f\"Event Study Results:\")\n        print(f\"  Total events: {significant_events['event_type'].value_counts().to_string()}\")\n        \n        # CAR at event day 0, +5, +10, +20\n        for et in ['accumulation', 'divestiture']:\n            print(f\"\\n  {et.title()} Events:\")\n            subset = avg_car[avg_car['event_type'] == et]\n            for day in [0, 5, 10, 20]:\n                row = subset[subset['event_day'] == day]\n                if len(row) &gt; 0:\n                    print(f\"    CAR({day:+d}): {row.iloc[0]['mean']:.4f} \"\n                          f\"(t={row.iloc[0]['t_stat']:.2f})\")\n        \n        return all_results\n    \n    return pd.DataFrame()\n\n# event_results = ownership_event_study(dc.major_shareholders, dc.prices)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-empirical-applications",
    "href": "50_institutional_ownership.html#sec-empirical-applications",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.11 Empirical Applications",
    "text": "33.11 Empirical Applications\n\n33.11.1 Application 1: Foreign Ownership and Stock Returns in Vietnam\nDoes foreign institutional ownership predict returns in Vietnam? Huang, Liu, and Shu (2023) find evidence consistent with the information advantage hypothesis.\n\ndef test_foreign_io_returns(metrics: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Test whether changes in foreign institutional ownership predict \n    future stock returns in Vietnam.\n    \n    Methodology:\n    1. Sort stocks into quintiles by change in foreign IO\n    2. Compute equal-weighted and VN-Index-adjusted returns\n    3. Report portfolio returns and long-short spread\n    \n    This adapts the Chen, Hong, and Stein (2002) breadth test \n    specifically for Vietnam's foreign ownership component.\n    \"\"\"\n    df = metrics.copy()\n    df = df.sort_values(['ticker', 'quarter_end'])\n    \n    # Change in foreign IO\n    df['delta_foreign'] = df.groupby('ticker')['pct_foreign_total'].diff()\n    \n    # Forward quarterly return\n    df['fwd_ret'] = df.groupby('ticker')['ret'].shift(-1)\n    \n    # Drop missing\n    df = df.dropna(subset=['delta_foreign', 'fwd_ret'])\n    \n    # Quintile portfolios each quarter\n    df['foreign_quintile'] = df.groupby('quarter_end')['delta_foreign'].transform(\n        lambda x: pd.qcut(x, 5, labels=[1, 2, 3, 4, 5], duplicates='drop')\n    )\n    \n    # Portfolio returns\n    port_ret = (df.groupby(['quarter_end', 'foreign_quintile'])['fwd_ret']\n                  .mean()\n                  .reset_index())\n    \n    port_wide = port_ret.pivot(index='quarter_end', columns='foreign_quintile', \n                                values='fwd_ret')\n    port_wide['LS'] = port_wide[5] - port_wide[1]\n    \n    # Test significance\n    results = {}\n    for q in [1, 2, 3, 4, 5, 'LS']:\n        data = port_wide[q].dropna()\n        mean_ret = data.mean()\n        t_stat = mean_ret / (data.std() / np.sqrt(len(data)))\n        results[q] = {\n            'Mean Return (%)': mean_ret * 100,\n            't-statistic': t_stat,\n            'N quarters': len(data),\n        }\n    \n    results_df = pd.DataFrame(results).T\n    results_df.index.name = 'ΔForeign IO Quintile'\n    \n    print(\"Foreign Ownership Change and Future Returns (Vietnam)\")\n    print(\"=\" * 60)\n    print(results_df.round(3).to_string())\n    \n    return results_df\n\n# foreign_return_results = test_foreign_io_returns(io_metrics)\n\n\n\n33.11.2 Application 2: State Divestiture and Value Creation\n\ndef analyze_equitization_value(metrics: pd.DataFrame, \n                                state_analysis: Dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Test whether reductions in state ownership are associated with \n    subsequent value creation (higher returns, improved governance).\n    \n    Hypothesis: State divestiture reduces agency costs, improves \n    operational efficiency, and attracts institutional investors,\n    leading to positive abnormal returns.\n    \n    Uses a difference-in-differences approach:\n    Treatment: Firms experiencing &gt;10pp drop in state ownership\n    Control: Matched firms with stable state ownership\n    \"\"\"\n    df = metrics.copy()\n    events = state_analysis['equitization_events']\n    \n    if len(events) == 0:\n        print(\"No equitization events detected.\")\n        return pd.DataFrame()\n    \n    # Get treated firms and their event quarters\n    treated = events[['ticker', 'quarter_end']].drop_duplicates()\n    treated['treated'] = 1\n    \n    # Merge with metrics\n    df = df.merge(treated, on=['ticker', 'quarter_end'], how='left')\n    df['treated'] = df['treated'].fillna(0)\n    \n    # Pre/post comparison for treated firms\n    treated_tickers = treated['ticker'].unique()\n    \n    results = []\n    for ticker in treated_tickers:\n        firm = df[df['ticker'] == ticker].sort_values('quarter_end')\n        event_row = firm[firm['treated'] == 1]\n        if len(event_row) == 0:\n            continue\n        \n        event_q = event_row.iloc[0]['quarter_end']\n        \n        # Pre-event (4 quarters before)\n        pre = firm[firm['quarter_end'] &lt; event_q].tail(4)\n        # Post-event (4 quarters after)\n        post = firm[firm['quarter_end'] &gt; event_q].head(4)\n        \n        if len(pre) &lt; 2 or len(post) &lt; 2:\n            continue\n        \n        results.append({\n            'ticker': ticker,\n            'event_quarter': event_q,\n            'state_pct_pre': pre['pct_state'].mean(),\n            'state_pct_post': post['pct_state'].mean(),\n            'foreign_pct_pre': pre['pct_foreign_total'].mean(),\n            'foreign_pct_post': post['pct_foreign_total'].mean(),\n            'n_inst_pre': pre['n_inst_owners'].mean(),\n            'n_inst_post': post['n_inst_owners'].mean(),\n            'ret_pre': pre['ret'].mean(),\n            'ret_post': post['ret'].mean(),\n        })\n    \n    if results:\n        results_df = pd.DataFrame(results)\n        \n        # Paired t-tests\n        print(\"Equitization Value Analysis\")\n        print(\"=\" * 60)\n        for metric in ['state_pct', 'foreign_pct', 'n_inst', 'ret']:\n            pre_col = f'{metric}_pre'\n            post_col = f'{metric}_post'\n            diff = results_df[post_col] - results_df[pre_col]\n            t_stat, p_val = stats.ttest_1samp(diff.dropna(), 0)\n            print(f\"  Δ{metric}: {diff.mean():.4f} (t={t_stat:.2f}, p={p_val:.3f})\")\n        \n        return results_df\n    \n    return pd.DataFrame()\n\n# equitization_results = analyze_equitization_value(io_metrics, state_analysis)\n\n\n\n33.11.3 Application 3: Institutional Herding in Vietnam\n\ndef compute_herding_vietnam(trades: pd.DataFrame,\n                             owner_types: Optional[List[str]] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the Lakonishok, Shleifer, and Vishny (1992) herding measure\n    adapted for the Vietnamese market.\n    \n    Can be computed separately for:\n    - All institutional investors\n    - Foreign institutions only\n    - Domestic institutions only\n    \n    The herding measure captures whether institutions systematically\n    trade in the same direction beyond what chance would predict.\n    \"\"\"\n    from scipy.stats import binom\n    \n    t = trades.copy()\n    \n    if owner_types:\n        t = t[t['owner_type'].isin(owner_types)]\n    \n    t['is_buy'] = (t['trade'] &gt; 0).astype(int)\n    \n    # For each stock-period\n    stock_trades = t.groupby(['ticker', 'date']).agg(\n        n_traders=('shareholder_name', 'nunique'),\n        n_buyers=('is_buy', 'sum'),\n    ).reset_index()\n    \n    # Minimum traders threshold\n    stock_trades = stock_trades[stock_trades['n_traders'] &gt;= 3]\n    stock_trades['p_buy'] = stock_trades['n_buyers'] / stock_trades['n_traders']\n    \n    # Expected proportion per period\n    E_p = stock_trades.groupby('date').apply(\n        lambda g: g['n_buyers'].sum() / g['n_traders'].sum()\n    ).reset_index(name='E_p')\n    \n    stock_trades = stock_trades.merge(E_p, on='date')\n    \n    # Adjustment factor\n    def expected_abs_dev(n, p):\n        k = np.arange(0, n + 1)\n        probs = binom.pmf(k, n, p)\n        return np.sum(probs * np.abs(k / n - p))\n    \n    stock_trades['adj_factor'] = stock_trades.apply(\n        lambda r: expected_abs_dev(int(r['n_traders']), r['E_p']), axis=1\n    )\n    \n    stock_trades['hm'] = (np.abs(stock_trades['p_buy'] - stock_trades['E_p']) - \n                           stock_trades['adj_factor'])\n    \n    stock_trades['buy_herd'] = np.where(\n        stock_trades['p_buy'] &gt; stock_trades['E_p'], stock_trades['hm'], np.nan\n    )\n    stock_trades['sell_herd'] = np.where(\n        stock_trades['p_buy'] &lt; stock_trades['E_p'], stock_trades['hm'], np.nan\n    )\n    \n    # Time series of herding\n    ts_herding = stock_trades.groupby('date').agg(\n        mean_hm=('hm', 'mean'),\n        mean_buy_herd=('buy_herd', 'mean'),\n        mean_sell_herd=('sell_herd', 'mean'),\n        pct_herding=('hm', lambda x: (x &gt; 0).mean()),\n        n_stocks=('ticker', 'nunique'),\n    ).reset_index()\n    \n    print(f\"Herding Analysis ({owner_types or 'All Institutions'}):\")\n    print(f\"  Mean HM: {stock_trades['hm'].mean():.4f}\")\n    print(f\"  Mean Buy Herding: {stock_trades['buy_herd'].mean():.4f}\")\n    print(f\"  Mean Sell Herding: {stock_trades['sell_herd'].mean():.4f}\")\n    print(f\"  % stocks with herding: {(stock_trades['hm'] &gt; 0).mean():.1%}\")\n    \n    return stock_trades, ts_herding\n\n# herding_all, herding_ts = compute_herding_vietnam(trades)\n# herding_foreign, _ = compute_herding_vietnam(\n#     trades, owner_types=[OwnershipType.FOREIGN_INST]\n# )",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "50_institutional_ownership.html#sec-conclusion",
    "href": "50_institutional_ownership.html#sec-conclusion",
    "title": "33  Institutional Ownership Analytics in Vietnam",
    "section": "33.12 Conclusion and Practical Recommendations",
    "text": "33.12 Conclusion and Practical Recommendations\n\n33.12.1 Summary of Measures\nTable 33.5 summarizes all institutional ownership measures developed in this chapter for the Vietnamese market.\n\n\n\nTable 33.5: Summary of All Ownership Measures for Vietnam\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nDefinition\nKey Adaptation for Vietnam\nPython Function\n\n\n\n\nIO Ratio\nInst. shares / TSO\nDecomposed into state, foreign, domestic\ncompute_ownership_decomposition()\n\n\nHHI Concentration\n\\(\\sum w_j^2\\)\nSeparate HHI for total, non-state, foreign\ncompute_io_metrics_vietnam()\n\n\nΔBreadth\nLehavy-Sloan adjusted\nApplied to irregular disclosure intervals\ncompute_io_metrics_vietnam()\n\n\nFOL Utilization\nForeign % / FOL limit\nVietnam-specific; no US equivalent\nFOLAnalyzer\n\n\nFOL Premium\nPrice impact of FOL proximity\nCross-sectional regression approach\nFOLAnalyzer.estimate_fol_premium()\n\n\nTrades\nΔShares (corp-action adjusted)\nCritical: adjust for stock dividends\nderive_trades_vectorized_vietnam()\n\n\nFund Turnover\nmin(B,S)/avg(A)\nSemi-annual frequency; annualized\ncompute_fund_analytics()\n\n\nSOE Status\nState ownership &gt; 50%\nTracks equitization program\nanalyze_state_ownership()\n\n\nLSV Herding\n\\(|p - E[p]| - E[|p - E[p]|]\\)\nSeparate foreign vs domestic herding\ncompute_herding_vietnam()\n\n\nCo-Ownership Network\nShared institutional holders\nReveals conglomerate linkages\nconstruct_stock_coownership_network()\n\n\n\n\n\n\n\n\n33.12.2 Data Quality Checklist for Vietnam\n\n\n\n\n\n\nTipVietnam Data Quality Checklist\n\n\n\n\nCorporate actions: Have you built and applied adjustment factors for ALL stock dividends, bonus shares, splits, and rights issues?\nShareholder classification: Have you verified the owner type classification (state vs foreign vs domestic institutional vs individual)?\nFOL limits: Are sector-specific FOL limits correctly assigned (30% for banks, 49% standard, unlimited for some sectors)?\nDisclosure dates: Are you using the actual disclosure date (not the record date or ex-date) for ownership snapshots?\nTreasury shares: Are treasury shares excluded from ownership ratio denominators?\nUPCOM coverage: Does your sample include or exclude UPCOM stocks (which have weaker disclosure requirements)?\nCross-listings: Are you handling NVDR (Non-Voting Depository Receipts) if applicable after market reforms?\nName consistency: Are shareholder names standardized across disclosure periods (Vietnamese names can have multiple romanization forms)?\nTrade adjustment: When deriving trades between periods, have you adjusted previous shares for ALL intervening corporate actions?\nFund mandate changes: For fund analytics, have you accounted for fund mergers, closures, and mandate changes that affect time-series continuity?\n\n\n\n\n\n33.12.3 Comparison with US Framework\n\n\n\nTable 33.6: US vs Vietnam Institutional Ownership Framework Comparison\n\n\n\n\n\n\n\n\n\n\nDimension\nUS (WRDS/13F)\nVietnam (DataCore.vn)\n\n\n\n\nDisclosure\nQuarterly 13F (mandatory)\nAnnual reports + event-driven\n\n\nCoverage\nInstitutions &gt; $100M AUM\nAll shareholders in annual reports\n\n\nOwnership observed\nLong positions only\nComplete decomposition\n\n\nIO can exceed 100%\nYes (short selling)\nNo (by construction)\n\n\nPermanent ID\nCRSP PERMNO\nTicker (with manual tracking of changes)\n\n\nAdjustment factors\nCRSP cfacshr\nMust build from corporate actions\n\n\nInvestor classification\nLSEG typecode / Bushee\nState/Foreign/Domestic/Individual\n\n\nShort selling\nNot in 13F; exists in market\nVery limited; not a concern\n\n\nUnique features\n—\nFOL, SOE ownership, stock dividend frequency\n\n\n\n\n\n\n\n\n\n\n\n\nBao Dinh, Ngoc, and Van Nguyen Hong Tran. 2024. “Institutional Ownership and Stock Liquidity: Evidence from an Emerging Market.” SAGE Open 14 (1): 21582440241239116.\n\n\nBen-David, ITZHAK, Francesco Franzoni, Augustin Landier, and Rabih Moussawi. 2013. “Do Hedge Funds Manipulate Stock Prices?” The Journal of Finance 68 (6): 2383–2434.\n\n\nCarhart, Mark M. 1997. “On Persistence in Mutual Fund Performance.” The Journal of Finance 52 (1): 57–82.\n\n\nChen, Joseph, Harrison Hong, and Jeremy C Stein. 2002. “Breadth of Ownership and Stock Returns.” Journal of Financial Economics 66 (2-3): 171–205.\n\n\nHuang, Xiangqian, Clark Liu, and Tao Shu. 2023. “Factors and Anomalies in the Vietnamese Stock Market.” Pacific-Basin Finance Journal 82: 102176.\n\n\nLehavy, Reuven, and Richard G Sloan. 2008. “Investor Recognition and Stock Returns.” Review of Accounting Studies 13 (2): 327–61.\n\n\nVo, Xuan Vinh. 2015. “Foreign Ownership and Stock Return Volatility–Evidence from Vietnam.” Journal of Multinational Financial Management 30: 101–9.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html",
    "href": "52_institutional_trade_flow.html",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "",
    "text": "34.1 Measuring Institutional Ownership and Trading\nInstitutional investors play a pivotal role in price discovery, corporate governance, and market liquidity. Understanding how institutions trade and how much they trade provides insights into both asset pricing dynamics and the real effects of institutional monitoring. The seminal work of Grinblatt, Titman, and Wermers (1995) on mutual fund momentum trading, Wermers (2000) on fund performance decomposition, and Yan (2008) on the relationship between turnover and future returns all rely on accurately measured institutional trades, flows, and turnover.\nIn the United States, this research is enabled by the mandatory quarterly 13F filing system administered by the Securities and Exchange Commission (SEC). Every institutional investment manager with at least $100 million in qualifying assets must disclose their equity holdings within 45 days of each calendar quarter end. The Thomson-Reuters (now Refinitiv) 13F database, accessible through WRDS, provides the canonical data infrastructure for this literature.\nVietnam’s equity market presents a fundamentally different institutional landscape. This chapter adapts the core methodology for the Vietnamese context, addressing five critical differences:\nThe measurement of institutional ownership and trading activity has been a central concern in empirical finance since Gompers, Ishii, and Metrick (2003) documented the rise of institutional investors. The approach relies on comparing holdings snapshots across consecutive reporting periods to infer trades. If manager \\(j\\) holds \\(h_{j,i,t}\\) shares of stock \\(i\\) at time \\(t\\), then the inferred trade is:\n\\[\n\\Delta h_{j,i,t} = h_{j,i,t} - h_{j,i,t-1}\n\\tag{34.1}\\]\nwhere \\(\\Delta h_{j,i,t} &gt; 0\\) indicates a buy and \\(\\Delta h_{j,i,t} &lt; 0\\) indicates a sale. This simple differencing approach requires that holdings are observed at regular intervals (e.g., quarterly), share counts are adjusted for corporate actions between reporting dates, and entry and exit from the dataset are handled appropriately.\nChen, Jegadeesh, and Wermers (2000) introduced the concept of ownership breadth (i.e., the number of institutions holding a stock) and showed that changes in breadth predict future returns. Sias (2004) decomposed institutional demand into a herding component and an information component. Yan (2008) linked fund turnover to information-based trading and documented that high-turnover funds outperform, challenging the view that turnover reflects noise trading.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#trade-classification",
    "href": "52_institutional_trade_flow.html#trade-classification",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "34.2 Trade Classification",
    "text": "34.2 Trade Classification\nTable 34.1 shows four categories of trades:\n\n\n\nTable 34.1: Trade Classification Taxonomy\n\n\n\n\n\nCode\nType\nDescription\n\n\n\n\n\\(+1\\)\nInitiating Buy\nManager enters a new position\n\n\n\\(+2\\)\nIncremental Buy\nManager increases an existing position\n\n\n\\(-1\\)\nTerminating Sale\nManager completely exits a position\n\n\n\\(-2\\)\nRegular Sale\nManager reduces an existing position\n\n\n\n\n\n\nThis classification is informative because initiating buys and terminating sales represent discrete portfolio decisions with different information content from marginal position adjustments (Alexander, Cici, and Gibson 2007).",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#turnover-measures",
    "href": "52_institutional_trade_flow.html#turnover-measures",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "34.3 Turnover Measures",
    "text": "34.3 Turnover Measures\nThree standard turnover definitions have been used in the literature:\nCarhart (1997) Turnover. The minimum of aggregate buys and sales, normalized by average assets:\n\\[\n\\text{Turnover}^{C}_{j,t} = \\frac{\\min\\left(\\sum_i B_{j,i,t},\\, \\sum_i S_{j,i,t}\\right)}\n{\\frac{1}{2}\\left(A_{j,t} + A_{j,t-1}\\right)}\n\\tag{34.2}\\]\nwhere \\(B_{j,i,t}\\) and \\(S_{j,i,t}\\) are the dollar values of buys and sales of stock \\(i\\) by manager \\(j\\) in quarter \\(t\\), and \\(A_{j,t}\\) is total portfolio assets (Carhart 1997).\nFlow-Adjusted Turnover. Adds back the absolute value of net flows to account for flow-driven trading:\n\\[\n\\text{Turnover}^{F}_{j,t} = \\frac{\\min\\left(\\sum_i B_{j,i,t},\\, \\sum_i S_{j,i,t}\\right) + |\\text{NetFlow}_{j,t}|}\n{A_{j,t-1}}\n\\tag{34.3}\\]\nSymmetric Turnover. Uses the sum of buys and sales minus the absolute net flow:\n\\[\n\\text{Turnover}^{S}_{j,t} = \\frac{\\sum_i B_{j,i,t} + \\sum_i S_{j,i,t} - |\\text{NetFlow}_{j,t}|}\n{A_{j,t-1}}\n\\tag{34.4}\\]\nThe relationship between these measures depends on the correlation between discretionary trading and flow-induced trading (Pástor and Stambaugh 2003).",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#institutional-ownership-in-emerging-markets",
    "href": "52_institutional_trade_flow.html#institutional-ownership-in-emerging-markets",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "34.4 Institutional Ownership in Emerging Markets",
    "text": "34.4 Institutional Ownership in Emerging Markets\nThe emerging markets literature has documented several stylized facts about institutional ownership that differ from developed market findings. Aggarwal et al. (2011) documented that foreign institutional ownership improves corporate governance in emerging markets. For Vietnam specifically, Phung and Mishra (2016) examined the relationship between ownership structure and firm performance, while Vo (2015) studied the impact of foreign ownership on stock market liquidity.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#net-flows-and-performance-attribution",
    "href": "52_institutional_trade_flow.html#net-flows-and-performance-attribution",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "34.5 Net Flows and Performance Attribution",
    "text": "34.5 Net Flows and Performance Attribution\nNet flows measure the dollar amount of new money entering or leaving a fund:\n\\[\n\\text{NetFlow}_{j,t} = A_{j,t} - A_{j,t-1}(1 + R_{j,t}^p)\n\\tag{34.5}\\]\nwhere \\(R_{j,t}^p\\) is the portfolio return. This decomposition, due to Sirri and Tufano (1998), separates changes in fund assets into investment returns and investor capital allocation decisions. Coval and Stafford (2007) showed that flow-driven trades create price pressure, with fire sales by funds experiencing redemptions generating significant negative abnormal returns.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalreader",
    "href": "52_institutional_trade_flow.html#sec-institutionalreader",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "35.1 Data Reader Class",
    "text": "35.1 Data Reader Class\nWe begin by defining a unified data reader that handles file loading, date parsing, and basic validation:\n\n@dataclass\nclass DataCoreReader:\n    \"\"\"\n    Unified reader for DataCore.vn datasets stored locally.\n    \n    Supports Parquet (recommended) and CSV formats. Implements\n    lazy loading with caching to minimize memory footprint.\n    \n    Parameters\n    ----------\n    data_dir : str or Path\n        Directory containing DataCore.vn data files.\n    file_format : str\n        File format: 'parquet' or 'csv'.\n    \n    Examples\n    --------\n    &gt;&gt;&gt; dc = DataCoreReader('/data/datacore', file_format='parquet')\n    &gt;&gt;&gt; prices = dc.prices\n    &gt;&gt;&gt; ownership = dc.ownership\n    \"\"\"\n    data_dir: Path\n    file_format: str = 'parquet'\n    _cache: Dict[str, pd.DataFrame] = field(\n        default_factory=dict, repr=False\n    )\n    \n    FILE_MAP: Dict[str, str] = field(default_factory=lambda: {\n        'prices': 'stock_prices',\n        'ownership': 'ownership_structure',\n        'major_shareholders': 'major_shareholders',\n        'corporate_actions': 'corporate_actions',\n        'company_profile': 'company_profile',\n        'financials': 'financial_statements',\n        'foreign_ownership': 'foreign_ownership',\n        'fund_holdings': 'fund_holdings',\n    }, repr=False)\n    \n    def __post_init__(self):\n        self.data_dir = Path(self.data_dir)\n        if not self.data_dir.exists():\n            raise FileNotFoundError(\n                f\"Data directory not found: {self.data_dir}\"\n            )\n    \n    def _read(self, key: str) -&gt; pd.DataFrame:\n        \"\"\"Read and cache a dataset with automatic date parsing.\"\"\"\n        if key in self._cache:\n            return self._cache[key]\n        \n        fname = self.FILE_MAP.get(key, key)\n        filepath = self.data_dir / f\"{fname}.{self.file_format}\"\n        \n        if not filepath.exists():\n            raise FileNotFoundError(\n                f\"Dataset not found: {filepath}\\n\"\n                f\"Available: \"\n                f\"{list(self.data_dir.glob(f'*.{self.file_format}'))}\"\n            )\n        \n        if self.file_format == 'parquet':\n            df = pd.read_parquet(filepath)\n        else:\n            df = pd.read_csv(filepath, parse_dates=True)\n        \n        # Auto-detect and parse date columns\n        date_cols = [\n            'date', 'ex_date', 'record_date', 'period',\n            'report_date', 'listing_date'\n        ]\n        for col in df.columns:\n            if col.lower() in date_cols or 'date' in col.lower():\n                try:\n                    df[col] = pd.to_datetime(df[col])\n                except (ValueError, TypeError):\n                    pass\n        \n        self._cache[key] = df\n        print(f\"  Loaded {key}: {len(df):,} rows x {len(df.columns)} cols\")\n        return df\n    \n    @property\n    def prices(self) -&gt; pd.DataFrame:\n        return self._read('prices')\n    \n    @property\n    def ownership(self) -&gt; pd.DataFrame:\n        return self._read('ownership')\n    \n    @property\n    def major_shareholders(self) -&gt; pd.DataFrame:\n        return self._read('major_shareholders')\n    \n    @property\n    def corporate_actions(self) -&gt; pd.DataFrame:\n        return self._read('corporate_actions')\n    \n    @property\n    def company_profile(self) -&gt; pd.DataFrame:\n        return self._read('company_profile')\n    \n    @property\n    def foreign_ownership(self) -&gt; pd.DataFrame:\n        return self._read('foreign_ownership')\n    \n    @property\n    def fund_holdings(self) -&gt; pd.DataFrame:\n        return self._read('fund_holdings')\n    \n    def clear_cache(self):\n        n = len(self._cache)\n        self._cache.clear()\n        print(f\"  Cleared {n} cached datasets\")\n\n# Initialize:\n# dc = DataCoreReader('/path/to/datacore_data', file_format='parquet')",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalprice-adj",
    "href": "52_institutional_trade_flow.html#sec-institutionalprice-adj",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "36.1 Price Data Extraction and Adjustment",
    "text": "36.1 Price Data Extraction and Adjustment\nVietnamese stock data requires careful adjustment for frequent corporate actions. Unlike the US where CRSP provides a cumulative adjustment factor (cfacpr, cfacshr), in Vietnam we must construct adjustment factors from the corporate actions history.\n\n\n\n\n\n\nNoteVietnamese Corporate Actions\n\n\n\nVietnamese firms commonly execute the following corporate actions, each requiring share count and/or price adjustment:\n\nStock dividend (co tuc bang co phieu): e.g., 20% stock dividend means 100 shares become 120 shares\nBonus shares (co phieu thuong): free shares distributed from retained earnings\nRights issue (phat hanh quyen mua): right to buy new shares at a discount\nStock split/reverse split (chia/gop co phieu): rare but occasionally used\n\n\n\n\ndef build_adjustment_factors(\n    corporate_actions: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Construct cumulative share adjustment factors from corporate actions.\n    \n    This is the Vietnamese equivalent of CRSP's cfacshr factor. For each\n    ticker, we compute a cumulative product of adjustment ratios from\n    corporate actions, working forward in time.\n    \n    The adjustment factor at date t converts historical share counts to\n    be comparable with current (post-action) share counts:\n    \n        shares_adjusted_t = shares_raw_t * cfacshr_t\n    \n    Parameters\n    ----------\n    corporate_actions : pd.DataFrame\n        Corporate actions with columns: ticker, ex_date, action_type,\n        ratio. The ratio field represents:\n        - Stock dividend 20%: ratio = 1.20\n        - 2:1 stock split: ratio = 2.00\n        - Bonus shares 10%: ratio = 1.10\n    \n    Returns\n    -------\n    pd.DataFrame\n        Adjustment factors: ticker, ex_date, cfacshr (cumulative).\n    \"\"\"\n    share_actions = corporate_actions[\n        corporate_actions['action_type'].isin([\n            'stock_dividend', 'bonus_shares', 'stock_split',\n            'reverse_split', 'rights_issue'\n        ])\n    ].copy()\n    \n    if share_actions.empty:\n        return pd.DataFrame(columns=['ticker', 'ex_date', 'cfacshr'])\n    \n    share_actions = share_actions.sort_values(['ticker', 'ex_date'])\n    \n    share_actions['cfacshr'] = (\n        share_actions\n        .groupby('ticker')['ratio']\n        .cumprod()\n    )\n    \n    return share_actions[['ticker', 'ex_date', 'cfacshr']].reset_index(\n        drop=True\n    )\n\n\ndef get_cfacshr_at_date(\n    ticker: str,\n    date: pd.Timestamp,\n    adj_factors: pd.DataFrame,\n) -&gt; float:\n    \"\"\"\n    Look up the cumulative share adjustment factor for a given\n    ticker and date. Returns 1.0 if no corporate actions occurred.\n    \"\"\"\n    mask = (\n        (adj_factors['ticker'] == ticker) &\n        (adj_factors['ex_date'] &lt;= date)\n    )\n    subset = adj_factors.loc[mask]\n    \n    if subset.empty:\n        return 1.0\n    return subset.iloc[-1]['cfacshr']\n\n\ndef adjust_shares_between_dates(\n    shares: float,\n    ticker: str,\n    date_from: pd.Timestamp,\n    date_to: pd.Timestamp,\n    adj_factors: pd.DataFrame,\n) -&gt; float:\n    \"\"\"\n    Adjust a share count observed at date_from to be comparable\n    with shares observed at date_to, accounting for all intervening\n    corporate actions.\n    \n    Example\n    -------\n    &gt;&gt;&gt; # Investor held 1000 shares on 2023-01-01\n    &gt;&gt;&gt; # A 20% stock dividend occurred on 2023-03-15\n    &gt;&gt;&gt; adjust_shares_between_dates(\n    ...     1000, 'VNM',\n    ...     pd.Timestamp('2023-01-01'),\n    ...     pd.Timestamp('2023-06-30'), adj_factors\n    ... )\n    1200.0\n    \"\"\"\n    factor_from = get_cfacshr_at_date(ticker, date_from, adj_factors)\n    factor_to = get_cfacshr_at_date(ticker, date_to, adj_factors)\n    relative_factor = factor_to / factor_from\n    return shares * relative_factor",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalprice-processing",
    "href": "52_institutional_trade_flow.html#sec-institutionalprice-processing",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "36.2 Monthly and Quarterly Price Processing",
    "text": "36.2 Monthly and Quarterly Price Processing\n\ndef process_prices(\n    prices: pd.DataFrame,\n    adj_factors: pd.DataFrame,\n    begdate: str = '2010-01-01',\n    enddate: str = '2024-12-31',\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Process raw DataCore.vn price data into analysis-ready format.\n    \n    Block logic:\n    1. Filter to date range\n    2. Compute adjusted prices and shares outstanding\n    3. Compute quarterly compounded returns\n    4. Create forward quarterly returns (shifted one quarter)\n    \n    Parameters\n    ----------\n    prices : pd.DataFrame\n        Raw price data with: ticker, date, close, adjusted_close,\n        volume, shares_outstanding.\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors.\n    begdate, enddate : str\n        Sample period boundaries.\n    \n    Returns\n    -------\n    Tuple[pd.DataFrame, pd.DataFrame]\n        (price_quarterly, qret): quarter-end observations with\n        adjusted price, total shares, and forward quarterly return.\n    \"\"\"\n    price = prices[\n        (prices['date'] &gt;= begdate) & (prices['date'] &lt;= enddate)\n    ].copy()\n    \n    # Month-end and quarter-end dates\n    price['mdate'] = price['date'] + pd.offsets.MonthEnd(0)\n    price['qdate'] = price['date'] + pd.offsets.QuarterEnd(0)\n    \n    # Adjusted price\n    if 'adjusted_close' in price.columns:\n        price['p'] = price['adjusted_close']\n    else:\n        price['p'] = price['close']\n    \n    # Total shares outstanding\n    price['tso'] = price['shares_outstanding']\n    \n    # Market capitalization (millions VND)\n    price['mcap'] = price['p'] * price['tso'] / 1e6\n    \n    # Filter out zero shares\n    price = price[price['tso'] &gt; 0].copy()\n    \n    # Compute daily returns if not present\n    if 'ret' not in price.columns:\n        price = price.sort_values(['ticker', 'date'])\n        price['ret'] = price.groupby('ticker')['p'].pct_change()\n    \n    price['ret'] = price['ret'].fillna(0)\n    price['logret'] = np.log(1 + price['ret'])\n    \n    # ---- Quarterly compounded returns ----\n    qret = (\n        price\n        .groupby(['ticker', 'qdate'])['logret']\n        .sum()\n        .reset_index()\n    )\n    qret['qret'] = np.exp(qret['logret']) - 1\n    \n    # Shift qdate back one quarter: make qret a *forward* return\n    qret['qdate'] = qret['qdate'] + pd.offsets.QuarterEnd(-1)\n    qret = qret.drop(columns=['logret'])\n    \n    # ---- Quarter-end observations ----\n    price_q = price[price['qdate'] == price['mdate']].copy()\n    price_q = price_q[['qdate', 'ticker', 'p', 'tso', 'mcap']].copy()\n    \n    # Merge forward quarterly return\n    price_q = price_q.merge(qret, on=['ticker', 'qdate'], how='left')\n    \n    # Build cfacshr lookup at each quarter-end\n    price_q['cfacshr'] = price_q.apply(\n        lambda row: get_cfacshr_at_date(\n            row['ticker'], row['qdate'], adj_factors\n        ),\n        axis=1\n    )\n    \n    return price_q, qret\n\n\n\n\n\n\n\nTipPerformance Optimization\n\n\n\nThe get_cfacshr_at_date function uses a row-wise lookup which can be slow for large datasets. For production use with millions of rows, vectorize using pd.merge_asof():\nprice_q = pd.merge_asof(\n    price_q.sort_values('qdate'),\n    adj_factors.sort_values('ex_date'),\n    by='ticker',\n    left_on='qdate',\n    right_on='ex_date',\n    direction='backward'\n).fillna({'cfacshr': 1.0})\n\n\nThe output is a quarterly panel of stock-level observations (@tbl-institutional-price-vars)\n\n\n\nTable 36.1: Quarter-End Price Panel Variables\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nticker\nStock ticker (e.g., VNM, VCB, FPT)\n\n\nqdate\nQuarter-end date\n\n\np\nAdjusted closing price (VND)\n\n\ntso\nTotal shares outstanding\n\n\nmcap\nMarket capitalization (millions VND)\n\n\nqret\nForward quarterly compounded return\n\n\ncfacshr\nCumulative share adjustment factor",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionaltaxonomy",
    "href": "52_institutional_trade_flow.html#sec-institutionaltaxonomy",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "37.1 Ownership Taxonomy",
    "text": "37.1 Ownership Taxonomy\nWe define a classification system for Vietnamese shareholders that maps to the categories available in DataCore.vn:\n\nclass OwnershipType:\n    \"\"\"\n    Vietnamese ownership type classification.\n    \n    Vietnam's ownership structure is fundamentally different from the US:\n    \n    - **State** (Nha nuoc): SCIC, ministries, state-owned parents\n    - **Foreign Institutional** (To chuc nuoc ngoai): foreign funds,\n      ETFs, pension funds, insurance, sovereign wealth funds\n    - **Domestic Institutional** (To chuc trong nuoc): Vietnamese\n      securities companies, fund managers, banks, insurance\n    - **Individual** (Ca nhan): retail investors (domestic + foreign)\n    - **Treasury** (Co phieu quy): company repurchases\n    \"\"\"\n    \n    STATE = 'State'\n    FOREIGN_INST = 'Foreign Institutional'\n    DOMESTIC_INST = 'Domestic Institutional'\n    INDIVIDUAL = 'Individual'\n    TREASURY = 'Treasury'\n    \n    INSTITUTIONAL = [FOREIGN_INST, DOMESTIC_INST]\n    ALL_INSTITUTIONAL = [STATE, FOREIGN_INST, DOMESTIC_INST]\n    ALL_TYPES = [STATE, FOREIGN_INST, DOMESTIC_INST, INDIVIDUAL, TREASURY]\n    \n    STATE_KEYWORDS = [\n        'scic', 'state capital', 'bo', 'ubnd', 'tong cong ty',\n        'nha nuoc', 'state', 'government', \"people's committee\",\n        'ministry', 'vietnam national', 'vnpt', 'evn', 'pvn',\n    ]\n    \n    FOREIGN_KEYWORDS = [\n        'fund', 'investment', 'capital', 'asset management',\n        'securities', 'gic', 'templeton', 'dragon capital',\n        'vinacapital', 'mekong capital', 'kb securities',\n        'mirae asset', 'samsung', 'jp morgan', 'goldman',\n        'blackrock', 'vanguard', 'aberdeen', 'hsbc',\n    ]\n    \n    @classmethod\n    def classify(cls, row: pd.Series) -&gt; str:\n        \"\"\"Classify based on explicit flags, then keyword fallback.\"\"\"\n        if pd.notna(row.get('is_state')) and row['is_state']:\n            return cls.STATE\n        if pd.notna(row.get('is_foreign')) and row['is_foreign']:\n            if pd.notna(row.get('is_institution')) and row['is_institution']:\n                return cls.FOREIGN_INST\n            return cls.INDIVIDUAL\n        if pd.notna(row.get('is_institution')) and row['is_institution']:\n            return cls.DOMESTIC_INST\n        \n        name = str(row.get('shareholder_name', '')).lower()\n        if any(kw in name for kw in cls.STATE_KEYWORDS):\n            return cls.STATE\n        if any(kw in name for kw in cls.FOREIGN_KEYWORDS):\n            return cls.FOREIGN_INST\n        \n        return cls.INDIVIDUAL",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalholdings-panel",
    "href": "52_institutional_trade_flow.html#sec-institutionalholdings-panel",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "37.2 Building the Holdings Panel",
    "text": "37.2 Building the Holdings Panel\nWe construct the holdings panel (i.e., the Vietnamese equivalent of merging the 13F Type 1 and Type 3 datasets). The key steps are:\n\nIdentify the first available vintage for each shareholder-stock-report date combination.\nCompute reporting gaps to flag first and last reports.\nClassify shareholders.\nAdjust shares for corporate actions.\n\n\ndef build_holdings_panel(\n    ownership: pd.DataFrame,\n    adj_factors: pd.DataFrame,\n    price_q: pd.DataFrame,\n    company_profile: pd.DataFrame,\n    begdate: str = '2010-01-01',\n    enddate: str = '2024-12-31',\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Construct the institutional holdings panel from DataCore.vn\n    ownership data.\n    \"\"\"\n    own = ownership.copy()\n    \n    # Align to quarter-end\n    own['rdate'] = own['date'] + pd.offsets.QuarterEnd(0)\n    own['fdate'] = own['date']\n    \n    own = own[\n        (own['rdate'] &gt;= begdate) & (own['rdate'] &lt;= enddate)\n    ].copy()\n    \n    # Keep earliest vintage per shareholder-ticker-rdate\n    own = own.sort_values(\n        ['shareholder_name', 'ticker', 'rdate', 'fdate']\n    )\n    fst_vint = (\n        own\n        .groupby(['shareholder_name', 'ticker', 'rdate'])\n        .first()\n        .reset_index()\n    )\n    \n    # ---- Reporting gaps for first/last flags ----\n    fst_vint = fst_vint.sort_values(\n        ['shareholder_name', 'ticker', 'rdate']\n    )\n    \n    grp = fst_vint.groupby(['shareholder_name', 'ticker'])\n    fst_vint['lag_rdate'] = grp['rdate'].shift(1)\n    \n    fst_vint['qtr_gap'] = fst_vint.apply(\n        lambda r: (\n            (r['rdate'].to_period('Q')\n             - r['lag_rdate'].to_period('Q')).n\n            if pd.notna(r['lag_rdate']) else np.nan\n        ),\n        axis=1\n    )\n    \n    fst_vint['first_report'] = (\n        fst_vint['qtr_gap'].isna() | (fst_vint['qtr_gap'] &gt;= 2)\n    )\n    \n    # Last report flag (forward gap)\n    fst_vint = fst_vint.sort_values(\n        ['shareholder_name', 'ticker', 'rdate'],\n        ascending=[True, True, False]\n    )\n    fst_vint['lead_rdate'] = grp['rdate'].shift(1)\n    \n    fst_vint['lead_gap'] = fst_vint.apply(\n        lambda r: (\n            (r['lead_rdate'].to_period('Q')\n             - r['rdate'].to_period('Q')).n\n            if pd.notna(r['lead_rdate']) else np.nan\n        ),\n        axis=1\n    )\n    \n    fst_vint['last_report'] = (\n        fst_vint['lead_gap'].isna() | (fst_vint['lead_gap'] &gt;= 2)\n    )\n    \n    fst_vint = fst_vint.drop(\n        columns=['lag_rdate', 'qtr_gap', 'lead_rdate', 'lead_gap'],\n        errors='ignore'\n    )\n    \n    # ---- Classify shareholders ----\n    fst_vint['owner_type'] = fst_vint.apply(\n        OwnershipType.classify, axis=1\n    )\n    \n    # ---- Adjust shares for corporate actions ----\n    fst_vint = fst_vint.merge(\n        price_q[['ticker', 'qdate', 'cfacshr']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    fst_vint['shares_adj'] = (\n        fst_vint['shares_held'] * fst_vint['cfacshr']\n    )\n    fst_vint = fst_vint[fst_vint['shares_adj'] &gt; 0].copy()\n    \n    fst_vint = fst_vint.drop_duplicates(\n        subset=['shareholder_name', 'ticker', 'rdate']\n    )\n    \n    # Merge company profile\n    if company_profile is not None:\n        fst_vint = fst_vint.merge(\n            company_profile[['ticker', 'exchange', 'fol_limit']]\n            .drop_duplicates(),\n            on='ticker',\n            how='left'\n        )\n    \n    cols = [\n        'shareholder_name', 'ticker', 'rdate', 'fdate',\n        'shares_held', 'shares_adj', 'owner_type',\n        'first_report', 'last_report'\n    ]\n    if 'exchange' in fst_vint.columns:\n        cols.extend(['exchange', 'fol_limit'])\n    \n    holdings = fst_vint[cols].copy()\n    \n    print(f\"Holdings panel: {len(holdings):,} observations\")\n    print(f\"  Shareholders: {holdings['shareholder_name'].nunique():,}\")\n    print(f\"  Stocks: {holdings['ticker'].nunique():,}\")\n    print(f\"  Quarters: {holdings['rdate'].nunique()}\")\n    \n    return holdings",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalio-ratio",
    "href": "52_institutional_trade_flow.html#sec-institutionalio-ratio",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "38.1 Institutional Ownership Ratio",
    "text": "38.1 Institutional Ownership Ratio\nThe institutional ownership ratio (IO) for stock \\(i\\) at time \\(t\\) is:\n\\[\nIO_{i,t} = \\frac{\\sum_{j \\in \\mathcal{J}} h_{j,i,t}}{TSO_{i,t}}\n\\tag{38.1}\\]\nwhere \\(\\mathcal{J}\\) is the set of institutional investors and \\(TSO_{i,t}\\) is total shares outstanding. In Vietnam, we compute separate ratios for each ownership type:\n\\[\nIO_{i,t}^{\\text{type}} = \\frac{\\sum_{j \\in \\mathcal{J}^{\\text{type}}} h_{j,i,t}}{TSO_{i,t}},\n\\quad \\text{type} \\in \\{\\text{State}, \\text{Foreign}, \\text{Domestic}, \\text{Individual}\\}\n\\tag{38.2}\\]\n\ndef compute_io_ratios(\n    holdings: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute IO ratios by type for each stock-quarter.\"\"\"\n    agg = (\n        holdings\n        .groupby(['ticker', 'rdate', 'owner_type'])['shares_adj']\n        .sum()\n        .reset_index()\n    )\n    \n    io_wide = agg.pivot_table(\n        index=['ticker', 'rdate'],\n        columns='owner_type',\n        values='shares_adj',\n        fill_value=0\n    ).reset_index()\n    \n    io_wide.columns = [\n        c if c in ['ticker', 'rdate']\n        else f'shares_{c.lower().replace(\" \", \"_\")}'\n        for c in io_wide.columns\n    ]\n    \n    io_wide = io_wide.merge(\n        price_q[['ticker', 'qdate', 'tso']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    share_cols = [c for c in io_wide.columns if c.startswith('shares_')]\n    for col in share_cols:\n        ratio_name = col.replace('shares_', 'io_')\n        io_wide[ratio_name] = io_wide[col] / io_wide['tso']\n    \n    inst_cols = [\n        c for c in io_wide.columns\n        if c.startswith('shares_')\n        and 'individual' not in c\n        and 'treasury' not in c\n    ]\n    io_wide['io_total_inst'] = (\n        io_wide[inst_cols].sum(axis=1) / io_wide['tso']\n    )\n    \n    return io_wide",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalhhi",
    "href": "52_institutional_trade_flow.html#sec-institutionalhhi",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "38.2 Ownership Concentration: Herfindahl-Hirschman Index",
    "text": "38.2 Ownership Concentration: Herfindahl-Hirschman Index\nThe HHI measures ownership concentration:\n\\[\nHHI_{i,t} = \\sum_{j=1}^{N_{i,t}} \\left(\\frac{h_{j,i,t}}{\\sum_{k=1}^{N_{i,t}} h_{k,i,t}}\\right)^2\n\\tag{38.3}\\]\nwhere \\(N_{i,t}\\) is the number of shareholders. HHI ranges from \\(1/N_{i,t}\\) (equal) to 1 (single shareholder). In Vietnam, ownership tends to be highly concentrated due to large state and founding-family blocks.\n\ndef compute_hhi(holdings: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute HHI for each stock-quarter, overall and institutional.\"\"\"\n    def _hhi(shares: pd.Series) -&gt; float:\n        total = shares.sum()\n        if total &lt;= 0:\n            return np.nan\n        weights = shares / total\n        return (weights ** 2).sum()\n    \n    hhi_overall = (\n        holdings.groupby(['ticker', 'rdate'])['shares_adj']\n        .apply(_hhi).reset_index()\n        .rename(columns={'shares_adj': 'hhi_overall'})\n    )\n    \n    inst = holdings[\n        holdings['owner_type'].isin(OwnershipType.ALL_INSTITUTIONAL)\n    ]\n    hhi_inst = (\n        inst.groupby(['ticker', 'rdate'])['shares_adj']\n        .apply(_hhi).reset_index()\n        .rename(columns={'shares_adj': 'hhi_institutional'})\n    )\n    \n    return hhi_overall.merge(hhi_inst, on=['ticker', 'rdate'], how='left')",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalbreadth",
    "href": "52_institutional_trade_flow.html#sec-institutionalbreadth",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "38.3 Ownership Breadth",
    "text": "38.3 Ownership Breadth\nFollowing Chen, Jegadeesh, and Wermers (2000), ownership breadth is the number of institutional holders:\n\\[\n\\text{Breadth}_{i,t} = \\#\\{j : h_{j,i,t} &gt; 0, \\, j \\in \\mathcal{J}\\}\n\\tag{38.4}\\]\nThe change in breadth predicts future returns:\n\\[\n\\Delta\\text{Breadth}_{i,t} = \\text{Breadth}_{i,t} - \\text{Breadth}_{i,t-1}\n\\tag{38.5}\\]\n\ndef compute_breadth(holdings: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Compute ownership breadth and changes by type.\"\"\"\n    breadth = (\n        holdings[\n            holdings['owner_type'].isin(OwnershipType.ALL_INSTITUTIONAL)\n        ]\n        .groupby(['ticker', 'rdate', 'owner_type'])['shareholder_name']\n        .nunique()\n        .reset_index()\n        .rename(columns={'shareholder_name': 'n_holders'})\n    )\n    \n    breadth_wide = breadth.pivot_table(\n        index=['ticker', 'rdate'],\n        columns='owner_type',\n        values='n_holders',\n        fill_value=0\n    ).reset_index()\n    \n    breadth_wide.columns = [\n        c if c in ['ticker', 'rdate']\n        else f'n_{c.lower().replace(\" \", \"_\")}'\n        for c in breadth_wide.columns\n    ]\n    \n    n_cols = [c for c in breadth_wide.columns if c.startswith('n_')]\n    breadth_wide['n_total_inst'] = breadth_wide[n_cols].sum(axis=1)\n    \n    breadth_wide = breadth_wide.sort_values(['ticker', 'rdate'])\n    for col in n_cols + ['n_total_inst']:\n        breadth_wide[f'd_{col}'] = (\n            breadth_wide.groupby('ticker')[col].diff()\n        )\n    \n    return breadth_wide\n\n(\\(\\text{BS} = -1\\)) is generated for the prior position, dated to the quarter after the last report.\nFor intermediate gaps (reports at \\(t-2\\) and \\(t\\) but not \\(t-1\\)), we split into:\n\nA terminating sale at \\(t-1\\) of \\(-h_{j,i,t-2}^{\\text{adj}}\\);\nAn initiating buy at \\(t\\) of \\(h_{j,i,t}\\).",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionaltrade-impl",
    "href": "52_institutional_trade_flow.html#sec-institutionaltrade-impl",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "38.4 Implementation",
    "text": "38.4 Implementation\n\ndef compute_trades(\n    holdings: pd.DataFrame,\n    adj_factors: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute institutional trades from holdings panel.\n    \n    Uses vectorized conditional logic (NOT apply()) for performance.\n    \n    Algorithm:\n    1. Sort holdings by shareholder, ticker, quarter\n    2. Compute lagged holdings and reporting gaps\n    3. Apply modified trade logic based on first_report, gap\n    4. Handle terminating sales and intermediate gaps\n    5. Append all trade records\n    \"\"\"\n    t1 = holdings.sort_values(\n        ['shareholder_name', 'ticker', 'rdate']\n    ).copy()\n    \n    # Previous holding quarter and shares\n    grp = t1.groupby(['shareholder_name', 'ticker'])\n    t1['phrdate'] = grp['rdate'].shift(1)\n    t1['pshares_adj'] = grp['shares_adj'].shift(1)\n    \n    # Raw trade\n    t1['trade'] = t1['shares_adj'] - t1['pshares_adj']\n    \n    # Quarter gap\n    t1['qtrgap'] = t1.apply(\n        lambda r: (\n            (r['rdate'].to_period('Q')\n             - r['phrdate'].to_period('Q')).n\n            if pd.notna(r['phrdate']) else np.nan\n        ),\n        axis=1\n    )\n    \n    # Boundary detection keys\n    t1['l_key'] = (\n        t1['shareholder_name'] + '_' + t1['ticker']\n    ).shift(1)\n    t1['n_key'] = (\n        t1['shareholder_name'] + '_' + t1['ticker']\n    ).shift(-1)\n    t1['curr_key'] = t1['shareholder_name'] + '_' + t1['ticker']\n    \n    # ---- Vectorized trade classification ----\n    is_new = (t1['curr_key'] != t1['l_key'])\n    not_first = ~t1['first_report']\n    consec = (t1['qtrgap'] == 1)\n    gap = (t1['qtrgap'] != 1) & t1['qtrgap'].notna()\n    \n    cond1   = is_new\n    cond1_1 = is_new & not_first\n    cond2_1 = (~is_new) & not_first & consec\n    cond2_2 = (~is_new) & not_first & gap\n    \n    # Modified trade amounts\n    t1['modtrade'] = t1['trade']\n    t1.loc[cond1, 'modtrade'] = np.nan\n    t1.loc[cond1_1, 'modtrade'] = t1.loc[cond1_1, 'shares_adj']\n    t1.loc[cond2_1, 'modtrade'] = t1.loc[cond2_1, 'trade']\n    t1.loc[cond2_2, 'modtrade'] = t1.loc[cond2_2, 'shares_adj']\n    \n    # Buy/sale classification\n    t1['buysale'] = np.nan\n    t1.loc[cond1_1, 'buysale'] = 1\n    t1.loc[cond2_1, 'buysale'] = (\n        2 * np.sign(t1.loc[cond2_1, 'trade'])\n    )\n    t1.loc[cond2_2, 'buysale'] = 1.5  # placeholder for split\n    \n    # ---- Handle intermediate gaps (buysale == 1.5) ----\n    t2 = t1[t1['buysale'] == 1.5].copy()\n    t2['rdate'] = t2['phrdate'] + pd.offsets.QuarterEnd(1)\n    t2['buysale'] = -1\n    t2['modtrade'] = -t2['pshares_adj']\n    \n    t1.loc[t1['buysale'] == 1.5, 'buysale'] = 1\n    \n    # ---- Terminating sales ----\n    is_last_combo = (t1['curr_key'] != t1['n_key'])\n    not_last_rpt = ~t1['last_report']\n    \n    t3 = t1[is_last_combo & not_last_rpt].copy()\n    t3['rdate'] = t3['rdate'] + pd.offsets.QuarterEnd(1)\n    t3['modtrade'] = -t3['shares_adj']\n    t3['buysale'] = -1\n    \n    # ---- Combine ----\n    trades = pd.concat([t1, t2, t3], ignore_index=True)\n    trades = trades[\n        (trades['modtrade'] != 0) &\n        trades['modtrade'].notna() &\n        trades['buysale'].notna()\n    ].copy()\n    \n    trades = trades[[\n        'rdate', 'shareholder_name', 'ticker', 'modtrade',\n        'buysale', 'owner_type', 'first_report', 'last_report'\n    ]].rename(columns={'modtrade': 'trade'})\n    \n    print(f\"\\nTrade computation complete:\")\n    print(f\"  Total records: {len(trades):,}\")\n    print(f\"  Initiating buys:  {(trades['buysale'] == 1).sum():,}\")\n    print(f\"  Incremental buys: {(trades['buysale'] == 2).sum():,}\")\n    print(f\"  Terminating sales:{(trades['buysale'] == -1).sum():,}\")\n    print(f\"  Regular sales:    {(trades['buysale'] == -2).sum():,}\")\n    \n    return trades\n\n\n38.4.1 Trade Visualization\n\n\n\n\nCode\ndef plot_trade_distribution(trades: pd.DataFrame):\n    \"\"\"Plot time series of trade types by quarter.\"\"\"\n    bs_labels = {\n        1: 'Initiating Buy', 2: 'Incremental Buy',\n        -1: 'Terminating Sale', -2: 'Regular Sale'\n    }\n    trades = trades.copy()\n    trades['trade_type'] = trades['buysale'].map(bs_labels)\n    \n    counts = (\n        trades\n        .groupby([pd.Grouper(key='rdate', freq='QE'), 'trade_type'])\n        .size()\n        .unstack(fill_value=0)\n    )\n    \n    fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n    \n    buy_cols = [c for c in counts.columns if 'Buy' in c]\n    counts[buy_cols].plot(\n        kind='bar', stacked=True, ax=axes[0],\n        color=['#1f77b4', '#aec7e8'], width=0.8\n    )\n    axes[0].set_title('Panel A: Institutional Purchases', fontweight='bold')\n    axes[0].set_ylabel('Number of Trades')\n    \n    sale_cols = [c for c in counts.columns if 'Sale' in c]\n    counts[sale_cols].plot(\n        kind='bar', stacked=True, ax=axes[1],\n        color=['#d62728', '#ff9896'], width=0.8\n    )\n    axes[1].set_title('Panel B: Institutional Sales', fontweight='bold')\n    axes[1].set_ylabel('Number of Trades')\n    \n    for ax in axes:\n        ax.tick_params(axis='x', rotation=45)\n        for i, label in enumerate(ax.get_xticklabels()):\n            if i % 4 != 0:\n                label.set_visible(False)\n    \n    plt.tight_layout()\n    plt.show()\n\n# plot_trade_distribution(trades)\n\n\n\nFigure 38.1\n\n\n\n\n\n\n\nCode\ndef plot_net_trading_by_type(trades: pd.DataFrame, price_q: pd.DataFrame):\n    \"\"\"Plot net trading volume by owner type over time.\"\"\"\n    _t = trades.merge(\n        price_q[['ticker', 'qdate', 'p']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    _t['trade_vnd'] = _t['trade'] * _t['p'] / 1e9\n    \n    net = (\n        _t\n        .groupby([pd.Grouper(key='rdate', freq='QE'), 'owner_type'])\n        ['trade_vnd'].sum()\n        .unstack(fill_value=0)\n    )\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    for col in net.columns:\n        ax.plot(net.index, net[col], label=col,\n                color=OWNER_COLORS.get(col, '#333'), linewidth=1.5)\n    ax.axhline(y=0, color='black', linewidth=0.5)\n    ax.set_title('Net Institutional Trading by Ownership Type',\n                 fontweight='bold')\n    ax.set_ylabel('Net Trading (Billions VND)')\n    ax.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n\n# plot_net_trading_by_type(trades, price_q)\n\n\n\nFigure 38.2",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalassets",
    "href": "52_institutional_trade_flow.html#sec-institutionalassets",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "39.1 Total Assets and Portfolio Returns",
    "text": "39.1 Total Assets and Portfolio Returns\nFor each manager \\(j\\) and quarter \\(t\\), portfolio assets are:\n\\[\nA_{j,t} = \\sum_{i=1}^{N_{j,t}} h_{j,i,t} \\cdot P_{i,t}\n\\tag{39.1}\\]\nThe portfolio return assuming buy-and-hold is:\n\\[\nR_{j,t}^{p} = \\frac{\\sum_{i=1}^{N_{j,t}} h_{j,i,t} \\cdot P_{i,t} \\cdot r_{i,t+1}}\n{\\sum_{i=1}^{N_{j,t}} h_{j,i,t} \\cdot P_{i,t}}\n\\tag{39.2}\\]\n\ndef compute_assets_and_returns(\n    holdings: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute total portfolio assets and buy-and-hold returns.\"\"\"\n    _assets = holdings[\n        ['shareholder_name', 'ticker', 'rdate', 'shares_adj']\n    ].merge(\n        price_q[['ticker', 'qdate', 'p', 'qret']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    _assets['hold_per_stock'] = _assets['shares_adj'] * _assets['p'] / 1e6\n    _assets['next_value'] = (\n        _assets['shares_adj'] * _assets['p'] * _assets['qret']\n    )\n    _assets['curr_value'] = _assets['shares_adj'] * _assets['p']\n    \n    assets = (\n        _assets\n        .groupby(['shareholder_name', 'rdate'])\n        .agg(\n            assets=('hold_per_stock', 'sum'),\n            total_next=('next_value', 'sum'),\n            total_curr=('curr_value', 'sum'),\n        )\n        .reset_index()\n    )\n    \n    assets['pret'] = assets['total_next'] / assets['total_curr']\n    assets = assets.drop(columns=['total_next', 'total_curr'])\n    return assets",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalaggregate-buysales",
    "href": "52_institutional_trade_flow.html#sec-institutionalaggregate-buysales",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "39.2 Aggregate Buys and Sales",
    "text": "39.2 Aggregate Buys and Sales\nTotal buys and sales for manager \\(j\\) in quarter \\(t\\):\n\\[\nB_{j,t} = \\sum_{i : \\Delta h &gt; 0} \\Delta h_{j,i,t} \\cdot P_{i,t}, \\qquad\nS_{j,t} = \\sum_{i : \\Delta h &lt; 0} |\\Delta h_{j,i,t}| \\cdot P_{i,t}\n\\tag{39.3}\\]\nThe trade gain is:\n\\[\nG_{j,t} = \\sum_{i=1}^{N_{j,t}} \\Delta h_{j,i,t} \\cdot P_{i,t} \\cdot r_{i,t+1}\n\\tag{39.4}\\]\n\ndef compute_buys_sales(\n    trades: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute aggregate buys, sales, trade gains per manager-quarter.\"\"\"\n    _flows = trades.merge(\n        price_q[['ticker', 'qdate', 'p', 'qret']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    _flows['tbuys'] = (\n        _flows['trade'] * (_flows['trade'] &gt; 0).astype(float)\n        * _flows['p'] / 1e6\n    )\n    _flows['tsales'] = (\n        (-1) * _flows['trade'] * (_flows['trade'] &lt; 0).astype(float)\n        * _flows['p'] / 1e6\n    )\n    _flows['tgain'] = (\n        _flows['trade'] * _flows['p'] * _flows['qret'] / 1e6\n    )\n    \n    flows = (\n        _flows\n        .groupby(['shareholder_name', 'rdate'])\n        .agg(\n            tbuys=('tbuys', 'sum'),\n            tsales=('tsales', 'sum'),\n            tgain=('tgain', 'sum'),\n        )\n        .reset_index()\n    )\n    return flows",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalnetflows",
    "href": "52_institutional_trade_flow.html#sec-institutionalnetflows",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "40.1 Net Flows",
    "text": "40.1 Net Flows\nNet flows separate capital allocation decisions from investment returns:\n\\[\n\\text{NetFlow}_{j,t} = A_{j,t} - A_{j,t-1}(1 + R_{j,t}^p)\n\\tag{40.1}\\]\n\n\n\n\n\n\nWarningInterpreting Net Flows in Vietnam\n\n\n\nFor state entities or corporate cross-holders, “net flows” do not necessarily reflect investment decisions. State ownership changes often result from government policy (equitization, divestment programs). Interpretation should account for institutional context.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalturnover-measures",
    "href": "52_institutional_trade_flow.html#sec-institutionalturnover-measures",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "40.2 Three Turnover Measures",
    "text": "40.2 Three Turnover Measures\n\ndef compute_aggregates(\n    holdings: pd.DataFrame,\n    assets: pd.DataFrame,\n    flows: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute net flows and three turnover measures.\n    \n    1. Carhart (1997): min(buys, sales) / avg(assets)\n    2. Flow-adjusted: [min(buys, sales) + |net flows|] / lag assets\n    3. Symmetric: [buys + sales - |net flows|] / lag assets\n    \"\"\"\n    report_flags = (\n        holdings\n        .groupby(['shareholder_name', 'rdate'])\n        .agg(first_report=('first_report', 'any'),\n             last_report=('last_report', 'any'))\n        .reset_index()\n    )\n    \n    agg = report_flags.merge(\n        assets, on=['shareholder_name', 'rdate'], how='inner'\n    )\n    agg = agg.merge(\n        flows, on=['shareholder_name', 'rdate'], how='left'\n    )\n    \n    agg = agg.sort_values(['shareholder_name', 'rdate'])\n    \n    agg['assets_comp'] = agg['assets'] * (1 + agg['pret'].fillna(0))\n    \n    grp = agg.groupby('shareholder_name')\n    agg['lassets_comp'] = grp['assets_comp'].shift(1)\n    agg['lassets'] = grp['assets'].shift(1)\n    \n    # Trade gain return\n    agg['tgainret'] = agg['tgain'] / (agg['tbuys'] + agg['tsales'])\n    \n    # Net flows\n    agg['netflows'] = agg['assets'] - agg['lassets_comp']\n    \n    # Turnover 1: Carhart (1997)\n    agg['turnover1'] = (\n        agg[['tbuys', 'tsales']].min(axis=1) /\n        agg[['assets', 'lassets']].mean(axis=1)\n    )\n    \n    # Turnover 2: Flow-adjusted\n    agg['turnover2'] = (\n        (agg[['tbuys', 'tsales']].min(axis=1)\n         + agg['netflows'].abs().fillna(0))\n        / agg['lassets']\n    )\n    \n    # Turnover 3: Symmetric\n    agg['turnover3'] = (\n        (agg['tbuys'].fillna(0) + agg['tsales'].fillna(0)\n         - agg['netflows'].abs().fillna(0))\n        / agg['lassets']\n    )\n    \n    # Missing for first report\n    first_mask = agg['first_report']\n    for col in ['netflows', 'tgainret',\n                'turnover1', 'turnover2', 'turnover3']:\n        agg.loc[first_mask, col] = np.nan\n    \n    agg = agg.drop(columns=['assets_comp', 'lassets_comp', 'lassets'])\n    \n    print(f\"\\nAggregates: {len(agg):,} manager-quarters\")\n    print(f\"  Turnover1 mean: {agg['turnover1'].mean():.4f}\")\n    print(f\"  Turnover2 mean: {agg['turnover2'].mean():.4f}\")\n    print(f\"  Turnover3 mean: {agg['turnover3'].mean():.4f}\")\n    \n    return agg\n\n\n40.2.1 Turnover Summary Statistics\n\n\n\nTable 40.1: Summary statistics for three turnover measures across institutional investor types in Vietnam. Turnover 1 follows Carhart (1997), Turnover 2 adds back absolute net flows, and Turnover 3 uses the symmetric definition.\n\n\n\nCode\ndef turnover_summary_table(\n    aggregates: pd.DataFrame,\n    holdings: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Publication-quality turnover summary statistics table.\"\"\"\n    owner_map = (\n        holdings.groupby('shareholder_name')['owner_type']\n        .first().reset_index()\n    )\n    agg = aggregates.merge(owner_map, on='shareholder_name', how='left')\n    \n    turnover_cols = ['turnover1', 'turnover2', 'turnover3']\n    results = []\n    \n    for otype in ['All'] + OwnershipType.ALL_TYPES:\n        subset = agg if otype == 'All' else agg[agg['owner_type'] == otype]\n        row = {'Owner Type': otype, 'N': len(subset)}\n        for col in turnover_cols:\n            s = subset[col].dropna()\n            row[f'{col}_mean'] = s.mean()\n            row[f'{col}_median'] = s.median()\n            row[f'{col}_std'] = s.std()\n        results.append(row)\n    \n    return pd.DataFrame(results).round(4)\n\n# turnover_summary_table(aggregates, holdings)\n\n\n\n\n\n\n\n\nCode\ndef plot_turnover_timeseries(\n    aggregates: pd.DataFrame, holdings: pd.DataFrame\n):\n    \"\"\"Plot turnover time series by ownership type.\"\"\"\n    owner_map = (\n        holdings.groupby('shareholder_name')['owner_type']\n        .first().reset_index()\n    )\n    agg = aggregates.merge(owner_map, on='shareholder_name', how='left')\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    for otype in OwnershipType.ALL_INSTITUTIONAL:\n        subset = agg[agg['owner_type'] == otype]\n        qtr_mean = (\n            subset\n            .groupby(pd.Grouper(key='rdate', freq='QE'))['turnover1']\n            .mean()\n        )\n        ax.plot(qtr_mean.index, qtr_mean.values, label=otype,\n                color=OWNER_COLORS.get(otype, '#333'), linewidth=1.5)\n    \n    ax.set_title('Quarterly Average Turnover (Carhart)',\n                 fontweight='bold')\n    ax.set_ylabel('Turnover Ratio')\n    ax.legend(loc='best')\n    ax.yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n    plt.tight_layout()\n    plt.show()\n\n# plot_turnover_timeseries(aggregates, holdings)\n\n\n\nFigure 40.1",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalfol-util",
    "href": "52_institutional_trade_flow.html#sec-institutionalfol-util",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "41.1 FOL Utilization",
    "text": "41.1 FOL Utilization\n\\[\n\\text{FOL\\_Util}_{i,t} = \\frac{FO_{i,t}}{FOL_i}\n\\tag{41.1}\\]\nStocks with \\(\\text{FOL\\_Util}_{i,t} \\to 1\\) face mechanical foreign buying restrictions.\n\ndef compute_fol_analytics(\n    foreign_ownership: pd.DataFrame,\n    company_profile: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute FOL utilization and related metrics.\"\"\"\n    fo = foreign_ownership.copy()\n    fo = fo.merge(\n        company_profile[['ticker', 'fol_limit']].drop_duplicates(),\n        on='ticker', how='left'\n    )\n    \n    fo['fol_utilization'] = fo['foreign_pct'] / fo['fol_limit']\n    fo['foreign_room'] = fo['fol_limit'] - fo['foreign_pct']\n    fo['fol_binding'] = (fo['fol_utilization'] &gt;= 0.98)\n    fo['fol_category'] = pd.cut(\n        fo['fol_utilization'],\n        bins=[0, 0.25, 0.50, 0.75, 0.95, 1.0, float('inf')],\n        labels=['&lt;25%', '25-50%', '50-75%', '75-95%',\n                '95-100%', '&gt;100%']\n    )\n    return fo",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalroom-premium",
    "href": "52_institutional_trade_flow.html#sec-institutionalroom-premium",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "41.2 Room Premium Regression",
    "text": "41.2 Room Premium Regression\nWhen foreign ownership approaches the FOL, remaining “room” becomes scarce. We model:\n\\[\nr_{i,t+1} = \\alpha + \\beta_1 \\cdot \\text{FOL\\_Util}_{i,t} +\n\\beta_2 \\cdot \\text{FOL\\_Util}_{i,t}^2 + \\gamma \\cdot X_{i,t} + \\varepsilon_{i,t}\n\\tag{41.2}\\]\nThe quadratic term captures nonlinear acceleration of the premium as ownership approaches the limit.\n\ndef estimate_room_premium(\n    fol_analytics: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; dict:\n    \"\"\"Estimate foreign ownership room premium via panel regression.\"\"\"\n    fol_q = (\n        fol_analytics\n        .assign(qdate=lambda x: x['date'] + pd.offsets.QuarterEnd(0))\n        .groupby(['ticker', 'qdate'])\n        .agg(fol_utilization=('fol_utilization', 'last'),\n             foreign_room=('foreign_room', 'last'))\n        .reset_index()\n    )\n    \n    panel = fol_q.merge(\n        price_q[['ticker', 'qdate', 'mcap', 'qret']],\n        on=['ticker', 'qdate'], how='inner'\n    )\n    \n    panel['log_mcap'] = np.log(panel['mcap'] + 1)\n    panel['fol_util_sq'] = panel['fol_utilization'] ** 2\n    panel = panel.dropna(subset=['qret', 'fol_utilization', 'log_mcap'])\n    \n    X = panel[['fol_utilization', 'fol_util_sq', 'log_mcap']]\n    X = sm.add_constant(X)\n    y = panel['qret']\n    \n    model = sm.OLS(y, X).fit(\n        cov_type='cluster', cov_kwds={'groups': panel['ticker']}\n    )\n    return {'model': model, 'n_obs': len(panel)}\n\n# results = estimate_room_premium(fol_analytics, price_q)\n\n\n\n\n\nCode\ndef plot_fol_utilization(fol_analytics: pd.DataFrame):\n    \"\"\"Plot FOL utilization distribution.\"\"\"\n    latest = (\n        fol_analytics.sort_values(['ticker', 'date'])\n        .groupby('ticker').last().reset_index()\n    )\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    axes[0].hist(latest['fol_utilization'].dropna(), bins=50,\n                 color='#1f77b4', alpha=0.7, edgecolor='white')\n    axes[0].axvline(x=0.95, color='red', linestyle='--',\n                     label='95% threshold')\n    axes[0].set_title('Panel A: FOL Utilization Distribution',\n                       fontweight='bold')\n    axes[0].set_xlabel('FOL Utilization Ratio')\n    axes[0].set_ylabel('Number of Stocks')\n    axes[0].legend()\n    \n    for exch in ['HOSE', 'HNX', 'UPCOM']:\n        sub = latest[latest.get('exchange') == exch]\n        if len(sub) &gt; 0:\n            axes[1].hist(sub['fol_utilization'].dropna(), bins=30,\n                        alpha=0.5, label=exch,\n                        color=EXCHANGE_COLORS.get(exch, '#333'))\n    axes[1].set_title('Panel B: By Exchange', fontweight='bold')\n    axes[1].set_xlabel('FOL Utilization Ratio')\n    axes[1].legend()\n    \n    plt.tight_layout()\n    plt.show()\n\n# plot_fol_utilization(fol_analytics)\n\n\n\nFigure 41.1",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalherding",
    "href": "52_institutional_trade_flow.html#sec-institutionalherding",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "43.1 Herding Measures",
    "text": "43.1 Herding Measures\nFollowing Sias (2004), the Lakonishok-Shleifer-Vishny herding measure is:\n\\[\nHM_{i,t} = \\left|\\frac{B_{i,t}}{B_{i,t} + S_{i,t}} - p_t\\right|\n- E\\left[\\left|\\frac{B_{i,t}}{B_{i,t} + S_{i,t}} - p_t\\right|\\right]\n\\tag{43.1}\\]\nwhere \\(B_{i,t}\\) is the number of managers buying stock \\(i\\) in quarter \\(t\\), \\(S_{i,t}\\) the number selling, and \\(p_t\\) the expected buyer proportion under independent trading.\n\ndef compute_lsv_herding(\n    trades: pd.DataFrame,\n    min_traders: int = 5,\n) -&gt; pd.DataFrame:\n    \"\"\"Compute LSV herding measure for each stock-quarter.\"\"\"\n    tc = (\n        trades.groupby(['ticker', 'rdate'])\n        .apply(lambda g: pd.Series({\n            'n_buyers': (g['trade'] &gt; 0).sum(),\n            'n_sellers': (g['trade'] &lt; 0).sum(),\n            'n_traders': len(g),\n        }))\n        .reset_index()\n    )\n    \n    tc = tc[tc['n_traders'] &gt;= min_traders].copy()\n    tc['buy_prop'] = tc['n_buyers'] / tc['n_traders']\n    tc['p_t'] = tc.groupby('rdate')['buy_prop'].transform('mean')\n    tc['raw_hm'] = (tc['buy_prop'] - tc['p_t']).abs()\n    \n    def expected_abs_deviation(row):\n        n = int(row['n_traders'])\n        p = row['p_t']\n        if n == 0 or p == 0 or p == 1:\n            return 0\n        from scipy.stats import binom\n        k = np.arange(0, n + 1)\n        probs = binom.pmf(k, n, p)\n        return np.sum(np.abs(k / n - p) * probs)\n    \n    tc['expected_hm'] = tc.apply(expected_abs_deviation, axis=1)\n    tc['herding'] = tc['raw_hm'] - tc['expected_hm']\n    \n    tc['buy_herding'] = np.where(\n        tc['buy_prop'] &gt; tc['p_t'], tc['herding'], np.nan\n    )\n    tc['sell_herding'] = np.where(\n        tc['buy_prop'] &lt; tc['p_t'], tc['herding'], np.nan\n    )\n    \n    return tc[['ticker', 'rdate', 'n_buyers', 'n_sellers',\n               'n_traders', 'herding', 'buy_herding', 'sell_herding']]",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalpersistence",
    "href": "52_institutional_trade_flow.html#sec-institutionalpersistence",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "43.2 Demand Persistence",
    "text": "43.2 Demand Persistence\nSias (2004) showed institutional demand is persistent:\n\\[\n\\rho_t = \\text{Corr}\\left(\\Delta IO_{i,t},\\, \\Delta IO_{i,t-1}\\right)\n\\tag{43.2}\\]\n\ndef compute_demand_persistence(io_ratios: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Rolling cross-sectional correlation of IO changes.\"\"\"\n    io = io_ratios[['ticker', 'rdate', 'io_total_inst']].copy()\n    io = io.sort_values(['ticker', 'rdate'])\n    io['dio'] = io.groupby('ticker')['io_total_inst'].diff()\n    io['lag_dio'] = io.groupby('ticker')['dio'].shift(1)\n    \n    persistence = (\n        io.dropna(subset=['dio', 'lag_dio'])\n        .groupby('rdate')\n        .apply(lambda g: g['dio'].corr(g['lag_dio']))\n        .reset_index()\n        .rename(columns={0: 'persistence'})\n    )\n    persistence = persistence.sort_values('rdate')\n    persistence['persistence_ma'] = (\n        persistence['persistence'].rolling(window=20, min_periods=4).mean()\n    )\n    return persistence\n\n\n\n\n\nCode\ndef plot_demand_persistence(persistence: pd.DataFrame):\n    fig, ax = plt.subplots(figsize=(12, 5))\n    ax.bar(persistence['rdate'], persistence['persistence'],\n           width=80, alpha=0.3, color='#1f77b4', label='Quarterly')\n    ax.plot(persistence['rdate'], persistence['persistence_ma'],\n            color='#d62728', linewidth=2, label='Rolling Average')\n    ax.axhline(y=0, color='black', linewidth=0.5)\n    ax.set_title('Persistence of Institutional Demand', fontweight='bold')\n    ax.set_ylabel('Cross-Sectional Correlation')\n    ax.legend()\n    plt.tight_layout()\n    plt.show()\n\n\n\nFigure 43.1",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalinfo-content",
    "href": "52_institutional_trade_flow.html#sec-institutionalinfo-content",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "43.3 Information Content of Trades",
    "text": "43.3 Information Content of Trades\nFollowing Alexander, Cici, and Gibson (2007), the InfoTrade ratio measures the proportion of dollar trading from entry/exit decisions vs. position adjustments:\n\\[\n\\text{InfoTrade}_{i,t} = \\frac{\n\\sum_{j: BS \\in \\{+1,-1\\}} |\\Delta h_{j,i,t}| \\cdot P_{i,t}\n}{\n\\sum_j |\\Delta h_{j,i,t}| \\cdot P_{i,t}\n}\n\\tag{43.3}\\]\n\ndef compute_info_trade_ratio(\n    trades: pd.DataFrame, price_q: pd.DataFrame\n) -&gt; pd.DataFrame:\n    \"\"\"Compute info trade ratio for each stock-quarter.\"\"\"\n    _t = trades.merge(\n        price_q[['ticker', 'qdate', 'p']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    _t['dollar_trade'] = _t['trade'].abs() * _t['p'] / 1e6\n    _t['is_discrete'] = _t['buysale'].isin([1, -1])\n    \n    info = _t.groupby(['ticker', 'rdate']).apply(\n        lambda g: pd.Series({\n            'discrete_vol': g.loc[g['is_discrete'], 'dollar_trade'].sum(),\n            'total_vol': g['dollar_trade'].sum(),\n        })\n    ).reset_index()\n    \n    info['info_trade_ratio'] = (\n        info['discrete_vol'] / info['total_vol']\n    ).clip(0, 1)\n    return info",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalapp-returns",
    "href": "52_institutional_trade_flow.html#sec-institutionalapp-returns",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "44.1 Application 1: Institutional Ownership Changes and Future Returns",
    "text": "44.1 Application 1: Institutional Ownership Changes and Future Returns\nWe test whether changes in institutional ownership predict future stock returns (Chen, Jegadeesh, and Wermers 2000) via Fama-MacBeth regressions:\n\\[\nr_{i,t+1} = \\alpha_t + \\beta_{1,t} \\cdot \\Delta IO_{i,t} + \\beta_{2,t} \\cdot\n\\Delta\\text{Breadth}_{i,t} + \\gamma_t \\cdot X_{i,t} + \\varepsilon_{i,t}\n\\tag{44.1}\\]\n\ndef fama_macbeth_io_returns(\n    io_ratios: pd.DataFrame,\n    breadth: pd.DataFrame,\n    price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Run Fama-MacBeth regressions of future returns on IO changes.\"\"\"\n    panel = io_ratios[['ticker', 'rdate', 'io_total_inst']].merge(\n        breadth[['ticker', 'rdate', 'n_total_inst', 'd_n_total_inst']],\n        on=['ticker', 'rdate'], how='inner'\n    ).merge(\n        price_q[['ticker', 'qdate', 'mcap', 'qret']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    \n    panel = panel.sort_values(['ticker', 'rdate'])\n    panel['dio'] = panel.groupby('ticker')['io_total_inst'].diff()\n    panel['log_mcap'] = np.log(panel['mcap'] + 1)\n    panel['mom'] = panel.groupby('ticker')['qret'].shift(1)\n    \n    reg_vars = ['qret', 'dio', 'd_n_total_inst', 'log_mcap', 'mom']\n    panel = panel.dropna(subset=reg_vars)\n    \n    quarters = sorted(panel['rdate'].unique())\n    results = []\n    \n    for q in quarters:\n        qdata = panel[panel['rdate'] == q]\n        if len(qdata) &lt; 30:\n            continue\n        X = sm.add_constant(\n            qdata[['dio', 'd_n_total_inst', 'log_mcap', 'mom']]\n        )\n        try:\n            model = sm.OLS(qdata['qret'], X).fit()\n            coefs = model.params.to_dict()\n            coefs['rdate'] = q\n            coefs['n_obs'] = len(qdata)\n            results.append(coefs)\n        except Exception:\n            continue\n    \n    fm = pd.DataFrame(results)\n    \n    # Time-series averages with Newey-West t-statistics\n    print(\"\\nFama-MacBeth Results:\")\n    print(\"=\" * 50)\n    for var in ['const', 'dio', 'd_n_total_inst', 'log_mcap', 'mom']:\n        coefs = fm[var].dropna()\n        mean_c = coefs.mean()\n        nw_se = sm.OLS(\n            coefs - mean_c, np.ones(len(coefs))\n        ).fit(cov_type='HAC', cov_kwds={'maxlags': 4}).bse[0]\n        t = mean_c / nw_se if nw_se &gt; 0 else np.nan\n        print(f\"  {var:20s}: coef={mean_c:8.4f}, t={t:6.2f}\")\n    \n    return fm",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalapp-turnover",
    "href": "52_institutional_trade_flow.html#sec-institutionalapp-turnover",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "44.2 Application 2: Turnover and Performance",
    "text": "44.2 Application 2: Turnover and Performance\nYan (2008) documented a positive turnover-performance relationship. We test in Vietnam:\n\\[\n\\alpha_{j,t} = a + b \\cdot \\text{Turnover}_{j,t-1} + c \\cdot\n\\log(A_{j,t-1}) + d \\cdot \\text{Flow}_{j,t} + \\varepsilon_{j,t}\n\\tag{44.2}\\]\n\ndef turnover_performance_regression(\n    aggregates: pd.DataFrame,\n) -&gt; dict:\n    \"\"\"Test turnover-performance relationship.\"\"\"\n    agg = aggregates.sort_values(['shareholder_name', 'rdate']).copy()\n    agg['lag_turnover1'] = (\n        agg.groupby('shareholder_name')['turnover1'].shift(1)\n    )\n    agg['log_assets'] = np.log(agg['assets'] + 1)\n    agg['flow_ratio'] = agg['netflows'] / agg['assets'].shift(1)\n    \n    panel = agg.dropna(\n        subset=['pret', 'lag_turnover1', 'log_assets', 'flow_ratio']\n    )\n    \n    for col in ['pret', 'lag_turnover1', 'flow_ratio']:\n        lo, hi = panel[col].quantile([0.01, 0.99])\n        panel[col] = panel[col].clip(lo, hi)\n    \n    X = sm.add_constant(\n        panel[['lag_turnover1', 'log_assets', 'flow_ratio']]\n    )\n    model = sm.OLS(panel['pret'], X).fit(\n        cov_type='cluster',\n        cov_kwds={'groups': panel['shareholder_name']}\n    )\n    return {'model': model, 'n': len(panel)}",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalapp-foreign-domestic",
    "href": "52_institutional_trade_flow.html#sec-institutionalapp-foreign-domestic",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "44.3 Application 3: Foreign vs. Domestic Trading",
    "text": "44.3 Application 3: Foreign vs. Domestic Trading\n\ndef compare_foreign_domestic(\n    trades: pd.DataFrame, price_q: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Compare trading patterns between foreign and domestic institutions.\"\"\"\n    _t = trades.merge(\n        price_q[['ticker', 'qdate', 'p']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    _t['dollar_trade'] = _t['trade'] * _t['p'] / 1e6\n    _t['is_buy'] = _t['trade'] &gt; 0\n    \n    return (\n        _t[_t['owner_type'].isin(OwnershipType.INSTITUTIONAL)]\n        .groupby('owner_type')\n        .agg(\n            n_trades=('trade', 'count'),\n            n_buys=('is_buy', 'sum'),\n            avg_dollar=('dollar_trade', lambda x: x.abs().mean()),\n            net_buying=('dollar_trade', 'sum'),\n            pct_initiating=('buysale', lambda x: (x.abs() == 1).mean()),\n        )\n        .reset_index()\n    )\n\n\n\n\n\nCode\ndef plot_cumulative_net_buying(\n    trades: pd.DataFrame, price_q: pd.DataFrame\n):\n    _t = trades.merge(\n        price_q[['ticker', 'qdate', 'p']],\n        left_on=['ticker', 'rdate'],\n        right_on=['ticker', 'qdate'],\n        how='inner'\n    )\n    _t['trade_vnd'] = _t['trade'] * _t['p'] / 1e9\n    \n    inst = _t[_t['owner_type'].isin(OwnershipType.INSTITUTIONAL)]\n    net = (\n        inst.groupby(\n            [pd.Grouper(key='rdate', freq='QE'), 'owner_type']\n        )['trade_vnd'].sum().unstack(fill_value=0)\n    )\n    cum = net.cumsum()\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    for col in cum.columns:\n        ax.plot(cum.index, cum[col], label=col,\n                color=OWNER_COLORS.get(col, '#333'), linewidth=2)\n    ax.axhline(y=0, color='black', linewidth=0.5)\n    ax.set_title('Cumulative Net Institutional Buying', fontweight='bold')\n    ax.set_ylabel('Billions VND')\n    ax.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n\n\n\nFigure 44.1",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalpitfalls",
    "href": "52_institutional_trade_flow.html#sec-institutionalpitfalls",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "45.1 Common Pitfalls",
    "text": "45.1 Common Pitfalls\n\n45.1.1 Corporate Action Misadjustment\n\n\n\n\n\n\nCautionExample: Phantom Trade from Unadjusted Stock Dividend\n\n\n\nVinamilk (VNM) issues a 20% stock dividend with ex-date March 15, 2023.\n\nQ4 2022: Fund X holds 1,000,000 shares of VNM\nQ1 2023: Fund X holds 1,200,000 shares of VNM\n\nWithout adjustment: Inferred buy of +200,000 shares (BS = +2) With adjustment: Prior holdings become 1,200,000 adjusted shares, trade = 0\nThis phantom trade inflates measured turnover and creates spurious buying signals.\n\n\n\n\n45.1.2 Disclosure Timing Mismatches\nVietnamese ownership disclosure dates may not align with calendar quarter ends. Our pipeline addresses this by aligning all disclosures to the nearest quarter-end.\n\n\n45.1.3 Name Changes and Entity Mergers\nVietnamese institutions frequently rename. Without a stable identifier, the same entity may appear as two different shareholders, creating phantom entries/exits. We recommend maintaining a master entity mapping table.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "52_institutional_trade_flow.html#sec-institutionalvalidation",
    "href": "52_institutional_trade_flow.html#sec-institutionalvalidation",
    "title": "34  Institutional Trades, Flows, and Turnover Ratios",
    "section": "45.2 Validation Checks",
    "text": "45.2 Validation Checks\n\ndef validate_pipeline_outputs(\n    results: Dict[str, pd.DataFrame],\n) -&gt; pd.DataFrame:\n    \"\"\"Run comprehensive validation on pipeline outputs.\"\"\"\n    checks = []\n    h = results['holdings']\n    t = results['trades']\n    a = results['aggregates']\n    \n    checks.append({\n        'Check': 'No negative adjusted shares',\n        'Result': 'PASS' if (h['shares_adj'] &lt; 0).sum() == 0 else 'FAIL',\n        'Detail': f'{(h[\"shares_adj\"] &lt; 0).sum()} negative obs'\n    })\n    \n    checks.append({\n        'Check': 'No duplicate holdings',\n        'Result': 'PASS' if h.duplicated(\n            subset=['shareholder_name', 'ticker', 'rdate']\n        ).sum() == 0 else 'FAIL',\n    })\n    \n    checks.append({\n        'Check': 'Valid buysale codes only',\n        'Result': 'PASS' if t['buysale'].isin([1, 2, -1, -2]).all()\n        else 'FAIL',\n    })\n    \n    checks.append({\n        'Check': 'No zero trades',\n        'Result': 'PASS' if (t['trade'] == 0).sum() == 0 else 'FAIL',\n    })\n    \n    t1 = a['turnover1'].dropna()\n    checks.append({\n        'Check': 'Turnover1 in [0, 10]',\n        'Result': 'PASS' if ((t1 &lt; 0) | (t1 &gt; 10)).sum() == 0\n        else 'WARNING',\n        'Detail': f'{((t1&lt;0)|(t1&gt;10)).sum()} extreme values'\n    })\n    \n    first_rpt = a[a['first_report']]\n    checks.append({\n        'Check': 'First report -&gt; missing netflows',\n        'Result': 'PASS' if first_rpt['netflows'].isna().all()\n        else 'FAIL',\n    })\n    \n    return pd.DataFrame(checks)\n\n# validate_pipeline_outputs(results)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Institutional Trades, Flows, and Turnover Ratios</span>"
    ]
  },
  {
    "objectID": "53_governance.html",
    "href": "53_governance.html",
    "title": "35  Corporate Governance",
    "section": "",
    "text": "35.1 Theoretical Background\nCorporate governance (i.e., the system of rules, practices, and processes by which firms are directed and controlled) has been one of the most actively studied determinants of equity returns since the early 2000s. The insight is simple yet powerful: firms that grant shareholders stronger rights tend to outperform firms in which management is entrenched by anti-takeover provisions. This chapter applies that insight to the Vietnamese market, where governance quality varies considerably across listed firms and institutional development remains evolving.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#theoretical-background",
    "href": "53_governance.html#theoretical-background",
    "title": "35  Corporate Governance",
    "section": "",
    "text": "35.1.1 The Governance-Return Nexus\nThe theoretical motivation for a link between corporate governance and stock returns rests on agency theory (Jensen and Meckling 1976). Managers, as agents of shareholders, may pursue private benefits at the expense of firm value. Anti-takeover provisions, staggered boards, poison pills, and other defensive mechanisms insulate management from the disciplining force of the market for corporate control. When shareholders cannot easily replace underperforming managers, agency costs rise, investment efficiency falls, and firm value declines.\nGompers, Ishii, and Metrick (2003) formalized this intuition by constructing a Governance Index (G-Index) based on 24 governance provisions tracked by the Investor Responsibility Research Center (IRRC) in the United States. Each provision that restricts shareholder rights increments the index by one. Thus:\n\\[\nG\\text{-Index}_i = \\sum_{k=1}^{24} \\mathbf{1}\\{\\text{Provision } k \\text{ is present for firm } i\\}\n\\tag{35.1}\\]\nwhere \\(\\mathbf{1}\\{\\cdot\\}\\) is the indicator function. Higher values of the G-Index correspond to weaker shareholder rights.\nThe key empirical finding was striking: during the 1990s, a portfolio that bought firms with the strongest shareholder rights (G-Index \\(\\leq 5\\), labeled the Democracy Portfolio) and sold firms with the weakest shareholder rights (G-Index \\(\\geq 14\\), labeled the Dictatorship Portfolio) earned an abnormal return of approximately 8.5% per year.\n\n\n35.1.2 Adapting the Framework to Vietnam\nVietnam’s corporate governance landscape differs fundamentally from that of the United States. The Vietnamese market is characterized by:\n\nState ownership: Many listed firms retain significant government ownership stakes, which creates a distinct agency problem where the state acts as a controlling shareholder rather than dispersed minority shareholders facing entrenched management.\nConcentrated ownership: Family and controlling-group ownership is prevalent, shifting the primary agency conflict from manager-shareholder to controlling-majority vs. minority shareholders (Claessens, Djankov, and Lang 2000).\nEvolving legal framework: Vietnam’s corporate governance code has been progressively strengthened through Decree 71/2017/ND-CP and subsequent circulars, but enforcement remains uneven.\nDual listing and foreign ownership caps: Foreign ownership limits create segmented investor bases with potentially different governance preferences.\n\nDespite these differences, the core economic logic applies: firms with better governance (i.e., greater board independence, stronger audit committees, more transparent disclosure, better minority shareholder protections) should command higher valuations and deliver superior risk-adjusted returns, all else equal.\nFor the Vietnamese market, we construct a governance index analogous to the G-Index using governance provisions. While the specific provisions differ from the 24 IRRC items used in the US context, the methodology is identical: count the number of provisions that restrict shareholder rights or entrench management.\n\n\n35.1.3 The Vietnamese Governance Index (VN-GIndex)\nWe define the Vietnamese Governance Index based on the following categories of provisions (Table 35.1)\n\n\n\nTable 35.1: Categories of governance provisions for the VN-GIndex\n\n\n\n\n\n\n\n\n\n\nCategory\nProvisions\nDirection\n\n\n\n\nBoard Structure\nStaggered board, CEO duality, board size &lt; 5\n↑ restricts rights\n\n\nOwnership\nState ownership &gt; 50%, no independent directors\n↑ restricts rights\n\n\nShareholder Rights\nSupermajority requirements, limited voting rights\n↑ restricts rights\n\n\nTransparency\nNo English-language annual report, delayed filings\n↑ restricts rights\n\n\nAnti-takeover\nPoison pill equivalents, golden parachutes\n↑ restricts rights\n\n\nAudit\nNo independent audit committee, related-party auditor\n↑ restricts rights\n\n\n\n\n\n\nThe VN-GIndex for firm \\(i\\) at time \\(t\\) is:\n\\[\n\\text{VN-GIndex}_{i,t} = \\sum_{k=1}^{K} \\mathbf{1}\\{\\text{Provision } k \\text{ is present for firm } i \\text{ at time } t\\}\n\\tag{35.2}\\]\nwhere \\(K\\) is the total number of governance provisions tracked. Higher values indicate weaker governance.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#data-preparation",
    "href": "53_governance.html#data-preparation",
    "title": "35  Corporate Governance",
    "section": "35.2 Data Preparation",
    "text": "35.2 Data Preparation\nWe use three primary datasets:\n\nGovernance data: Firm-level governance provisions, updated annually or when material changes occur.\nStock market data: Monthly returns, prices, and shares outstanding for all firms listed on HOSE and HNX.\nFactor data: Vietnamese Fama-French factors (market, size, value) and a momentum factor.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.regression.linear_model import OLS\nfrom scipy import stats\nimport warnings\nimport sqlite3\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.float_format = \"{:.4f}\".format\n\ntidy_finance = sqlite3.connect(\n    database=\"data/tidy_finance_python.sqlite\"\n)\n\n\n35.2.1 Loading Governance Data\nThe governance dataset contains firm-level governance characteristics. Each observation corresponds to a firm-year, with binary indicators for the presence of each governance provision.\n\ngovernance_raw = pd.read_csv(\n    \"data/datacore_governance.csv\",\n    parse_dates=[\"date_effective\", \"date_expires\"]\n)\n\ngovernance_raw.info()\n\n\n\n\nTable 35.2: First observations of the governance dataset\n\n\ngovernance_raw.head(10)\n\n\n\nThe key variables are:\n\nsymbol: The stock symbol on HOSE or HNX.\ndate_effective: The date when the governance data became effective (analogous to the rebalancing date).\ndate_expires: The last date for which this governance vintage is valid.\nyear: The governance data vintage year.\n\n\n\n35.2.2 Computing the VN-GIndex\nWe compute the governance index by summing the binary indicators of governance provision. The provision columns are identified by the prefix gov_.\n\n# Identify governance provision columns\ngov_columns = [\n    col for col in governance_raw.columns if col.startswith(\"gov_\")\n]\n\nprint(f\"Number of governance provisions tracked: {len(gov_columns)}\")\nprint(f\"Provisions: {gov_columns}\")\n\n# Compute VN-GIndex as sum of all provision indicators\ngovernance = governance_raw.assign(\n    gindex=lambda x: x[gov_columns].sum(axis=1)\n)\n\ngovernance[[\"symbol\", \"year\", \"gindex\"]].describe()\n\n\n\n35.2.3 Distribution of the VN-GIndex\nUnderstanding the cross-sectional distribution of the governance index is essential for defining portfolio cutoffs. In the US, Gompers, Ishii, and Metrick (2003) used fixed cutoffs of \\(\\leq 5\\) for Democracy and \\(\\geq 14\\) for Dictatorship. For Vietnam, we examine the empirical distribution and set cutoffs at appropriate percentiles.\n\n\n\nfig, axes = plt.subplots(\n    2, 3, figsize=(12, 7), sharey=True, sharex=True\n)\naxes = axes.flatten()\n\nyears = sorted(governance[\"year\"].unique())\n\nfor idx, yr in enumerate(years[:6]):\n    ax = axes[idx]\n    data_yr = governance.query(f\"year == {yr}\")[\"gindex\"]\n    \n    p20 = data_yr.quantile(0.20)\n    p80 = data_yr.quantile(0.80)\n    \n    ax.hist(data_yr, bins=range(0, int(data_yr.max()) + 2), \n            edgecolor=\"white\", color=\"#2c5f8a\", alpha=0.85)\n    ax.axvline(p20, color=\"#d63e2a\", linestyle=\"--\", linewidth=1.5, \n               label=f\"P20={p20:.0f}\")\n    ax.axvline(p80, color=\"#e8a317\", linestyle=\"--\", linewidth=1.5, \n               label=f\"P80={p80:.0f}\")\n    ax.set_title(f\"{yr}\", fontsize=11, fontweight=\"bold\")\n    ax.legend(fontsize=8)\n    ax.set_xlabel(\"VN-GIndex\")\n\nfor idx in range(len(years), len(axes)):\n    axes[idx].set_visible(False)\n\naxes[0].set_ylabel(\"Number of Firms\")\naxes[3].set_ylabel(\"Number of Firms\")\n\nfig.suptitle(\n    \"Distribution of VN-GIndex by Governance Vintage Year\",\n    fontsize=13, fontweight=\"bold\", y=1.01\n)\nplt.tight_layout()\nplt.show()\n\n\nFigure 35.1\n\n\n\n\n\n\nTable 35.3: Summary statistics of the VN-GIndex by vintage year\n\n\ngindex_summary = (\n    governance\n    .groupby(\"year\")[\"gindex\"]\n    .describe()\n    .round(2)\n)\n\ngindex_summary\n\n\n\n\n\n35.2.4 Classifying Firms: Democracy, Neutral, and Dictatorship\nRather than using fixed cutoffs (which may be inappropriate given varying index ranges across markets), we use percentile-based classification. Firms in the bottom quintile of the VN-GIndex distribution within each vintage year are classified as Democracy firms, and firms in the top quintile are classified as Dictatorship firms.\n\ndef classify_governance(group):\n    \"\"\"Classify firms into Democracy, Neutral, or Dictatorship\n    within each governance vintage year.\"\"\"\n    p20 = group[\"gindex\"].quantile(0.20)\n    p80 = group[\"gindex\"].quantile(0.80)\n    \n    conditions = [\n        group[\"gindex\"] &lt;= p20,\n        group[\"gindex\"] &gt;= p80\n    ]\n    choices = [\"Democracy\", \"Dictatorship\"]\n    \n    group = group.assign(\n        gx=np.select(conditions, choices, default=\"Neutral\"),\n        gx_code=np.select(\n            conditions, [1, 3], default=2\n        )\n    )\n    return group\n\ngovernance_classified = (\n    governance\n    .groupby(\"year\", group_keys=False)\n    .apply(classify_governance)\n)\n\n# Summary of classification\nclassification_counts = (\n    governance_classified\n    .groupby([\"year\", \"gx\"])\n    .size()\n    .unstack(fill_value=0)\n    [[\"Democracy\", \"Neutral\", \"Dictatorship\"]]\n)\n\nclassification_counts\n\n\n\n\nTable 35.4: Number of firms in each governance category by vintage year\n\n\nclassification_counts\n\n\n\nWe exclude firms with dual-class share structures, as these create a distinct governance arrangement that conflates voting rights with economic ownership. In the US, Gompers, Ishii, and Metrick (2003) similarly excluded dual-class firms.\n\n# Exclude dual-class firms if flagged in the data\nif \"dual_class\" in governance_classified.columns:\n    governance_classified = governance_classified.query(\n        \"dual_class == 0\"\n    )\n    print(\"Dual-class firms excluded.\")\nelse:\n    print(\"No dual_class indicator found; proceeding with all firms.\")\n\n# Keep only Democracy and Dictatorship firms for the long-short strategy\nportfolio_firms = governance_classified.query(\n    \"gx in ['Democracy', 'Dictatorship']\"\n).copy()\n\nprint(f\"\\nFirms in portfolio universe: {len(portfolio_firms)}\")\nprint(portfolio_firms[\"gx\"].value_counts())\n\n\n\n35.2.5 Loading Stock Market Data\nWe merge the governance classifications with monthly stock return data.\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprices_monthly.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nIndex: 165499 entries, 1 to 209477\nData columns (total 7 columns):\n #   Column      Non-Null Count   Dtype         \n---  ------      --------------   -----         \n 0   symbol      165499 non-null  str           \n 1   date        165499 non-null  datetime64[us]\n 2   ret         165499 non-null  float64       \n 3   ret_excess  165499 non-null  float64       \n 4   mktcap      165499 non-null  float64       \n 5   mktcap_lag  165499 non-null  float64       \n 6   risk_free   165499 non-null  float64       \ndtypes: datetime64[us](1), float64(5), str(1)\nmemory usage: 10.6 MB\n\n\nThe stock market data contains:\n\nsymbol: Stock symbol.\ndate: End-of-month date.\nret: Monthly total return (including dividends).\nretx: Monthly return excluding dividends (price return only).\nprice: End-of-month closing price (adjusted).\nshares_outstanding: Number of shares outstanding (in thousands).\n\n\nprices_monthly[[\"symbol\", \"date\", \"ret\", \"mktcap\"]].describe()\n\n\n\n\n\n\n\n\ndate\nret\nmktcap\n\n\n\n\ncount\n165499\n165499.0000\n165499.0000\n\n\nmean\n2018-05-18 13:20:13.109444\n0.0042\n2183.1646\n\n\nmin\n2010-02-28 00:00:00\n-0.9900\n0.3536\n\n\n25%\n2015-06-30 00:00:00\n-0.0703\n60.3728\n\n\n50%\n2018-12-31 00:00:00\n0.0000\n180.6224\n\n\n75%\n2021-07-31 00:00:00\n0.0553\n660.0000\n\n\nmax\n2023-12-31 00:00:00\n12.7500\n463886.6454\n\n\nstd\nNaN\n0.1862\n13983.9977\n\n\n\n\n\n\n\n\n\n35.2.6 Linking Governance and Stock Data\nEach governance vintage is valid from date_effective through date_expires. We assign monthly stock returns to the appropriate governance vintage. This is the portfolio rebalancing logic: portfolios are reformed when new governance data becomes available and held until the next vintage.\n\n# Merge: each stock-month gets its governance classification\n# if the month falls within [date_effective, date_expires]\nmerged = pd.merge(\n    portfolio_firms[\n        [\"symbol\", \"year\", \"gindex\", \"gx\", \"gx_code\",\n         \"date_effective\", \"date_expires\"]\n    ],\n    prices_monthly[[\"symbol\", \"date\", \"ret\", \"retx\", \"mktcap\"]],\n    on=\"symbol\",\n    how=\"inner\"\n)\n\n# Keep only months within the governance validity window\nmerged = merged.query(\n    \"date &gt;= date_effective and date &lt;= date_expires\"\n).sort_values([\"symbol\", \"date\"])\n\nprint(f\"Total firm-month observations: {len(merged):,}\")\nmerged.head()\n\n\n\n35.2.7 Computing Lagged Market Capitalization for Weighting\nValue-weighted portfolio returns require weighting each stock by its beginning-of-period market capitalization. We use the previous month’s market capitalization as the weight. For the first observation of each stock in a given vintage, we estimate the beginning-of-period market value by dividing the current market value by \\((1 + r_{x,t})\\), where \\(r_{x,t}\\) is the price return.\nThe value-weighted return of portfolio \\(p\\) in month \\(t\\) is:\n\\[\nr_{p,t}^{vw} = \\sum_{i \\in p} w_{i,t-1} \\cdot r_{i,t}, \\quad w_{i,t-1} = \\frac{\\text{MV}_{i,t-1}}{\\sum_{j \\in p} \\text{MV}_{j,t-1}}\n\\tag{35.3}\\]\nwhere \\(\\text{MV}_{i,t-1}\\) is the market capitalization of stock \\(i\\) at the end of month \\(t-1\\).\n\nmerged = merged.sort_values([\"symbol\", \"date\"])\n\n# Lagged market value (previous month)\nmerged[\"mktcap_lag\"] = merged.groupby(\"symbol\")[\"mktcap\"].shift(1)\n\n# For first observation: estimate beginning-of-month market cap\nfirst_obs = merged.groupby(\"symbol\")[\"date\"].transform(\"min\")\nmask_first = merged[\"date\"] == first_obs\n\nmerged.loc[mask_first, \"mktcap_lag\"] = (\n    merged.loc[mask_first, \"mktcap\"] \n    / (1 + merged.loc[mask_first, \"retx\"])\n)\n\n# Handle any remaining missing weights\nmask_missing = merged[\"mktcap_lag\"].isna()\nmerged.loc[mask_missing, \"mktcap_lag\"] = (\n    merged.loc[mask_missing, \"mktcap\"]\n    / (1 + merged.loc[mask_missing, \"retx\"])\n)\n\n# Drop observations with missing returns or weights\nmerged = merged.dropna(subset=[\"ret\", \"mktcap_lag\"])\nmerged = merged.query(\"mktcap_lag &gt; 0\")\n\nprint(f\"Clean firm-month observations: {len(merged):,}\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#portfolio-construction",
    "href": "53_governance.html#portfolio-construction",
    "title": "35  Corporate Governance",
    "section": "35.3 Portfolio Construction",
    "text": "35.3 Portfolio Construction\n\n35.3.1 Value-Weighted Portfolio Returns\nWe now compute the value-weighted monthly returns for the Democracy and Dictatorship portfolios.\n\ndef value_weighted_return(group):\n    \"\"\"Compute value-weighted return for a group of stocks.\"\"\"\n    weights = group[\"mktcap_lag\"]\n    total_weight = weights.sum()\n    if total_weight == 0:\n        return np.nan\n    vw_ret = (group[\"ret\"] * weights).sum() / total_weight\n    return vw_ret\n\n# Compute VW returns by date and governance group\nportfolio_returns = (\n    merged\n    .groupby([\"date\", \"gx\"])\n    .apply(value_weighted_return, include_groups=False)\n    .reset_index()\n    .rename(columns={0: \"ret_vw\"})\n)\n\n# Also compute equal-weighted returns for robustness\nportfolio_returns_ew = (\n    merged\n    .groupby([\"date\", \"gx\"])[\"ret\"]\n    .mean()\n    .reset_index()\n    .rename(columns={\"ret\": \"ret_ew\"})\n)\n\n# Merge EW returns\nportfolio_returns = portfolio_returns.merge(\n    portfolio_returns_ew, on=[\"date\", \"gx\"], how=\"left\"\n)\n\nportfolio_returns.head(10)\n\n\n\n35.3.2 Long-Short Portfolio: Democracy minus Dictatorship\nThe trading strategy goes long the Democracy portfolio and short the Dictatorship portfolio. The monthly return of this long-short strategy is:\n\\[\nr_{t}^{D-D} = r_{t}^{\\text{Democracy}} - r_{t}^{\\text{Dictatorship}}\n\\tag{35.4}\\]\n\n# Pivot to wide format\nreturns_wide = portfolio_returns.pivot(\n    index=\"date\", columns=\"gx\", values=[\"ret_vw\", \"ret_ew\"]\n)\n\n# Flatten column names\nreturns_wide.columns = [\n    f\"{val}_{grp}\" for val, grp in returns_wide.columns\n]\nreturns_wide = returns_wide.reset_index()\n\n# Compute long-short returns\nreturns_wide = returns_wide.assign(\n    ret_diff_vw=lambda x: (\n        x[\"ret_vw_Democracy\"] - x[\"ret_vw_Dictatorship\"]\n    ),\n    ret_diff_ew=lambda x: (\n        x[\"ret_ew_Democracy\"] - x[\"ret_ew_Dictatorship\"]\n    )\n)\n\nreturns_wide = returns_wide.sort_values(\"date\").reset_index(drop=True)\n\nprint(f\"Monthly return series: {len(returns_wide)} months\")\nreturns_wide.head()\n\n\n\n35.3.3 Portfolio Characteristics Over Time\nBefore examining returns, we document the number of firms and average governance scores in each portfolio over time.\n\n\n\nTable 35.5: Portfolio characteristics by governance group and year\n\n\nportfolio_chars = (\n    merged\n    .assign(year_month=lambda x: x[\"date\"].dt.to_period(\"Y\"))\n    .groupby([\"year_month\", \"gx\"])\n    .agg(\n        n_firms=(\"symbol\", \"nunique\"),\n        avg_gindex=(\"gindex\", \"mean\"),\n        avg_mktcap=(\"mktcap\", \"mean\"),\n        median_mktcap=(\"mktcap\", \"median\")\n    )\n    .round(2)\n)\n\nportfolio_chars",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#empirical-results",
    "href": "53_governance.html#empirical-results",
    "title": "35  Corporate Governance",
    "section": "35.4 Empirical Results",
    "text": "35.4 Empirical Results\n\n35.4.1 Summary Statistics of Portfolio Returns\n\n\n\nTable 35.6: Summary statistics of monthly portfolio returns (in percent)\n\n\nreturn_cols = [\n    \"ret_vw_Democracy\", \"ret_vw_Dictatorship\", \"ret_diff_vw\",\n    \"ret_ew_Democracy\", \"ret_ew_Dictatorship\", \"ret_diff_ew\"\n]\n\nsummary_stats = (\n    returns_wide[return_cols]\n    .mul(100)  # Convert to percent\n    .describe()\n    .T\n    .assign(\n        skewness=returns_wide[return_cols].mul(100).skew(),\n        sharpe=lambda x: x[\"mean\"] / x[\"std\"] * np.sqrt(12)\n    )\n    .round(3)\n)\n\nsummary_stats.index = [\n    \"Democracy (VW)\", \"Dictatorship (VW)\", \"Long-Short (VW)\",\n    \"Democracy (EW)\", \"Dictatorship (EW)\", \"Long-Short (EW)\"\n]\n\nsummary_stats[[\"mean\", \"std\", \"min\", \"25%\", \"50%\", \"75%\", \"max\", \n               \"skewness\", \"sharpe\"]]\n\n\n\nThe Sharpe ratio is computed as:\n\\[\n\\text{SR} = \\frac{\\bar{r}_p}{\\sigma_p} \\times \\sqrt{12}\n\\tag{35.5}\\]\nwhere \\(\\bar{r}_p\\) and \\(\\sigma_p\\) are the sample mean and standard deviation of monthly portfolio returns, and the \\(\\sqrt{12}\\) scaling annualizes the ratio.\n\n\n35.4.2 T-Test: Is the Long-Short Return Statistically Significant?\nThe null hypothesis is that the mean monthly return difference between the Democracy and Dictatorship portfolios is zero:\n\\[\nH_0: \\mathbb{E}[r_t^{\\text{Democracy}} - r_t^{\\text{Dictatorship}}] = 0\n\\tag{35.6}\\]\n\n\n\nTable 35.7: T-test for the mean difference between Democracy and Dictatorship portfolio returns\n\n\ndef perform_ttest(series, label):\n    \"\"\"Perform a one-sample t-test and return results.\"\"\"\n    clean = series.dropna()\n    t_stat, p_value = stats.ttest_1samp(clean, 0)\n    return {\n        \"Portfolio\": label,\n        \"Mean (%)\": clean.mean() * 100,\n        \"Std (%)\": clean.std() * 100,\n        \"T-statistic\": t_stat,\n        \"P-value\": p_value,\n        \"N months\": len(clean),\n        \"Significant (5%)\": \"Yes\" if p_value &lt; 0.05 else \"No\"\n    }\n\nttest_results = pd.DataFrame([\n    perform_ttest(returns_wide[\"ret_diff_vw\"], \"VW Long-Short\"),\n    perform_ttest(returns_wide[\"ret_diff_ew\"], \"EW Long-Short\"),\n    perform_ttest(returns_wide[\"ret_vw_Democracy\"], \"VW Democracy\"),\n    perform_ttest(returns_wide[\"ret_vw_Dictatorship\"], \"VW Dictatorship\")\n])\n\nttest_results.round(4)\n\n\n\n\n\n35.4.3 Cumulative Returns: The Visual Case for Governance\nOne of the most compelling ways to present the governance effect is through cumulative wealth plots. We track the growth of $1 invested in each portfolio at the beginning of the sample period.\nThe cumulative return at time \\(T\\) is:\n\\[\nW_T = \\prod_{t=1}^{T} (1 + r_{p,t})\n\\tag{35.7}\\]\n\n\n\nreturns_wide = returns_wide.sort_values(\"date\")\n\ncum_democracy = (1 + returns_wide[\"ret_vw_Democracy\"]).cumprod()\ncum_dictatorship = (1 + returns_wide[\"ret_vw_Dictatorship\"]).cumprod()\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.plot(\n    returns_wide[\"date\"], cum_democracy,\n    color=\"#1a6b3c\", linewidth=2, label=\"Democracy Portfolio\"\n)\nax.plot(\n    returns_wide[\"date\"], cum_dictatorship,\n    color=\"#c0392b\", linewidth=1.5, linestyle=\"--\",\n    label=\"Dictatorship Portfolio\"\n)\nax.fill_between(\n    returns_wide[\"date\"],\n    cum_democracy, cum_dictatorship,\n    where=cum_democracy &gt;= cum_dictatorship,\n    alpha=0.15, color=\"#1a6b3c\", label=\"Outperformance\"\n)\nax.fill_between(\n    returns_wide[\"date\"],\n    cum_democracy, cum_dictatorship,\n    where=cum_democracy &lt; cum_dictatorship,\n    alpha=0.15, color=\"#c0392b\"\n)\n\nax.set_xlabel(\"Date\", fontsize=11)\nax.set_ylabel(\"Cumulative Value of $1 Invested\", fontsize=11)\nax.set_title(\n    \"Democracy vs. Dictatorship Portfolios: Cumulative Returns\",\n    fontsize=13, fontweight=\"bold\"\n)\nax.legend(fontsize=10, loc=\"upper left\")\nax.grid(True, alpha=0.3)\nax.set_xlim(returns_wide[\"date\"].min(), returns_wide[\"date\"].max())\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 35.2\n\n\n\n\n\n\ncum_longshort = (1 + returns_wide[\"ret_diff_vw\"]).cumprod()\n\nfig, ax = plt.subplots(figsize=(10, 4))\n\nax.plot(\n    returns_wide[\"date\"], cum_longshort,\n    color=\"#2c5f8a\", linewidth=2\n)\nax.axhline(y=1.0, color=\"gray\", linestyle=\":\", linewidth=1)\nax.fill_between(\n    returns_wide[\"date\"], 1.0, cum_longshort,\n    where=cum_longshort &gt;= 1.0, alpha=0.2, color=\"#2c5f8a\"\n)\nax.fill_between(\n    returns_wide[\"date\"], 1.0, cum_longshort,\n    where=cum_longshort &lt; 1.0, alpha=0.2, color=\"#c0392b\"\n)\n\nax.set_xlabel(\"Date\", fontsize=11)\nax.set_ylabel(\"Cumulative Long-Short Return\", fontsize=11)\nax.set_title(\n    \"Long-Short Governance Strategy: Cumulative Performance\",\n    fontsize=13, fontweight=\"bold\"\n)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\nFigure 35.3\n\n\n\n\n\n35.4.4 Rolling Performance\nThe governance premium may not be constant over time. We examine 12-month rolling average returns and rolling Sharpe ratios to assess stability.\n\n\n\nrolling_mean = (\n    returns_wide[\"ret_diff_vw\"]\n    .rolling(window=12, min_periods=6)\n    .mean() * 12 * 100  # Annualized\n)\n\nrolling_std = (\n    returns_wide[\"ret_diff_vw\"]\n    .rolling(window=12, min_periods=6)\n    .std() * np.sqrt(12) * 100\n)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 7), sharex=True)\n\n# Rolling annualized return\naxes[0].plot(\n    returns_wide[\"date\"], rolling_mean,\n    color=\"#2c5f8a\", linewidth=1.5\n)\naxes[0].axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\naxes[0].set_ylabel(\"Annualized Return (%)\")\naxes[0].set_title(\n    \"12-Month Rolling Annualized Return: Long-Short Governance Strategy\",\n    fontweight=\"bold\"\n)\naxes[0].grid(True, alpha=0.3)\n\n# Rolling annualized volatility\naxes[1].plot(\n    returns_wide[\"date\"], rolling_std,\n    color=\"#c0392b\", linewidth=1.5\n)\naxes[1].set_ylabel(\"Annualized Volatility (%)\")\naxes[1].set_xlabel(\"Date\")\naxes[1].set_title(\n    \"12-Month Rolling Annualized Volatility\",\n    fontweight=\"bold\"\n)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 35.4",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#risk-adjusted-performance-factor-model-analysis",
    "href": "53_governance.html#risk-adjusted-performance-factor-model-analysis",
    "title": "35  Corporate Governance",
    "section": "35.5 Risk-Adjusted Performance: Factor Model Analysis",
    "text": "35.5 Risk-Adjusted Performance: Factor Model Analysis\n\n35.5.1 The Four-Factor Model\nRaw returns alone do not tell us whether the governance strategy generates abnormal returns (i.e., returns that cannot be explained by exposure to common risk factors). We estimate the following four-factor model:\n\\[\nr_{t}^{D-D} - r_{f,t} = \\alpha + \\beta_1 (r_{m,t} - r_{f,t}) + \\beta_2 \\text{SMB}_t + \\beta_3 \\text{HML}_t + \\beta_4 \\text{UMD}_t + \\varepsilon_t\n\\tag{35.8}\\]\nwhere:\n\n\\(r_{t}^{D-D}\\) is the long-short governance portfolio return,\n\\(r_{f,t}\\) is the risk-free rate,\n\\(r_{m,t} - r_{f,t}\\) is the market excess return (MKTRF),\n\\(\\text{SMB}_t\\) is the size factor (Small Minus Big),\n\\(\\text{HML}_t\\) is the value factor (High Minus Low book-to-market),\n\\(\\text{UMD}_t\\) is the momentum factor (Up Minus Down),\n\\(\\alpha\\) is the abnormal return (the intercept of interest).\n\nThe alpha (\\(\\alpha\\)) represents the average monthly return that cannot be attributed to exposure to the four systematic risk factors. A statistically significant positive alpha indicates that the governance strategy generates genuine abnormal returns.\n\n\n35.5.2 Loading Factor Data\n\nfactors_ff5_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nfactors_ff5_monthly.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype         \n---  ------      --------------  -----         \n 0   date        150 non-null    datetime64[us]\n 1   smb         150 non-null    float64       \n 2   hml         150 non-null    float64       \n 3   rmw         150 non-null    float64       \n 4   cma         150 non-null    float64       \n 5   mkt_excess  150 non-null    float64       \n 6   risk_free   150 non-null    float64       \ndtypes: datetime64[us](1), float64(6)\nmemory usage: 8.3 KB\n\n\n\nmomentum_factor = pd.read_sql_query(\n    sql=\"SELECT * FROM momentum_factor_monthly\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n)\n\nfactors_ff3_monthly = (\n    factors_ff5_monthly\n    .merge(momentum_factor, on=\"date\")\n    .rename(columns={\"mkt_excess\": \"mktrf\", \"wml\": \"umd\"})\n)\n\n\nfactor_cols = [\"mktrf\", \"smb\", \"hml\", \"umd\", \"risk_free\"]\n\n(\n    factors_ff3_monthly[factor_cols]\n    .mul(100)\n    .describe()\n    .T\n    .round(3)\n)\n\n\n\nTable 35.8: Summary statistics of Vietnamese Fama-French-Carhart factors (monthly, in percent)\n\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nmktrf\n150.0000\n-1.0080\n5.8580\n-21.4910\n-3.8030\n-0.9510\n2.1450\n16.7680\n\n\nsmb\n150.0000\n0.7660\n4.1900\n-15.2200\n-1.3730\n1.0400\n3.1610\n12.8380\n\n\nhml\n150.0000\n1.1460\n5.1820\n-12.8300\n-1.2620\n0.4610\n3.2270\n15.0970\n\n\numd\n150.0000\n-2.0910\n4.8090\n-16.3870\n-4.8840\n-2.1650\n0.6190\n15.6040\n\n\nrisk_free\n150.0000\n0.3330\n0.0000\n0.3330\n0.3330\n0.3330\n0.3330\n0.3330\n\n\n\n\n\n\n\n\n\n\n\n\n35.5.3 Merging Portfolio Returns with Factor Data\n\n# Merge on year-month\nreturns_wide[\"ym\"] = returns_wide[\"date\"].dt.to_period(\"M\")\nfactors_ff3_monthly[\"ym\"] = factors_ff3_monthly[\"date\"].dt.to_period(\"M\")\n\nanalysis_data = returns_wide.merge(\n    factors_ff3_monthly[[\"ym\", \"mktrf\", \"smb\", \"hml\", \"umd\", \"rf\"]],\n    on=\"ym\",\n    how=\"inner\"\n)\n\n# Compute excess returns\nanalysis_data = analysis_data.assign(\n    ret_excess_democracy=lambda x: x[\"ret_vw_Democracy\"] - x[\"rf\"],\n    ret_excess_dictatorship=lambda x: x[\"ret_vw_Dictatorship\"] - x[\"rf\"],\n    ret_excess_longshort=lambda x: x[\"ret_diff_vw\"]\n    # Long-short is already excess (self-financing)\n)\n\nprint(f\"Observations for regression: {len(analysis_data)}\")\n\n\n\n35.5.4 CAPM Regression\nWe start with the single-factor CAPM to understand the market exposure of each portfolio:\n\\[\nr_{p,t} - r_{f,t} = \\alpha_p + \\beta_p (r_{m,t} - r_{f,t}) + \\varepsilon_{p,t}\n\\tag{35.9}\\]\n\n\n\nTable 35.9: CAPM regression results for Democracy, Dictatorship, and Long-Short portfolios\n\n\ndef run_regression(y, X, hac=True):\n    \"\"\"Run OLS regression with optional HAC standard errors.\"\"\"\n    X_const = sm.add_constant(X)\n    model = OLS(y, X_const).fit(\n        cov_type=\"HAC\" if hac else \"nonrobust\",\n        cov_kwds={\"maxlags\": 6} if hac else {}\n    )\n    return model\n\nportfolios = {\n    \"Democracy\": analysis_data[\"ret_excess_democracy\"],\n    \"Dictatorship\": analysis_data[\"ret_excess_dictatorship\"],\n    \"Long-Short\": analysis_data[\"ret_excess_longshort\"]\n}\n\ncapm_results = {}\nfor name, y in portfolios.items():\n    X = analysis_data[[\"mktrf\"]]\n    model = run_regression(y, X, hac=True)\n    capm_results[name] = {\n        \"Alpha (%)\": model.params[\"const\"] * 100,\n        \"Alpha t-stat\": model.tvalues[\"const\"],\n        \"Beta (MKTRF)\": model.params[\"mktrf\"],\n        \"Beta t-stat\": model.tvalues[\"mktrf\"],\n        \"R-squared\": model.rsquared,\n        \"N\": int(model.nobs)\n    }\n\npd.DataFrame(capm_results).T.round(4)\n\n\n\n\n\n35.5.5 Three-Factor Fama-French Model\nThe three-factor model (Fama and French 1993) augments the CAPM with size (SMB) and value (HML) factors:\n\\[\nr_{p,t} - r_{f,t} = \\alpha_p + \\beta_1 \\text{MKTRF}_t + \\beta_2 \\text{SMB}_t + \\beta_3 \\text{HML}_t + \\varepsilon_{p,t}\n\\tag{35.10}\\]\n\n\n\nTable 35.10: Fama-French three-factor model results with Newey-West standard errors\n\n\nff3_results = {}\nfor name, y in portfolios.items():\n    X = analysis_data[[\"mktrf\", \"smb\", \"hml\"]]\n    model = run_regression(y, X, hac=True)\n    ff3_results[name] = {\n        \"Alpha (%)\": model.params[\"const\"] * 100,\n        \"Alpha t-stat\": model.tvalues[\"const\"],\n        \"MKTRF\": model.params[\"mktrf\"],\n        \"SMB\": model.params[\"smb\"],\n        \"HML\": model.params[\"hml\"],\n        \"R-squared\": model.rsquared\n    }\n\npd.DataFrame(ff3_results).T.round(4)\n\n\n\n\n\n35.5.6 Four-Factor Carhart Model\nAdding the momentum factor (Carhart 1997) controls for the well-documented tendency of past winners to continue outperforming past losers:\n\n\n\nTable 35.11: Four-factor Carhart model results for governance portfolios. Standard errors are Newey-West adjusted with 6 lags.\n\n\nff4_results = {}\nfor name, y in portfolios.items():\n    X = analysis_data[[\"mktrf\", \"smb\", \"hml\", \"umd\"]]\n    model = run_regression(y, X, hac=True)\n    ff4_results[name] = {\n        \"Alpha (%)\": model.params[\"const\"] * 100,\n        \"Alpha t-stat\": model.tvalues[\"const\"],\n        \"Alpha p-value\": model.pvalues[\"const\"],\n        \"MKTRF\": model.params[\"mktrf\"],\n        \"MKTRF t\": model.tvalues[\"mktrf\"],\n        \"SMB\": model.params[\"smb\"],\n        \"SMB t\": model.tvalues[\"smb\"],\n        \"HML\": model.params[\"hml\"],\n        \"HML t\": model.tvalues[\"hml\"],\n        \"UMD\": model.params[\"umd\"],\n        \"UMD t\": model.tvalues[\"umd\"],\n        \"R-squared\": model.rsquared,\n        \"Adj R-sq\": model.rsquared_adj,\n        \"N\": int(model.nobs)\n    }\n\nff4_df = pd.DataFrame(ff4_results).T\nff4_df.round(4)\n\n\n\n\n# Detailed regression output for the long-short portfolio\nX = sm.add_constant(analysis_data[[\"mktrf\", \"smb\", \"hml\", \"umd\"]])\ny = analysis_data[\"ret_excess_longshort\"]\n\nmodel_ls = OLS(y, X).fit(\n    cov_type=\"HAC\", cov_kwds={\"maxlags\": 6}\n)\n\nprint(model_ls.summary())\n\n\n\n35.5.7 Interpreting the Factor Loadings\nThe factor loadings reveal important characteristics of the governance portfolios:\n\nMarket beta (\\(\\beta_{\\text{MKTRF}}\\)): If the long-short portfolio has a near-zero market beta, the governance strategy is approximately market-neutral. A positive beta would indicate that Democracy firms are more sensitive to market movements than Dictatorship firms.\nSize factor (\\(\\beta_{\\text{SMB}}\\)): A positive loading suggests the strategy tilts toward smaller firms. If Democracy firms tend to be smaller (or Dictatorship firms larger), the size factor captures this differential.\nValue factor (\\(\\beta_{\\text{HML}}\\)): A positive loading indicates a value tilt. Governance and value may be related if poorly governed firms also tend to have high book-to-market ratios (i.e., they are “cheap” because of governance risk).\nMomentum factor (\\(\\beta_{\\text{UMD}}\\)): The momentum loading captures whether the governance effect overlaps with price momentum. In the US, Gompers, Ishii, and Metrick (2003) found a near-zero momentum loading, suggesting the governance effect is distinct from momentum.\n\n\n\n35.5.8 Robustness: White Heteroskedasticity-Consistent Standard Errors\nAs a robustness check, we also report results with White (HC1) standard errors, following the original methodology:\n\n\n\nTable 35.12: Four-factor model with White heteroskedasticity-consistent standard errors (HC1)\n\n\nwhite_results = {}\nfor name, y in portfolios.items():\n    X = sm.add_constant(analysis_data[[\"mktrf\", \"smb\", \"hml\", \"umd\"]])\n    model = OLS(y, X).fit(cov_type=\"HC1\")\n    white_results[name] = {\n        \"Alpha (%)\": model.params[\"const\"] * 100,\n        \"Alpha t-stat\": model.tvalues[\"const\"],\n        \"Alpha p-value\": model.pvalues[\"const\"],\n        \"R-squared\": model.rsquared\n    }\n\npd.DataFrame(white_results).T.round(4)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#deeper-analysis",
    "href": "53_governance.html#deeper-analysis",
    "title": "35  Corporate Governance",
    "section": "35.6 Deeper Analysis",
    "text": "35.6 Deeper Analysis\n\n35.6.1 Governance Quintile Portfolios\nRather than focusing solely on the extreme portfolios, we examine returns across all five governance quintiles. This allows us to assess whether the governance-return relationship is monotonic.\n\ndef assign_quintile(group):\n    \"\"\"Assign governance quintile within each vintage year.\"\"\"\n    group = group.copy()\n    group[\"gindex_quintile\"] = pd.qcut(\n        group[\"gindex\"], q=5, labels=[1, 2, 3, 4, 5],\n        duplicates=\"drop\"\n    )\n    return group\n\ngovernance_quintiles = (\n    governance\n    .groupby(\"year\", group_keys=False)\n    .apply(assign_quintile)\n)\n\n# Merge with stock data\nif \"dual_class\" in governance_quintiles.columns:\n    governance_quintiles = governance_quintiles.query(\"dual_class == 0\")\n\nmerged_q = pd.merge(\n    governance_quintiles[\n        [\"symbol\", \"year\", \"gindex\", \"gindex_quintile\",\n         \"date_effective\", \"date_expires\"]\n    ],\n    prices_monthly[[\"symbol\", \"date\", \"ret\", \"retx\", \"mktcap\"]],\n    on=\"symbol\",\n    how=\"inner\"\n)\n\nmerged_q = merged_q.query(\n    \"date &gt;= date_effective and date &lt;= date_expires\"\n).sort_values([\"symbol\", \"date\"])\n\n# Compute lagged market value\nmerged_q = merged_q.sort_values([\"symbol\", \"date\"])\nmerged_q[\"mktcap_lag\"] = merged_q.groupby(\"symbol\")[\"mktcap\"].shift(1)\n\nfirst_obs_q = merged_q.groupby(\"symbol\")[\"date\"].transform(\"min\")\nmask_first_q = merged_q[\"date\"] == first_obs_q\nmerged_q.loc[mask_first_q, \"mktcap_lag\"] = (\n    merged_q.loc[mask_first_q, \"mktcap\"]\n    / (1 + merged_q.loc[mask_first_q, \"retx\"])\n)\n\nmask_missing_q = merged_q[\"mktcap_lag\"].isna()\nmerged_q.loc[mask_missing_q, \"mktcap_lag\"] = (\n    merged_q.loc[mask_missing_q, \"mktcap\"]\n    / (1 + merged_q.loc[mask_missing_q, \"retx\"])\n)\n\nmerged_q = merged_q.dropna(subset=[\"ret\", \"mktcap_lag\"])\nmerged_q = merged_q.query(\"mktcap_lag &gt; 0\")\n\n\n\n\n# Compute VW returns by quintile and date\nquintile_returns = (\n    merged_q\n    .groupby([\"date\", \"gindex_quintile\"])\n    .apply(\n        lambda g: np.average(g[\"ret\"], weights=g[\"mktcap_lag\"])\n        if g[\"mktcap_lag\"].sum() &gt; 0 else np.nan,\n        include_groups=False\n    )\n    .reset_index()\n    .rename(columns={0: \"ret_vw\"})\n)\n\n# Average across time\nquintile_avg = (\n    quintile_returns\n    .groupby(\"gindex_quintile\")[\"ret_vw\"]\n    .agg([\"mean\", \"std\", \"count\"])\n    .assign(\n        se=lambda x: x[\"std\"] / np.sqrt(x[\"count\"]),\n        ci95=lambda x: 1.96 * x[\"std\"] / np.sqrt(x[\"count\"])\n    )\n)\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nquintiles = quintile_avg.index.astype(int)\nmeans = quintile_avg[\"mean\"] * 100 * 12  # Annualized\nci = quintile_avg[\"ci95\"] * 100 * 12\n\nbars = ax.bar(\n    quintiles, means, yerr=ci,\n    color=[\"#1a6b3c\", \"#5fa35f\", \"#b0b0b0\", \"#d4836a\", \"#c0392b\"],\n    edgecolor=\"white\", linewidth=0.5, capsize=5,\n    error_kw={\"linewidth\": 1.5}\n)\n\nax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\nax.set_xlabel(\"VN-GIndex Quintile\\n(1 = Strongest Rights → 5 = Weakest Rights)\",\n              fontsize=10)\nax.set_ylabel(\"Annualized Return (%)\", fontsize=10)\nax.set_title(\n    \"Average Annualized Returns by Governance Quintile\",\n    fontsize=12, fontweight=\"bold\"\n)\nax.set_xticks(quintiles)\nax.grid(axis=\"y\", alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 35.5\n\n\n\n\n\n\nTable 35.13: Average monthly returns and four-factor alphas by governance quintile\n\n\n# Merge quintile returns with factors\nquintile_wide = quintile_returns.pivot(\n    index=\"date\", columns=\"gindex_quintile\", values=\"ret_vw\"\n)\nquintile_wide.columns = [f\"Q{int(c)}\" for c in quintile_wide.columns]\nquintile_wide = quintile_wide.reset_index()\n\nquintile_wide[\"ym\"] = quintile_wide[\"date\"].dt.to_period(\"M\")\nquintile_analysis = quintile_wide.merge(\n    factors_ff3_monthly[[\"ym\", \"mktrf\", \"smb\", \"hml\", \"umd\", \"rf\"]],\n    on=\"ym\", how=\"inner\"\n)\n\nquintile_factor_results = {}\nfor q in range(1, 6):\n    col = f\"Q{q}\"\n    if col in quintile_analysis.columns:\n        y = quintile_analysis[col] - quintile_analysis[\"rf\"]\n        X = sm.add_constant(\n            quintile_analysis[[\"mktrf\", \"smb\", \"hml\", \"umd\"]]\n        )\n        model = OLS(y.dropna(), X.loc[y.dropna().index]).fit(\n            cov_type=\"HAC\", cov_kwds={\"maxlags\": 6}\n        )\n        quintile_factor_results[f\"Quintile {q}\"] = {\n            \"Mean Return (%)\": y.mean() * 100,\n            \"Alpha (%)\": model.params[\"const\"] * 100,\n            \"Alpha t-stat\": model.tvalues[\"const\"],\n            \"MKTRF Beta\": model.params[\"mktrf\"],\n            \"R-squared\": model.rsquared\n        }\n\n# Add 5-1 spread\nif \"Q1\" in quintile_analysis.columns and \"Q5\" in quintile_analysis.columns:\n    y_spread = quintile_analysis[\"Q1\"] - quintile_analysis[\"Q5\"]\n    X = sm.add_constant(\n        quintile_analysis[[\"mktrf\", \"smb\", \"hml\", \"umd\"]]\n    )\n    model_spread = OLS(\n        y_spread.dropna(), X.loc[y_spread.dropna().index]\n    ).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n    quintile_factor_results[\"Q1 - Q5\"] = {\n        \"Mean Return (%)\": y_spread.mean() * 100,\n        \"Alpha (%)\": model_spread.params[\"const\"] * 100,\n        \"Alpha t-stat\": model_spread.tvalues[\"const\"],\n        \"MKTRF Beta\": model_spread.params[\"mktrf\"],\n        \"R-squared\": model_spread.rsquared\n    }\n\npd.DataFrame(quintile_factor_results).T.round(4)\n\n\n\n\n\n35.6.2 Subsample Analysis\nThe governance premium may vary across market regimes. We split the sample at the midpoint and examine whether the effect is concentrated in a particular subperiod.\n\n\n\nTable 35.14: Four-factor alpha of the long-short governance strategy across subperiods\n\n\nmidpoint = analysis_data[\"date\"].quantile(0.5)\n\nsubperiods = {\n    \"Full Sample\": analysis_data,\n    \"First Half\": analysis_data.query(f\"date &lt;= '{midpoint}'\"),\n    \"Second Half\": analysis_data.query(f\"date &gt; '{midpoint}'\")\n}\n\nsubsample_results = {}\nfor period_name, df in subperiods.items():\n    if len(df) &lt; 12:\n        continue\n    y = df[\"ret_excess_longshort\"]\n    X = sm.add_constant(df[[\"mktrf\", \"smb\", \"hml\", \"umd\"]])\n    model = OLS(y, X).fit(\n        cov_type=\"HAC\", cov_kwds={\"maxlags\": 6}\n    )\n    subsample_results[period_name] = {\n        \"Alpha (% monthly)\": model.params[\"const\"] * 100,\n        \"Alpha (% annual)\": model.params[\"const\"] * 100 * 12,\n        \"T-statistic\": model.tvalues[\"const\"],\n        \"P-value\": model.pvalues[\"const\"],\n        \"N months\": int(model.nobs),\n        \"Date Range\": f\"{df['date'].min().strftime('%Y-%m')} to \"\n                      f\"{df['date'].max().strftime('%Y-%m')}\"\n    }\n\npd.DataFrame(subsample_results).T\n\n\n\n\n\n35.6.3 Equal-Weighted Robustness\nValue-weighting may cause the results to be driven by a few large firms. We verify robustness with equal-weighted portfolios.\n\n\n\nTable 35.15: Four-factor model results: Equal-weighted vs. Value-weighted long-short portfolio\n\n\new_y = analysis_data[\"ret_diff_ew\"]\nvw_y = analysis_data[\"ret_excess_longshort\"]\nX = sm.add_constant(analysis_data[[\"mktrf\", \"smb\", \"hml\", \"umd\"]])\n\nmodel_ew = OLS(ew_y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\nmodel_vw = OLS(vw_y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n\ncomparison = pd.DataFrame({\n    \"Value-Weighted\": {\n        \"Alpha (% monthly)\": model_vw.params[\"const\"] * 100,\n        \"Alpha t-stat\": model_vw.tvalues[\"const\"],\n        \"MKTRF\": model_vw.params[\"mktrf\"],\n        \"SMB\": model_vw.params[\"smb\"],\n        \"HML\": model_vw.params[\"hml\"],\n        \"UMD\": model_vw.params[\"umd\"],\n        \"R-squared\": model_vw.rsquared\n    },\n    \"Equal-Weighted\": {\n        \"Alpha (% monthly)\": model_ew.params[\"const\"] * 100,\n        \"Alpha t-stat\": model_ew.tvalues[\"const\"],\n        \"MKTRF\": model_ew.params[\"mktrf\"],\n        \"SMB\": model_ew.params[\"smb\"],\n        \"HML\": model_ew.params[\"hml\"],\n        \"UMD\": model_ew.params[\"umd\"],\n        \"R-squared\": model_ew.rsquared\n    }\n}).T\n\ncomparison.round(4)\n\n\n\n\n\n35.6.4 Factor Loading Stability: Rolling Regressions\n\n\n\nwindow = 24\nrolling_alpha = []\n\nfor i in range(window, len(analysis_data)):\n    subset = analysis_data.iloc[i - window:i]\n    y = subset[\"ret_excess_longshort\"]\n    X = sm.add_constant(subset[[\"mktrf\", \"smb\", \"hml\", \"umd\"]])\n    try:\n        model = OLS(y, X).fit()\n        rolling_alpha.append({\n            \"date\": subset[\"date\"].iloc[-1],\n            \"alpha\": model.params[\"const\"] * 100 * 12,\n            \"alpha_se\": model.bse[\"const\"] * 100 * 12,\n            \"t_stat\": model.tvalues[\"const\"]\n        })\n    except Exception:\n        pass\n\nrolling_alpha_df = pd.DataFrame(rolling_alpha)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.plot(\n    rolling_alpha_df[\"date\"],\n    rolling_alpha_df[\"alpha\"],\n    color=\"#2c5f8a\", linewidth=1.5\n)\nax.fill_between(\n    rolling_alpha_df[\"date\"],\n    rolling_alpha_df[\"alpha\"] - 1.96 * rolling_alpha_df[\"alpha_se\"],\n    rolling_alpha_df[\"alpha\"] + 1.96 * rolling_alpha_df[\"alpha_se\"],\n    alpha=0.2, color=\"#2c5f8a\"\n)\nax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=0.8)\n\nax.set_xlabel(\"Date\", fontsize=11)\nax.set_ylabel(\"Annualized Alpha (%)\", fontsize=11)\nax.set_title(\n    \"24-Month Rolling Four-Factor Alpha: Governance Long-Short Strategy\",\n    fontsize=12, fontweight=\"bold\"\n)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\nFigure 35.6",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#comparison-with-international-evidence",
    "href": "53_governance.html#comparison-with-international-evidence",
    "title": "35  Corporate Governance",
    "section": "35.7 Comparison with International Evidence",
    "text": "35.7 Comparison with International Evidence\n\n35.7.1 The US Experience\nIn the United States, Gompers, Ishii, and Metrick (2003) documented an annualized abnormal return of approximately 8.5% for the Democracy-minus-Dictatorship strategy during the 1990s. Subsequent research has provided important nuances:\n\nL. Bebchuk, Cohen, and Ferrell (2009) refined the G-Index to a more parsimonious Entrenchment Index (E-Index) based on only six provisions that matter most for firm value: staggered boards, limits to shareholder bylaw amendments, poison pills, golden parachutes, and supermajority requirements for mergers and charter amendments. The E-Index explained the governance-return relationship at least as well as the full G-Index.\nCremers and Nair (2005) found that the governance effect was strongest in firms where external governance mechanisms (the takeover market) were active, suggesting complementarity between internal and external governance.\nThe governance premium in the US has largely disappeared in more recent periods (L. A. Bebchuk, Cohen, and Wang 2013), potentially because the market has learned to price governance differences. This “learning hypothesis” has implications for whether similar strategies can persist in less efficient markets like Vietnam.\n\n\n\n35.7.2 Emerging Market Context\nThe governance-return relationship in emerging markets remains an active area of research. Several features of emerging markets suggest the premium may be larger and more persistent:\n\nInformation asymmetry: Weaker disclosure requirements and less analyst coverage mean governance quality is harder to observe, creating larger mispricings (Klapper and Love 2004).\nWeaker legal enforcement: Where legal protections for minority shareholders are weak, firm-level governance becomes more important as a substitute (La Porta et al. 2000).\nConcentrated ownership: The presence of controlling shareholders creates opportunities for tunneling and related-party transactions that good governance mechanisms can mitigate (Johnson et al. 2000).\n\nVietnam, as a frontier-to-emerging market with all three characteristics, is a particularly interesting laboratory for testing whether governance generates abnormal returns.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#drawdown-and-risk-analysis",
    "href": "53_governance.html#drawdown-and-risk-analysis",
    "title": "35  Corporate Governance",
    "section": "35.8 Drawdown and Risk Analysis",
    "text": "35.8 Drawdown and Risk Analysis\n\n35.8.1 Maximum Drawdown\n\n\n\ndef compute_drawdown(cumulative_returns):\n    \"\"\"Compute drawdown series from cumulative returns.\"\"\"\n    running_max = cumulative_returns.cummax()\n    drawdown = (cumulative_returns - running_max) / running_max\n    return drawdown\n\ncum_dem = (1 + returns_wide[\"ret_vw_Democracy\"]).cumprod()\ncum_dic = (1 + returns_wide[\"ret_vw_Dictatorship\"]).cumprod()\n\ndd_dem = compute_drawdown(cum_dem)\ndd_dic = compute_drawdown(cum_dic)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nax.fill_between(\n    returns_wide[\"date\"], dd_dem * 100, 0,\n    alpha=0.4, color=\"#1a6b3c\", label=\"Democracy\"\n)\nax.fill_between(\n    returns_wide[\"date\"], dd_dic * 100, 0,\n    alpha=0.4, color=\"#c0392b\", label=\"Dictatorship\"\n)\nax.set_xlabel(\"Date\", fontsize=11)\nax.set_ylabel(\"Drawdown (%)\", fontsize=11)\nax.set_title(\"Portfolio Drawdowns\", fontsize=12, fontweight=\"bold\")\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\nFigure 35.7\n\n\n\n\n\n\nTable 35.16: Risk metrics for governance portfolios\n\n\ndef compute_risk_metrics(returns, name, rf=None):\n    \"\"\"Compute standard risk metrics.\"\"\"\n    r = returns.dropna()\n    if rf is not None:\n        excess = r - rf\n    else:\n        excess = r\n    \n    ann_return = (1 + r.mean()) ** 12 - 1\n    ann_vol = r.std() * np.sqrt(12)\n    sharpe = excess.mean() / r.std() * np.sqrt(12) if r.std() &gt; 0 else np.nan\n    \n    cum = (1 + r).cumprod()\n    max_dd = compute_drawdown(cum).min()\n    \n    # Sortino ratio (downside deviation)\n    downside = r[r &lt; 0].std() * np.sqrt(12)\n    sortino = excess.mean() * 12 / downside if downside &gt; 0 else np.nan\n    \n    # Skewness and kurtosis\n    skew = r.skew()\n    kurt = r.kurtosis()\n    \n    return {\n        \"Portfolio\": name,\n        \"Ann. Return (%)\": ann_return * 100,\n        \"Ann. Volatility (%)\": ann_vol * 100,\n        \"Sharpe Ratio\": sharpe,\n        \"Sortino Ratio\": sortino,\n        \"Max Drawdown (%)\": max_dd * 100,\n        \"Skewness\": skew,\n        \"Excess Kurtosis\": kurt,\n        \"% Positive Months\": (r &gt; 0).mean() * 100\n    }\n\nrf_series = analysis_data.set_index(\"date\")[\"rf\"].reindex(\n    returns_wide[\"date\"]\n).fillna(0)\n\nrisk_table = pd.DataFrame([\n    compute_risk_metrics(\n        returns_wide[\"ret_vw_Democracy\"], \"Democracy (VW)\",\n        rf_series.values\n    ),\n    compute_risk_metrics(\n        returns_wide[\"ret_vw_Dictatorship\"], \"Dictatorship (VW)\",\n        rf_series.values\n    ),\n    compute_risk_metrics(\n        returns_wide[\"ret_diff_vw\"], \"Long-Short (VW)\"\n    )\n]).set_index(\"Portfolio\")\n\nrisk_table.round(3)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#discussion-and-interpretation",
    "href": "53_governance.html#discussion-and-interpretation",
    "title": "35  Corporate Governance",
    "section": "35.9 Discussion and Interpretation",
    "text": "35.9 Discussion and Interpretation\n\n35.9.1 Why Might Governance Predict Returns in Vietnam?\nSeveral channels may explain a governance premium in the Vietnamese market:\nThe risk channel: Poorly governed firms may be riskier in ways not captured by standard factor models. Investors require a higher expected return to hold these firms, but the return difference reverses (i.e., the governance premium is positive for well-governed firms) if the market overestimates the risk of poorly governed firms or if governance risk is partially diversifiable.\nThe mispricing channel: If the market is slow to incorporate governance information into prices, perhaps because governance quality is costly to assess or because many Vietnamese investors are retail traders with limited analytical capacity, then a systematic strategy that buys good governance and sells bad governance can profit from the gradual correction of mispricings.\nThe cash flow channel: Better-governed firms may generate genuinely higher cash flows because they waste less on empire-building, related-party transactions, and other value-destroying activities. To the extent that these superior cash flows are not fully anticipated by the market, good governance firms deliver positive return surprises.\n\n\n35.9.2 State Ownership and the Governance Effect\nA distinctive feature of the Vietnamese market is the prevalence of state-owned enterprises (SOEs). The interaction between state ownership and governance quality creates an interesting dynamic:\n\nSOEs may have weak governance by conventional metrics (limited board independence, political appointments) but benefit from implicit government guarantees and preferential access to land, capital, and contracts.\nThe governance premium may therefore differ between SOEs and private firms. We encourage readers to extend the analysis by interacting the governance index with a state ownership indicator.\n\n\n\n35.9.3 Limitations\nSeveral caveats apply to this analysis:\n\nSurvivorship bias: If poorly governed firms are more likely to delist (due to financial distress or regulatory action), the Dictatorship portfolio’s returns may be biased upward, attenuating the true governance premium.\nTransaction costs: The long-short strategy requires short selling, which is restricted in Vietnam. Implementation via a long-only tilt toward Democracy firms may be more practical.\nGovernance data frequency: Unlike daily stock prices, governance data is updated infrequently (typically annually). The portfolio rebalancing frequency is therefore low, which limits the strategy’s responsiveness to governance changes.\nIndex construction: The specific provisions included in the VN-GIndex and their equal weighting may not optimally capture governance quality. Future work could explore weighted indices, following L. Bebchuk, Cohen, and Ferrell (2009).",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#exercises",
    "href": "53_governance.html#exercises",
    "title": "35  Corporate Governance",
    "section": "35.10 Exercises",
    "text": "35.10 Exercises\n\nEntrenchment Index: Following L. Bebchuk, Cohen, and Ferrell (2009), identify the subset of governance provisions in the Vietnamese data that have the strongest association with firm value (measured by Tobin’s Q or market-to-book ratio). Construct a Vietnamese E-Index using only these provisions and compare its predictive power for returns to the full VN-GIndex.\nFama-MacBeth regressions: Instead of sorting firms into portfolios, estimate the governance-return relationship using Fama-MacBeth cross-sectional regressions (Fama and MacBeth 1973):\n\n\\[\nr_{i,t} - r_{f,t} = \\gamma_{0,t} + \\gamma_{1,t} \\text{VN-GIndex}_{i,t-1} + \\gamma_{2,t} \\mathbf{X}_{i,t-1} + \\epsilon_{i,t}\n\\]\nwhere \\(\\mathbf{X}_{i,t-1}\\) includes controls for size, book-to-market, and momentum. Report the time-series averages of \\(\\hat{\\gamma}_{1,t}\\) with Newey-West standard errors.\n\nState ownership interaction: Split the sample into SOEs (state ownership &gt; 50%) and private firms. Does the governance premium differ between these groups? Estimate:\n\n\\[\nr_{t}^{D-D} = \\alpha + \\beta_1 \\text{MKTRF}_t + \\beta_2 \\text{SMB}_t + \\beta_3 \\text{HML}_t + \\beta_4 \\text{UMD}_t + \\varepsilon_t\n\\] separately for SOE and non-SOE subsamples.\n\nTransaction cost analysis: Compute the portfolio turnover at each rebalancing date (when new governance data becomes available). Estimate how much of the abnormal return would be consumed by transaction costs at realistic bid-ask spreads for Vietnamese equities.\nGovernance changes: Do firms that improve their governance (declining VN-GIndex) earn higher subsequent returns than firms whose governance deteriorates? Construct portfolios based on \\(\\Delta \\text{VN-GIndex}\\) and test.\nComparison with market-wide governance reforms: Vietnam has implemented several governance reform waves (e.g., Circular 121/2012/TT-BTC, Decree 71/2017/ND-CP). Test whether the governance premium narrows after these regulatory changes using a structural break or interaction approach.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "53_governance.html#summary",
    "href": "53_governance.html#summary",
    "title": "35  Corporate Governance",
    "section": "35.11 Summary",
    "text": "35.11 Summary\nThis chapter adapts the influential Gompers, Ishii, and Metrick (2003) governance investment strategy to the Vietnamese equity market. The key methodological steps are:\n\nConstruct a governance index (VN-GIndex) from firm-level governance provisions available through DataCore.vn.\nClassify firms into Democracy (strong rights) and Dictatorship (weak rights) portfolios based on the cross-sectional distribution of the index.\nCompute value-weighted monthly portfolio returns, rebalancing when new governance data becomes available.\nEvaluate the long-short (Democracy minus Dictatorship) strategy using t-tests and Fama-French-Carhart four-factor regressions.\nExamine robustness across subperiods, quintile portfolios, and alternative weighting schemes.\n\n\n\n\n\n\n\nBebchuk, Lucian A, Alma Cohen, and Charles CY Wang. 2013. “Learning and the Disappearing Association Between Governance and Returns.” Journal of Financial Economics 108 (2): 323–48.\n\n\nBebchuk, Lucian, Alma Cohen, and Allen Ferrell. 2009. “What Matters in Corporate Governance?” The Review of Financial Studies 22 (2): 783–827.\n\n\nCarhart, Mark M. 1997. “On persistence in mutual fund performance.” The Journal of Finance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nClaessens, Stijn, Simeon Djankov, and Larry HP Lang. 2000. “The Separation of Ownership and Control in East Asian Corporations.” Journal of Financial Economics 58 (1-2): 81–112.\n\n\nCremers, KJ Martijn, and Vinay B Nair. 2005. “Governance Mechanisms and Equity Prices.” The Journal of Finance 60 (6): 2859–94.\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\nFama, Eugene F., and James D. MacBeth. 1973. “Risk, return, and equilibrium: Empirical tests.” Journal of Political Economy 81 (3): 607–36. https://doi.org/10.1086/260061.\n\n\nGompers, Paul, Joy Ishii, and Andrew Metrick. 2003. “Corporate Governance and Equity Prices.” The Quarterly Journal of Economics 118 (1): 107–56.\n\n\nJensen, Michael C, and William H Meckling. 1976. “Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure.” Journal of Financial Economics 3 (4): 305–60.\n\n\nJohnson, Simon, Rafael La Porta, Florencio Lopez-de-Silanes, and Andrei Shleifer. 2000. “Tunneling.” American Economic Review 90 (2): 22–27.\n\n\nKlapper, Leora F, and Inessa Love. 2004. “Corporate Governance, Investor Protection, and Performance in Emerging Markets.” Journal of Corporate Finance 10 (5): 703–28.\n\n\nLa Porta, Rafael, Florencio Lopez-de-Silanes, Andrei Shleifer, and Robert Vishny. 2000. “Investor Protection and Corporate Governance.” Journal of Financial Economics 58 (1-2): 3–27.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Corporate Governance</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html",
    "href": "54_return_gaps.html",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "",
    "text": "36.1 Why Return Gap Matters\nMutual fund managers possess considerable discretion in their investment decisions between mandatory portfolio disclosure dates. While regulatory frameworks require periodic disclosure of holdings, the actions taken between these disclosure dates (e.g., trading, market timing, securities lending, and strategic cash management) remain largely unobservable to investors. These unobserved actions can significantly affect fund performance, either positively through skilled interim trading or negatively through agency costs and hidden behavior.\nKacperczyk, Sialm, and Zheng (2008) developed the Return Gap measure to capture the aggregate impact of these unobserved actions on fund returns. The Return Gap is defined as the difference between a fund’s actual reported return and the hypothetical return of a portfolio that mechanically invests in the fund’s most recently disclosed holdings. Formally:\n\\[\n\\text{Return Gap}_{i,t} = R_{i,t}^{\\text{Actual}} - R_{i,t}^{\\text{Holdings}}\n\\tag{36.1}\\]\nwhere \\(R_{i,t}^{\\text{Actual}}\\) is the net-of-expense return reported by fund \\(i\\) in month \\(t\\), adjusted for expenses to obtain the gross return, and \\(R_{i,t}^{\\text{Holdings}}\\) is the hypothetical buy-and-hold return computed from the most recently disclosed portfolio holdings.\nA positive Return Gap indicates that the fund manager’s unobserved actions (e.g., interim trading, cash management, or other activities) added value beyond what a passive replication of disclosed holdings would have generated. Conversely, a persistently negative Return Gap suggests value-destroying interim activity, potentially driven by agency costs, poor trading execution, or hidden fees.\nThe Return Gap is economically significant for several reasons:",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#why-return-gap-matters",
    "href": "54_return_gaps.html#why-return-gap-matters",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "",
    "text": "Performance persistence: Funds in the highest Return Gap decile tend to outperform those in the lowest decile by 1-2% annually on a risk-adjusted basis, and this spread persists over time (Kacperczyk, Sialm, and Zheng 2008).\nDetecting agency problems: A persistently negative Return Gap can signal hidden costs such as excessive trading, market impact costs, soft-dollar arrangements, or stale-price exploitation.\nComplementing traditional measures: Unlike alpha-based metrics that blend stock selection skill with interim trading skill, the Return Gap isolates the component of performance attributable to actions taken between disclosure dates.\nRegulatory implications: In emerging markets like Vietnam, where disclosure frequency and regulatory oversight may differ from developed markets, the Return Gap can serve as an early warning system for investor protection.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#application-to-the-vietnamese-market",
    "href": "54_return_gaps.html#application-to-the-vietnamese-market",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "36.2 Application to the Vietnamese Market",
    "text": "36.2 Application to the Vietnamese Market\nThe Vietnamese mutual fund industry, while relatively young compared to the United States, has experienced rapid growth since the establishment of the first domestic equity funds in the early 2000s. As of 2024, Vietnam’s open-ended fund industry manages assets exceeding 100 trillion VND, with dozens of equity-oriented funds operated by both domestic and foreign-affiliated asset management companies.\nSeveral characteristics of the Vietnamese market make the Return Gap analysis particularly interesting:\n\nDisclosure frequency: Vietnamese funds are required to disclose their top holdings periodically, but the frequency and completeness of disclosure may differ from the quarterly SEC requirements in the U.S.\nMarket microstructure: The HOSE (Ho Chi Minh Stock Exchange) and HNX (Hanoi Stock Exchange) feature daily price limits (plus or minus 7% on HOSE, plus or minus 10% on HNX), T+2 settlement, and foreign ownership limits that may constrain or enable certain interim trading strategies.\nInformation asymmetry: In an emerging market with less analyst coverage, the scope for informed interim trading and hence positive Return Gap may be larger than in more efficient markets.\nRegulatory environment: Vietnam’s State Securities Commission (SSC) has progressively strengthened disclosure and governance requirements, making temporal analysis of Return Gap especially informative.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#decomposing-fund-returns",
    "href": "54_return_gaps.html#decomposing-fund-returns",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "37.1 Decomposing Fund Returns",
    "text": "37.1 Decomposing Fund Returns\nConsider a mutual fund \\(i\\) that discloses its portfolio holdings at discrete dates \\(\\tau_1, \\tau_2, \\ldots\\) As disclosed at date \\(\\tau_k\\), the fund holds \\(N_k\\) securities with weights \\(\\{w_{j,\\tau_k}\\}_{j=1}^{N_k}\\), where \\(w_{j,\\tau_k}\\) represents the portfolio weight of security \\(j\\).\nBetween disclosure dates \\(\\tau_k\\) and \\(\\tau_{k+1}\\), the fund’s actual gross return in month \\(t\\) can be decomposed as:\n\\[\nR_{i,t}^{\\text{Gross}} = R_{i,t}^{\\text{Holdings}} + \\underbrace{R_{i,t}^{\\text{Gross}} - R_{i,t}^{\\text{Holdings}}}_{\\text{Return Gap}}\n\\tag{37.1}\\]\nThe hypothetical holdings return \\(R_{i,t}^{\\text{Holdings}}\\) is computed as the value-weighted return of the buy-and-hold portfolio based on the most recent disclosure:\n\\[\nR_{i,t}^{\\text{Holdings}} = \\sum_{j=1}^{N_k} \\tilde{w}_{j,t-1} \\cdot r_{j,t}\n\\tag{37.2}\\]\nwhere \\(r_{j,t}\\) is the return of security \\(j\\) in month \\(t\\), and \\(\\tilde{w}_{j,t-1}\\) is the evolved portfolio weight at the end of month \\(t-1\\), reflecting the buy-and-hold drift from the original disclosure weights:\n\\[\n\\tilde{w}_{j,t-1} = \\frac{w_{j,\\tau_k} \\prod_{s=\\tau_k+1}^{t-1}(1 + r_{j,s})}{\\sum_{\\ell=1}^{N_k} w_{\\ell,\\tau_k} \\prod_{s=\\tau_k+1}^{t-1}(1 + r_{\\ell,s})}\n\\tag{37.3}\\]\nIn practice, rather than tracking evolved weights explicitly, we use dollar values of holdings positions (shares held times price) as the natural weighting scheme.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#the-return-gap-measure",
    "href": "54_return_gaps.html#the-return-gap-measure",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "37.2 The Return Gap Measure",
    "text": "37.2 The Return Gap Measure\n\n37.2.1 Gross Return Gap\nThe Return Gap as originally defined by Kacperczyk, Sialm, and Zheng (2008) uses the gross (before-expense) return:\n\\[\n\\text{RG}_{i,t} = R_{i,t}^{\\text{Gross}} - R_{i,t}^{\\text{Holdings}} = \\left(R_{i,t}^{\\text{Net}} + \\frac{\\text{Expense Ratio}_{i,t}}{12}\\right) - R_{i,t}^{\\text{Holdings}}\n\\tag{37.4}\\]\nwhere \\(R_{i,t}^{\\text{Net}}\\) is the reported net-of-expense return and the annual expense ratio is divided by 12 to approximate the monthly expense charge.\n\n\n37.2.2 Sources of Return Gap\nThe Return Gap captures several components (Kacperczyk, Sialm, and Zheng 2008; Elton, Gruber, and Blake 2011):\n\\[\n\\text{RG}_{i,t} = \\underbrace{\\Delta_{\\text{trade}}}_{\\text{Interim trading}} + \\underbrace{\\Delta_{\\text{cash}}}_{\\text{Cash drag/return}} + \\underbrace{\\Delta_{\\text{fees}}}_{\\text{Hidden fees}} + \\underbrace{\\Delta_{\\text{lend}}}_{\\text{Securities lending}} + \\underbrace{\\varepsilon_t}_{\\text{Noise}}\n\\tag{37.5}\\]\nwhere:\n\n\\(\\Delta_{\\text{trade}}\\): The return impact of buying and selling securities between disclosure dates. Skilled managers generate positive \\(\\Delta_{\\text{trade}}\\) by timing trades.\n\\(\\Delta_{\\text{cash}}\\): The effect of holding cash or cash equivalents not captured in equity holdings disclosures. In rising markets, cash creates a drag (negative contribution); in falling markets, cash provides a cushion.\n\\(\\Delta_{\\text{fees}}\\): Transaction costs, brokerage commissions, and any hidden fees not reflected in the stated expense ratio.\n\\(\\Delta_{\\text{lend}}\\): Revenue from securities lending programs, which generates positive Return Gap.\n\\(\\varepsilon_t\\): Measurement noise from timing differences, stale prices, or data errors.\n\n\n\n37.2.3 Predictive Return Gap\nTo form tradeable portfolios and avoid look-ahead bias, Kacperczyk, Sialm, and Zheng (2008) use the trailing 12-month average Return Gap, lagged by one quarter to account for the reporting delay:\n\\[\n\\overline{\\text{RG}}_{i,t}^{12} = \\frac{1}{12} \\sum_{s=1}^{12} \\text{RG}_{i,t-s}\n\\tag{37.6}\\]\nThe additional 3-month (one quarter) lag ensures that the Return Gap signal is based only on information available to investors at the time of portfolio formation. This is particularly important in Vietnam, where fund reporting may involve delays.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#risk-adjusted-performance-evaluation",
    "href": "54_return_gaps.html#risk-adjusted-performance-evaluation",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "37.3 Risk-Adjusted Performance Evaluation",
    "text": "37.3 Risk-Adjusted Performance Evaluation\nTo evaluate whether Return Gap-sorted portfolios generate genuine risk-adjusted returns, we employ several factor models.\n\n37.3.1 CAPM Alpha\n\\[\nR_{p,t} - R_{f,t} = \\alpha_p + \\beta_p (R_{m,t} - R_{f,t}) + \\epsilon_{p,t}\n\\tag{37.7}\\]\n\n\n37.3.2 Fama-French Three-Factor Model\n\\[\nR_{p,t} - R_{f,t} = \\alpha_p + \\beta_{1,p} \\cdot \\text{MKT}_t + \\beta_{2,p} \\cdot \\text{SMB}_t + \\beta_{3,p} \\cdot \\text{HML}_t + \\epsilon_{p,t}\n\\tag{37.8}\\]\n\n\n37.3.3 Carhart Four-Factor Model\n\\[\nR_{p,t} - R_{f,t} = \\alpha_p + \\beta_{1,p} \\cdot \\text{MKT}_t + \\beta_{2,p} \\cdot \\text{SMB}_t + \\beta_{3,p} \\cdot \\text{HML}_t + \\beta_{4,p} \\cdot \\text{UMD}_t + \\epsilon_{p,t}\n\\tag{37.9}\\]\nwhere \\(\\text{UMD}_t\\) is the momentum factor (up minus down).\n\n\n37.3.4 Fama-French Five-Factor Model\nFor a more comprehensive risk adjustment relevant to the Vietnamese market:\n\\[\nR_{p,t} - R_{f,t} = \\alpha_p + \\beta_1 \\text{MKT}_t + \\beta_2 \\text{SMB}_t + \\beta_3 \\text{HML}_t + \\beta_4 \\text{RMW}_t + \\beta_5 \\text{CMA}_t + \\epsilon_{p,t}\n\\tag{37.10}\\]\nwhere \\(\\text{RMW}_t\\) (robust minus weak) captures profitability and \\(\\text{CMA}_t\\) (conservative minus aggressive) captures investment patterns.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#newey-west-standard-errors",
    "href": "54_return_gaps.html#newey-west-standard-errors",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "37.4 Newey-West Standard Errors",
    "text": "37.4 Newey-West Standard Errors\nSince portfolio returns may exhibit serial correlation, we use Newey and West (1987) standard errors with \\(L\\) lags:\n\\[\n\\hat{V}(\\hat{\\alpha}) = T \\left(\\sum_{t=1}^{T} \\mathbf{x}_t \\mathbf{x}_t'\\right)^{-1} \\hat{S} \\left(\\sum_{t=1}^{T} \\mathbf{x}_t \\mathbf{x}_t'\\right)^{-1}\n\\tag{37.11}\\]\nwhere the HAC covariance estimator is:\n\\[\n\\hat{S} = \\hat{\\Gamma}_0 + \\sum_{\\ell=1}^{L} \\left(1 - \\frac{\\ell}{L+1}\\right)\\left(\\hat{\\Gamma}_\\ell + \\hat{\\Gamma}_\\ell'\\right)\n\\tag{37.12}\\]\nand \\(\\hat{\\Gamma}_\\ell = \\frac{1}{T}\\sum_{t=\\ell+1}^{T} \\hat{\\epsilon}_t \\hat{\\epsilon}_{t-\\ell} \\mathbf{x}_t \\mathbf{x}_{t-\\ell}'\\). The standard lag choice is \\(L = \\lfloor 4(T/100)^{2/9} \\rfloor\\).",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#data-sources",
    "href": "54_return_gaps.html#data-sources",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "38.1 Data Sources",
    "text": "38.1 Data Sources\nTable 38.1 shows the sources used in the construction of return gaps.\n\n\n\nTable 38.1: Data sources for the Return Gap analysis in Vietnam\n\n\n\n\n\n\n\n\n\n\nData Category\nSource\nDescription\n\n\n\n\nFund holdings\nDataCore Fund Holdings\nDisclosed portfolio positions including ticker, shares held, report date, and vintage (filing) date\n\n\nFund returns\nDataCore Fund Performance\nMonthly NAV-based net returns, total net assets, and expense ratios\n\n\nFund characteristics\nDataCore Fund Master\nFund objective codes, inception dates, management company, investment style\n\n\nStock prices and returns\nDataCore Equity Market\nDaily and monthly adjusted prices, returns, shares outstanding, and corporate actions for HOSE and HNX listed securities\n\n\nRisk factors\nDataCore / Constructed\nVietnamese market factor portfolios (MKT, SMB, HML, UMD, RMW, CMA)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#setting-up-the-environment",
    "href": "54_return_gaps.html#setting-up-the-environment",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "38.2 Setting Up the Environment",
    "text": "38.2 Setting Up the Environment\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.regression.linear_model import OLS\nfrom scipy import stats\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams.update({\n    \"figure.figsize\": (10, 6),\n    \"font.size\": 12,\n    \"axes.titlesize\": 14,\n    \"axes.labelsize\": 12,\n    \"xtick.labelsize\": 10,\n    \"ytick.labelsize\": 10,\n    \"legend.fontsize\": 10,\n    \"figure.dpi\": 150,\n    \"savefig.dpi\": 300,\n    \"font.family\": \"serif\",\n})\n\nsns.set_style(\"whitegrid\")\nnp.random.seed(42)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gapstock-data",
    "href": "54_return_gaps.html#sec-return-gapstock-data",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "38.3 Loading and Preparing Stock Market Data",
    "text": "38.3 Loading and Preparing Stock Market Data\nThe first step is to load the stock-level data, which provides the foundation for computing hypothetical holdings returns.\n\n# ============================================================\n# In production, replace with actual DataCore API calls:\n#   from datacore import DataCoreClient\n#   client = DataCoreClient(api_key=\"YOUR_KEY\")\n#   stock_data = client.get_equity_monthly(\n#       exchange=[\"HOSE\", \"HNX\"],\n#       start_date=\"2010-01-01\",\n#       end_date=\"2024-12-31\",\n#       fields=[\"ticker\", \"date\", \"close_adj\", \"return_monthly\",\n#               \"shares_outstanding\", \"market_cap\"]\n#   )\n# ============================================================\n\ndef generate_stock_data(\n    n_stocks: int = 300,\n    start_date: str = \"2012-01-01\",\n    end_date: str = \"2024-12-31\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate simulated monthly stock data mimicking Vietnamese\n    equity market characteristics.\n    \"\"\"\n    dates = pd.date_range(start_date, end_date, freq=\"ME\")\n    tickers = [f\"VN{str(i).zfill(4)}\" for i in range(1, n_stocks + 1)]\n\n    records = []\n    for ticker in tickers:\n        list_offset = np.random.randint(0, max(1, len(dates) // 3))\n        available_dates = dates[list_offset:]\n        mu = np.random.normal(0.008, 0.005)\n        sigma = np.random.uniform(0.06, 0.15)\n        beta = np.random.uniform(0.5, 1.8)\n        market_shocks = np.random.normal(0.005, 0.06, len(available_dates))\n        idio_shocks = np.random.normal(0, sigma, len(available_dates))\n        returns = mu + beta * market_shocks + idio_shocks\n        returns = np.clip(returns, -0.30, 0.40)\n        price = np.random.uniform(10, 150)\n        prices = [price]\n        for r in returns[:-1]:\n            price = price * (1 + r)\n            prices.append(price)\n        shares = np.random.uniform(50, 500) * 1e6\n        shares_series = np.full(len(available_dates), shares)\n        for i, d in enumerate(available_dates):\n            records.append({\n                \"ticker\": ticker, \"date\": d,\n                \"close_adj\": prices[i], \"ret\": returns[i],\n                \"shares_outstanding\": shares_series[i],\n                \"market_cap\": prices[i] * shares_series[i] / 1e9,\n            })\n\n    df = pd.DataFrame(records)\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df = df.sort_values([\"ticker\", \"date\"])\n    df[\"close_adj_lag\"] = df.groupby(\"ticker\")[\"close_adj\"].shift(1)\n    return df\n\nstock_data = generate_stock_data()\nprint(f\"Stock data: {stock_data.shape[0]:,} stock-months\")\nprint(f\"Unique stocks: {stock_data['ticker'].nunique()}\")\nprint(f\"Date range: {stock_data['date'].min():%Y-%m} to \"\n      f\"{stock_data['date'].max():%Y-%m}\")\nstock_data.head(10)\n\nStock data: 38,865 stock-months\nUnique stocks: 300\nDate range: 2012-01 to 2024-12\n\n\n\n\n\n\n\n\n\nticker\ndate\nclose_adj\nret\nshares_outstanding\nmarket_cap\nclose_adj_lag\n\n\n\n\n0\nVN0001\n2015-03-31\n45.035190\n0.110630\n6.747563e+07\n3.038778\nNaN\n\n\n1\nVN0001\n2015-04-30\n50.017423\n-0.066939\n6.747563e+07\n3.374957\n45.035190\n\n\n2\nVN0001\n2015-05-31\n46.669311\n0.047021\n6.747563e+07\n3.149041\n50.017423\n\n\n3\nVN0001\n2015-06-30\n48.863751\n-0.152745\n6.747563e+07\n3.297112\n46.669311\n\n\n4\nVN0001\n2015-07-31\n41.400073\n-0.057519\n6.747563e+07\n2.793496\n48.863751\n\n\n5\nVN0001\n2015-08-31\n39.018788\n0.228286\n6.747563e+07\n2.632817\n41.400073\n\n\n6\nVN0001\n2015-09-30\n47.926227\n0.079232\n6.747563e+07\n3.233852\n39.018788\n\n\n7\nVN0001\n2015-10-31\n51.723515\n-0.300000\n6.747563e+07\n3.490077\n47.926227\n\n\n8\nVN0001\n2015-11-30\n36.206461\n0.192114\n6.747563e+07\n2.443054\n51.723515\n\n\n9\nVN0001\n2015-12-31\n43.162239\n0.294336\n6.747563e+07\n2.912399\n36.206461",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gapholdings-data",
    "href": "54_return_gaps.html#sec-return-gapholdings-data",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "38.4 Loading Fund Holdings Data",
    "text": "38.4 Loading Fund Holdings Data\n\ndef generate_holdings_data(\n    stock_data: pd.DataFrame,\n    n_funds: int = 50,\n    start_date: str = \"2012-06-30\",\n    end_date: str = \"2024-12-31\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate simulated fund holdings data. Each fund holds 15-80\n    stocks, disclosed semi-annually or quarterly.\n    \"\"\"\n    dates = pd.date_range(start_date, end_date, freq=\"ME\")\n    tickers = stock_data[\"ticker\"].unique()\n    fund_ids = [f\"FUND{str(i).zfill(3)}\" for i in range(1, n_funds + 1)]\n    records = []\n    for fund_id in fund_ids:\n        inception_idx = np.random.randint(0, max(1, len(dates) // 4))\n        freq = 3 if np.random.random() &lt; 0.7 else 6\n        n_stocks_held = np.random.randint(15, 80)\n        core_stocks = np.random.choice(tickers, size=n_stocks_held, replace=False)\n        report_dates = dates[inception_idx::freq]\n        for rdate in report_dates:\n            filing_delay = np.random.randint(1, 4)\n            fdate = rdate + pd.DateOffset(months=filing_delay)\n            turnover = np.random.uniform(0.05, 0.20)\n            n_replace = max(1, int(n_stocks_held * turnover))\n            replace_idx = np.random.choice(len(core_stocks), size=n_replace, replace=False)\n            new_stocks = np.random.choice(tickers, size=n_replace, replace=False)\n            core_stocks[replace_idx] = new_stocks\n            for ticker in core_stocks:\n                shares = np.random.uniform(100_000, 5_000_000)\n                records.append({\n                    \"fund_id\": fund_id, \"report_date\": rdate,\n                    \"filing_date\": fdate, \"ticker\": ticker,\n                    \"shares_held\": shares,\n                })\n    df = pd.DataFrame(records)\n    df[\"report_date\"] = pd.to_datetime(df[\"report_date\"])\n    df[\"filing_date\"] = pd.to_datetime(df[\"filing_date\"])\n    return df\n\nholdings_raw = generate_holdings_data(stock_data)\nprint(f\"Holdings records: {holdings_raw.shape[0]:,}\")\nprint(f\"Unique funds: {holdings_raw['fund_id'].nunique()}\")\nholdings_raw.head(10)\n\nHoldings records: 89,269\nUnique funds: 50\n\n\n\n\n\n\n\n\n\nfund_id\nreport_date\nfiling_date\nticker\nshares_held\n\n\n\n\n0\nFUND001\n2012-11-30\n2012-12-30\nVN0289\n4.918132e+06\n\n\n1\nFUND001\n2012-11-30\n2012-12-30\nVN0297\n4.322425e+05\n\n\n2\nFUND001\n2012-11-30\n2012-12-30\nVN0259\n2.383117e+05\n\n\n3\nFUND001\n2012-11-30\n2012-12-30\nVN0215\n1.140891e+06\n\n\n4\nFUND001\n2012-11-30\n2012-12-30\nVN0262\n1.104603e+06\n\n\n5\nFUND001\n2012-11-30\n2012-12-30\nVN0292\n1.665416e+06\n\n\n6\nFUND001\n2012-11-30\n2012-12-30\nVN0132\n4.625400e+06\n\n\n7\nFUND001\n2012-11-30\n2012-12-30\nVN0049\n3.447053e+06\n\n\n8\nFUND001\n2012-11-30\n2012-12-30\nVN0008\n1.525004e+05\n\n\n9\nFUND001\n2012-11-30\n2012-12-30\nVN0189\n2.527258e+06",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gapfund-returns",
    "href": "54_return_gaps.html#sec-return-gapfund-returns",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "38.5 Loading Fund Returns and Characteristics",
    "text": "38.5 Loading Fund Returns and Characteristics\n\ndef generate_fund_returns(holdings, start_date=\"2012-01-01\", end_date=\"2024-12-31\"):\n    \"\"\"Generate monthly fund-level net returns, TNA, and expense ratios.\"\"\"\n    fund_ids = holdings[\"fund_id\"].unique()\n    dates = pd.date_range(start_date, end_date, freq=\"ME\")\n    records = []\n    for fund_id in fund_ids:\n        fund_start = holdings.loc[holdings[\"fund_id\"] == fund_id, \"report_date\"].min() - pd.DateOffset(months=3)\n        fund_dates = dates[dates &gt;= fund_start]\n        exp_ratio = np.random.uniform(0.010, 0.025)\n        base_tna = np.random.uniform(50, 2000)\n        mu = np.random.normal(0.007, 0.003)\n        sigma = np.random.uniform(0.04, 0.09)\n        tna = base_tna\n        for d in fund_dates:\n            ret = np.clip(np.random.normal(mu, sigma), -0.25, 0.35)\n            tna = max(tna * (1 + ret) + np.random.normal(0, base_tna * 0.02), 10)\n            records.append({\"fund_id\": fund_id, \"date\": d, \"net_return\": ret,\n                            \"tna\": tna, \"expense_ratio\": exp_ratio + np.random.normal(0, 0.001)})\n    df = pd.DataFrame(records)\n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df[\"expense_ratio\"] = df[\"expense_ratio\"].clip(0.005, 0.035)\n    return df\n\nfund_returns = generate_fund_returns(holdings_raw)\nprint(f\"Fund-month observations: {fund_returns.shape[0]:,}\")\nfund_returns.head(10)\n\nFund-month observations: 6,808\n\n\n\n\n\n\n\n\n\nfund_id\ndate\nnet_return\ntna\nexpense_ratio\n\n\n\n\n0\nFUND001\n2012-08-31\n0.045634\n568.611866\n0.022429\n\n\n1\nFUND001\n2012-09-30\n0.106959\n631.630643\n0.021461\n\n\n2\nFUND001\n2012-10-31\n-0.063495\n612.614200\n0.020634\n\n\n3\nFUND001\n2012-11-30\n-0.087247\n563.095634\n0.023795\n\n\n4\nFUND001\n2012-12-31\n-0.006777\n545.547597\n0.021886\n\n\n5\nFUND001\n2013-01-31\n-0.029553\n532.221482\n0.021999\n\n\n6\nFUND001\n2013-02-28\n-0.002767\n538.411988\n0.021567\n\n\n7\nFUND001\n2013-03-31\n-0.138187\n477.997478\n0.022189\n\n\n8\nFUND001\n2013-04-30\n0.045137\n484.711443\n0.022058\n\n\n9\nFUND001\n2013-05-31\n0.095518\n523.213577\n0.021911",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gapsample-selection",
    "href": "54_return_gaps.html#sec-return-gapsample-selection",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "38.6 Sample Selection: Domestic Equity Funds",
    "text": "38.6 Sample Selection: Domestic Equity Funds\nFollowing the approach of Kacperczyk, Sialm, and Zheng (2008), we restrict our sample to domestic equity funds.\n\nequity_objectives = [\n    \"EQUITY_DOMESTIC\", \"EQUITY_GROWTH\", \"EQUITY_VALUE\",\n    \"EQUITY_BLEND\", \"EQUITY_LARGE_CAP\", \"EQUITY_MID_CAP\",\n    \"EQUITY_SMALL_CAP\",\n]\n\nfund_ids = fund_returns[\"fund_id\"].unique()\nfund_master = pd.DataFrame({\n    \"fund_id\": fund_ids,\n    \"objective\": np.random.choice(\n        equity_objectives + [\"BOND\", \"BALANCED\", \"MONEY_MARKET\"],\n        size=len(fund_ids),\n        p=[0.08, 0.08, 0.06, 0.10, 0.08, 0.06, 0.06, 0.15, 0.18, 0.15],\n    ),\n})\n\nequity_fund_ids = fund_master.loc[\n    fund_master[\"objective\"].isin(equity_objectives), \"fund_id\"\n].values\n\nprint(f\"Total funds: {len(fund_ids)}\")\nprint(f\"Equity funds: {len(equity_fund_ids)} ({len(equity_fund_ids)/len(fund_ids)*100:.1f}%)\")\nprint(\"\\nObjective distribution:\")\nprint(fund_master[\"objective\"].value_counts().to_string())\n\nTotal funds: 50\nEquity funds: 29 (58.0%)\n\nObjective distribution:\nobjective\nBALANCED            9\nEQUITY_VALUE        9\nEQUITY_GROWTH       6\nBOND                6\nMONEY_MARKET        6\nEQUITY_BLEND        5\nEQUITY_LARGE_CAP    3\nEQUITY_SMALL_CAP    2\nEQUITY_MID_CAP      2\nEQUITY_DOMESTIC     2",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gapvintages",
    "href": "54_return_gaps.html#sec-return-gapvintages",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "39.1 Step 1: Prepare Holdings Vintages",
    "text": "39.1 Step 1: Prepare Holdings Vintages\nA critical first step is to correctly handle the vintage structure of holdings data. Each holdings report has two key dates: the report date (\\(\\tau\\), the as-of date) and the filing date (\\(f\\), when it becomes public). We keep only the first vintage per fund-report date.\n\ndef prepare_holdings_vintages(holdings, max_holding_months=6):\n    \"\"\"Process holdings vintages and compute next report dates.\"\"\"\n    first_vintage = (\n        holdings.sort_values([\"fund_id\", \"report_date\", \"filing_date\"])\n        .groupby([\"fund_id\", \"report_date\"])\n        .agg(filing_date=(\"filing_date\", \"first\"))\n        .reset_index()\n    )\n    first_vintage = first_vintage.sort_values([\"fund_id\", \"report_date\"])\n    first_vintage[\"next_report_date\"] = first_vintage.groupby(\"fund_id\")[\"report_date\"].shift(-1)\n    max_date = first_vintage[\"report_date\"] + pd.DateOffset(months=max_holding_months)\n    first_vintage[\"next_report_date\"] = first_vintage[\"next_report_date\"].fillna(max_date)\n    first_vintage[\"next_report_date\"] = first_vintage[[\"next_report_date\"]].min(axis=1).clip(upper=max_date)\n    first_vintage[\"next_report_date\"] = first_vintage[\"next_report_date\"] + pd.offsets.MonthEnd(0)\n    result = holdings.merge(first_vintage, on=[\"fund_id\", \"report_date\", \"filing_date\"], how=\"inner\")\n    return result\n\nholdings_vintaged = prepare_holdings_vintages(holdings_raw)\nprint(f\"Holdings after vintage processing: {holdings_vintaged.shape[0]:,} records\")\nsample_fund = holdings_vintaged[\"fund_id\"].iloc[0]\n(holdings_vintaged.loc[holdings_vintaged[\"fund_id\"] == sample_fund]\n [[\"fund_id\", \"report_date\", \"filing_date\", \"next_report_date\"]]\n .drop_duplicates().head(8))\n\nHoldings after vintage processing: 89,269 records\n\n\n\n\n\n\n\n\n\nfund_id\nreport_date\nfiling_date\nnext_report_date\n\n\n\n\n0\nFUND001\n2012-11-30\n2012-12-30\n2013-02-28\n\n\n52\nFUND001\n2013-02-28\n2013-03-28\n2013-05-31\n\n\n104\nFUND001\n2013-05-31\n2013-08-31\n2013-08-31\n\n\n156\nFUND001\n2013-08-31\n2013-11-30\n2013-11-30\n\n\n208\nFUND001\n2013-11-30\n2014-02-28\n2014-02-28\n\n\n260\nFUND001\n2014-02-28\n2014-04-28\n2014-05-31\n\n\n312\nFUND001\n2014-05-31\n2014-08-31\n2014-08-31\n\n\n364\nFUND001\n2014-08-31\n2014-10-31\n2014-11-30",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gapadjust-shares",
    "href": "54_return_gaps.html#sec-return-gapadjust-shares",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "39.2 Step 2: Adjust Shares for Corporate Actions",
    "text": "39.2 Step 2: Adjust Shares for Corporate Actions\n\ndef adjust_holdings_shares(holdings, stock_data):\n    \"\"\"Adjust shares for splits, bonuses, rights. Simulated: factor=1.\"\"\"\n    holdings[\"shares_adj\"] = holdings[\"shares_held\"]\n    return holdings\n\nholdings_adj = adjust_holdings_shares(holdings_vintaged, stock_data)",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gaphypothetical-returns",
    "href": "54_return_gaps.html#sec-return-gaphypothetical-returns",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "39.3 Step 3: Compute Hypothetical Holdings Returns",
    "text": "39.3 Step 3: Compute Hypothetical Holdings Returns\nThis is the core computation. For each fund, we take the disclosed holdings as of report date \\(\\tau\\), and for each month \\(t\\) in \\((\\tau, \\tau_{\\text{next}}]\\), compute the value-weighted return using lagged dollar values as weights.\n\ndef compute_holdings_returns(holdings, stock_data, min_stocks=10, min_assets_bn=5.0):\n    \"\"\"Compute monthly hypothetical buy-and-hold portfolio returns.\"\"\"\n    merged = holdings.merge(stock_data, on=\"ticker\", how=\"inner\")\n    mask = (merged[\"date\"] &gt; merged[\"report_date\"]) & (merged[\"date\"] &lt;= merged[\"next_report_date\"])\n    merged = merged.loc[mask].copy()\n    merged[\"hvalue_lag\"] = merged[\"shares_adj\"] * merged[\"close_adj_lag\"]\n    merged = merged.loc[merged[\"hvalue_lag\"] &gt; 0].copy()\n    merged = merged.drop_duplicates(subset=[\"fund_id\", \"date\", \"report_date\", \"ticker\"], keep=\"first\")\n\n    def weighted_return(group):\n        weights = group[\"hvalue_lag\"]\n        total_weight = weights.sum()\n        if total_weight &lt;= 0:\n            return pd.Series({\"hret\": np.nan, \"n_stocks\": 0, \"assets_lag_bn\": 0})\n        wret = np.average(group[\"ret\"], weights=weights)\n        return pd.Series({\"hret\": wret, \"n_stocks\": len(group), \"assets_lag_bn\": total_weight / 1e9})\n\n    portfolio_returns = (\n        merged.groupby([\"fund_id\", \"date\"])\n        .apply(weighted_return, include_groups=False).reset_index()\n    )\n    portfolio_returns[\"assets_bn\"] = portfolio_returns[\"assets_lag_bn\"] * (1 + portfolio_returns[\"hret\"])\n    mask = (portfolio_returns[\"n_stocks\"] &gt;= min_stocks) & (portfolio_returns[\"assets_bn\"] &gt;= min_assets_bn)\n    return portfolio_returns.loc[mask].copy()\n\nholdings_returns = compute_holdings_returns(holdings_adj, stock_data)\nprint(f\"Fund-month observations (hypothetical returns): {holdings_returns.shape[0]:,}\")\nprint(f\"Unique funds: {holdings_returns['fund_id'].nunique()}\")\nprint(f\"\\nSummary:\")\nprint(holdings_returns[[\"hret\", \"n_stocks\", \"assets_bn\"]].describe().round(4).to_string())\n\nFund-month observations (hypothetical returns): 6,170\nUnique funds: 50\n\nSummary:\n            hret   n_stocks  assets_bn\ncount  6170.0000  6170.0000  6170.0000\nmean      0.0149    42.7476    27.8827\nstd       0.0426    14.5061    20.1749\nmin      -0.1997    13.0000     5.0241\n25%      -0.0111    31.0000    13.3775\n50%       0.0146    41.0000    22.6388\n75%       0.0411    53.0000    35.6065\nmax       0.2209    74.0000   167.3511",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gapgross-returns",
    "href": "54_return_gaps.html#sec-return-gapgross-returns",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "39.4 Step 4: Compute Gross Fund Returns",
    "text": "39.4 Step 4: Compute Gross Fund Returns\n\ndef prepare_fund_returns(fund_returns, equity_fund_ids):\n    \"\"\"Prepare fund-level gross returns.\"\"\"\n    df = fund_returns.loc[fund_returns[\"fund_id\"].isin(equity_fund_ids)].copy()\n    df[\"expense_ratio\"] = df[\"expense_ratio\"].fillna(df.groupby(\"fund_id\")[\"expense_ratio\"].transform(\"median\"))\n    df[\"gross_return\"] = df[\"net_return\"] + df[\"expense_ratio\"] / 12\n    df = df.sort_values([\"fund_id\", \"date\"])\n    df[\"tna_lag\"] = df.groupby(\"fund_id\")[\"tna\"].shift(1).fillna(df[\"tna\"])\n    return df\n\nfund_ret_clean = prepare_fund_returns(fund_returns, equity_fund_ids)\nprint(f\"Equity fund-months: {fund_ret_clean.shape[0]:,}\")\n\nEquity fund-months: 3,993",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gapmerge-return-gap",
    "href": "54_return_gaps.html#sec-return-gapmerge-return-gap",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "39.5 Step 5: Merge and Compute Return Gap",
    "text": "39.5 Step 5: Merge and Compute Return Gap\n\ndef compute_return_gap(holdings_returns, fund_returns):\n    \"\"\"Compute Return Gap and trailing averages.\"\"\"\n    merged = holdings_returns.merge(\n        fund_returns[[\"fund_id\", \"date\", \"net_return\", \"gross_return\", \"expense_ratio\", \"tna\"]],\n        on=[\"fund_id\", \"date\"], how=\"inner\",\n    )\n    merged[\"return_gap\"] = merged[\"gross_return\"] - merged[\"hret\"]\n    merged = merged.sort_values([\"fund_id\", \"date\"])\n    merged[\"rg_12m\"] = merged.groupby(\"fund_id\")[\"return_gap\"].transform(\n        lambda x: x.rolling(12, min_periods=8).mean()\n    )\n    merged[\"rg_12m_lag4\"] = merged.groupby(\"fund_id\")[\"rg_12m\"].shift(4)\n    return merged\n\nreturn_gap_data = compute_return_gap(holdings_returns, fund_ret_clean)\nprint(f\"Return Gap observations: {return_gap_data.shape[0]:,}\")\nprint(f\"\\nSummary:\")\nprint(return_gap_data[[\"return_gap\", \"rg_12m\", \"rg_12m_lag4\"]].describe().round(6).to_string())\n\nReturn Gap observations: 3,592\n\nSummary:\n        return_gap       rg_12m  rg_12m_lag4\ncount  3592.000000  3389.000000  3273.000000\nmean     -0.005734    -0.005349    -0.005329\nstd       0.080211     0.022955     0.022864\nmin      -0.307136    -0.094187    -0.094187\n25%      -0.058760    -0.019080    -0.019057\n50%      -0.006127    -0.005284    -0.005242\n75%       0.047785     0.008620     0.008653\nmax       0.277137     0.079589     0.079589",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#distribution-of-the-return-gap",
    "href": "54_return_gaps.html#distribution-of-the-return-gap",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "39.6 Distribution of the Return Gap",
    "text": "39.6 Distribution of the Return Gap\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\nrg = return_gap_data[\"return_gap\"].dropna()\nrg_trimmed = rg.clip(rg.quantile(0.01), rg.quantile(0.99))\naxes[0].hist(rg_trimmed, bins=80, density=True, alpha=0.7, color=\"#2C5F8A\", edgecolor=\"white\", linewidth=0.5)\naxes[0].axvline(rg.mean(), color=\"#D32F2F\", linestyle=\"--\", linewidth=2, label=f\"Mean = {rg.mean():.4f}\")\naxes[0].axvline(rg.median(), color=\"#FF8F00\", linestyle=\"-.\", linewidth=2, label=f\"Median = {rg.median():.4f}\")\naxes[0].set_xlabel(\"Monthly Return Gap\")\naxes[0].set_ylabel(\"Density\")\naxes[0].set_title(\"Panel A: Monthly Return Gap\")\naxes[0].legend(frameon=True)\n\nrg12 = return_gap_data[\"rg_12m\"].dropna()\nrg12_trimmed = rg12.clip(rg12.quantile(0.01), rg12.quantile(0.99))\naxes[1].hist(rg12_trimmed, bins=80, density=True, alpha=0.7, color=\"#1B5E20\", edgecolor=\"white\", linewidth=0.5)\naxes[1].axvline(rg12.mean(), color=\"#D32F2F\", linestyle=\"--\", linewidth=2, label=f\"Mean = {rg12.mean():.4f}\")\naxes[1].axvline(rg12.median(), color=\"#FF8F00\", linestyle=\"-.\", linewidth=2, label=f\"Median = {rg12.median():.4f}\")\naxes[1].set_xlabel(\"Trailing 12-Month Average Return Gap\")\naxes[1].set_ylabel(\"Density\")\naxes[1].set_title(\"Panel B: 12-Month Average Return Gap\")\naxes[1].legend(frameon=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 39.1: Distribution of monthly Return Gap across all fund-month observations.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#time-series-of-cross-sectional-return-gap",
    "href": "54_return_gaps.html#time-series-of-cross-sectional-return-gap",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "39.7 Time Series of Cross-Sectional Return Gap",
    "text": "39.7 Time Series of Cross-Sectional Return Gap\n\nts_stats = (\n    return_gap_data.groupby(\"date\")[\"return_gap\"]\n    .agg([\"mean\", \"median\", lambda x: x.quantile(0.25), lambda x: x.quantile(0.75)])\n    .rename(columns={\"&lt;lambda_0&gt;\": \"p25\", \"&lt;lambda_1&gt;\": \"p75\"}).reset_index()\n)\n\nfig, ax = plt.subplots(figsize=(12, 5))\nax.fill_between(ts_stats[\"date\"], ts_stats[\"p25\"], ts_stats[\"p75\"], alpha=0.3, color=\"#2C5F8A\", label=\"IQR\")\nax.plot(ts_stats[\"date\"], ts_stats[\"median\"], color=\"#2C5F8A\", linewidth=2, label=\"Median\")\nax.plot(ts_stats[\"date\"], ts_stats[\"mean\"], color=\"#D32F2F\", linestyle=\"--\", linewidth=1.5, label=\"Mean\")\nax.axhline(0, color=\"black\", linewidth=0.8)\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Monthly Return Gap\")\nax.set_title(\"Cross-Sectional Distribution of Return Gap Over Time\")\nax.legend(frameon=True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 39.2: Time series of cross-sectional Return Gap statistics. Solid: median, shaded: IQR, dashed: mean.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#sec-return-gapdecile-portfolios",
    "href": "54_return_gaps.html#sec-return-gapdecile-portfolios",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "40.1 Forming Return Gap Decile Portfolios",
    "text": "40.1 Forming Return Gap Decile Portfolios\nEach month \\(t\\), we sort funds into decile portfolios based on their lagged 12-month average Return Gap (\\(\\overline{\\text{RG}}_{i,t-4}^{12}\\)). Portfolio 1 contains funds with the lowest Return Gap.\n\ndef form_return_gap_portfolios(data, n_portfolios=10, sort_var=\"rg_12m_lag4\"):\n    \"\"\"Form portfolios by sorting funds into quantile groups.\"\"\"\n    df = data.dropna(subset=[sort_var]).copy()\n    df[\"portfolio\"] = (\n        df.groupby(\"date\")[sort_var]\n        .transform(lambda x: pd.qcut(x, n_portfolios, labels=False, duplicates=\"drop\"))\n    ) + 1\n    return df\n\nn_portfolios = 10\nportfolio_data = form_return_gap_portfolios(return_gap_data, n_portfolios=n_portfolios)\nprint(f\"Observations with portfolio assignment: {portfolio_data.shape[0]:,}\")\n\nObservations with portfolio assignment: 3,273",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#portfolio-returns",
    "href": "54_return_gaps.html#portfolio-returns",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "40.2 Portfolio Returns",
    "text": "40.2 Portfolio Returns\n\ndef compute_portfolio_returns(data, n_portfolios=10):\n    \"\"\"Compute equal- and value-weighted monthly returns.\"\"\"\n    ew = data.groupby([\"date\", \"portfolio\"]).agg(\n        ew_ret=(\"net_return\", \"mean\"), n_funds=(\"fund_id\", \"count\")).reset_index()\n    def vw_func(group):\n        w = group[\"tna\"].clip(lower=0)\n        return np.average(group[\"net_return\"], weights=w) if w.sum() &gt; 0 else group[\"net_return\"].mean()\n    vw = data.groupby([\"date\", \"portfolio\"]).apply(vw_func, include_groups=False).reset_index(name=\"vw_ret\")\n    return ew.merge(vw, on=[\"date\", \"portfolio\"], how=\"left\")\n\nport_returns = compute_portfolio_returns(portfolio_data)\nport_wide = port_returns.pivot_table(index=\"date\", columns=\"portfolio\", values=[\"ew_ret\", \"vw_ret\"])\nport_wide[(\"ew_ret\", \"LS\")] = port_wide[(\"ew_ret\", n_portfolios)] - port_wide[(\"ew_ret\", 1)]\nport_wide[(\"vw_ret\", \"LS\")] = port_wide[(\"vw_ret\", n_portfolios)] - port_wide[(\"vw_ret\", 1)]\n\nprint(\"EW portfolio returns (annualized, %):\")\nprint((port_wide[\"ew_ret\"].mean() * 12 * 100).round(2).to_string())\n\nEW portfolio returns (annualized, %):\nportfolio\n1.0      4.42\n2.0     12.09\n3.0     15.52\n4.0      8.35\n5.0     10.31\n6.0     14.38\n7.0      1.18\n8.0     10.99\n9.0      1.33\n10.0    15.19\nLS      10.77",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#characteristics-of-return-gap-portfolios",
    "href": "54_return_gaps.html#characteristics-of-return-gap-portfolios",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "40.3 Characteristics of Return Gap Portfolios",
    "text": "40.3 Characteristics of Return Gap Portfolios\n\n\n\nTable 40.1: Characteristics of Return Gap-sorted decile portfolios.\n\n\nchars = portfolio_data.groupby(\"portfolio\").agg(\n    avg_rg=(\"return_gap\", \"mean\"), avg_rg12=(\"rg_12m_lag4\", \"mean\"),\n    avg_net_ret=(\"net_return\", \"mean\"), avg_gross_ret=(\"gross_return\", \"mean\"),\n    avg_hret=(\"hret\", \"mean\"), avg_expense=(\"expense_ratio\", \"mean\"),\n    avg_tna=(\"tna\", \"mean\"), avg_nstocks=(\"n_stocks\", \"mean\"), n_obs=(\"fund_id\", \"count\"),\n).round(4)\ndc = chars.copy()\ndc.columns = [\"Avg RG\", \"Avg RG(12m)\", \"Net Ret\", \"Gross Ret\", \"Hold Ret\", \"Expense\", \"TNA(Bn)\", \"#Stocks\", \"#Obs\"]\nfor col in [\"Avg RG\", \"Avg RG(12m)\", \"Net Ret\", \"Gross Ret\", \"Hold Ret\", \"Expense\"]:\n    dc[col] = (dc[col] * 100).round(3)\ndc[\"TNA(Bn)\"] = dc[\"TNA(Bn)\"].round(1)\ndc[\"#Stocks\"] = dc[\"#Stocks\"].round(1)\nprint(dc.to_string())\n\n           Avg RG  Avg RG(12m)  Net Ret  Gross Ret  Hold Ret  Expense  TNA(Bn)  #Stocks  #Obs\nportfolio                                                                                    \n1.0         -0.81        -4.32     0.50       0.66      1.48     1.91   1395.7     41.9   353\n2.0          0.16        -2.73     1.02       1.18      1.02     1.86   1941.3     43.9   333\n3.0          0.06        -1.85     1.27       1.42      1.36     1.79   2042.0     45.5   335\n4.0         -1.11        -1.22     0.51       0.66      1.78     1.81   2052.1     47.6   325\n5.0         -0.57        -0.69     0.61       0.76      1.33     1.83   2010.7     46.8   341\n6.0          0.04        -0.20     1.23       1.39      1.35     1.86   2138.9     44.8   243\n7.0         -1.53         0.22    -0.09       0.07      1.60     1.86   2184.0     44.2   322\n8.0         -0.51         0.85     1.14       1.30      1.82     1.93   2340.3     43.7   338\n9.0         -1.30         1.61     0.09       0.25      1.54     1.87   2552.9     41.9   330\n10.0         0.11         3.17     1.31       1.46      1.35     1.83   2864.7     40.2   351",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#cumulative-returns-of-extreme-portfolios",
    "href": "54_return_gaps.html#cumulative-returns-of-extreme-portfolios",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "40.4 Cumulative Returns of Extreme Portfolios",
    "text": "40.4 Cumulative Returns of Extreme Portfolios\n\ncum_ret = pd.DataFrame(index=port_wide.index)\ncum_ret[\"P1 (Low RG)\"] = (1 + port_wide[(\"ew_ret\", 1)]).cumprod()\ncum_ret[\"P10 (High RG)\"] = (1 + port_wide[(\"ew_ret\", n_portfolios)]).cumprod()\ncum_ret[\"L/S (P10-P1)\"] = (1 + port_wide[(\"ew_ret\", \"LS\")]).cumprod()\n\nfig, ax = plt.subplots(figsize=(12, 6))\ncolors = {\"P1 (Low RG)\": \"#D32F2F\", \"P10 (High RG)\": \"#1B5E20\", \"L/S (P10-P1)\": \"#1565C0\"}\nstyles = {\"P1 (Low RG)\": \"--\", \"P10 (High RG)\": \"-\", \"L/S (P10-P1)\": \"-.\"}\nfor col in cum_ret.columns:\n    ax.plot(cum_ret.index, cum_ret[col], label=col, color=colors[col], linestyle=styles[col], linewidth=2)\nax.axhline(1, color=\"black\", linewidth=0.8, alpha=0.5)\nax.set_xlabel(\"Date\")\nax.set_ylabel(\"Cumulative Return (Growth of 1 VND)\")\nax.set_title(\"Cumulative Performance of Return Gap Portfolios\")\nax.legend(frameon=True, loc=\"upper left\")\nax.set_yscale(\"log\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 40.1: Cumulative returns of Return Gap-sorted portfolios: P10 (highest RG) vs P1 (lowest), and the long-short spread.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#risk-factors",
    "href": "54_return_gaps.html#risk-factors",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "41.1 Risk Factors",
    "text": "41.1 Risk Factors\n\ndef generate_factor_returns(start_date=\"2012-01-01\", end_date=\"2024-12-31\"):\n    \"\"\"Generate simulated Vietnamese market factor returns.\"\"\"\n    dates = pd.date_range(start_date, end_date, freq=\"ME\")\n    n = len(dates)\n    return pd.DataFrame({\n        \"date\": dates,\n        \"rf\": np.random.normal(0.004, 0.001, n).clip(0.001, 0.008),\n        \"mkt_rf\": np.random.normal(0.008, 0.055, n),\n        \"smb\": np.random.normal(0.003, 0.035, n),\n        \"hml\": np.random.normal(0.002, 0.030, n),\n        \"umd\": np.random.normal(0.005, 0.045, n),\n        \"rmw\": np.random.normal(0.002, 0.025, n),\n        \"cma\": np.random.normal(0.001, 0.020, n),\n    })\n\nfactors = generate_factor_returns()\nprint(\"Factor summary (monthly %):\")\nprint((factors.drop(columns=\"date\").describe() * 100).round(3).to_string())\n\nFactor summary (monthly %):\n              rf     mkt_rf        smb        hml        umd        rmw        cma\ncount  15600.000  15600.000  15600.000  15600.000  15600.000  15600.000  15600.000\nmean       0.399      0.789      0.454     -0.073      0.396      0.076     -0.043\nstd        0.099      5.276      3.744      2.899      4.615      2.400      1.940\nmin        0.132    -12.061     -9.021     -6.937    -11.322     -6.592     -5.971\n25%        0.344     -2.929     -1.830     -2.036     -2.440     -1.425     -1.554\n50%        0.397      0.844      0.611     -0.056      0.632      0.033      0.003\n75%        0.464      3.844      2.981      1.976      3.597      1.707      1.317\nmax        0.653     15.678      9.744      8.316     13.215      6.142      4.607",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#alpha-estimation",
    "href": "54_return_gaps.html#alpha-estimation",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "41.2 Alpha Estimation",
    "text": "41.2 Alpha Estimation\n\ndef estimate_portfolio_alphas(port_returns, factors, n_portfolios=10, nw_lags=6):\n    \"\"\"Estimate alphas using multiple factor models.\"\"\"\n    ew_wide = port_returns.pivot_table(index=\"date\", columns=\"portfolio\", values=\"ew_ret\")\n    if n_portfolios in ew_wide.columns and 1 in ew_wide.columns:\n        ew_wide[\"LS\"] = ew_wide[n_portfolios] - ew_wide[1]\n    merged = ew_wide.merge(factors, on=\"date\", how=\"inner\")\n    results = []\n    portfolios = list(range(1, n_portfolios + 1)) + [\"LS\"]\n    for port in portfolios:\n        if port not in merged.columns: continue\n        y_raw = merged[port].dropna()\n        idx = y_raw.index\n        rf = merged.loc[idx, \"rf\"]; mkt = merged.loc[idx, \"mkt_rf\"]\n        smb = merged.loc[idx, \"smb\"]; hml = merged.loc[idx, \"hml\"]\n        umd = merged.loc[idx, \"umd\"]; rmw = merged.loc[idx, \"rmw\"]\n        cma = merged.loc[idx, \"cma\"]\n        y_ex = y_raw - rf\n        models = {\n            \"Raw Mean\": (y_raw, None),\n            \"Excess Return\": (y_raw - rf - mkt, None),\n            \"CAPM\": (y_ex, sm.add_constant(mkt)),\n            \"FF3\": (y_ex, sm.add_constant(pd.concat([mkt, smb, hml], axis=1))),\n            \"Carhart\": (y_ex, sm.add_constant(pd.concat([mkt, smb, hml, umd], axis=1))),\n            \"FF5\": (y_ex, sm.add_constant(pd.concat([mkt, smb, hml, rmw, cma], axis=1))),\n        }\n        for mname, (y, X) in models.items():\n            if X is None:\n                mean_val = y.mean(); se = y.std() / np.sqrt(len(y))\n                t_stat = mean_val / se if se &gt; 0 else np.nan\n                p_val = 2 * (1 - stats.t.cdf(abs(t_stat), len(y)-1))\n                alpha = mean_val\n            else:\n                try:\n                    reg = OLS(y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": nw_lags})\n                    alpha = reg.params.iloc[0]; t_stat = reg.tvalues.iloc[0]; p_val = reg.pvalues.iloc[0]\n                except: alpha, t_stat, p_val = np.nan, np.nan, np.nan\n            results.append({\"portfolio\": port, \"model\": mname, \"alpha\": alpha, \"t_stat\": t_stat, \"p_value\": p_val})\n    return pd.DataFrame(results)\n\nalpha_results = estimate_portfolio_alphas(port_returns, factors, n_portfolios)\nprint(f\"Alpha estimates: {len(alpha_results)}\")\n\nAlpha estimates: 66",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#alpha-table",
    "href": "54_return_gaps.html#alpha-table",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "41.3 Alpha Table",
    "text": "41.3 Alpha Table\n\n\n\nTable 41.1: Risk-adjusted monthly alphas (%) for Return Gap decile portfolios. t-stats (NW, 6 lags) in parentheses.\n\n\ndef stars(p):\n    if pd.isna(p): return \"\"\n    if p &lt; 0.01: return \"***\"\n    if p &lt; 0.05: return \"**\"\n    if p &lt; 0.10: return \"*\"\n    return \"\"\n\nmodels_list = [\"Raw Mean\", \"Excess Return\", \"CAPM\", \"FF3\", \"Carhart\", \"FF5\"]\nrows = []\nfor model in models_list:\n    md = alpha_results.loc[alpha_results[\"model\"] == model]\n    for _, row in md.iterrows():\n        a = row[\"alpha\"] * 100\n        rows.append({\"Portfolio\": row[\"portfolio\"], \"Model\": model,\n                     \"Alpha (%)\": f\"{a:.3f}{stars(row['p_value'])}\", \"t-stat\": f\"({row['t_stat']:.2f})\"})\npivot = pd.DataFrame(rows).pivot_table(index=\"Portfolio\", columns=\"Model\", values=\"Alpha (%)\", aggfunc=\"first\")\npivot = pivot.reindex(columns=models_list)\nprint(pivot.to_string())\n\nModel      Raw Mean Excess Return      CAPM       FF3   Carhart       FF5\nPortfolio                                                                \n1             0.368        -0.817    -0.126    -0.098    -0.183    -0.058\n2           1.007**        -0.266     0.596     0.518     0.251     0.561\n3          1.293***         0.017  0.902***  0.956***  0.995***  0.925***\n4            0.696*        -0.577     0.345     0.344     0.270     0.283\n5           0.859**        -0.347     0.615     0.604    0.721*     0.590\n6          1.198***        -0.036    0.779*   0.837**    0.819*    0.782*\n7             0.099       -1.128*    -0.274    -0.231    -0.119    -0.203\n8           0.916**        -0.405     0.515     0.566     0.603     0.555\n9             0.111       -1.116*    -0.357    -0.320    -0.318    -0.345\n10         1.266***         0.080   0.765**    0.699*    0.784*    0.733*\nLS            0.897        -0.289     0.492     0.398     0.567     0.392",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#alpha-plot",
    "href": "54_return_gaps.html#alpha-plot",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "41.4 Alpha Plot",
    "text": "41.4 Alpha Plot\n\nmodels_plot = [\"Excess Return\", \"CAPM\", \"FF3\", \"Carhart\", \"FF5\"]\ncolors_m = {\"Excess Return\": \"#D32F2F\", \"CAPM\": \"#FF8F00\", \"FF3\": \"#1B5E20\", \"Carhart\": \"#1565C0\", \"FF5\": \"#6A1B9A\"}\n\nfig, ax = plt.subplots(figsize=(12, 7))\nfor model in models_plot:\n    md = alpha_results.loc[(alpha_results[\"model\"]==model) & (alpha_results[\"portfolio\"]!=\"LS\")].sort_values(\"portfolio\")\n    ax.plot(md[\"portfolio\"], md[\"alpha\"]*100, marker=\"o\", linewidth=2.5, markersize=8, label=model, color=colors_m[model])\nax.axhline(0, color=\"black\", linewidth=0.8, alpha=0.5)\nax.set_xlabel(\"Return Gap Portfolio (1=Lowest, 10=Highest)\")\nax.set_ylabel(\"Monthly Alpha (%)\")\nax.set_title(\"Abnormal Returns of Return Gap-Sorted Portfolios\\nVietnamese Domestic Equity Funds\")\nax.legend(frameon=True, title=\"Risk Model\")\nax.set_xticks(range(1, 11))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 41.1: Risk-adjusted alphas of Return Gap-sorted decile portfolios under different factor models.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#long-short-portfolio-analysis",
    "href": "54_return_gaps.html#long-short-portfolio-analysis",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "41.5 Long-Short Portfolio Analysis",
    "text": "41.5 Long-Short Portfolio Analysis\n\n\n\nTable 41.2: Performance of the long-short Return Gap strategy (P10 minus P1).\n\n\ndef long_short_analysis(port_returns, factors, n_portfolios=10):\n    wide = port_returns.pivot_table(index=\"date\", columns=\"portfolio\", values=\"ew_ret\")\n    ls = (wide[n_portfolios] - wide[1]).dropna()\n    merged = pd.DataFrame({\"ls_ret\": ls}).merge(factors, on=\"date\", how=\"inner\")\n    ann_ret = ls.mean() * 12; ann_vol = ls.std() * np.sqrt(12)\n    sharpe = ann_ret / ann_vol if ann_vol &gt; 0 else np.nan\n    cum = (1 + ls).cumprod(); max_dd = (cum / cum.cummax() - 1).min()\n    sd = {\"Ann. Return (%)\": ann_ret*100, \"Ann. Volatility (%)\": ann_vol*100, \"Sharpe Ratio\": sharpe,\n          \"Max Drawdown (%)\": max_dd*100, \"% Positive Months\": (ls&gt;0).mean()*100}\n    y = merged[\"ls_ret\"] - merged[\"rf\"]\n    for mn, fc in {\"CAPM\":[\"mkt_rf\"],\"FF3\":[\"mkt_rf\",\"smb\",\"hml\"],\"Carhart\":[\"mkt_rf\",\"smb\",\"hml\",\"umd\"],\n                    \"FF5\":[\"mkt_rf\",\"smb\",\"hml\",\"rmw\",\"cma\"]}.items():\n        X = sm.add_constant(merged[fc])\n        reg = OLS(y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n        sd[f\"{mn} Alpha (%ann.)\"] = reg.params.iloc[0]*12*100\n        sd[f\"{mn} t-stat\"] = reg.tvalues.iloc[0]\n    return pd.DataFrame(list(sd.items()), columns=[\"Statistic\", \"Value\"]).round(3)\n\nprint(long_short_analysis(port_returns, factors).to_string(index=False))\n\n            Statistic   Value\n      Ann. Return (%)  10.766\n  Ann. Volatility (%)  21.431\n         Sharpe Ratio   0.502\n     Max Drawdown (%) -35.849\n    % Positive Months  58.400\n   CAPM Alpha (%ann.)   5.905\n          CAPM t-stat   1.062\n    FF3 Alpha (%ann.)   4.777\n           FF3 t-stat   0.781\nCarhart Alpha (%ann.)   6.810\n       Carhart t-stat   1.112\n    FF5 Alpha (%ann.)   4.699\n           FF5 t-stat   0.780",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#fama-macbeth-regressions",
    "href": "54_return_gaps.html#fama-macbeth-regressions",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "42.1 Fama-MacBeth Regressions",
    "text": "42.1 Fama-MacBeth Regressions\n\\[\n\\text{RG}_{i,t} = \\gamma_0 + \\gamma_1 \\text{Size}_{i,t-1} + \\gamma_2 \\text{Expense}_{i,t} + \\gamma_3 \\text{NStocks}_{i,t} + \\epsilon_{i,t}\n\\tag{42.1}\\]\n\ndef fama_macbeth_regression(data, y_var, x_vars, nw_lags=6):\n    \"\"\"Fama-MacBeth (1973) regression with Newey-West SEs.\"\"\"\n    dates = sorted(data[\"date\"].unique())\n    all_coefs = []\n    for d in dates:\n        cross = data.loc[data[\"date\"]==d, [y_var]+x_vars].dropna()\n        if len(cross) &lt; 10: continue\n        try:\n            reg = OLS(cross[y_var], sm.add_constant(cross[x_vars])).fit()\n            coefs = reg.params.to_dict(); coefs[\"date\"] = d; all_coefs.append(coefs)\n        except: continue\n    coef_df = pd.DataFrame(all_coefs).set_index(\"date\")\n    results = []\n    for col in [\"const\"] + x_vars:\n        s = coef_df[col].dropna(); mean = s.mean(); T = len(s)\n        g = s - mean; v0 = (g**2).mean()\n        for lag in range(1, nw_lags+1):\n            w = 1 - lag/(nw_lags+1)\n            v0 += 2*w*(g.iloc[lag:].values*g.iloc[:-lag].values).mean()\n        se = np.sqrt(v0/T); t = mean/se if se&gt;0 else np.nan\n        results.append({\"Variable\": col, \"Coeff\": f\"{mean:.6f}\", \"NW SE\": f\"{se:.6f}\",\n                        \"t-stat\": f\"{t:.3f}\", \"p-value\": f\"{2*(1-stats.t.cdf(abs(t),T-1)):.4f}\"})\n    return pd.DataFrame(results)\n\nreg_data = return_gap_data.copy()\nreg_data[\"log_tna\"] = np.log(reg_data[\"tna\"].clip(lower=1))\nreg_data[\"log_nstocks\"] = np.log(reg_data[\"n_stocks\"].clip(lower=1))\nreg_data[\"expense_pct\"] = reg_data[\"expense_ratio\"] * 100\n\nprint(\"Fama-MacBeth: Determinants of Return Gap\")\nprint(\"=\" * 60)\nprint(fama_macbeth_regression(reg_data, \"return_gap\", [\"log_tna\",\"expense_pct\",\"log_nstocks\"]).to_string(index=False))\n\nFama-MacBeth: Determinants of Return Gap\n============================================================\n   Variable     Coeff    NW SE t-stat p-value\n      const -0.061641 0.018159 -3.395  0.0009\n    log_tna  0.009955 0.001553  6.410  0.0000\nexpense_pct -0.003321 0.002839 -1.170  0.2443\nlog_nstocks -0.003203 0.003508 -0.913  0.3629",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#alternative-holding-periods",
    "href": "54_return_gaps.html#alternative-holding-periods",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "44.1 Alternative Holding Periods",
    "text": "44.1 Alternative Holding Periods\n\n\n\nTable 44.1: Long-short strategy under different maximum holding periods.\n\n\nhp_results = {}\nfor hp in [3, 6, 9]:\n    hv = prepare_holdings_vintages(holdings_raw, max_holding_months=hp)\n    ha = adjust_holdings_shares(hv, stock_data)\n    hr = compute_holdings_returns(ha, stock_data)\n    rg = compute_return_gap(hr, fund_ret_clean)\n    pd_data = form_return_gap_portfolios(rg)\n    pr = compute_portfolio_returns(pd_data)\n    pw = pr.pivot_table(index=\"date\", columns=\"portfolio\", values=\"ew_ret\")\n    if 10 in pw.columns and 1 in pw.columns:\n        ls = pw[10] - pw[1]\n        hp_results[hp] = {\"Ann.Ret(%)\": ls.mean()*12*100,\n            \"Sharpe\": ls.mean()/ls.std()*np.sqrt(12) if ls.std()&gt;0 else np.nan, \"#Months\": len(ls.dropna())}\n\nprint(pd.DataFrame(hp_results).T.round(3).to_string())\n\n   Ann.Ret(%)  Sharpe  #Months\n3       4.601   0.200    125.0\n6      10.766   0.502    125.0\n9      10.766   0.502    125.0",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#subperiod-analysis",
    "href": "54_return_gaps.html#subperiod-analysis",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "44.2 Subperiod Analysis",
    "text": "44.2 Subperiod Analysis\n\n\n\nTable 44.2: Subperiod analysis of the long-short strategy.\n\n\nwide = port_returns.pivot_table(index=\"date\", columns=\"portfolio\", values=\"ew_ret\")\nls = (wide[n_portfolios] - wide[1]).dropna()\nds = sorted(ls.index); n = len(ds); bp1 = ds[n//3]; bp2 = ds[2*n//3]\nperiods = {\"Full\": (ls.index.min(), ls.index.max()),\n           f\"Early-{bp1:%Y}\": (ls.index.min(), bp1),\n           f\"{bp1:%Y}-{bp2:%Y}\": (bp1, bp2),\n           f\"{bp2:%Y}-Late\": (bp2, ls.index.max())}\nsub = []\nfor pn, (s, e) in periods.items():\n    ss = ls.loc[(ls.index&gt;=s)&(ls.index&lt;=e)]\n    if len(ss)&lt;12: continue\n    sub.append({\"Period\": pn, \"Ann.Ret(%)\": ss.mean()*12*100, \"Ann.Vol(%)\": ss.std()*np.sqrt(12)*100,\n                \"Sharpe\": ss.mean()/ss.std()*np.sqrt(12) if ss.std()&gt;0 else np.nan, \"#Mo\": len(ss)})\nprint(pd.DataFrame(sub).round(3).to_string(index=False))\n\n    Period  Ann.Ret(%)  Ann.Vol(%)  Sharpe  #Mo\n      Full      10.766      21.431   0.502  125\nEarly-2018      19.756      24.313   0.813   42\n 2018-2021      -6.730      20.551  -0.327   43\n 2021-Late      18.572      18.506   1.004   42",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#ew-vs-vw",
    "href": "54_return_gaps.html#ew-vs-vw",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "44.3 EW vs VW",
    "text": "44.3 EW vs VW\n\nls_ew = port_wide[(\"ew_ret\", n_portfolios)] - port_wide[(\"ew_ret\", 1)]\nls_vw = port_wide[(\"vw_ret\", n_portfolios)] - port_wide[(\"vw_ret\", 1)]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\naxes[0].plot((1+ls_ew.dropna()).cumprod(), label=\"EW\", color=\"#1565C0\", linewidth=2)\naxes[0].plot((1+ls_vw.dropna()).cumprod(), label=\"VW\", color=\"#D32F2F\", linewidth=2, linestyle=\"--\")\naxes[0].axhline(1, color=\"black\", linewidth=0.5)\naxes[0].set_title(\"Cumulative L/S Returns\"); axes[0].legend()\n\naxes[1].plot(ls_ew.rolling(12).mean()*12*100, label=\"EW\", color=\"#1565C0\", linewidth=2)\naxes[1].plot(ls_vw.rolling(12).mean()*12*100, label=\"VW\", color=\"#D32F2F\", linewidth=2, linestyle=\"--\")\naxes[1].axhline(0, color=\"black\", linewidth=0.5)\naxes[1].set_title(\"Rolling 12M Ann. L/S Returns (%)\"); axes[1].legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 44.1: Equal-weighted vs value-weighted Return Gap long-short strategies.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#extension-1-decomposing-return-gap-by-source",
    "href": "54_return_gaps.html#extension-1-decomposing-return-gap-by-source",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "45.1 Extension 1: Decomposing Return Gap by Source",
    "text": "45.1 Extension 1: Decomposing Return Gap by Source\nFollowing Elton, Gruber, and Blake (2011), we approximate cash-drag and trading components:\n\\[\n\\text{RG}_{i,t} \\approx \\underbrace{(1 - \\omega_t^{\\text{eq}}) \\cdot (r_t^{\\text{cash}} - R_{i,t}^{\\text{Holdings}})}_{\\text{Cash Effect}} + \\underbrace{\\omega_t^{\\text{eq}} \\cdot (R_{i,t}^{\\text{Traded}} - R_{i,t}^{\\text{Holdings}})}_{\\text{Trading Effect}}\n\\tag{45.1}\\]\n\n\n\nTable 45.1: Return Gap decomposition by quintile (monthly %).\n\n\ndf_d = return_gap_data.copy()\nmonthly_cash = 0.05 / 12\ndf_d[\"equity_frac\"] = (df_d[\"assets_bn\"] / df_d[\"tna\"].clip(lower=1)).clip(0, 1)\ndf_d[\"cash_effect\"] = (1 - df_d[\"equity_frac\"]) * (monthly_cash - df_d[\"hret\"])\ndf_d[\"trading_effect\"] = df_d[\"return_gap\"] - df_d[\"cash_effect\"]\ndf_d[\"quintile\"] = df_d.groupby(\"date\")[\"rg_12m_lag4\"].transform(\n    lambda x: pd.qcut(x.dropna(), 5, labels=False, duplicates=\"drop\")+1 if len(x.dropna())&gt;=5 else np.nan)\n\ndecomp = (df_d.groupby(\"quintile\")[[\"return_gap\",\"cash_effect\",\"trading_effect\"]].mean()*100).round(4)\ndecomp.columns = [\"Return Gap (%)\", \"Cash Effect (%)\", \"Trading Effect (%)\"]\nprint(decomp.to_string())\n\n          Return Gap (%)  Cash Effect (%)  Trading Effect (%)\nquintile                                                     \n1.0              -0.3548          -0.7989              0.4441\n2.0              -0.5181          -1.1191              0.6010\n3.0              -0.3266          -0.9025              0.5760\n4.0              -1.0100          -1.2648              0.2548\n5.0              -0.6057          -1.0115              0.4058",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#extension-2-conditional-return-gap",
    "href": "54_return_gaps.html#extension-2-conditional-return-gap",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "45.2 Extension 2: Conditional Return Gap",
    "text": "45.2 Extension 2: Conditional Return Gap\n\nwide2 = port_returns.pivot_table(index=\"date\", columns=\"portfolio\", values=\"ew_ret\")\nls2 = (wide2[n_portfolios] - wide2[1]).dropna()\ncond = pd.DataFrame({\"ls\": ls2}).merge(factors[[\"date\",\"mkt_rf\"]], on=\"date\", how=\"inner\")\ncond[\"bull\"] = cond[\"mkt_rf\"] &gt; 0\ncond[\"vol6\"] = cond[\"mkt_rf\"].rolling(6).std() * np.sqrt(12)\ncond[\"hi_vol\"] = cond[\"vol6\"] &gt; cond[\"vol6\"].median()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nm_a = [cond.loc[cond[\"bull\"],\"ls\"].mean()*12*100, cond.loc[~cond[\"bull\"],\"ls\"].mean()*12*100]\nbars = axes[0].bar([\"Bull\",\"Bear\"], m_a, color=[\"#1B5E20\",\"#D32F2F\"], width=0.5)\naxes[0].axhline(0, color=\"black\", linewidth=0.8)\naxes[0].set_ylabel(\"Ann. L/S Ret (%)\")\naxes[0].set_title(\"Bull vs Bear\")\nfor b, v in zip(bars, m_a):\n    axes[0].text(b.get_x()+b.get_width()/2, v, f\"{v:.2f}%\", ha=\"center\", va=\"bottom\" if v&gt;0 else \"top\", fontweight=\"bold\")\n\ncv = cond.dropna(subset=[\"hi_vol\"])\nm_b = [cv.loc[~cv[\"hi_vol\"],\"ls\"].mean()*12*100, cv.loc[cv[\"hi_vol\"],\"ls\"].mean()*12*100]\nbars2 = axes[1].bar([\"Low Vol\",\"High Vol\"], m_b, color=[\"#1565C0\",\"#FF8F00\"], width=0.5)\naxes[1].axhline(0, color=\"black\", linewidth=0.8)\naxes[1].set_ylabel(\"Ann. L/S Ret (%)\")\naxes[1].set_title(\"Low vs High Volatility\")\nfor b, v in zip(bars2, m_b):\n    axes[1].text(b.get_x()+b.get_width()/2, v, f\"{v:.2f}%\", ha=\"center\", va=\"bottom\" if v&gt;0 else \"top\", fontweight=\"bold\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure 45.1: L/S performance in bull vs bear markets and volatility regimes.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#extension-3-return-gap-and-fund-flows",
    "href": "54_return_gaps.html#extension-3-return-gap-and-fund-flows",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "45.3 Extension 3: Return Gap and Fund Flows",
    "text": "45.3 Extension 3: Return Gap and Fund Flows\nAn important question is whether investors respond to the Return Gap signal:\n\\[\n\\text{Flow}_{i,t+1} = \\delta_0 + \\delta_1 \\overline{\\text{RG}}_{i,t}^{12} + \\delta_2 R_{i,t}^{\\text{Net}} + \\delta_3 \\ln(\\text{TNA}_{i,t}) + \\epsilon_{i,t+1}\n\\tag{45.2}\\]\n\nflow_data = return_gap_data.sort_values([\"fund_id\",\"date\"]).copy()\nflow_data[\"tna_lag\"] = flow_data.groupby(\"fund_id\")[\"tna\"].shift(1)\nflow_data[\"flow\"] = ((flow_data[\"tna\"] - flow_data[\"tna_lag\"]*(1+flow_data[\"net_return\"])) / flow_data[\"tna_lag\"])\nflow_data[\"flow\"] = flow_data[\"flow\"].clip(flow_data[\"flow\"].quantile(0.01), flow_data[\"flow\"].quantile(0.99))\nflow_data[\"log_tna\"] = np.log(flow_data[\"tna\"].clip(lower=1))\nflow_data[\"flow_lead\"] = flow_data.groupby(\"fund_id\")[\"flow\"].shift(-1)\n\nprint(\"Fama-MacBeth: Return Gap and Future Fund Flows\")\nprint(\"=\" * 60)\nprint(fama_macbeth_regression(\n    flow_data.dropna(subset=[\"flow_lead\",\"rg_12m\",\"net_return\"]),\n    \"flow_lead\", [\"rg_12m\",\"net_return\",\"log_tna\"]\n).to_string(index=False))\n\nFama-MacBeth: Return Gap and Future Fund Flows\n============================================================\n  Variable     Coeff    NW SE t-stat p-value\n     const  0.006440 0.003466  1.858  0.0656\n    rg_12m -0.025341 0.013819 -1.834  0.0692\nnet_return  0.007547 0.006567  1.149  0.2527\n   log_tna -0.000863 0.000450 -1.917  0.0576",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#extension-4-return-gap-and-stock-selection-skill",
    "href": "54_return_gaps.html#extension-4-return-gap-and-stock-selection-skill",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "45.4 Extension 4: Return Gap and Stock Selection Skill",
    "text": "45.4 Extension 4: Return Gap and Stock Selection Skill\nDoes Return Gap predict future stock-picking ability? We test using characteristic-adjusted selectivity:\n\\[\n\\text{CS}_{i,t} = \\sum_{j=1}^{N} w_{j,t-1}\\left(r_{j,t} - r_{t}^{\\text{bench}(j)}\\right)\n\\tag{45.3}\\]\n\n\n\nTable 45.2: Characteristic selectivity by Return Gap quintile.\n\n\ncs_data = return_gap_data[[\"fund_id\",\"date\",\"rg_12m_lag4\"]].dropna(subset=[\"rg_12m_lag4\"]).copy()\ncs_data[\"cs_score\"] = 0.3 * cs_data[\"rg_12m_lag4\"] + np.random.normal(0, 0.005, len(cs_data))\ncs_data[\"rg_q\"] = cs_data.groupby(\"date\")[\"rg_12m_lag4\"].transform(\n    lambda x: pd.qcut(x, 5, labels=False, duplicates=\"drop\")+1)\ncs_by_q = cs_data.groupby(\"rg_q\")[\"cs_score\"].agg([\"mean\",\"std\",\"count\"])\ncs_by_q[\"t\"] = cs_by_q[\"mean\"] / (cs_by_q[\"std\"] / np.sqrt(cs_by_q[\"count\"]))\ncs_by_q[\"Mean CS (%)\"] = (cs_by_q[\"mean\"]*100).round(4)\nprint(cs_by_q[[\"Mean CS (%)\",\"t\"]].round(3).to_string())\n\n      Mean CS (%)       t\nrg_q                     \n1.0        -1.090 -44.079\n2.0        -0.475 -23.265\n3.0        -0.111  -4.733\n4.0         0.180   8.615\n5.0         0.748  29.016",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#extension-5-double-sorts",
    "href": "54_return_gaps.html#extension-5-double-sorts",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "45.5 Extension 5: Double Sorts",
    "text": "45.5 Extension 5: Double Sorts\n\n\n\nTable 45.3: Double sort: Fund Size (rows) x Return Gap (columns). Monthly net returns (%).\n\n\nds = return_gap_data.copy()\nds[\"log_tna\"] = np.log(ds[\"tna\"].clip(lower=1))\nds2 = ds.dropna(subset=[\"log_tna\",\"rg_12m_lag4\"]).copy()\nfor s, name in [(\"log_tna\",\"g1\"),(\"rg_12m_lag4\",\"g2\")]:\n    ds2[name] = ds2.groupby(\"date\")[s].transform(\n        lambda x: pd.qcut(x, 3, labels=False, duplicates=\"drop\")+1)\nresult = (ds2.groupby([\"g1\",\"g2\"])[\"net_return\"].mean()*100).unstack().round(3)\nresult.index.name = \"Size Tercile\"; result.columns.name = \"RG Tercile\"\nprint(result.to_string())\n\nRG Tercile      1.0    2.0    3.0\nSize Tercile                     \n1.0           0.254  0.250 -0.773\n2.0           1.206  0.551  0.745\n3.0           1.708  1.051  1.494",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#institutional-features-affecting-return-gap",
    "href": "54_return_gaps.html#institutional-features-affecting-return-gap",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "46.1 Institutional Features Affecting Return Gap",
    "text": "46.1 Institutional Features Affecting Return Gap\nSeveral institutional features of the Vietnamese market require special attention when interpreting Return Gap:\n\n46.1.1 Foreign Ownership Limits (FOL)\nVietnamese regulations impose foreign ownership limits on listed companies (typically 49% for most sectors, with lower limits in banking and media). When a stock approaches its FOL, it trades at a premium through “pre-funded” transactions. A fund manager who anticipates FOL-driven price movements through interim trading may generate positive Return Gap.\n\n\n46.1.2 Daily Price Limits\nHOSE imposes plus or minus 7% daily price limits, and HNX plus or minus 10%. These limits can prevent full price discovery within a single day, creating opportunities for informed interim trading over multi-day horizons.\n\n\n46.1.3 T+2 Settlement and Margin Trading\nVietnam’s T+2 settlement cycle and the evolving margin trading framework affect the speed and leverage with which fund managers can execute interim trades.\n\n\n46.1.4 Disclosure Norms\nVietnamese fund disclosure norms differ from the U.S. quarterly mandate. The SSC requires periodic reports, but detailed position-level disclosure may be less frequent, expanding the window for unobserved actions.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "54_return_gaps.html#comparison-with-developed-market-evidence",
    "href": "54_return_gaps.html#comparison-with-developed-market-evidence",
    "title": "36  Return Gap: Measuring Unobserved Actions of Fund Managers",
    "section": "46.2 Comparison with Developed Market Evidence",
    "text": "46.2 Comparison with Developed Market Evidence\nTable 46.1 shows a comparison between developed and emerging markets.\n\n\n\nTable 46.1: Comparison of Return Gap context between developed and Vietnamese markets\n\n\n\n\n\n\n\n\n\n\nDimension\nDeveloped Markets (U.S.)\nVietnamese Market\n\n\n\n\nDisclosure frequency\nQuarterly (mandatory)\nSemi-annual to quarterly\n\n\nReporting lag\n~60 days\nVariable, potentially longer\n\n\nMarket efficiency\nHigh\nModerate/emerging\n\n\nAnalyst coverage\nDense\nSparse\n\n\nPrice limits\nNone\nPlus or minus 7% (HOSE), plus or minus 10% (HNX)\n\n\nForeign ownership\nGenerally unrestricted\nCapped (49% typical)\n\n\nSecurities lending\nMature market\nLimited/nascent\n\n\nExpected RG magnitude\nSmaller\nPotentially larger\n\n\nExpected RG persistence\nModerate\nPotentially higher",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Return Gap: Measuring Unobserved Actions of Fund Managers</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html",
    "href": "55_price_limits_and_volatility.html",
    "title": "37  Price Limits and Volatility",
    "section": "",
    "text": "37.1 The Vietnamese Price Limit Regime\nVietnam is one of a handful of active equity markets that still enforce daily price limits on individual stocks. HOSE imposes a \\(\\pm\\) 7% limit, HNX imposes \\(\\pm\\) 10%, and UPCoM imposes \\(\\pm\\) 15%, each measured relative to the prior day’s closing (or reference) price. When a stock’s equilibrium price change exceeds the limit, the observed return is censored at the boundary. The stock closes at the limit price, but the unobserved “true” return—the price change that would have occurred without the constraint—remains unknown.\nThis censoring has pervasive consequences for empirical finance. Return distributions are truncated, biasing mean and variance estimates. Volatility models that ignore censoring understate true risk. Factor betas are attenuated. Event study abnormal returns are compressed. Bid-ask spread estimators that rely on return serial correlation are distorted. Any researcher working with Vietnamese equity data must understand these effects and either correct for them or demonstrate that they do not materially affect conclusions.\nPrice limits exist for a stated policy purpose: to prevent panic selling and speculative excess, thereby “cooling” the market during periods of stress (Brennan 1986). Whether they achieve this objective—or merely delay price discovery and create magnet effects—is an empirical question with a large international literature and no consensus. We examine the Vietnamese evidence.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-regime",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-regime",
    "title": "37  Price Limits and Volatility",
    "section": "",
    "text": "37.1.1 Institutional Details\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats, optimize\nfrom arch import arch_model\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\nThe price limit structure has evolved over time. HOSE began trading in July 2000 with a \\(\\pm\\) 2% limit, which was widened to \\(\\pm\\) 5% in 2002 and to \\(\\pm\\) 7% in 2013. HNX has operated at \\(\\pm\\) 10% since its current form, and UPCoM at \\(\\pm\\) 15% Table 37.1. The limits apply to the adjusted closing price relative to the reference price (typically the prior day’s close, adjusted for corporate actions).\n\n\n\nTable 37.1: Vietnamese daily price limit regime by exchange.\n\n\n\n\n\n\n\n\n\n\n\nExchange\nCurrent Limit\nEffective Date\nPrior Limits\n\n\n\n\nHOSE\n\\(\\pm\\) 7%\nJune 2013\n\\(\\pm\\) 2% (2000), \\(\\pm\\) 5% (2002)\n\n\nHNX\n\\(\\pm\\) 10%\n—\nVarious, stabilized at \\(\\pm\\) 10%\n\n\nUPCoM\n\\(\\pm\\) 15%\n—\nWider limits reflecting OTC nature\n\n\n\n\n\n\nImportantly, the limits are asymmetric in practice: they apply equally to up and down moves, but the economic consequences differ. A stock hitting the upper limit prevents buyers from bidding higher (excess demand persists), while hitting the lower limit prevents sellers from offering lower (excess supply persists). Both create unfilled orders that spill over to subsequent trading days.\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Daily data with high, low, open, close, volume, and limit indicators\ndaily = client.get_daily_prices(\n    exchanges=['HOSE', 'HNX', 'UPCoM'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    include_delisted=True,\n    fields=[\n        'ticker', 'date', 'exchange',\n        'open', 'high', 'low', 'close', 'adjusted_close',\n        'reference_price', 'ceiling_price', 'floor_price',\n        'volume', 'turnover_value',\n        'limit_up_hit', 'limit_down_hit'\n    ]\n)\n\ndaily['date'] = pd.to_datetime(daily['date'])\ndaily = daily.sort_values(['ticker', 'date'])\n\n# Compute daily returns\ndaily['daily_return'] = daily.groupby('ticker')['adjusted_close'].pct_change()\n\n# Flag limit hits from price data if not provided\nif 'limit_up_hit' not in daily.columns or daily['limit_up_hit'].isna().all():\n    daily['limit_up_hit'] = (daily['close'] &gt;= daily['ceiling_price'])\n    daily['limit_down_hit'] = (daily['close'] &lt;= daily['floor_price'])\n\n# Exchange-specific limits\nexchange_limits = {'HOSE': 0.07, 'HNX': 0.10, 'UPCoM': 0.15}\ndaily['limit_pct'] = daily['exchange'].map(exchange_limits)\n\nprint(f\"Daily observations: {len(daily):,}\")\nprint(f\"Date range: {daily['date'].min()} to {daily['date'].max()}\")\nprint(f\"Unique tickers: {daily['ticker'].nunique()}\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-prevalence",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-prevalence",
    "title": "37  Price Limits and Volatility",
    "section": "37.2 Prevalence of Limit Hits",
    "text": "37.2 Prevalence of Limit Hits\n\n37.2.1 Aggregate Frequency\nHow often do Vietnamese stocks hit their price limits? The answer varies dramatically by exchange, market capitalization, and market conditions.\n\n# Overall frequencies\nlimit_stats = daily.groupby('exchange').agg(\n    n_obs=('daily_return', 'count'),\n    n_up=('limit_up_hit', 'sum'),\n    n_down=('limit_down_hit', 'sum'),\n).assign(\n    pct_up=lambda x: x['n_up'] / x['n_obs'] * 100,\n    pct_down=lambda x: x['n_down'] / x['n_obs'] * 100,\n    pct_either=lambda x: (x['n_up'] + x['n_down']) / x['n_obs'] * 100\n)\n\nprint(\"Limit Hit Frequencies by Exchange:\")\nprint(limit_stats[['pct_up', 'pct_down', 'pct_either']].round(2).to_string())\n\n# Monthly aggregate: fraction of stock-days hitting limits\ndaily['year_month'] = daily['date'].dt.to_period('M')\nmonthly_limit = (\n    daily.groupby(['year_month', 'exchange'])\n    .agg(\n        n_obs=('daily_return', 'count'),\n        n_up=('limit_up_hit', 'sum'),\n        n_down=('limit_down_hit', 'sum')\n    )\n    .assign(\n        pct_up=lambda x: x['n_up'] / x['n_obs'] * 100,\n        pct_down=lambda x: x['n_down'] / x['n_obs'] * 100,\n        pct_any=lambda x: (x['n_up'] + x['n_down']) / x['n_obs'] * 100\n    )\n    .reset_index()\n)\nmonthly_limit['date'] = monthly_limit['year_month'].dt.to_timestamp()\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True,\n                          gridspec_kw={'height_ratios': [3, 1]})\n\nhose_monthly = monthly_limit[monthly_limit['exchange'] == 'HOSE']\n\naxes[0].fill_between(hose_monthly['date'], 0, hose_monthly['pct_up'],\n                      color='#27AE60', alpha=0.6, label='Upper limit hits')\naxes[0].fill_between(hose_monthly['date'], 0, -hose_monthly['pct_down'],\n                      color='#C0392B', alpha=0.6, label='Lower limit hits')\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].set_ylabel('% of Stock-Days')\naxes[0].set_title('Panel A: HOSE Daily Price Limit Hits')\naxes[0].legend(loc='upper left')\n\n# VN-Index for context\nvnindex = client.get_index_returns(\n    index='VNINDEX', start_date='2008-01-01', end_date='2024-12-31',\n    frequency='monthly'\n)\nvnindex['date'] = pd.to_datetime(vnindex['date'])\naxes[1].bar(vnindex['date'], vnindex['return'] * 100,\n            color=np.where(vnindex['return'] &gt; 0, '#27AE60', '#C0392B'),\n            width=25, alpha=0.7)\naxes[1].set_ylabel('VN-Index (%)')\naxes[1].set_title('Panel B: VN-Index Monthly Returns')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 37.1\n\n\n\n\n\n37.2.2 By Market Capitalization\n\n\n\n# Merge with lagged market cap\nmonthly_mcap = client.get_monthly_returns(\n    exchanges=['HOSE'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    fields=['ticker', 'month_end', 'market_cap']\n)\nmonthly_mcap['month_end'] = pd.to_datetime(monthly_mcap['month_end'])\n\n# Assign size quintiles each month\nmonthly_mcap['size_quintile'] = (\n    monthly_mcap.groupby('month_end')['market_cap']\n    .transform(lambda x: pd.qcut(x.rank(method='first'), 5,\n                                   labels=['Q1\\n(Small)', 'Q2', 'Q3', 'Q4',\n                                           'Q5\\n(Big)']))\n)\n\n# Map to daily\ndaily_hose = daily[daily['exchange'] == 'HOSE'].copy()\ndaily_hose['month_end'] = daily_hose['date'].dt.to_period('M').dt.to_timestamp('M')\ndaily_hose = daily_hose.merge(\n    monthly_mcap[['ticker', 'month_end', 'size_quintile']],\n    on=['ticker', 'month_end'], how='left'\n)\n\nsize_limit = (\n    daily_hose.dropna(subset=['size_quintile'])\n    .groupby('size_quintile')\n    .agg(\n        pct_up=('limit_up_hit', 'mean'),\n        pct_down=('limit_down_hit', 'mean'),\n        n=('daily_return', 'count')\n    )\n)\nsize_limit[['pct_up', 'pct_down']] *= 100\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nx = np.arange(len(size_limit))\nwidth = 0.35\nax.bar(x - width / 2, size_limit['pct_up'], width,\n       color='#27AE60', alpha=0.85, label='Upper limit', edgecolor='white')\nax.bar(x + width / 2, size_limit['pct_down'], width,\n       color='#C0392B', alpha=0.85, label='Lower limit', edgecolor='white')\n\nax.set_xticks(x)\nax.set_xticklabels(size_limit.index)\nax.set_ylabel('% of Stock-Days')\nax.set_title('Price Limit Hit Frequency by Size Quintile (HOSE)')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 37.2\n\n\n\n\n\n37.2.3 Consecutive Limit Days\nA single limit hit might simply reflect a large information event that is absorbed within one day. Consecutive limit hits in the same direction are more problematic because they indicate that the limit is actively preventing price discovery over multiple days.\n\ndef count_consecutive_limits(group):\n    \"\"\"Count consecutive limit-up and limit-down sequences.\"\"\"\n    up_runs = []\n    down_runs = []\n    \n    up_count = 0\n    down_count = 0\n    \n    for _, row in group.iterrows():\n        if row['limit_up_hit']:\n            up_count += 1\n            if down_count &gt; 0:\n                down_runs.append(down_count)\n                down_count = 0\n        elif row['limit_down_hit']:\n            down_count += 1\n            if up_count &gt; 0:\n                up_runs.append(up_count)\n                up_count = 0\n        else:\n            if up_count &gt; 0:\n                up_runs.append(up_count)\n            if down_count &gt; 0:\n                down_runs.append(down_count)\n            up_count = 0\n            down_count = 0\n    \n    if up_count &gt; 0:\n        up_runs.append(up_count)\n    if down_count &gt; 0:\n        down_runs.append(down_count)\n    \n    return up_runs, down_runs\n\n# Sample: compute for HOSE stocks\nhose_tickers = daily_hose['ticker'].unique()\nall_up_runs = []\nall_down_runs = []\n\nfor ticker in hose_tickers:\n    group = daily_hose[daily_hose['ticker'] == ticker].sort_values('date')\n    up_runs, down_runs = count_consecutive_limits(group)\n    all_up_runs.extend(up_runs)\n    all_down_runs.extend(down_runs)\n\nprint(\"Consecutive Limit Hit Distribution (HOSE):\")\nfor direction, runs in [('Upper', all_up_runs), ('Lower', all_down_runs)]:\n    if not runs:\n        continue\n    runs_series = pd.Series(runs)\n    print(f\"\\n  {direction} limit sequences:\")\n    print(f\"    Total sequences: {len(runs_series):,}\")\n    print(f\"    1 day:  {(runs_series == 1).sum():,} ({(runs_series == 1).mean():.1%})\")\n    print(f\"    2 days: {(runs_series == 2).sum():,} ({(runs_series == 2).mean():.1%})\")\n    print(f\"    3 days: {(runs_series == 3).sum():,} ({(runs_series == 3).mean():.1%})\")\n    print(f\"    4+ days: {(runs_series &gt;= 4).sum():,} ({(runs_series &gt;= 4).mean():.1%})\")\n    print(f\"    Max consecutive: {runs_series.max()}\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-distortion",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-distortion",
    "title": "37  Price Limits and Volatility",
    "section": "37.3 Return Distribution Distortion",
    "text": "37.3 Return Distribution Distortion\n\n37.3.1 Censoring Mechanics\nPrice limits create Type I censoring (also called “truncation at a known point”): the latent (unobserved) return \\(r^*\\) is generated from some continuous distribution, but the observed return is:\n\\[\nr^{\\text{obs}} = \\begin{cases}\n\\bar{L} & \\text{if } r^* \\geq \\bar{L} \\quad \\text{(upper limit hit)} \\\\\nr^* & \\text{if } \\underline{L} &lt; r^* &lt; \\bar{L} \\quad \\text{(interior)} \\\\\n\\underline{L} & \\text{if } r^* \\leq \\underline{L} \\quad \\text{(lower limit hit)}\n\\end{cases}\n\\tag{37.1}\\]\nwhere \\(\\bar{L}\\) and \\(\\underline{L}\\) are the upper and lower limits. For HOSE, \\(\\bar{L} = +0.07\\) and \\(\\underline{L} = -0.07\\).\nThe censoring has predictable effects on the observed distribution:\n\nMean bias. If the uncensored distribution is symmetric, censoring from both sides preserves the mean approximately. But if the distribution is skewed (as stock returns are, with negative skewness), the bias can go either way.\nVariance underestimation. Censoring always reduces the observed variance relative to the true variance, because extreme returns are compressed to the limit values.\nKurtosis distortion. Probability mass piles up at the limit values, creating spikes in the distribution.\n\n\n\n\nhose_returns = daily_hose['daily_return'].dropna()\nhose_returns = hose_returns[hose_returns.abs() &lt; 0.15]  # Remove data errors\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Full distribution\naxes[0].hist(hose_returns, bins=200, density=True,\n             color='#2C5F8A', alpha=0.7, edgecolor='none')\naxes[0].axvline(x=0.07, color='#C0392B', linewidth=2, linestyle='--',\n                label='$\\pm$ 7% limit')\naxes[0].axvline(x=-0.07, color='#C0392B', linewidth=2, linestyle='--')\naxes[0].set_xlabel('Daily Return')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Panel A: HOSE Daily Return Distribution')\naxes[0].legend()\n\n# Panel B: Zoom on tails\nbins_tail = np.linspace(-0.09, -0.05, 40)\nbins_tail_up = np.linspace(0.05, 0.09, 40)\n\naxes[1].hist(hose_returns[hose_returns &lt; -0.04], bins=80, density=True,\n             color='#C0392B', alpha=0.6, label='Left tail')\naxes[1].hist(hose_returns[hose_returns &gt; 0.04], bins=80, density=True,\n             color='#27AE60', alpha=0.6, label='Right tail')\naxes[1].axvline(x=0.07, color='black', linewidth=2)\naxes[1].axvline(x=-0.07, color='black', linewidth=2)\naxes[1].set_xlabel('Daily Return')\naxes[1].set_ylabel('Density')\naxes[1].set_title('Panel B: Tail Behavior at Limits')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Quantify the spike\nn_at_upper = ((hose_returns &gt;= 0.069) & (hose_returns &lt;= 0.071)).sum()\nn_at_lower = ((hose_returns &gt;= -0.071) & (hose_returns &lt;= -0.069)).sum()\nn_total = len(hose_returns)\nprint(f\"Observations at upper limit ($\\pm$ 0.1% of 7%): {n_at_upper:,} \"\n      f\"({n_at_upper/n_total:.2%})\")\nprint(f\"Observations at lower limit: {n_at_lower:,} \"\n      f\"({n_at_lower/n_total:.2%})\")\n\n\nFigure 37.3\n\n\n\n\n\n37.3.2 Comparing HOSE vs. HNX vs. UPCoM\nThe three Vietnamese exchanges have different limit widths, creating a natural experiment: if limits distort the distribution, wider limits should produce distributions closer to the uncensored benchmark.\n\n\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n\nfor i, (exchange, limit, color) in enumerate([\n    ('HOSE', 0.07, '#2C5F8A'),\n    ('HNX', 0.10, '#C0392B'),\n    ('UPCoM', 0.15, '#27AE60')\n]):\n    rets = daily[daily['exchange'] == exchange]['daily_return'].dropna()\n    rets = rets[rets.abs() &lt; limit + 0.05]\n    \n    axes[i].hist(rets, bins=150, density=True,\n                  color=color, alpha=0.7, edgecolor='none')\n    axes[i].axvline(x=limit, color='black', linewidth=1.5, linestyle='--')\n    axes[i].axvline(x=-limit, color='black', linewidth=1.5, linestyle='--')\n    axes[i].set_title(f'{exchange} ($\\pm$ {limit*100:.0f}%)')\n    axes[i].set_xlabel('Daily Return')\n    if i == 0:\n        axes[i].set_ylabel('Density')\n    \n    # Stats\n    pct_at_limit = ((rets.abs() &gt;= limit - 0.001).sum() / len(rets) * 100)\n    axes[i].text(0.95, 0.95, f'At limit: {pct_at_limit:.2f}%',\n                  transform=axes[i].transAxes, ha='right', va='top',\n                  fontsize=9, bbox=dict(boxstyle='round', facecolor='white',\n                                         alpha=0.8))\n\nplt.suptitle('Return Distributions by Exchange', fontsize=13)\nplt.tight_layout()\nplt.show()\n\n\nFigure 37.4",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-variance-bias",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-variance-bias",
    "title": "37  Price Limits and Volatility",
    "section": "37.4 Variance Bias from Censoring",
    "text": "37.4 Variance Bias from Censoring\n\n37.4.1 Analytical Bias\nIf the true return follows \\(r^* \\sim N(\\mu, \\sigma^2)\\), the variance of the censored return can be derived analytically. Let \\(a = (\\underline{L} - \\mu) / \\sigma\\) and \\(b = (\\bar{L} - \\mu) / \\sigma\\):\n\\[\n\\text{Var}(r^{\\text{obs}}) = \\sigma^2 \\left[1 - \\frac{b \\phi(b) - a \\phi(a)}{\\Phi(b) - \\Phi(a)} - \\left(\\frac{\\phi(a) - \\phi(b)}{\\Phi(b) - \\Phi(a)}\\right)^2 \\right] + \\text{boundary terms}\n\\tag{37.2}\\]\nwhere \\(\\phi\\) and \\(\\Phi\\) are the standard normal PDF and CDF. The key result is that \\(\\text{Var}(r^{\\text{obs}}) &lt; \\sigma^2\\) always—censoring systematically underestimates variance.\n\ndef simulate_censored_variance(true_sigma, limit, n_sim=100000, mu=0):\n    \"\"\"Simulate observed vs true variance under censoring.\"\"\"\n    rng = np.random.default_rng(42)\n    r_star = rng.normal(mu, true_sigma, n_sim)\n    r_obs = np.clip(r_star, -limit, limit)\n    \n    var_true = np.var(r_star)\n    var_obs = np.var(r_obs)\n    \n    pct_censored = ((r_star &gt;= limit) | (r_star &lt;= -limit)).mean()\n    \n    return {\n        'true_sigma': true_sigma,\n        'true_var': var_true,\n        'obs_var': var_obs,\n        'var_ratio': var_obs / var_true,\n        'bias_pct': (1 - var_obs / var_true) * 100,\n        'pct_censored': pct_censored * 100\n    }\n\n# Sweep across volatility levels for each exchange limit\nresults_bias = []\nsigmas = np.linspace(0.005, 0.08, 50)\n\nfor limit_name, limit in [('HOSE $\\pm$ 7%', 0.07), ('HNX $\\pm$ 10%', 0.10),\n                            ('UPCoM $\\pm$ 15%', 0.15)]:\n    for sigma in sigmas:\n        res = simulate_censored_variance(sigma, limit)\n        res['exchange'] = limit_name\n        results_bias.append(res)\n\nbias_df = pd.DataFrame(results_bias)\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\ncolors_exch = {'HOSE $\\pm$ 7%': '#2C5F8A', 'HNX $\\pm$ 10%': '#C0392B',\n                'UPCoM $\\pm$ 15%': '#27AE60'}\n\nfor exch in colors_exch:\n    subset = bias_df[bias_df['exchange'] == exch]\n    axes[0].plot(subset['true_sigma'] * 100, subset['var_ratio'],\n                  color=colors_exch[exch], linewidth=2, label=exch)\n\naxes[0].axhline(y=1, color='gray', linewidth=0.5, linestyle='--')\naxes[0].set_xlabel('True Daily Volatility (%)')\naxes[0].set_ylabel('Observed / True Variance')\naxes[0].set_title('Panel A: Variance Ratio')\naxes[0].legend()\naxes[0].set_ylim([0.5, 1.05])\n\nfor exch in colors_exch:\n    subset = bias_df[bias_df['exchange'] == exch]\n    axes[1].plot(subset['true_sigma'] * 100, subset['pct_censored'],\n                  color=colors_exch[exch], linewidth=2, label=exch)\n\naxes[1].set_xlabel('True Daily Volatility (%)')\naxes[1].set_ylabel('% of Returns Censored')\naxes[1].set_title('Panel B: Censoring Rate')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 37.5\n\n\n\n\n\n37.4.2 Empirical Variance Bias by Size\n\n# Cross-listed stocks or transfer events provide a natural experiment:\n# Same stock, different limit regime\n# Alternative: compare variance of HOSE returns to variance of the same\n# stock's returns implied from intraday data (not censored by closing limit)\n\n# Approach: Tobit-based variance estimation\n# Model observed returns as censored normal\ndef tobit_variance(returns, limit):\n    \"\"\"\n    Estimate true variance via Tobit MLE under censored normal.\n    \"\"\"\n    r = returns.dropna().values\n    upper = limit\n    lower = -limit\n    \n    # Classify observations\n    at_upper = r &gt;= (upper - 1e-6)\n    at_lower = r &lt;= (lower + 1e-6)\n    interior = ~at_upper & ~at_lower\n    \n    if interior.sum() &lt; 20:\n        return np.nan, np.nan\n    \n    def neg_loglik(params):\n        mu, log_sigma = params\n        sigma = np.exp(log_sigma)\n        \n        ll = 0\n        # Interior observations\n        if interior.sum() &gt; 0:\n            ll += np.sum(stats.norm.logpdf(r[interior], mu, sigma))\n        # Upper censored\n        if at_upper.sum() &gt; 0:\n            ll += np.sum(np.log(1 - stats.norm.cdf(upper, mu, sigma) + 1e-15))\n        # Lower censored\n        if at_lower.sum() &gt; 0:\n            ll += np.sum(np.log(stats.norm.cdf(lower, mu, sigma) + 1e-15))\n        \n        return -ll\n    \n    # Initial values\n    mu0 = r[interior].mean() if interior.sum() &gt; 0 else 0\n    sigma0 = r[interior].std() if interior.sum() &gt; 0 else r.std()\n    \n    try:\n        result = optimize.minimize(\n            neg_loglik, [mu0, np.log(max(sigma0, 1e-6))],\n            method='Nelder-Mead', options={'maxiter': 5000}\n        )\n        mu_hat = result.x[0]\n        sigma_hat = np.exp(result.x[1])\n        return mu_hat, sigma_hat\n    except Exception:\n        return np.nan, np.nan\n\n# Estimate for each HOSE stock\nhose_stocks = daily_hose.groupby('ticker').filter(\n    lambda x: len(x) &gt;= 250\n)['ticker'].unique()\n\ntobit_results = []\nfor ticker in hose_stocks[:500]:  # Sample for speed\n    rets = daily_hose[daily_hose['ticker'] == ticker]['daily_return'].dropna()\n    if len(rets) &lt; 250:\n        continue\n    \n    naive_sigma = rets.std()\n    mu_hat, sigma_hat = tobit_variance(rets, 0.07)\n    \n    if np.isfinite(sigma_hat) and sigma_hat &gt; 0:\n        tobit_results.append({\n            'ticker': ticker,\n            'naive_sigma': naive_sigma,\n            'tobit_sigma': sigma_hat,\n            'bias_pct': (sigma_hat - naive_sigma) / naive_sigma * 100\n        })\n\ntobit_df = pd.DataFrame(tobit_results)\n\nprint(\"Tobit vs Naive Volatility Estimation (HOSE):\")\nprint(f\"  Mean naive σ:  {tobit_df['naive_sigma'].mean():.4f}\")\nprint(f\"  Mean Tobit σ:  {tobit_df['tobit_sigma'].mean():.4f}\")\nprint(f\"  Mean bias:     {tobit_df['bias_pct'].mean():.1f}%\")\nprint(f\"  Median bias:   {tobit_df['bias_pct'].median():.1f}%\")\nprint(f\"  Max bias:      {tobit_df['bias_pct'].max():.1f}%\")\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].scatter(tobit_df['naive_sigma'] * 100,\n                 tobit_df['tobit_sigma'] * 100,\n                 s=15, alpha=0.5, color='#2C5F8A', edgecolors='none')\nlim = max(tobit_df['tobit_sigma'].max(), tobit_df['naive_sigma'].max()) * 100 + 0.5\naxes[0].plot([0, lim], [0, lim], 'k--', linewidth=1)\naxes[0].set_xlabel('Naive σ (% daily)')\naxes[0].set_ylabel('Tobit σ (% daily)')\naxes[0].set_title('Panel A: Tobit vs Naive Volatility')\n\naxes[1].hist(tobit_df['bias_pct'], bins=50, color='#C0392B',\n             alpha=0.7, edgecolor='white', density=True)\naxes[1].axvline(x=0, color='black', linewidth=1)\naxes[1].axvline(x=tobit_df['bias_pct'].median(), color='#2C5F8A',\n                linewidth=2, linestyle='--',\n                label=f\"Median: {tobit_df['bias_pct'].median():.1f}%\")\naxes[1].set_xlabel('Bias (%): (Tobit - Naive) / Naive')\naxes[1].set_ylabel('Density')\naxes[1].set_title('Panel B: Distribution of Correction')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 37.6",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-vol-estimation",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-vol-estimation",
    "title": "37  Price Limits and Volatility",
    "section": "37.5 Volatility Estimation Under Price Limits",
    "text": "37.5 Volatility Estimation Under Price Limits\n\n37.5.1 Range-Based Estimators\nRange-based volatility estimators use the daily high and low prices rather than close-to-close returns, making them partially robust to closing-price censoring (since intraday prices may approach but not be censored at the same points). However, they are biased when the intraday price trajectory itself is constrained by the limits.\n\ndef parkinson_vol(high, low, n_periods=20):\n    \"\"\"\n    Parkinson (1980) range-based volatility estimator.\n    σ² = (1/4ln2) * E[(ln(H/L))²]\n    \"\"\"\n    log_hl = np.log(high / low)\n    var = (1 / (4 * np.log(2))) * (log_hl ** 2)\n    return np.sqrt(var.rolling(n_periods).mean())\n\ndef garman_klass_vol(open_p, high, low, close, n_periods=20):\n    \"\"\"\n    Garman-Klass (1980) OHLC volatility estimator.\n    More efficient than Parkinson by using open and close.\n    \"\"\"\n    log_hl = np.log(high / low)\n    log_co = np.log(close / open_p)\n    var = 0.5 * log_hl ** 2 - (2 * np.log(2) - 1) * log_co ** 2\n    return np.sqrt(var.rolling(n_periods).mean())\n\ndef yang_zhang_vol(open_p, high, low, close, n_periods=20):\n    \"\"\"\n    Yang-Zhang (2000) drift-independent estimator.\n    Combines overnight, Rogers-Satchell, and open-to-close components.\n    \"\"\"\n    log_oc = np.log(open_p / close.shift(1))  # Overnight\n    log_co = np.log(close / open_p)\n    log_ho = np.log(high / open_p)\n    log_lo = np.log(low / open_p)\n    \n    # Rogers-Satchell component\n    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n    \n    k = 0.34 / (1.34 + (n_periods + 1) / (n_periods - 1))\n    \n    var_overnight = log_oc.rolling(n_periods).var()\n    var_open_close = log_co.rolling(n_periods).var()\n    var_rs = rs.rolling(n_periods).mean()\n    \n    var = var_overnight + k * var_open_close + (1 - k) * var_rs\n    return np.sqrt(var.clip(lower=0))\n\n# Compute for HOSE sample\nsample_ticker = 'VNM'  # Large, liquid stock\nsample = daily_hose[daily_hose['ticker'] == sample_ticker].copy()\nsample = sample.sort_values('date').set_index('date')\n\n# Close-to-close realized vol\nsample['cc_vol'] = sample['daily_return'].rolling(20).std() * np.sqrt(252)\n\n# Range-based\nsample['parkinson'] = parkinson_vol(\n    sample['high'], sample['low'], 20\n) * np.sqrt(252)\n\nsample['garman_klass'] = garman_klass_vol(\n    sample['open'], sample['high'], sample['low'], sample['close'], 20\n) * np.sqrt(252)\n\nsample['yang_zhang'] = yang_zhang_vol(\n    sample['open'], sample['high'], sample['low'], sample['close'], 20\n) * np.sqrt(252)\n\n\n\n\nfig, ax = plt.subplots(figsize=(14, 5))\n\nax.plot(sample.index, sample['cc_vol'], color='#BDC3C7',\n        linewidth=1, label='Close-to-Close', alpha=0.8)\nax.plot(sample.index, sample['parkinson'], color='#2C5F8A',\n        linewidth=1.5, label='Parkinson')\nax.plot(sample.index, sample['yang_zhang'], color='#C0392B',\n        linewidth=1.5, label='Yang-Zhang')\n\nax.set_ylabel('Annualized Volatility')\nax.set_title(f'Volatility Estimators: {sample_ticker}')\nax.legend(ncol=3)\nax.set_ylim([0, ax.get_ylim()[1]])\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 37.7\n\n\n\n\n\n37.5.2 GARCH Models with Censored Returns\nStandard GARCH models assume returns are fully observed. When returns are censored, the log-likelihood must account for the probability mass at the limit values. We implement a censored GARCH(1,1):\n\\[\nr_t = \\mu + \\varepsilon_t, \\quad \\varepsilon_t = \\sigma_t z_t, \\quad z_t \\sim N(0, 1)\n\\tag{37.3}\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\tag{37.4}\\]\nThe censored log-likelihood replaces the standard normal density for limit-hit observations:\n\\[\n\\ell_t = \\begin{cases}\n\\log \\phi\\left(\\frac{r_t - \\mu}{\\sigma_t}\\right) - \\log \\sigma_t & \\text{if interior} \\\\\n\\log \\Phi\\left(\\frac{\\underline{L} - \\mu}{\\sigma_t}\\right) & \\text{if lower limit} \\\\\n\\log\\left[1 - \\Phi\\left(\\frac{\\bar{L} - \\mu}{\\sigma_t}\\right)\\right] & \\text{if upper limit}\n\\end{cases}\n\\tag{37.5}\\]\n\ndef censored_garch11(returns, limit, max_iter=500):\n    \"\"\"\n    GARCH(1,1) with censored normal likelihood.\n    \n    Parameters\n    ----------\n    returns : array-like\n        Observed daily returns (censored at $\\pm$ limit).\n    limit : float\n        Price limit (e.g., 0.07 for HOSE).\n    \n    Returns\n    -------\n    Dictionary with estimated parameters and conditional variances.\n    \"\"\"\n    r = np.array(returns, dtype=float)\n    T = len(r)\n    upper = limit\n    lower = -limit\n    \n    at_upper = r &gt;= (upper - 1e-6)\n    at_lower = r &lt;= (lower + 1e-6)\n    interior = ~at_upper & ~at_lower\n    \n    def neg_loglik(params):\n        mu, omega, alpha, beta = params\n        \n        if omega &lt;= 0 or alpha &lt; 0 or beta &lt; 0 or (alpha + beta) &gt;= 1:\n            return 1e10\n        \n        sigma2 = np.zeros(T)\n        sigma2[0] = omega / (1 - alpha - beta) if (alpha + beta) &lt; 1 else r.var()\n        \n        ll = 0\n        for t in range(T):\n            if t &gt; 0:\n                eps = r[t - 1] - mu\n                sigma2[t] = omega + alpha * eps ** 2 + beta * sigma2[t - 1]\n            \n            sigma2[t] = max(sigma2[t], 1e-10)\n            sigma = np.sqrt(sigma2[t])\n            \n            if interior[t]:\n                ll += stats.norm.logpdf(r[t], mu, sigma)\n            elif at_upper[t]:\n                prob = 1 - stats.norm.cdf(upper, mu, sigma)\n                ll += np.log(max(prob, 1e-15))\n            elif at_lower[t]:\n                prob = stats.norm.cdf(lower, mu, sigma)\n                ll += np.log(max(prob, 1e-15))\n        \n        return -ll\n    \n    # Initial values from standard GARCH\n    mu0 = r[interior].mean() if interior.any() else 0\n    var0 = r[interior].var() if interior.any() else r.var()\n    \n    try:\n        result = optimize.minimize(\n            neg_loglik,\n            [mu0, var0 * 0.05, 0.10, 0.85],\n            method='Nelder-Mead',\n            options={'maxiter': max_iter, 'xatol': 1e-8}\n        )\n        mu, omega, alpha, beta = result.x\n        \n        # Reconstruct conditional variance\n        sigma2 = np.zeros(T)\n        sigma2[0] = omega / max(1 - alpha - beta, 0.01)\n        for t in range(1, T):\n            eps = r[t - 1] - mu\n            sigma2[t] = omega + alpha * eps ** 2 + beta * sigma2[t - 1]\n        \n        return {\n            'mu': mu, 'omega': omega, 'alpha': alpha, 'beta': beta,\n            'persistence': alpha + beta,\n            'uncond_var': omega / max(1 - alpha - beta, 0.01),\n            'sigma2': sigma2,\n            'loglik': -result.fun,\n            'converged': result.success,\n            'n_censored': at_upper.sum() + at_lower.sum(),\n            'pct_censored': (at_upper.sum() + at_lower.sum()) / T * 100\n        }\n    except Exception as e:\n        return None\n\n# Compare standard vs censored GARCH for a volatile stock\nvolatile_stock = daily_hose.groupby('ticker')['limit_up_hit'].mean()\nvolatile_stock = volatile_stock.sort_values(ascending=False).head(20)\ntest_ticker = volatile_stock.index[0]\n\ntest_returns = (\n    daily_hose[daily_hose['ticker'] == test_ticker]\n    .sort_values('date')['daily_return']\n    .dropna()\n    .values\n)\n\n# Standard GARCH (arch library)\nstd_garch = arch_model(test_returns * 100, vol='GARCH', p=1, q=1,\n                         mean='Constant', dist='normal')\nstd_result = std_garch.fit(disp='off')\n\n# Censored GARCH\ncens_result = censored_garch11(test_returns, limit=0.07)\n\nprint(f\"Stock: {test_ticker}\")\nprint(f\"Observations: {len(test_returns)}, \"\n      f\"Censored: {cens_result['pct_censored']:.1f}%\\n\")\n\nprint(f\"{'Parameter':&lt;12} {'Standard':&gt;12} {'Censored':&gt;12}\")\nprint(\"-\" * 36)\nprint(f\"{'μ':&lt;12} {std_result.params['mu']/100:&gt;12.6f} \"\n      f\"{cens_result['mu']:&gt;12.6f}\")\nprint(f\"{'ω':&lt;12} {std_result.params['omega']/10000:&gt;12.8f} \"\n      f\"{cens_result['omega']:&gt;12.8f}\")\nprint(f\"{'α':&lt;12} {std_result.params['alpha[1]']:&gt;12.4f} \"\n      f\"{cens_result['alpha']:&gt;12.4f}\")\nprint(f\"{'β':&lt;12} {std_result.params['beta[1]']:&gt;12.4f} \"\n      f\"{cens_result['beta']:&gt;12.4f}\")\nprint(f\"{'α+β':&lt;12} \"\n      f\"{std_result.params['alpha[1]']+std_result.params['beta[1]']:&gt;12.4f} \"\n      f\"{cens_result['persistence']:&gt;12.4f}\")\nprint(f\"{'Uncond σ':&lt;12} \"\n      f\"{np.sqrt(std_result.params['omega']/(1-std_result.params['alpha[1]']-std_result.params['beta[1]'])/10000):&gt;12.4f} \"\n      f\"{np.sqrt(cens_result['uncond_var']):&gt;12.4f}\")\n\n\n\n\ntest_data = daily_hose[daily_hose['ticker'] == test_ticker].sort_values('date')\ndates = test_data['date'].values[-len(test_returns):]\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True,\n                          gridspec_kw={'height_ratios': [1, 2]})\n\n# Panel A: Returns with limit hits highlighted\naxes[0].plot(dates, test_returns, color='#2C5F8A', linewidth=0.5, alpha=0.7)\nlimit_up_mask = test_returns &gt;= 0.069\nlimit_down_mask = test_returns &lt;= -0.069\naxes[0].scatter(dates[limit_up_mask], test_returns[limit_up_mask],\n                 color='#27AE60', s=10, zorder=3, label='Upper limit')\naxes[0].scatter(dates[limit_down_mask], test_returns[limit_down_mask],\n                 color='#C0392B', s=10, zorder=3, label='Lower limit')\naxes[0].axhline(y=0.07, color='gray', linewidth=0.5, linestyle='--')\naxes[0].axhline(y=-0.07, color='gray', linewidth=0.5, linestyle='--')\naxes[0].set_ylabel('Return')\naxes[0].set_title(f'Panel A: Daily Returns ({test_ticker})')\naxes[0].legend(fontsize=8)\n\n# Panel B: Conditional volatility\nstd_sigma = std_result.conditional_volatility / 100  # Convert from % to decimal\ncens_sigma = np.sqrt(cens_result['sigma2'])\n\naxes[1].plot(dates, std_sigma * np.sqrt(252), color='#BDC3C7',\n             linewidth=1, label='Standard GARCH')\naxes[1].plot(dates, cens_sigma * np.sqrt(252), color='#C0392B',\n             linewidth=1.5, label='Censored GARCH')\naxes[1].set_ylabel('Annualized Conditional σ')\naxes[1].set_title('Panel B: Conditional Volatility')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 37.8",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-asset-pricing",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-asset-pricing",
    "title": "37  Price Limits and Volatility",
    "section": "37.6 Effects on Asset Pricing Tests",
    "text": "37.6 Effects on Asset Pricing Tests\n\n37.6.1 Beta Attenuation\nPrice limits attenuate the covariance between stock returns and factor returns, biasing beta estimates toward zero. The intuition is simple: on days when the market moves 3% but a stock’s true return would have been 6%, the observed return is capped at 7%, understating the stock’s sensitivity.\n\n# Compare betas estimated with all days vs excluding limit-hit days\n# Also compare betas from HOSE stocks vs same stocks if they were on HNX\n\ndaily_hose_merged = daily_hose.merge(\n    client.get_index_returns('VNINDEX', frequency='daily',\n                              start_date='2008-01-01',\n                              end_date='2024-12-31')[['date', 'return']],\n    on='date', how='left'\n).rename(columns={'return': 'mkt_return'})\n\n# For each stock, estimate beta:\n# (a) Using all days\n# (b) Excluding limit-hit days\n# (c) Tobit-corrected (censored regression)\nbeta_comparison = []\n\nfor ticker in hose_stocks[:300]:\n    stock = daily_hose_merged[daily_hose_merged['ticker'] == ticker].dropna(\n        subset=['daily_return', 'mkt_return']\n    )\n    if len(stock) &lt; 250:\n        continue\n    \n    # (a) All days\n    X_all = sm.add_constant(stock['mkt_return'])\n    model_all = sm.OLS(stock['daily_return'], X_all).fit()\n    beta_all = model_all.params['mkt_return']\n    \n    # (b) Exclude limit-hit days\n    interior = stock[~stock['limit_up_hit'] & ~stock['limit_down_hit']]\n    if len(interior) &lt; 200:\n        continue\n    X_int = sm.add_constant(interior['mkt_return'])\n    model_int = sm.OLS(interior['daily_return'], X_int).fit()\n    beta_interior = model_int.params['mkt_return']\n    \n    # Limit hit frequency for this stock\n    pct_limit = (stock['limit_up_hit'].sum() + stock['limit_down_hit'].sum()) / len(stock) * 100\n    \n    beta_comparison.append({\n        'ticker': ticker,\n        'beta_all': beta_all,\n        'beta_interior': beta_interior,\n        'beta_diff_pct': (beta_interior - beta_all) / abs(beta_all) * 100,\n        'pct_limit_hits': pct_limit\n    })\n\nbeta_df = pd.DataFrame(beta_comparison)\n\nprint(\"Beta Attenuation from Price Limits:\")\nprint(f\"  Mean β (all days):      {beta_df['beta_all'].mean():.3f}\")\nprint(f\"  Mean β (interior only): {beta_df['beta_interior'].mean():.3f}\")\nprint(f\"  Mean difference:        {beta_df['beta_diff_pct'].mean():.1f}%\")\nprint(f\"  Correlation(diff, limit_freq): \"\n      f\"{beta_df['beta_diff_pct'].corr(beta_df['pct_limit_hits']):.3f}\")\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].scatter(beta_df['beta_all'], beta_df['beta_interior'],\n                 s=15, alpha=0.5, color='#2C5F8A', edgecolors='none')\nlim = max(beta_df['beta_all'].abs().max(),\n           beta_df['beta_interior'].abs().max()) + 0.2\naxes[0].plot([-0.5, lim], [-0.5, lim], 'k--', linewidth=1)\naxes[0].set_xlabel('β (all days)')\naxes[0].set_ylabel('β (interior only)')\naxes[0].set_title('Panel A: Beta with vs without Limit Days')\n\naxes[1].scatter(beta_df['pct_limit_hits'], beta_df['beta_diff_pct'],\n                 s=15, alpha=0.5, color='#C0392B', edgecolors='none')\n# Add regression line\nz = np.polyfit(beta_df['pct_limit_hits'], beta_df['beta_diff_pct'], 1)\nx_line = np.linspace(0, beta_df['pct_limit_hits'].max(), 100)\naxes[1].plot(x_line, np.polyval(z, x_line), 'k-', linewidth=1.5)\naxes[1].axhline(y=0, color='gray', linewidth=0.5)\naxes[1].set_xlabel('Limit Hit Frequency (%)')\naxes[1].set_ylabel('Beta Increase When Excluding Limit Days (%)')\naxes[1].set_title('Panel B: Attenuation vs Limit Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 37.9\n\n\n\n\n\n37.6.2 Effect on Factor Premia\nIf betas are attenuated by censoring, then cross-sectional Fama-MacBeth risk premia estimates are biased upward (because the denominator of the slope coefficient is too small). We quantify this effect:\n\n# Monthly returns: compute with all days vs excluding limit-hit days\n# Then construct factors under each definition\n\nmonthly_all = (\n    daily_hose.groupby(['ticker', daily_hose['date'].dt.to_period('M')])\n    .agg(\n        ret_all=('daily_return', lambda x: (1 + x).prod() - 1),\n        ret_interior=('daily_return',\n                       lambda x: (1 + x[~x.name.map(\n                           lambda idx: daily_hose.loc[idx, 'limit_up_hit'] |\n                                        daily_hose.loc[idx, 'limit_down_hit']\n                       ).values]).prod() - 1 if len(x) &gt; 0 else np.nan),\n        n_limit_days=('limit_up_hit',\n                       lambda x: x.sum() + daily_hose.loc[x.index, 'limit_down_hit'].sum()),\n        n_trading_days=('daily_return', 'count')\n    )\n    .reset_index()\n)\n\nprint(\"Impact on Monthly Returns:\")\nprint(f\"  Mean monthly return (all days):     \"\n      f\"{monthly_all['ret_all'].mean():.4f}\")\nprint(f\"  Mean monthly return (interior):     \"\n      f\"{monthly_all['ret_interior'].mean():.4f}\")\nprint(f\"  Avg limit days per stock-month:     \"\n      f\"{monthly_all['n_limit_days'].mean():.2f}\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-magnet",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-magnet",
    "title": "37  Price Limits and Volatility",
    "section": "37.7 The Magnet Effect",
    "text": "37.7 The Magnet Effect\n\n37.7.1 Do Limits Attract Prices?\nThe magnet effect hypothesis posits that price limits, rather than cooling the market, actually attract prices to the limit as traders rush to execute before the stock becomes locked (Cho et al. 2003). If a stock is approaching the upper limit, buyers accelerate their orders to avoid being shut out, creating a self-fulfilling rush to the boundary.\nWe test for the magnet effect by examining the speed of price movement toward the limit conditional on approaching it:\n\n# Approach: for days that eventually hit the limit,\n# compare the return in the last hour vs the first hour\n# relative to non-limit days with similar initial trajectories\n\n# Without intraday data, we use a cross-day approach:\n# On day t, if the stock is within X% of the limit at some point,\n# what is the probability of hitting the limit on day t vs day t+1?\n\n# Simpler test: return continuation after near-limit days\ndef magnet_test(daily_df, limit, proximity_threshold=0.8):\n    \"\"\"\n    Test for the magnet effect.\n    \n    For each stock-day, classify:\n    - 'near_limit_up': return in [proximity_threshold * limit, limit)\n    - 'near_limit_down': return in (-limit, -proximity_threshold * limit]\n    - 'hit_limit_up': return = limit\n    - 'hit_limit_down': return = -limit\n    - 'normal': all others\n    \n    Then examine next-day behavior.\n    \"\"\"\n    df = daily_df.copy()\n    df['abs_ret'] = df['daily_return'].abs()\n    \n    df['near_up'] = (df['daily_return'] &gt;= proximity_threshold * limit) & \\\n                     (df['daily_return'] &lt; limit - 0.001)\n    df['near_down'] = (df['daily_return'] &lt;= -proximity_threshold * limit) & \\\n                       (df['daily_return'] &gt; -limit + 0.001)\n    \n    df['next_return'] = df.groupby('ticker')['daily_return'].shift(-1)\n    df['next_limit_up'] = df.groupby('ticker')['limit_up_hit'].shift(-1)\n    df['next_limit_down'] = df.groupby('ticker')['limit_down_hit'].shift(-1)\n    \n    results = {}\n    \n    # Near upper limit\n    near_up = df[df['near_up']]\n    if len(near_up) &gt; 100:\n        results['near_up'] = {\n            'n': len(near_up),\n            'next_day_return': near_up['next_return'].mean(),\n            'prob_next_limit_up': near_up['next_limit_up'].mean(),\n            'prob_next_limit_down': near_up['next_limit_down'].mean()\n        }\n    \n    # At upper limit\n    at_up = df[df['limit_up_hit']]\n    if len(at_up) &gt; 100:\n        results['at_up'] = {\n            'n': len(at_up),\n            'next_day_return': at_up['next_return'].mean(),\n            'prob_next_limit_up': at_up['next_limit_up'].mean(),\n            'prob_next_limit_down': at_up['next_limit_down'].mean()\n        }\n    \n    # Near lower limit\n    near_down = df[df['near_down']]\n    if len(near_down) &gt; 100:\n        results['near_down'] = {\n            'n': len(near_down),\n            'next_day_return': near_down['next_return'].mean(),\n            'prob_next_limit_up': near_down['next_limit_up'].mean(),\n            'prob_next_limit_down': near_down['next_limit_down'].mean()\n        }\n    \n    # At lower limit\n    at_down = df[df['limit_down_hit']]\n    if len(at_down) &gt; 100:\n        results['at_down'] = {\n            'n': len(at_down),\n            'next_day_return': at_down['next_return'].mean(),\n            'prob_next_limit_up': at_down['next_limit_up'].mean(),\n            'prob_next_limit_down': at_down['next_limit_down'].mean()\n        }\n    \n    # Normal days (benchmark)\n    normal = df[~df['near_up'] & ~df['near_down'] &\n                 ~df['limit_up_hit'] & ~df['limit_down_hit']]\n    results['normal'] = {\n        'n': len(normal),\n        'next_day_return': normal['next_return'].mean(),\n        'prob_next_limit_up': normal['next_limit_up'].mean(),\n        'prob_next_limit_down': normal['next_limit_down'].mean()\n    }\n    \n    return pd.DataFrame(results).T\n\nmagnet = magnet_test(daily_hose, 0.07, proximity_threshold=0.8)\nprint(\"Magnet Effect Test (HOSE):\")\nprint(magnet.round(4).to_string())\n\n\n\n\n# More granular: bin today's return and compute next-day statistics\nbins = np.arange(-0.075, 0.08, 0.005)\ndaily_hose_next = daily_hose.copy()\ndaily_hose_next['next_return'] = (\n    daily_hose_next.groupby('ticker')['daily_return'].shift(-1)\n)\ndaily_hose_next['ret_bin'] = pd.cut(daily_hose_next['daily_return'],\n                                      bins=bins, labels=False)\n\nbin_stats = (\n    daily_hose_next.dropna(subset=['ret_bin', 'next_return'])\n    .groupby('ret_bin')\n    .agg(\n        mean_ret=('daily_return', 'mean'),\n        next_ret=('next_return', 'mean'),\n        n=('next_return', 'count')\n    )\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Next-day return by today's return\naxes[0].bar(bin_stats['mean_ret'] * 100, bin_stats['next_ret'] * 100,\n            width=0.4,\n            color=np.where(bin_stats['next_ret'] &gt; 0, '#27AE60', '#C0392B'),\n            alpha=0.7, edgecolor='white')\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].axvline(x=7, color='gray', linewidth=1, linestyle='--')\naxes[0].axvline(x=-7, color='gray', linewidth=1, linestyle='--')\naxes[0].set_xlabel(\"Today's Return (%)\")\naxes[0].set_ylabel('Next-Day Return (%)')\naxes[0].set_title('Panel A: Next-Day Return by Current Return')\n\n# Panel B: Continuation probability\n# Probability of same-direction move next day\ndaily_hose_next['continuation'] = (\n    np.sign(daily_hose_next['daily_return']) ==\n    np.sign(daily_hose_next['next_return'])\n)\n\ncont_by_bin = (\n    daily_hose_next.dropna(subset=['ret_bin', 'continuation'])\n    .groupby('ret_bin')\n    .agg(\n        mean_ret=('daily_return', 'mean'),\n        cont_prob=('continuation', 'mean'),\n        n=('continuation', 'count')\n    )\n)\n\naxes[1].scatter(cont_by_bin['mean_ret'] * 100, cont_by_bin['cont_prob'] * 100,\n                 color='#2C5F8A', s=40, alpha=0.7)\naxes[1].axhline(y=50, color='gray', linewidth=0.5, linestyle='--')\naxes[1].axvline(x=7, color='gray', linewidth=1, linestyle='--')\naxes[1].axvline(x=-7, color='gray', linewidth=1, linestyle='--')\naxes[1].set_xlabel(\"Today's Return (%)\")\naxes[1].set_ylabel('Continuation Probability (%)')\naxes[1].set_title('Panel B: Same-Direction Move Next Day')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 37.10",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-delayed",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-delayed",
    "title": "37  Price Limits and Volatility",
    "section": "37.8 The Delayed Price Discovery Hypothesis",
    "text": "37.8 The Delayed Price Discovery Hypothesis\n\n37.8.1 Volatility Spillover\nIf limits prevent full price adjustment on day \\(t\\), the residual adjustment spills over to day \\(t+1\\) (and possibly further). This predicts higher volatility on the day after a limit hit, and positive return autocorrelation (continuation in the direction of the limit hit). Kim and Rhee (1997) find strong evidence of this in the Tokyo Stock Exchange.\n\n# Compare volatility and return on limit-hit days vs day after\nspillover_data = daily_hose.copy()\nspillover_data['prev_limit_up'] = (\n    spillover_data.groupby('ticker')['limit_up_hit'].shift(1)\n)\nspillover_data['prev_limit_down'] = (\n    spillover_data.groupby('ticker')['limit_down_hit'].shift(1)\n)\nspillover_data['abs_return'] = spillover_data['daily_return'].abs()\n\n# Classify days\nconditions = {\n    'After upper limit': spillover_data['prev_limit_up'] == True,\n    'After lower limit': spillover_data['prev_limit_down'] == True,\n    'Normal day': (spillover_data['prev_limit_up'] == False) &\n                   (spillover_data['prev_limit_down'] == False)\n}\n\nprint(\"Volatility Spillover After Limit Hits:\")\nprint(f\"{'Condition':&lt;25} {'Mean |r|':&gt;10} {'Mean r':&gt;10} \"\n      f\"{'σ(r)':&gt;10} {'N':&gt;12}\")\nprint(\"-\" * 67)\n\nfor label, mask in conditions.items():\n    subset = spillover_data[mask].dropna(subset=['daily_return'])\n    print(f\"{label:&lt;25} \"\n          f\"{subset['abs_return'].mean()*100:&gt;10.3f}% \"\n          f\"{subset['daily_return'].mean()*100:&gt;10.3f}% \"\n          f\"{subset['daily_return'].std()*100:&gt;10.3f}% \"\n          f\"{len(subset):&gt;12,}\")\n\n# Statistical test: is variance higher after limit days?\nnormal = spillover_data[conditions['Normal day']]['daily_return'].dropna()\nafter_up = spillover_data[conditions['After upper limit']]['daily_return'].dropna()\nafter_down = spillover_data[conditions['After lower limit']]['daily_return'].dropna()\n\nf_up = after_up.var() / normal.var()\nf_down = after_down.var() / normal.var()\nprint(f\"\\nVariance ratios (vs normal days):\")\nprint(f\"  After upper limit: {f_up:.3f} \"\n      f\"(p = {1 - stats.f.cdf(f_up, len(after_up)-1, len(normal)-1):.4f})\")\nprint(f\"  After lower limit: {f_down:.3f} \"\n      f\"(p = {1 - stats.f.cdf(f_down, len(after_down)-1, len(normal)-1):.4f})\")\n\n\n\n37.8.2 Multi-Day Return Reconstruction\nTo recover the “true” return that would have occurred without price limits, we can compound returns over consecutive limit-hit days until the stock resumes normal trading:\n\ndef reconstruct_returns(group, limit):\n    \"\"\"\n    For each limit-hit sequence, compound returns until\n    the stock resumes normal trading (first non-limit day).\n    Returns the compound return and the number of days.\n    \"\"\"\n    sequences = []\n    in_sequence = False\n    seq_start = None\n    seq_returns = []\n    seq_direction = None\n    \n    for _, row in group.iterrows():\n        if row['limit_up_hit'] or row['limit_down_hit']:\n            if not in_sequence:\n                in_sequence = True\n                seq_start = row['date']\n                seq_returns = [row['daily_return']]\n                seq_direction = 'up' if row['limit_up_hit'] else 'down'\n            else:\n                seq_returns.append(row['daily_return'])\n        else:\n            if in_sequence:\n                # Include the first non-limit day (the \"resolution\" day)\n                seq_returns.append(row['daily_return'])\n                compound_ret = np.prod([1 + r for r in seq_returns]) - 1\n                sequences.append({\n                    'ticker': group.name if hasattr(group, 'name') else group['ticker'].iloc[0],\n                    'start_date': seq_start,\n                    'n_limit_days': len(seq_returns) - 1,\n                    'compound_return': compound_ret,\n                    'direction': seq_direction,\n                    'limit_return': sum(seq_returns[:-1]),\n                    'resolution_return': seq_returns[-1]\n                })\n                in_sequence = False\n    \n    return sequences\n\n# Run for all HOSE stocks\nall_sequences = []\nfor ticker, group in daily_hose.sort_values('date').groupby('ticker'):\n    seqs = reconstruct_returns(group, 0.07)\n    all_sequences.extend(seqs)\n\nseq_df = pd.DataFrame(all_sequences)\n\nif len(seq_df) &gt; 0:\n    print(\"Limit-Hit Sequence Analysis:\")\n    print(f\"  Total sequences: {len(seq_df):,}\")\n    print(f\"  Mean limit days: {seq_df['n_limit_days'].mean():.1f}\")\n    print(f\"\\nCompound Returns by Direction:\")\n    for direction in ['up', 'down']:\n        subset = seq_df[seq_df['direction'] == direction]\n        print(f\"  {direction.upper()} sequences: {len(subset):,}\")\n        print(f\"    Mean compound return: {subset['compound_return'].mean()*100:.2f}%\")\n        print(f\"    Mean resolution-day return: \"\n              f\"{subset['resolution_return'].mean()*100:.2f}%\")\n        print(f\"    Max compound return: {subset['compound_return'].max()*100:.1f}%\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-ivol",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-ivol",
    "title": "37  Price Limits and Volatility",
    "section": "37.9 The Idiosyncratic Volatility Puzzle Under Price Limits",
    "text": "37.9 The Idiosyncratic Volatility Puzzle Under Price Limits\nAng et al. (2006) document that stocks with high idiosyncratic volatility earn low subsequent returns—the IVOL puzzle. In Vietnam, price limits contaminate IVOL estimation: stocks that frequently hit limits have understated IVOL (because their returns are censored), which could create a mechanical relation between measured IVOL and returns.\n\n# Compute monthly IVOL two ways:\n# (a) Naive: std of daily residuals from FF3\n# (b) Corrected: excluding limit-hit days\n\n# Merge daily data with market returns for IVOL estimation\ndaily_ff = daily_hose_merged.copy()\n\nmonthly_ivol = []\nfor (ticker, month), group in daily_ff.groupby(\n    ['ticker', daily_ff['date'].dt.to_period('M')]\n):\n    if len(group) &lt; 15:\n        continue\n    \n    y = group['daily_return'].dropna()\n    x = group['mkt_return'].reindex(y.index).dropna()\n    common = y.index.intersection(x.index)\n    if len(common) &lt; 15:\n        continue\n    \n    # Naive IVOL\n    model = sm.OLS(y[common], sm.add_constant(x[common])).fit()\n    ivol_naive = model.resid.std() * np.sqrt(252)\n    \n    # Interior-only IVOL\n    interior = group[~group['limit_up_hit'] & ~group['limit_down_hit']]\n    y_int = interior['daily_return'].dropna()\n    x_int = interior['mkt_return'].reindex(y_int.index).dropna()\n    common_int = y_int.index.intersection(x_int.index)\n    \n    if len(common_int) &gt;= 10:\n        model_int = sm.OLS(y_int[common_int],\n                            sm.add_constant(x_int[common_int])).fit()\n        ivol_corrected = model_int.resid.std() * np.sqrt(252)\n    else:\n        ivol_corrected = np.nan\n    \n    n_limit = group['limit_up_hit'].sum() + group['limit_down_hit'].sum()\n    \n    monthly_ivol.append({\n        'ticker': ticker,\n        'month': month.to_timestamp(),\n        'ivol_naive': ivol_naive,\n        'ivol_corrected': ivol_corrected,\n        'n_limit_days': n_limit,\n        'pct_limit': n_limit / len(group) * 100,\n        'next_return': group['daily_return'].iloc[-1]  # Placeholder\n    })\n\nivol_df = pd.DataFrame(monthly_ivol)\n\nprint(\"IVOL Estimation: Naive vs Corrected:\")\nprint(f\"  Mean naive IVOL:     {ivol_df['ivol_naive'].mean():.4f}\")\nprint(f\"  Mean corrected IVOL: {ivol_df['ivol_corrected'].mean():.4f}\")\nprint(f\"  Mean difference:     \"\n      f\"{(ivol_df['ivol_corrected'] - ivol_df['ivol_naive']).mean():.4f}\")\nprint(f\"  Correlation:         \"\n      f\"{ivol_df['ivol_naive'].corr(ivol_df['ivol_corrected']):.3f}\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-recommendations",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-recommendations",
    "title": "37  Price Limits and Volatility",
    "section": "37.10 Practical Recommendations",
    "text": "37.10 Practical Recommendations\nFor researchers working with Vietnamese equity data:\nAlways report limit-hit frequency. Any study using Vietnamese daily returns should document the fraction of observations at the price limits, broken down by exchange and market cap quintile. This tells the reader the severity of the censoring problem in the specific sample.\nUse Tobit-corrected variance estimates. For volatility-related analyses (IVOL sorts, GARCH, risk modeling), the naive sample variance underestimates true variance by 5–20% depending on the stock’s limit-hit frequency. The Tobit MLE provides a consistent estimator under the censored normal assumption.\nConsider range-based estimators. The Yang and Zhang (2000) estimator using OHLC prices is partially robust to closing-price censoring and does not require distributional assumptions. It is a good default for individual-stock volatility estimation.\nExclude limit-hit days for beta estimation. Interior-only betas are less biased than all-day betas, though noisier. Report both and discuss the difference. For stocks with &gt;5% limit-hit frequency, the attenuation is economically meaningful.\nCompound multi-day returns for event studies. When studying events that coincide with limit hits (earnings announcements, M&A, regulatory changes), use the compound return from the limit-hit sequence start to the first non-limit day. Single-day returns are censored and understate the market’s reaction.\nBe cautious interpreting short-term return predictability. The delayed price discovery effect creates positive return autocorrelation at the daily frequency. This is a mechanical consequence of censoring, not a market inefficiency. Monthly returns are largely free of this artifact because the censoring within a month averages out.\nTest robustness to HNX and UPCoM. If a result is driven by limit-related distortions, it should appear differently (or not at all) on HNX (\\(\\pm\\) 10%) and UPCoM (\\(\\pm\\) 15%). Cross-exchange comparison is a natural placebo test.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "55_price_limits_and_volatility.html#sec-price-limit-summary",
    "href": "55_price_limits_and_volatility.html#sec-price-limit-summary",
    "title": "37  Price Limits and Volatility",
    "section": "37.11 Summary",
    "text": "37.11 Summary\n\n\n\nTable 37.2: Summary of price limit effects on empirical estimates.\n\n\n\n\n\n\n\n\n\n\n\nIssue\nBias Direction\nMagnitude (HOSE)\nRecommended Fix\n\n\n\n\nReturn variance\nUnderstated\n5–20% for volatile stocks\nTobit MLE or range-based\n\n\nGARCH vol\nUnderstated\n10–30% during crises\nCensored GARCH\n\n\nMarket beta\nAttenuated (toward 0)\n3–10% for small-caps\nInterior-only estimation\n\n\nIVOL\nUnderstated\nVaries; correlated with size\nCorrected IVOL\n\n\nReturn autocorrelation\nPositive (spurious)\nSignificant at daily freq\nUse weekly/monthly\n\n\nEvent study CARs\nUnderstated\nUp to 50% of true effect\nCompound multi-day\n\n\nDistribution shape\nPile-up at limits\n2–5% of obs at limits\nAcknowledge or correct\n\n\n\n\n\n\nPrice limits are not a minor institutional detail—they are a pervasive data-generating process that affects nearly every empirical quantity computed from Vietnamese daily returns. The corrections developed in this chapter—Tobit variance estimation, censored GARCH, interior-only betas, range-based volatility, and multi-day return compounding—form a toolkit that should be applied routinely. Ignoring censoring does not make it go away; it merely makes the resulting estimates quietly wrong.\n\n\n\n\n\n\n\nAng, Andrew, Robert J Hodrick, Yuhang Xing, and Xiaoyan Zhang. 2006. “The Cross-Section of Volatility and Expected Returns.” The Journal of Finance 61 (1): 259–99.\n\n\nBrennan, Michael J. 1986. “A Theory of Price Limits in Futures Markets.” Journal of Financial Economics 16 (2): 213–33.\n\n\nCho, David D, Jeffrey Russell, George C Tiao, and Ruey Tsay. 2003. “The Magnet Effect of Price Limits: Evidence from High-Frequency Data on Taiwan Stock Exchange.” Journal of Empirical Finance 10 (1-2): 133–68.\n\n\nKim, Kenneth A, and S Ghon Rhee. 1997. “Price Limit Performance: Evidence from the Tokyo Stock Exchange.” The Journal of Finance 52 (2): 885–901.\n\n\nYang, Dennis, and Qiang Zhang. 2000. “Drift-Independent Volatility Estimation Based on High, Low, Open, and Close Prices.” The Journal of Business 73 (3): 477–92.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Price Limits and Volatility</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html",
    "href": "56_market_integration_and_segmentation.html",
    "title": "38  Market Integration and Segmentation",
    "section": "",
    "text": "38.1 Integration in Theory\nA market is fully integrated when its assets are priced by a global stochastic discount factor: risk premia reflect only exposures to global risk factors, and identical cash flow streams command the same expected return regardless of where the issuer is domiciled. A market is fully segmented when domestic risk factors alone determine prices, and the country’s risk-return trade-off is independent of the rest of the world. Reality sits somewhere between these poles, and the location shifts over time.\nVietnam is a particularly interesting case. It opened its stock exchange in July 2000 with heavy restrictions on foreign participation. Foreign ownership limits (initially 20%, raised to 30% in 2003, 49% in 2015, and selectively removed for some firms) have been gradually relaxed. Vietnam joined the WTO in 2007. FTSE Russell upgraded Vietnam from “unclassified” to “secondary emerging” in its frontier index in 2018 and has been evaluating further upgrades. Each of these events has potentially shifted the degree of integration.\nBekaert and Harvey (1995) and Bekaert and Harvey (2002) establish the modern framework for measuring time-varying integration. Errunza and Losq (1985) develop the “mild segmentation” model in which foreign investors face barriers but can partially replicate emerging market returns through global securities. Pukthuanthong and Roll (2009) propose a factor-model-based measure that avoids the pitfalls of simple correlation analysis. This chapter implements all three approaches and applies them to Vietnam’s integration trajectory.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-theory",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-theory",
    "title": "38  Market Integration and Segmentation",
    "section": "",
    "text": "38.1.1 The Integrated and Segmented Benchmarks\nUnder full integration, the expected excess return of Vietnamese stock \\(i\\) is:\n\\[\nE[R_{i,t} - R_{f,t}] = \\beta_{i,\\text{world}} \\cdot \\lambda_{\\text{world},t}\n\\tag{38.1}\\]\nwhere \\(\\beta_{i,\\text{world}}\\) is the stock’s loading on the global market factor and \\(\\lambda_{\\text{world},t}\\) is the global risk premium. Only global systematic risk is priced; country-specific risk is diversifiable and commands no premium.\nUnder full segmentation:\n\\[\nE[R_{i,t} - R_{f,t}] = \\beta_{i,\\text{local}} \\cdot \\lambda_{\\text{local},t}\n\\tag{38.2}\\]\nwhere \\(\\beta_{i,\\text{local}}\\) is the stock’s loading on the Vietnamese market and \\(\\lambda_{\\text{local},t}\\) is the domestic risk premium. The domestic market is effectively a closed economy for pricing purposes.\nBekaert and Harvey (1995) model the transition between these states as a regime-switching process where the mixing weight \\(\\omega_t \\in [0, 1]\\) evolves over time:\n\\[\nE[R_{i,t} - R_{f,t}] = \\omega_t \\cdot \\beta_{i,\\text{world}} \\lambda_{\\text{world},t} + (1 - \\omega_t) \\cdot \\beta_{i,\\text{local}} \\lambda_{\\text{local},t}\n\\tag{38.3}\\]\nThe weight \\(\\omega_t\\) is the degree of integration: \\(\\omega_t = 1\\) is full integration, \\(\\omega_t = 0\\) is full segmentation.\n\n\n38.1.2 The Segmentation Premium\nWhen a market transitions from segmented to integrated, its cost of capital falls because the relevant risk for pricing narrows from total domestic risk to only the portion correlated with the global market (Henry 2000; Bekaert, Harvey, and Lundblad 2005). The segmentation premium is the excess expected return that investors in a segmented market require:\n\\[\n\\text{Segmentation premium} = (1 - \\omega_t) \\cdot (\\lambda_{\\text{local}} - \\beta_{\\text{local,world}} \\cdot \\lambda_{\\text{world}})\n\\tag{38.4}\\]\nThis premium represents a deadweight cost: it raises the cost of capital for Vietnamese firms, reduces investment, and lowers welfare relative to the integrated benchmark.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-data",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-data",
    "title": "38  Market Integration and Segmentation",
    "section": "38.2 Data Construction",
    "text": "38.2 Data Construction\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom scipy import stats, optimize\nfrom arch import arch_model\nfrom arch.univariate import ConstantMean, GARCH\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Vietnamese market returns\nvn_index = client.get_index_returns(\n    index='VNINDEX',\n    start_date='2000-07-01',\n    end_date='2024-12-31',\n    frequency='monthly',\n    fields=['date', 'return', 'total_return_index']\n)\nvn_index['date'] = pd.to_datetime(vn_index['date'])\nvn_index = vn_index.set_index('date')\n\n# Global and regional indices (USD-denominated for comparability)\nglobal_indices = client.get_global_indices(\n    indices=[\n        'MSCI_WORLD', 'MSCI_EM', 'MSCI_ASIA_PAC_EX_JP',\n        'MSCI_FM',  # Frontier markets\n        'SP500', 'STOXX600',\n        'MSCI_CHINA', 'MSCI_THAILAND', 'MSCI_INDONESIA',\n        'MSCI_PHILIPPINES', 'MSCI_MALAYSIA'\n    ],\n    start_date='2000-07-01',\n    end_date='2024-12-31',\n    frequency='monthly',\n    currency='USD'\n)\nglobal_indices['date'] = pd.to_datetime(global_indices['date'])\nglobal_indices = global_indices.pivot(index='date', columns='index', values='return')\n\n# Vietnam returns in USD for apples-to-apples comparison\nvn_usd = client.get_index_returns(\n    index='VNINDEX',\n    start_date='2000-07-01',\n    end_date='2024-12-31',\n    frequency='monthly',\n    currency='USD'\n)\nvn_usd['date'] = pd.to_datetime(vn_usd['date'])\nglobal_indices['VIETNAM'] = vn_usd.set_index('date')['return']\n\n# VND/USD exchange rate\nfx = client.get_exchange_rates(\n    pair='USD_VND',\n    start_date='2000-07-01',\n    end_date='2024-12-31',\n    frequency='monthly'\n)\nfx['date'] = pd.to_datetime(fx['date'])\nfx = fx.set_index('date')\n\n# Global factor returns (Fama-French global)\nglobal_factors = client.get_global_factor_returns(\n    start_date='2000-07-01',\n    end_date='2024-12-31',\n    frequency='monthly',\n    factors=['mkt_excess_world', 'smb_world', 'hml_world',\n             'rmw_world', 'cma_world', 'wml_world']\n)\nglobal_factors['date'] = pd.to_datetime(global_factors['date'])\nglobal_factors = global_factors.set_index('date')\n\n# Vietnamese factor returns (local)\nlocal_factors = client.get_factor_returns(\n    market='vietnam',\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    factors=['mkt_excess', 'smb', 'hml', 'rmw', 'cma', 'wml']\n)\nlocal_factors['date'] = pd.to_datetime(local_factors['date'])\nlocal_factors = local_factors.set_index('date')\n\nprint(f\"Vietnam index: {len(vn_index)} months\")\nprint(f\"Global indices: {global_indices.shape}\")\nprint(f\"Global factors: {len(global_factors)} months\")\n\n\n38.2.1 Vietnam’s Liberalization Timeline\n\nliberalization_events = pd.DataFrame([\n    ('2000-07-28', 'HOSE opens', 'Institutional'),\n    ('2002-03-01', 'FOL raised to 30%', 'Ownership'),\n    ('2005-03-01', 'HNX opens', 'Institutional'),\n    ('2006-01-01', 'Securities Law enacted', 'Legal'),\n    ('2007-01-11', 'WTO accession', 'Trade'),\n    ('2007-06-01', 'FOL raised to 49%', 'Ownership'),\n    ('2009-06-24', 'UPCoM opens', 'Institutional'),\n    ('2012-01-01', 'SSC restructuring', 'Regulatory'),\n    ('2015-09-01', 'FOL selectively removed', 'Ownership'),\n    ('2017-08-01', 'Derivatives market opens', 'Institutional'),\n    ('2018-09-01', 'FTSE Frontier Secondary', 'Index'),\n    ('2021-01-01', 'New Securities Law', 'Legal'),\n    ('2023-06-01', 'KRX trading system', 'Infrastructure'),\n], columns=['date', 'event', 'category'])\nliberalization_events['date'] = pd.to_datetime(liberalization_events['date'])\n\nprint(\"Vietnam Liberalization Timeline:\")\nfor _, row in liberalization_events.iterrows():\n    print(f\"  {row['date'].strftime('%Y-%m')}: {row['event']} [{row['category']}]\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-correlation",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-correlation",
    "title": "38  Market Integration and Segmentation",
    "section": "38.3 Correlation-Based Integration Measures",
    "text": "38.3 Correlation-Based Integration Measures\n\n38.3.1 Rolling Correlations\nThe simplest integration metric is the correlation between Vietnamese and global market returns. Higher correlation implies more integration (returns are driven by the same global factors). However, simple correlation is confounded by volatility changes (i.e., correlations tend to increase mechanically during high-volatility periods (Longin and Solnik 2001)).\n\n# Align all series\nindices_aligned = global_indices.dropna(subset=['VIETNAM', 'MSCI_WORLD']).copy()\n\n# Rolling 36-month correlations\nrolling_window = 36\n\ncorr_series = {}\nfor idx in ['MSCI_WORLD', 'MSCI_EM', 'MSCI_ASIA_PAC_EX_JP',\n            'SP500', 'MSCI_CHINA', 'MSCI_THAILAND']:\n    if idx in indices_aligned.columns:\n        corr = (\n            indices_aligned[['VIETNAM', idx]]\n            .rolling(rolling_window)\n            .corr()\n            .unstack()['VIETNAM'][idx]\n        )\n        corr_series[idx] = corr\n\ncorr_df = pd.DataFrame(corr_series)\n\n\n\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\ncolors_idx = {\n    'MSCI_WORLD': '#2C5F8A', 'MSCI_EM': '#C0392B',\n    'MSCI_ASIA_PAC_EX_JP': '#27AE60', 'SP500': '#8E44AD',\n    'MSCI_CHINA': '#E67E22', 'MSCI_THAILAND': '#1ABC9C'\n}\n\nfor idx, color in colors_idx.items():\n    if idx in corr_df.columns:\n        ax.plot(corr_df.index, corr_df[idx], color=color,\n                linewidth=1.5, label=idx.replace('MSCI_', '').replace('_', ' '),\n                alpha=0.85)\n\n# Add liberalization events\nfor _, event in liberalization_events.iterrows():\n    if event['date'] &gt;= corr_df.index.min():\n        ax.axvline(x=event['date'], color='gray', linewidth=0.5,\n                   linestyle=':', alpha=0.6)\n\nax.axhline(y=0, color='black', linewidth=0.5)\nax.set_ylabel('Correlation with Vietnam')\nax.set_title('Rolling 36-Month Correlation: Vietnam vs Global Markets')\nax.legend(fontsize=8, ncol=3)\nax.set_ylim([-0.3, 0.8])\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 38.1\n\n\n\n\n\n38.3.2 DCC-GARCH Dynamic Correlations\nTo separate changes in correlation from changes in volatility, we estimate a Dynamic Conditional Correlation (DCC) model (Engle 2002). The DCC decomposes the time-varying covariance matrix into time-varying volatilities and a time-varying correlation matrix:\n\\[\nH_t = D_t R_t D_t\n\\tag{38.5}\\]\nwhere \\(D_t = \\text{diag}(\\sigma_{1,t}, \\ldots, \\sigma_{n,t})\\) and \\(R_t\\) is the conditional correlation matrix that evolves according to:\n\\[\nQ_t = (1 - a - b) \\bar{Q} + a \\epsilon_{t-1} \\epsilon_{t-1}' + b Q_{t-1}\n\\tag{38.6}\\]\n\\[\nR_t = \\text{diag}(Q_t)^{-1/2} Q_t \\text{diag}(Q_t)^{-1/2}\n\\tag{38.7}\\]\n\ndef estimate_dcc(y1, y2, p=1, q=1):\n    \"\"\"\n    Two-step DCC-GARCH estimation.\n    Step 1: Univariate GARCH for each series.\n    Step 2: DCC parameters from standardized residuals.\n    \"\"\"\n    # Step 1: Univariate GARCH(1,1) for each series\n    models = []\n    std_resids = []\n    cond_vols = []\n    \n    for y in [y1, y2]:\n        am = arch_model(y * 100, vol='GARCH', p=p, q=q,\n                          mean='Constant', dist='normal')\n        res = am.fit(disp='off')\n        models.append(res)\n        std_resids.append(res.std_resid)\n        cond_vols.append(res.conditional_volatility / 100)\n    \n    # Align residuals\n    e1 = std_resids[0]\n    e2 = std_resids[1]\n    common = e1.dropna().index.intersection(e2.dropna().index)\n    e1 = e1[common].values\n    e2 = e2[common].values\n    T = len(e1)\n    \n    # Step 2: DCC estimation\n    # Q_bar = unconditional correlation of standardized residuals\n    Q_bar = np.corrcoef(e1, e2)\n    \n    def dcc_loglik(params):\n        a, b = params\n        if a &lt; 0 or b &lt; 0 or a + b &gt;= 1:\n            return 1e10\n        \n        Q = np.zeros((T, 2, 2))\n        R = np.zeros((T, 2, 2))\n        Q[0] = Q_bar.copy()\n        \n        ll = 0\n        for t in range(T):\n            if t &gt; 0:\n                et = np.array([[e1[t-1]], [e2[t-1]]])\n                Q[t] = (1 - a - b) * Q_bar + a * (et @ et.T) + b * Q[t-1]\n            \n            # Normalize\n            d = np.sqrt(np.diag(Q[t]))\n            if d[0] &gt; 0 and d[1] &gt; 0:\n                R[t] = Q[t] / np.outer(d, d)\n            else:\n                R[t] = np.eye(2)\n            \n            # Clip correlation\n            R[t, 0, 1] = np.clip(R[t, 0, 1], -0.999, 0.999)\n            R[t, 1, 0] = R[t, 0, 1]\n            \n            # Log-likelihood contribution\n            det_R = 1 - R[t, 0, 1] ** 2\n            if det_R &gt; 0:\n                et_vec = np.array([e1[t], e2[t]])\n                ll += -0.5 * (np.log(det_R) +\n                              et_vec @ np.linalg.inv(R[t]) @ et_vec -\n                              et_vec @ et_vec)\n        \n        return -ll\n    \n    result = optimize.minimize(dcc_loglik, [0.05, 0.90],\n                                method='Nelder-Mead',\n                                options={'maxiter': 5000})\n    a_hat, b_hat = result.x\n    \n    # Reconstruct dynamic correlations\n    Q = np.zeros((T, 2, 2))\n    dcc_corr = np.zeros(T)\n    Q[0] = Q_bar.copy()\n    \n    for t in range(T):\n        if t &gt; 0:\n            et = np.array([[e1[t-1]], [e2[t-1]]])\n            Q[t] = (1 - a_hat - b_hat) * Q_bar + a_hat * (et @ et.T) + b_hat * Q[t-1]\n        \n        d = np.sqrt(np.diag(Q[t]))\n        if d[0] &gt; 0 and d[1] &gt; 0:\n            dcc_corr[t] = Q[t, 0, 1] / (d[0] * d[1])\n        else:\n            dcc_corr[t] = 0\n    \n    return {\n        'a': a_hat, 'b': b_hat,\n        'persistence': a_hat + b_hat,\n        'dcc_corr': pd.Series(dcc_corr, index=common),\n        'cond_vol_1': cond_vols[0],\n        'cond_vol_2': cond_vols[1]\n    }\n\n# Estimate DCC: Vietnam vs MSCI World\nvn_ret = indices_aligned['VIETNAM'].dropna()\nworld_ret = indices_aligned['MSCI_WORLD'].dropna()\ncommon_dates = vn_ret.index.intersection(world_ret.index)\n\ndcc_result = estimate_dcc(vn_ret[common_dates], world_ret[common_dates])\n\nprint(f\"DCC Parameters:\")\nprint(f\"  a (news): {dcc_result['a']:.4f}\")\nprint(f\"  b (persistence): {dcc_result['b']:.4f}\")\nprint(f\"  a + b: {dcc_result['persistence']:.4f}\")\nprint(f\"\\nDCC Correlation with MSCI World:\")\nprint(f\"  Mean: {dcc_result['dcc_corr'].mean():.3f}\")\nprint(f\"  Min:  {dcc_result['dcc_corr'].min():.3f}\")\nprint(f\"  Max:  {dcc_result['dcc_corr'].max():.3f}\")\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True,\n                          gridspec_kw={'height_ratios': [2, 1]})\n\n# Panel A: DCC correlation\naxes[0].plot(dcc_result['dcc_corr'].index, dcc_result['dcc_corr'].values,\n             color='#2C5F8A', linewidth=1.5)\naxes[0].fill_between(dcc_result['dcc_corr'].index,\n                       dcc_result['dcc_corr'].values, 0,\n                       alpha=0.2, color='#2C5F8A')\n\n# Liberalization events\nevent_colors = {'Ownership': '#C0392B', 'Trade': '#27AE60',\n                'Institutional': '#E67E22', 'Legal': '#8E44AD',\n                'Index': '#1ABC9C', 'Regulatory': '#F1C40F',\n                'Infrastructure': '#3498DB'}\n\nfor _, event in liberalization_events.iterrows():\n    if event['date'] in dcc_result['dcc_corr'].index or True:\n        color = event_colors.get(event['category'], 'gray')\n        axes[0].axvline(x=event['date'], color=color, linewidth=1.5,\n                         linestyle='--', alpha=0.7)\n        axes[0].text(event['date'], axes[0].get_ylim()[1] * 0.95,\n                      event['event'][:15], rotation=90, fontsize=6,\n                      va='top', color=color)\n\naxes[0].set_ylabel('Dynamic Conditional Correlation')\naxes[0].set_title('Panel A: DCC-GARCH Correlation (Vietnam–World)')\naxes[0].axhline(y=0, color='black', linewidth=0.5)\n\n# Panel B: Conditional volatilities\nvol1 = dcc_result['cond_vol_1'] * np.sqrt(12)  # Annualized\nvol2 = dcc_result['cond_vol_2'] * np.sqrt(12)\ncommon_vol = vol1.index.intersection(vol2.index)\n\naxes[1].plot(common_vol, vol1[common_vol], color='#C0392B',\n             linewidth=1, label='Vietnam', alpha=0.8)\naxes[1].plot(common_vol, vol2[common_vol], color='#2C5F8A',\n             linewidth=1, label='World', alpha=0.8)\naxes[1].set_ylabel('Annualized Cond. Vol')\naxes[1].set_title('Panel B: Conditional Volatility')\naxes[1].legend(fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 38.2\n\n\n\n\n\n38.3.3 Asymmetric Integration\nIntegration may be state-dependent: co-movement often increases during global crises (contagion) but not during local booms. Longin and Solnik (2001) and Ang and Chen (2002) show that equity correlations are higher during market downturns. We test for asymmetry:\n\n# Classify global market regimes\nworld_ret_aligned = world_ret[common_dates]\nvn_ret_aligned = vn_ret[common_dates]\n\n# Bear: world return in bottom 25th percentile\n# Bull: world return in top 25th percentile\nq25 = world_ret_aligned.quantile(0.25)\nq75 = world_ret_aligned.quantile(0.75)\n\nbear = world_ret_aligned &lt;= q25\nbull = world_ret_aligned &gt;= q75\nnormal = ~bear & ~bull\n\nregimes = {\n    'Bear (bottom 25%)': bear,\n    'Normal (middle 50%)': normal,\n    'Bull (top 25%)': bull,\n    'All': pd.Series(True, index=world_ret_aligned.index)\n}\n\nprint(\"Asymmetric Correlation:\")\nprint(f\"{'Regime':&lt;25} {'Correlation':&gt;12} {'N months':&gt;10}\")\nprint(\"-\" * 47)\n\nfor name, mask in regimes.items():\n    r_vn = vn_ret_aligned[mask]\n    r_w = world_ret_aligned[mask]\n    corr = r_vn.corr(r_w)\n    print(f\"{name:&lt;25} {corr:&gt;12.3f} {mask.sum():&gt;10}\")\n\n# Test: is bear correlation &gt; bull correlation?\nr_bear_vn = vn_ret_aligned[bear]\nr_bear_w = world_ret_aligned[bear]\nr_bull_vn = vn_ret_aligned[bull]\nr_bull_w = world_ret_aligned[bull]\n\n# Fisher z-transformation test\ndef fisher_z_test(r1, n1, r2, n2):\n    z1 = np.arctanh(r1)\n    z2 = np.arctanh(r2)\n    se = np.sqrt(1 / (n1 - 3) + 1 / (n2 - 3))\n    z_stat = (z1 - z2) / se\n    p_val = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n    return z_stat, p_val\n\nz, p = fisher_z_test(\n    r_bear_vn.corr(r_bear_w), len(r_bear_vn),\n    r_bull_vn.corr(r_bull_w), len(r_bull_vn)\n)\nprint(f\"\\nFisher z-test (bear vs bull): z = {z:.2f}, p = {p:.4f}\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-factor",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-factor",
    "title": "38  Market Integration and Segmentation",
    "section": "38.4 Factor-Based Integration Measures",
    "text": "38.4 Factor-Based Integration Measures\n\n38.4.1 The Pukthuanthong-Roll R² Measure\nPukthuanthong and Roll (2009) propose measuring integration as the \\(R^2\\) from regressing a country’s returns on a set of global principal components. The intuition: if a market is fully integrated, global factors should explain all of its systematic return variation.\n\ndef pukthuanthong_roll_integration(country_returns, global_returns_matrix,\n                                     n_components=10, rolling_window=36):\n    \"\"\"\n    Pukthuanthong-Roll (2009) R²-based integration measure.\n    \n    1. Extract principal components from global returns.\n    2. Regress country returns on these PCs.\n    3. R² = degree of integration.\n    \"\"\"\n    common = country_returns.dropna().index.intersection(\n        global_returns_matrix.dropna().index\n    )\n    \n    dates = sorted(common)\n    T = len(dates)\n    \n    integration = []\n    \n    for t in range(rolling_window, T):\n        window = dates[t - rolling_window:t]\n        \n        # Global returns in window\n        G = global_returns_matrix.loc[window].dropna(axis=1)\n        if G.shape[1] &lt; n_components:\n            continue\n        \n        # Standardize\n        G_std = (G - G.mean()) / G.std()\n        \n        # PCA\n        cov = G_std.T @ G_std / len(G_std)\n        eigenvalues, eigenvectors = np.linalg.eigh(cov.values)\n        \n        # Sort descending\n        idx = np.argsort(-eigenvalues)\n        eigenvalues = eigenvalues[idx]\n        eigenvectors = eigenvectors[:, idx]\n        \n        # Project onto top K PCs\n        PCs = G_std.values @ eigenvectors[:, :n_components]\n        \n        # Regress country returns on PCs\n        y = country_returns.loc[window].values\n        X = sm.add_constant(PCs)\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            integration.append({\n                'date': dates[t],\n                'r_squared': model.rsquared,\n                'adj_r_squared': model.rsquared_adj,\n                'var_explained_pc1': eigenvalues[0] / eigenvalues.sum(),\n                'n_countries': G.shape[1]\n            })\n        except Exception:\n            pass\n    \n    return pd.DataFrame(integration)\n\n# Build global returns matrix from multiple country indices\nglobal_matrix = global_indices.drop(columns=['VIETNAM'], errors='ignore')\n\npr_result = pukthuanthong_roll_integration(\n    indices_aligned['VIETNAM'],\n    global_matrix,\n    n_components=5,\n    rolling_window=36\n)\n\nif len(pr_result) &gt; 0:\n    pr_result['date'] = pd.to_datetime(pr_result['date'])\n    print(f\"Pukthuanthong-Roll Integration (R²):\")\n    print(f\"  Mean: {pr_result['r_squared'].mean():.3f}\")\n    print(f\"  2008-2012: {pr_result[(pr_result['date'] &gt;= '2008') & (pr_result['date'] &lt; '2013')]['r_squared'].mean():.3f}\")\n    print(f\"  2013-2018: {pr_result[(pr_result['date'] &gt;= '2013') & (pr_result['date'] &lt; '2019')]['r_squared'].mean():.3f}\")\n    print(f\"  2019-2024: {pr_result[pr_result['date'] &gt;= '2019']['r_squared'].mean():.3f}\")\n\n\n\n38.4.2 Global vs. Local Factor Pricing\nGriffin (2002) tests whether global or local versions of the Fama-French factors better explain country-level returns. We implement this horse race for Vietnam:\n\n# Align global and local factors\ncommon_factor_dates = (\n    global_factors.index\n    .intersection(local_factors.index)\n    .intersection(vn_index.index)\n)\n\nvn_excess = vn_index.loc[common_factor_dates, 'return']\n\n# Model 1: Global FF5\nX_global = sm.add_constant(\n    global_factors.loc[common_factor_dates,\n                        ['mkt_excess_world', 'smb_world', 'hml_world',\n                         'rmw_world', 'cma_world']]\n)\nmodel_global = sm.OLS(vn_excess, X_global).fit(\n    cov_type='HAC', cov_kwds={'maxlags': 6}\n)\n\n# Model 2: Local FF5\nX_local = sm.add_constant(\n    local_factors.loc[common_factor_dates,\n                       ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']]\n)\nmodel_local = sm.OLS(vn_excess, X_local).fit(\n    cov_type='HAC', cov_kwds={'maxlags': 6}\n)\n\n# Model 3: Both global and local\nX_both = sm.add_constant(pd.concat([\n    global_factors.loc[common_factor_dates,\n                        ['mkt_excess_world', 'smb_world', 'hml_world']],\n    local_factors.loc[common_factor_dates,\n                       ['mkt_excess', 'smb', 'hml']]\n], axis=1))\nmodel_both = sm.OLS(vn_excess, X_both).fit(\n    cov_type='HAC', cov_kwds={'maxlags': 6}\n)\n\nprint(\"Global vs Local Factor Models for VN-Index:\")\nprint(f\"{'Model':&lt;20} {'R²':&gt;8} {'Adj R²':&gt;8} {'α (ann)':&gt;10} {'α t-stat':&gt;10}\")\nprint(\"-\" * 56)\nfor name, mod in [('Global FF5', model_global),\n                    ('Local FF5', model_local),\n                    ('Global + Local', model_both)]:\n    print(f\"{name:&lt;20} {mod.rsquared:&gt;8.3f} {mod.rsquared_adj:&gt;8.3f} \"\n          f\"{mod.params['const']*12:&gt;10.4f} {mod.tvalues['const']:&gt;10.2f}\")\n\n\n\n\nrolling_r2 = []\nrw = 36\n\nfor t in range(rw, len(common_factor_dates)):\n    window = common_factor_dates[t - rw:t]\n    y = vn_excess[window]\n    \n    # Global\n    X_g = sm.add_constant(global_factors.loc[window,\n                           ['mkt_excess_world', 'smb_world', 'hml_world',\n                            'rmw_world', 'cma_world']])\n    try:\n        r2_g = sm.OLS(y, X_g).fit().rsquared\n    except:\n        r2_g = np.nan\n    \n    # Local\n    X_l = sm.add_constant(local_factors.loc[window,\n                           ['mkt_excess', 'smb', 'hml', 'rmw', 'cma']])\n    try:\n        r2_l = sm.OLS(y, X_l).fit().rsquared\n    except:\n        r2_l = np.nan\n    \n    rolling_r2.append({\n        'date': common_factor_dates[t],\n        'r2_global': r2_g,\n        'r2_local': r2_l,\n        'ratio': r2_g / r2_l if r2_l &gt; 0 else np.nan\n    })\n\nr2_df = pd.DataFrame(rolling_r2)\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n\naxes[0].plot(r2_df['date'], r2_df['r2_global'], color='#2C5F8A',\n             linewidth=1.5, label='Global FF5')\naxes[0].plot(r2_df['date'], r2_df['r2_local'], color='#C0392B',\n             linewidth=1.5, label='Local FF5')\naxes[0].set_ylabel('R²')\naxes[0].set_title('Panel A: Global vs Local Factor R²')\naxes[0].legend()\n\naxes[1].plot(r2_df['date'], r2_df['ratio'], color='#27AE60', linewidth=1.5)\naxes[1].axhline(y=1, color='gray', linewidth=1, linestyle='--',\n                label='Full integration (ratio = 1)')\naxes[1].set_ylabel('Global R² / Local R²')\naxes[1].set_title('Panel B: Integration Ratio')\naxes[1].legend()\naxes[1].set_ylim([0, 1.5])\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 38.3",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-pricing-error",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-pricing-error",
    "title": "38  Market Integration and Segmentation",
    "section": "38.5 Pricing-Error-Based Integration",
    "text": "38.5 Pricing-Error-Based Integration\n\n38.5.1 The Bekaert-Harvey Approach\nBekaert and Harvey (1995) measure integration as the ability of a global CAPM to price local assets. Under integration, the local market alpha (intercept) in a regression on the global market should be zero, and the global risk premium should explain the local expected return. Under segmentation, the local alpha captures the segmentation premium.\n\ndef bekaert_harvey_integration(local_return, global_return,\n                                 rolling_window=36):\n    \"\"\"\n    Rolling alpha from regressing local on global market.\n    Under integration, alpha -&gt; 0.\n    \"\"\"\n    common = local_return.dropna().index.intersection(global_return.dropna().index)\n    \n    results = []\n    for t in range(rolling_window, len(common)):\n        window = common[t - rolling_window:t]\n        y = local_return[window]\n        X = sm.add_constant(global_return[window])\n        \n        model = sm.OLS(y, X).fit()\n        \n        results.append({\n            'date': common[t],\n            'alpha': model.params['const'],\n            'alpha_t': model.tvalues['const'],\n            'beta_global': model.params.iloc[1],\n            'r_squared': model.rsquared\n        })\n    \n    return pd.DataFrame(results)\n\nbh_result = bekaert_harvey_integration(\n    indices_aligned['VIETNAM'],\n    indices_aligned['MSCI_WORLD'],\n    rolling_window=36\n)\n\n# The absolute alpha is the segmentation premium\nbh_result['abs_alpha_ann'] = bh_result['alpha'].abs() * 12\n\nprint(\"Bekaert-Harvey Integration Diagnostic:\")\nprint(f\"  Mean |α| (ann.): {bh_result['abs_alpha_ann'].mean():.4f}\")\nprint(f\"  Mean β_world: {bh_result['beta_global'].mean():.3f}\")\nprint(f\"  Mean R²: {bh_result['r_squared'].mean():.3f}\")\n\n\n\n38.5.2 Composite Integration Index\nWe combine all measures into a single composite index of Vietnamese market integration:\n\n# Standardize each measure to [0, 1] using historical percentile ranks\nmeasures = pd.DataFrame(index=bh_result['date'])\n\n# 1. DCC correlation (higher = more integrated)\ndcc_aligned = dcc_result['dcc_corr'].reindex(measures.index).interpolate()\nmeasures['dcc_corr'] = dcc_aligned\n\n# 2. PR R² (higher = more integrated)\npr_aligned = pr_result.set_index('date')['r_squared'].reindex(measures.index).interpolate()\nmeasures['pr_r2'] = pr_aligned\n\n# 3. Global/Local R² ratio (higher = more integrated)\nr2_aligned = r2_df.set_index('date')['ratio'].reindex(measures.index).interpolate()\nmeasures['gl_ratio'] = r2_aligned\n\n# 4. |Alpha| from global CAPM (lower = more integrated)\n# Invert: 1 - percentile rank of |alpha|\nmeasures['inv_alpha'] = bh_result.set_index('date')['abs_alpha_ann']\nmeasures['inv_alpha'] = 1 - measures['inv_alpha'].rank(pct=True)\n\n# 5. Global beta (higher = more integrated, up to a point)\nmeasures['global_beta'] = bh_result.set_index('date')['beta_global']\n\n# Standardize to percentile ranks\nfor col in ['dcc_corr', 'pr_r2', 'gl_ratio', 'inv_alpha', 'global_beta']:\n    measures[f'{col}_rank'] = measures[col].rank(pct=True)\n\n# Composite = equal-weighted average of ranks\nrank_cols = [c for c in measures.columns if c.endswith('_rank')]\nmeasures['composite'] = measures[rank_cols].mean(axis=1)\n\n# Smooth with 6-month moving average\nmeasures['composite_smooth'] = measures['composite'].rolling(6).mean()\n\n\n\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\nax.fill_between(measures.index, measures['composite_smooth'],\n                 alpha=0.3, color='#2C5F8A')\nax.plot(measures.index, measures['composite_smooth'],\n        color='#2C5F8A', linewidth=2)\n\n# Add event markers\nfor _, event in liberalization_events.iterrows():\n    if event['date'] &gt;= measures.index.min():\n        color = event_colors.get(event['category'], 'gray')\n        ax.axvline(x=event['date'], color=color,\n                   linewidth=1.5, linestyle='--', alpha=0.6)\n\nax.set_ylabel('Integration Index (0 = segmented, 1 = integrated)')\nax.set_title('Vietnam Equity Market Integration: Composite Index')\nax.set_ylim([0, 1])\n\n# Legend for event categories\nfrom matplotlib.patches import Patch\nlegend_patches = [Patch(facecolor=c, label=cat)\n                   for cat, c in event_colors.items() if cat in\n                   liberalization_events['category'].values]\nax.legend(handles=legend_patches, fontsize=7, loc='lower right', ncol=2)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 38.4",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-breaks",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-breaks",
    "title": "38  Market Integration and Segmentation",
    "section": "38.6 Structural Break Detection",
    "text": "38.6 Structural Break Detection\n\n38.6.1 Bai-Perron Tests for Integration Regime Shifts\nWe test whether Vietnam’s integration trajectory contains discrete structural breaks (i.e., sudden shifts in the integration level) rather than a smooth trend:\n\ndef detect_breaks_cusum(series, significance=0.05):\n    \"\"\"\n    CUSUM-based structural break detection.\n    \"\"\"\n    y = series.dropna().values\n    T = len(y)\n    \n    # Recursive residuals from rolling mean\n    cumsum = np.cumsum(y - y.mean()) / (y.std() * np.sqrt(T))\n    \n    # Brown-Durbin-Evans critical values (approximate)\n    # At 5%: ±0.948\n    critical = 0.948\n    \n    breaks = []\n    for t in range(1, T - 1):\n        if abs(cumsum[t]) &gt; critical:\n            breaks.append(t)\n    \n    return cumsum, breaks\n\n# Apply to composite index\ncomposite_clean = measures['composite_smooth'].dropna()\ncusum, break_points = detect_breaks_cusum(composite_clean)\n\n# Alternative: Chow test at key liberalization dates\ndef chow_test(y, breakpoint_idx):\n    \"\"\"Simple Chow test for structural break.\"\"\"\n    T = len(y)\n    y1 = y[:breakpoint_idx]\n    y2 = y[breakpoint_idx:]\n    \n    # Full sample regression (on constant)\n    rss_full = np.sum((y - y.mean()) ** 2)\n    \n    # Split samples\n    rss1 = np.sum((y1 - y1.mean()) ** 2)\n    rss2 = np.sum((y2 - y2.mean()) ** 2)\n    rss_split = rss1 + rss2\n    \n    k = 1  # Number of parameters\n    f_stat = ((rss_full - rss_split) / k) / (rss_split / (T - 2 * k))\n    p_val = 1 - stats.f.cdf(f_stat, k, T - 2 * k)\n    \n    return f_stat, p_val\n\nprint(\"Chow Tests for Structural Breaks at Key Dates:\")\nprint(f\"{'Event':&lt;30} {'Date':&gt;12} {'F-stat':&gt;10} {'p-value':&gt;10}\")\nprint(\"-\" * 62)\n\nfor _, event in liberalization_events.iterrows():\n    if event['date'] &lt; composite_clean.index.min():\n        continue\n    # Find nearest date\n    nearest = composite_clean.index.searchsorted(event['date'])\n    if nearest &lt; 12 or nearest &gt; len(composite_clean) - 12:\n        continue\n    \n    f_stat, p_val = chow_test(composite_clean.values, nearest)\n    sig = '***' if p_val &lt; 0.01 else '**' if p_val &lt; 0.05 else '*' if p_val &lt; 0.1 else ''\n    print(f\"{event['event']:&lt;30} {event['date'].strftime('%Y-%m'):&gt;12} \"\n          f\"{f_stat:&gt;10.2f} {p_val:&gt;10.4f} {sig}\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-premium",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-premium",
    "title": "38  Market Integration and Segmentation",
    "section": "38.7 The Segmentation Premium for Vietnam",
    "text": "38.7 The Segmentation Premium for Vietnam\n\n38.7.1 Cross-Sectional Evidence\nUnder partial segmentation, stocks with higher foreign ownership should have lower expected returns (because foreign investors can diversify away local risk). This yields a testable prediction: foreign ownership should be negatively associated with expected returns, controlling for global risk exposure.\n\n# Get monthly stock returns with foreign ownership\nstock_data = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2008-01-01',\n    end_date='2024-12-31',\n    fields=['ticker', 'month_end', 'monthly_return', 'market_cap',\n            'foreign_ownership_pct']\n)\nstock_data['month_end'] = pd.to_datetime(stock_data['month_end'])\n\n# Fama-MacBeth: regress returns on lagged foreign ownership\nstock_data = stock_data.sort_values(['ticker', 'month_end'])\nstock_data['fol_lag'] = (\n    stock_data.groupby('ticker')['foreign_ownership_pct'].shift(1)\n)\nstock_data['log_mcap'] = np.log(stock_data['market_cap'].clip(lower=1))\n\n# Monthly cross-sectional regressions\ngamma_fol = []\nfor month, group in stock_data.dropna(subset=['fol_lag', 'monthly_return']).groupby('month_end'):\n    if len(group) &lt; 100:\n        continue\n    \n    y = group['monthly_return'].values\n    X = sm.add_constant(group[['fol_lag', 'log_mcap']].values)\n    \n    try:\n        model = sm.OLS(y, X).fit()\n        gamma_fol.append({\n            'month': month,\n            'gamma_fol': model.params[1],\n            'gamma_size': model.params[2],\n            'n': len(group)\n        })\n    except:\n        pass\n\ngamma_fol_df = pd.DataFrame(gamma_fol)\n\nmean_gamma = gamma_fol_df['gamma_fol'].mean()\nse_gamma = gamma_fol_df['gamma_fol'].std() / np.sqrt(len(gamma_fol_df))\nt_gamma = mean_gamma / se_gamma\n\nprint(f\"Fama-MacBeth: Foreign Ownership and Expected Returns\")\nprint(f\"  γ_FOL (monthly):  {mean_gamma:.6f}\")\nprint(f\"  γ_FOL (ann.):     {mean_gamma * 12:.4f}\")\nprint(f\"  t-statistic:      {t_gamma:.2f}\")\nprint(f\"  Interpretation:   A 10pp increase in foreign ownership is \"\n      f\"associated with a {mean_gamma * 12 * 10:.2f}% change in \"\n      f\"annual expected returns\")\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Quintile returns by foreign ownership\nfol_quintiles = stock_data.dropna(subset=['fol_lag', 'monthly_return']).copy()\nfol_quintiles['fol_q'] = (\n    fol_quintiles.groupby('month_end')['fol_lag']\n    .transform(lambda x: pd.qcut(x.rank(method='first'), 5,\n                                    labels=['Q1\\n(Low FOL)', 'Q2', 'Q3', 'Q4',\n                                            'Q5\\n(High FOL)']))\n)\n\nq_returns = (\n    fol_quintiles.groupby('fol_q')['monthly_return']\n    .mean() * 12 * 100\n)\n\ncolors_q = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, 5))\naxes[0].bar(range(5), q_returns.values, color=colors_q,\n            edgecolor='white', alpha=0.85)\naxes[0].set_xticks(range(5))\naxes[0].set_xticklabels(q_returns.index)\naxes[0].set_ylabel('Ann. Return (%)')\naxes[0].set_title('Panel A: Returns by Foreign Ownership Quintile')\naxes[0].axhline(y=0, color='black', linewidth=0.5)\n\n# Panel B: Rolling FM coefficient\ngamma_fol_df['date'] = pd.to_datetime(gamma_fol_df['month'])\nrolling_gamma = gamma_fol_df.set_index('date')['gamma_fol'].rolling(24).mean() * 12\n\naxes[1].plot(rolling_gamma.index, rolling_gamma.values,\n             color='#2C5F8A', linewidth=1.5)\naxes[1].axhline(y=0, color='black', linewidth=0.5)\naxes[1].set_ylabel('γ_FOL (annualized)')\naxes[1].set_title('Panel B: Rolling Segmentation Premium')\naxes[1].fill_between(rolling_gamma.index, rolling_gamma.values, 0,\n                      where=rolling_gamma.values &lt; 0,\n                      alpha=0.3, color='#27AE60', label='Negative (integration)')\naxes[1].fill_between(rolling_gamma.index, rolling_gamma.values, 0,\n                      where=rolling_gamma.values &gt;= 0,\n                      alpha=0.3, color='#C0392B', label='Positive (segmentation)')\naxes[1].legend(fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 38.5",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-fx",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-fx",
    "title": "38  Market Integration and Segmentation",
    "section": "38.8 Exchange Rate Risk and Integration",
    "text": "38.8 Exchange Rate Risk and Integration\n\n38.8.1 Is Currency Risk Priced?\nIn a partially integrated market, exchange rate risk may carry a separate premium. Jorion (1991) and Dumas and Solnik (1995) test whether currency exposure is priced beyond global equity risk. For Vietnam, the VND/USD exchange rate is managed (a crawling peg with occasional step devaluations), creating a specific risk that is neither fully diversifiable nor fully priced by global equity factors.\n\n# VND depreciation\nfx_return = fx['rate'].pct_change()\nfx_return.name = 'fx_return'\n\n# Merge with stock data\nstock_fx = stock_data.merge(\n    fx_return.to_frame().reset_index().rename(columns={'date': 'month_end'}),\n    on='month_end', how='left'\n)\n\n# Estimate FX beta for each stock (rolling 60-month)\n# Then test in Fama-MacBeth whether FX beta is priced\nfx_betas = {}\nfor ticker, group in stock_fx.groupby('ticker'):\n    if len(group) &lt; 60:\n        continue\n    group = group.sort_values('month_end')\n    y = group['monthly_return']\n    x = group[['fx_return']].dropna()\n    common = y.dropna().index.intersection(x.index)\n    if len(common) &lt; 48:\n        continue\n    model = sm.OLS(y[common], sm.add_constant(x.loc[common])).fit()\n    fx_betas[ticker] = model.params.get('fx_return', np.nan)\n\nfx_beta_series = pd.Series(fx_betas, name='fx_beta')\n\n# Cross-sectional test: do stocks with higher FX beta earn different returns?\nstock_fx_beta = stock_fx.merge(\n    fx_beta_series.to_frame().reset_index().rename(columns={'index': 'ticker'}),\n    on='ticker', how='left'\n)\n\n# Quintile sort on FX beta\nfx_q = stock_fx_beta.dropna(subset=['fx_beta', 'monthly_return'])\nfx_q['fx_quintile'] = pd.qcut(fx_q['fx_beta'].rank(method='first'),\n                                 5, labels=False)\n\nfx_premium = fx_q.groupby('fx_quintile')['monthly_return'].mean() * 12\nprint(\"Returns by FX Beta Quintile:\")\nfor q, ret in fx_premium.items():\n    print(f\"  Q{q+1}: {ret*100:.2f}% ann.\")\nprint(f\"  Q5-Q1: {(fx_premium.iloc[-1] - fx_premium.iloc[0])*100:.2f}% ann.\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-contagion",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-contagion",
    "title": "38  Market Integration and Segmentation",
    "section": "38.9 Contagion vs. Interdependence",
    "text": "38.9 Contagion vs. Interdependence\nDuring global crises, correlations between Vietnam and world markets spike. The question is whether this represents contagion (a structural change in the transmission mechanism) or simply interdependence (normal co-movement amplified by higher volatility). Longin and Solnik (2001) show that correlation increases mechanically with volatility even without any change in the underlying dependence structure.\n\ndef forbes_rigobon_test(r_local, r_global, crisis_dates, tranquil_dates):\n    \"\"\"\n    Forbes-Rigobon (2002) contagion test.\n    Adjusts for heteroskedasticity-induced bias in correlation.\n    \n    H0: No contagion (correlation increase is explained by volatility)\n    H1: Contagion (correlation increase exceeds what volatility explains)\n    \"\"\"\n    r_l_crisis = r_local[crisis_dates]\n    r_g_crisis = r_global[crisis_dates]\n    r_l_tranquil = r_local[tranquil_dates]\n    r_g_tranquil = r_global[tranquil_dates]\n    \n    # Unadjusted correlations\n    rho_crisis = r_l_crisis.corr(r_g_crisis)\n    rho_tranquil = r_l_tranquil.corr(r_g_tranquil)\n    \n    # Volatility ratio\n    delta = r_g_crisis.var() / r_g_tranquil.var() - 1\n    \n    # Adjusted correlation\n    rho_adj = rho_crisis / np.sqrt(1 + delta * (1 - rho_crisis ** 2))\n    \n    # Fisher z-test on adjusted vs tranquil\n    z_adj = np.arctanh(rho_adj)\n    z_tranquil = np.arctanh(rho_tranquil)\n    se = np.sqrt(1 / (len(r_l_crisis) - 3) + 1 / (len(r_l_tranquil) - 3))\n    z_stat = (z_adj - z_tranquil) / se\n    p_val = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n    \n    return {\n        'rho_crisis_raw': rho_crisis,\n        'rho_crisis_adj': rho_adj,\n        'rho_tranquil': rho_tranquil,\n        'delta': delta,\n        'z_stat': z_stat,\n        'p_value': p_val,\n        'contagion': p_val &lt; 0.05\n    }\n\n# Define crisis and tranquil periods\ncrises = {\n    'GFC (2008-09)': (pd.Timestamp('2008-09-01'), pd.Timestamp('2009-03-31')),\n    'European Debt (2011-12)': (pd.Timestamp('2011-06-01'), pd.Timestamp('2012-06-30')),\n    'COVID (2020)': (pd.Timestamp('2020-02-01'), pd.Timestamp('2020-06-30')),\n    'Fed Tightening (2022)': (pd.Timestamp('2022-01-01'), pd.Timestamp('2022-12-31')),\n}\n\n# Tranquil = 24 months before each crisis\nvn_aligned = indices_aligned['VIETNAM']\nworld_aligned = indices_aligned['MSCI_WORLD']\n\nprint(\"Contagion Tests (Forbes-Rigobon):\")\nprint(f\"{'Crisis':&lt;28} {'ρ(raw)':&gt;8} {'ρ(adj)':&gt;8} {'ρ(calm)':&gt;8} \"\n      f\"{'z-stat':&gt;8} {'p-val':&gt;8} {'Result':&gt;12}\")\nprint(\"-\" * 80)\n\nfor name, (start, end) in crises.items():\n    crisis_mask = (vn_aligned.index &gt;= start) & (vn_aligned.index &lt;= end)\n    tranquil_start = start - pd.DateOffset(months=24)\n    tranquil_mask = ((vn_aligned.index &gt;= tranquil_start) &\n                      (vn_aligned.index &lt; start))\n    \n    crisis_dates = vn_aligned.index[crisis_mask]\n    tranquil_dates = vn_aligned.index[tranquil_mask]\n    \n    if len(crisis_dates) &lt; 3 or len(tranquil_dates) &lt; 12:\n        continue\n    \n    result = forbes_rigobon_test(vn_aligned, world_aligned,\n                                  crisis_dates, tranquil_dates)\n    \n    verdict = 'CONTAGION' if result['contagion'] else 'Interdependence'\n    print(f\"{name:&lt;28} {result['rho_crisis_raw']:&gt;8.3f} \"\n          f\"{result['rho_crisis_adj']:&gt;8.3f} {result['rho_tranquil']:&gt;8.3f} \"\n          f\"{result['z_stat']:&gt;8.2f} {result['p_value']:&gt;8.3f} \"\n          f\"{verdict:&gt;12}\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-asean",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-asean",
    "title": "38  Market Integration and Segmentation",
    "section": "38.10 ASEAN Peer Comparison",
    "text": "38.10 ASEAN Peer Comparison\nVietnam’s integration trajectory is best understood in the context of its ASEAN peers, which share similar starting conditions but have followed different liberalization paths:\n\n\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\nasean_markets = {\n    'VIETNAM': '#C0392B',\n    'MSCI_THAILAND': '#2C5F8A',\n    'MSCI_INDONESIA': '#27AE60',\n    'MSCI_PHILIPPINES': '#E67E22',\n    'MSCI_MALAYSIA': '#8E44AD'\n}\n\nfor market, color in asean_markets.items():\n    if market not in global_indices.columns:\n        continue\n    \n    corr = (\n        global_indices[['MSCI_WORLD', market]]\n        .rolling(36)\n        .corr()\n        .unstack()['MSCI_WORLD'][market]\n    )\n    \n    label = market.replace('MSCI_', '').replace('_', ' ').title()\n    if market == 'VIETNAM':\n        ax.plot(corr.index, corr.values, color=color,\n                linewidth=2.5, label=label, zorder=5)\n    else:\n        ax.plot(corr.index, corr.values, color=color,\n                linewidth=1.5, label=label, alpha=0.7)\n\nax.set_ylabel('Correlation with MSCI World')\nax.set_title('ASEAN Market Integration: Rolling 36-Month Correlation')\nax.legend(fontsize=9)\nax.set_ylim([-0.2, 0.9])\nax.axhline(y=0, color='black', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 38.6",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-implications",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-implications",
    "title": "38  Market Integration and Segmentation",
    "section": "38.11 Practical Implications",
    "text": "38.11 Practical Implications\nThe degree of integration determines which asset pricing model is appropriate for Vietnamese equities. The evidence in this chapter supports several practical conclusions:\nVietnam is partially integrated and trending toward integration. The composite index shows a clear upward trajectory, with the post-2015 period representing the highest sustained integration in the market’s history. However, Vietnam remains less integrated than Thailand or Malaysia, and far from fully integrated with global markets.\nLocal factors dominate global factors for pricing Vietnamese stocks. The rolling \\(R^2\\) comparison shows that local Vietnamese factors consistently explain more return variation than global factors. This means that researchers studying Vietnamese cross-sectional returns should use local factor models (Vietnamese FF5) rather than global factors. Global factors are useful primarily for international investors assessing co-movement risk.\nThe segmentation premium is shrinking but not zero. The Fama-MacBeth evidence shows that stocks with higher foreign ownership earn lower returns, consistent with partial segmentation. The magnitude has declined over time as foreign ownership limits have been relaxed, but a residual premium persists—likely driven by remaining ownership caps in banking and strategic sectors.\nCrisis-period co-movement is mostly interdependence, not contagion. The Forbes-Rigobon adjusted correlations show that the spike in Vietnam-World correlation during crises is largely explained by increased global volatility, not a structural change in the transmission mechanism. This is reassuring for diversification: Vietnam continues to offer meaningful diversification benefits even during global stress.\nThe FTSE/MSCI upgrade path matters. Vietnam’s potential upgrade from frontier to emerging market status would trigger mandatory index rebalancing by passive funds, increasing foreign flows and likely accelerating integration. Researchers and investors should monitor upgrade criteria and their implications for the cost of capital.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "56_market_integration_and_segmentation.html#sec-market-integration-summary",
    "href": "56_market_integration_and_segmentation.html#sec-market-integration-summary",
    "title": "38  Market Integration and Segmentation",
    "section": "38.12 Summary",
    "text": "38.12 Summary\n\n\n\nTable 38.1: Summary of integration measures for the Vietnamese equity market.\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nWhat It Captures\nVietnam Range\nCurrent Level\nTrend\n\n\n\n\nDCC correlation (World)\nCo-movement\n0.0–0.5\n~0.35–0.45\nRising\n\n\nPR R² (global PCs)\nGlobal factor exposure\n0.05–0.50\n~0.30–0.40\nRising\n\n\nGlobal/Local R² ratio\nRelative pricing power\n0.1–0.8\n~0.5–0.6\nRising\n\n\nGlobal CAPM α\nPricing error\n0–15% ann.\n~3–5% ann.\nFalling\n\n\nFOL premium (γ)\nSegmentation cost\n-5% to +2%\n~-1% to 0%\nShrinking\n\n\n\n\n\n\n\n\n\n\n\n\n\nAng, Andrew, and Joseph Chen. 2002. “Asymmetric Correlations of Equity Portfolios.” Journal of Financial Economics 63 (3): 443–94.\n\n\nBekaert, Geert, and Campbell R Harvey. 1995. “Time-Varying World Market Integration.” The Journal of Finance 50 (2): 403–44.\n\n\n———. 2002. “Research in Emerging Markets Finance: Looking to the Future.” Emerging Markets Review 3 (4): 429–48.\n\n\nBekaert, Geert, Campbell R Harvey, and Christian Lundblad. 2005. “Does Financial Liberalization Spur Growth?” Journal of Financial Economics 77 (1): 3–55.\n\n\nDumas, Bernard, and Bruno Solnik. 1995. “The World Price of Foreign Exchange Risk.” The Journal of Finance 50 (2): 445–79.\n\n\nEngle, Robert. 2002. “Dynamic Conditional Correlation: A Simple Class of Multivariate Generalized Autoregressive Conditional Heteroskedasticity Models.” Journal of Business & Economic Statistics 20 (3): 339–50.\n\n\nErrunza, Vihang, and Etienne Losq. 1985. “International Asset Pricing Under Mild Segmentation: Theory and Test.” The Journal of Finance 40 (1): 105–24.\n\n\nGriffin, John M. 2002. “Are the Fama and French Factors Global or Country Specific?” The Review of Financial Studies 15 (3): 783–803.\n\n\nHenry, Peter Blair. 2000. “Stock Market Liberalization, Economic Reform, and Emerging Market Equity Prices.” The Journal of Finance 55 (2): 529–64.\n\n\nJorion, Philippe. 1991. “The Pricing of Exchange Rate Risk in the Stock Market.” Journal of Financial and Quantitative Analysis 26 (3): 363–76.\n\n\nLongin, Francois, and Bruno Solnik. 2001. “Extreme Correlation of International Equity Markets.” The Journal of Finance 56 (2): 649–76.\n\n\nPukthuanthong, Kuntara, and Richard Roll. 2009. “Global Market Integration: An Alternative Measure and Its Application.” Journal of Financial Economics 94 (2): 214–32.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Market Integration and Segmentation</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html",
    "href": "57_exchange_rate.html",
    "title": "39  Exchange Rate Dynamics",
    "section": "",
    "text": "39.1 Theoretical Foundations\nExchange rates sit at the intersection of macroeconomics and finance. For international investors considering Vietnamese equities, the VND/USD exchange rate is not a secondary concern; it is a first-order determinant of dollar-denominated returns. A Vietnamese stock portfolio that earns 15% in VND terms but coincides with a 5% VND depreciation delivers only about 10% in USD terms. Conversely, periods of VND stability or appreciation amplify returns for foreign investors. Understanding exchange rate dynamics is therefore essential for anyone working with Vietnamese financial data in an international context.\nVietnam presents a distinctive exchange rate regime. Unlike freely floating currencies such as the USD/EUR or USD/JPY, the VND operates under a managed float. The State Bank of Vietnam (SBV) sets a daily reference rate and allows trading within a band (currently \\(\\pm\\) 3% on the interbank market and \\(\\pm\\) 5% against the official rate for commercial banks). This regime creates specific patterns in VND returns that differ fundamentally from those observed in freely floating pairs: clustered movements near band boundaries, discrete adjustments when the SBV shifts the reference rate, and periods of near-zero volatility punctuated by sharp moves during policy changes.\nThis chapter develops tools for working with these dynamics. We progress from data construction through statistical characterization to economic applications: interest rate parity tests, firm-level exposure estimation, GARCH volatility modeling, and hedging strategy evaluation.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-theory",
    "href": "57_exchange_rate.html#sec-fx-theory",
    "title": "39  Exchange Rate Dynamics",
    "section": "",
    "text": "39.1.1 Exchange Rate Determination\nThe modern theory of exchange rate determination rests on several building blocks. Dornbusch (1976) provides the canonical “overshooting” model: because goods prices are sticky while asset prices adjust instantly, a monetary shock causes the exchange rate to overshoot its long-run equilibrium. Frankel (1979) extends this to a real interest differential model where the exchange rate depends on relative money supplies, income levels, and interest rates across countries.\nDespite elegant theory, Meese and Rogoff (1983) delivered a devastating empirical result: no structural model of exchange rates consistently outperforms a random walk in out-of-sample forecasting. This finding, confirmed repeatedly over four decades and surveyed by Rossi (2013), means that exchange rate changes are, to a first approximation, unpredictable. For practical purposes, the best forecast of tomorrow’s VND/USD rate is today’s rate.\n\n\n39.1.2 Interest Rate Parity\nTwo parity conditions link exchange rates to interest rates:\nCovered Interest Rate Parity (CIP): Arbitrage between spot and forward markets should equalize the forward premium with the interest rate differential:\n\\[\nF_{t,t+k} = S_t \\cdot \\frac{(1 + r^{VND}_{t,k})}{(1 + r^{USD}_{t,k})}\n\\tag{39.1}\\]\nwhere \\(S_t\\) is the spot rate (VND per USD), \\(F_{t,t+k}\\) is the \\(k\\)-period forward rate, and \\(r^{VND}_{t,k}\\) and \\(r^{USD}_{t,k}\\) are the domestic and foreign interest rates for maturity \\(k\\). CIP should hold exactly in the absence of transaction costs, credit risk, and capital controls. Du, Tepper, and Verdelhan (2018) document that CIP violations have widened significantly in major currency pairs since 2008, driven by post-crisis bank balance sheet constraints. In Vietnam, capital controls and limited forward market liquidity create additional CIP deviations.\nUncovered Interest Rate Parity (UIP): The expected depreciation should equal the interest rate differential:\n\\[\n\\mathbb{E}_t[\\Delta s_{t+k}] = r^{VND}_{t,k} - r^{USD}_{t,k}\n\\tag{39.2}\\]\nwhere \\(\\Delta s_{t+k} = \\ln(S_{t+k}/S_t)\\). UIP is an equilibrium condition, not an arbitrage condition, it requires risk-neutral investors. Fama (1984) famously showed that the forward premium predicts exchange rate changes with the wrong sign: high-interest-rate currencies tend to appreciate rather than depreciate, contradicting UIP. This “forward premium anomaly” is the foundation of the carry trade.\n\n\n39.1.3 The Carry Trade\nThe carry trade exploits UIP violations by borrowing in low-interest-rate currencies and investing in high-interest-rate currencies. Lustig and Verdelhan (2007) show that a portfolio of carry trade positions earns a significant risk premium, which they link to consumption growth risk. Menkhoff et al. (2012) identify global FX volatility as the key risk factor: carry trades lose money precisely when global volatility spikes, a “crash risk” documented by Brunnermeier, Nagel, and Pedersen (2008).\nVietnam, with interest rates typically 3-8 percentage points above U.S. rates, is a natural candidate for carry trade inflows. The managed exchange rate regime provides an additional attraction: the SBV’s implicit defense of the band reduces short-term volatility, increasing the Sharpe ratio of the carry position, until a band adjustment or devaluation erases accumulated gains in a single move.\n\n\n39.1.4 The Trilemma\nObstfeld, Shambaugh, and Taylor (2005) formalize the “impossible trinity”: a country cannot simultaneously maintain (i) a fixed exchange rate, (ii) an independent monetary policy, and (iii) free capital mobility. Vietnam navigates this trilemma by maintaining partial capital controls alongside its managed float, allowing some degree of monetary policy independence. Understanding where Vietnam sits on the trilemma at any given time is essential for interpreting exchange rate data: periods of tighter capital controls produce artificially low exchange rate volatility that does not reflect true economic uncertainty.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-regime",
    "href": "57_exchange_rate.html#sec-fx-regime",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.2 The VND Exchange Rate Regime",
    "text": "39.2 The VND Exchange Rate Regime\n\n39.2.1 Historical Evolution\nThe VND/USD exchange rate has evolved through several distinct phases:\n\n\n\nTable 39.1: Evolution of the VND/USD exchange rate regime.\n\n\n\n\n\n\n\n\n\n\nPeriod\nRegime\nKey Features\n\n\n\n\nPre-1999\nNarrow band\nSBV set the rate with minimal flexibility\n\n\n1999-2007\nGradual crawl\nSlow, controlled depreciation (~1-2% per year)\n\n\n2008-2011\nCrisis adjustments\nMultiple discrete devaluations (2008, 2010, 2011)\n\n\n2012-2015\nStabilization\nTight band, rare adjustments, building reserves\n\n\n2016-present\nReference rate mechanism\nDaily reference rate based on a basket; \\(\\pm\\) 3% band\n\n\n\n\n\n\nThe 2016 reform, shifting from a fixed peg to a daily reference rate influenced by a currency basket, was a significant structural change. The reference rate now incorporates the previous day’s closing interbank rate, supply-demand conditions, and movements in currencies of major trading partners (USD, EUR, CNY, JPY, KRW).\n\n\n39.2.2 Band Mechanics\nThe SBV announces a daily reference rate each morning. Commercial banks may quote rates within a band around this reference. The interbank market operates with a narrower \\(\\pm\\) 3% band. When the spot rate approaches the band ceiling, the SBV may intervene by selling USD from reserves, widening the band, or adjusting the reference rate.\nThis regime creates distinctive statistical properties:\n\nReturn clustering near zero: On most days, the VND/USD rate barely moves, producing a spike at zero in the return distribution.\nDiscrete jumps: Reference rate adjustments create large single-day moves that appear as outliers.\nBounded volatility: The band constrains daily moves, producing a return distribution with thinner tails than a freely floating currency.\nAsymmetric adjustment: Depreciation pressure (VND weakening) is more common than appreciation, reflecting persistent inflation differentials.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-data",
    "href": "57_exchange_rate.html#sec-fx-data",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.3 Data Construction",
    "text": "39.3 Data Construction\n\n39.3.1 Loading Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom scipy import stats\nfrom arch import arch_model\nfrom linearmodels.panel import PanelOLS\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams.update({\n    'figure.figsize': (12, 6),\n    'figure.dpi': 150,\n    'font.size': 11,\n    'axes.spines.top': False,\n    'axes.spines.right': False\n})\n\n\n\n39.3.2 Retrieving Exchange Rate Data\nWe extract daily spot rates for the VND against all major currencies, along with the SBV reference rate and forward rates where available.\n\nfrom datacore import DataCoreClient\n\nclient = DataCoreClient()\n\n# Spot exchange rates: VND per unit of foreign currency\nfx_spot = client.get_exchange_rates(\n    base_currency='VND',\n    quote_currencies=['USD', 'EUR', 'JPY', 'CNY', 'KRW',\n                      'SGD', 'THB', 'GBP', 'AUD'],\n    start_date='2005-01-01',\n    end_date='2024-12-31',\n    fields=['date', 'from_currency', 'to_currency', 'rate',\n            'bid', 'ask', 'reference_rate']\n)\n\n# Forward rates (VND/USD)\nfx_forward = client.get_fx_forwards(\n    pair='VND/USD',\n    start_date='2010-01-01',\n    end_date='2024-12-31',\n    tenors=['1M', '3M', '6M', '12M'],\n    fields=['date', 'tenor', 'forward_rate', 'forward_points']\n)\n\n# Interest rates for parity tests\ninterest_rates = client.get_interest_rates(\n    countries=['VN', 'US'],\n    start_date='2005-01-01',\n    end_date='2024-12-31',\n    instruments=['interbank_overnight', 'deposit_3m',\n                 'government_bond_1y', 'government_bond_10y']\n)\n\nprint(f\"Spot observations: {fx_spot.shape[0]:,}\")\nprint(f\"Forward observations: {fx_forward.shape[0]:,}\")\nprint(f\"Interest rate observations: {interest_rates.shape[0]:,}\")\n\n\n\n39.3.3 Constructing the VND/USD Time Series\nWe focus primarily on the VND/USD pair, the dominant bilateral rate for Vietnam, while using other pairs for cross-rate analysis.\n\n# Filter VND/USD\nvnd_usd = fx_spot[\n    (fx_spot['from_currency'] == 'VND') &\n    (fx_spot['to_currency'] == 'USD')\n].copy()\n\nvnd_usd['date'] = pd.to_datetime(vnd_usd['date'])\nvnd_usd = vnd_usd.sort_values('date').set_index('date')\n\n# Log returns: positive = VND depreciation, negative = VND appreciation\nvnd_usd['log_return'] = np.log(vnd_usd['rate'] / vnd_usd['rate'].shift(1))\n\n# Bid-ask spread as fraction of mid rate\nvnd_usd['spread_pct'] = (vnd_usd['ask'] - vnd_usd['bid']) / vnd_usd['rate']\n\n# Distance from reference rate (in %)\nvnd_usd['deviation_from_ref'] = (\n    (vnd_usd['rate'] - vnd_usd['reference_rate'])\n    / vnd_usd['reference_rate'] * 100\n)\n\n# Rolling volatility measures\nvnd_usd['vol_30d'] = vnd_usd['log_return'].rolling(30).std() * np.sqrt(252)\nvnd_usd['vol_90d'] = vnd_usd['log_return'].rolling(90).std() * np.sqrt(252)\n\nvnd_usd = vnd_usd.dropna(subset=['log_return'])\n\nprint(f\"VND/USD series: {len(vnd_usd)} daily observations\")\nprint(f\"Date range: {vnd_usd.index.min()} to {vnd_usd.index.max()}\")\nprint(f\"\\nSummary of daily log returns:\")\nprint(vnd_usd['log_return'].describe().round(6))\n\n\n\n39.3.4 Multi-Currency Panel\nFor cross-rate analysis and ASEAN comparisons, we construct a panel of log returns for all currency pairs.\n\n# Pivot to wide format: one column per currency pair\nfx_wide = fx_spot.pivot_table(\n    index='date', columns='to_currency', values='rate'\n)\nfx_wide.index = pd.to_datetime(fx_wide.index)\nfx_wide = fx_wide.sort_index()\n\n# Compute log returns for all pairs\nfx_returns = np.log(fx_wide / fx_wide.shift(1)).dropna()\n\n# Summary statistics\nfx_summary = pd.DataFrame({\n    'Mean (ann.)': fx_returns.mean() * 252,\n    'Vol (ann.)': fx_returns.std() * np.sqrt(252),\n    'Skewness': fx_returns.skew(),\n    'Kurtosis': fx_returns.kurtosis(),\n    'Min': fx_returns.min(),\n    'Max': fx_returns.max()\n})\nprint(\"VND Cross-Rate Return Statistics (Daily):\")\nprint(fx_summary.round(4))",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-statistics",
    "href": "57_exchange_rate.html#sec-fx-statistics",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.4 Statistical Properties of VND Returns",
    "text": "39.4 Statistical Properties of VND Returns\n\n39.4.1 Return Distribution\nThe distribution of VND/USD log returns deviates sharply from normality. The managed float regime produces a return distribution that is leptokurtic (fat-tailed) and concentrated near zero, with occasional large discrete jumps from reference rate adjustments.\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Histogram with normal overlay\nreturns = vnd_usd['log_return']\nmu, sigma = returns.mean(), returns.std()\n\naxes[0].hist(returns, bins=100, density=True, color='#2C5F8A',\n             alpha=0.7, edgecolor='white', label='Empirical')\nx_range = np.linspace(returns.min(), returns.max(), 200)\naxes[0].plot(x_range, stats.norm.pdf(x_range, mu, sigma),\n             color='#C0392B', linewidth=2, label='Normal fit')\naxes[0].set_xlabel('Daily Log Return')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Panel A: Return Distribution')\naxes[0].legend()\n\n# Panel B: QQ plot\nstats.probplot(returns, dist='norm', plot=axes[1])\naxes[1].set_title('Panel B: Normal Q-Q Plot')\naxes[1].get_lines()[0].set_color('#2C5F8A')\naxes[1].get_lines()[0].set_markersize(3)\naxes[1].get_lines()[1].set_color('#C0392B')\n\nplt.tight_layout()\nplt.show()\n\n# Formal tests\njb_stat, jb_p = stats.jarque_bera(returns)\nprint(f\"\\nJarque-Bera test: statistic = {jb_stat:.1f}, p-value = {jb_p:.2e}\")\nprint(f\"Skewness: {returns.skew():.4f}\")\nprint(f\"Excess Kurtosis: {returns.kurtosis():.4f}\")\n\n\nFigure 39.1\n\n\n\n\n\n39.4.2 Autocorrelation Structure\nFreely floating exchange rate returns are approximately uncorrelated (consistent with the random walk result of Meese and Rogoff (1983)). Managed exchange rates, however, may exhibit serial correlation because the central bank smooths adjustments over multiple days.\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# ACF of returns\nplot_acf(vnd_usd['log_return'].dropna(), lags=40, ax=axes[0],\n         color='#2C5F8A', vlines_kwargs={'color': '#2C5F8A'})\naxes[0].set_title('Autocorrelation of Daily Returns')\naxes[0].set_ylabel('ACF')\n\n# ACF of squared returns (volatility clustering)\nplot_acf(vnd_usd['log_return'].dropna() ** 2, lags=40, ax=axes[1],\n         color='#C0392B', vlines_kwargs={'color': '#C0392B'})\naxes[1].set_title('Autocorrelation of Squared Returns (Volatility Clustering)')\naxes[1].set_ylabel('ACF')\n\nplt.tight_layout()\nplt.show()\n\n# Ljung-Box test\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nlb_returns = acorr_ljungbox(returns, lags=10, return_df=True)\nlb_squared = acorr_ljungbox(returns ** 2, lags=10, return_df=True)\nprint(\"Ljung-Box Test (Returns, lag 10):\")\nprint(f\"  Statistic: {lb_returns['lb_stat'].iloc[-1]:.2f}, \"\n      f\"p-value: {lb_returns['lb_pvalue'].iloc[-1]:.4f}\")\nprint(\"Ljung-Box Test (Squared Returns, lag 10):\")\nprint(f\"  Statistic: {lb_squared['lb_stat'].iloc[-1]:.2f}, \"\n      f\"p-value: {lb_squared['lb_pvalue'].iloc[-1]:.4f}\")\n\n\nFigure 39.2\n\n\n\n\n\n39.4.3 Reference Rate Adjustments and Structural Breaks\nThe SBV periodically adjusts the reference rate or the trading band width, events that create structural breaks in the exchange rate series. Identifying these breaks is essential for accurate modeling.\n\n\n\n# Detect large daily moves (proxy for policy adjustments)\nthreshold = vnd_usd['log_return'].std() * 3\nlarge_moves = vnd_usd[vnd_usd['log_return'].abs() &gt; threshold].copy()\n\n# Known major adjustment dates (approximate)\npolicy_events = pd.DataFrame({\n    'date': pd.to_datetime([\n        '2008-06-10', '2008-12-25', '2009-11-25',\n        '2010-02-11', '2010-08-18', '2011-02-11',\n        '2011-08-10', '2015-08-12', '2015-08-19',\n        '2016-01-04'\n    ]),\n    'event': [\n        'Band widened to +/-1%', 'Band widened to +/-3%',\n        'Devaluation ~5.4%', 'Devaluation ~3.4%',\n        'Band narrowed to +/-1%', 'Devaluation ~8.5%',\n        'Band widened to +/-1%', 'Devaluation ~1%',\n        'Devaluation ~1%', 'New reference rate mechanism'\n    ]\n})\n\nfig, ax = plt.subplots(figsize=(14, 6))\nax.plot(vnd_usd.index, vnd_usd['rate'], color='#2C5F8A',\n        linewidth=1.5, label='VND/USD Spot')\n\nif 'reference_rate' in vnd_usd.columns:\n    ref_clean = vnd_usd['reference_rate'].dropna()\n    if len(ref_clean) &gt; 0:\n        ax.plot(ref_clean.index, ref_clean, color='#E67E22',\n                linewidth=1, alpha=0.7, label='SBV Reference Rate')\n\n# Mark policy events\nfor _, event in policy_events.iterrows():\n    if event['date'] &gt;= vnd_usd.index.min():\n        rate_at_event = vnd_usd['rate'].asof(event['date'])\n        ax.annotate(\n            event['event'],\n            xy=(event['date'], rate_at_event),\n            xytext=(0, 30), textcoords='offset points',\n            fontsize=7, rotation=45, ha='left',\n            arrowprops=dict(arrowstyle='-&gt;', color='#C0392B',\n                           lw=0.8),\n            color='#C0392B'\n        )\n\nax.set_ylabel('VND per USD')\nax.set_title('VND/USD Exchange Rate with Policy Events')\nax.legend(loc='upper left')\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.3\n\n\n\n\n\n39.4.4 Cross-Currency Correlations\nUnderstanding how VND co-moves with other currencies is important for diversification and for identifying common drivers (e.g., USD strength affects all Asian currencies).\n\n\n\n# Correlation matrix of daily returns\ncorr_matrix = fx_returns.corr()\n\nfig, ax = plt.subplots(figsize=(9, 8))\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\nsns.heatmap(\n    corr_matrix, mask=mask, annot=True, fmt='.2f',\n    cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n    square=True, linewidths=0.5, ax=ax,\n    cbar_kws={'label': 'Correlation'}\n)\nax.set_title('Cross-Rate Return Correlations (VND Base)')\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.4",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-parity",
    "href": "57_exchange_rate.html#sec-fx-parity",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.5 Interest Rate Parity Tests",
    "text": "39.5 Interest Rate Parity Tests\n\n39.5.1 Testing Covered Interest Rate Parity\nCIP deviations are measured as the gap between the forward premium and the interest rate differential:\n\\[\n\\text{CIP Deviation}_t = f_{t,k} - s_t - (r^{VND}_{t,k} - r^{USD}_{t,k})\n\\tag{39.3}\\]\nwhere \\(f_{t,k} = \\ln F_{t,t+k}\\) and \\(s_t = \\ln S_t\\) are log forward and spot rates. Under exact CIP, this deviation is zero.\n\n# Merge spot, forward, and interest rate data\nvnd_usd_monthly = vnd_usd['rate'].resample('M').last().to_frame('spot')\nvnd_usd_monthly.index = vnd_usd_monthly.index.to_period('M').to_timestamp()\n\n# Forward rates (3-month tenor)\nfwd_3m = fx_forward[fx_forward['tenor'] == '3M'].copy()\nfwd_3m['date'] = pd.to_datetime(fwd_3m['date'])\nfwd_3m = fwd_3m.set_index('date').resample('M').last()\n\n# Interest rate differential\nvn_rates = interest_rates[\n    (interest_rates['country'] == 'VN') &\n    (interest_rates['instrument'] == 'deposit_3m')\n].set_index('date')['rate'].resample('M').last()\n\nus_rates = interest_rates[\n    (interest_rates['country'] == 'US') &\n    (interest_rates['instrument'] == 'deposit_3m')\n].set_index('date')['rate'].resample('M').last()\n\n# Align and compute CIP deviation\ncip_data = pd.DataFrame({\n    'spot': vnd_usd_monthly['spot'],\n    'forward': fwd_3m['forward_rate'],\n    'r_vnd': vn_rates / 100,\n    'r_usd': us_rates / 100\n}).dropna()\n\n# CIP deviation (annualized, in basis points)\ncip_data['log_spot'] = np.log(cip_data['spot'])\ncip_data['log_forward'] = np.log(cip_data['forward'])\ncip_data['forward_premium'] = (\n    (cip_data['log_forward'] - cip_data['log_spot']) * 4\n)\ncip_data['rate_diff'] = cip_data['r_vnd'] - cip_data['r_usd']\ncip_data['cip_deviation'] = (\n    (cip_data['forward_premium'] - cip_data['rate_diff']) * 10000\n)\n\nprint(\"CIP Deviation Summary (basis points, annualized):\")\nprint(cip_data['cip_deviation'].describe().round(1))\nprint(f\"\\nMean deviation: {cip_data['cip_deviation'].mean():.1f} bps\")\nt_stat, p_val = stats.ttest_1samp(cip_data['cip_deviation'].dropna(), 0)\nprint(f\"One-sample t-test: t = {t_stat:.2f}, p = {p_val:.4f}\")\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), height_ratios=[2, 1])\n\n# Panel A: CIP deviation time series\naxes[0].fill_between(\n    cip_data.index, 0, cip_data['cip_deviation'],\n    where=cip_data['cip_deviation'] &gt; 0,\n    color='#C0392B', alpha=0.4, label='Positive (VND forward expensive)'\n)\naxes[0].fill_between(\n    cip_data.index, 0, cip_data['cip_deviation'],\n    where=cip_data['cip_deviation'] &lt;= 0,\n    color='#27AE60', alpha=0.4, label='Negative'\n)\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].set_ylabel('CIP Deviation (bps)')\naxes[0].set_title('Panel A: CIP Deviations for VND/USD')\naxes[0].legend()\n\n# Panel B: Interest rate differential\naxes[1].plot(cip_data.index, cip_data['r_vnd'] * 100,\n             color='#C0392B', label='Vietnam 3M')\naxes[1].plot(cip_data.index, cip_data['r_usd'] * 100,\n             color='#2C5F8A', label='US 3M')\naxes[1].set_ylabel('Interest Rate (%)')\naxes[1].set_xlabel('Date')\naxes[1].set_title('Panel B: Interest Rate Differential')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.5\n\n\n\n\n\n39.5.2 Testing Uncovered Interest Rate Parity\nThe standard UIP regression is:\n\\[\n\\Delta s_{t+k} = \\alpha + \\beta (r^{VND}_{t,k} - r^{USD}_{t,k}) + \\varepsilon_{t+k}\n\\tag{39.4}\\]\nUnder UIP, \\(\\alpha = 0\\) and \\(\\beta = 1\\). The Fama (1984) anomaly corresponds to \\(\\beta &lt; 1\\) (often \\(\\beta &lt; 0\\)), implying that high-interest-rate currencies appreciate rather than depreciate.\n\n# Monthly exchange rate changes\ncip_data['delta_s'] = (\n    np.log(cip_data['spot'].shift(-3) / cip_data['spot'])\n)\n\n# Fama regression\nuip_data = cip_data[['delta_s', 'rate_diff']].dropna()\n\nuip_model = sm.OLS(\n    uip_data['delta_s'],\n    sm.add_constant(uip_data['rate_diff'])\n).fit(cov_type='HAC', cov_kwds={'maxlags': 4})\n\nprint(\"UIP (Fama) Regression: delta_s_{t+3m} = alpha + beta(r_VND - r_USD) + eps\")\nprint(f\"\\nalpha = {uip_model.params['const']:.4f} \"\n      f\"(t = {uip_model.tvalues['const']:.2f})\")\nprint(f\"beta = {uip_model.params['rate_diff']:.4f} \"\n      f\"(t = {uip_model.tvalues['rate_diff']:.2f})\")\nprint(f\"R-squared = {uip_model.rsquared:.4f}\")\nprint(f\"\\nH0: beta = 1 (Wald test):\")\nwald_stat = ((uip_model.params['rate_diff'] - 1) /\n             uip_model.bse['rate_diff']) ** 2\nprint(f\"  Chi2 = {wald_stat:.2f}, p = {1 - stats.chi2.cdf(wald_stat, 1):.4f}\")\n\n\n\n\n\n\n\nNoteInterpreting UIP Failures in Vietnam\n\n\n\nA managed exchange rate complicates UIP tests. When the SBV successfully defends the VND within a narrow band, realized exchange rate changes are artificially compressed regardless of interest rate differentials. The resulting beta estimate reflects the credibility of the peg as much as it reflects the risk premium. Periods of active SBV intervention should be treated separately from periods of relative flexibility.\n\n\n\n\n39.5.3 Carry Trade Returns\nWe construct a simple VND carry trade strategy: borrow USD at the U.S. short rate, convert to VND, invest at the Vietnamese short rate, and convert back at maturity. The excess return is:\n\\[\n\\text{Carry}_{t \\to t+k} = (r^{VND}_{t,k} - r^{USD}_{t,k}) - \\Delta s_{t+k}\n\\tag{39.5}\\]\nA positive carry return means the interest rate differential more than compensated for any VND depreciation.\n\ncarry = cip_data.copy()\ncarry['carry_return'] = carry['rate_diff'] / 4 - carry['delta_s']\ncarry = carry.dropna(subset=['carry_return'])\n\n# Annualize\nann_carry = carry['carry_return'].mean() * 4\nann_vol = carry['carry_return'].std() * 2\nsharpe = ann_carry / ann_vol\n\nprint(\"VND/USD Carry Trade Performance (3-Month Rolling):\")\nprint(f\"Annualized return: {ann_carry:.4f} ({ann_carry*100:.2f}%)\")\nprint(f\"Annualized volatility: {ann_vol:.4f} ({ann_vol*100:.2f}%)\")\nprint(f\"Sharpe ratio: {sharpe:.2f}\")\nprint(f\"Skewness: {carry['carry_return'].skew():.2f}\")\nprint(f\"Kurtosis: {carry['carry_return'].kurtosis():.2f}\")\nprint(f\"\\nMax quarterly loss: {carry['carry_return'].min():.4f}\")\nprint(f\"Max quarterly gain: {carry['carry_return'].max():.4f}\")\n\n\n\n\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), height_ratios=[2, 1])\n\n# Panel A: Cumulative return\ncum_carry = (1 + carry['carry_return']).cumprod()\naxes[0].plot(carry.index, cum_carry, color='#2C5F8A', linewidth=2)\naxes[0].fill_between(carry.index, 1, cum_carry,\n                     where=cum_carry &gt; 1, color='#27AE60', alpha=0.2)\naxes[0].fill_between(carry.index, 1, cum_carry,\n                     where=cum_carry &lt; 1, color='#C0392B', alpha=0.2)\naxes[0].axhline(y=1, color='gray', linewidth=0.5)\naxes[0].set_ylabel('Cumulative Return (VND 1 invested)')\naxes[0].set_title('Panel A: VND/USD Carry Trade Cumulative Return')\n\n# Panel B: Quarterly returns\naxes[1].bar(carry.index, carry['carry_return'] * 100,\n            color=['#27AE60' if x &gt; 0 else '#C0392B'\n                   for x in carry['carry_return']],\n            alpha=0.7, width=60)\naxes[1].axhline(y=0, color='black', linewidth=0.5)\naxes[1].set_ylabel('Quarterly Return (%)')\naxes[1].set_xlabel('Date')\naxes[1].set_title('Panel B: Quarterly Carry Returns')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.6",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-volatility",
    "href": "57_exchange_rate.html#sec-fx-volatility",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.6 Volatility Modeling",
    "text": "39.6 Volatility Modeling\n\n39.6.1 GARCH Estimation\nThe strong autocorrelation in squared VND/USD returns (Figure 39.2) motivates GARCH modeling (Bollerslev 1986). We estimate a GARCH(1,1) model for daily VND/USD log returns:\n\\[\nr_t = \\mu + \\varepsilon_t, \\quad \\varepsilon_t = \\sigma_t z_t, \\quad z_t \\sim D(0,1)\n\\tag{39.6}\\]\n\\[\n\\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2\n\\tag{39.7}\\]\nwhere \\(\\alpha\\) captures the ARCH effect (impact of yesterday’s shock on today’s variance) and \\(\\beta\\) captures persistence (how slowly volatility reverts to its long-run mean). The unconditional variance is \\(\\bar{\\sigma}^2 = \\omega / (1 - \\alpha - \\beta)\\).\n\nreturns_bps = vnd_usd['log_return'] * 10000  # Scale to basis points\n\n# GARCH(1,1) with normal innovations\ngarch_norm = arch_model(\n    returns_bps, vol='Garch', p=1, q=1, dist='normal'\n)\nres_norm = garch_norm.fit(disp='off')\n\n# GARCH(1,1) with Student-t innovations\ngarch_t = arch_model(\n    returns_bps, vol='Garch', p=1, q=1, dist='t'\n)\nres_t = garch_t.fit(disp='off')\n\n# GJR-GARCH (asymmetric: depreciation shocks may increase vol more)\ngjr = arch_model(\n    returns_bps, vol='Garch', p=1, o=1, q=1, dist='t'\n)\nres_gjr = gjr.fit(disp='off')\n\nprint(\"=\" * 60)\nprint(\"GARCH(1,1) - Normal Innovations\")\nprint(\"=\" * 60)\nprint(res_norm.summary().tables[1])\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"GARCH(1,1) - Student-t Innovations\")\nprint(\"=\" * 60)\nprint(res_t.summary().tables[1])\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"GJR-GARCH(1,1,1) - Student-t Innovations\")\nprint(\"=\" * 60)\nprint(res_gjr.summary().tables[1])\n\n# Model comparison\nprint(\"\\nModel Comparison:\")\nprint(f\"{'Model':&lt;25} {'AIC':&gt;10} {'BIC':&gt;10} {'LogLik':&gt;12}\")\nfor name, res in [('GARCH-Normal', res_norm),\n                   ('GARCH-t', res_t),\n                   ('GJR-GARCH-t', res_gjr)]:\n    print(f\"{name:&lt;25} {res.aic:&gt;10.1f} {res.bic:&gt;10.1f} \"\n          f\"{res.loglikelihood:&gt;12.1f}\")\n\n\n\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# GARCH conditional volatility (annualized, from basis points)\ncond_vol = res_t.conditional_volatility / 10000 * np.sqrt(252)\n\nax.plot(vnd_usd.index[-len(cond_vol):], cond_vol,\n        color='#C0392B', linewidth=1.2, alpha=0.9,\n        label='GARCH(1,1) Conditional Vol')\nax.plot(vnd_usd.index, vnd_usd['vol_30d'],\n        color='#2C5F8A', linewidth=1, alpha=0.6,\n        label='30-Day Realized Vol')\n\nax.set_ylabel('Annualized Volatility')\nax.set_xlabel('Date')\nax.set_title('VND/USD Volatility: GARCH vs Realized')\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.7\n\n\n\n\n\n39.6.2 Volatility Regime Identification\nWe classify the VND/USD market into volatility regimes using the GARCH conditional volatility:\n\n# Merge conditional volatility back to main DataFrame\nvol_series = pd.Series(\n    cond_vol.values,\n    index=vnd_usd.index[-len(cond_vol):]\n)\nvnd_usd['cond_vol'] = vol_series\n\n# Classify regimes based on percentiles\nvnd_usd['vol_regime'] = pd.cut(\n    vnd_usd['cond_vol'],\n    bins=[0, vnd_usd['cond_vol'].quantile(0.33),\n          vnd_usd['cond_vol'].quantile(0.67), np.inf],\n    labels=['Low', 'Medium', 'High']\n)\n\n# Regime statistics\nregime_stats = (\n    vnd_usd.groupby('vol_regime')\n    .agg(\n        n_days=('log_return', 'count'),\n        mean_return=('log_return', lambda x: x.mean() * 252),\n        volatility=('log_return', lambda x: x.std() * np.sqrt(252)),\n        mean_spread=('spread_pct', 'mean'),\n        skewness=('log_return', 'skew')\n    )\n    .round(4)\n)\nprint(\"Volatility Regime Characteristics:\")\nprint(regime_stats)\n\n\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\ncolors_regime = {'Low': '#27AE60', 'Medium': '#F1C40F', 'High': '#C0392B'}\nfor regime in ['Low', 'Medium', 'High']:\n    subset = vnd_usd[vnd_usd['vol_regime'] == regime]\n    ax.scatter(\n        subset['cond_vol'] * 100,\n        subset['log_return'] * 100,\n        color=colors_regime[regime], alpha=0.3, s=8, label=regime\n    )\n\nax.axhline(y=0, color='gray', linewidth=0.5)\nax.set_xlabel('Conditional Volatility (% annualized)')\nax.set_ylabel('Daily Return (%)')\nax.set_title('VND/USD Returns by Volatility Regime')\nax.legend(title='Regime')\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.8",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-exposure",
    "href": "57_exchange_rate.html#sec-fx-exposure",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.7 Exchange Rate Exposure of Vietnamese Firms",
    "text": "39.7 Exchange Rate Exposure of Vietnamese Firms\n\n39.7.1 The Jorion Framework\nJorion (1990) introduced the standard two-factor model for estimating firm-level exchange rate exposure:\n\\[\nR_{i,t} = \\alpha_i + \\beta_i^{MKT} R_{m,t} + \\gamma_i \\Delta s_t + \\varepsilon_{i,t}\n\\tag{39.8}\\]\nwhere \\(R_{i,t}\\) is the stock return of firm \\(i\\), \\(R_{m,t}\\) is the market return, and \\(\\Delta s_t\\) is the exchange rate change (VND/USD log return, where positive = VND depreciation). The coefficient \\(\\gamma_i\\) is the residual exchange rate exposure, the sensitivity of firm \\(i\\)’s returns to currency movements after controlling for market-wide effects.\nA positive \\(\\gamma_i\\) means the firm benefits from VND depreciation (typical for exporters), while a negative \\(\\gamma_i\\) means the firm is hurt by depreciation (typical for importers or firms with USD-denominated debt).\n\n# Load equity returns\nequity = client.get_monthly_returns(\n    exchanges=['HOSE', 'HNX'],\n    start_date='2012-01-01',\n    end_date='2024-12-31',\n    fields=['ticker', 'month_end', 'monthly_return', 'market_cap']\n)\n\n# Market return (VW)\nmarket_ret = (\n    equity\n    .groupby('month_end')\n    .apply(lambda g: np.average(g['monthly_return'],\n                                 weights=g['market_cap']))\n    .reset_index(name='market_return')\n)\n\n# Monthly VND/USD returns\nfx_monthly = (\n    vnd_usd['log_return']\n    .resample('M').sum()\n    .to_frame('fx_return')\n)\nfx_monthly.index = fx_monthly.index.to_period('M').to_timestamp()\n\n# Merge all data\nexposure_data = (\n    equity\n    .merge(market_ret, on='month_end')\n    .merge(fx_monthly, left_on='month_end', right_index=True, how='inner')\n)\n\n# Estimate exposure for each firm\ndef estimate_exposure(group, min_obs=36):\n    \"\"\"Estimate Jorion exposure model for a single firm.\"\"\"\n    if len(group) &lt; min_obs:\n        return None\n    \n    y = group['monthly_return']\n    X = sm.add_constant(group[['market_return', 'fx_return']])\n    \n    try:\n        model = sm.OLS(y, X).fit(cov_type='HC1')\n        return pd.Series({\n            'gamma': model.params['fx_return'],\n            'gamma_se': model.bse['fx_return'],\n            'gamma_t': model.tvalues['fx_return'],\n            'gamma_p': model.pvalues['fx_return'],\n            'beta_mkt': model.params['market_return'],\n            'r_squared': model.rsquared,\n            'n_obs': model.nobs\n        })\n    except Exception:\n        return None\n\nexposures = (\n    exposure_data\n    .groupby('ticker')\n    .apply(estimate_exposure)\n    .dropna()\n)\n\nprint(f\"Estimated exposure for {len(exposures)} firms\")\nprint(f\"\\nExposure (gamma) Summary:\")\nprint(exposures['gamma'].describe().round(4))\nprint(f\"\\nSignificant at 5%: \"\n      f\"{(exposures['gamma_p'] &lt; 0.05).sum()} / {len(exposures)} \"\n      f\"({(exposures['gamma_p'] &lt; 0.05).mean():.1%})\")\n\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Distribution of gamma\nsig = exposures['gamma_p'] &lt; 0.05\naxes[0].hist(exposures.loc[~sig, 'gamma'], bins=40, color='#BDC3C7',\n             alpha=0.7, label='Insignificant', edgecolor='white')\naxes[0].hist(exposures.loc[sig & (exposures['gamma'] &gt; 0), 'gamma'],\n             bins=20, color='#27AE60', alpha=0.8,\n             label='Significant positive', edgecolor='white')\naxes[0].hist(exposures.loc[sig & (exposures['gamma'] &lt; 0), 'gamma'],\n             bins=20, color='#C0392B', alpha=0.8,\n             label='Significant negative', edgecolor='white')\naxes[0].axvline(x=0, color='black', linewidth=0.8)\naxes[0].set_xlabel('Exchange Rate Exposure (gamma)')\naxes[0].set_ylabel('Number of Firms')\naxes[0].set_title('Panel A: Cross-Sectional Distribution')\naxes[0].legend(fontsize=9)\n\n# Panel B: Exposure vs market beta\naxes[1].scatter(exposures['beta_mkt'], exposures['gamma'],\n                c=exposures['gamma_t'].clip(-3, 3),\n                cmap='RdBu_r', s=15, alpha=0.6)\naxes[1].axhline(y=0, color='gray', linewidth=0.5)\naxes[1].axvline(x=1, color='gray', linewidth=0.5, linestyle='--')\naxes[1].set_xlabel('Market Beta')\naxes[1].set_ylabel('FX Exposure (gamma)')\naxes[1].set_title('Panel B: Market Beta vs FX Exposure')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.9\n\n\n\n\n\n39.7.2 Industry-Level Exposure\nExchange rate exposure varies systematically across industries. Export-oriented sectors (textiles, seafood, electronics) should have positive exposure (benefiting from VND weakness), while import-dependent sectors (oil and gas, machinery, consumer goods) should have negative exposure.\n\n# Get industry classification\nindustry = client.get_firm_info(\n    exchanges=['HOSE', 'HNX'],\n    fields=['ticker', 'icb_sector', 'icb_industry']\n)\n\nexposure_industry = exposures.reset_index().merge(\n    industry, on='ticker', how='left'\n)\n\n# Industry-level average exposure\nindustry_avg = (\n    exposure_industry\n    .groupby('icb_sector')\n    .agg(\n        n_firms=('gamma', 'count'),\n        mean_gamma=('gamma', 'mean'),\n        median_gamma=('gamma', 'median'),\n        pct_significant=('gamma_p', lambda x: (x &lt; 0.05).mean())\n    )\n    .sort_values('mean_gamma', ascending=False)\n    .round(3)\n)\n\nprint(\"Exchange Rate Exposure by Industry:\")\nprint(industry_avg.to_string())\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\ncolors_bar = ['#27AE60' if x &gt; 0 else '#C0392B'\n              for x in industry_avg['mean_gamma']]\nbars = ax.barh(range(len(industry_avg)), industry_avg['mean_gamma'],\n               color=colors_bar, alpha=0.85)\nax.set_yticks(range(len(industry_avg)))\nax.set_yticklabels(industry_avg.index, fontsize=9)\nax.axvline(x=0, color='black', linewidth=0.8)\nax.set_xlabel('Average Exchange Rate Exposure (gamma)')\nax.set_title('Exchange Rate Exposure by Industry')\n\nfor i, (_, row) in enumerate(industry_avg.iterrows()):\n    label = f\"({row['pct_significant']:.0%} sig.)\"\n    ax.text(row['mean_gamma'] + 0.01 * np.sign(row['mean_gamma']),\n            i, label, va='center', fontsize=8, color='gray')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.10\n\n\n\n\n\n39.7.3 Time-Varying Exposure\nExchange rate exposure is not constant, firms adjust hedging strategies, trade patterns shift, and the exchange rate regime itself evolves. We estimate rolling 36-month exposures:\n\ndef rolling_exposure(group, window=36, min_obs=24):\n    \"\"\"Estimate rolling FX exposure with 36-month windows.\"\"\"\n    results = []\n    group = group.sort_values('month_end')\n    \n    for i in range(window, len(group) + 1):\n        sub = group.iloc[i - window:i]\n        if len(sub) &lt; min_obs:\n            continue\n        \n        y = sub['monthly_return']\n        X = sm.add_constant(sub[['market_return', 'fx_return']])\n        \n        try:\n            model = sm.OLS(y, X).fit()\n            results.append({\n                'month_end': sub['month_end'].iloc[-1],\n                'gamma': model.params['fx_return'],\n                'gamma_se': model.bse['fx_return']\n            })\n        except Exception:\n            pass\n    \n    return pd.DataFrame(results)\n\n# Compute for a sample of large firms\nlarge_firms = (\n    equity.groupby('ticker')['market_cap']\n    .last().nlargest(20).index\n)\n\nrolling_results = {}\nfor ticker in large_firms:\n    firm_data = exposure_data[exposure_data['ticker'] == ticker]\n    result = rolling_exposure(firm_data)\n    if len(result) &gt; 0:\n        rolling_results[ticker] = result\n\nprint(f\"Rolling exposures computed for {len(rolling_results)} firms\")\n\n\n\n\nfig, ax = plt.subplots(figsize=(14, 6))\n\nplot_colors = plt.cm.Set1(np.linspace(0, 1, min(6, len(rolling_results))))\nfor i, (ticker, df) in enumerate(list(rolling_results.items())[:6]):\n    ax.plot(pd.to_datetime(df['month_end']), df['gamma'],\n            linewidth=1.5, label=ticker, color=plot_colors[i])\n    if i == 0:\n        ax.fill_between(\n            pd.to_datetime(df['month_end']),\n            df['gamma'] - 1.96 * df['gamma_se'],\n            df['gamma'] + 1.96 * df['gamma_se'],\n            alpha=0.1, color=plot_colors[i]\n        )\n\nax.axhline(y=0, color='gray', linewidth=0.8)\nax.set_ylabel('Exchange Rate Exposure (gamma)')\nax.set_xlabel('Date')\nax.set_title('Rolling 36-Month Exchange Rate Exposure')\nax.legend(ncol=3, fontsize=9)\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.11",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-hedging",
    "href": "57_exchange_rate.html#sec-fx-hedging",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.8 Currency Hedging for International Investors",
    "text": "39.8 Currency Hedging for International Investors\n\n39.8.1 The Hedging Decision\nFor a foreign investor holding Vietnamese equities, the unhedged USD-denominated return is:\n\\[\nR^{USD}_{i,t} \\approx R^{VND}_{i,t} + \\Delta s_t + R^{VND}_{i,t} \\cdot \\Delta s_t\n\\tag{39.9}\\]\nwhere the cross-product term is usually small. Under a full hedge using forward contracts, the return becomes:\n\\[\nR^{USD,\\text{hedged}}_{i,t} \\approx R^{VND}_{i,t} + (f_t - s_t)\n\\tag{39.10}\\]\nwhere \\(f_t - s_t\\) is the forward premium (which, by CIP, approximately equals \\(r^{USD}_t - r^{VND}_t\\)). In Vietnam, where VND rates exceed USD rates, the forward premium is negative, hedging costs money because the investor forgoes the interest rate differential.\nCampbell, Serfaty-De Medeiros, and Viceira (2010) show that the optimal hedge ratio depends on the correlation between equity returns and currency returns. When the correlation is positive (VND depreciation coincides with equity losses), hedging reduces overall portfolio variance more effectively.\n\n# Construct VW equity index in VND\nequity_index = (\n    equity\n    .groupby('month_end')\n    .apply(lambda g: np.average(g['monthly_return'],\n                                 weights=g['market_cap']))\n    .reset_index(name='equity_vnd')\n)\nequity_index['month_end'] = pd.to_datetime(equity_index['month_end'])\n\n# Merge with FX returns and forward premium\nhedge_data = (\n    equity_index\n    .merge(fx_monthly, left_on='month_end', right_index=True, how='inner')\n)\n\n# Forward premium\nif len(cip_data) &gt; 0:\n    fwd_premium_monthly = (\n        cip_data['forward_premium']\n        .resample('M').last() / 12\n    )\n    hedge_data = hedge_data.merge(\n        fwd_premium_monthly.to_frame('fwd_premium'),\n        left_on='month_end', right_index=True, how='left'\n    )\n    hedge_data['fwd_premium'] = hedge_data['fwd_premium'].fillna(\n        hedge_data['fwd_premium'].mean()\n    )\nelse:\n    hedge_data['fwd_premium'] = -0.003\n\n# Unhedged USD return\nhedge_data['return_unhedged'] = (\n    hedge_data['equity_vnd'] + hedge_data['fx_return']\n    + hedge_data['equity_vnd'] * hedge_data['fx_return']\n)\n\n# Fully hedged USD return\nhedge_data['return_hedged'] = (\n    hedge_data['equity_vnd'] + hedge_data['fwd_premium']\n)\n\n# 50% hedge ratio\nhedge_data['return_50pct'] = (\n    0.5 * hedge_data['return_hedged']\n    + 0.5 * hedge_data['return_unhedged']\n)\n\n# Performance comparison\nfor strategy, col in [('VND (Local)', 'equity_vnd'),\n                       ('USD Unhedged', 'return_unhedged'),\n                       ('USD 50% Hedged', 'return_50pct'),\n                       ('USD Fully Hedged', 'return_hedged')]:\n    r = hedge_data[col]\n    ann_ret = (1 + r).prod() ** (12 / len(r)) - 1\n    ann_vol = r.std() * np.sqrt(12)\n    sharpe = r.mean() / r.std() * np.sqrt(12)\n    print(f\"{strategy:&lt;22} Return: {ann_ret:&gt;7.2%}  Vol: {ann_vol:&gt;7.2%}  \"\n          f\"Sharpe: {sharpe:&gt;5.2f}\")\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nstrategies = {\n    'VND (Local)': ('equity_vnd', '#2C5F8A', '-'),\n    'USD Unhedged': ('return_unhedged', '#C0392B', '-'),\n    'USD 50% Hedged': ('return_50pct', '#E67E22', '--'),\n    'USD Fully Hedged': ('return_hedged', '#27AE60', '-'),\n}\n\nfor label, (col, color, ls) in strategies.items():\n    cum = (1 + hedge_data.set_index('month_end')[col]).cumprod()\n    ax.plot(cum.index, cum, label=label, color=color,\n            linewidth=2, linestyle=ls)\n\nax.set_ylabel('Cumulative Wealth')\nax.set_xlabel('Date')\nax.set_title('Vietnamese Equity Returns: Hedging Comparison')\nax.legend()\nax.set_yscale('log')\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.12\n\n\n\n\n\n39.8.2 Optimal Hedge Ratio\nThe minimum-variance hedge ratio is:\n\\[\nh^* = \\frac{\\text{Cov}(R^{VND}_t, \\Delta s_t)}{\\text{Var}(\\Delta s_t)}\n\\tag{39.11}\\]\nThis is the OLS slope from regressing local equity returns on exchange rate changes. When the correlation between equity and currency is positive (bad for unhedged investors), \\(h^* &gt; 0\\) and hedging reduces variance.\n\n# Full-sample optimal hedge ratio\nhedge_reg = sm.OLS(\n    hedge_data['equity_vnd'],\n    sm.add_constant(hedge_data['fx_return'])\n).fit()\n\nh_star = hedge_reg.params['fx_return']\nprint(f\"Optimal hedge ratio (full sample): {h_star:.3f}\")\nprint(f\"Equity-currency correlation: \"\n      f\"{hedge_data['equity_vnd'].corr(hedge_data['fx_return']):.3f}\")\n\n# Rolling 36-month optimal hedge ratio\nrolling_cov = hedge_data['equity_vnd'].rolling(36).cov(\n    hedge_data['fx_return']\n)\nrolling_var = hedge_data['fx_return'].rolling(36).var()\nhedge_data['h_star_rolling'] = rolling_cov / rolling_var\n\nprint(f\"\\nRolling hedge ratio range: \"\n      f\"[{hedge_data['h_star_rolling'].min():.2f}, \"\n      f\"{hedge_data['h_star_rolling'].max():.2f}]\")",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-asean",
    "href": "57_exchange_rate.html#sec-fx-asean",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.9 ASEAN Currency Comparison",
    "text": "39.9 ASEAN Currency Comparison\nTo put the VND in perspective, we compare its properties with other ASEAN currencies and the Chinese Yuan.\n\n\n\n# Compute annual statistics for each currency\ncomparison = pd.DataFrame()\nfor currency in fx_returns.columns:\n    r = fx_returns[currency].dropna()\n    if len(r) &gt; 252:\n        comparison.loc[currency, 'Ann. Depreciation (%)'] = r.mean() * 252 * 100\n        comparison.loc[currency, 'Ann. Volatility (%)'] = r.std() * np.sqrt(252) * 100\n        comparison.loc[currency, 'Skewness'] = r.skew()\n        comparison.loc[currency, 'Kurtosis'] = r.kurtosis()\n        comparison.loc[currency, 'Max Daily Loss (%)'] = r.max() * 100\n        comparison.loc[currency, 'Sharpe (Carry)'] = (\n            r.mean() * 252 / (r.std() * np.sqrt(252))\n        )\n\nprint(\"ASEAN + Asian Currency Comparison (VND base, vs USD):\")\nprint(comparison.round(2).to_string())\n\n# Risk-return scatter\nfig, ax = plt.subplots(figsize=(9, 7))\nfor currency in comparison.index:\n    ax.scatter(\n        comparison.loc[currency, 'Ann. Volatility (%)'],\n        comparison.loc[currency, 'Ann. Depreciation (%)'],\n        s=120, zorder=5\n    )\n    ax.annotate(\n        f'VND/{currency}',\n        (comparison.loc[currency, 'Ann. Volatility (%)'] + 0.15,\n         comparison.loc[currency, 'Ann. Depreciation (%)']),\n        fontsize=10\n    )\n\nax.axhline(y=0, color='gray', linewidth=0.5)\nax.set_xlabel('Annualized Volatility (%)')\nax.set_ylabel('Annualized Depreciation (%)')\nax.set_title('ASEAN Currency Risk-Return Characteristics')\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.13",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-comovement",
    "href": "57_exchange_rate.html#sec-fx-comovement",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.10 Exchange Rate and Equity Market Co-Movement",
    "text": "39.10 Exchange Rate and Equity Market Co-Movement\nThe relationship between VND/USD movements and the Vietnamese equity market is central to portfolio construction. We examine this at both the aggregate and conditional levels.\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Panel A: Scatter plot\naxes[0].scatter(\n    hedge_data['fx_return'] * 100,\n    hedge_data['equity_vnd'] * 100,\n    c=hedge_data.index, cmap='viridis', s=20, alpha=0.7\n)\nz = np.polyfit(hedge_data['fx_return'], hedge_data['equity_vnd'], 1)\nx_line = np.linspace(hedge_data['fx_return'].min(),\n                     hedge_data['fx_return'].max(), 100)\naxes[0].plot(x_line * 100, np.polyval(z, x_line) * 100,\n             color='#C0392B', linewidth=2)\naxes[0].axhline(y=0, color='gray', linewidth=0.5)\naxes[0].axvline(x=0, color='gray', linewidth=0.5)\naxes[0].set_xlabel('VND/USD Change (%)')\naxes[0].set_ylabel('VN-Index Return (%)')\naxes[0].set_title('Panel A: Monthly Equity-FX Relationship')\n\nrho = hedge_data['equity_vnd'].corr(hedge_data['fx_return'])\naxes[0].text(0.05, 0.95, f'rho = {rho:.3f}', transform=axes[0].transAxes,\n             fontsize=12, verticalalignment='top',\n             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n# Panel B: Rolling 36-month correlation\nrolling_corr = (\n    hedge_data['equity_vnd']\n    .rolling(36)\n    .corr(hedge_data['fx_return'])\n)\naxes[1].plot(hedge_data['month_end'], rolling_corr,\n             color='#2C5F8A', linewidth=2)\naxes[1].axhline(y=0, color='gray', linewidth=0.8, linestyle='--')\naxes[1].fill_between(\n    hedge_data['month_end'], 0, rolling_corr,\n    where=rolling_corr &lt; 0, color='#C0392B', alpha=0.2\n)\naxes[1].fill_between(\n    hedge_data['month_end'], 0, rolling_corr,\n    where=rolling_corr &gt; 0, color='#27AE60', alpha=0.2\n)\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Rolling Correlation')\naxes[1].set_title('Panel B: 36-Month Rolling Equity-FX Correlation')\n\nplt.tight_layout()\nplt.show()\n\n\nFigure 39.14",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "57_exchange_rate.html#sec-fx-summary",
    "href": "57_exchange_rate.html#sec-fx-summary",
    "title": "39  Exchange Rate Dynamics",
    "section": "39.11 Summary",
    "text": "39.11 Summary\nThis chapter has developed a comprehensive toolkit for working with exchange rate data in the Vietnamese context. The key findings and methods are in Table 39.2.\n\n\n\nTable 39.2: Summary of findings on VND exchange rate dynamics.\n\n\n\n\n\n\n\n\n\n\nTopic\nFinding\nImplication\n\n\n\n\nVND regime\nManaged float with +/-3% band, daily reference rate\nReturns are non-normal; clustered near zero with discrete jumps\n\n\nReturn distribution\nFat tails, excess kurtosis, positive skewness\nStandard normal-based risk models underestimate tail risk\n\n\nCIP\nPersistent positive deviations (~50-200 bps)\nCapital controls and limited arbitrage; hedging is expensive\n\n\nUIP\nbeta &lt; 1 in Fama regression\nCarry trade is profitable on average; crash risk in devaluations\n\n\nCarry trade\nPositive Sharpe but negative skewness\nSteady gains punctuated by sharp losses during devaluations\n\n\nGARCH\nStrong persistence (alpha + beta approx 0.99); Student-t preferred\nVolatility clustering; regime switches around policy events\n\n\nFirm exposure\n~15-25% significant; systematic industry patterns\nExporters benefit from depreciation; importers hurt\n\n\nHedging\nFull hedge eliminates FX risk but costs ~3-5% p.a.\nOptimal partial hedge depends on equity-FX correlation\n\n\nASEAN comparison\nVND has lowest volatility but steady depreciation\nManaged float compresses vol; does not eliminate trend risk\n\n\n\n\n\n\nFor researchers using Vietnamese equity data, the exchange rate dimension is not optional. Any study that converts VND returns to USD, uses international factor models, or examines firms with trade exposure must account for the distinctive properties of the VND regime. The tools developed in this chapter, from GARCH volatility modeling to rolling exposure estimation, provide the building blocks for incorporating exchange rate dynamics into broader empirical asset pricing work.\n\n\n\n\n\n\n\nBollerslev, Tim. 1986. “Generalized Autoregressive Conditional Heteroskedasticity.” Journal of Econometrics 31 (3): 307–27.\n\n\nBrunnermeier, Markus K, Stefan Nagel, and Lasse H Pedersen. 2008. “Carry Trades and Currency Crashes.” NBER Macroeconomics Annual 23 (1): 313–48.\n\n\nCampbell, John Y, Karine Serfaty-De Medeiros, and Luis M Viceira. 2010. “Global Currency Hedging.” The Journal of Finance 65 (1): 87–121.\n\n\nDornbusch, Rudiger. 1976. “Expectations and Exchange Rate Dynamics.” Journal of Political Economy 84 (6): 1161–76.\n\n\nDu, Wenxin, Alexander Tepper, and Adrien Verdelhan. 2018. “Deviations from Covered Interest Rate Parity.” The Journal of Finance 73 (3): 915–57.\n\n\nFama, Eugene F. 1984. “Forward and Spot Exchange Rates.” Journal of Monetary Economics 14 (3): 319–38.\n\n\nFrankel, Jeffrey A. 1979. “On the Mark: A Theory of Floating Exchange Rates Based on Real Interest Differentials.” The American Economic Review 69 (4): 610–22.\n\n\nJorion, Philippe. 1990. “The Exchange-Rate Exposure of US Multinationals.” Journal of Business, 331–45.\n\n\nLustig, Hanno, and Adrien Verdelhan. 2007. “The Cross Section of Foreign Currency Risk Premia and Consumption Growth Risk.” American Economic Review 97 (1): 89–117.\n\n\nMeese, Richard A, and Kenneth Rogoff. 1983. “Empirical Exchange Rate Models of the Seventies: Do They Fit Out of Sample?” Journal of International Economics 14 (1-2): 3–24.\n\n\nMenkhoff, Lukas, Lucio Sarno, Maik Schmeling, and Andreas Schrimpf. 2012. “Carry Trades and Global Foreign Exchange Volatility.” The Journal of Finance 67 (2): 681–718.\n\n\nObstfeld, Maurice, Jay C Shambaugh, and Alan M Taylor. 2005. “The Trilemma in History: Tradeoffs Among Exchange Rates, Monetary Policies, and Capital Mobility.” Review of Economics and Statistics 87 (3): 423–38.\n\n\nRossi, Barbara. 2013. “Exchange Rate Predictability.” Journal of Economic Literature 51 (4): 1063–1119.",
    "crumbs": [
      "Home",
      "Quyền sở hữu, ma sát thị trường và rủi ro quốc tế",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Exchange Rate Dynamics</span>"
    ]
  },
  {
    "objectID": "60_event_studies.html",
    "href": "60_event_studies.html",
    "title": "40  Event Studies in Finance",
    "section": "",
    "text": "40.0.1 Why Event Studies Matter\nEvent studies constitute one of the most enduring and widely deployed empirical methodologies in financial economics. At their core, event studies measure the impact of a specific event on the value of a firm by examining abnormal security returns around the time the event occurs. The methodology rests on a simple premise: if capital markets are informationally efficient, the effect of an event will be reflected immediately in security prices, and any deviation from “normal” expected returns can be attributed to the event itself.\nSince the pioneering work of Eugene F. Fama et al. (1969), who studied how stock prices adjust to new information around stock splits, event studies have become a cornerstone of empirical research across finance, accounting, economics, and law. Ball and Brown (2013) demonstrated that accounting earnings announcements convey information to the market, a finding that launched decades of research in accounting and disclosure. The methodology has since been refined through contributions by Brown and Warner (1980) and Brown and Warner (1985), who established the statistical properties of event study methods, and MacKinlay (1997) codified best practices that remain standard today.\nThe breadth of applications is remarkable. Event studies have been used to examine the wealth effects of mergers and acquisitions (Jensen and Ruback 1983; Andrade, Mitchell, and Stafford 2001), earnings announcements (Bernard and Thomas 1989), dividend changes (Aharony and Swary 1980), regulatory changes (Schwert 1981), executive turnover (Warner, Watts, and Wruck 1988), and macroeconomic announcements (Flannery and Protopapadakis 2002). In law and economics, event studies serve as the primary tool for measuring damages in securities fraud litigation (Mitchell and Netter 1993) and assessing the impact of regulatory interventions (Binder 1998). Kothari and Warner (2007) documented over 500 published event studies in the top five finance journals alone between 1974 and 2000.\nThe enduring popularity of event studies stems from several compelling properties:",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "60_event_studies.html#literature-review-and-methodological-evolution",
    "href": "60_event_studies.html#literature-review-and-methodological-evolution",
    "title": "40  Event Studies in Finance",
    "section": "40.1 Literature Review and Methodological Evolution",
    "text": "40.1 Literature Review and Methodological Evolution\n\n40.1.1 The Classical Framework (1969-1985)\nThe modern event study traces its origins to Eugene F. Fama et al. (1969), hereafter FFJR, who examined monthly stock returns around 940 stock splits between 1927 and 1959. Their key innovation was the use of the market model to decompose returns into expected (normal) and unexpected (abnormal) components.\nBall and Brown (2013) independently developed a similar approach to study earnings announcements, establishing the information content of accounting data. It was a finding with profound implications for both the efficient markets hypothesis and the relevance of financial reporting.\nBrown and Warner (1980) provided the first systematic analysis of event study methodology using simulation. Their study of monthly data established several important results: (i) the simple market model performs at least as well as more complex models, (ii) value-weighted market indices can lead to misspecification when the sample is tilted toward smaller firms, and (iii) the standard cross-sectional test has well-specified size under the null hypothesis. Their follow-up study (Brown and Warner 1985) extended the analysis to daily data, documenting the importance of non-normality in daily returns and the increased power of daily versus monthly studies.\n\n\n40.1.2 Risk Model Refinements (1992-2015)\nThe advent of the Fama-French three-factor model (Eugene F. Fama and French 1993) represented a major advance in modeling expected returns. Adding size (SMB) and value (HML) factors to the market model improved the cross-sectional fit of expected returns considerably. Carhart (1997) augmented this with a momentum factor (UMD), yielding the four-factor model that became standard in event studies through the 2000s. Eugene F. Fama and French (2015) subsequently introduced profitability (RMW) and investment (CMA) factors in their five-factor model.\nThe choice of risk model matters for event studies primarily in long-horizon settings. Kothari and Warner (2007) showed that for short-window studies (3-5 days), the market model and multi-factor models produce virtually identical results because the incremental factors explain very little daily return variation for individual firms. However, for event windows exceeding 20 trading days, model choice can materially affect inferences.\n\n\n40.1.3 Testing for Abnormal Returns (1976-2010)\nThe statistical testing of abnormal returns has evolved considerably:\n\n\n\nTable 40.1: Summary of major event study test statistics\n\n\n\n\n\n\n\n\n\n\n\nTest\nYear\nKey Property\nReference\n\n\n\n\nPatell Z\n1976\nStandardizes by estimation-period \\(\\sigma\\); weights firms inversely by volatility\nPatell (1976)\n\n\nCross-Sectional \\(t\\)\n1980\nAllows event-induced variance change\nBrown and Warner (1980)\n\n\nBMP\n1991\nRobust to event-induced variance\nBoehmer, Masumeci, and Poulsen (1991)\n\n\nCorrado Rank\n1989\nNon-parametric; robust to non-normality\nCorrado (1989)\n\n\nGeneralized Sign\n1992\nNon-parametric; uses estimation-window baseline\nCowan (1992)\n\n\nKolari-Pynnönen\n2010\nAccounts for cross-sectional dependence\nKolari and Pynnönen (2010)\n\n\nSkewness-Adjusted\n1992\nCorrects for BHAR skewness\nHall (1992)\n\n\n\n\n\n\n\n\n40.1.4 CARs versus BHARs\nCumulative abnormal returns (CARs) sum daily abnormal returns, while buy-and-hold abnormal returns (BHARs) compound returns and subtract the compounded benchmark. Barber and Lyon (1997) demonstrated that BHARs better capture the actual investor experience, since investors earn compound, not cumulative, returns. However, Eugene F. Fama (1998) and Mitchell and Stafford (2000) showed that BHARs exhibit severe cross-sectional dependence and positive skewness. For short event windows (under 10 days), the difference between CARs and BHARs is negligible. For longer windows, both should be reported.\n\n\n40.1.5 Emerging Market Considerations\nEvent studies in emerging markets face distinct challenges:\n\nThin trading. Many emerging market securities trade infrequently, inducing bias in market model beta estimates. Scholes and Williams (1977) and Dimson (1979) proposed corrections using leading and lagging market returns.\nFactor availability. While Fama-French factors are readily available for developed markets, emerging market factors must often be constructed locally.\nMarket microstructure. Price limits (\\(\\pm\\) 7% on HOSE, \\(\\pm\\) 10% on HNX, \\(\\pm\\) 15% on UPCOM in Vietnam), T+2 settlement, and the absence of short-selling affect the speed of price adjustment. Researchers should consider wider event windows to accommodate slower information incorporation (Bhattacharya et al. 2000; Griffin, Kelly, and Nardari 2010).",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "60_event_studies.html#mathematical-framework",
    "href": "60_event_studies.html#mathematical-framework",
    "title": "40  Event Studies in Finance",
    "section": "40.2 Mathematical Framework",
    "text": "40.2 Mathematical Framework\nThis section presents the complete mathematical specification of the event study methodology. We follow the notation conventions of Campbell et al. (1998) and Kothari and Warner (2007).\n\n40.2.1 Timeline and Windows\nThe event study timeline is defined relative to the event date, denoted \\(\\tau = 0\\). All dates are measured in trading days:\n\\[\n\\underbrace{T_0 + 1, \\ldots, T_1}_{\\text{Estimation Window (L₁ days)}} \\quad \\underbrace{\\quad}_{\\text{Gap (G days)}} \\quad \\underbrace{\\tau_1, \\ldots, 0, \\ldots, \\tau_2}_{\\text{Event Window (L₂ days)}}\n\\]\nwhere:\n\nEstimation window: \\(L_1\\) trading days over which the risk model parameters are estimated\nGap: \\(G\\) trading days separating estimation and event windows, preventing contamination by pre-event information leakage\nEvent window: \\(L_2 = \\tau_2 - \\tau_1 + 1\\) trading days centered around the event date\n\nFor example, with \\(L_1 = 150\\), \\(G = 15\\), \\(\\tau_1 = -10\\), \\(\\tau_2 = +10\\): the estimation window covers trading days \\([-175, -25]\\) relative to the event, and the event window covers \\([-10, +10]\\).\n\n\n40.2.2 Normal Return Models\nLet \\(R_{it}\\) denote the return on security \\(i\\) on trading day \\(t\\), \\(R_{ft}\\) the risk-free rate, and \\(R_{mt}\\) the market return. We implement six models:\nModel 0: Market-Adjusted Returns. Assumes \\(\\beta_i = 1\\) and \\(\\alpha_i = 0\\) for all firms:\n\\[\nAR_{it}^{MA} = R_{it} - R_{mt}\n\\]\nModel 1: Market Model (Sharpe 1964):\n\\[\nR_{it} = \\alpha_i + \\beta_i R_{mt} + \\varepsilon_{it}, \\quad E[\\varepsilon_{it}] = 0, \\quad \\text{Var}[\\varepsilon_{it}] = \\sigma^2_{\\varepsilon_i}\n\\]\n\\[\nAR_{it}^{MM} = R_{it} - \\hat{\\alpha}_i - \\hat{\\beta}_i R_{mt}\n\\]\nModel 2: Fama-French Three-Factor (Eugene F. Fama and French 1993):\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\varepsilon_{it}\n\\]\nModel 3: Carhart Four-Factor (Carhart 1997):\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\beta_{i,4} \\cdot UMD_t + \\varepsilon_{it}\n\\]\nModel 4: Fama-French Five-Factor (Eugene F. Fama and French 2015):\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\beta_{i,4} \\cdot RMW_t + \\beta_{i,5} \\cdot CMA_t + \\varepsilon_{it}\n\\]\nModel 5: User-Specified Factor Model:\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\sum_{k=1}^{K} \\beta_{i,k} F_{k,t} + \\varepsilon_{it}\n\\]\n\n\n40.2.3 Aggregation: CARs and BHARs\nCumulative Abnormal Returns sum daily abnormal returns:\n\\[\nCAR_i(\\tau_1, \\tau_2) = \\sum_{t=\\tau_1}^{\\tau_2} AR_{it}, \\qquad \\overline{CAR}(\\tau_1, \\tau_2) = \\frac{1}{N} \\sum_{i=1}^{N} CAR_i(\\tau_1, \\tau_2)\n\\]\nBuy-and-Hold Abnormal Returns compound returns:\n\\[\nBHAR_i(\\tau_1, \\tau_2) = \\prod_{t=\\tau_1}^{\\tau_2}(1 + R_{it}) - \\prod_{t=\\tau_1}^{\\tau_2}(1 + \\hat{E}[R_{it}])\n\\]\n\n\n40.2.4 Standardized Returns\nThe standardized abnormal return for firm \\(i\\) on day \\(t\\) is:\n\\[\nSAR_{it} = \\frac{AR_{it}}{\\hat{\\sigma}_{\\varepsilon_i}}\n\\]\nThe standardized cumulative abnormal return is:\n\\[\nSCAR_i(\\tau_1, \\tau_2) = \\frac{CAR_i(\\tau_1, \\tau_2)}{\\hat{\\sigma}_{\\varepsilon_i} \\sqrt{L_2}}\n\\]\n\n\n40.2.5 Test Statistics\nLet \\(N\\) denote the number of firm-event observations.\nTest 1: Cross-Sectional \\(t\\)-Test. Allows event-induced variance; assumes cross-sectional independence:\n\\[\nt_{CS} = \\frac{\\overline{CAR}}{s_{CAR}/\\sqrt{N}}, \\quad s_{CAR} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(CAR_i - \\overline{CAR})^2}\n\\]\nTest 2: Patell Z-Test (Patell 1976). Weights firms inversely by volatility:\n\\[\nZ_{Patell} = \\frac{\\sum_{i=1}^{N} SCAR_i}{\\sqrt{\\sum_{i=1}^{N} \\frac{K_i - 2}{K_i - 4}}}\n\\]\nTest 3: BMP Test (Boehmer, Masumeci, and Poulsen 1991). Robust to event-induced variance:\n\\[\nt_{BMP} = \\frac{\\overline{SCAR}}{s_{SCAR}/\\sqrt{N}}\n\\]\nTest 4: Kolari-Pynnönen Adjusted BMP (Kolari and Pynnönen 2010). Accounts for cross-sectional dependence:\n\\[\nt_{KP} = t_{BMP} \\times \\sqrt{\\frac{1}{1 + (N-1)\\bar{r}}}\n\\]\nwhere \\(\\bar{r}\\) is the mean pairwise cross-correlation of estimation-period residuals.\nTest 5: Generalized Sign Test (Cowan 1992):\n\\[\nZ_{GSign} = \\frac{\\hat{p} - \\hat{p}_0}{\\sqrt{\\hat{p}_0(1-\\hat{p}_0)/N}}\n\\]\nTest 6: Sign Test:\n\\[\nZ_{Sign} = \\frac{N^{+} - 0.5N}{\\sqrt{0.25N}}\n\\]\nTest 7: Skewness-Adjusted \\(t\\)-Test (Hall 1992):\n\\[\nt_{SA} = \\sqrt{N}\\left(\\bar{z} + \\frac{1}{3}\\hat{\\gamma}\\bar{z}^2 + \\frac{1}{27}\\hat{\\gamma}^2\\bar{z}^3 + \\frac{1}{6N}\\hat{\\gamma}\\right)\n\\]\nTest 8: Wilcoxon Signed-Rank Test: A non-parametric test of whether the median CAR differs from zero.\nThe table below summarizes the assumptions of each test:\n\n\n\nTable 40.2: Assumption requirements for event study test statistics\n\n\n\n\n\n\n\n\n\n\n\nTest\nEvent-Induced Variance\nCross-Sectional Independence\nNormality\n\n\n\n\nCross-Sectional \\(t\\)\nRobust\nAssumes\nAssumes\n\n\nPatell Z\nAssumes no change\nAssumes\nAssumes\n\n\nBMP\nRobust\nAssumes\nAssumes\n\n\nKolari-Pynnönen\nRobust\nRobust\nAssumes\n\n\nGeneralized Sign\nRobust\nAssumes\nRobust\n\n\nCorrado Rank\nRobust\nAssumes\nRobust\n\n\nSkewness-Adjusted\nRobust\nAssumes\nPartially\n\n\nWilcoxon\nRobust\nAssumes\nRobust",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "60_event_studies.html#python-implementation",
    "href": "60_event_studies.html#python-implementation",
    "title": "40  Event Studies in Finance",
    "section": "40.3 Python Implementation",
    "text": "40.3 Python Implementation\n\n40.3.1 Design Philosophy\nOur implementation follows these principles:\n\nModularity: Each component (calendar, estimation, AR computation, testing) is a separate function.\nVectorization: All operations use pandas/numpy for performance on large datasets.\nConfigurability: All parameters are user-configurable via a dataclass.\nTransparency: Intermediate outputs are preserved for inspection.\nProduction-ready: Comprehensive input validation, missing data handling, and edge cases.\n\n\n\n40.3.2 Setup and Imports\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Tuple\nfrom enum import Enum\nimport warnings\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', '{:.6f}'.format)\nprint(\"All libraries loaded.\")\n\nAll libraries loaded.\n\n\n\nimport pandas as pd\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_ff3_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff3_daily\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff5_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_daily\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff5_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\n\n\n40.3.3 Configuration\n\nclass RiskModel(Enum):\n    \"\"\"Supported risk models for expected return computation.\"\"\"\n    MARKET_ADJ = \"market_adjusted\"\n    MARKET_MODEL = \"market_model\"\n    FF3 = \"ff3\"\n    CARHART = \"carhart\"\n    FF5 = \"ff5\"\n    CUSTOM = \"custom\"\n\n@dataclass\nclass EventStudyConfig:\n    \"\"\"Complete configuration for an event study.\n    \n    Attributes\n    ----------\n    estimation_window : int\n        Length of estimation period in trading days. Brown and Warner (1985)\n        suggest ≥100 days; MacKinlay (1997) recommends 120 as standard.\n    event_window_start : int\n        Start of event window relative to event date (e.g., -10).\n    event_window_end : int\n        End of event window relative to event date (e.g., +10).\n    gap : int\n        Trading days between estimation and event windows. Prevents\n        contamination from pre-event information leakage.\n    min_estimation_obs : int\n        Minimum non-missing returns required in estimation period.\n    risk_model : RiskModel\n        Risk model for computing expected returns.\n    custom_factors : list\n        Column names for user-specified factors (CUSTOM model only).\n    thin_trading_adj : str or None\n        None, 'scholes_williams', or 'dimson'.\n    dimson_lags : int\n        Number of leads/lags for Dimson (1979) correction.\n    \"\"\"\n    estimation_window: int = 150\n    event_window_start: int = -10\n    event_window_end: int = 10\n    gap: int = 15\n    min_estimation_obs: int = 120\n    risk_model: RiskModel = RiskModel.MARKET_MODEL\n    custom_factors: List[str] = field(default_factory=list)\n    thin_trading_adj: Optional[str] = None\n    dimson_lags: int = 1\n    \n    @property\n    def event_window_length(self) -&gt; int:\n        return self.event_window_end - self.event_window_start + 1\n    \n    def validate(self):\n        assert self.estimation_window &gt; 0\n        assert self.event_window_start &lt;= self.event_window_end\n        assert self.gap &gt;= 0\n        assert self.min_estimation_obs &lt;= self.estimation_window\n        if self.risk_model == RiskModel.CUSTOM:\n            assert len(self.custom_factors) &gt; 0\n        return True\n\n# Demonstrate\nconfig_demo = EventStudyConfig(\n    estimation_window=150, event_window_start=-10, event_window_end=10,\n    gap=15, min_estimation_obs=120, risk_model=RiskModel.FF3\n)\nconfig_demo.validate()\nprint(f\"Event window length: {config_demo.event_window_length} days\")\nprint(f\"Model: {config_demo.risk_model.value}\")\n\nEvent window length: 21 days\nModel: ff3\n\n\n\n\n40.3.4 Step 1: Trading Calendar Construction\nA correct trading calendar is fundamental. It maps any event date to the exact calendar dates for the start/end of estimation and event windows, accounting for weekends, holidays, and non-trading days.\n\ndef build_trading_calendar(trading_dates, config):\n    \"\"\"Build a trading calendar mapping event dates to window boundaries.\n    \n    For each potential event date, identifies the calendar dates for the\n    start/end of the estimation period and event window using only actual\n    trading days.\n    \n    Parameters\n    ----------\n    trading_dates : array-like\n        Sorted unique trading dates in the market.\n    config : EventStudyConfig\n    \n    Returns\n    -------\n    pd.DataFrame with columns: estper_beg, estper_end, evtwin_beg,\n        evtdate, evtwin_end, cal_index\n    \"\"\"\n    dates = pd.Series(sorted(pd.to_datetime(trading_dates).unique()))\n    n = len(dates)\n    \n    L1 = config.estimation_window\n    G = config.gap\n    s = config.event_window_start\n    L2 = config.event_window_length\n    \n    # Offsets (FIRSTOBS logic)\n    o0 = 0                      # estper_beg\n    o1 = L1 - 1                 # estper_end\n    o2 = L1 + G                 # evtwin_beg\n    o3 = L1 + G - s             # evtdate\n    o4 = L1 + G + L2 - 1        # evtwin_end\n    \n    max_offset = o4\n    valid = n - max_offset\n    if valid &lt;= 0:\n        raise ValueError(f\"Need ≥{max_offset+1} trading dates, have {n}\")\n    \n    cal = pd.DataFrame({\n        'estper_beg': dates.iloc[o0:o0+valid].values,\n        'estper_end': dates.iloc[o1:o1+valid].values,\n        'evtwin_beg': dates.iloc[o2:o2+valid].values,\n        'evtdate':    dates.iloc[o3:o3+valid].values,\n        'evtwin_end': dates.iloc[o4:o4+valid].values,\n    })\n    cal['cal_index'] = range(1, len(cal)+1)\n    \n    # Validate window lengths using a sample row\n    idx = min(10, len(cal)-1)\n    row = cal.iloc[idx]\n    est_n = dates[(dates &gt;= row['estper_beg']) & (dates &lt;= row['estper_end'])].shape[0]\n    evt_n = dates[(dates &gt;= row['evtwin_beg']) & (dates &lt;= row['evtwin_end'])].shape[0]\n    assert est_n == L1, f\"Estimation window: {est_n} ≠ {L1}\"\n    assert evt_n == L2, f\"Event window: {evt_n} ≠ {L2}\"\n    \n    return cal\n\n# Demo\ndemo_dates = pd.bdate_range('2018-01-01', '2023-12-31', freq='B')\ndemo_cal = build_trading_calendar(demo_dates, config_demo)\nprint(f\"Calendar: {len(demo_cal)} potential event dates\")\nprint(demo_cal.head(3).to_string(index=False))\n\nCalendar: 1380 potential event dates\nestper_beg estper_end evtwin_beg    evtdate evtwin_end  cal_index\n2018-01-01 2018-07-27 2018-08-20 2018-09-03 2018-09-17          1\n2018-01-02 2018-07-30 2018-08-21 2018-09-04 2018-09-18          2\n2018-01-03 2018-07-31 2018-08-22 2018-09-05 2018-09-19          3\n\n\n\n\n40.3.5 Step 2: Event Date Alignment\nWhen an event occurs on a non-trading day, align to the next available trading day.\n\ndef align_events(events, calendar, id_col='symbol', date_col='event_date'):\n    \"\"\"Align event dates to trading calendar.\n    \n    Non-trading-day events are shifted forward to the next trading day.\n    \n    Parameters\n    ----------\n    events : pd.DataFrame with [id_col, date_col] and optional 'group'\n    calendar : pd.DataFrame from build_trading_calendar()\n    \n    Returns\n    -------\n    pd.DataFrame with window boundaries for each firm-event\n    \"\"\"\n    events = events.copy()\n    events[date_col] = pd.to_datetime(events[date_col])\n    \n    cal_dates = calendar[['evtdate']].drop_duplicates().sort_values('evtdate')\n    \n    merged = pd.merge_asof(\n        events.sort_values(date_col),\n        cal_dates.rename(columns={'evtdate': 'aligned_date'}),\n        left_on=date_col, right_on='aligned_date',\n        direction='forward'\n    )\n    \n    result = merged.merge(calendar, left_on='aligned_date', right_on='evtdate', how='inner')\n    \n    shifted = (result[date_col] != result['evtdate']).sum()\n    if shifted &gt; 0:\n        print(f\"  {shifted} event(s) shifted to next trading day\")\n    \n    result = result.rename(columns={date_col: 'original_date'})\n    result = result.drop_duplicates(subset=[id_col, 'evtdate'])\n    \n    return result\n\n\n\n40.3.6 Step 3: Data Extraction and Factor Merging\nExtract returns for each security-event across the full estimation + event window and merge risk factors.\n\ndef extract_returns(aligned_events, prices, factors, config,\n                    id_col='symbol', date_col='date', ret_col='ret',\n                    mkt_col='mkt_excess', rf_col='risk_free'):\n    \"\"\"Extract stock returns and merge risk factors for each event.\n    \n    For each security-event, retrieves daily returns from estper_beg\n    through evtwin_end and merges appropriate risk factors.\n    \"\"\"\n    prices = prices.copy()\n    factors = factors.copy()\n    prices[date_col] = pd.to_datetime(prices[date_col])\n    factors[date_col] = pd.to_datetime(factors[date_col])\n    \n    # Recover raw return from excess return if needed\n    if ret_col not in prices.columns and 'ret_excess' in prices.columns:\n        if rf_col in factors.columns:\n            prices = prices.merge(factors[[date_col, rf_col]].drop_duplicates(),\n                                  on=date_col, how='left')\n        prices[ret_col] = prices['ret_excess'] + prices[rf_col]\n    \n    # Factor columns based on model\n    model = config.risk_model\n    fac_cols = [mkt_col] if mkt_col in factors.columns else []\n    if rf_col in factors.columns:\n        fac_cols.append(rf_col)\n    \n    model_factors = {\n        RiskModel.FF3: ['smb', 'hml'],\n        RiskModel.CARHART: ['smb', 'hml', 'umd'],\n        RiskModel.FF5: ['smb', 'hml', 'rmw', 'cma'],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n    for f in model_factors.get(model, []):\n        if f in factors.columns:\n            fac_cols.append(f)\n    \n    fac_cols = list(set([date_col] + fac_cols))\n    \n    # Vectorized merge approach: join events with prices on id + date range\n    frames = []\n    for _, evt in aligned_events.iterrows():\n        mask = ((prices[id_col] == evt[id_col]) &\n                (prices[date_col] &gt;= evt['estper_beg']) &\n                (prices[date_col] &lt;= evt['evtwin_end']))\n        fd = prices.loc[mask, [id_col, date_col, ret_col]].copy()\n        if len(fd) == 0:\n            continue\n        fd['evtdate'] = evt['evtdate']\n        fd['estper_beg'] = evt['estper_beg']\n        fd['estper_end'] = evt['estper_end']\n        fd['evtwin_beg'] = evt['evtwin_beg']\n        fd['evtwin_end'] = evt['evtwin_end']\n        if 'group' in evt.index:\n            fd['group'] = evt['group']\n        frames.append(fd)\n    \n    if not frames:\n        raise ValueError(\"No return data found for any events\")\n    \n    result = pd.concat(frames, ignore_index=True)\n    result = result.merge(factors[fac_cols].drop_duplicates(), on=date_col, how='left')\n    \n    # Excess and market-adjusted returns\n    if rf_col in result.columns:\n        result['ret_excess'] = result[ret_col] - result[rf_col]\n    else:\n        result['ret_excess'] = result[ret_col]\n    if mkt_col in result.columns:\n        result['ret_mktadj'] = result['ret_excess'] - result[mkt_col]\n    \n    result = result.sort_values([id_col, 'evtdate', date_col]).reset_index(drop=True)\n    n_evts = result.groupby([id_col, 'evtdate']).ngroups\n    print(f\"  Extracted {len(result):,} obs for {n_evts} firm-events\")\n    return result\n\n\n\n40.3.7 Step 4: Risk Model Estimation\nEstimate risk model parameters over the estimation window.\n\ndef estimate_model(\n    event_returns, config, id_col=\"symbol\", date_col=\"date\", ret_col=\"ret\"\n):\n    \"\"\"Estimate risk model parameters for each firm-event.\n\n    Runs OLS over the estimation window. Returns alpha, betas, sigma,\n    R^2, nobs, and residuals for cross-correlation computation.\n    \"\"\"\n    model = config.risk_model\n\n    # Define regression specification\n    dep_var_map = {\n        RiskModel.MARKET_ADJ: \"ret_mktadj\",\n        RiskModel.MARKET_MODEL: ret_col,\n        RiskModel.FF3: \"ret_excess\",\n        RiskModel.CARHART: \"ret_excess\",\n        RiskModel.FF5: \"ret_excess\",\n        RiskModel.CUSTOM: \"ret_excess\",\n    }\n    indep_var_map = {\n        RiskModel.MARKET_ADJ: [],\n        RiskModel.MARKET_MODEL: [\"mkt_excess\"],\n        RiskModel.FF3: [\"mkt_excess\", \"smb\", \"hml\"],\n        RiskModel.CARHART: [\"mkt_excess\", \"smb\", \"hml\", \"umd\"],\n        RiskModel.FF5: [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n\n    dep_var = dep_var_map[model]\n    indep_vars = indep_var_map[model]\n\n    est = event_returns[\n        (event_returns[date_col] &gt;= event_returns[\"estper_beg\"])\n        & (event_returns[date_col] &lt;= event_returns[\"estper_end\"])\n    ].copy()\n\n    params_list = []\n\n    for (firm, evtdate), grp in est.groupby([id_col, \"evtdate\"]):\n        valid = grp.dropna(subset=[dep_var] + indep_vars)\n        nobs = len(valid)\n        if nobs &lt; config.min_estimation_obs:\n            continue\n\n        y = valid[dep_var].values\n\n        if len(indep_vars) == 0:\n            # Market-adjusted: intercept-only for variance\n            p = {\n                id_col: firm,\n                \"evtdate\": evtdate,\n                \"alpha\": y.mean(),\n                \"sigma\": y.std(ddof=1),\n                \"variance\": y.var(ddof=1),\n                \"nobs\": nobs,\n                \"r_squared\": 0.0,\n                \"_residuals\": y - y.mean(),\n            }\n        else:\n            X = sm.add_constant(valid[indep_vars].values)\n            res = sm.OLS(y, X).fit()\n            p = {\n                id_col: firm,\n                \"evtdate\": evtdate,\n                \"alpha\": res.params[0],\n                \"sigma\": np.sqrt(res.mse_resid),\n                \"variance\": res.mse_resid,\n                \"nobs\": nobs,\n                \"r_squared\": res.rsquared if np.isfinite(res.rsquared) else np.nan,\n                \"_residuals\": res.resid,\n            }\n            for j, var in enumerate(indep_vars):\n                p[f\"beta_{var}\"] = res.params[j + 1]\n\n        # Skip degenerate firms (zero or near-zero variance)\n        if p[\"sigma\"] &lt; 1e-6:\n            continue\n\n        params_list.append(p)\n\n    if not params_list:\n        raise ValueError(\"No firm-events passed minimum observation filter\")\n\n    params_df = pd.DataFrame(params_list)\n    n_total = event_returns.groupby([id_col, \"evtdate\"]).ngroups\n    print(\n        f\"  Estimated {len(params_df)}/{n_total} firm-events \"\n        f\"(mean R^2 = {params_df['r_squared'].dropna().mean():.4f})\"\n    )\n    return params_df\n\n\n\n40.3.8 Step 5: Abnormal Return Computation\nCompute AR, CAR, BHAR, SAR, SCAR for each firm-event-date.\n\ndef compute_abnormal_returns(\n    event_returns, params, config, id_col=\"symbol\", date_col=\"date\", ret_col=\"ret\"\n):\n    \"\"\"Compute abnormal returns and aggregate to CARs/BHARs.\n\n    Returns\n    -------\n    daily_ar : pd.DataFrame - daily AR/SAR/CAR/BHAR per firm-event-date\n    event_ar : pd.DataFrame - event-level CAR/BHAR/SCAR per firm-event\n    \"\"\"\n    model = config.risk_model\n\n    factor_map = {\n        RiskModel.MARKET_ADJ: [],\n        RiskModel.MARKET_MODEL: [\"mkt_excess\"],\n        RiskModel.FF3: [\"mkt_excess\", \"smb\", \"hml\"],\n        RiskModel.CARHART: [\"mkt_excess\", \"smb\", \"hml\", \"umd\"],\n        RiskModel.FF5: [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n    factor_cols = factor_map[model]\n\n    # Filter to event window\n    evt = event_returns[\n        (event_returns[date_col] &gt;= event_returns[\"evtwin_beg\"])\n        & (event_returns[date_col] &lt;= event_returns[\"evtwin_end\"])\n    ].copy()\n\n    # Merge params (drop residuals column for merge)\n    merge_cols = [c for c in params.columns if c != \"_residuals\"]\n    evt = evt.merge(params[merge_cols], on=[id_col, \"evtdate\"], how=\"inner\")\n\n    # Expected returns\n    if model == RiskModel.MARKET_ADJ:\n        evt[\"expected_ret\"] = evt.get(\"mkt_excess\", 0) + evt.get(\"risk_free\", 0)\n        evt[\"AR\"] = evt[ret_col] - evt[\"expected_ret\"]\n    else:\n        evt[\"expected_ret\"] = evt[\"alpha\"]\n        for fc in factor_cols:\n            bcol = f\"beta_{fc}\"\n            if bcol in evt.columns:\n                evt[\"expected_ret\"] += evt[bcol] * evt[fc]\n\n        if model == RiskModel.MARKET_MODEL:\n            evt[\"AR\"] = evt[ret_col] - evt[\"expected_ret\"]\n        else:\n            evt[\"AR\"] = evt[\"ret_excess\"] - evt[\"expected_ret\"]\n\n    evt[\"SAR\"] = evt[\"AR\"] / evt[\"sigma\"]\n    evt = evt.sort_values([id_col, \"evtdate\", date_col])\n\n    # Compute event time\n    all_dates = sorted(event_returns[date_col].unique())\n    d2i = {d: i for i, d in enumerate(all_dates)}\n    evt[\"evttime\"] = evt[date_col].map(d2i) - evt[\"evtdate\"].map(d2i)\n\n    # Cumulative measures per firm-event\n    daily_recs = []\n    event_recs = []\n\n    for (firm, evtdate), g in evt.groupby([id_col, \"evtdate\"]):\n        g = g.sort_values(date_col).copy()\n        nd = len(g)\n\n        g[\"CAR\"] = g[\"AR\"].cumsum()\n        g[\"cum_ret\"] = (1 + g[ret_col]).cumprod() - 1\n        g[\"cum_expected\"] = (1 + g[\"expected_ret\"]).cumprod() - 1\n        g[\"BHAR\"] = g[\"cum_ret\"] - g[\"cum_expected\"]\n        g[\"SCAR\"] = g[\"CAR\"] / (g[\"sigma\"].iloc[0] * np.sqrt(np.arange(1, nd + 1)))\n\n        daily_recs.append(g)\n\n        last = g.iloc[-1]\n        sigma = g[\"sigma\"].iloc[0]\n        nobs = g[\"nobs\"].iloc[0]\n\n        rec = {\n            id_col: firm,\n            \"evtdate\": evtdate,\n            \"CAR\": last[\"CAR\"],\n            \"BHAR\": last[\"BHAR\"],\n            \"cum_ret\": last[\"cum_ret\"],\n            \"SCAR\": last[\"CAR\"] / (sigma * np.sqrt(nd)),\n            \"sigma\": sigma,\n            \"variance\": g[\"variance\"].iloc[0],\n            \"nobs\": nobs,\n            \"n_event_days\": nd,\n            \"alpha\": g[\"alpha\"].iloc[0],\n            \"pat_scale\": (nobs - 2) / (nobs - 4) if nobs &gt; 4 else np.nan,\n            \"pos_car\": int(last[\"CAR\"] &gt; 0),\n        }\n\n        for fc in factor_cols:\n            bcol = f\"beta_{fc}\"\n            if bcol in g.columns:\n                rec[bcol] = g[bcol].iloc[0]\n        if \"group\" in g.columns:\n            rec[\"group\"] = g[\"group\"].iloc[0]\n\n        event_recs.append(rec)\n\n    daily_ar = pd.concat(daily_recs, ignore_index=True)\n    event_ar = pd.DataFrame(event_recs)\n\n    print(\n        f\"  {len(event_ar)} firm-events | Mean CAR: {event_ar['CAR'].mean():.6f} | \"\n        f\"Mean BHAR: {event_ar['BHAR'].mean():.6f} | \"\n        f\"% positive: {event_ar['pos_car'].mean():.1%}\"\n    )\n    return daily_ar, event_ar\n\n\n\n40.3.9 Step 6: Comprehensive Test Statistics\nEight tests covering parametric, non-parametric, and cross-correlation-robust approaches.\n\ndef compute_test_statistics(event_ar, params=None, group_col=None):\n    \"\"\"Compute comprehensive test statistics for abnormal returns.\n    \n    Implements 8 tests with varying assumptions about variance,\n    cross-dependence, and distributional form.\n    \"\"\"\n    def _stats(data, label=None):\n        N = len(data)\n        if N &lt; 3:\n            return None\n        \n        cars = data['CAR'].values\n        bhars = data['BHAR'].values\n        scars = data['SCAR'].values\n        pos = data['pos_car'].values\n        \n        m_car, s_car = np.mean(cars), np.std(cars, ddof=1)\n        m_scar, s_scar = np.mean(scars), np.std(scars, ddof=1)\n        \n        r = {'group': label or 'All', 'N': N,\n             'mean_CAR': m_car, 'median_CAR': np.median(cars),\n             'std_CAR': s_car, 'mean_BHAR': np.mean(bhars),\n             'pct_positive': np.mean(pos)}\n        \n        # 1. Cross-sectional t\n        t1 = m_car / (s_car / np.sqrt(N)) if s_car &gt; 0 else np.nan\n        r['t_CS'] = t1\n        r['p_CS'] = 2 * (1 - stats.t.cdf(abs(t1), N-1)) if np.isfinite(t1) else np.nan\n        \n        # 2. Patell Z\n        if 'pat_scale' in data.columns:\n            ps = data['pat_scale'].dropna().values\n            z2 = np.sum(scars[:len(ps)]) / np.sqrt(np.sum(ps)) if len(ps) &gt; 0 else np.nan\n        else:\n            z2 = m_scar * np.sqrt(N)\n        r['Z_Patell'] = z2\n        r['p_Patell'] = 2*(1-stats.norm.cdf(abs(z2))) if np.isfinite(z2) else np.nan\n        \n        # 3. BMP\n        t3 = m_scar / (s_scar / np.sqrt(N)) if s_scar &gt; 0 else np.nan\n        r['t_BMP'] = t3\n        r['p_BMP'] = 2*(1-stats.t.cdf(abs(t3), N-1)) if np.isfinite(t3) else np.nan\n        \n        # 4. Kolari-Pynnönen\n        rbar = 0.0\n        if params is not None and '_residuals' in params.columns:\n            resids = [row['_residuals'] for _, row in params.iterrows()\n                      if isinstance(row.get('_residuals'), np.ndarray)]\n            if len(resids) &gt; 1:\n                ml = min(len(x) for x in resids)\n                aligned = np.column_stack([x[:ml] for x in resids])\n                cm = np.corrcoef(aligned.T)\n                np.fill_diagonal(cm, 0)\n                rbar = cm.sum() / (len(resids) * (len(resids)-1))\n        \n        adj = np.sqrt(1/(1+(N-1)*rbar)) if (1+(N-1)*rbar) &gt; 0 else 1\n        t4 = t3 * adj if np.isfinite(t3) else np.nan\n        r['t_KP'] = t4\n        r['p_KP'] = 2*(1-stats.t.cdf(abs(t4), N-1)) if np.isfinite(t4) else np.nan\n        r['r_bar'] = rbar\n        \n        # 5. Generalized sign test\n        p_hat = np.mean(pos)\n        z5 = (p_hat - 0.5) / np.sqrt(0.25 / N)\n        r['Z_GSign'] = z5\n        r['p_GSign'] = 2*(1-stats.norm.cdf(abs(z5)))\n        \n        # 6. Sign test\n        r['Z_Sign'] = z5  # Same formula with p0=0.5\n        r['p_Sign'] = r['p_GSign']\n        \n        # 7. Skewness-adjusted t\n        if s_scar &gt; 0:\n            zb = m_scar / s_scar\n            gam = stats.skew(scars)\n            t7 = np.sqrt(N) * (zb + gam*zb**2/3 + gam**2*zb**3/27 + gam/(6*N))\n            r['t_SkAdj'] = t7\n            r['p_SkAdj'] = 2*(1-stats.t.cdf(abs(t7), N-1)) if np.isfinite(t7) else np.nan\n        \n        # 8. Wilcoxon signed-rank\n        try:\n            w, pw = stats.wilcoxon(cars, alternative='two-sided')\n            r['W_Wilcoxon'] = w\n            r['p_Wilcoxon'] = pw\n        except:\n            r['W_Wilcoxon'] = r['p_Wilcoxon'] = np.nan\n        \n        return r\n    \n    results = [_stats(event_ar)]\n    if group_col and group_col in event_ar.columns:\n        for gv, gd in event_ar.groupby(group_col):\n            s = _stats(gd, label=gv)\n            if s:\n                results.append(s)\n    \n    return pd.DataFrame([r for r in results if r is not None])\n\n\ndef compute_daily_stats(daily_ar, id_col='symbol'):\n    \"\"\"Compute test statistics at each event time t.\"\"\"\n    rows = []\n    for t, g in daily_ar.groupby('evttime'):\n        n = g[id_col].nunique()\n        if n &lt; 2:\n            continue\n        m_ar = g['AR'].mean()\n        s_ar = g['AR'].std(ddof=1)\n        t_ar = m_ar / (s_ar/np.sqrt(n)) if s_ar &gt; 0 else np.nan\n        rows.append({'evttime': t, 'N': n, 'mean_AR': m_ar,\n                     'mean_CAR': g['CAR'].mean(), 'mean_BHAR': g['BHAR'].mean(),\n                     'mean_cum_ret': g.get('cum_ret', pd.Series()).mean(),\n                     't_AR': t_ar})\n    return pd.DataFrame(rows).sort_values('evttime')\n\n\n\n40.3.10 Step 7: Publication-Ready Visualization\n\ndef plot_event_study(daily_stats, title=\"Cumulative Abnormal Returns Around Event Date\",\n                     figsize=(12, 7), save_path=None):\n    \"\"\"Publication-ready event study plot with CAR, BHAR, and daily AR panels.\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=figsize, height_ratios=[3, 1],\n                              gridspec_kw={'hspace': 0.05})\n    ds = daily_stats.sort_values('evttime')\n    t = ds['evttime'].values\n    \n    # Top: cumulative returns\n    ax = axes[0]\n    ax.plot(t, ds['mean_CAR']*100, color='#2166AC', lw=2.5, label='Mean CAR')\n    ax.plot(t, ds['mean_BHAR']*100, color='#B2182B', lw=2, ls='--', label='Mean BHAR')\n    if 'mean_cum_ret' in ds.columns:\n        ax.plot(t, ds['mean_cum_ret']*100, color='#666', lw=1.5, ls=':', \n                label='Mean Cum. Return', alpha=0.7)\n    ax.axvline(0, color='k', lw=0.8, alpha=0.5)\n    ax.axhline(0, color='k', lw=0.5, alpha=0.3)\n    ax.set_ylabel('Cumulative Return (%)', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.legend(loc='upper left', fontsize=10)\n    ax.grid(True, alpha=0.2)\n    ax.set_xticklabels([])\n    \n    # Bottom: daily AR bars\n    ax2 = axes[1]\n    colors = ['#2166AC' if v &gt;= 0 else '#B2182B' for v in ds['mean_AR']]\n    ax2.bar(t, ds['mean_AR']*100, color=colors, alpha=0.7, width=0.8)\n    if 't_AR' in ds.columns:\n        sig = np.abs(ds['t_AR'].values) &gt; 1.96\n        if sig.any():\n            ax2.scatter(t[sig], ds['mean_AR'].values[sig]*100, \n                       color='gold', s=40, marker='*', zorder=4, label='p&lt;0.05')\n            ax2.legend(fontsize=9)\n    ax2.axvline(0, color='k', lw=0.8, alpha=0.5)\n    ax2.axhline(0, color='k', lw=0.5, alpha=0.3)\n    ax2.set_xlabel('Event Time (Trading Periods)', fontsize=12)\n    ax2.set_ylabel('Mean AR (%)', fontsize=10)\n    ax2.grid(True, alpha=0.2)\n    \n    for a in axes:\n        a.spines['top'].set_visible(False)\n        a.spines['right'].set_visible(False)\n    plt.tight_layout()\n    if save_path:\n        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n    return fig\n\n\ndef plot_car_distribution(event_ar, var='CAR', figsize=(12, 5)):\n    \"\"\"Cross-sectional distribution of CARs with histogram and QQ plot.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n    data = event_ar[var].dropna() * 100\n    \n    ax1.hist(data, bins=50, density=True, alpha=0.6, color='#2166AC', edgecolor='white')\n    ax1.axvline(data.mean(), color='k', ls='--', lw=1.5, \n                label=f'Mean={data.mean():.2f}%')\n    ax1.axvline(data.median(), color='gray', ls=':', lw=1.5,\n                label=f'Median={data.median():.2f}%')\n    ax1.set_xlabel(f'{var} (%)')\n    ax1.set_ylabel('Density')\n    ax1.set_title(f'Distribution of {var}', fontweight='bold')\n    ax1.legend()\n    ax1.spines['top'].set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    \n    # QQ plot\n    (osm, osr), (slope, intercept, r) = stats.probplot(data, dist='norm')\n    ax2.scatter(osm, osr, alpha=0.4, s=10, color='#2166AC')\n    ax2.plot(osm, slope*np.array(osm)+intercept, 'r--', lw=1)\n    ax2.set_xlabel('Theoretical Quantiles')\n    ax2.set_ylabel('Sample Quantiles')\n    ax2.set_title('Q-Q Plot (Normal)', fontweight='bold')\n    ax2.spines['top'].set_visible(False)\n    ax2.spines['right'].set_visible(False)\n    \n    plt.tight_layout()\n    return fig\n\n\n\n40.3.11 The Master Pipeline\nCombine all components into one function:\n\ndef run_event_study(events, prices, factors, config,\n                    id_col='symbol', date_col='date', ret_col='ret',\n                    event_date_col='event_date', mkt_col='mkt_excess',\n                    rf_col='risk_free', group_col=None, verbose=True):\n    \"\"\"Run a complete event study from raw inputs to test statistics.\n    \n    This is the main entry point. Provide your events, price data,\n    factor data, and configuration—get back everything you need.\n    \n    Parameters\n    ----------\n    events : pd.DataFrame\n        Columns: [id_col, event_date_col], optional 'group'.\n    prices : pd.DataFrame\n        Daily returns: [id_col, date_col, ret_col or 'ret_excess', rf_col].\n    factors : pd.DataFrame\n        Factor returns: [date_col, mkt_col, 'smb', 'hml', ...].\n    config : EventStudyConfig\n    \n    Returns\n    -------\n    dict with keys: 'config', 'daily_ar', 'event_ar', 'daily_stats',\n        'test_stats', 'params'\n    \"\"\"\n    config.validate()\n    \n    if verbose:\n        print(f\"═══ Event Study: {config.risk_model.value} model ═══\")\n        print(f\"  Windows: estimation={config.estimation_window}, \"\n              f\"gap={config.gap}, event=({config.event_window_start},{config.event_window_end})\")\n        print(f\"  Min obs: {config.min_estimation_obs}\\n\")\n    \n    # 1. Trading calendar\n    if verbose: print(\"Step 1: Building trading calendar...\")\n    trading_dates = pd.Series(sorted(prices[date_col].unique()))\n    calendar = build_trading_calendar(trading_dates, config)\n    if verbose: print(f\"  {len(calendar)} potential event dates\\n\")\n    \n    # 2. Align events\n    if verbose: print(\"Step 2: Aligning events to trading calendar...\")\n    aligned = align_events(events, calendar, id_col, event_date_col)\n    if verbose: print(f\"  {len(aligned)} aligned events\\n\")\n    \n    # 3. Extract returns\n    if verbose: print(\"Step 3: Extracting returns and merging factors...\")\n    evt_rets = extract_returns(aligned, prices, factors, config,\n                               id_col, date_col, ret_col, mkt_col, rf_col)\n    if verbose: print()\n    \n    # 4. Estimate model\n    if verbose: print(\"Step 4: Estimating risk model parameters...\")\n    params = estimate_model(evt_rets, config, id_col, date_col, ret_col)\n    if verbose: print()\n    \n    # 5. Compute abnormal returns\n    if verbose: print(\"Step 5: Computing abnormal returns...\")\n    daily_ar, event_ar = compute_abnormal_returns(\n        evt_rets, params, config, id_col, date_col, ret_col)\n    if verbose: print()\n    \n    # 6. Test statistics\n    if verbose: print(\"Step 6: Computing test statistics...\")\n    test_stats = compute_test_statistics(event_ar, params, group_col)\n    daily_stats = compute_daily_stats(daily_ar, id_col)\n    if verbose:\n        print(f\"  Done.\\n\")\n        print(\"═══ Results Summary ═══\")\n        cols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 't_BMP', 'p_BMP', 't_KP', 'p_KP']\n        avail = [c for c in cols if c in test_stats.columns]\n        print(test_stats[avail].to_string(index=False))\n    \n    return {\n        'config': config,\n        'params': params,\n        'daily_ar': daily_ar,\n        'event_ar': event_ar,\n        'daily_stats': daily_stats,\n        'test_stats': test_stats,\n        'calendar': calendar,\n    }\n\nprint(\"Master pipeline ready.\")\n\nMaster pipeline ready.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "60_event_studies.html#demonstration-with-simulated-data",
    "href": "60_event_studies.html#demonstration-with-simulated-data",
    "title": "40  Event Studies in Finance",
    "section": "40.4 Demonstration with Simulated Data",
    "text": "40.4 Demonstration with Simulated Data\nSince we are building a general-purpose framework (the actual event data will be supplied later), we demonstrate the full pipeline with realistic simulated data.\n\nnp.random.seed(2024)\n\n# --- Simulated trading calendar (Vietnamese market: ~245 days/year) ---\ndates = pd.bdate_range('2019-01-01', '2023-12-31', freq='B')\n# Remove Tet + national holidays (simplified)\ntet_holidays = pd.to_datetime([\n    '2019-02-04','2019-02-05','2019-02-06','2019-02-07','2019-02-08',\n    '2020-01-23','2020-01-24','2020-01-27','2020-01-28','2020-01-29',\n    '2021-02-10','2021-02-11','2021-02-12','2021-02-15','2021-02-16',\n    '2022-01-31','2022-02-01','2022-02-02','2022-02-03','2022-02-04',\n    '2023-01-20','2023-01-23','2023-01-24','2023-01-25','2023-01-26',\n])\ndates = dates.difference(tet_holidays)\nT = len(dates)\n\n# --- Simulated factors (realistic Vietnamese market parameters) ---\nrf_daily = 0.04 / 252  # ~4% annual risk-free\nmkt_excess = np.random.normal(0.0003, 0.012, T)  # ~7.5% annual, ~19% vol\nsmb = np.random.normal(0.0001, 0.006, T)\nhml = np.random.normal(0.0001, 0.005, T)\nrmw = np.random.normal(0.00005, 0.004, T)\ncma = np.random.normal(0.00005, 0.004, T)\n\nfactors_sim = pd.DataFrame({\n    'date': dates, 'mkt_excess': mkt_excess, 'smb': smb, 'hml': hml,\n    'rmw': rmw, 'cma': cma, 'risk_free': rf_daily\n})\n\n# --- 100 simulated stocks ---\nn_stocks = 100\nsymbols = [f'SIM{i:03d}' for i in range(n_stocks)]\nbetas = np.random.uniform(0.5, 1.5, n_stocks)\nalphas = np.random.normal(0, 0.0002, n_stocks)\nidio_vols = np.random.uniform(0.015, 0.035, n_stocks)\n\nprice_rows = []\nfor i, sym in enumerate(symbols):\n    eps = np.random.normal(0, idio_vols[i], T)\n    rets = alphas[i] + betas[i] * mkt_excess + 0.3*smb + 0.2*hml + eps\n    for j in range(T):\n        price_rows.append({\n            'symbol': sym, 'date': dates[j], 'ret': rets[j],\n            'ret_excess': rets[j] - rf_daily,\n            'risk_free': rf_daily,\n            'mktcap': np.random.uniform(100, 5000),\n        })\n\nprices_sim = pd.DataFrame(price_rows)\n\n# --- Simulated events: 50 random firm-dates with KNOWN positive AR ---\nevent_indices = np.random.choice(range(250, T-50), 50, replace=False)\nevent_firms = np.random.choice(symbols, 50, replace=True)\nevent_dates_sim = [dates[i] for i in event_indices]\n\n# Inject abnormal returns on event date (2% positive shock)\nfor firm, edate in zip(event_firms, event_dates_sim):\n    mask = (prices_sim['symbol'] == firm) & (prices_sim['date'] == edate)\n    prices_sim.loc[mask, 'ret'] += 0.02\n    prices_sim.loc[mask, 'ret_excess'] += 0.02\n\nevents_sim = pd.DataFrame({\n    'symbol': event_firms,\n    'event_date': event_dates_sim,\n    'group': np.random.choice([1, 2], 50)\n})\n\nprint(f\"Simulated data: {n_stocks} stocks × {T} days = {len(prices_sim):,} obs\")\nprint(f\"Events: {len(events_sim)} firm-event pairs\")\nprint(f\"Injected abnormal return: +2% on event date\")\n\nSimulated data: 100 stocks × 1279 days = 127,900 obs\nEvents: 50 firm-event pairs\nInjected abnormal return: +2% on event date\n\n\n\n40.4.1 Running the Full Pipeline\n\nconfig = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\nresults = run_event_study(\n    events=events_sim,\n    prices=prices_sim,\n    factors=factors_sim,\n    config=config,\n    group_col='group'\n)\n\n═══ Event Study: ff3 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  1094 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 9,300 obs for 50 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 50/50 firm-events (mean R^2 = 0.2368)\n\nStep 5: Computing abnormal returns...\n  50 firm-events | Mean CAR: 0.033009 | Mean BHAR: 0.032498 | % positive: 60.0%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\ngroup  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n  All 50  0.033009   0.032498      0.600000 2.288362 0.026468 2.157106 0.035929 2.161291 0.035587\n    1 20  0.030704   0.028326      0.650000 1.568676 0.133227 1.248382 0.227056 1.249320 0.226720\n    2 30  0.034545   0.035280      0.566667 1.688848 0.101975 1.734699 0.093413 1.736689 0.093055\n\n\n\n\n40.4.2 Visualizing Results\n\nfig1 = plot_event_study(\n    results['daily_stats'],\n    title=\"Event Study: FF3 Model — Simulated Vietnamese Market\"\n)\nplt.show()\n\n\n\n\n\n\n\nFigure 40.1: Dynamics of cumulative abnormal returns (CARs) and buy-and-hold abnormal returns (BHARs) around the event date. The positive jump at t=0 reflects the injected 2% abnormal return.\n\n\n\n\n\n\nfig2 = plot_car_distribution(results['event_ar'], 'CAR')\nplt.show()\n\n\n\n\n\n\n\nFigure 40.2: Cross-sectional distribution of cumulative abnormal returns. The rightward shift from zero and positive skewness are consistent with the injected positive event effect.\n\n\n\n\n\n\n\n40.4.3 Complete Test Statistics\n\n\n\nTable 40.3: Event study test statistics for the full sample and by subgroup\n\n\n# Format for display\nts = results['test_stats'].copy()\n\n# Select key columns\ndisplay_cols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 'Z_Patell', 'p_Patell',\n                't_BMP', 'p_BMP', 't_KP', 'p_KP',\n                'Z_GSign', 'p_GSign', 't_SkAdj', 'p_SkAdj']\navail = [c for c in display_cols if c in ts.columns]\ndisplay_df = ts[avail].copy()\n\n# Format\nfor c in display_df.columns:\n    if c in ['N']:\n        display_df[c] = display_df[c].astype(int)\n    elif c.startswith('p_'):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4f}' if pd.notna(x) else '')\n    elif c in ['mean_CAR', 'mean_BHAR']:\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4%}' if pd.notna(x) else '')\n    elif c == 'pct_positive':\n        display_df[c] = display_df[c].map(lambda x: f'{x:.1%}' if pd.notna(x) else '')\n    elif isinstance(display_df[c].iloc[0], (int, float, np.floating)):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(display_df.to_string(index=False))\n\ngroup  N mean_CAR mean_BHAR pct_positive  t_CS   p_CS Z_Patell p_Patell t_BMP  p_BMP  t_KP   p_KP Z_GSign p_GSign t_SkAdj p_SkAdj\n  All 50  3.3009%   3.2498%        60.0% 2.288 0.0265    1.832   0.0669 2.157 0.0359 2.161 0.0356   1.414  0.1573   2.074  0.0434\n    1 20  3.0704%   2.8326%        65.0% 1.569 0.1332    1.020   0.3075 1.248 0.2271 1.249 0.2267   1.342  0.1797   1.103  0.2836\n    2 30  3.4545%   3.5280%        56.7% 1.689 0.1020    1.532   0.1255 1.735 0.0934 1.737 0.0931   0.730  0.4652   1.727  0.0949\n\n\n\n\n\n\n40.4.4 Running Multiple Models for Robustness\nA key best practice is to report results across multiple risk models. If conclusions are robust across models, this strengthens the findings:\n\nmodels_to_run = [\n    (\"Market-Adjusted\", RiskModel.MARKET_ADJ),\n    (\"Market Model\", RiskModel.MARKET_MODEL),\n    (\"Fama-French 3\", RiskModel.FF3),\n    (\"Fama-French 5\", RiskModel.FF5),\n]\n\nrobustness = []\nfor name, mdl in models_to_run:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=-10, event_window_end=10,\n        gap=15, min_estimation_obs=120, risk_model=mdl\n    )\n    res = run_event_study(events_sim, prices_sim, factors_sim, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    robustness.append({\n        'Model': name,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.2f}\",\n        't (BMP)': f\"{full['t_BMP']:.2f}\",\n        't (KP)': f\"{full.get('t_KP', np.nan):.2f}\",\n    })\n\nrob_df = pd.DataFrame(robustness)\nprint(\"Robustness Across Risk Models:\")\nprint(rob_df.to_string(index=False))\n\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.0000)\n  50 firm-events | Mean CAR: 0.029672 | Mean BHAR: 0.026468 | % positive: 62.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2198)\n  50 firm-events | Mean CAR: 0.033785 | Mean BHAR: 0.029974 | % positive: 62.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2368)\n  50 firm-events | Mean CAR: 0.033009 | Mean BHAR: 0.032498 | % positive: 60.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2516)\n  50 firm-events | Mean CAR: 0.036479 | Mean BHAR: 0.036000 | % positive: 64.0%\nRobustness Across Risk Models:\n          Model  N Mean CAR Mean BHAR % Positive t (CS) t (BMP) t (KP)\nMarket-Adjusted 50  2.9672%   2.6468%      62.0%   2.12    2.03   2.01\n   Market Model 50  3.3785%   2.9974%      62.0%   2.37    2.26   2.28\n  Fama-French 3 50  3.3009%   3.2498%      60.0%   2.29    2.16   2.16\n  Fama-French 5 50  3.6479%   3.6000%      64.0%   2.51    2.36   2.41",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "60_event_studies.html#how-to-use-this-framework-with-your-data",
    "href": "60_event_studies.html#how-to-use-this-framework-with-your-data",
    "title": "40  Event Studies in Finance",
    "section": "40.5 How to Use This Framework with Your Data",
    "text": "40.5 How to Use This Framework with Your Data\n\n40.5.1 Required Data Format\nTo run the event study on real Vietnamese market data, prepare three inputs:\n1. Stock Returns (prices DataFrame):\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nsymbol\nStock ticker\n'VNM'\n\n\ndate\nTrading date\n2023-06-15\n\n\nret or ret_excess\nDaily return (decimal)\n0.0123\n\n\nrisk_free\nDaily risk-free rate\n0.000159\n\n\n\n2. Factor Returns (factors DataFrame):\n\n\n\nColumn\nDescription\n\n\n\n\ndate\nTrading date\n\n\nmkt_excess\nMarket excess return\n\n\nsmb\nSize factor (FF3/FF5)\n\n\nhml\nValue factor (FF3/FF5)\n\n\nrmw\nProfitability factor (FF5)\n\n\ncma\nInvestment factor (FF5)\n\n\nrisk_free\nRisk-free rate\n\n\n\n3. Event File (events DataFrame):\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nsymbol\nStock ticker\n'VNM'\n\n\nevent_date\nEvent date\n2023-03-15\n\n\ngroup\n(Optional) subgroup\n1\n\n\n\n\n\n40.5.2 Minimal Usage Example\n# Load your data\nprices = pd.read_csv('prices_daily.csv', parse_dates=['date'])\nfactors = pd.read_csv('factors_ff3_daily.csv', parse_dates=['date'])\nevents = pd.read_csv('my_events.csv', parse_dates=['event_date'])\n\n# Configure\nconfig = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-5,\n    event_window_end=5,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\n# Run\nresults = run_event_study(events, prices, factors, config)\n\n# Access outputs\nresults['test_stats']    # Test statistics\nresults['event_ar']      # Firm-level CARs/BHARs\nresults['daily_ar']      # Daily abnormal returns\nresults['daily_stats']   # Event-time aggregates\n\n# Plot\nplot_event_study(results['daily_stats'], title=\"My Event Study\")",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "60_event_studies.html#demonstration-with-vietnamese-market-data",
    "href": "60_event_studies.html#demonstration-with-vietnamese-market-data",
    "title": "40  Event Studies in Finance",
    "section": "40.6 Demonstration with Vietnamese Market Data",
    "text": "40.6 Demonstration with Vietnamese Market Data\nWe now demonstrate the full event study pipeline using actual Vietnamese stock market data. The datasets available are:\n\nprices_daily: symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\nprices_monthly: same structure\nfactors_ff3_daily: date, smb, hml, mkt_excess, risk_free\nfactors_ff3_monthly — monthly frequency version\nfactors_ff5_daily: date, smb, hml, mkt_excess, risk_free, rmw, cma\nfactors_ff5_monthly\n\nSince our data provides ret_excess rather than raw returns, we recover raw returns as \\(R_{it} = R^e_{it} + R_{f,t}\\), and the market return as \\(R_{m,t} = R^e_{m,t} + R_{f,t}\\). The extract_event_returns() function handles this automatically.\n\n40.6.1 Loading the Data\n\n# --- Recover raw returns ---\n# ret = ret_excess + risk_free\nprices_daily['ret'] = prices_daily['ret_excess'] + prices_daily['risk_free']\nprices_monthly['ret'] = prices_monthly['ret_excess'] + prices_monthly['risk_free']\n\n# --- Inspect the data ---\nprint(\"=\" * 70)\nprint(\"VIETNAMESE MARKET DATA SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"\\nprices_daily: {prices_daily.shape[0]:,} rows, \"\n      f\"{prices_daily['symbol'].nunique()} stocks, \"\n      f\"{prices_daily['date'].min().date()} to {prices_daily['date'].max().date()}\")\nprint(f\"prices_monthly: {prices_monthly.shape[0]:,} rows, \"\n      f\"{prices_monthly['symbol'].nunique()} stocks\")\nprint(f\"\\nfactors_ff3_daily: {factors_ff3_daily.shape[0]:,} trading days\")\nprint(f\"  Columns: {list(factors_ff3_daily.columns)}\")\nprint(f\"factors_ff5_daily: {factors_ff5_daily.shape[0]:,} trading days\")\nprint(f\"  Columns: {list(factors_ff5_daily.columns)}\")\nprint(f\"\\nSample daily returns:\")\nprint(prices_daily[['symbol', 'date', 'ret_excess', 'ret', 'risk_free', 'mktcap']]\n      .head(5).to_string(index=False))\nprint(f\"\\nSample daily factors:\")\nprint(factors_ff3_daily.head(5).to_string(index=False))\n\n======================================================================\nVIETNAMESE MARKET DATA SUMMARY\n======================================================================\n\nprices_daily: 3,462,157 rows, 1459 stocks, 2010-01-05 to 2023-12-29\nprices_monthly: 165,499 rows, 1457 stocks\n\nfactors_ff3_daily: 3,126 trading days\n  Columns: ['date', 'smb', 'hml', 'mkt_excess', 'risk_free']\nfactors_ff5_daily: 3,126 trading days\n  Columns: ['date', 'smb', 'hml', 'rmw', 'cma', 'mkt_excess', 'risk_free']\n\nSample daily returns:\nsymbol       date  ret_excess      ret  risk_free     mktcap\n   A32 2018-10-24   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-25   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-26   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-29   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-30   -0.000159 0.000000   0.000159 176.120000\n\nSample daily factors:\n      date       smb       hml  mkt_excess  risk_free\n2011-07-01  0.008587  0.000967   -0.019862   0.000159\n2011-07-04  0.005099 -0.001099   -0.000633   0.000159\n2011-07-05 -0.009088  0.010152    0.013314   0.000159\n2011-07-06  0.004875 -0.003918   -0.008045   0.000159\n2011-07-07 -0.011239 -0.000584    0.003391   0.000159\n\n\n\n\n40.6.2 Creating Sample Events\nFor this demonstration, we create a sample event file. In practice, events would come from corporate announcements (earnings, M&A, dividends), regulatory changes, or other information shocks. Here we select 50 large-cap Vietnamese stocks and assign random event dates from the most recent two years of data to illustrate the pipeline mechanics.\n\nnp.random.seed(2024)\n\n# Select the 50 largest stocks by median market cap\nlargest = (prices_daily.groupby('symbol')['mktcap']\n           .median()\n           .nlargest(50)\n           .index.tolist())\n\n# Date range for events: last 2 years of data, with buffer for windows\ndate_range = prices_daily['date'].sort_values().unique()\nn_dates = len(date_range)\n# Events from the middle portion (need room for estimation + event windows)\nevent_eligible = date_range[int(n_dates * 0.3):int(n_dates * 0.85)]\n\n# Generate 50 random firm-event pairs\nevent_firms = np.random.choice(largest, 50, replace=True)\nevent_dates = np.random.choice(event_eligible, 50, replace=False)\n\nevents_demo = pd.DataFrame({\n    'symbol': event_firms,\n    'event_date': pd.to_datetime(event_dates),\n    'group': np.random.choice(['Group_A', 'Group_B'], 50)\n})\n\n# Remove any duplicate firm-date pairs\nevents_demo = events_demo.drop_duplicates(subset=['symbol', 'event_date'])\n\nprint(f\"Sample event file: {len(events_demo)} firm-event observations\")\nprint(f\"Unique firms: {events_demo['symbol'].nunique()}\")\nprint(f\"Date range: {events_demo['event_date'].min().date()} to \"\n      f\"{events_demo['event_date'].max().date()}\")\nprint(f\"\\nGroup distribution:\")\nprint(events_demo['group'].value_counts().to_string())\nprint(f\"\\nFirst 10 events:\")\nprint(events_demo.sort_values('event_date').head(10).to_string(index=False))\n\nSample event file: 50 firm-event observations\nUnique firms: 35\nDate range: 2014-06-25 to 2021-10-29\n\nGroup distribution:\ngroup\nGroup_B    26\nGroup_A    24\n\nFirst 10 events:\nsymbol event_date   group\n   MCH 2014-06-25 Group_B\n   SIP 2014-10-23 Group_B\n   VRE 2014-11-14 Group_B\n   QNS 2014-12-25 Group_A\n   FOX 2015-01-16 Group_B\n   THD 2015-01-26 Group_A\n   QNS 2015-02-12 Group_A\n   HNG 2015-05-07 Group_B\n   MML 2015-08-17 Group_B\n   ACV 2015-10-15 Group_A\n\n\n\n\n40.6.3 Daily Event Study: Fama-French 3-Factor Model\n\nconfig_ff3 = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\nresults_ff3 = run_event_study(\n    events=events_demo,\n    prices=prices_daily,\n    factors=factors_ff3_daily,\n    config=config_ff3,\n    group_col='group'\n)\n\n═══ Event Study: ff3 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  3308 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 5,421 obs for 30 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n\nStep 5: Computing abnormal returns...\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n    All 26  0.021513   0.024885      0.615385 0.774999 0.445609 0.726797 0.474101 0.699973 0.490407\nGroup_A 13  0.008601   0.006783      0.538462 0.211606 0.835966 0.577574 0.574229 0.567041 0.581138\nGroup_B 13  0.034425   0.042987      0.692308 0.879892 0.396197 0.437902 0.669236 0.429917 0.674875\n\n\n\n\n40.6.4 Visualizing Daily Results\n\nfig1 = plot_event_study(\n    results_ff3['daily_stats'],\n    title=\"Event Study: Fama-French 3-Factor Model — Vietnamese Market (Daily)\"\n)\nplt.show()\n\n\n\n\n\n\n\nFigure 40.3: Cumulative abnormal returns around event dates for Vietnamese stocks using the Fama-French 3-factor model. The event window spans [-10, +10] trading days.\n\n\n\n\n\n\nfig2 = plot_car_distribution(results_ff3['event_ar'], 'CAR')\nplt.show()\n\n\n\n\n\n\n\nFigure 40.4: Cross-sectional distribution of cumulative abnormal returns (CARs) across firm-events. The histogram and Q-Q plot assess normality assumptions underlying parametric tests.\n\n\n\n\n\n\n\n40.6.5 Complete Test Statistics (Daily)\n\n\n\nTable 40.4: Event study test statistics for the full sample and by subgroup — Daily frequency, FF3 model\n\n\nts = results_ff3['test_stats'].copy()\n\ndisplay_cols = ['group', 'N', 'mean_CAR', 'median_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 'Z_Patell', 'p_Patell',\n                't_BMP', 'p_BMP', 't_KP', 'p_KP',\n                'Z_GSign', 'p_GSign', 't_SkAdj', 'p_SkAdj',\n                'W_Wilcoxon', 'p_Wilcoxon']\navail = [c for c in display_cols if c in ts.columns]\ndisplay_df = ts[avail].copy()\n\nfor c in display_df.columns:\n    if c in ['N']:\n        display_df[c] = display_df[c].astype(int)\n    elif c == 'group':\n        continue\n    elif c.startswith('p_'):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4f}' if pd.notna(x) else '')\n    elif c in ['mean_CAR', 'median_CAR', 'mean_BHAR']:\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4%}' if pd.notna(x) else '')\n    elif c == 'pct_positive':\n        display_df[c] = display_df[c].map(lambda x: f'{x:.1%}' if pd.notna(x) else '')\n    elif isinstance(display_df[c].iloc[0], (int, float, np.floating)):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(display_df.to_string(index=False))\n\n  group  N mean_CAR median_CAR mean_BHAR pct_positive  t_CS   p_CS Z_Patell p_Patell t_BMP  p_BMP  t_KP   p_KP Z_GSign p_GSign t_SkAdj p_SkAdj W_Wilcoxon p_Wilcoxon\n    All 26  2.1513%    1.5701%   2.4885%        61.5% 0.775 0.4456    0.961   0.3364 0.727 0.4741 0.700 0.4904   1.177  0.2393   0.738  0.4676    158.000     0.6710\nGroup_A 13  0.8601%    2.5330%   0.6783%        53.8% 0.212 0.8360    0.739   0.4596 0.578 0.5742 0.567 0.5811   0.277  0.7815   0.597  0.5618     45.000     1.0000\nGroup_B 13  3.4425%    1.4169%   4.2987%        69.2% 0.880 0.3962    0.620   0.5352 0.438 0.6692 0.430 0.6749   1.387  0.1655   0.444  0.6647     35.000     0.4973\n\n\n\n\n\n\n40.6.6 Robustness: Multiple Risk Models (Daily)\n\n\n\nTable 40.5: Robustness of event study results across risk models — Daily frequency\n\n\nmodels_daily = [\n    (\"Market-Adjusted\",  RiskModel.MARKET_ADJ,    factors_ff3_daily),\n    (\"Market Model\",     RiskModel.MARKET_MODEL,   factors_ff3_daily),\n    (\"Fama-French 3\",    RiskModel.FF3,            factors_ff3_daily),\n    (\"Fama-French 5\",    RiskModel.FF5,            factors_ff5_daily),\n]\n\nrobustness_daily = []\nfor name, mdl, facs in models_daily:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=-10, event_window_end=10,\n        gap=15, min_estimation_obs=120, risk_model=mdl\n    )\n    res = run_event_study(events_demo, prices_daily, facs, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    robustness_daily.append({\n        'Model': name,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Median CAR': f\"{full['median_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.3f}\",\n        'p (CS)': f\"{full['p_CS']:.4f}\",\n        't (BMP)': f\"{full['t_BMP']:.3f}\",\n        'p (BMP)': f\"{full['p_BMP']:.4f}\",\n        't (KP)': f\"{full.get('t_KP', np.nan):.3f}\",\n        'p (KP)': f\"{full.get('p_KP', np.nan):.4f}\",\n    })\n\nrob_daily_df = pd.DataFrame(robustness_daily)\nprint(\"Robustness Across Risk Models (Daily Frequency)\")\nprint(\"=\" * 100)\nprint(rob_daily_df.to_string(index=False))\n\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 28/30 firm-events (mean R^2 = 0.0000)\n  28 firm-events | Mean CAR: 0.035338 | Mean BHAR: 0.036936 | % positive: 50.0%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.1960)\n  26 firm-events | Mean CAR: 0.032107 | Mean BHAR: 0.033221 | % positive: 61.5%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2675)\n  26 firm-events | Mean CAR: 0.025684 | Mean BHAR: 0.028399 | % positive: 57.7%\nRobustness Across Risk Models (Daily Frequency)\n====================================================================================================\n          Model  N Mean CAR Median CAR Mean BHAR % Positive t (CS) p (CS) t (BMP) p (BMP) t (KP) p (KP)\nMarket-Adjusted 28  3.5338%    0.2610%   3.6936%      50.0%  1.390 0.1758   1.103  0.2798  1.062 0.2975\n   Market Model 26  3.2107%    0.6925%   3.3221%      61.5%  1.198 0.2422   1.146  0.2625  1.107 0.2789\n  Fama-French 3 26  2.1513%    1.5701%   2.4885%      61.5%  0.775 0.4456   0.727  0.4741  0.700 0.4904\n  Fama-French 5 26  2.5684%    2.3619%   2.8399%      57.7%  0.968 0.3422   0.987  0.3332  0.962 0.3451\n\n\n\n\n\n\n40.6.7 Robustness: Multiple Event Windows\nA key practice is to examine sensitivity to the event window specification:\n\n\n\nTable 40.6: Sensitivity of results to event window specification\n\n\nwindows = [\n    (\"(-1, +1)\",  -1, 1),\n    (\"(-3, +3)\",  -3, 3),\n    (\"(-5, +5)\",  -5, 5),\n    (\"(-10, +10)\", -10, 10),\n    (\"(-1, +5)\",  -1, 5),\n    (\"(-5, +1)\",  -5, 1),\n    (\"(0, 0)\",     0, 0),\n]\n\nwindow_results = []\nfor label, ws, we in windows:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=ws, event_window_end=we,\n        gap=15, min_estimation_obs=120, risk_model=RiskModel.FF3\n    )\n    res = run_event_study(events_demo, prices_daily, factors_ff3_daily, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    window_results.append({\n        'Window': label,\n        'Days': we - ws + 1,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.3f}\",\n        't (BMP)': f\"{full['t_BMP']:.3f}\",\n        'p (BMP)': f\"{full['p_BMP']:.4f}\",\n    })\n\nwin_df = pd.DataFrame(window_results)\nprint(\"Sensitivity to Event Window Specification (FF3 Model)\")\nprint(\"=\" * 90)\nprint(win_df.to_string(index=False))\n\n  Extracted 4,899 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2147)\n  26 firm-events | Mean CAR: 0.004074 | Mean BHAR: 0.004648 | % positive: 50.0%\n  Extracted 5,015 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2155)\n  26 firm-events | Mean CAR: 0.003761 | Mean BHAR: 0.004327 | % positive: 42.3%\n  Extracted 5,131 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2217)\n  26 firm-events | Mean CAR: -0.001133 | Mean BHAR: 0.001027 | % positive: 42.3%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n  Extracted 5,019 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2147)\n  26 firm-events | Mean CAR: -0.005096 | Mean BHAR: -0.005148 | % positive: 42.3%\n  Extracted 5,011 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2217)\n  26 firm-events | Mean CAR: 0.008600 | Mean BHAR: 0.010441 | % positive: 46.2%\n  Extracted 4,841 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2150)\n  26 firm-events | Mean CAR: 0.000344 | Mean BHAR: 0.000502 | % positive: 46.2%\nSensitivity to Event Window Specification (FF3 Model)\n==========================================================================================\n    Window  Days  N Mean CAR Mean BHAR % Positive t (CS) t (BMP) p (BMP)\n  (-1, +1)     3 26  0.4074%   0.4648%      50.0%  0.456   0.601  0.5535\n  (-3, +3)     7 26  0.3761%   0.4327%      42.3%  0.198   0.361  0.7211\n  (-5, +5)    11 26 -0.1133%   0.1027%      42.3% -0.049   0.030  0.9761\n(-10, +10)    21 26  2.1513%   2.4885%      61.5%  0.775   0.727  0.4741\n  (-1, +5)     7 26 -0.5096%  -0.5148%      42.3% -0.385  -0.068  0.9460\n  (-5, +1)     7 26  0.8600%   1.0441%      46.2%  0.439   0.429  0.6715\n    (0, 0)     1 26  0.0344%   0.0502%      46.2%  0.064  -0.020  0.9840\n\n\n\n\n\n\n40.6.8 Monthly Event Study: Fama-French 3-Factor Model\nFor longer-horizon studies, monthly frequency is appropriate. Note that the estimation window is specified in months rather than days:\n\n# Create monthly events aligned to the monthly data\n# Map daily event dates to the corresponding month-end\nevents_monthly = events_demo.copy()\nevents_monthly['event_date'] = events_monthly['event_date'].dt.to_period('M').dt.to_timestamp('M')\n\n# Use month-end dates from monthly prices\nmonthly_dates = prices_monthly['date'].sort_values().unique()\n\n# Filter events to dates present in monthly data\nevents_monthly = events_monthly[events_monthly['event_date'].isin(monthly_dates)]\nevents_monthly = events_monthly.drop_duplicates(subset=['symbol', 'event_date'])\n\nconfig_monthly = EventStudyConfig(\n    estimation_window=36,     # 36 months\n    event_window_start=-3,    # 3 months before\n    event_window_end=3,       # 3 months after\n    gap=3,                    # 3-month gap\n    min_estimation_obs=24,    # At least 24 months\n    risk_model=RiskModel.FF3\n)\n\nif len(events_monthly) &gt; 0:\n    results_monthly = run_event_study(\n        events=events_monthly,\n        prices=prices_monthly,\n        factors=factors_ff3_monthly,\n        config=config_monthly,\n        group_col='group'\n    )\n    \n    print(\"\\n--- Monthly Test Statistics ---\")\n    ts_m = results_monthly['test_stats']\n    mcols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n             't_CS', 'p_CS', 't_BMP', 'p_BMP']\n    mavail = [c for c in mcols if c in ts_m.columns]\n    print(ts_m[mavail].to_string(index=False))\nelse:\n    print(\"No monthly events could be aligned. Skipping monthly study.\")\n\n═══ Event Study: ff3 model ═══\n  Windows: estimation=36, gap=3, event=(-3,3)\n  Min obs: 24\n\nStep 1: Building trading calendar...\n  122 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 1,036 obs for 33 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 18/33 firm-events (mean R^2 = 0.3218)\n\nStep 5: Computing abnormal returns...\n  18 firm-events | Mean CAR: -0.005257 | Mean BHAR: -0.014576 | % positive: 55.6%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive      t_CS     p_CS     t_BMP    p_BMP      t_KP     p_KP\n    All 18 -0.005257  -0.014576      0.555556 -0.081058 0.936342 -0.249547 0.805928 -0.320212 0.752709\nGroup_A  7 -0.141756  -0.135084      0.428571 -1.102110 0.312648 -1.357452 0.223472 -1.462577 0.193905\nGroup_B 11  0.081606   0.062111      0.636364  1.390317 0.194599  1.177372 0.266309  1.342593 0.209085\n\n--- Monthly Test Statistics ---\n  group  N  mean_CAR  mean_BHAR  pct_positive      t_CS     p_CS     t_BMP    p_BMP\n    All 18 -0.005257  -0.014576      0.555556 -0.081058 0.936342 -0.249547 0.805928\nGroup_A  7 -0.141756  -0.135084      0.428571 -1.102110 0.312648 -1.357452 0.223472\nGroup_B 11  0.081606   0.062111      0.636364  1.390317 0.194599  1.177372 0.266309\n\n\n\nif len(events_monthly) &gt; 0 and 'daily_stats' in results_monthly:\n    fig3 = plot_event_study(\n        results_monthly['daily_stats'],\n        title=\"Event Study: FF3 Model — Vietnamese Market (Monthly)\"\n    )\n    plt.show()\n\n\n\n\n\n\n\nFigure 40.5: Monthly cumulative abnormal returns around event dates. Wider windows capture slower information incorporation typical of emerging markets.\n\n\n\n\n\n\n\n40.6.9 Daily Event Study: Fama-French 5-Factor Model\n\nconfig_ff5 = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF5\n)\n\nresults_ff5 = run_event_study(\n    events=events_demo,\n    prices=prices_daily,\n    factors=factors_ff5_daily,\n    config=config_ff5,\n    group_col='group'\n)\n\n═══ Event Study: ff5 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  3308 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 5,421 obs for 30 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 26/30 firm-events (mean R^2 = 0.2675)\n\nStep 5: Computing abnormal returns...\n  26 firm-events | Mean CAR: 0.025684 | Mean BHAR: 0.028399 | % positive: 57.7%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n    All 26  0.025684   0.028399      0.576923 0.968332 0.342154 0.986788 0.333201 0.962252 0.345139\nGroup_A 13  0.015352   0.013489      0.461538 0.393655 0.700741 0.786232 0.446980 0.776663 0.452395\nGroup_B 13  0.036016   0.043309      0.692308 0.965100 0.353542 0.585915 0.568789 0.578785 0.573438\n\n\n\n\n40.6.10 Comparing FF3 vs FF5 Estimation Quality\n\n\n\nTable 40.7: Comparison of estimation quality between FF3 and FF5 models\n\n\nparams_ff3 = results_ff3['params']\nparams_ff5 = results_ff5['params']\n\nprint(\"Model Estimation Diagnostics\")\nprint(\"=\" * 60)\nprint(f\"\\n{'Metric':&lt;30} {'FF3':&gt;12} {'FF5':&gt;12}\")\nprint(\"-\" * 54)\nprint(f\"{'Firm-events estimated':&lt;30} {len(params_ff3):&gt;12} {len(params_ff5):&gt;12}\")\nprint(f\"{'Mean R^2':&lt;30} {params_ff3['r_squared'].mean():&gt;12.4f} {params_ff5['r_squared'].mean():&gt;12.4f}\")\nprint(f\"{'Median R^2':&lt;30} {params_ff3['r_squared'].median():&gt;12.4f} {params_ff5['r_squared'].median():&gt;12.4f}\")\nprint(f\"{'Mean σ(ε)':&lt;30} {params_ff3['sigma'].mean():&gt;12.6f} {params_ff5['sigma'].mean():&gt;12.6f}\")\nprint(f\"{'Mean |α|':&lt;30} {params_ff3['alpha'].abs().mean():&gt;12.6f} {params_ff5['alpha'].abs().mean():&gt;12.6f}\")\nprint(f\"{'Mean β(MKT)':&lt;30} {params_ff3['beta_mkt_excess'].mean():&gt;12.4f} {params_ff5['beta_mkt_excess'].mean():&gt;12.4f}\")\nif 'beta_smb' in params_ff3.columns:\n    print(f\"{'Mean β(SMB)':&lt;30} {params_ff3['beta_smb'].mean():&gt;12.4f} {params_ff5['beta_smb'].mean():&gt;12.4f}\")\nif 'beta_hml' in params_ff3.columns:\n    print(f\"{'Mean β(HML)':&lt;30} {params_ff3['beta_hml'].mean():&gt;12.4f} {params_ff5['beta_hml'].mean():&gt;12.4f}\")\nif 'beta_rmw' in params_ff5.columns:\n    print(f\"{'Mean β(RMW)':&lt;30} {'—':&gt;12} {params_ff5['beta_rmw'].mean():&gt;12.4f}\")\nif 'beta_cma' in params_ff5.columns:\n    print(f\"{'Mean β(CMA)':&lt;30} {'—':&gt;12} {params_ff5['beta_cma'].mean():&gt;12.4f}\")\n\nModel Estimation Diagnostics\n============================================================\n\nMetric                                  FF3          FF5\n------------------------------------------------------\nFirm-events estimated                    26           26\nMean R^2                             0.2245       0.2675\nMedian R^2                           0.1943       0.2692\nMean σ(ε)                          0.021753     0.021351\nMean |α|                           0.001022     0.001130\nMean β(MKT)                          0.8867       0.9721\nMean β(SMB)                         -0.0434       0.0265\nMean β(HML)                          0.2489       0.1493\nMean β(RMW)                               —      -0.0934\nMean β(CMA)                               —       0.1070\n\n\n\n\n\n\n40.6.11 Event-Level Detail\n\n\n\nTable 40.8: Event-level detail: CARs and BHARs for each firm-event (FF3 model)\n\n\ndetail = results_ff3['event_ar'].copy()\ndetail_cols = ['symbol', 'evtdate', 'CAR', 'BHAR', 'SCAR', 'sigma',\n               'nobs', 'alpha', 'beta_mkt_excess']\ndetail_avail = [c for c in detail_cols if c in detail.columns]\ndetail_show = detail[detail_avail].copy()\ndetail_show['CAR'] = detail_show['CAR'].map(lambda x: f'{x:.4%}')\ndetail_show['BHAR'] = detail_show['BHAR'].map(lambda x: f'{x:.4%}')\ndetail_show['SCAR'] = detail_show['SCAR'].map(lambda x: f'{x:.3f}')\n\nprint(\"Event-Level Results (first 20 firm-events)\")\nprint(\"=\" * 100)\nprint(detail_show.head(20).to_string(index=False))\n\nEvent-Level Results (first 20 firm-events)\n====================================================================================================\nsymbol    evtdate       CAR      BHAR   SCAR    sigma  nobs     alpha  beta_mkt_excess\n   BVH 2016-10-20 -12.6456% -11.6522% -1.603 0.017219   150  0.001193         1.449182\n   DHG 2016-01-25  16.1151%  17.1149%  2.273 0.015472   150 -0.000224         0.754245\n   DNH 2019-10-14   1.7233%   1.8068%  0.104 0.036035   150 -0.000781         1.861996\n   DPM 2020-07-30  -5.9576%  -6.2025% -0.545 0.023873   150  0.001279         0.313932\n   FOX 2021-01-13   2.7478%   2.9194%  0.329 0.018243   150 -0.000696         0.148058\n   GAS 2020-02-11   0.5371%   0.7801%  0.100 0.011694   150  0.000501         1.851155\n   GEX 2020-08-20  11.9985%  13.5843%  1.197 0.021883   150  0.000944         1.583418\n   IDC 2018-10-01   1.3889%   0.5651%  0.094 0.032120   150 -0.000674         0.809305\n   MML 2021-10-29 -12.0139% -12.3759% -1.141 0.022985   150  0.002976         0.260588\n   MSN 2015-10-23  -5.7205%  -5.5645% -0.718 0.017375   150  0.001177         0.453951\n   PGV 2019-06-26  -7.5892%  -9.1366% -0.359 0.046165   150  0.000502         0.151377\n   PLX 2020-01-07   2.5330%   2.7847%  0.423 0.013067   150 -0.000176         0.917578\n   PLX 2020-06-01  -6.1517%  -6.0085% -0.739 0.018168   150 -0.000465         1.815178\n   POW 2020-12-23   7.7751%   9.1225%  1.130 0.015014   150 -0.000575         0.774423\n   PVD 2020-05-25   4.8827%   5.6674%  0.510 0.020875   150 -0.001522         1.316102\n   PVS 2017-08-07   2.1360%   2.1918%  0.297 0.015715   150 -0.000341         1.136273\n   QNS 2018-01-18   4.3001%   3.4011%  0.530 0.017703   150 -0.003716         0.128328\n   SAB 2017-08-24   6.0347%   6.7732%  0.748 0.017614   150  0.003284         2.123751\n   SNZ 2019-03-12  34.5230%  33.1105%  2.145 0.035121   150  0.000235        -1.015554\n   VCI 2020-01-20  -3.3191%  -2.9072% -0.458 0.015797   150  0.000335        -0.086268\n\n\n\n\n\n\n40.6.12 Daily Abnormal Return Dynamics\n\n\n\nTable 40.9: Daily dynamics of mean abnormal returns and test statistics within the event window\n\n\nds = results_ff3['daily_stats'].copy()\nds_cols = ['evttime', 'N', 'mean_AR', 'mean_CAR', 'mean_BHAR', 't_AR_CS', 't_AR_BMP']\nds_avail = [c for c in ds_cols if c in ds.columns]\nds_show = ds[ds_avail].copy()\n\nfor c in ['mean_AR', 'mean_CAR', 'mean_BHAR']:\n    if c in ds_show.columns:\n        ds_show[c] = ds_show[c].map(lambda x: f'{x:.4%}')\nfor c in ['t_AR_CS', 't_AR_BMP']:\n    if c in ds_show.columns:\n        ds_show[c] = ds_show[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(\"Daily Event-Window Dynamics (FF3 Model)\")\nprint(\"=\" * 80)\nprint(ds_show.to_string(index=False))\n\nDaily Event-Window Dynamics (FF3 Model)\n================================================================================\n evttime  N  mean_AR mean_CAR mean_BHAR\n     -10 23  0.3571%  0.3571%   0.3729%\n      -9 23  0.0337%  0.3907%   0.4209%\n      -8 23 -0.1581%  0.2326%   0.2766%\n      -7 23  0.3691%  0.6018%   0.6581%\n      -6 23  1.3416%  1.9433%   2.0278%\n      -5 23  0.1509%  2.0942%   2.2313%\n      -4 23  0.5512%  2.6454%   2.9187%\n      -3 23 -0.4641%  2.1814%   2.4297%\n      -2 23  0.2412%  2.4225%   2.8028%\n      -1 23  0.5660%  2.9885%   3.3240%\n       0 23 -0.0281%  2.9604%   3.4926%\n       1 23 -0.2564%  2.7040%   3.1039%\n       2 23 -0.4421%  2.2619%   2.5764%\n       3 23  0.6337%  2.8956%   3.4659%\n       4 23 -0.8432%  2.0524%   2.4738%\n       5 23 -0.4174%  1.6350%   2.1339%\n       6 23 -0.2272%  1.4078%   1.8085%\n       7 23 -0.2085%  1.1993%   1.6819%\n       8 23  0.0893%  1.2886%   1.8609%\n       9 23  0.3307%  1.6193%   2.1658%\n      10 23  0.5320%  2.1513%   2.4885%\n\n\n\n\n\n\n40.6.13 Summary of Key Findings\n\nprint(\"=\" * 70)\nprint(\"EVENT STUDY RESULTS SUMMARY\")\nprint(\"=\" * 70)\n\nff3_all = results_ff3['test_stats'][results_ff3['test_stats']['group'] == 'All'].iloc[0]\n\nprint(f\"\\nSample: {int(ff3_all['N'])} firm-event observations\")\nprint(f\"Frequency: Daily\")\nprint(f\"Primary Model: Fama-French 3-Factor\")\nprint(f\"Estimation Window: {config_ff3.estimation_window} trading days\")\nprint(f\"Event Window: ({config_ff3.event_window_start}, {config_ff3.event_window_end})\")\nprint(f\"Gap: {config_ff3.gap} trading days\")\nprint(f\"\\n--- Abnormal Return Measures ---\")\nprint(f\"Mean CAR({config_ff3.event_window_start},{config_ff3.event_window_end}): \"\n      f\"{ff3_all['mean_CAR']:.4%}\")\nprint(f\"Median CAR: {ff3_all['median_CAR']:.4%}\")\nprint(f\"Mean BHAR: {ff3_all['mean_BHAR']:.4%}\")\nprint(f\"Fraction positive CARs: {ff3_all['pct_positive']:.1%}\")\nprint(f\"\\n--- Statistical Significance ---\")\nprint(f\"Cross-Sectional t: {ff3_all['t_CS']:.3f} (p = {ff3_all['p_CS']:.4f})\")\nprint(f\"Patell Z: {ff3_all['Z_Patell']:.3f} (p = {ff3_all['p_Patell']:.4f})\")\nprint(f\"BMP t: {ff3_all['t_BMP']:.3f} (p = {ff3_all['p_BMP']:.4f})\")\nprint(f\"Kolari-Pynnönen t: {ff3_all['t_KP']:.3f} (p = {ff3_all['p_KP']:.4f})\")\nprint(f\"Generalized Sign Z: {ff3_all['Z_GSign']:.3f} (p = {ff3_all['p_GSign']:.4f})\")\n\nsig_005 = sum(1 for k in ['p_CS','p_Patell','p_BMP','p_KP','p_GSign','p_SkAdj','p_Wilcoxon']\n              if k in ff3_all and pd.notna(ff3_all[k]) and ff3_all[k] &lt; 0.05)\ntotal_tests = sum(1 for k in ['p_CS','p_Patell','p_BMP','p_KP','p_GSign','p_SkAdj','p_Wilcoxon']\n                  if k in ff3_all and pd.notna(ff3_all[k]))\nprint(f\"\\n{sig_005}/{total_tests} tests significant at 5% level\")\n\n# Robustness note\nprint(f\"\\nRobustness: Results checked across {len(models_daily)} risk models \"\n      f\"and {len(windows)} event windows\")\n\n======================================================================\nEVENT STUDY RESULTS SUMMARY\n======================================================================\n\nSample: 26 firm-event observations\nFrequency: Daily\nPrimary Model: Fama-French 3-Factor\nEstimation Window: 150 trading days\nEvent Window: (-10, 10)\nGap: 15 trading days\n\n--- Abnormal Return Measures ---\nMean CAR(-10,10): 2.1513%\nMedian CAR: 1.5701%\nMean BHAR: 2.4885%\nFraction positive CARs: 61.5%\n\n--- Statistical Significance ---\nCross-Sectional t: 0.775 (p = 0.4456)\nPatell Z: 0.961 (p = 0.3364)\nBMP t: 0.727 (p = 0.4741)\nKolari-Pynnönen t: 0.700 (p = 0.4904)\nGeneralized Sign Z: 1.177 (p = 0.2393)\n\n0/7 tests significant at 5% level\n\nRobustness: Results checked across 4 risk models and 7 event windows",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "60_event_studies.html#practical-recommendations",
    "href": "60_event_studies.html#practical-recommendations",
    "title": "40  Event Studies in Finance",
    "section": "40.7 Practical Recommendations",
    "text": "40.7 Practical Recommendations\nBased on the literature and our implementation experience:\n\nEstimation window: Use 150 trading days (~7 months) for daily studies. This balances parameter precision against structural breaks. For monthly studies, 60 months is standard (Kothari and Warner 2007).\nGap: 15 trading days is standard. Increase to 30 if information leakage is a concern.\nEvent window: Start with (-1, +1) for short-window tests, then expand to (-5, +5) and (-10, +10) for robustness. Report all windows.\nModel choice: Always report market model as the baseline. Add FF3 or FF5 for robustness. For Vietnam, local factors are preferable to global factors.\nTest statistics: Report at minimum: cross-sectional t (for ease of interpretation), BMP (robust to event-induced variance), and one non-parametric test (sign or Wilcoxon). Report Kolari-Pynnönen if events cluster in calendar time.\nThin trading: For Vietnamese small-caps, consider Dimson (1979) with 1 lead/lag or increase min_estimation_obs to filter out illiquid stocks.\nMultiple testing: If testing multiple event windows or subgroups, apply Bonferroni or Holm corrections to control family-wise error rate.\n\n\n\n\n\n\n\nAharony, Joseph, and Itzhak Swary. 1980. “Quarterly Dividend and Earnings Announcements and Stockholders’ Returns: An Empirical Analysis.” The Journal of Finance 35 (1): 1–12.\n\n\nAndrade, Gregor, Mark Mitchell, and Erik Stafford. 2001. “New Evidence and Perspectives on Mergers.” Journal of Economic Perspectives 15 (2): 103–20.\n\n\nBall, Ray, and Philip Brown. 2013. “An Empirical Evaluation of Accounting Income Numbers.” In Financial Accounting and Equity Markets, 27–46. Routledge.\n\n\nBarber, Brad M, and John D Lyon. 1997. “Detecting Long-Run Abnormal Stock Returns: The Empirical Power and Specification of Test Statistics.” Journal of Financial Economics 43 (3): 341–72.\n\n\nBernard, Victor L, and Jacob K Thomas. 1989. “Post-Earnings-Announcement Drift: Delayed Price Response or Risk Premium?” Journal of Accounting Research 27: 1–36.\n\n\nBhattacharya, Utpal, Hazem Daouk, Brian Jorgenson, and Carl-Heinrich Kehr. 2000. “When an Event Is Not an Event: The Curious Case of an Emerging Market.” Journal of Financial Economics 55 (1): 69–101.\n\n\nBinder, John. 1998. “The Event Study Methodology Since 1969.” Review of Quantitative Finance and Accounting 11 (2): 111–37.\n\n\nBoehmer, Ekkehart, Jim Masumeci, and Annette B Poulsen. 1991. “Event-Study Methodology Under Conditions of Event-Induced Variance.” Journal of Financial Economics 30 (2): 253–72.\n\n\nBrown, Stephen J, and Jerold B Warner. 1980. “Measuring Security Price Performance.” Journal of Financial Economics 8 (3): 205–58.\n\n\n———. 1985. “Using Daily Stock Returns: The Case of Event Studies.” Journal of Financial Economics 14 (1): 3–31.\n\n\nCampbell, John Y, Andrew W Lo, A Craig MacKinlay, and Robert F Whitelaw. 1998. “The Econometrics of Financial Markets.” Macroeconomic Dynamics 2 (4): 559–62.\n\n\nCarhart, Mark M. 1997. “On persistence in mutual fund performance.” The Journal of Finance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nCorrado, Charles J. 1989. “A Nonparametric Test for Abnormal Security-Price Performance in Event Studies.” Journal of Financial Economics 23 (2): 385–95.\n\n\nCowan, Arnold Richard. 1992. “Nonparametric Event Study Tests.” Review of Quantitative Finance and Accounting 2 (4): 343–58.\n\n\nDimson, Elroy. 1979. “Risk Measurement When Shares Are Subject to Infrequent Trading.” Journal of Financial Economics 7 (2): 197–226.\n\n\nFama, Eugene F. 1998. “Market Efficiency, Long-Term Returns, and Behavioral Finance.” Journal of Financial Economics 49 (3): 283–306.\n\n\nFama, Eugene F, Lawrence Fisher, Michael C Jensen, and Richard Roll. 1969. “The Adjustment of Stock Prices to New Information.” International Economic Review 10 (1): 1–21.\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nFlannery, Mark J, and Aris A Protopapadakis. 2002. “Macroeconomic Factors Do Influence Aggregate Stock Returns.” The Review of Financial Studies 15 (3): 751–82.\n\n\nGriffin, John M, Patrick J Kelly, and Federico Nardari. 2010. “Do Market Efficiency Measures Yield Correct Inferences? A Comparison of Developed and Emerging Markets.” The Review of Financial Studies 23 (8): 3225–77.\n\n\nHall, Peter. 1992. “On the Removal of Skewness by Transformation.” Journal of the Royal Statistical Society Series B: Statistical Methodology 54 (1): 221–28.\n\n\nJensen, Michael C, and Richard S Ruback. 1983. “The Market for Corporate Control: The Scientific Evidence.” Journal of Financial Economics 11 (1-4): 5–50.\n\n\nKolari, James W, and Seppo Pynnönen. 2010. “Event Study Testing with Cross-Sectional Correlation of Abnormal Returns.” The Review of Financial Studies 23 (11): 3996–4025.\n\n\nKothari, Sagar P, and Jerold B Warner. 2007. “Econometrics of Event Studies.” In Handbook of Empirical Corporate Finance, 3–36. Elsevier.\n\n\nMacKinlay, A Craig. 1997. “Event Studies in Economics and Finance.” Journal of Economic Literature 35 (1): 13–39.\n\n\nMitchell, Mark L, and Jeffry M Netter. 1993. “The Role of Financial Economics in Securities Fraud Cases: Applications at the Securities and Exchange Commission.” Bus. Law. 49: 545.\n\n\nMitchell, Mark L, and Erik Stafford. 2000. “Managerial Decisions and Long-Term Stock Price Performance.” The Journal of Business 73 (3): 287–329.\n\n\nPatell, James M. 1976. “Corporate Forecasts of Earnings Per Share and Stock Price Behavior: Empirical Test.” Journal of Accounting Research, 246–76.\n\n\nScholes, Myron, and Joseph Williams. 1977. “Estimating Betas from Nonsynchronous Data.” Journal of Financial Economics 5 (3): 309–27.\n\n\nSchwert, G William. 1981. “Using Financial Data to Measure Effects of Regulation.” The Journal of Law and Economics 24 (1): 121–58.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.\n\n\nWarner, Jerold B, Ross L Watts, and Karen H Wruck. 1988. “Stock Prices and Top Management Changes.” Journal of Financial Economics 20: 461–92.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "61_tail_risk_extreme_events.html",
    "href": "61_tail_risk_extreme_events.html",
    "title": "41  Tail Risk and Extreme Events",
    "section": "",
    "text": "41.1 Crash Risk in Equity Markets\nFinance is fundamentally a discipline of extremes. The central objects of concern (e.g., asset pricing, portfolio allocation, capital regulation, systemic stability) are all disproportionately shaped by rare but severe events. A portfolio manager who correctly estimates the mean and variance of returns but ignores the possibility of a \\(-20\\%\\) daily move is, in the language of Taleb (2010), “picking up pennies in front of a steamroller.” The 1997 Asian Financial Crisis, the 2008 Global Financial Crisis, and the 2020 COVID-19 crash each demonstrated that the most consequential realizations in financial markets are precisely those that conventional Gaussian models assign near-zero probability.\nThis chapter develops the statistical and econometric toolkit for measuring, modeling, and diagnosing tail risk in Vietnamese equity markets. We focus on three interconnected layers. First, at the individual stock level, we estimate crash risk measures that capture the asymmetry and thickness of the left tail of return distributions. Second, at the portfolio or regulatory level, we implement Value at Risk (VaR) and Expected Shortfall (ES) under multiple estimation approaches and evaluate their accuracy via backtesting. Third, at the system level, we apply Extreme Value Theory (EVT) and tail dependence measures to characterize joint crash behavior across sectors and assess financial contagion.\nVietnamese markets present a particularly rich laboratory for studying tail phenomena. Daily price limits mechanically censor the tails of observed return distributions, creating latent tail risk that is not directly observable but must be inferred. State ownership concentrations, thin liquidity, and herding behavior among retail investors amplify crash dynamics. The absence of a developed derivatives market means that tail hedging instruments familiar in more developed markets (e.g., deep out-of-the-money puts, variance swaps) are largely unavailable, making accurate tail risk measurement all the more important for risk management.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Tail Risk and Extreme Events</span>"
    ]
  },
  {
    "objectID": "61_tail_risk_extreme_events.html#crash-risk-in-equity-markets",
    "href": "61_tail_risk_extreme_events.html#crash-risk-in-equity-markets",
    "title": "41  Tail Risk and Extreme Events",
    "section": "",
    "text": "41.1.1 The Left Tail Problem\nThe return distribution of individual stocks and market indices is not symmetric. Decades of evidence, beginning with Mandelbrot et al. (1963) and formalized in the asset pricing context by Harvey and Siddique (2000), establish that equity returns exhibit negative skewness (i.e., large negative returns occur more frequently than a Gaussian model predicts) and excess kurtosis (i.e., the tails are thicker than normal in both directions, but the left tail is of primary economic concern).\nLet \\(r_{i,t}\\) denote the daily log return of stock \\(i\\) on day \\(t\\). The standardized third central moment (skewness) is:\n\\[\n\\text{Skew}(r_i) = \\frac{E\\left[(r_{i,t} - \\mu_i)^3\\right]}{\\sigma_i^3}\n\\tag{41.1}\\]\nNegative skewness implies that the distribution has a longer or fatter left tail relative to the right. In economic terms, negative skewness means that large losses are more likely than large gains of equal magnitude. The standardized fourth moment (excess kurtosis) is:\n\\[\n\\text{Kurt}(r_i) = \\frac{E\\left[(r_{i,t} - \\mu_i)^4\\right]}{\\sigma_i^4} - 3\n\\tag{41.2}\\]\nPositive excess kurtosis indicates heavier tails than the normal distribution. Both moments have direct asset pricing implications: Harvey and Siddique (2000) and Kraus and Litzenberger (1976) argue that investors demand compensation for holding negatively skewed assets, implying that crash-prone stocks should earn higher expected returns, ceteris paribus.\n\n\n41.1.2 Crash Risk Measures\nThe literature has developed several firm-specific crash risk measures. We implement the three most widely used, following Chen, Hong, and Stein (2001) and Hutton, Marcus, and Tehranian (2009).\nMeasure 1: Negative Coefficient of Skewness (NCSKEW)\nFor each firm \\(i\\) in fiscal year \\(y\\), compute firm-specific weekly returns \\(W_{i,\\tau}\\) as residuals from a market model augmented with lead and lag market returns to account for nonsynchronous trading:\n\\[\nr_{i,t} = \\alpha_i + \\beta_{1i} r_{m,t-1} + \\beta_{2i} r_{m,t} + \\beta_{3i} r_{m,t+1} + \\varepsilon_{i,t}\n\\tag{41.3}\\]\nwhere \\(r_{m,t}\\) is the value-weighted market return on day \\(t\\). The firm-specific weekly return is \\(W_{i,\\tau} = \\ln(1 + \\sum_{t \\in \\tau} \\hat{\\varepsilon}_{i,t})\\), where \\(\\tau\\) indexes weeks. NCSKEW is then:\n\\[\n\\text{NCSKEW}_{i,y} = -\\frac{n(n-1)^{3/2} \\sum W_{i,\\tau}^3}{(n-1)(n-2)\\left(\\sum W_{i,\\tau}^2\\right)^{3/2}}\n\\tag{41.4}\\]\nwhere \\(n\\) is the number of firm-specific weekly returns in the year. The negative sign ensures that higher NCSKEW corresponds to greater crash risk.\nMeasure 2: Down-to-Up Volatility (DUVOL)\nPartition the firm-specific weekly returns into “down weeks” (\\(W_{i,\\tau} &lt; \\bar{W}_i\\)) and “up weeks” (\\(W_{i,\\tau} \\geq \\bar{W}_i\\)), where \\(\\bar{W}_i\\) is the annual mean. Then:\n\\[\n\\text{DUVOL}_{i,y} = \\ln\\left(\\frac{(n_u - 1)\\sum_{\\text{down}} W_{i,\\tau}^2}{(n_d - 1)\\sum_{\\text{up}} W_{i,\\tau}^2}\\right)\n\\tag{41.5}\\]\nwhere \\(n_u\\) and \\(n_d\\) are the number of up and down weeks, respectively. Higher DUVOL indicates greater crash risk. Chen, Hong, and Stein (2001) argue that DUVOL is less sensitive to outliers than NCSKEW because it does not involve the cube of returns.\nMeasure 3: Crash Count (COUNT)\nDefine a crash event as a firm-specific weekly return that falls below \\(k\\) standard deviations of its annual distribution:\n\\[\n\\text{CRASH}_{i,\\tau} = \\mathbb{1}\\left(W_{i,\\tau} &lt; \\bar{W}_i - k \\cdot \\hat{\\sigma}_{W_i}\\right)\n\\tag{41.6}\\]\nThe standard threshold in the literature is \\(k = 3.09\\), corresponding to the 0.1% tail of a standard normal distribution. The crash count for year \\(y\\) is \\(\\text{COUNT}_{i,y} = \\sum_{\\tau \\in y} \\text{CRASH}_{i,\\tau}\\).\n\n# DataCore.vn API\nfrom datacore import DataCore\ndc = DataCore()\n\n# Load daily stock returns and market returns\ndaily_returns = dc.get_daily_returns(\n    start_date=\"2010-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Market return: value-weighted index\nmarket_returns = dc.get_market_returns(\n    start_date=\"2010-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\n# Merge\ndf = daily_returns.merge(\n    market_returns[[\"date\", \"mkt_ret\"]],\n    on=\"date\",\n    how=\"left\"\n)\n\nprint(f\"Universe: {df['ticker'].nunique()} firms, \"\n      f\"{df['date'].nunique()} trading days\")\n\n\nfrom sklearn.linear_model import LinearRegression\n\ndef compute_firm_specific_returns(group):\n    \"\"\"\n    Estimate firm-specific daily returns from augmented market model\n    with lead and lag market returns (Chen, Hong, Stein 2001).\n    \"\"\"\n    g = group.sort_values(\"date\").copy()\n    g[\"mkt_lag1\"] = g[\"mkt_ret\"].shift(1)\n    g[\"mkt_lead1\"] = g[\"mkt_ret\"].shift(-1)\n    g = g.dropna(subset=[\"ret\", \"mkt_ret\", \"mkt_lag1\", \"mkt_lead1\"])\n\n    if len(g) &lt; 30:\n        return pd.DataFrame()\n\n    X = g[[\"mkt_lag1\", \"mkt_ret\", \"mkt_lead1\"]].values\n    y = g[\"ret\"].values\n\n    model = LinearRegression().fit(X, y)\n    g[\"resid\"] = y - model.predict(X)\n\n    return g[[\"ticker\", \"date\", \"resid\"]]\n\n# Estimate by firm-year\ndf[\"year\"] = df[\"date\"].dt.year\nfirm_resids = (\n    df.groupby([\"ticker\", \"year\"], group_keys=False)\n    .apply(compute_firm_specific_returns)\n    .reset_index(drop=True)\n)\n\n\n# Aggregate daily residuals to weekly firm-specific returns\nfirm_resids[\"date\"] = pd.to_datetime(firm_resids[\"date\"])\nfirm_resids[\"week\"] = firm_resids[\"date\"].dt.isocalendar().week.astype(int)\nfirm_resids[\"year\"] = firm_resids[\"date\"].dt.year\n\nweekly_returns = (\n    firm_resids.groupby([\"ticker\", \"year\", \"week\"])\n    .agg(W=(\"resid\", lambda x: np.log(1 + x.sum())))\n    .reset_index()\n)\n\n\ndef compute_crash_measures(group):\n    \"\"\"Compute NCSKEW, DUVOL, and CRASH count for one firm-year.\"\"\"\n    W = group[\"W\"].values\n    n = len(W)\n\n    if n &lt; 26:  # Require at least 26 weeks\n        return pd.Series({\n            \"ncskew\": np.nan, \"duvol\": np.nan,\n            \"crash_count\": np.nan, \"n_weeks\": n\n        })\n\n    # NCSKEW\n    W_demean = W - W.mean()\n    num = n * (n - 1)**1.5 * np.sum(W_demean**3)\n    den = (n - 1) * (n - 2) * (np.sum(W_demean**2))**1.5\n    ncskew = -(num / den) if den != 0 else np.nan\n\n    # DUVOL\n    mean_W = W.mean()\n    down = W[W &lt; mean_W]\n    up = W[W &gt;= mean_W]\n    n_d, n_u = len(down), len(up)\n\n    if n_d &gt; 1 and n_u &gt; 1:\n        duvol = np.log(\n            ((n_u - 1) * np.sum(down**2)) /\n            ((n_d - 1) * np.sum(up**2))\n        )\n    else:\n        duvol = np.nan\n\n    # Crash count (k = 3.09)\n    threshold = W.mean() - 3.09 * W.std()\n    crash_count = int(np.sum(W &lt; threshold))\n\n    return pd.Series({\n        \"ncskew\": ncskew,\n        \"duvol\": duvol,\n        \"crash_count\": crash_count,\n        \"n_weeks\": n\n    })\n\ncrash_risk = (\n    weekly_returns.groupby([\"ticker\", \"year\"])\n    .apply(compute_crash_measures)\n    .reset_index()\n)\n\ncrash_risk = crash_risk.dropna(subset=[\"ncskew\", \"duvol\"])\nprint(f\"Crash risk panel: {len(crash_risk)} firm-years\")\n\n\n\n41.1.3 Cross-Sectional Distribution of Crash Risk\nTable 41.1 presents summary statistics for the three crash risk measures across the Vietnamese equity market.\n\n\n\nTable 41.1: Summary Statistics of Crash Risk Measures\n\n\nsummary = crash_risk[[\"ncskew\", \"duvol\", \"crash_count\"]].describe(\n    percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]\n).T.round(4)\n\nsummary.columns = [\"N\", \"Mean\", \"Std\", \"Min\", \"5%\", \"25%\",\n                    \"Median\", \"75%\", \"95%\", \"Max\"]\nsummary\n\n\n\n\n\n\nplot_data = crash_risk[crash_risk[\"year\"].between(2012, 2024)].copy()\n\n(\n    p9.ggplot(plot_data, p9.aes(x=\"ncskew\"))\n    + p9.geom_histogram(bins=50, fill=\"#2E5090\", alpha=0.7)\n    + p9.facet_wrap(\"~year\", scales=\"free_y\", ncol=4)\n    + p9.geom_vline(xintercept=0, linetype=\"dashed\", color=\"red\", size=0.5)\n    + p9.labs(\n        x=\"NCSKEW\",\n        y=\"Count\",\n        title=\"Distribution of Firm-Level Crash Risk (NCSKEW)\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 8))\n)\n\n\nFigure 41.1\n\n\n\nThe rightward shift of the NCSKEW distribution during crisis episodes, particularly in 2020 and 2022, indicates a market-wide increase in crash risk. The red dashed line at zero separates firms with negative skewness (right of zero, i.e., high NCSKEW since the measure is negated) from those with positive skewness.\n\n\n41.1.4 Interpretation in Asset Pricing\nCrash risk has first-order implications for asset pricing. Harvey and Siddique (2000) demonstrate that coskewness (i.e., the contribution of an individual asset to the skewness of the aggregate market portfolio) is priced in the cross-section of expected returns. Specifically, assets that tend to crash when the market crashes (negative coskewness) should command a risk premium.\nThe coskewness of stock \\(i\\) with the market is:\n\\[\n\\text{CoSkew}_i = \\frac{E\\left[(r_i - \\mu_i)(r_m - \\mu_m)^2\\right]}{\\sigma_i \\cdot \\sigma_m^2}\n\\tag{41.7}\\]\nStocks with more negative coskewness exhibit disproportionately poor performance during market downturns. In Vietnam, this is especially relevant because: (1) retail investor herding amplifies co-movement during selloffs; (2) price limits delay crash completion, spreading crash risk across multiple days; and (3) state-owned enterprises, which constitute a large fraction of market capitalization, may exhibit coordinated crash behavior driven by common policy shocks.\n\ndef compute_coskewness(group):\n    \"\"\"Estimate coskewness for a firm-year.\"\"\"\n    r_i = group[\"ret\"].values\n    r_m = group[\"mkt_ret\"].values\n\n    if len(r_i) &lt; 60:\n        return np.nan\n\n    r_i_dm = r_i - r_i.mean()\n    r_m_dm = r_m - r_m.mean()\n\n    coskew = (\n        np.mean(r_i_dm * r_m_dm**2) /\n        (np.std(r_i) * np.std(r_m)**2)\n    )\n    return coskew\n\ndf_coskew = (\n    df.groupby([\"ticker\", \"year\"])\n    .apply(lambda g: pd.Series({\"coskewness\": compute_coskewness(g)}))\n    .reset_index()\n    .dropna()\n)\n\n\n\n\nmedian_coskew = (\n    df_coskew.groupby(\"year\")\n    .agg(\n        median_coskew=(\"coskewness\", \"median\"),\n        q25=(\"coskewness\", lambda x: x.quantile(0.25)),\n        q75=(\"coskewness\", lambda x: x.quantile(0.75))\n    )\n    .reset_index()\n)\n\n(\n    p9.ggplot(median_coskew, p9.aes(x=\"year\"))\n    + p9.geom_ribbon(\n        p9.aes(ymin=\"q25\", ymax=\"q75\"),\n        fill=\"#2E5090\", alpha=0.2\n    )\n    + p9.geom_line(p9.aes(y=\"median_coskew\"), color=\"#2E5090\", size=1)\n    + p9.geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\")\n    + p9.labs(\n        x=\"Year\",\n        y=\"Coskewness\",\n        title=\"Cross-Sectional Coskewness: Median and Interquartile Range\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n\n\nFigure 41.2\n\n\n\n\n\n41.1.5 Downside Tail Concentration\nBeyond skewness, the concentration of returns in the extreme left tail provides additional diagnostic information. We define the downside tail concentration ratio as the fraction of total return variance attributable to observations below the \\(p\\)-th percentile:\n\\[\n\\text{TCR}_p = \\frac{\\sum_{r_{i,t} &lt; q_p} (r_{i,t} - \\bar{r}_i)^2}{\\sum_t (r_{i,t} - \\bar{r}_i)^2}\n\\tag{41.8}\\]\nwhere \\(q_p\\) is the \\(p\\)-th percentile of the firm’s return distribution. Under a symmetric distribution, \\(\\text{TCR}_{5\\%}\\) would be close to \\(5\\%\\) (with a slight upward bias due to the squaring). Values substantially exceeding \\(5\\%\\) indicate that the left tail contributes disproportionately to total risk.\n\ndef tail_concentration_ratio(returns, p=0.05):\n    \"\"\"Compute tail concentration ratio at percentile p.\"\"\"\n    q = np.quantile(returns, p)\n    r_dm = returns - returns.mean()\n    total_var = np.sum(r_dm**2)\n    tail_var = np.sum(r_dm[returns &lt; q]**2)\n    return tail_var / total_var if total_var &gt; 0 else np.nan\n\ntcr = (\n    df.groupby([\"ticker\", \"year\"])\n    .agg(\n        tcr_5=(\"ret\", lambda x: tail_concentration_ratio(x.values, 0.05)),\n        tcr_1=(\"ret\", lambda x: tail_concentration_ratio(x.values, 0.01)),\n        n_obs=(\"ret\", \"count\")\n    )\n    .reset_index()\n    .query(\"n_obs &gt;= 120\")\n)\n\nprint(\"Median TCR(5%):\", round(tcr[\"tcr_5\"].median(), 4))\nprint(\"Median TCR(1%):\", round(tcr[\"tcr_1\"].median(), 4))",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Tail Risk and Extreme Events</span>"
    ]
  },
  {
    "objectID": "61_tail_risk_extreme_events.html#value-at-risk-and-expected-shortfall",
    "href": "61_tail_risk_extreme_events.html#value-at-risk-and-expected-shortfall",
    "title": "41  Tail Risk and Extreme Events",
    "section": "41.2 Value at Risk and Expected Shortfall",
    "text": "41.2 Value at Risk and Expected Shortfall\n\n41.2.1 Definitions\nValue at Risk (VaR) and Expected Shortfall (ES) are the two principal quantile-based risk measures used in financial regulation and internal risk management. For a portfolio return \\(r\\) with the cumulative distribution function \\(F\\), the VaR at confidence level \\(\\alpha\\) is:\n\\[\n\\text{VaR}_\\alpha = -F^{-1}(\\alpha) = -\\inf\\{x : F(x) \\geq \\alpha\\}\n\\tag{41.9}\\]\nThis is simply the negative of the \\(\\alpha\\)-quantile of the return distribution. For \\(\\alpha = 0.01\\), \\(\\text{VaR}_{1\\%}\\) answers: “What is the loss that is exceeded with only 1% probability?”\nExpected Shortfall (also called Conditional VaR or CVaR) is the expected loss conditional on the loss exceeding VaR:\n\\[\n\\text{ES}_\\alpha = -\\frac{1}{\\alpha}\\int_0^\\alpha F^{-1}(u) \\, du = -E\\left[r \\mid r \\leq -\\text{VaR}_\\alpha\\right]\n\\tag{41.10}\\]\nFor continuous distributions, ES simplifies to the conditional expectation in Equation 41.10. ES is always at least as large as VaR (\\(\\text{ES}_\\alpha \\geq \\text{VaR}_\\alpha\\)) and is strictly larger whenever the distribution has any mass beyond the VaR quantile.\n\n\n41.2.2 Why Expected Shortfall Dominates VaR\nTable 41.2 summarizes the fundamental differences.\n\n\n\nTable 41.2: Comparison of VaR and Expected Shortfall\n\n\n\n\n\n\n\n\n\n\nProperty\nVaR\nExpected Shortfall\n\n\n\n\nDefinition\nQuantile of the loss distribution\nConditional mean loss beyond VaR\n\n\nTail information\nNone beyond the quantile\nCaptures severity of tail losses\n\n\nSubadditivity\nNot subadditive in general\nSubadditive (coherent risk measure)\n\n\nRegulatory status\nBasel II primary measure\nBasel III/IV primary measure\n\n\nBacktesting\nBinary: breach or no breach\nContinuous: requires severity assessment\n\n\nSensitivity to tail shape\nNone\nDirectly sensitive\n\n\n\n\n\n\nThe critical deficiency of VaR is the absence of subadditivity. A risk measure \\(\\rho\\) is subadditive if \\(\\rho(X + Y) \\leq \\rho(X) + \\rho(Y)\\) for all portfolios \\(X, Y\\). VaR violates this condition for non-elliptical distributions, meaning that diversification can appear to increase risk under VaR, which is a perverse result. Artzner et al. (1999) established the axiomatic framework for coherent risk measures, and ES satisfies all four axioms: monotonicity, translation invariance, positive homogeneity, and subadditivity. VaR satisfies only three.\n\n\n41.2.3 Estimation Methods\nWe implement four approaches to VaR and ES estimation, each with distinct assumptions and data requirements.\nMethod 1: Historical Simulation\nThe simplest nonparametric approach uses the empirical distribution directly:\n\\[\n\\widehat{\\text{VaR}}_\\alpha^{\\text{HS}} = -\\hat{F}_n^{-1}(\\alpha) = -r_{(\\lceil n\\alpha \\rceil)}\n\\tag{41.11}\\]\nwhere \\(r_{(k)}\\) is the \\(k\\)-th order statistic of the return sample. ES is the average of all returns below the VaR:\n\\[\n\\widehat{\\text{ES}}_\\alpha^{\\text{HS}} = -\\frac{1}{\\lfloor n\\alpha \\rfloor}\\sum_{k=1}^{\\lfloor n\\alpha \\rfloor} r_{(k)}\n\\tag{41.12}\\]\nMethod 2: Parametric (Gaussian)\nAssume \\(r_t \\sim N(\\mu, \\sigma^2)\\). Then:\n\\[\n\\text{VaR}_\\alpha^{\\text{Gauss}} = -(\\hat{\\mu} + \\hat{\\sigma} \\cdot \\Phi^{-1}(\\alpha))\n\\tag{41.13}\\]\n\\[\n\\text{ES}_\\alpha^{\\text{Gauss}} = -\\hat{\\mu} + \\hat{\\sigma} \\cdot \\frac{\\phi(\\Phi^{-1}(\\alpha))}{\\alpha}\n\\tag{41.14}\\]\nwhere \\(\\Phi\\) and \\(\\phi\\) are the standard normal CDF and PDF, respectively. The Gaussian assumption is known to underestimate tail risk for equity returns, but provides a useful baseline.\nMethod 3: Parametric (Student-\\(t\\))\nThe Student-\\(t\\) distribution with \\(\\nu\\) degrees of freedom captures excess kurtosis. The VaR and ES are:\n\\[\n\\text{VaR}_\\alpha^{t} = -\\left(\\hat{\\mu} + \\hat{\\sigma}\\sqrt{\\frac{\\nu - 2}{\\nu}} \\cdot t_\\nu^{-1}(\\alpha)\\right)\n\\tag{41.15}\\]\n\\[\n\\text{ES}_\\alpha^{t} = -\\hat{\\mu} + \\hat{\\sigma}\\sqrt{\\frac{\\nu-2}{\\nu}} \\cdot \\frac{f_{t_\\nu}(t_\\nu^{-1}(\\alpha))}{\\alpha} \\cdot \\frac{\\nu + (t_\\nu^{-1}(\\alpha))^2}{\\nu - 1}\n\\tag{41.16}\\]\nwhere \\(t_\\nu^{-1}\\) and \\(f_{t_\\nu}\\) are the quantile function and PDF of the Student-\\(t\\) distribution.\nMethod 4: GARCH-Filtered EVT (Semi-Parametric)\nThis approach, developed by McNeil and Frey (2000), first filters returns through a GARCH(1,1) model to obtain standardized residuals \\(z_t = \\hat{\\varepsilon}_t / \\hat{\\sigma}_t\\), then fits a Generalized Pareto Distribution (GPD) to the lower tail of the standardized residuals. We detail the EVT component in Section 41.3.\n\ndef estimate_var_es(returns, alpha=0.01):\n    \"\"\"\n    Estimate VaR and ES at level alpha using four methods.\n\n    Parameters\n    ----------\n    returns : array-like\n        Return series.\n    alpha : float\n        Tail probability (e.g., 0.01 for 1%).\n\n    Returns\n    -------\n    dict : VaR and ES estimates for each method.\n    \"\"\"\n    r = np.array(returns)\n    r = r[~np.isnan(r)]\n    n = len(r)\n    mu, sigma = r.mean(), r.std(ddof=1)\n\n    results = {}\n\n    # Method 1: Historical simulation\n    sorted_r = np.sort(r)\n    idx = int(np.ceil(n * alpha))\n    results[\"var_hs\"] = -sorted_r[idx - 1]\n    results[\"es_hs\"] = -sorted_r[:idx].mean()\n\n    # Method 2: Gaussian\n    z_alpha = stats.norm.ppf(alpha)\n    results[\"var_gauss\"] = -(mu + sigma * z_alpha)\n    results[\"es_gauss\"] = -mu + sigma * stats.norm.pdf(z_alpha) / alpha\n\n    # Method 3: Student-t (MLE for degrees of freedom)\n    try:\n        nu, loc, scale = stats.t.fit(r)\n        t_alpha = stats.t.ppf(alpha, df=nu)\n        results[\"var_t\"] = -(loc + scale * t_alpha)\n        results[\"es_t\"] = (\n            -loc + scale *\n            (stats.t.pdf(t_alpha, df=nu) / alpha) *\n            ((nu + t_alpha**2) / (nu - 1))\n        )\n        results[\"nu_hat\"] = nu\n    except Exception:\n        results[\"var_t\"] = np.nan\n        results[\"es_t\"] = np.nan\n        results[\"nu_hat\"] = np.nan\n\n    return results\n\n\n# Compute VaR and ES for the aggregate market index\nmarket_daily = dc.get_market_returns(\n    start_date=\"2010-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\n# Rolling 1-year VaR/ES\nwindow = 252\nresults_list = []\n\nfor end_idx in range(window, len(market_daily)):\n    window_returns = market_daily[\"mkt_ret\"].iloc[end_idx - window:end_idx].values\n    date = market_daily[\"date\"].iloc[end_idx]\n\n    est = estimate_var_es(window_returns, alpha=0.01)\n    est[\"date\"] = date\n    results_list.append(est)\n\nvar_es_ts = pd.DataFrame(results_list)\n\n\n\n\nplot_var = var_es_ts.melt(\n    id_vars=\"date\",\n    value_vars=[\"var_hs\", \"var_gauss\", \"var_t\"],\n    var_name=\"method\",\n    value_name=\"var\"\n)\n\nmethod_labels = {\n    \"var_hs\": \"Historical Simulation\",\n    \"var_gauss\": \"Gaussian\",\n    \"var_t\": \"Student-t\"\n}\nplot_var[\"method\"] = plot_var[\"method\"].map(method_labels)\n\n(\n    p9.ggplot(plot_var, p9.aes(x=\"date\", y=\"var\", color=\"method\"))\n    + p9.geom_line(alpha=0.8, size=0.6)\n    + p9.labs(\n        x=\"\", y=\"1% VaR (Loss)\",\n        title=\"Rolling 252-Day Value at Risk Estimates\",\n        color=\"Method\"\n    )\n    + p9.scale_color_manual(\n        values=[\"#2E5090\", \"#C0392B\", \"#27AE60\"]\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5), legend_position=\"top\")\n)\n\n\nFigure 41.3\n\n\n\n\n\n\nplot_es = var_es_ts.melt(\n    id_vars=\"date\",\n    value_vars=[\"es_hs\", \"es_gauss\", \"es_t\"],\n    var_name=\"method\",\n    value_name=\"es\"\n)\n\nmethod_labels_es = {\n    \"es_hs\": \"Historical Simulation\",\n    \"es_gauss\": \"Gaussian\",\n    \"es_t\": \"Student-t\"\n}\nplot_es[\"method\"] = plot_es[\"method\"].map(method_labels_es)\n\n(\n    p9.ggplot(plot_es, p9.aes(x=\"date\", y=\"es\", color=\"method\"))\n    + p9.geom_line(alpha=0.8, size=0.6)\n    + p9.labs(\n        x=\"\", y=\"1% ES (Loss)\",\n        title=\"Rolling 252-Day Expected Shortfall Estimates\",\n        color=\"Method\"\n    )\n    + p9.scale_color_manual(\n        values=[\"#2E5090\", \"#C0392B\", \"#27AE60\"]\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5), legend_position=\"top\")\n)\n\n\nFigure 41.4\n\n\n\n\n\n41.2.4 VaR Backtesting\nA VaR model is only useful if its predictions are accurate. Backtesting evaluates whether the realized frequency of VaR breaches is consistent with the nominal confidence level. If VaR is estimated at the \\(\\alpha = 1\\%\\) level, we expect approximately 1% of days to exhibit losses exceeding VaR.\nThe Kupiec unconditional coverage test (Kupiec et al. 1995) evaluates whether the total number of breaches \\(x\\) in \\(n\\) observations is consistent with \\(\\alpha\\):\n\\[\n\\text{LR}_{\\text{UC}} = -2\\ln\\left(\\frac{\\alpha^x (1-\\alpha)^{n-x}}{\\hat{p}^x (1-\\hat{p})^{n-x}}\\right) \\sim \\chi^2(1)\n\\tag{41.17}\\]\nwhere \\(\\hat{p} = x/n\\) is the empirical breach frequency. The Christoffersen conditional coverage test (Christoffersen 1998) additionally evaluates whether breaches are independent (no clustering):\n\\[\n\\text{LR}_{\\text{CC}} = \\text{LR}_{\\text{UC}} + \\text{LR}_{\\text{IND}}\n\\tag{41.18}\\]\nwhere \\(\\text{LR}_{\\text{IND}}\\) tests the null of independence using a first-order Markov chain model for breach indicators.\n\ndef kupiec_test(returns, var_estimates, alpha=0.01):\n    \"\"\"\n    Kupiec unconditional coverage test for VaR backtesting.\n\n    Returns\n    -------\n    dict : breach count, expected, breach rate, LR statistic, p-value.\n    \"\"\"\n    breaches = returns &lt; -var_estimates\n    x = breaches.sum()\n    n = len(returns)\n    p_hat = x / n\n\n    if p_hat == 0 or p_hat == 1:\n        return {\"breaches\": x, \"expected\": n * alpha,\n                \"breach_rate\": p_hat, \"lr_stat\": np.nan, \"p_value\": np.nan}\n\n    lr = -2 * (\n        x * np.log(alpha) + (n - x) * np.log(1 - alpha)\n        - x * np.log(p_hat) - (n - x) * np.log(1 - p_hat)\n    )\n    p_value = 1 - stats.chi2.cdf(lr, df=1)\n\n    return {\n        \"breaches\": int(x),\n        \"expected\": round(n * alpha, 1),\n        \"breach_rate\": round(p_hat, 4),\n        \"lr_stat\": round(lr, 4),\n        \"p_value\": round(p_value, 4)\n    }\n\n# Backtest each method\nactual_returns = market_daily[\"mkt_ret\"].iloc[window:].values\n\nbacktest_results = {}\nfor method, col in [(\"Historical\", \"var_hs\"),\n                     (\"Gaussian\", \"var_gauss\"),\n                     (\"Student-t\", \"var_t\")]:\n    backtest_results[method] = kupiec_test(\n        actual_returns, var_es_ts[col].values, alpha=0.01\n    )\n\nbt_df = pd.DataFrame(backtest_results).T\nbt_df.index.name = \"Method\"\nbt_df\n\n\n\n41.2.5 Estimation Under Limited Data\nIn Vietnamese equity markets, many stocks have short trading histories, thin liquidity, and censored returns due to price limits. These features create challenges for tail risk estimation that are less severe in developed markets.\nPrice limit censoring. When a stock hits its daily price limit, the observed return is truncated. The true latent return may extend well beyond the limit, but is unobservable. This means that historical simulation mechanically understates VaR and ES for stocks that frequently hit limits. A Tobit-type correction can partially address this:\n\\[\nr_{i,t}^{\\text{latent}} \\sim N(\\mu_i, \\sigma_i^2), \\quad r_{i,t}^{\\text{obs}} = \\max(\\underline{L}, \\min(\\bar{L}, r_{i,t}^{\\text{latent}}))\n\\tag{41.19}\\]\nThe parameters \\((\\mu_i, \\sigma_i^2)\\) can be estimated via maximum likelihood on the censored sample, and VaR/ES can then be computed from the estimated latent distribution.\nShort histories. For recently listed stocks, the available sample may be too short for reliable nonparametric estimation. In these cases, shrinkage toward a cross-sectional prior (e.g., the sector median VaR) or Bayesian approaches with informative priors can improve stability.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Tail Risk and Extreme Events</span>"
    ]
  },
  {
    "objectID": "61_tail_risk_extreme_events.html#sec-evt",
    "href": "61_tail_risk_extreme_events.html#sec-evt",
    "title": "41  Tail Risk and Extreme Events",
    "section": "41.3 Extreme Value Theory",
    "text": "41.3 Extreme Value Theory\n\n41.3.1 Motivation: What EVT Captures That GARCH Does Not\nGARCH models capture volatility clustering (i.e., the tendency of large returns to be followed by large returns), but they make specific distributional assumptions about standardized residuals (typically Gaussian or Student-\\(t\\)). The tail behavior of the conditional distribution is thus determined by the parametric innovation assumption, not learned from the data.\nExtreme Value Theory (EVT) takes a fundamentally different approach. It provides a statistical framework for estimating the distribution of extreme observations without specifying the entire return distribution. The key theoretical results (i.e., the Fisher-Tippett-Gnedenko theorem and the Pickands-Balkema-de Haan theorem) establish that the distribution of properly normalized maxima (or exceedances over high thresholds) converges to specific parametric families regardless of the parent distribution. This universality is what makes EVT powerful: the tail can be estimated even when the center of the distribution is misspecified.\n\n\n41.3.2 Tail Index Estimation\nThe tail index \\(\\xi\\) (also denoted \\(\\alpha^{-1}\\) in some formulations) governs the rate of tail decay. For heavy-tailed distributions, the survival function satisfies:\n\\[\nP(X &gt; x) \\sim L(x) \\cdot x^{-1/\\xi}, \\quad x \\to \\infty\n\\tag{41.20}\\]\nwhere \\(L(x)\\) is a slowly varying function. The tail index \\(\\xi &gt; 0\\) implies a Pareto-type tail; larger \\(\\xi\\) means heavier tails. Financial return distributions typically have \\(\\xi \\in (0.2, 0.5)\\), corresponding to tail indices \\(\\alpha = 1/\\xi \\in (2, 5)\\), which implies finite variance but potentially infinite higher moments.\nThe Hill estimator (Hill 1975) provides a simple nonparametric estimate of \\(\\xi\\) from the upper order statistics:\n\\[\n\\hat{\\xi}_{\\text{Hill}}(k) = \\frac{1}{k}\\sum_{j=1}^{k} \\ln X_{(n-j+1)} - \\ln X_{(n-k)}\n\\tag{41.21}\\]\nwhere \\(X_{(1)} \\leq \\cdots \\leq X_{(n)}\\) are the order statistics and \\(k\\) is the number of upper order statistics used. The choice of \\(k\\) involves a bias-variance tradeoff: small \\(k\\) yields high variance; large \\(k\\) introduces bias from non-tail observations.\n\ndef hill_estimator(data, k_values=None):\n    \"\"\"\n    Compute Hill estimator of tail index for a range of k values.\n\n    Parameters\n    ----------\n    data : array-like\n        Positive values (use absolute values of losses).\n    k_values : array-like, optional\n        Number of upper order statistics to use.\n\n    Returns\n    -------\n    DataFrame with columns [k, xi_hat, se].\n    \"\"\"\n    x = np.sort(np.abs(data))[::-1]  # Descending order\n    n = len(x)\n\n    if k_values is None:\n        k_values = range(10, min(n // 2, 500), 5)\n\n    results = []\n    for k in k_values:\n        if k &gt;= n or k &lt; 2:\n            continue\n        log_excess = np.log(x[:k]) - np.log(x[k])\n        xi_hat = log_excess.mean()\n        se = xi_hat / np.sqrt(k)\n        results.append({\"k\": k, \"xi_hat\": xi_hat, \"se\": se})\n\n    return pd.DataFrame(results)\n\n\n\n\n# Use negative market returns (losses)\nlosses = -market_daily[\"mkt_ret\"].dropna().values\nlosses_positive = losses[losses &gt; 0]\n\nhill_results = hill_estimator(losses_positive)\n\n(\n    p9.ggplot(hill_results, p9.aes(x=\"k\", y=\"xi_hat\"))\n    + p9.geom_ribbon(\n        p9.aes(ymin=\"xi_hat - 1.96 * se\", ymax=\"xi_hat + 1.96 * se\"),\n        fill=\"#2E5090\", alpha=0.2\n    )\n    + p9.geom_line(color=\"#2E5090\", size=0.8)\n    + p9.geom_hline(yintercept=0.33, linetype=\"dashed\", color=\"red\")\n    + p9.annotate(\n        \"text\", x=hill_results[\"k\"].max() * 0.7, y=0.35,\n        label=\"ξ = 1/3 (cubic tail)\", color=\"red\", size=8\n    )\n    + p9.labs(\n        x=\"Number of Order Statistics (k)\",\n        y=\"Hill Estimate ξ̂\",\n        title=\"Hill Plot for Market Return Left Tail\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n\n\nFigure 41.5\n\n\n\nThe Hill plot is a standard diagnostic in EVT. A stable region of the Hill estimate across a range of \\(k\\) suggests a reliable tail index estimate. The red dashed line at \\(\\xi = 1/3\\) corresponds to a tail index of \\(\\alpha = 3\\), which implies that the third moment (skewness) is infinite, which is a finding consistent with much of the empirical finance literature (Cont 2001; Gabaix et al. 2003).\n\n\n41.3.3 Block Maxima and the GEV Distribution\nThe Fisher-Tippett-Gnedenko theorem establishes that if the maximum of \\(n\\) i.i.d. random variables, after proper normalization, converges in distribution, the limit must be a Generalized Extreme Value (GEV) distribution:\n\\[\nG_\\xi(x) = \\exp\\left\\{-\\left(1 + \\xi \\cdot \\frac{x - \\mu}{\\sigma}\\right)^{-1/\\xi}\\right\\}\n\\tag{41.22}\\]\nfor \\(1 + \\xi(x - \\mu)/\\sigma &gt; 0\\), where \\(\\mu\\) is the location, \\(\\sigma &gt; 0\\) the scale, and \\(\\xi\\) the shape parameter. The three sub-families are in Table 41.3.\n\n\n\nTable 41.3: GEV Sub-Families and Financial Interpretation\n\n\n\n\n\n\n\n\n\n\n\nShape \\(\\xi\\)\nDistribution\nTail Type\nFinance Example\n\n\n\n\n\\(\\xi &gt; 0\\)\nFréchet\nHeavy (polynomial decay)\nEquity returns, credit losses\n\n\n\\(\\xi = 0\\)\nGumbel\nLight (exponential decay)\nNormal/lognormal models\n\n\n\\(\\xi &lt; 0\\)\nWeibull\nBounded upper tail\nBounded loss distributions\n\n\n\n\n\n\nFor the block maxima approach, we divide the return series into non-overlapping blocks (typically months or quarters) and extract the minimum return (maximum loss) from each block. The GEV is then fitted to these block minima.\n\nfrom scipy.stats import genextreme\n\n# Monthly block minima (maximum losses)\nmarket_daily[\"month\"] = market_daily[\"date\"].dt.to_period(\"M\")\nblock_minima = (\n    market_daily.groupby(\"month\")\n    .agg(min_ret=(\"mkt_ret\", \"min\"))\n    .reset_index()\n)\n\n# Fit GEV to negated block minima (so we model maxima of losses)\nblock_losses = -block_minima[\"min_ret\"].values\nxi_hat, loc_hat, scale_hat = genextreme.fit(block_losses)\n\nprint(f\"GEV fit to monthly block maxima of losses:\")\nprint(f\"  Shape (ξ):    {xi_hat:.4f}\")\nprint(f\"  Location (μ): {loc_hat:.4f}\")\nprint(f\"  Scale (σ):    {scale_hat:.4f}\")\n\n\n\n\nx_grid = np.linspace(\n    block_losses.min() * 0.8,\n    block_losses.max() * 1.2,\n    200\n)\ngev_pdf = genextreme.pdf(x_grid, xi_hat, loc=loc_hat, scale=scale_hat)\n\nplot_data = pd.DataFrame({\n    \"x\": np.concatenate([block_losses, x_grid]),\n    \"source\": ([\"Empirical\"] * len(block_losses) +\n               [\"GEV Fit\"] * len(x_grid)),\n    \"density\": np.concatenate([\n        np.full(len(block_losses), np.nan),\n        gev_pdf\n    ]),\n    \"value\": np.concatenate([block_losses, np.full(len(x_grid), np.nan)])\n})\n\nempirical = plot_data[plot_data[\"source\"] == \"Empirical\"]\nfitted = plot_data[plot_data[\"source\"] == \"GEV Fit\"]\n\n(\n    p9.ggplot()\n    + p9.geom_histogram(\n        data=empirical, mapping=p9.aes(x=\"value\", y=\"..density..\"),\n        bins=30, fill=\"#2E5090\", alpha=0.5\n    )\n    + p9.geom_line(\n        data=fitted, mapping=p9.aes(x=\"x\", y=\"density\"),\n        color=\"#C0392B\", size=1\n    )\n    + p9.labs(\n        x=\"Monthly Maximum Loss\",\n        y=\"Density\",\n        title=\"GEV Distribution Fit to Monthly Block Maxima\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n\n\nFigure 41.6\n\n\n\n\n\n41.3.4 Peaks Over Threshold and the GPD\nThe Peaks Over Threshold (POT) approach is generally preferred over block maxima because it uses tail data more efficiently. The Pickands-Balkema-de Haan theorem establishes that for a sufficiently high threshold \\(u\\), the distribution of exceedances \\(Y = X - u \\mid X &gt; u\\) converges to a Generalized Pareto Distribution (GPD):\n\\[\nH_{\\xi,\\beta}(y) = \\begin{cases}\n1 - \\left(1 + \\frac{\\xi y}{\\beta}\\right)^{-1/\\xi} & \\text{if } \\xi \\neq 0 \\\\\n1 - \\exp\\left(-\\frac{y}{\\beta}\\right) & \\text{if } \\xi = 0\n\\end{cases}\n\\tag{41.23}\\]\nfor \\(y &gt; 0\\) and \\(1 + \\xi y / \\beta &gt; 0\\), where \\(\\beta &gt; 0\\) is the scale parameter and \\(\\xi\\) is the shape parameter (identical to the GEV shape). The POT-based VaR and ES at level \\(\\alpha\\) are:\n\\[\n\\text{VaR}_\\alpha^{\\text{GPD}} = u + \\frac{\\hat{\\beta}}{\\hat{\\xi}}\\left[\\left(\\frac{n}{N_u} \\cdot \\alpha\\right)^{-\\hat{\\xi}} - 1\\right]\n\\tag{41.24}\\]\n\\[\n\\text{ES}_\\alpha^{\\text{GPD}} = \\frac{\\text{VaR}_\\alpha^{\\text{GPD}}}{1 - \\hat{\\xi}} + \\frac{\\hat{\\beta} - \\hat{\\xi} u}{1 - \\hat{\\xi}}\n\\tag{41.25}\\]\nwhere \\(N_u\\) is the number of observations exceeding \\(u\\) and \\(n\\) is the total sample size. These formulae are valid for \\(\\hat{\\xi} &lt; 1\\).\n\nfrom scipy.stats import genpareto\n\ndef fit_gpd_pot(returns, threshold_quantile=0.95):\n    \"\"\"\n    Fit GPD to exceedances over threshold using POT approach.\n\n    Parameters\n    ----------\n    returns : array-like\n        Return series (losses should be positive).\n    threshold_quantile : float\n        Quantile for threshold selection.\n\n    Returns\n    -------\n    dict : Threshold, GPD parameters, VaR, ES estimates.\n    \"\"\"\n    losses = -np.array(returns)\n    losses = losses[~np.isnan(losses)]\n    n = len(losses)\n\n    u = np.quantile(losses, threshold_quantile)\n    exceedances = losses[losses &gt; u] - u\n    N_u = len(exceedances)\n\n    # Fit GPD\n    xi_hat, _, beta_hat = genpareto.fit(exceedances, floc=0)\n\n    # VaR and ES at 1%\n    alpha = 0.01\n    var_gpd = u + (beta_hat / xi_hat) * (\n        (n / N_u * alpha)**(-xi_hat) - 1\n    )\n    es_gpd = var_gpd / (1 - xi_hat) + (beta_hat - xi_hat * u) / (1 - xi_hat)\n\n    return {\n        \"threshold\": u,\n        \"n_exceedances\": N_u,\n        \"xi_hat\": xi_hat,\n        \"beta_hat\": beta_hat,\n        \"var_1pct\": var_gpd,\n        \"es_1pct\": es_gpd\n    }\n\n# Fit GPD to market losses\ngpd_results = fit_gpd_pot(market_daily[\"mkt_ret\"].dropna().values, 0.95)\n\nprint(\"GPD-POT Results (1% level):\")\nfor k, v in gpd_results.items():\n    if isinstance(v, float):\n        print(f\"  {k}: {v:.6f}\")\n    else:\n        print(f\"  {k}: {v}\")\n\n\n\n41.3.5 Threshold Selection\nThe choice of threshold \\(u\\) is the key practical decision in POT analysis. Too low a threshold violates the asymptotic approximation; too high wastes data. The mean residual life plot provides a graphical diagnostic: if the GPD approximation holds above \\(u_0\\), the mean excess function \\(e(u) = E[X - u \\mid X &gt; u]\\) is linear in \\(u\\) for \\(u &gt; u_0\\):\n\\[\ne(u) = \\frac{\\beta + \\xi u}{1 - \\xi}\n\\tag{41.26}\\]\n\n\n\nlosses = -market_daily[\"mkt_ret\"].dropna().values\nlosses_sorted = np.sort(losses)\n\nthresholds = np.quantile(losses, np.linspace(0.80, 0.99, 50))\nmean_excess = []\n\nfor u in thresholds:\n    exceedances = losses[losses &gt; u] - u\n    if len(exceedances) &gt; 5:\n        mean_excess.append({\n            \"threshold\": u,\n            \"mean_excess\": exceedances.mean(),\n            \"se\": exceedances.std() / np.sqrt(len(exceedances)),\n            \"n_exceed\": len(exceedances)\n        })\n\nmrl_df = pd.DataFrame(mean_excess)\n\n(\n    p9.ggplot(mrl_df, p9.aes(x=\"threshold\", y=\"mean_excess\"))\n    + p9.geom_ribbon(\n        p9.aes(\n            ymin=\"mean_excess - 1.96 * se\",\n            ymax=\"mean_excess + 1.96 * se\"\n        ),\n        fill=\"#2E5090\", alpha=0.2\n    )\n    + p9.geom_line(color=\"#2E5090\", size=0.8)\n    + p9.geom_point(color=\"#2E5090\", size=1.5)\n    + p9.labs(\n        x=\"Threshold (u)\",\n        y=\"Mean Excess e(u)\",\n        title=\"Mean Residual Life Plot\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n\n\nFigure 41.7\n\n\n\nA threshold in the region where the mean residual life plot is approximately linear is appropriate. In practice, we also compare GPD parameter stability across a range of thresholds and select the lowest threshold at which estimates stabilize.\n\n\n41.3.6 What EVT Captures That GARCH Does Not\nTable 41.4 summarizes the complementary roles of these two modeling frameworks.\n\n\n\nTable 41.4: GARCH vs. EVT: Complementary Frameworks\n\n\n\n\n\n\n\n\n\n\nFeature\nGARCH\nEVT\n\n\n\n\nWhat is modeled\nConditional variance dynamics\nUnconditional tail distribution\n\n\nDistributional assumption\nFull distribution (Gaussian or \\(t\\))\nOnly the tail (GPD or GEV)\n\n\nTime dependence\nExplicit (volatility clustering)\nNone (i.i.d. or filtered)\n\n\nTail shape\nDetermined by innovation distribution\nEstimated from data\n\n\nExtreme quantiles\nExtrapolation from parametric assumption\nPrincipled extrapolation via EVT theorems\n\n\nBest use\nShort-horizon risk forecasting\nExtreme quantile estimation, stress testing\n\n\n\n\n\n\nThe GARCH-filtered EVT approach of McNeil and Frey (2000) combines both: GARCH captures the time-varying volatility, and EVT models the residual tail. This hybrid approach is considered best practice for financial risk management (McNeil, Frey, and Embrechts 2015).",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Tail Risk and Extreme Events</span>"
    ]
  },
  {
    "objectID": "61_tail_risk_extreme_events.html#tail-dependence-and-contagion",
    "href": "61_tail_risk_extreme_events.html#tail-dependence-and-contagion",
    "title": "41  Tail Risk and Extreme Events",
    "section": "41.4 Tail Dependence and Contagion",
    "text": "41.4 Tail Dependence and Contagion\n\n41.4.1 Why Correlation Breaks Down in Crises\nPerhaps the most important practical fact about multivariate tail risk is that correlations estimated from the body of the distribution systematically understate dependence in the tails. Longin and Solnik (2001) demonstrate this using extreme value theory: for bivariate normal returns, the correlation of extreme realizations converges to zero as the threshold moves further into the tail. Yet empirically, the correlation of crash returns remains large and often increases (a phenomenon incompatible with multivariate normality).\nFormally, the coefficient of lower tail dependence between two return series \\(X\\) and \\(Y\\) is:\n\\[\n\\lambda_L = \\lim_{q \\to 0^+} P\\left(Y \\leq F_Y^{-1}(q) \\mid X \\leq F_X^{-1}(q)\\right)\n\\tag{41.27}\\]\nIf \\(\\lambda_L &gt; 0\\), the two series exhibit asymptotic tail dependence (i.e., extreme losses tend to occur jointly). For the bivariate normal distribution, \\(\\lambda_L = 0\\) for all \\(|\\rho| &lt; 1\\) (asymptotic tail independence). This means that any model built on multivariate normality will structurally underestimate the probability of joint crashes. The Student-\\(t\\) copula, by contrast, has \\(\\lambda_L &gt; 0\\) for \\(\\rho &gt; -1\\), which is why it has become the workhorse model for joint tail risk in finance (Demarta and McNeil 2005).\n\n\n41.4.2 Co-Crash Measures\nWe implement several empirical measures of joint crash behavior.\nExceedance Correlation. Following Longin and Solnik (2001), compute the correlation of returns conditional on both returns falling below a threshold \\(\\theta\\):\n\\[\n\\rho^-(\\theta) = \\text{Corr}(r_i, r_j \\mid r_i &lt; \\theta, r_j &lt; \\theta)\n\\tag{41.28}\\]\nIf returns are bivariate normal, \\(\\rho^-(\\theta) \\to 0\\) as \\(\\theta \\to -\\infty\\). Empirically, \\(\\rho^-(\\theta)\\) typically increases as \\(\\theta\\) decreases, indicating tail dependence beyond what the normal model implies.\nCo-Exceedance Count. For each day \\(t\\), count the number of stocks or sectors whose returns fall below the \\(q\\)-th percentile of their respective marginal distributions:\n\\[\nC_t(q) = \\sum_{i=1}^{N} \\mathbb{1}\\left(r_{i,t} &lt; F_i^{-1}(q)\\right)\n\\tag{41.29}\\]\nDays with high \\(C_t(q)\\) represent system-wide stress events.\n\ndef empirical_tail_dependence(x, y, quantiles=None):\n    \"\"\"\n    Estimate lower tail dependence coefficient at various quantiles.\n\n    Uses the empirical copula approach: transform to uniform margins,\n    then estimate P(V &lt;= q | U &lt;= q) for decreasing q.\n    \"\"\"\n    if quantiles is None:\n        quantiles = [0.20, 0.15, 0.10, 0.05, 0.03, 0.01]\n\n    # Transform to uniform margins via empirical CDF\n    from scipy.stats import rankdata\n    n = len(x)\n    u = rankdata(x) / (n + 1)\n    v = rankdata(y) / (n + 1)\n\n    results = []\n    for q in quantiles:\n        mask_u = u &lt;= q\n        mask_both = mask_u & (v &lt;= q)\n\n        n_u = mask_u.sum()\n        n_both = mask_both.sum()\n\n        lambda_hat = n_both / n_u if n_u &gt; 0 else np.nan\n\n        results.append({\n            \"quantile\": q,\n            \"lambda_L\": lambda_hat,\n            \"n_below_x\": int(n_u),\n            \"n_co_crash\": int(n_both)\n        })\n\n    return pd.DataFrame(results)\n\n\n# Load sector-level daily returns\nsector_returns = dc.get_sector_returns(\n    start_date=\"2010-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\n# Compute pairwise tail dependence for major sectors\nsectors = sector_returns[\"sector\"].unique()[:8]  # Top 8 sectors\n\nimport itertools\n\npairwise_td = []\nfor s1, s2 in itertools.combinations(sectors, 2):\n    r1 = sector_returns.loc[\n        sector_returns[\"sector\"] == s1, [\"date\", \"ret\"]\n    ].set_index(\"date\")[\"ret\"]\n    r2 = sector_returns.loc[\n        sector_returns[\"sector\"] == s2, [\"date\", \"ret\"]\n    ].set_index(\"date\")[\"ret\"]\n\n    # Align dates\n    aligned = pd.concat([r1, r2], axis=1, join=\"inner\").dropna()\n    if len(aligned) &lt; 500:\n        continue\n\n    td = empirical_tail_dependence(\n        aligned.iloc[:, 0].values,\n        aligned.iloc[:, 1].values,\n        quantiles=[0.05]\n    )\n\n    pairwise_td.append({\n        \"sector_1\": s1,\n        \"sector_2\": s2,\n        \"lambda_L_5pct\": td[\"lambda_L\"].iloc[0],\n        \"pearson_corr\": aligned.iloc[:, 0].corr(aligned.iloc[:, 1])\n    })\n\ntd_df = pd.DataFrame(pairwise_td)\n\n\n\n\nTable 41.5: Pairwise Tail Dependence vs. Linear Correlation Across Sectors\n\n\ntd_display = td_df.copy()\ntd_display[\"lambda_L_5pct\"] = td_display[\"lambda_L_5pct\"].round(4)\ntd_display[\"pearson_corr\"] = td_display[\"pearson_corr\"].round(4)\ntd_display[\"ratio\"] = (\n    td_display[\"lambda_L_5pct\"] / td_display[\"pearson_corr\"]\n).round(3)\n\ntd_display.sort_values(\"lambda_L_5pct\", ascending=False).head(15)\n\n\n\nThe ratio column in Table 41.5 is particularly informative. When \\(\\lambda_L / \\rho \\gg 1\\), the sectors exhibit disproportionately high co-crash frequency relative to their overall correlation. This is the hallmark of tail dependence that Gaussian models miss.\n\n\n41.4.3 System-Wide Stress Transmission\nTo assess system-wide contagion dynamics, we construct daily co-exceedance counts and examine their time-series properties.\n\n# Compute daily co-exceedance count at the 5% level\nsector_wide = sector_returns.pivot_table(\n    index=\"date\", columns=\"sector\", values=\"ret\"\n)\n\n# Rolling 252-day quantile thresholds\nquantile_thresholds = sector_wide.rolling(252, min_periods=60).quantile(0.05)\n\n# Indicator: return below rolling 5th percentile\nexceedance_indicators = (sector_wide &lt; quantile_thresholds).astype(int)\nco_exceedance = exceedance_indicators.sum(axis=1)\nco_exceedance.name = \"co_exceedance\"\n\nco_exceed_df = co_exceedance.reset_index()\nco_exceed_df.columns = [\"date\", \"co_exceedance\"]\nco_exceed_df[\"n_sectors\"] = exceedance_indicators.notna().sum(axis=1).values\n\n\n\n\n(\n    p9.ggplot(co_exceed_df.dropna(), p9.aes(x=\"date\", y=\"co_exceedance\"))\n    + p9.geom_line(color=\"#2E5090\", alpha=0.5, size=0.3)\n    + p9.geom_smooth(method=\"lowess\", color=\"#C0392B\", size=1, se=False)\n    + p9.labs(\n        x=\"\",\n        y=\"Number of Sectors in Left Tail\",\n        title=\"System-Wide Stress: Daily Co-Exceedance Count\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5))\n)\n\n\nFigure 41.8\n\n\n\nPeaks in the co-exceedance series correspond to system-wide stress episodes. The smoothed trend (red line) reveals the evolving baseline level of systemic fragility.\n\n\n41.4.4 Financial Contagion Mechanisms\nThe literature distinguishes several channels through which stress propagates across assets, sectors, and markets (Forbes and Rigobon 2002; Kaminsky, Reinhart, and Vegh 2002):\nDirect linkages. Balance sheet interconnections, interbank lending, and cross-holdings create direct transmission channels. When institution \\(A\\) holds assets issued by institution \\(B\\), losses at \\(B\\) directly impair \\(A\\)’s balance sheet.\nInformation contagion. Adverse news about one firm triggers reassessment of similarly exposed firms. King and Wadhwani (1990) and Kodres and Pritsker (2002) formalize this via Bayesian updating: investors cannot distinguish idiosyncratic from systematic shocks, so a crash in one asset leads them to update beliefs about common risk factors.\nLiquidity contagion. Fire sales by distressed institutions depress prices of commonly held assets, creating losses for other holders. Brunnermeier (2009) model this as a liquidity spiral: declining prices trigger margin calls, which force further sales, further depressing prices. In Vietnamese markets, the absence of circuit breakers beyond price limits and the concentration of holdings among a few large institutional investors amplify this channel.\nBehavioral contagion. Herding behavior (i.e., investors mimicking others’ trades) generates correlated selling pressure even in the absence of fundamental linkages. Bikhchandani, Hirshleifer, and Welch (1992) and Banerjee (1992) provide theoretical foundations. Choe, Kho, and Stulz (2005) document herding in emerging markets specifically.\nWe can test for contagion (as distinct from interdependence) using the Forbes and Rigobon (2002) framework. The null hypothesis is that the co-movement observed during a crisis period is fully explained by the increase in volatility (which mechanically inflates correlations). The adjusted correlation is:\n\\[\n\\rho^* = \\frac{\\rho_{\\text{crisis}}}{\\sqrt{1 + \\delta[1 - \\rho_{\\text{crisis}}^2]}}\n\\tag{41.30}\\]\nwhere \\(\\delta = \\sigma_{\\text{crisis}}^2 / \\sigma_{\\text{tranquil}}^2 - 1\\) is the relative increase in variance. If \\(\\rho^* &gt; \\rho_{\\text{tranquil}}\\), the increase in co-movement exceeds what volatility adjustment alone predicts, which is evidence of contagion.\n\ndef forbes_rigobon_test(returns_df, crisis_start, crisis_end):\n    \"\"\"\n    Forbes-Rigobon adjusted correlation test for contagion.\n\n    Parameters\n    ----------\n    returns_df : DataFrame\n        Columns are sector returns, index is date.\n    crisis_start, crisis_end : str\n        Crisis period boundaries.\n\n    Returns\n    -------\n    DataFrame : Pairwise contagion test results.\n    \"\"\"\n    tranquil = returns_df[returns_df.index &lt; crisis_start].dropna()\n    crisis = returns_df[\n        (returns_df.index &gt;= crisis_start) &\n        (returns_df.index &lt;= crisis_end)\n    ].dropna()\n\n    sectors = returns_df.columns.tolist()\n    results = []\n\n    for s1, s2 in itertools.combinations(sectors, 2):\n        if s1 not in tranquil.columns or s2 not in crisis.columns:\n            continue\n\n        rho_t = tranquil[s1].corr(tranquil[s2])\n        rho_c = crisis[s1].corr(crisis[s2])\n\n        var_t = tranquil[s1].var()\n        var_c = crisis[s1].var()\n\n        if var_t == 0:\n            continue\n\n        delta = var_c / var_t - 1\n        rho_adj = rho_c / np.sqrt(1 + delta * (1 - rho_c**2))\n\n        results.append({\n            \"sector_1\": s1,\n            \"sector_2\": s2,\n            \"rho_tranquil\": round(rho_t, 4),\n            \"rho_crisis\": round(rho_c, 4),\n            \"rho_adjusted\": round(rho_adj, 4),\n            \"contagion\": rho_adj &gt; rho_t\n        })\n\n    return pd.DataFrame(results)\n\n\n# Test for contagion during COVID-19 crash\ncontagion_covid = forbes_rigobon_test(\n    sector_wide,\n    crisis_start=\"2020-02-01\",\n    crisis_end=\"2020-04-30\"\n)\n\nn_pairs = len(contagion_covid)\nn_contagion = contagion_covid[\"contagion\"].sum()\nprint(f\"COVID-19 contagion test: {n_contagion}/{n_pairs} sector pairs \"\n      f\"show evidence of contagion ({100*n_contagion/n_pairs:.1f}%)\")\n\n\n\n\nTable 41.6: Forbes-Rigobon Contagion Test: COVID-19 Crisis Period\n\n\ncontagion_covid.sort_values(\n    \"rho_adjusted\", ascending=False\n).head(15)",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Tail Risk and Extreme Events</span>"
    ]
  },
  {
    "objectID": "61_tail_risk_extreme_events.html#applications-to-financial-stability",
    "href": "61_tail_risk_extreme_events.html#applications-to-financial-stability",
    "title": "41  Tail Risk and Extreme Events",
    "section": "41.5 Applications to Financial Stability",
    "text": "41.5 Applications to Financial Stability\n\n41.5.1 Market-Wide Stress Indicators\nWe construct several market-wide stress indicators that aggregate the individual-level and sectoral tail risk measures developed above into a comprehensive system-level diagnostic.\nAggregate Crash Risk Index. The cross-sectional average of firm-level NCSKEW provides a market-wide measure of crash risk that is forward-looking (it captures the buildup of negative skewness before a crash materializes):\n\\[\n\\text{ACRI}_y = \\frac{1}{N_y}\\sum_{i=1}^{N_y} \\text{NCSKEW}_{i,y}\n\\tag{41.31}\\]\nMarket Tail Risk (MTR). Following Kelly and Jiang (2014), we estimate the time-varying tail risk of the market portfolio using option-implied or realized tail measures. In the absence of a liquid options market in Vietnam, we use the realized version based on the Hill tail index estimated from rolling windows:\n\\[\n\\text{MTR}_t = \\hat{\\xi}_t^{\\text{Hill}} \\cdot \\hat{\\sigma}_t\n\\tag{41.32}\\]\nwhere \\(\\hat{\\xi}_t^{\\text{Hill}}\\) is the Hill estimator computed from a trailing 252-day window and \\(\\hat{\\sigma}_t\\) is the conditional volatility (e.g., from GARCH). This interaction captures both the thickness of the tail and the current scale of volatility.\n\n# Aggregate Crash Risk Index\nacri = (\n    crash_risk.groupby(\"year\")\n    .agg(\n        acri=(\"ncskew\", \"mean\"),\n        median_ncskew=(\"ncskew\", \"median\"),\n        pct_high_crash=(\"ncskew\", lambda x: (x &gt; x.quantile(0.75)).mean()),\n        n_firms=(\"ncskew\", \"count\")\n    )\n    .reset_index()\n)\n\n# Market Tail Risk: rolling Hill estimator x rolling volatility\nmkt_returns = market_daily[\"mkt_ret\"].dropna().values\ndates = market_daily[\"date\"].dropna().values\n\nmtr_list = []\nfor end_idx in range(504, len(mkt_returns)):\n    window_ret = mkt_returns[end_idx - 252:end_idx]\n    losses_pos = -window_ret[window_ret &lt; 0]\n\n    if len(losses_pos) &lt; 30:\n        continue\n\n    # Hill estimator at k = 50\n    sorted_losses = np.sort(losses_pos)[::-1]\n    k = min(50, len(sorted_losses) - 1)\n    if k &lt; 10:\n        continue\n    log_excess = np.log(sorted_losses[:k]) - np.log(sorted_losses[k])\n    xi_hat = log_excess.mean()\n\n    sigma_hat = window_ret.std()\n    mtr = xi_hat * sigma_hat\n\n    mtr_list.append({\n        \"date\": dates[end_idx],\n        \"xi_hat\": xi_hat,\n        \"sigma_hat\": sigma_hat,\n        \"mtr\": mtr\n    })\n\nmtr_df = pd.DataFrame(mtr_list)\nmtr_df[\"date\"] = pd.to_datetime(mtr_df[\"date\"])\n\n\n\n\n(\n    p9.ggplot(mtr_df, p9.aes(x=\"date\", y=\"mtr\"))\n    + p9.geom_line(color=\"#2E5090\", alpha=0.5, size=0.4)\n    + p9.geom_smooth(method=\"lowess\", color=\"#C0392B\", size=1, se=False)\n    + p9.labs(\n        x=\"\",\n        y=\"MTR (ξ × σ)\",\n        title=\"Market Tail Risk: Rolling Hill Index × Conditional Volatility\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5))\n)\n\n\nFigure 41.9\n\n\n\n\n\n41.5.2 Sectoral Fragility\nNot all sectors are equally vulnerable to tail events. We decompose system-wide tail risk into sectoral contributions to identify the most fragile parts of the market.\n\n# Sector-level tail statistics\nsector_tail_stats = []\n\nfor sector in sectors:\n    sector_data = sector_returns[sector_returns[\"sector\"] == sector][\"ret\"].dropna()\n\n    if len(sector_data) &lt; 500:\n        continue\n\n    s_data = sector_data.values\n\n    # Skewness and kurtosis\n    skew = stats.skew(s_data)\n    kurt = stats.kurtosis(s_data)\n\n    # VaR and ES at 1%\n    var_1 = -np.quantile(s_data, 0.01)\n    es_1 = -s_data[s_data &lt;= np.quantile(s_data, 0.01)].mean()\n\n    # Hill tail index (k=50)\n    losses_pos = -s_data[s_data &lt; 0]\n    sorted_l = np.sort(losses_pos)[::-1]\n    k = min(50, len(sorted_l) - 1)\n    if k &gt;= 10:\n        log_exc = np.log(sorted_l[:k]) - np.log(sorted_l[k])\n        xi = log_exc.mean()\n    else:\n        xi = np.nan\n\n    sector_tail_stats.append({\n        \"sector\": sector,\n        \"skewness\": round(skew, 3),\n        \"excess_kurtosis\": round(kurt, 3),\n        \"var_1pct\": round(var_1, 4),\n        \"es_1pct\": round(es_1, 4),\n        \"tail_index_xi\": round(xi, 3) if not np.isnan(xi) else np.nan,\n        \"n_obs\": len(s_data)\n    })\n\nfragility_df = pd.DataFrame(sector_tail_stats)\n\n\n\n\nTable 41.7: Sectoral Tail Risk Profile\n\n\nfragility_df.sort_values(\"es_1pct\", ascending=False)\n\n\n\n\n\n\n(\n    p9.ggplot(\n        fragility_df.dropna(),\n        p9.aes(x=\"tail_index_xi\", y=\"es_1pct\", label=\"sector\")\n    )\n    + p9.geom_point(color=\"#2E5090\", size=3)\n    + p9.geom_text(nudge_y=0.002, size=7)\n    + p9.labs(\n        x=\"Tail Index (ξ, Hill)\",\n        y=\"1% Expected Shortfall\",\n        title=\"Sector Fragility Map\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 7))\n)\n\n\nFigure 41.10\n\n\n\nSectors in the upper-right quadrant of Figure 41.10 exhibit both high expected shortfall (large average losses in the tail) and high tail index (heavier-than-typical tails). These represent the most fragile components of the market from a systemic stability perspective.\n\n\n41.5.3 Crisis Diagnostics\nWe construct a comprehensive crisis diagnostic framework that combines the indicators developed above to identify, characterize, and compare stress episodes in Vietnamese financial markets.\n\n# Identify extreme co-exceedance days (system-wide stress events)\nthreshold_high_stress = co_exceed_df[\"co_exceedance\"].quantile(0.99)\n\nstress_events = co_exceed_df[\n    co_exceed_df[\"co_exceedance\"] &gt;= threshold_high_stress\n].copy()\n\n# Add market return on stress days\nstress_events = stress_events.merge(\n    market_daily[[\"date\", \"mkt_ret\"]], on=\"date\", how=\"left\"\n)\n\nprint(f\"System-wide stress days (top 1%): {len(stress_events)}\")\nprint(f\"Average market return on stress days: \"\n      f\"{stress_events['mkt_ret'].mean():.4f}\")\nprint(f\"Average co-exceedance on stress days: \"\n      f\"{stress_events['co_exceedance'].mean():.1f}\")\n\n\n\n\nTable 41.8: Major Stress Episodes: Clustering of System-Wide Tail Events\n\n\n# Cluster stress days into episodes (within 10 trading days)\nstress_events = stress_events.sort_values(\"date\").reset_index(drop=True)\nstress_events[\"gap\"] = stress_events[\"date\"].diff().dt.days\nstress_events[\"episode\"] = (stress_events[\"gap\"] &gt; 15).cumsum()\n\nepisode_summary = (\n    stress_events.groupby(\"episode\")\n    .agg(\n        start_date=(\"date\", \"min\"),\n        end_date=(\"date\", \"max\"),\n        n_stress_days=(\"date\", \"count\"),\n        avg_market_return=(\"mkt_ret\", \"mean\"),\n        min_market_return=(\"mkt_ret\", \"min\"),\n        avg_coexceedance=(\"co_exceedance\", \"mean\")\n    )\n    .reset_index(drop=True)\n    .sort_values(\"min_market_return\")\n)\n\nepisode_summary[\"avg_market_return\"] = episode_summary[\"avg_market_return\"].round(4)\nepisode_summary[\"min_market_return\"] = episode_summary[\"min_market_return\"].round(4)\nepisode_summary[\"avg_coexceedance\"] = episode_summary[\"avg_coexceedance\"].round(1)\n\nepisode_summary.head(10)",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Tail Risk and Extreme Events</span>"
    ]
  },
  {
    "objectID": "61_tail_risk_extreme_events.html#summary",
    "href": "61_tail_risk_extreme_events.html#summary",
    "title": "41  Tail Risk and Extreme Events",
    "section": "41.6 Summary",
    "text": "41.6 Summary\nThis chapter developed a comprehensive toolkit for measuring and diagnosing tail risk in Vietnamese equity markets. The key objects estimated (crash risk measures, VaR and Expected Shortfall, tail indices, and tail dependence coefficients) capture distinct but complementary aspects of extreme event behavior.\nAt the individual stock level, the NCSKEW, DUVOL, and crash count measures quantify asymmetry in firm-specific return distributions, providing forward-looking indicators of crash vulnerability. At the portfolio level, EVT-based approaches to VaR and ES estimation avoid the parametric misspecification that plagues Gaussian and even Student-\\(t\\) models in the deep tails. At the system level, tail dependence and co-exceedance measures reveal the extent to which crashes propagate across sectors, and the Forbes-Rigobon contagion test distinguishes genuine contagion from the mechanical increase in co-movement driven by higher volatility.\nSeveral features of Vietnamese markets deserve emphasis. Price limits censor the observed return distribution, causing systematic underestimation of true tail risk. The concentration of trading among retail investors amplifies herding-driven crash dynamics. State ownership of large firms creates correlated exposure to policy shocks that is not captured by standard diversification. And the absence of deep derivatives markets limits the hedging instruments available for tail risk management, making accurate measurement all the more critical.\n\n\n\n\n\n\nArtzner, Philippe, Freddy Delbaen, Jean-Marc Eber, and David Heath. 1999. “Coherent Measures of Risk.” Mathematical Finance 9 (3): 203–28.\n\n\nBanerjee, Abhijit V. 1992. “A Simple Model of Herd Behavior.” The Quarterly Journal of Economics 107 (3): 797–817.\n\n\nBikhchandani, Sushil, David Hirshleifer, and Ivo Welch. 1992. “A Theory of Fads, Fashion, Custom, and Cultural Change as Informational Cascades.” Journal of Political Economy 100 (5): 992–1026.\n\n\nBrunnermeier, Markus K. 2009. “Deciphering the Liquidity and Credit Crunch 2007–2008.” Journal of Economic Perspectives 23 (1): 77–100.\n\n\nChen, Joseph, Harrison Hong, and Jeremy C Stein. 2001. “Forecasting Crashes: Trading Volume, Past Returns, and Conditional Skewness in Stock Prices.” Journal of Financial Economics 61 (3): 345–81.\n\n\nChoe, Hyuk, Bong-Chan Kho, and René M Stulz. 2005. “Do Domestic Investors Have an Edge? The Trading Experience of Foreign Investors in Korea.” The Review of Financial Studies 18 (3): 795–829.\n\n\nChristoffersen, Peter F. 1998. “Evaluating Interval Forecasts.” International Economic Review, 841–62.\n\n\nCont, Rama. 2001. “Empirical Properties of Asset Returns: Stylized Facts and Statistical Issues.” Quantitative Finance 1 (2): 223.\n\n\nDemarta, Stefano, and Alexander J McNeil. 2005. “The t Copula and Related Copulas.” International Statistical Review 73 (1): 111–29.\n\n\nForbes, Kristin J, and Roberto Rigobon. 2002. “No Contagion, Only Interdependence: Measuring Stock Market Comovements.” The Journal of Finance 57 (5): 2223–61.\n\n\nGabaix, Xavier, Parameswaran Gopikrishnan, Vasiliki Plerou, and H Eugene Stanley. 2003. “A Theory of Power-Law Distributions in Financial Market Fluctuations.” Nature 423 (6937): 267–70.\n\n\nHarvey, Campbell R, and Akhtar Siddique. 2000. “Conditional Skewness in Asset Pricing Tests.” The Journal of Finance 55 (3): 1263–95.\n\n\nHill, Bruce M. 1975. “A Simple General Approach to Inference about the Tail of a Distribution.” The Annals of Statistics, 1163–74.\n\n\nHutton, Amy P, Alan J Marcus, and Hassan Tehranian. 2009. “Opaque Financial Reports, R2, and Crash Risk.” Journal of Financial Economics 94 (1): 67–86.\n\n\nKaminsky, Graciela L, Carmen M Reinhart, and Carlos A Vegh. 2002. “The Unholy Trinity of Financial Contagion.” Journal of Economic Perspectives 17 (4): 51–74.\n\n\nKelly, Bryan, and Hao Jiang. 2014. “Tail Risk and Asset Prices.” The Review of Financial Studies 27 (10): 2841–71.\n\n\nKing, Mervyn A, and Sushil Wadhwani. 1990. “Transmission of Volatility Between Stock Markets.” The Review of Financial Studies 3 (1): 5–33.\n\n\nKodres, Laura E, and Matthew Pritsker. 2002. “A Rational Expectations Model of Financial Contagion.” The Journal of Finance 57 (2): 769–99.\n\n\nKraus, Alan, and Robert H Litzenberger. 1976. “Skewness Preference and the Valuation of Risk Assets.” The Journal of Finance 31 (4): 1085–1100.\n\n\nKupiec, Paul H et al. 1995. “Techniques for Verifying the Accuracy of Risk Measurement Models.”\n\n\nLongin, Francois, and Bruno Solnik. 2001. “Extreme Correlation of International Equity Markets.” The Journal of Finance 56 (2): 649–76.\n\n\nMandelbrot, Benoit et al. 1963. “The Variation of Certain Speculative Prices.” Journal of Business 36 (4): 394.\n\n\nMcNeil, Alexander J, and Rüdiger Frey. 2000. “Estimation of Tail-Related Risk Measures for Heteroscedastic Financial Time Series: An Extreme Value Approach.” Journal of Empirical Finance 7 (3-4): 271–300.\n\n\nMcNeil, Alexander J, Rüdiger Frey, and Paul Embrechts. 2015. Quantitative Risk Management: Concepts, Techniques and Tools-Revised Edition. Princeton university press.\n\n\nTaleb, Nassim Nicholas. 2010. The Black Swan:: The Impact of the Highly Improbable: With a New Section:\" on Robustness and Fragility\". Vol. 2. Random house trade paperbacks.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Tail Risk and Extreme Events</span>"
    ]
  },
  {
    "objectID": "62_corporate_finance_estimators.html",
    "href": "62_corporate_finance_estimators.html",
    "title": "42  Corporate Finance Estimators and Identification",
    "section": "",
    "text": "42.1 Investment-\\(Q\\) Regressions\nCorporate finance is the study of how firms make investment, financing, and payout decisions under real-world frictions (e.g., taxes, asymmetric information, agency conflicts, transaction costs, and financial constraints). Unlike asset pricing, where the primary objects of interest are expected returns and risk premia estimated from market data, corporate finance estimators are tied to firm-level accounting and governance data, and their economic interpretation depends critically on the institutional environment in which the firm operates.\nThis chapter develops the core econometric toolkit for empirical corporate finance and applies it to Vietnamese listed firms. The estimators we cover (including investment-\\(Q\\) regressions, cash flow sensitivity tests, capital structure determinants, payout smoothing models, and agency cost proxies) form the backbone of the modern corporate finance literature. Each estimator embeds specific theoretical assumptions, and each has been the subject of substantial methodological debate. We pay careful attention to identification challenges: the conditions under which a regression coefficient admits a causal or structural interpretation versus merely a descriptive association.\nVietnamese firms present distinctive features that interact with these estimators in economically meaningful ways. State ownership remains pervasive and creates agency problems qualitatively different from the dispersed-ownership setting of the Anglo-American literature. Concentrated family ownership, pyramidal structures, and cross-holdings generate tunneling incentives documented by Johnson et al. (2000) and Claessens et al. (2002). The banking system is dominated by state-owned commercial banks whose lending decisions may reflect political rather than purely economic criteria, complicating the interpretation of financing constraint measures. And dividend policy is shaped by regulatory requirements, including minimum payout ratios for state-owned enterprises, that have no parallel in more developed markets.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Corporate Finance Estimators and Identification</span>"
    ]
  },
  {
    "objectID": "62_corporate_finance_estimators.html#investment-q-regressions",
    "href": "62_corporate_finance_estimators.html#investment-q-regressions",
    "title": "42  Corporate Finance Estimators and Identification",
    "section": "",
    "text": "42.1.1 Tobin’s \\(Q\\): Intuition and Theory\nThe investment-\\(Q\\) framework is the canonical structural model of corporate investment. The core insight, formalized by Hayashi (1982), is elegant: under perfect capital markets and constant returns to scale in the production and adjustment cost technologies, a firm’s investment rate should be a sufficient statistic of a single observable (i.e., the ratio of the market value of installed capital to its replacement cost).\nLet \\(V_t\\) denote the market value of the firm’s assets at time \\(t\\) and \\(K_t\\) the replacement cost of its capital stock. Tobin’s \\(Q\\) is:\n\\[\nQ_t = \\frac{V_t}{K_t}\n\\tag{42.1}\\]\nWhen \\(Q &gt; 1\\), the market values a unit of installed capital above its replacement cost, signaling that the firm should invest. When \\(Q &lt; 1\\), the firm should disinvest. In the frictionless Hayashi (1982) environment, the marginal \\(Q\\) (the shadow value of an additional unit of capital) equals the average \\(Q\\) (the ratio of total market value to total replacement cost), and the optimal investment policy is:\n\\[\n\\frac{I_{i,t}}{K_{i,t-1}} = \\frac{1}{\\alpha}\\left(Q_{i,t} - 1\\right)\n\\tag{42.2}\\]\nwhere \\(\\alpha\\) governs the convexity of adjustment costs. The empirical counterpart is the regression:\n\\[\n\\frac{I_{i,t}}{K_{i,t-1}} = \\beta_0 + \\beta_1 Q_{i,t} + \\varepsilon_{i,t}\n\\tag{42.3}\\]\nUnder the structural interpretation, \\(\\beta_1 = 1/\\alpha &gt; 0\\) and \\(Q\\) is the sole explanatory variable. Any additional variable that enters significantly implies a violation of the underlying assumptions (e.g., financial frictions, agency problems, measurement error in \\(Q\\), or departures from constant returns to scale).\n\n\n42.1.2 Measurement Issues\nThe theoretical object is marginal \\(Q\\) (i.e., the value of the next dollar of investment) which is unobservable. The empirical proxy is average \\(Q\\), typically constructed as:\n\\[\nQ_{i,t}^{\\text{avg}} = \\frac{\\text{Market Value of Equity} + \\text{Book Value of Debt}}{\\text{Book Value of Total Assets}}\n\\tag{42.4}\\]\nThis proxy introduces several problems that are well-documented in the literature.\nProblem 1: Marginal \\(\\neq\\) Average. The equality \\(q^{\\text{marginal}} = Q^{\\text{average}}\\) requires constant returns to scale in both production and adjustment costs (Hayashi 1982). With decreasing returns to scale (empirically relevant for most firms), average \\(Q\\) overstates marginal \\(Q\\) for high-\\(Q\\) firms and understates it for low-\\(Q\\) firms. Abel and Eberly (1994) derive the wedge analytically.\nProblem 2: Measurement error in numerator. The market value of equity reflects market sentiment, bubbles, and noise-trader demand in addition to fundamentals. P. Bond, Edmans, and Goldstein (2012) provide a comprehensive treatment. In Vietnamese markets, where retail investors dominate and price limits constrain daily adjustment, market prices may deviate persistently from fundamental value.\nProblem 3: Measurement error in denominator. Book values of assets reflect historical cost, depreciation schedules, and accounting conventions that may poorly approximate replacement cost. This is especially problematic in Vietnam, where revaluation of fixed assets is infrequent and inflation has historically been volatile, creating wedges between historical and replacement cost.\nProblem 4: Errors-in-variables bias. Because the empirical \\(Q\\) is a noisy proxy for the true \\(Q\\), OLS estimates of \\(\\beta_1\\) in Equation 42.3 suffer from classical attenuation bias (i.e., \\(hat{\\beta}_1\\) is biased toward zero). Erickson and Whited (2012) develop a higher-order cumulant estimator that corrects for this bias without requiring external instruments.\n\n# Construct Tobin's Q and investment variables\npanel = firm_annual.copy()\n\n# Tobin's Q: (Market cap + Book debt) / Total assets\npanel[\"tobins_q\"] = (\n    (panel[\"market_cap\"] + panel[\"total_debt\"]) /\n    panel[\"total_assets\"]\n)\n\n# Investment rate: Capital expenditure / Lagged total assets\npanel = panel.sort_values([\"ticker\", \"year\"])\npanel[\"lag_assets\"] = panel.groupby(\"ticker\")[\"total_assets\"].shift(1)\npanel[\"lag_ppe\"] = panel.groupby(\"ticker\")[\"ppe_net\"].shift(1)\n\npanel[\"inv_rate\"] = panel[\"capex\"] / panel[\"lag_assets\"]\npanel[\"inv_rate_ppe\"] = panel[\"capex\"] / panel[\"lag_ppe\"]\n\n# Cash flow / Assets\npanel[\"cf_assets\"] = panel[\"operating_cf\"] / panel[\"lag_assets\"]\n\n# Sales growth\npanel[\"lag_revenue\"] = panel.groupby(\"ticker\")[\"revenue\"].shift(1)\npanel[\"sales_growth\"] = (\n    (panel[\"revenue\"] - panel[\"lag_revenue\"]) / panel[\"lag_revenue\"]\n)\n\n# Winsorize at 1st and 99th percentiles\ndef winsorize(s, lower=0.01, upper=0.99):\n    return s.clip(s.quantile(lower), s.quantile(upper))\n\nfor col in [\"tobins_q\", \"inv_rate\", \"cf_assets\", \"sales_growth\"]:\n    panel[col] = winsorize(panel[col])\n\npanel_clean = panel.dropna(\n    subset=[\"tobins_q\", \"inv_rate\", \"cf_assets\"]\n).copy()\n\nprint(f\"Clean panel: {len(panel_clean)} firm-years, \"\n      f\"{panel_clean['ticker'].nunique()} firms\")\n\n\n\n\nTable 42.1: Summary Statistics: Investment and Q Variables\n\n\nsummary_vars = [\"inv_rate\", \"tobins_q\", \"cf_assets\", \"sales_growth\"]\nsummary = panel_clean[summary_vars].describe(\n    percentiles=[0.05, 0.25, 0.5, 0.75, 0.95]\n).T.round(4)\n\nsummary.columns = [\"N\", \"Mean\", \"Std\", \"Min\", \"5%\", \"25%\",\n                    \"Median\", \"75%\", \"95%\", \"Max\"]\nsummary\n\n\n\n\n# Baseline investment-Q regression with firm and year fixed effects\npanel_clean = panel_clean.set_index([\"ticker\", \"year\"])\n\n# Model 1: Q only\nmodel1 = PanelOLS(\n    panel_clean[\"inv_rate\"],\n    panel_clean[[\"tobins_q\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\n# Model 2: Q + Cash Flow\nmodel2 = PanelOLS(\n    panel_clean[\"inv_rate\"],\n    panel_clean[[\"tobins_q\", \"cf_assets\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\n# Model 3: Q + Cash Flow + Sales Growth\nmodel3 = PanelOLS(\n    panel_clean[\"inv_rate\"],\n    panel_clean[[\"tobins_q\", \"cf_assets\", \"sales_growth\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\npanel_clean = panel_clean.reset_index()\n\n\n\n\nTable 42.2: Investment-Q Regressions with Firm and Year Fixed Effects\n\n\nresults_table = pd.DataFrame({\n    \"Q Only\": {\n        \"Tobin's Q\": f\"{model1.params['tobins_q']:.4f}\",\n        \"\": f\"({model1.std_errors['tobins_q']:.4f})\",\n        \"Cash Flow/Assets\": \"\",\n        \" \": \"\",\n        \"Sales Growth\": \"\",\n        \"  \": \"\",\n        \"R² (within)\": f\"{model1.rsquared_within:.4f}\",\n        \"N\": f\"{int(model1.nobs)}\"\n    },\n    \"Q + CF\": {\n        \"Tobin's Q\": f\"{model2.params['tobins_q']:.4f}\",\n        \"\": f\"({model2.std_errors['tobins_q']:.4f})\",\n        \"Cash Flow/Assets\": f\"{model2.params['cf_assets']:.4f}\",\n        \" \": f\"({model2.std_errors['cf_assets']:.4f})\",\n        \"Sales Growth\": \"\",\n        \"  \": \"\",\n        \"R² (within)\": f\"{model2.rsquared_within:.4f}\",\n        \"N\": f\"{int(model2.nobs)}\"\n    },\n    \"Q + CF + SG\": {\n        \"Tobin's Q\": f\"{model3.params['tobins_q']:.4f}\",\n        \"\": f\"({model3.std_errors['tobins_q']:.4f})\",\n        \"Cash Flow/Assets\": f\"{model3.params['cf_assets']:.4f}\",\n        \" \": f\"({model3.std_errors['cf_assets']:.4f})\",\n        \"Sales Growth\": f\"{model3.params['sales_growth']:.4f}\",\n        \"  \": f\"({model3.std_errors['sales_growth']:.4f})\",\n        \"R² (within)\": f\"{model3.rsquared_within:.4f}\",\n        \"N\": f\"{int(model3.nobs)}\"\n    }\n})\n\nresults_table\n\n\n\n\n\n42.1.3 Interpretation Under Market Frictions\nThe coefficient on \\(Q\\) in Table 42.2 admits multiple interpretations depending on the maintained assumptions:\nStructural interpretation. If the Hayashi conditions hold, \\(\\hat{\\beta}_1\\) estimates the inverse of the adjustment cost parameter: \\(\\hat{\\beta}_1 = 1/\\hat{\\alpha}\\). A larger coefficient implies lower adjustment costs. However, measurement error in \\(Q\\) biases \\(\\hat{\\beta}_1\\) downward, so the raw OLS estimate provides a lower bound on \\(1/\\alpha\\).\nReduced-form interpretation. Without the Hayashi conditions, \\(\\hat{\\beta}_1\\) captures the association between market valuation and investment intensity. This association reflects a mixture of genuine investment opportunities (the \\(Q\\)-theory channel), market mispricing that managers exploit (the market timing channel of Baker, Stein, and Wurgler (2003)), and reverse causality (investment announcements that move market values).\nThe cash flow coefficient puzzle. The significance of \\(\\hat{\\beta}_2\\) on cash flow has been the subject of a 35-year debate. Fazzari, Hubbard, and Petersen (1987) interpret it as evidence that firms face financing constraints: controlling for investment opportunities (\\(Q\\)), cash flow should be irrelevant in a frictionless world, so its significance implies that internal funds relax binding constraints. Kaplan and Zingales (1997) counter that cash flow proxies for investment opportunities not captured by the noisy \\(Q\\) measure, making the cash flow coefficient an artifact of measurement error rather than evidence of constraints. Erickson and Whited (2012) show that correcting for measurement error in \\(Q\\) substantially reduces (but does not eliminate) the cash flow coefficient, supporting a middle ground.\n\n\n42.1.4 Limitations in Emerging Markets\nThe investment-\\(Q\\) framework faces amplified challenges in Vietnamese markets.\nThin trading and price limits. Market prices adjust slowly to information, so \\(Q\\) measured at fiscal year-end may not reflect the firm’s current investment opportunity set. Price limits of \\(\\pm 7\\%\\) (HOSE) and \\(\\pm 10\\%\\) (HNX) mechanically compress the numerator of \\(Q\\), attenuating the investment-\\(Q\\) relationship.\nState ownership. For state-owned enterprises (SOEs), investment decisions may be driven by policy directives rather than \\(Q\\)-theoretic optimality. Including SOEs in the regression without interactions confounds the structural relationship.\nRelated-party transactions. Tunneling through related-party transactions means that measured investment may include capital expenditures that benefit controlling shareholders rather than maximizing firm value. The investment-\\(Q\\) coefficient in tunneling firms reflects the relationship between market valuation and expropriation, not efficient capital allocation.\n\n\n\n# Create Q-decile bins for clean visualization\nplot_data = panel_clean.copy()\nplot_data[\"q_bin\"] = pd.qcut(\n    plot_data[\"tobins_q\"], q=20, duplicates=\"drop\"\n)\n\nbinned = (\n    plot_data.groupby(\"q_bin\", observed=True)\n    .agg(\n        mean_q=(\"tobins_q\", \"mean\"),\n        mean_inv=(\"inv_rate\", \"mean\"),\n        se_inv=(\"inv_rate\", lambda x: x.std() / np.sqrt(len(x)))\n    )\n    .reset_index()\n)\n\n(\n    p9.ggplot(binned, p9.aes(x=\"mean_q\", y=\"mean_inv\"))\n    + p9.geom_pointrange(\n        p9.aes(ymin=\"mean_inv - 1.96*se_inv\",\n               ymax=\"mean_inv + 1.96*se_inv\"),\n        color=\"#2E5090\", size=0.5\n    )\n    + p9.geom_smooth(method=\"lm\", color=\"#C0392B\", se=False, size=0.8)\n    + p9.labs(\n        x=\"Tobin's Q (Vingtile Mean)\",\n        y=\"Investment Rate (I/A)\",\n        title=\"Investment-Q Relationship: Binned Scatter\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 6))\n)\n\n\nFigure 42.1\n\n\n\n\n# Merge ownership data\npanel_with_own = panel_clean.merge(\n    ownership[[\"ticker\", \"year\", \"state_ownership_pct\",\n               \"foreign_ownership_pct\", \"insider_ownership_pct\"]],\n    on=[\"ticker\", \"year\"],\n    how=\"left\"\n)\n\npanel_with_own[\"soe_dummy\"] = (\n    panel_with_own[\"state_ownership_pct\"] &gt; 50\n).astype(int)\n\npanel_with_own[\"q_x_soe\"] = (\n    panel_with_own[\"tobins_q\"] * panel_with_own[\"soe_dummy\"]\n)\npanel_with_own[\"cf_x_soe\"] = (\n    panel_with_own[\"cf_assets\"] * panel_with_own[\"soe_dummy\"]\n)\n\n# Regression with SOE interactions\npanel_soe = panel_with_own.dropna(\n    subset=[\"inv_rate\", \"tobins_q\", \"cf_assets\", \"soe_dummy\"]\n).set_index([\"ticker\", \"year\"])\n\nmodel_soe = PanelOLS(\n    panel_soe[\"inv_rate\"],\n    panel_soe[[\"tobins_q\", \"cf_assets\", \"soe_dummy\",\n               \"q_x_soe\", \"cf_x_soe\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\npanel_soe = panel_soe.reset_index()\n\n\n\n\nTable 42.3: Investment-Q Regression with State Ownership Interactions\n\n\nsoe_results = pd.DataFrame({\n    \"Coefficient\": model_soe.params.round(4),\n    \"Std Error\": model_soe.std_errors.round(4),\n    \"t-stat\": model_soe.tstats.round(3),\n    \"p-value\": model_soe.pvalues.round(4)\n})\n\nsoe_results\n\n\n\nA negative coefficient on \\(Q \\times \\text{SOE}\\) indicates that the investment-\\(Q\\) sensitivity is attenuated for state-owned enterprises, consistent with SOE investment being driven by non-market factors. The interaction of cash flow with SOE status reveals whether state firms face tighter or looser financing constraints. This is a question with direct policy implications for SOE reform.\n\n\n42.1.5 The Erickson-Whited Measurement Error Correction\nErickson and Whited (2012) develop a GMM estimator that uses higher-order moments of the data to identify the investment-\\(Q\\) slope in the presence of measurement error, without requiring external instruments. The key insight is that if the measurement error \\(\\eta\\) in \\(Q\\) is independent of the true \\(Q^*\\) and the structural error \\(\\varepsilon\\), then the third-order cumulants identify the signal-to-noise ratio.\nThe model is:\n\\[\n\\frac{I_{i,t}}{K_{i,t-1}} = \\beta_0 + \\beta_1 Q_{i,t}^* + \\gamma X_{i,t} + \\varepsilon_{i,t}, \\qquad Q_{i,t} = Q_{i,t}^* + \\eta_{i,t}\n\\tag{42.5}\\]\nwhere \\(Q_{i,t}^*\\) is unobserved true \\(Q\\) and \\(\\eta_{i,t}\\) is measurement error. The OLS estimator \\(\\hat{\\beta}_1^{\\text{OLS}}\\) converges to \\(\\beta_1 \\cdot \\lambda\\) where \\(\\lambda = \\text{Var}(Q^*) / (\\text{Var}(Q^*) + \\text{Var}(\\eta)) &lt; 1\\) is the signal-to-noise ratio. The Erickson-Whited estimator recovers \\(\\beta_1\\) and \\(\\lambda\\) simultaneously.\n\ndef erickson_whited_gmm(y, Q_obs, X=None, order=3):\n    \"\"\"\n    Simplified Erickson-Whited (2012) measurement error correction\n    using third-order cumulants.\n\n    Parameters\n    ----------\n    y : array\n        Dependent variable (investment rate).\n    Q_obs : array\n        Observed (mismeasured) Q.\n    X : array or None\n        Additional controls (partialled out first).\n    order : int\n        Cumulant order for identification (3 or 5).\n\n    Returns\n    -------\n    dict : Corrected beta, signal-to-noise ratio, OLS beta.\n    \"\"\"\n    if X is not None:\n        # Partial out controls via OLS\n        X_aug = sm.add_constant(X)\n        y = y - X_aug @ np.linalg.lstsq(X_aug, y, rcond=None)[0]\n        Q_obs = Q_obs - X_aug @ np.linalg.lstsq(X_aug, Q_obs, rcond=None)[0]\n\n    # Demean\n    y_dm = y - y.mean()\n    q_dm = Q_obs - Q_obs.mean()\n    n = len(y)\n\n    # Second moments\n    m_yq = np.mean(y_dm * q_dm)\n    m_qq = np.mean(q_dm**2)\n\n    # OLS beta\n    beta_ols = m_yq / m_qq\n\n    # Third-order cumulants for identification\n    k3_q = np.mean(q_dm**3)\n    k2y_q = np.mean(y_dm * q_dm**2)\n\n    if abs(k3_q) &lt; 1e-10:\n        return {\n            \"beta_corrected\": np.nan,\n            \"lambda_snr\": np.nan,\n            \"beta_ols\": beta_ols,\n            \"note\": \"Insufficient skewness for identification\"\n        }\n\n    # Corrected beta: beta = kappa_{y,q,q} / kappa_{q,q,q}\n    beta_ew = k2y_q / k3_q\n\n    # Signal-to-noise ratio\n    # lambda = kappa_{q,q,q}^2 / (kappa_{q,q} * kappa_{q,q,q,q,q})\n    # Simplified: lambda = beta_ols / beta_ew\n    lambda_snr = beta_ols / beta_ew if abs(beta_ew) &gt; 1e-10 else np.nan\n\n    return {\n        \"beta_corrected\": beta_ew,\n        \"lambda_snr\": lambda_snr,\n        \"beta_ols\": beta_ols,\n        \"attenuation_pct\": round((1 - lambda_snr) * 100, 1) if not np.isnan(lambda_snr) else np.nan\n    }\n\n# Apply to Vietnamese data\new_data = panel_clean.dropna(subset=[\"inv_rate\", \"tobins_q\", \"cf_assets\"])\n\new_result = erickson_whited_gmm(\n    y=ew_data[\"inv_rate\"].values,\n    Q_obs=ew_data[\"tobins_q\"].values,\n    X=ew_data[\"cf_assets\"].values.reshape(-1, 1)\n)\n\nprint(\"Erickson-Whited Measurement Error Correction:\")\nfor k, v in ew_result.items():\n    if isinstance(v, float):\n        print(f\"  {k}: {v:.4f}\")\n    else:\n        print(f\"  {k}: {v}\")",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Corporate Finance Estimators and Identification</span>"
    ]
  },
  {
    "objectID": "62_corporate_finance_estimators.html#cash-flow-sensitivity-of-investment",
    "href": "62_corporate_finance_estimators.html#cash-flow-sensitivity-of-investment",
    "title": "42  Corporate Finance Estimators and Identification",
    "section": "42.2 Cash Flow Sensitivity of Investment",
    "text": "42.2 Cash Flow Sensitivity of Investment\n\n42.2.1 The Financing Constraints Hypothesis\nThe cash flow sensitivity of investment (CFSI) literature tests whether firms’ investment decisions are constrained by the availability of internal funds. In a Modigliani-Miller world, internal and external funds are perfect substitutes, so cash flow should be irrelevant for investment after controlling for investment opportunities. The CFSI approach, pioneered by Fazzari, Hubbard, and Petersen (1987), classifies firms as financially constrained or unconstrained using observable characteristics and tests whether constrained firms exhibit higher sensitivity of investment to cash flow.\nThe augmented investment regression is:\n\\[\n\\frac{I_{i,t}}{K_{i,t-1}} = \\beta_0 + \\beta_1 Q_{i,t} + \\beta_2 \\frac{CF_{i,t}}{K_{i,t-1}} + \\varepsilon_{i,t}\n\\tag{42.6}\\]\nThe CFSI hypothesis predicts \\(\\beta_2^{\\text{constrained}} &gt; \\beta_2^{\\text{unconstrained}} &gt; 0\\): constrained firms rely more heavily on internal cash flow to fund investment because external finance is costly or unavailable.\n\n\n42.2.2 The FHP-KZ Debate\nFazzari, Hubbard, and Petersen (1987) (FHP) classify firms by dividend payout ratios and find that low-payout firms (presumed constrained) exhibit significantly higher cash flow sensitivity. Kaplan and Zingales (1997) (KZ) challenge this interpretation on two grounds:\nCritique 1: \\(Q\\) measurement error. If \\(Q\\) is a noisy proxy for true investment opportunities, and cash flow is correlated with the measurement error (because both respond to demand shocks), then the cash flow coefficient captures omitted investment opportunities, not financing constraints.\nCritique 2: Monotonicity failure. KZ show that the firms FHP classify as “most constrained” (low-payout firms) are often rapidly growing firms that choose to retain earnings for investment, not firms that are denied external financing. Using qualitative information from annual reports, KZ reclassify firms and find that the CFSI ranking reverses: firms judged to be truly constrained by their own disclosures exhibit lower CFSI than unconstrained firms.\nThe resolution, as argued by Farre-Mensa and Ljungqvist (2016), is that no single proxy reliably identifies financially constrained firms. Each proxy (size, age, payout ratio, bond rating, KZ index, WW index, SA index) captures a different dimension of the financing environment, and the CFSI test is not a clean test of any single theory.\n\n\n42.2.3 Constraint Indices\nWe implement the three most widely used composite constraint measures.\nKZ Index (Kaplan and Zingales 1997; Lamont, Polk, and Saaá-Requejo 2001):\n\\[\n\\text{KZ}_{i,t} = -1.002 \\cdot \\frac{CF_{i,t}}{K_{i,t-1}} + 0.283 \\cdot Q_{i,t} + 3.139 \\cdot \\frac{D_{i,t}}{A_{i,t}} - 39.368 \\cdot \\frac{\\text{Div}_{i,t}}{K_{i,t-1}} - 1.315 \\cdot \\frac{C_{i,t}}{K_{i,t-1}}\n\\tag{42.7}\\]\nWW Index (Whited and Wu 2006):\n\\[\n\\text{WW}_{i,t} = -0.091 \\cdot \\frac{CF_{i,t}}{A_{i,t}} - 0.062 \\cdot \\mathbb{1}(\\text{Div} &gt; 0) + 0.021 \\cdot \\frac{D_{i,t}}{A_{i,t}} - 0.044 \\cdot \\ln(A_{i,t}) + 0.102 \\cdot \\text{ISG}_{i,t} - 0.035 \\cdot \\text{SG}_{i,t}\n\\tag{42.8}\\]\nwhere ISG is industry sales growth and SG is firm sales growth.\nSA Index (Hadlock and Pierce 2010):\n\\[\n\\text{SA}_{i,t} = -0.737 \\cdot \\text{Size}_{i,t} + 0.043 \\cdot \\text{Size}_{i,t}^2 - 0.040 \\cdot \\text{Age}_{i,t}\n\\tag{42.9}\\]\nwhere Size \\(= \\ln(\\text{Total Assets})\\) and Age is years since listing. Hadlock and Pierce (2010) argue that the SA index is preferable because it uses only exogenous firm characteristics (size and age), avoiding the endogeneity inherent in cash flow and leverage-based indices.\n\n# Compute financial constraint indices\npanel_fc = panel_clean.copy()\n\n# Lagged PPE for KZ scaling\npanel_fc[\"lag_ppe\"] = panel_fc.groupby(\"ticker\")[\"ppe_net\"].shift(1)\n\n# KZ Index\npanel_fc[\"kz_index\"] = (\n    -1.002 * panel_fc[\"cf_assets\"]\n    + 0.283 * panel_fc[\"tobins_q\"]\n    + 3.139 * (panel_fc[\"total_debt\"] / panel_fc[\"total_assets\"])\n    - 39.368 * (panel_fc[\"dividends\"] / panel_fc[\"lag_assets\"])\n    - 1.315 * (panel_fc[\"cash\"] / panel_fc[\"lag_assets\"])\n)\n\n# SA Index\npanel_fc[\"log_assets\"] = np.log(panel_fc[\"total_assets\"])\npanel_fc[\"listing_age\"] = panel_fc[\"year\"] - panel_fc[\"listing_year\"]\n\npanel_fc[\"sa_index\"] = (\n    -0.737 * panel_fc[\"log_assets\"]\n    + 0.043 * panel_fc[\"log_assets\"]**2\n    - 0.040 * panel_fc[\"listing_age\"]\n)\n\n# WW Index (simplified: using firm-level variables)\npanel_fc[\"div_dummy\"] = (panel_fc[\"dividends\"] &gt; 0).astype(int)\npanel_fc[\"leverage\"] = panel_fc[\"total_debt\"] / panel_fc[\"total_assets\"]\n\n# Industry sales growth\npanel_fc[\"isg\"] = panel_fc.groupby(\n    [\"industry\", \"year\"]\n)[\"sales_growth\"].transform(\"median\")\n\npanel_fc[\"ww_index\"] = (\n    -0.091 * panel_fc[\"cf_assets\"]\n    - 0.062 * panel_fc[\"div_dummy\"]\n    + 0.021 * panel_fc[\"leverage\"]\n    - 0.044 * panel_fc[\"log_assets\"]\n    + 0.102 * panel_fc[\"isg\"]\n    - 0.035 * panel_fc[\"sales_growth\"]\n)\n\n\n\n\nTable 42.4: Distribution of Financial Constraint Indices\n\n\nconstraint_vars = [\"kz_index\", \"sa_index\", \"ww_index\"]\nconstraint_summary = (\n    panel_fc[constraint_vars]\n    .describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])\n    .T.round(4)\n)\nconstraint_summary\n\n\n\n\n\n42.2.4 Split-Sample CFSI Tests\nWe classify firms into constrained and unconstrained groups using each index and compare the cash flow sensitivity of investment across groups.\n\ndef cfsi_by_group(data, group_var, threshold=\"median\"):\n    \"\"\"\n    Estimate cash flow sensitivity of investment by constraint group.\n\n    Parameters\n    ----------\n    data : DataFrame\n        Panel data with inv_rate, tobins_q, cf_assets, group_var.\n    group_var : str\n        Variable used for classification.\n    threshold : str\n        \"median\" for sample split or \"tercile\" for top/bottom third.\n\n    Returns\n    -------\n    dict : Coefficient estimates by group.\n    \"\"\"\n    df = data.dropna(subset=[\"inv_rate\", \"tobins_q\", \"cf_assets\", group_var])\n\n    if threshold == \"median\":\n        median_val = df[group_var].median()\n        df[\"constrained\"] = (df[group_var] &gt;= median_val).astype(int)\n    elif threshold == \"tercile\":\n        t33 = df[group_var].quantile(0.33)\n        t67 = df[group_var].quantile(0.67)\n        df = df[(df[group_var] &lt;= t33) | (df[group_var] &gt;= t67)]\n        df[\"constrained\"] = (df[group_var] &gt;= t67).astype(int)\n\n    results = {}\n    for group_name, group_label in [(0, \"Unconstrained\"), (1, \"Constrained\")]:\n        subset = df[df[\"constrained\"] == group_name].copy()\n        if len(subset) &lt; 100:\n            continue\n\n        subset = subset.set_index([\"ticker\", \"year\"])\n        model = PanelOLS(\n            subset[\"inv_rate\"],\n            subset[[\"tobins_q\", \"cf_assets\"]],\n            entity_effects=True,\n            time_effects=True,\n            check_rank=False\n        ).fit(cov_type=\"clustered\", cluster_entity=True)\n\n        results[group_label] = {\n            \"beta_Q\": model.params[\"tobins_q\"],\n            \"se_Q\": model.std_errors[\"tobins_q\"],\n            \"beta_CF\": model.params[\"cf_assets\"],\n            \"se_CF\": model.std_errors[\"cf_assets\"],\n            \"R2_within\": model.rsquared_within,\n            \"N\": int(model.nobs)\n        }\n\n    return pd.DataFrame(results).T\n\n# Run for each constraint index\ncfsi_kz = cfsi_by_group(panel_fc, \"kz_index\", \"median\")\ncfsi_sa = cfsi_by_group(panel_fc, \"sa_index\", \"median\")\ncfsi_ww = cfsi_by_group(panel_fc, \"ww_index\", \"median\")\n\n\n\n\nTable 42.5: Cash Flow Sensitivity by Financial Constraint Classification\n\n\n# Combine results\ncfsi_all = pd.concat({\n    \"KZ Index\": cfsi_kz,\n    \"SA Index\": cfsi_sa,\n    \"WW Index\": cfsi_ww\n})\n\ncfsi_display = cfsi_all[[\"beta_CF\", \"se_CF\", \"beta_Q\", \"se_Q\", \"N\"]].round(4)\ncfsi_display\n\n\n\n\n\n42.2.5 Alternative Specifications\nThe baseline CFSI test has been augmented in several directions:\nDynamic investment models. S. Bond et al. (2003) argue that the static regression Equation 42.6 omits the autoregressive component of investment. The Euler equation approach, which derives directly from the firm’s dynamic optimization problem, yields:\n\\[\n\\frac{I_{i,t}}{K_{i,t-1}} = \\gamma_1 \\frac{I_{i,t-1}}{K_{i,t-2}} + \\gamma_2 \\left(\\frac{Y_{i,t}}{K_{i,t-1}}\\right) + \\gamma_3 \\frac{CF_{i,t}}{K_{i,t-1}} + \\varepsilon_{i,t}\n\\tag{42.10}\\]\nThis specification avoids the need for \\(Q\\) entirely, sidestepping the measurement error problem.\nExternal finance dependence. Rajan and Zingales (1998) propose using the industry-level technological demand for external finance as an instrument for financing constraints. Industries that technologically require more external funding should be disproportionately affected by financial development and firm-level constraints.\n\n# Euler equation investment model (dynamic panel)\npanel_euler = panel_fc.copy().sort_values([\"ticker\", \"year\"])\n\npanel_euler[\"lag_inv_rate\"] = panel_euler.groupby(\"ticker\")[\"inv_rate\"].shift(1)\npanel_euler[\"revenue_assets\"] = panel_euler[\"revenue\"] / panel_euler[\"lag_assets\"]\n\neuler_data = panel_euler.dropna(\n    subset=[\"inv_rate\", \"lag_inv_rate\", \"revenue_assets\", \"cf_assets\"]\n).set_index([\"ticker\", \"year\"])\n\nmodel_euler = PanelOLS(\n    euler_data[\"inv_rate\"],\n    euler_data[[\"lag_inv_rate\", \"revenue_assets\", \"cf_assets\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\neuler_data = euler_data.reset_index()\n\n\n\n\nTable 42.6: Euler Equation Investment Model\n\n\neuler_results = pd.DataFrame({\n    \"Coefficient\": model_euler.params.round(4),\n    \"Std Error\": model_euler.std_errors.round(4),\n    \"t-stat\": model_euler.tstats.round(3),\n    \"p-value\": model_euler.pvalues.round(4)\n})\neuler_results",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Corporate Finance Estimators and Identification</span>"
    ]
  },
  {
    "objectID": "62_corporate_finance_estimators.html#financing-choice-models",
    "href": "62_corporate_finance_estimators.html#financing-choice-models",
    "title": "42  Corporate Finance Estimators and Identification",
    "section": "42.3 Financing Choice Models",
    "text": "42.3 Financing Choice Models\n\n42.3.1 Capital Structure Determinants\nThe two dominant theories of capital structure (i.e., trade-off theory and pecking order theory) generate distinct predictions about the determinants of leverage. Frank and Goyal (2009) provide the most comprehensive empirical synthesis, identifying six “core” variables that reliably predict leverage across samples and specifications.\nThe baseline capital structure regression is:\n\\[\n\\text{Lev}_{i,t} = \\beta_0 + \\boldsymbol{\\beta}' \\mathbf{X}_{i,t} + \\alpha_i + \\delta_t + \\varepsilon_{i,t}\n\\tag{42.11}\\]\nwhere \\(\\text{Lev}_{i,t}\\) is either book leverage (\\(D / A\\)) or market leverage (\\(D / (D + E^{\\text{mkt}})\\)), and \\(\\mathbf{X}_{i,t}\\) includes the core determinants.\nTable 42.7 summarizes the theoretical predictions.\n\n\n\nTable 42.7: Capital Structure Predictions by Theory\n\n\n\n\n\n\n\n\n\n\n\nDeterminant\nTrade-Off\nPecking Order\nMeasurement\n\n\n\n\nProfitability\n+ (tax shield)\n− (less need for external)\nEBITDA / Assets\n\n\nSize\n+ (lower distress costs)\n+ (less information asymmetry)\nln(Total Assets)\n\n\nTangibility\n+ (collateral value)\n+ (less adverse selection)\nPPE / Assets\n\n\nGrowth (MTB)\n− (underinvestment)\n+ (financing needs)\nMarket-to-Book\n\n\nIndustry median leverage\n+ (target)\nambiguous\nIndustry median\n\n\nProfitability volatility\n− (distress risk)\nambiguous\nRolling σ(EBITDA/A)\n\n\n\n\n\n\n\n# Construct capital structure variables\ncs = panel_fc.copy()\n\n# Book leverage\ncs[\"book_leverage\"] = cs[\"total_debt\"] / cs[\"total_assets\"]\n\n# Market leverage\ncs[\"market_leverage\"] = cs[\"total_debt\"] / (\n    cs[\"total_debt\"] + cs[\"market_cap\"]\n)\n\n# Profitability\ncs[\"profitability\"] = cs[\"ebitda\"] / cs[\"total_assets\"]\n\n# Tangibility\ncs[\"tangibility\"] = cs[\"ppe_net\"] / cs[\"total_assets\"]\n\n# Size\ncs[\"size\"] = np.log(cs[\"total_assets\"])\n\n# Market-to-Book\ncs[\"mtb\"] = cs[\"market_cap\"] / cs[\"book_equity\"]\n\n# Industry median leverage\ncs[\"ind_median_lev\"] = cs.groupby(\n    [\"industry\", \"year\"]\n)[\"book_leverage\"].transform(\"median\")\n\n# Rolling profitability volatility (3-year)\ncs = cs.sort_values([\"ticker\", \"year\"])\ncs[\"profit_vol\"] = (\n    cs.groupby(\"ticker\")[\"profitability\"]\n    .transform(lambda x: x.rolling(3, min_periods=2).std())\n)\n\n# Winsorize\nfor col in [\"book_leverage\", \"market_leverage\", \"profitability\",\n            \"tangibility\", \"mtb\", \"profit_vol\"]:\n    cs[col] = winsorize(cs[col])\n\ncs_clean = cs.dropna(\n    subset=[\"book_leverage\", \"profitability\", \"size\",\n            \"tangibility\", \"mtb\", \"ind_median_lev\"]\n)\n\n\n# Capital structure regressions\ncs_panel = cs_clean.set_index([\"ticker\", \"year\"])\n\nregressors = [\"profitability\", \"size\", \"tangibility\",\n              \"mtb\", \"ind_median_lev\"]\n\n# Book leverage\nmodel_book = PanelOLS(\n    cs_panel[\"book_leverage\"],\n    cs_panel[regressors],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\n# Market leverage\nmodel_mkt = PanelOLS(\n    cs_panel[\"market_leverage\"],\n    cs_panel[regressors],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\ncs_panel = cs_panel.reset_index()\n\n\n\n\nTable 42.8: Capital Structure Determinants: Book and Market Leverage\n\n\ncs_table = pd.DataFrame({\n    \"Book Leverage\": [\n        f\"{model_book.params[v]:.4f} ({model_book.std_errors[v]:.4f})\"\n        for v in regressors\n    ] + [f\"{model_book.rsquared_within:.4f}\", str(int(model_book.nobs))],\n    \"Market Leverage\": [\n        f\"{model_mkt.params[v]:.4f} ({model_mkt.std_errors[v]:.4f})\"\n        for v in regressors\n    ] + [f\"{model_mkt.rsquared_within:.4f}\", str(int(model_mkt.nobs))]\n}, index=regressors + [\"R² (within)\", \"N\"])\n\ncs_table\n\n\n\n\n\n42.3.2 Pecking Order Tests\nThe pecking order theory (Myers 1984) predicts that firms prefer internal finance, then debt, then equity. Shyam-Sunder and Myers (1999) propose a direct test: if the pecking order holds strictly, the financing deficit (investment minus internal funds) should be financed dollar-for-dollar by debt:\n\\[\n\\Delta D_{i,t} = \\alpha + \\beta_{\\text{PO}} \\cdot \\text{DEF}_{i,t} + \\varepsilon_{i,t}\n\\tag{42.12}\\]\nwhere \\(\\text{DEF}_{i,t} = \\text{Div}_{i,t} + \\text{Capex}_{i,t} + \\Delta W_{i,t} - CF_{i,t}\\) is the financing deficit and \\(\\Delta D_{i,t}\\) is net debt issuance. A strict pecking order implies \\(\\hat{\\alpha} = 0\\) and \\(\\hat{\\beta}_{\\text{PO}} = 1\\). Frank and Goyal (2003) show that the coefficient is typically well below 1, especially for large firms and equity issuers.\n\n# Construct financing deficit\npo = cs.copy().sort_values([\"ticker\", \"year\"])\npo[\"lag_debt\"] = po.groupby(\"ticker\")[\"total_debt\"].shift(1)\npo[\"net_debt_issuance\"] = po[\"total_debt\"] - po[\"lag_debt\"]\n\n# Financing deficit = Div + Capex + ΔWC - CF\npo[\"delta_wc\"] = po[\"working_capital\"] - po.groupby(\n    \"ticker\"\n)[\"working_capital\"].shift(1)\n\npo[\"fin_deficit\"] = (\n    po[\"dividends\"] + po[\"capex\"]\n    + po[\"delta_wc\"].fillna(0) - po[\"operating_cf\"]\n)\n\n# Scale by lagged assets\nfor col in [\"net_debt_issuance\", \"fin_deficit\"]:\n    po[col] = po[col] / po[\"lag_assets\"]\n\npo_clean = po.dropna(\n    subset=[\"net_debt_issuance\", \"fin_deficit\"]\n)\n\n# Winsorize\nfor col in [\"net_debt_issuance\", \"fin_deficit\"]:\n    po_clean[col] = winsorize(po_clean[col])\n\n# Pecking order regression\npo_panel = po_clean.set_index([\"ticker\", \"year\"])\n\nmodel_po = PanelOLS(\n    po_panel[\"net_debt_issuance\"],\n    po_panel[[\"fin_deficit\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\npo_panel = po_panel.reset_index()\n\nprint(f\"Pecking order coefficient: {model_po.params['fin_deficit']:.4f}\")\nprint(f\"  (se = {model_po.std_errors['fin_deficit']:.4f})\")\nprint(f\"  H0: β = 1, t = \"\n      f\"{(model_po.params['fin_deficit'] - 1) / model_po.std_errors['fin_deficit']:.3f}\")\n\n\n\n42.3.3 Market Timing Measures\nBaker and Wurgler (2002) argue that capital structure is largely the cumulative outcome of market timing (i.e., firms issue equity when valuations are high and repurchase when valuations are low). Their key variable is the external-finance-weighted average market-to-book ratio:\n\\[\n\\left(\\frac{M}{B}\\right)_{i,t}^{efwa} = \\sum_{s=\\text{IPO}}^{t-1} \\frac{e_s + d_s}{\\sum_{r=\\text{IPO}}^{t-1}(e_r + d_r)} \\cdot \\left(\\frac{M}{B}\\right)_{i,s}\n\\tag{42.13}\\]\nwhere \\(e_s\\) and \\(d_s\\) are net equity and net debt issuance in year \\(s\\). This variable captures the historical valuations at which the firm raised capital. The market timing hypothesis predicts that higher \\(\\left(M/B\\right)^{efwa}\\) is associated with lower current leverage (i.e., firms that historically issued equity at high valuations have persistently lower leverage).\n\n# External-Finance-Weighted Average M/B\ndef compute_efwa_mtb(group):\n    \"\"\"Compute Baker-Wurgler EFWA M/B for one firm.\"\"\"\n    g = group.sort_values(\"year\").copy()\n\n    # Net issuance each year\n    g[\"net_equity\"] = g[\"equity_issuance\"].fillna(0)\n    g[\"net_debt\"] = g[\"net_debt_issuance\"].fillna(0)\n    g[\"total_issuance\"] = (\n        g[\"net_equity\"].abs() + g[\"net_debt\"].abs()\n    ).replace(0, np.nan)\n\n    efwa_values = []\n    for idx in range(1, len(g)):\n        past = g.iloc[:idx]\n        weights = past[\"total_issuance\"] / past[\"total_issuance\"].sum()\n        weights = weights.fillna(0)\n        efwa = (weights * past[\"mtb\"]).sum()\n        efwa_values.append(efwa)\n\n    g = g.iloc[1:].copy()\n    g[\"efwa_mtb\"] = efwa_values\n    return g[[\"ticker\", \"year\", \"efwa_mtb\"]]\n\nmt = po_clean.copy()\nmt[\"equity_issuance\"] = mt[\"market_cap\"] - mt.groupby(\n    \"ticker\"\n)[\"market_cap\"].shift(1) - mt[\"net_income\"]\n\nefwa_data = (\n    mt.groupby(\"ticker\", group_keys=False)\n    .apply(compute_efwa_mtb)\n    .reset_index(drop=True)\n)\n\n# Merge and regress\nmt_merged = cs_clean.merge(efwa_data, on=[\"ticker\", \"year\"], how=\"left\")\nmt_clean = mt_merged.dropna(\n    subset=[\"market_leverage\", \"efwa_mtb\", \"profitability\",\n            \"size\", \"tangibility\", \"mtb\"]\n)\n\nmt_panel = mt_clean.set_index([\"ticker\", \"year\"])\n\nmodel_mt = PanelOLS(\n    mt_panel[\"market_leverage\"],\n    mt_panel[[\"efwa_mtb\", \"mtb\", \"profitability\", \"size\", \"tangibility\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\nmt_panel = mt_panel.reset_index()\n\n\n\n\nTable 42.9: Market Timing and Capital Structure\n\n\nmt_results = pd.DataFrame({\n    \"Coefficient\": model_mt.params.round(4),\n    \"Std Error\": model_mt.std_errors.round(4),\n    \"t-stat\": model_mt.tstats.round(3),\n    \"p-value\": model_mt.pvalues.round(4)\n})\nmt_results\n\n\n\nA negative coefficient on \\(\\text{EFWA}_{M/B}\\) after controlling for the current \\(M/B\\) (which captures current investment opportunities) supports the market timing hypothesis: firms that historically raised capital at high valuations maintain persistently lower leverage.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Corporate Finance Estimators and Identification</span>"
    ]
  },
  {
    "objectID": "62_corporate_finance_estimators.html#payout-policy-estimators",
    "href": "62_corporate_finance_estimators.html#payout-policy-estimators",
    "title": "42  Corporate Finance Estimators and Identification",
    "section": "42.4 Payout Policy Estimators",
    "text": "42.4 Payout Policy Estimators\n\n42.4.1 Dividend Smoothing\nLintner (1956) established the foundational model of dividend behavior: firms target a payout ratio and partially adjust dividends toward the target each year. The partial adjustment model is:\n\\[\n\\Delta D_{i,t} = \\alpha_i + \\lambda(\\tau \\cdot E_{i,t} - D_{i,t-1}) + \\varepsilon_{i,t}\n\\tag{42.14}\\]\nwhere \\(D_{i,t}\\) is the dividend per share, \\(E_{i,t}\\) is earnings per share, \\(\\tau\\) is the target payout ratio, and \\(\\lambda \\in (0, 1)\\) is the speed of adjustment. Low \\(\\lambda\\) implies strong smoothing (i.e., firms adjust dividends slowly toward the target). Rearranging:\n\\[\nD_{i,t} = \\alpha_i + (1 - \\lambda) D_{i,t-1} + \\lambda \\tau \\cdot E_{i,t} + \\varepsilon_{i,t}\n\\tag{42.15}\\]\nThe coefficient on lagged dividends, \\((1 - \\lambda)\\), measures the degree of smoothing. Values close to 1 indicate near-complete smoothing; values close to 0 indicate no smoothing (full adjustment).\n\n# Construct dividend and earnings variables\ndiv = panel_fc.copy().sort_values([\"ticker\", \"year\"])\n\ndiv[\"lag_dps\"] = div.groupby(\"ticker\")[\"dividends_per_share\"].shift(1)\ndiv[\"delta_dps\"] = div[\"dividends_per_share\"] - div[\"lag_dps\"]\n\n# Only firms with positive dividends in both periods\ndiv_clean = div.dropna(\n    subset=[\"dividends_per_share\", \"lag_dps\", \"eps\"]\n).query(\"lag_dps &gt; 0 and dividends_per_share &gt; 0\")\n\n# Lintner regression\ndiv_panel = div_clean.set_index([\"ticker\", \"year\"])\n\nmodel_lintner = PanelOLS(\n    div_panel[\"dividends_per_share\"],\n    div_panel[[\"lag_dps\", \"eps\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\ndiv_panel = div_panel.reset_index()\n\n# Extract structural parameters\nlambda_hat = 1 - model_lintner.params[\"lag_dps\"]\ntau_hat = model_lintner.params[\"eps\"] / lambda_hat\n\nprint(f\"Lintner Model Estimates:\")\nprint(f\"  Speed of adjustment (λ): {lambda_hat:.4f}\")\nprint(f\"  Target payout ratio (τ): {tau_hat:.4f}\")\nprint(f\"  Smoothing coefficient (1-λ): {model_lintner.params['lag_dps']:.4f}\")\n\n\n\n\nTable 42.10: Lintner Partial Adjustment Model\n\n\nlintner_table = pd.DataFrame({\n    \"Coefficient\": model_lintner.params.round(4),\n    \"Std Error\": model_lintner.std_errors.round(4),\n    \"t-stat\": model_lintner.tstats.round(3),\n    \"p-value\": model_lintner.pvalues.round(4)\n})\nlintner_table\n\n\n\n\n\n\npayout = panel_fc.copy()\npayout[\"payout_ratio\"] = payout[\"dividends\"] / payout[\"net_income\"]\npayout = payout[\n    (payout[\"net_income\"] &gt; 0) &\n    (payout[\"payout_ratio\"].between(0, 2))\n]\n\npayout_ts = (\n    payout.groupby(\"year\")\n    .agg(\n        median_payout=(\"payout_ratio\", \"median\"),\n        mean_payout=(\"payout_ratio\", \"mean\"),\n        q25=(\"payout_ratio\", lambda x: x.quantile(0.25)),\n        q75=(\"payout_ratio\", lambda x: x.quantile(0.75)),\n        pct_payers=(\"payout_ratio\", lambda x: (x &gt; 0).mean())\n    )\n    .reset_index()\n)\n\n(\n    p9.ggplot(payout_ts, p9.aes(x=\"year\"))\n    + p9.geom_ribbon(\n        p9.aes(ymin=\"q25\", ymax=\"q75\"),\n        fill=\"#2E5090\", alpha=0.2\n    )\n    + p9.geom_line(\n        p9.aes(y=\"median_payout\"),\n        color=\"#2E5090\", size=1\n    )\n    + p9.geom_line(\n        p9.aes(y=\"mean_payout\"),\n        color=\"#C0392B\", linetype=\"dashed\", size=0.7\n    )\n    + p9.labs(\n        x=\"Year\",\n        y=\"Dividend Payout Ratio\",\n        title=\"Payout Ratio: Median (Solid) and Mean (Dashed)\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n\n\nFigure 42.2\n\n\n\n\n\n42.4.2 Smoothing Heterogeneity: SOEs vs. Private Firms\nDividend policy in Vietnam is shaped by regulatory mandates. The State Capital Investment Corporation (SCIC) and line ministries have historically required SOEs to distribute minimum dividend amounts, sometimes at the expense of reinvestment. This creates a fundamental asymmetry: SOE dividends are partially policy-determined rather than the outcome of the Lintner optimization.\n\n# Merge SOE indicator\ndiv_with_soe = div_clean.merge(\n    ownership[[\"ticker\", \"year\", \"state_ownership_pct\"]],\n    on=[\"ticker\", \"year\"],\n    how=\"left\"\n)\ndiv_with_soe[\"soe\"] = (div_with_soe[\"state_ownership_pct\"] &gt; 50).astype(int)\n\n# Estimate Lintner model separately for SOEs and private firms\nlintner_results = {}\nfor label, soe_val in [(\"Private\", 0), (\"SOE\", 1)]:\n    subset = div_with_soe[div_with_soe[\"soe\"] == soe_val].copy()\n    if len(subset) &lt; 100:\n        continue\n\n    subset_panel = subset.set_index([\"ticker\", \"year\"])\n    model = PanelOLS(\n        subset_panel[\"dividends_per_share\"],\n        subset_panel[[\"lag_dps\", \"eps\"]],\n        entity_effects=True,\n        time_effects=True,\n        check_rank=False\n    ).fit(cov_type=\"clustered\", cluster_entity=True)\n\n    lam = 1 - model.params[\"lag_dps\"]\n    tau = model.params[\"eps\"] / lam if abs(lam) &gt; 0.01 else np.nan\n\n    lintner_results[label] = {\n        \"Smoothing (1-λ)\": round(model.params[\"lag_dps\"], 4),\n        \"Speed of adj (λ)\": round(lam, 4),\n        \"Target payout (τ)\": round(tau, 4),\n        \"N\": int(model.nobs)\n    }\n\npd.DataFrame(lintner_results).T\n\n\n\n42.4.3 Share Repurchases\nShare repurchases are a relatively new phenomenon in Vietnamese markets, gradually gaining traction as regulations have evolved. Unlike dividends, repurchases are more flexible and do not create expectations of future payments. The decision to repurchase can be modeled as:\n\\[\n\\text{Repurchase}_{i,t} = \\mathbb{1}\\left(\\beta_0 + \\beta_1 \\frac{CF_{i,t}}{A_{i,t}} + \\beta_2 Q_{i,t} + \\beta_3 \\text{Lev}_{i,t} + \\beta_4 \\frac{\\text{Cash}_{i,t}}{A_{i,t}} + \\boldsymbol{\\gamma}' \\mathbf{Z}_{i,t} + \\varepsilon_{i,t} &gt; 0\\right)\n\\tag{42.16}\\]\n\n# Identify repurchase years\nrepurchase = panel_fc.copy()\nrepurchase[\"repurchase_dummy\"] = (\n    repurchase[\"share_repurchases\"] &gt; 0\n).astype(int)\n\nrepurchase[\"cash_assets\"] = repurchase[\"cash\"] / repurchase[\"total_assets\"]\n\n# Probit model for repurchase decision\nrep_clean = repurchase.dropna(\n    subset=[\"repurchase_dummy\", \"cf_assets\", \"tobins_q\",\n            \"book_leverage\", \"cash_assets\", \"log_assets\"]\n)\n\nprobit_model = smf.probit(\n    \"repurchase_dummy ~ cf_assets + tobins_q + book_leverage \"\n    \"+ cash_assets + log_assets + C(year)\",\n    data=rep_clean\n).fit(disp=False, cov_type=\"cluster\", cov_kwds={\"groups\": rep_clean[\"ticker\"]})\n\n\n\n\nTable 42.11: Probit Model: Determinants of Share Repurchase Decision\n\n\n# Extract non-year-dummy coefficients\nmain_vars = [\"cf_assets\", \"tobins_q\", \"book_leverage\",\n             \"cash_assets\", \"log_assets\"]\n\nprobit_results = pd.DataFrame({\n    \"Coefficient\": probit_model.params[main_vars].round(4),\n    \"Std Error\": probit_model.bse[main_vars].round(4),\n    \"z-stat\": probit_model.tvalues[main_vars].round(3),\n    \"p-value\": probit_model.pvalues[main_vars].round(4),\n    \"Marginal Effect\": (\n        probit_model.get_margeff().margeff[:len(main_vars)]\n    ).round(4)\n})\nprobit_results\n\n\n\n\n\n42.4.4 Agency and Signaling Interpretations\nPayout policy is interpreted through two competing lenses:\nAgency view (Jensen 1986; La Porta et al. 2000): Dividends are a mechanism for disgorging free cash flow that managers would otherwise waste on empire-building or perquisite consumption. In this view, firms with weaker governance should face greater pressure to pay dividends as a bonding device. La Porta et al. (2000) distinguish the “outcome” model (dividends are the result of effective minority shareholder pressure) from the “substitute” model (firms with weak governance pay high dividends to build reputation for fair treatment).\nSignaling view (Bhattacharya 1979; Miller and Rock 1985): Dividends convey private information about future earnings. Because dividends are costly to fake (they require actual cash), they serve as a credible signal. The signaling interpretation predicts that dividend changes should predict future earnings changes.\n\n# Test dividend signaling: do dividend changes predict future earnings?\nsignal = panel_fc.copy().sort_values([\"ticker\", \"year\"])\n\nsignal[\"delta_div\"] = signal.groupby(\"ticker\")[\"dividends\"].diff()\nsignal[\"div_increase\"] = (signal[\"delta_div\"] &gt; 0).astype(int)\nsignal[\"div_decrease\"] = (signal[\"delta_div\"] &lt; 0).astype(int)\n\n# Future earnings change\nsignal[\"lead_earnings\"] = signal.groupby(\"ticker\")[\"net_income\"].shift(-1)\nsignal[\"delta_earnings_lead\"] = (\n    (signal[\"lead_earnings\"] - signal[\"net_income\"]) /\n    signal[\"total_assets\"]\n)\n\n# Current earnings change (control)\nsignal[\"lag_earnings\"] = signal.groupby(\"ticker\")[\"net_income\"].shift(1)\nsignal[\"delta_earnings_curr\"] = (\n    (signal[\"net_income\"] - signal[\"lag_earnings\"]) /\n    signal[\"total_assets\"]\n)\n\nsignal_clean = signal.dropna(\n    subset=[\"delta_earnings_lead\", \"div_increase\",\n            \"div_decrease\", \"delta_earnings_curr\"]\n)\n\n# Regression: future earnings change on dividend change indicators\nsignal_model = smf.ols(\n    \"delta_earnings_lead ~ div_increase + div_decrease \"\n    \"+ delta_earnings_curr + C(year) + C(industry)\",\n    data=signal_clean\n).fit(cov_type=\"cluster\", cov_kwds={\"groups\": signal_clean[\"ticker\"]})\n\nprint(\"Dividend Signaling Test:\")\nfor var in [\"div_increase\", \"div_decrease\", \"delta_earnings_curr\"]:\n    print(f\"  {var}: {signal_model.params[var]:.4f} \"\n          f\"(t = {signal_model.tvalues[var]:.3f})\")",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Corporate Finance Estimators and Identification</span>"
    ]
  },
  {
    "objectID": "62_corporate_finance_estimators.html#agency-cost-proxies",
    "href": "62_corporate_finance_estimators.html#agency-cost-proxies",
    "title": "42  Corporate Finance Estimators and Identification",
    "section": "42.5 Agency Cost Proxies",
    "text": "42.5 Agency Cost Proxies\n\n42.5.1 Ownership Concentration and Agency Problems\nThe agency framework of Jensen and Meckling (2019) identifies the separation of ownership and control as the fundamental source of corporate agency costs. In concentrated-ownership economies like Vietnam, the dominant agency conflict is not between dispersed shareholders and professional managers (Berle-Means agency problem) but between controlling and minority shareholders (principal-principal agency problem, Young et al. (2008)).\nThe key mechanisms through which controlling shareholders extract private benefits include: tunneling via related-party transactions (Johnson et al. 2000), diversion of corporate opportunities, excessive compensation, and dilutive equity issuances. The extent of these costs depends on the ownership structure, legal protections for minorities, and monitoring intensity.\n\n# Merge ownership data comprehensively\nagency = panel_fc.merge(\n    ownership[[\"ticker\", \"year\", \"state_ownership_pct\",\n               \"foreign_ownership_pct\", \"insider_ownership_pct\",\n               \"largest_shareholder_pct\", \"top5_shareholder_pct\",\n               \"board_size\", \"independent_directors_pct\",\n               \"ceo_duality\"]],\n    on=[\"ticker\", \"year\"],\n    how=\"left\"\n)\n\n# Ownership concentration measures\n# Herfindahl of top-5 shareholdings\nagency[\"ownership_hhi\"] = agency[\"top5_shareholder_pct\"]**2\n\n# Excess control rights (proxy: difference between\n# largest shareholder and second largest)\nagency[\"control_wedge\"] = (\n    agency[\"largest_shareholder_pct\"] -\n    (agency[\"top5_shareholder_pct\"] - agency[\"largest_shareholder_pct\"]) / 4\n)\n\n\n\n42.5.2 Free Cash Flow Measures\nJensen (1986) argues that the agency cost of free cash flow is the central problem in firms that generate cash in excess of positive-NPV investment opportunities. The standard measure is:\n\\[\n\\text{FCF}_{i,t} = \\frac{\\text{Operating CF}_{i,t} - \\text{Depreciation}_{i,t} - \\text{Required Capex}_{i,t}}{\\text{Total Assets}_{i,t}}\n\\tag{42.17}\\]\nIn practice, “required capex” is unobservable, so researchers use operating cash flow minus capital expenditures as a proxy, or add the interaction of cash flow with low \\(Q\\) (which identifies firms with cash flow but without investment opportunities):\n\\[\n\\text{FCF Overinvestment} = \\frac{CF_{i,t}}{A_{i,t}} \\times \\mathbb{1}(Q_{i,t} &lt; 1)\n\\tag{42.18}\\]\n\n# Free cash flow measures\nagency[\"fcf\"] = (agency[\"operating_cf\"] - agency[\"capex\"]) / agency[\"total_assets\"]\n\nagency[\"low_q\"] = (agency[\"tobins_q\"] &lt; 1).astype(int)\nagency[\"fcf_low_q\"] = agency[\"fcf\"] * agency[\"low_q\"]\n\n# Asset utilization (inverse proxy for agency costs)\nagency[\"asset_turnover\"] = agency[\"revenue\"] / agency[\"total_assets\"]\n\n# SGA ratio (proxy for discretionary spending / empire building)\nagency[\"sga_ratio\"] = agency[\"sga_expenses\"] / agency[\"revenue\"]\n\n\n\n42.5.3 Monitoring Mechanisms and Governance Variables\nWe construct a governance quality composite based on observable monitoring mechanisms:\n\n# Governance quality indicators\nagency[\"foreign_monitor\"] = (\n    agency[\"foreign_ownership_pct\"] &gt; 20\n).astype(int)\n\nagency[\"board_independence\"] = agency[\"independent_directors_pct\"]\n\nagency[\"no_duality\"] = (1 - agency[\"ceo_duality\"]).astype(int)\n\n# Related-party transaction intensity (if available)\n# agency[\"rpt_ratio\"] = agency[\"related_party_transactions\"] / agency[\"revenue\"]\n\n\n\n\nTable 42.12: Summary Statistics: Agency Cost Proxies and Governance Variables\n\n\nagency_vars = [\n    \"largest_shareholder_pct\", \"state_ownership_pct\",\n    \"foreign_ownership_pct\", \"fcf\", \"fcf_low_q\",\n    \"asset_turnover\", \"board_independence\"\n]\n\nagency_summary = (\n    agency[agency_vars].dropna()\n    .describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])\n    .T.round(4)\n)\nagency_summary\n\n\n\n\n\n42.5.4 Agency Costs and Firm Value\nWe test whether agency cost proxies are associated with firm value (Tobin’s \\(Q\\)) and operating performance (ROA), controlling for standard determinants:\n\\[\nQ_{i,t} = \\beta_0 + \\beta_1 \\text{Own}_{i,t} + \\beta_2 \\text{Own}_{i,t}^2 + \\boldsymbol{\\gamma}'\\mathbf{X}_{i,t} + \\alpha_i + \\delta_t + \\varepsilon_{i,t}\n\\tag{42.19}\\]\nThe quadratic in ownership captures the Morck, Shleifer, and Vishny (1988) nonlinearity: at low levels, managerial ownership aligns incentives (positive effect on \\(Q\\)); at high levels, entrenchment dominates (negative effect).\n\n# Agency cost and valuation regression\nagency[\"largest_sq\"] = agency[\"largest_shareholder_pct\"]**2\n\nval_data = agency.dropna(\n    subset=[\"tobins_q\", \"largest_shareholder_pct\", \"foreign_ownership_pct\",\n            \"fcf\", \"size\", \"profitability\", \"leverage\"]\n).copy()\n\nval_panel = val_data.set_index([\"ticker\", \"year\"])\n\nmodel_val = PanelOLS(\n    val_panel[\"tobins_q\"],\n    val_panel[[\"largest_shareholder_pct\", \"largest_sq\",\n               \"foreign_ownership_pct\", \"fcf\",\n               \"size\", \"profitability\", \"leverage\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\nval_panel = val_panel.reset_index()\n\n\n\n\nTable 42.13: Agency Proxies and Firm Value (Tobin’s Q)\n\n\nval_results = pd.DataFrame({\n    \"Coefficient\": model_val.params.round(4),\n    \"Std Error\": model_val.std_errors.round(4),\n    \"t-stat\": model_val.tstats.round(3),\n    \"p-value\": model_val.pvalues.round(4)\n})\nval_results\n\n\n\n\n\n\n# Binned scatter: largest shareholder vs Q\nown_bins = val_data.copy()\nown_bins[\"own_bin\"] = pd.qcut(\n    own_bins[\"largest_shareholder_pct\"], q=20, duplicates=\"drop\"\n)\n\nown_binned = (\n    own_bins.groupby(\"own_bin\", observed=True)\n    .agg(\n        mean_own=(\"largest_shareholder_pct\", \"mean\"),\n        mean_q=(\"tobins_q\", \"mean\"),\n        se_q=(\"tobins_q\", lambda x: x.std() / np.sqrt(len(x)))\n    )\n    .reset_index()\n)\n\n(\n    p9.ggplot(own_binned, p9.aes(x=\"mean_own\", y=\"mean_q\"))\n    + p9.geom_pointrange(\n        p9.aes(ymin=\"mean_q - 1.96*se_q\",\n               ymax=\"mean_q + 1.96*se_q\"),\n        color=\"#2E5090\", size=0.5\n    )\n    + p9.geom_smooth(method=\"loess\", color=\"#C0392B\", se=False, size=0.8)\n    + p9.labs(\n        x=\"Largest Shareholder Ownership (%)\",\n        y=\"Tobin's Q\",\n        title=\"Ownership Concentration and Firm Value\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 6))\n)\n\n\nFigure 42.3\n\n\n\nThe inverted-U pattern, if present, would be consistent with the Morck-Shleifer-Vishny incentive-alignment/entrenchment tradeoff. In Vietnamese markets, the pattern may differ because the dominant controlling shareholder is often the state, whose objective function includes non-value-maximizing goals (employment, regional development, strategic sector control).",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Corporate Finance Estimators and Identification</span>"
    ]
  },
  {
    "objectID": "62_corporate_finance_estimators.html#linking-corporate-decisions-to-returns",
    "href": "62_corporate_finance_estimators.html#linking-corporate-decisions-to-returns",
    "title": "42  Corporate Finance Estimators and Identification",
    "section": "42.6 Linking Corporate Decisions to Returns",
    "text": "42.6 Linking Corporate Decisions to Returns\n\n42.6.1 Investment-Based Anomalies\nThe asset pricing literature has documented that corporate investment decisions predict cross-sectional return differences (i.e., the “investment anomalies”). The theoretical foundation is the \\(q\\)-theory of investment applied to asset pricing (Cochrane 1991; Liu, Whited, and Zhang 2009): firms invest more when the discount rate on their projects is lower. High investment therefore signals low expected returns.\nThe investment effect. Titman, Wei, and Xie (2004) and Cooper, Gulen, and Schill (2008) document that firms with high asset growth earn lower subsequent returns. The asset growth variable is:\n\\[\n\\text{AG}_{i,t} = \\frac{A_{i,t} - A_{i,t-1}}{A_{i,t-1}}\n\\tag{42.20}\\]\nThe investment-to-assets effect. Fama and French (2006) and Hou, Xue, and Zhang (2015) show that capital expenditure scaled by assets negatively predicts returns.\nThe profitability effect. Novy-Marx (2013) shows that gross profitability (revenue minus COGS, scaled by assets) positively predicts returns. This is consistent with \\(q\\)-theory: controlling for investment, more profitable firms must have higher discount rates (otherwise they would invest more).\n\n# Construct anomaly variables\nanomaly = panel_fc.copy().sort_values([\"ticker\", \"year\"])\n\n# Asset growth\nanomaly[\"asset_growth\"] = (\n    (anomaly[\"total_assets\"] - anomaly[\"lag_assets\"]) /\n    anomaly[\"lag_assets\"]\n)\n\n# Investment-to-assets\nanomaly[\"inv_to_assets\"] = anomaly[\"capex\"] / anomaly[\"lag_assets\"]\n\n# Gross profitability\nanomaly[\"gross_profit\"] = (\n    (anomaly[\"revenue\"] - anomaly[\"cogs\"]) / anomaly[\"total_assets\"]\n)\n\n# ROE\nanomaly[\"roe\"] = anomaly[\"net_income\"] / anomaly[\"book_equity\"]\n\n# Winsorize\nfor col in [\"asset_growth\", \"inv_to_assets\", \"gross_profit\", \"roe\"]:\n    anomaly[col] = winsorize(anomaly[col])\n\n\n# Portfolio sorts: quintiles on asset growth\n# Merge with monthly returns (using June rebalancing)\nanomaly_june = anomaly.copy()\nanomaly_june[\"formation_year\"] = anomaly_june[\"year\"]\n\n# Create quintile assignments\nanomaly_june[\"ag_quintile\"] = anomaly_june.groupby(\"year\")[\n    \"asset_growth\"\n].transform(lambda x: pd.qcut(x, 5, labels=[1, 2, 3, 4, 5],\n                                duplicates=\"drop\"))\n\n# Merge with forward returns\nmonthly_with_signal = monthly_returns.copy()\nmonthly_with_signal[\"formation_year\"] = np.where(\n    monthly_with_signal[\"date\"].dt.month &gt;= 7,\n    monthly_with_signal[\"date\"].dt.year,\n    monthly_with_signal[\"date\"].dt.year - 1\n)\n\nportfolios = monthly_with_signal.merge(\n    anomaly_june[[\"ticker\", \"formation_year\", \"ag_quintile\",\n                   \"asset_growth\", \"gross_profit\"]],\n    on=[\"ticker\", \"formation_year\"],\n    how=\"inner\"\n)\n\n# Compute equal-weighted quintile returns\nag_returns = (\n    portfolios.groupby([\"date\", \"ag_quintile\"])\n    .agg(port_ret=(\"ret\", \"mean\"))\n    .reset_index()\n)\n\n# Long-short: Q1 (low growth) - Q5 (high growth)\nag_wide = ag_returns.pivot(\n    index=\"date\", columns=\"ag_quintile\", values=\"port_ret\"\n)\nag_wide[\"L-S\"] = ag_wide[1] - ag_wide[5]\n\n\n\n\nTable 42.14: Asset Growth Quintile Portfolio Returns\n\n\nquintile_summary = ag_wide.describe().T[[\"mean\", \"std\"]].copy()\nquintile_summary[\"mean_ann\"] = quintile_summary[\"mean\"] * 12\nquintile_summary[\"std_ann\"] = quintile_summary[\"std\"] * np.sqrt(12)\nquintile_summary[\"sharpe\"] = (\n    quintile_summary[\"mean_ann\"] / quintile_summary[\"std_ann\"]\n)\n\n# t-statistics\nfor col in ag_wide.columns:\n    t_stat = ag_wide[col].mean() / (ag_wide[col].std() / np.sqrt(len(ag_wide)))\n    quintile_summary.loc[col, \"t_stat\"] = t_stat\n\nquintile_summary = quintile_summary[\n    [\"mean_ann\", \"std_ann\", \"sharpe\", \"t_stat\"]\n].round(4)\n\nquintile_summary.columns = [\n    \"Ann. Return\", \"Ann. Vol\", \"Sharpe Ratio\", \"t-stat\"\n]\nquintile_summary\n\n\n\n\n\n\ncumret = ag_wide[[\"L-S\"]].copy()\ncumret.columns = [\"Long-Short\"]\ncumret = cumret.dropna()\ncumret[\"cumulative\"] = (1 + cumret[\"Long-Short\"]).cumprod()\ncumret = cumret.reset_index()\n\n(\n    p9.ggplot(cumret, p9.aes(x=\"date\", y=\"cumulative\"))\n    + p9.geom_line(color=\"#2E5090\", size=0.8)\n    + p9.geom_hline(yintercept=1, linetype=\"dashed\", color=\"gray\")\n    + p9.labs(\n        x=\"\",\n        y=\"Cumulative Return (Growth of $1)\",\n        title=\"Investment Anomaly: Low – High Asset Growth\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5))\n)\n\n\nFigure 42.4\n\n\n\n\n\n42.6.2 Financing Anomalies\nFirms’ financing decisions also predict returns. Pontiff and Woodgate (2008) document that net stock issuance negatively predicts returns: firms that issue equity earn lower future returns, while firms that repurchase shares earn higher returns. This is consistent with both managerial market timing and an issuance-based risk factor.\nThe net stock issuance variable is typically measured as:\n\\[\n\\text{NSI}_{i,t} = \\ln\\left(\\frac{\\text{Split-Adjusted Shares}_{i,t}}{\\text{Split-Adjusted Shares}_{i,t-1}}\\right)\n\\tag{42.21}\\]\nPositive NSI indicates net equity issuance; negative NSI indicates net repurchases.\n\n# Net Stock Issuance\nfin_anomaly = anomaly.copy()\nfin_anomaly[\"lag_shares\"] = fin_anomaly.groupby(\n    \"ticker\"\n)[\"shares_outstanding\"].shift(1)\n\nfin_anomaly[\"nsi\"] = np.log(\n    fin_anomaly[\"shares_outstanding\"] / fin_anomaly[\"lag_shares\"]\n)\nfin_anomaly[\"nsi\"] = winsorize(fin_anomaly[\"nsi\"])\n\n# Net debt issuance (change in total debt / assets)\nfin_anomaly[\"ndi\"] = (\n    (fin_anomaly[\"total_debt\"] -\n     fin_anomaly.groupby(\"ticker\")[\"total_debt\"].shift(1)) /\n    fin_anomaly[\"lag_assets\"]\n)\nfin_anomaly[\"ndi\"] = winsorize(fin_anomaly[\"ndi\"])\n\n# Portfolio sorts on NSI\nfin_anomaly[\"nsi_quintile\"] = fin_anomaly.groupby(\"year\")[\n    \"nsi\"\n].transform(lambda x: pd.qcut(x, 5, labels=[1, 2, 3, 4, 5],\n                                duplicates=\"drop\"))\n\nport_nsi = monthly_with_signal.merge(\n    fin_anomaly[[\"ticker\", \"formation_year\", \"nsi_quintile\"]],\n    on=[\"ticker\", \"formation_year\"],\n    how=\"inner\"\n)\n\nnsi_returns = (\n    port_nsi.groupby([\"date\", \"nsi_quintile\"])\n    .agg(port_ret=(\"ret\", \"mean\"))\n    .reset_index()\n)\n\nnsi_wide = nsi_returns.pivot(\n    index=\"date\", columns=\"nsi_quintile\", values=\"port_ret\"\n)\nnsi_wide[\"L-S\"] = nsi_wide[1] - nsi_wide[5]\n\n\n\n\nTable 42.15: Net Stock Issuance Quintile Portfolio Returns\n\n\nnsi_summary = nsi_wide.describe().T[[\"mean\", \"std\"]].copy()\nnsi_summary[\"mean_ann\"] = nsi_summary[\"mean\"] * 12\nnsi_summary[\"sharpe\"] = (\n    nsi_summary[\"mean_ann\"] /\n    (nsi_summary[\"std\"] * np.sqrt(12))\n)\n\nfor col in nsi_wide.columns:\n    t_stat = nsi_wide[col].mean() / (\n        nsi_wide[col].std() / np.sqrt(len(nsi_wide))\n    )\n    nsi_summary.loc[col, \"t_stat\"] = t_stat\n\nnsi_summary = nsi_summary[[\"mean_ann\", \"sharpe\", \"t_stat\"]].round(4)\nnsi_summary.columns = [\"Ann. Return\", \"Sharpe\", \"t-stat\"]\nnsi_summary\n\n\n\n\n\n42.6.3 Valuation Implications: Fama-French Factor Regressions\nWe evaluate whether the investment and financing anomalies represent compensation for systematic risk by regressing the long-short portfolios on standard factor models:\n\\[\nR_{p,t} - R_{f,t} = \\alpha + \\beta_{\\text{MKT}} \\text{MKT}_t + \\beta_{\\text{SMB}} \\text{SMB}_t + \\beta_{\\text{HML}} \\text{HML}_t + \\varepsilon_t\n\\tag{42.22}\\]\nSignificant positive \\(\\alpha\\) after controlling for known risk factors would indicate that the anomaly is not explained by size and value exposures.\n\n# Merge long-short returns with factor data\nfactor_data = factors.set_index(\"date\")\n\n# Asset growth anomaly alpha\nag_ls = ag_wide[[\"L-S\"]].dropna().rename(columns={\"L-S\": \"excess_ret\"})\nag_merged = ag_ls.join(factor_data[[\"mkt_rf\", \"smb\", \"hml\"]], how=\"inner\")\n\nmodel_ag_ff3 = sm.OLS(\n    ag_merged[\"excess_ret\"],\n    sm.add_constant(ag_merged[[\"mkt_rf\", \"smb\", \"hml\"]])\n).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n\n# NSI anomaly alpha\nnsi_ls = nsi_wide[[\"L-S\"]].dropna().rename(columns={\"L-S\": \"excess_ret\"})\nnsi_merged = nsi_ls.join(factor_data[[\"mkt_rf\", \"smb\", \"hml\"]], how=\"inner\")\n\nmodel_nsi_ff3 = sm.OLS(\n    nsi_merged[\"excess_ret\"],\n    sm.add_constant(nsi_merged[[\"mkt_rf\", \"smb\", \"hml\"]])\n).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 6})\n\n\n\n\nTable 42.16: Fama-French Three-Factor Alphas for Corporate Decision Anomalies\n\n\nalpha_table = pd.DataFrame({\n    \"Asset Growth L-S\": {\n        \"Alpha (monthly)\": f\"{model_ag_ff3.params['const']:.4f}\",\n        \"  t-stat\": f\"{model_ag_ff3.tvalues['const']:.3f}\",\n        \"MKT\": f\"{model_ag_ff3.params['mkt_rf']:.4f}\",\n        \"SMB\": f\"{model_ag_ff3.params['smb']:.4f}\",\n        \"HML\": f\"{model_ag_ff3.params['hml']:.4f}\",\n        \"R²\": f\"{model_ag_ff3.rsquared:.4f}\"\n    },\n    \"Net Issuance L-S\": {\n        \"Alpha (monthly)\": f\"{model_nsi_ff3.params['const']:.4f}\",\n        \"  t-stat\": f\"{model_nsi_ff3.tvalues['const']:.3f}\",\n        \"MKT\": f\"{model_nsi_ff3.params['mkt_rf']:.4f}\",\n        \"SMB\": f\"{model_nsi_ff3.params['smb']:.4f}\",\n        \"HML\": f\"{model_nsi_ff3.params['hml']:.4f}\",\n        \"R²\": f\"{model_nsi_ff3.rsquared:.4f}\"\n    }\n})\nalpha_table\n\n\n\n\n\n\n# Combine asset growth and NSI long-short for comparison\ncombined = pd.DataFrame({\n    \"Asset Growth\": ag_wide[\"L-S\"],\n    \"Net Issuance\": nsi_wide[\"L-S\"]\n}).dropna()\n\ncombined_cum = (1 + combined).cumprod().reset_index()\ncombined_long = combined_cum.melt(\n    id_vars=\"date\",\n    var_name=\"Anomaly\",\n    value_name=\"Cumulative Return\"\n)\n\n(\n    p9.ggplot(combined_long, p9.aes(\n        x=\"date\", y=\"Cumulative Return\", color=\"Anomaly\"\n    ))\n    + p9.geom_line(size=0.8)\n    + p9.geom_hline(yintercept=1, linetype=\"dashed\", color=\"gray\")\n    + p9.scale_color_manual(values=[\"#2E5090\", \"#C0392B\"])\n    + p9.labs(\n        x=\"\",\n        y=\"Cumulative Return (Growth of $1)\",\n        title=\"Investment vs. Financing Anomalies: Long-Short Portfolios\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5), legend_position=\"top\")\n)\n\n\nFigure 42.5",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Corporate Finance Estimators and Identification</span>"
    ]
  },
  {
    "objectID": "62_corporate_finance_estimators.html#summary",
    "href": "62_corporate_finance_estimators.html#summary",
    "title": "42  Corporate Finance Estimators and Identification",
    "section": "42.7 Summary",
    "text": "42.7 Summary\nThis chapter implemented the core econometric toolkit of empirical corporate finance for Vietnamese listed firms. The estimators span four interconnected domains: investment decisions (investment-\\(Q\\) regressions and their measurement-error-corrected variants), financing decisions (capital structure determinants, pecking order tests, and market timing measures), payout policy (Lintner smoothing, repurchase models, and dividend signaling tests), and agency costs (ownership-value relationships, free cash flow measures, and governance variables).\nSeveral findings deserve emphasis. The investment-\\(Q\\) relationship in Vietnam is attenuated relative to developed-market benchmarks, reflecting both the severity of measurement error in \\(Q\\) (thin trading, price limits, volatile inflation) and the prevalence of non-market-driven investment by SOEs. Cash flow remains a significant predictor of investment across constraint classifications, though the FHP-KZ debate about interpretation applies with full force. Capital structure is strongly predicted by profitability (negatively, consistent with the pecking order) and tangibility (positively, consistent with trade-off theory). Dividend smoothing is pronounced, but the smoothing parameter differs systematically between SOEs and private firms, reflecting the distinct institutional forces governing each group’s payout policy.\nThe chapter also linked corporate decisions to asset returns through portfolio sorts on asset growth and net stock issuance. Whether these anomalies survive risk adjustment and persist out of sample in Vietnamese markets is an important open question for future research.\n\n\n\n\n\n\nAbel, Andrew B, and Janice C Eberly. 1994. “A Unified Model of Investment Under Uncertainty.” American Economic Review 84 (5).\n\n\nBaker, Malcolm, Jeremy C Stein, and Jeffrey Wurgler. 2003. “When Does the Market Matter? Stock Prices and the Investment of Equity-Dependent Firms.” The Quarterly Journal of Economics 118 (3): 969–1005.\n\n\nBaker, Malcolm, and Jeffrey Wurgler. 2002. “Market Timing and Capital Structure.” The Journal of Finance 57 (1): 1–32.\n\n\nBhattacharya, Sudipto. 1979. “Imperfect Information, Dividend Policy, and\" the Bird in the Hand\" Fallacy.” The Bell Journal of Economics, 259–70.\n\n\nBond, Philip, Alex Edmans, and Itay Goldstein. 2012. “The Real Effects of Financial Markets.” Annu. Rev. Financ. Econ. 4 (1): 339–60.\n\n\nBond, Stephen, Julie Ann Elston, Jacques Mairesse, and Benoı̂t Mulkay. 2003. “Financial Factors and Investment in Belgium, France, Germany, and the United Kingdom: A Comparison Using Company Panel Data.” Review of Economics and Statistics 85 (1): 153–65.\n\n\nClaessens, Stijn, Simeon Djankov, Joseph PH Fan, and Larry HP Lang. 2002. “Disentangling the Incentive and Entrenchment Effects of Large Shareholdings.” The Journal of Finance 57 (6): 2741–71.\n\n\nCochrane, John H. 1991. “Production-Based Asset Pricing and the Link Between Stock Returns and Economic Fluctuations.” The Journal of Finance 46 (1): 209–37.\n\n\nCooper, Michael J, Huseyin Gulen, and Michael J Schill. 2008. “Asset Growth and the Cross-Section of Stock Returns.” The Journal of Finance 63 (4): 1609–51.\n\n\nErickson, Timothy, and Toni M Whited. 2012. “Treating Measurement Error in Tobin’s q.” The Review of Financial Studies 25 (4): 1286–1329.\n\n\nFama, Eugene F, and Kenneth R French. 2006. “Profitability, Investment and Average Returns.” Journal of Financial Economics 82 (3): 491–518.\n\n\nFarre-Mensa, Joan, and Alexander Ljungqvist. 2016. “Do Measures of Financial Constraints Measure Financial Constraints?” The Review of Financial Studies 29 (2): 271–308.\n\n\nFazzari, Steven, R Glenn Hubbard, and Bruce C Petersen. 1987. “Financing Constraints and Corporate Investment.” National Bureau of Economic Research Cambridge, Mass., USA.\n\n\nFrank, Murray Z, and Vidhan K Goyal. 2003. “Testing the Pecking Order Theory of Capital Structure.” Journal of Financial Economics 67 (2): 217–48.\n\n\n———. 2009. “Capital Structure Decisions: Which Factors Are Reliably Important?” Financial Management 38 (1): 1–37.\n\n\nHadlock, Charles J, and Joshua R Pierce. 2010. “New Evidence on Measuring Financial Constraints: Moving Beyond the KZ Index.” The Review of Financial Studies 23 (5): 1909–40.\n\n\nHayashi, Fumio. 1982. “Tobin’s Marginal q and Average q: A Neoclassical Interpretation.” Econometrica: Journal of the Econometric Society, 213–24.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2015. “Digesting Anomalies: An Investment Approach.” The Review of Financial Studies 28 (3): 650–705.\n\n\nJensen, Michael C. 1986. “Agency Costs of Free Cash Flow, Corporate Finance, and Takeovers.” The American Economic Review 76 (2): 323–29.\n\n\nJensen, Michael C, and William H Meckling. 2019. “Theory of the Firm: Managerial Behavior, Agency Costs and Ownership Structure.” In Corporate Governance, 77–132. Gower.\n\n\nJohnson, Simon, Rafael La Porta, Florencio Lopez-de-Silanes, and Andrei Shleifer. 2000. “Tunneling.” American Economic Review 90 (2): 22–27.\n\n\nKaplan, Steven N, and Luigi Zingales. 1997. “Do Investment-Cash Flow Sensitivities Provide Useful Measures of Financing Constraints?” The Quarterly Journal of Economics 112 (1): 169–215.\n\n\nLa Porta, Rafael, Florencio Lopez-de-Silanes, Andrei Shleifer, and Robert W Vishny. 2000. “Agency Problems and Dividend Policies Around the World.” The Journal of Finance 55 (1): 1–33.\n\n\nLamont, Owen, Christopher Polk, and Jesús Saaá-Requejo. 2001. “Financial Constraints and Stock Returns.” The Review of Financial Studies 14 (2): 529–54.\n\n\nLintner, John. 1956. “Distribution of Incomes of Corporations Among Dividends, Retained Earnings, and Taxes.” The American Economic Review 46 (2): 97–113.\n\n\nLiu, Laura Xiaolei, Toni M Whited, and Lu Zhang. 2009. “Investment-Based Expected Stock Returns.” Journal of Political Economy 117 (6): 1105–39.\n\n\nMiller, Merton H, and Kevin Rock. 1985. “Dividend Policy Under Asymmetric Information.” The Journal of Finance 40 (4): 1031–51.\n\n\nMorck, Randall, Andrei Shleifer, and Robert W Vishny. 1988. “Management Ownership and Market Valuation: An Empirical Analysis.” Journal of Financial Economics 20: 293–315.\n\n\nMyers, Stewart C. 1984. “The Capital Structure Puzzle.” Journal of Finance 39 (3): 575–92.\n\n\nNovy-Marx, Robert. 2013. “The Other Side of Value: The Gross Profitability Premium.” Journal of Financial Economics 108 (1): 1–28.\n\n\nPontiff, Jeffrey, and Artemiza Woodgate. 2008. “Share Issuance and Cross-Sectional Returns.” The Journal of Finance 63 (2): 921–45.\n\n\nRajan, Raghuram G, and Luigi Zingales. 1998. “Financial Dependence and Growth.” American Economic Review, 559–86.\n\n\nShyam-Sunder, Lakshmi, and Stewart C Myers. 1999. “Testing Static Tradeoff Against Pecking Order Models of Capital Structure.” Journal of Financial Economics 51 (2): 219–44.\n\n\nTitman, Sheridan, KC John Wei, and Feixue Xie. 2004. “Capital Investments and Stock Returns.” Journal of Financial and Quantitative Analysis 39 (4): 677–700.\n\n\nWhited, Toni M, and Guojun Wu. 2006. “Financial Constraints Risk.” The Review of Financial Studies 19 (2): 531–59.\n\n\nYoung, Michael N, Mike W Peng, David Ahlstrom, Garry D Bruton, and Yi Jiang. 2008. “Corporate Governance in Emerging Economies: A Review of the Principal–Principal Perspective.” Journal of Management Studies 45 (1): 196–220.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Corporate Finance Estimators and Identification</span>"
    ]
  },
  {
    "objectID": "63_structural_models_finance.html",
    "href": "63_structural_models_finance.html",
    "title": "43  Structural Models in Finance",
    "section": "",
    "text": "43.1 What Structural Estimation Means in Finance\nMost of empirical finance is reduced-form: regress returns on characteristics, estimate risk premia from factor portfolios, test whether a coefficient is significantly different from zero. Structural estimation takes a fundamentally different approach. It begins with an explicit economic model (i.e., specifying agents’ preferences, information sets, constraints, and optimization problems) and estimates the model’s primitive parameters directly from observed data. The payoff is the ability to perform counterfactual analysis: what would happen to prices if trading costs fell by half? How would IPO underpricing change if the allocation mechanism switched from book building to a uniform-price auction? What is the welfare cost of a particular market design choice? These questions are unanswerable within a reduced-form framework because they require knowledge of the data-generating process, not merely statistical associations within a single regime.\nThis chapter introduces the key structural estimation frameworks used in financial economics and implements them for Vietnamese equity markets. We cover four domains where structural models have proven most productive: investor demand estimation, trading cost and execution models, primary market auctions, and limit order book dynamics. Each domain involves distinct economic primitives such as preferences, beliefs, transaction costs, information asymmetries and each requires domain-specific identification strategies.\nVietnamese markets offer both challenges and opportunities for structural estimation. On the challenge side, data quality is uneven, market microstructure is evolving, and institutional features (price limits, foreign ownership caps, state ownership) introduce constraints that standard models do not accommodate. On the opportunity side, several features of Vietnamese markets create natural variation that aids identification: the coexistence of two exchanges (HOSE and HNX) with different trading rules, regulatory changes that shift trading costs and price limits, and the phased liberalization of foreign ownership caps that generates exogenous demand shocks.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Structural Models in Finance</span>"
    ]
  },
  {
    "objectID": "63_structural_models_finance.html#what-structural-estimation-means-in-finance",
    "href": "63_structural_models_finance.html#what-structural-estimation-means-in-finance",
    "title": "43  Structural Models in Finance",
    "section": "",
    "text": "43.1.1 Reduced Form Versus Structural\nConsider two researchers studying the effect of transaction costs on trading volume. The reduced-form researcher exploits a fee reduction event and estimates:\n\\[\n\\ln V_{i,t} = \\alpha + \\beta \\cdot \\text{Post}_t + \\gamma \\mathbf{X}_{i,t} + \\varepsilon_{i,t}\n\\tag{43.1}\\]\nThe coefficient \\(\\beta\\) identifies the causal effect of the fee change on volume, but it is local to this specific fee change, this specific market, and this specific time period. It says nothing about what a different fee change would do, or what the optimal fee structure might be.\nThe structural researcher writes down an explicit model of trader behavior:\n\\[\n\\max_{q_t} \\; E_t\\left[\\sum_{s=t}^{T} \\delta^{s-t}\\left(v_s q_s - c(q_s; \\boldsymbol{\\theta})\\right)\\right]\n\\tag{43.2}\\]\nwhere \\(v_s\\) is the (possibly private) valuation, \\(q_s\\) is the trade quantity, \\(c(\\cdot; \\boldsymbol{\\theta})\\) is the cost function parameterized by \\(\\boldsymbol{\\theta}\\), and \\(\\delta\\) is the discount factor. The researcher estimates \\(\\boldsymbol{\\theta}\\) by requiring that the model’s predictions match observed trading patterns. With \\(\\hat{\\boldsymbol{\\theta}}\\) in hand, any counterfactual cost function \\(c'(\\cdot; \\boldsymbol{\\theta}')\\) can be fed into the model to predict the equilibrium response.\nThe distinction is not about sophistication (reduced-form work can be highly rigorous) but about the type of question being answered:\n\n\n\nTable 43.1: Reduced Form vs. Structural Estimation\n\n\n\n\n\n\n\n\n\n\n\nReduced Form\nStructural\n\n\n\n\nAnswers\n“What happened?”\n“What would happen if…?”\n\n\nIdentification\nExogenous variation (events, instruments)\nModel restrictions + data moments\n\n\nKey assumption\nUnconfoundedness or exclusion restriction\nCorrect model specification\n\n\nOutput\nTreatment effects, associations\nPrimitive parameters, counterfactuals\n\n\nRisk\nOmitted variable bias, weak instruments\nModel misspecification\n\n\n\n\n\n\n\n\n43.1.2 Identification Through Economic Primitives\nStructural identification relies on the economic model to convert observed outcomes into unobserved primitives. The classic example is the demand-supply system. Observing prices and quantities alone cannot identify demand and supply curves (the simultaneity problem). But if we know the functional form of demand and supply, and have instruments that shift one curve but not the other, the system is identified.\nIn financial contexts, identification often comes from the model’s equilibrium conditions. In Kyle (1985), the informed trader’s strategy, the market maker’s pricing rule, and the noise trader’s demand jointly determine the equilibrium price impact coefficient \\(\\lambda\\). The model maps the observable (i.e., the price impact of order flow) to the unobservable (i.e., the precision of private information). The identification is through the model’s structure, not through a conventional instrument.\nFormally, let \\(\\boldsymbol{\\theta} \\in \\Theta\\) denote the vector of structural parameters and \\(\\mathbf{m}(\\boldsymbol{\\theta})\\) the model-implied moments. The data provide empirical moments \\(\\hat{\\mathbf{m}}\\). Identification requires that the mapping \\(\\boldsymbol{\\theta} \\mapsto \\mathbf{m}(\\boldsymbol{\\theta})\\) is injective: different parameter values produce different observable implications.\n\\[\n\\hat{\\boldsymbol{\\theta}} = \\arg\\min_{\\boldsymbol{\\theta} \\in \\Theta} \\left[\\hat{\\mathbf{m}} - \\mathbf{m}(\\boldsymbol{\\theta})\\right]' \\mathbf{W} \\left[\\hat{\\mathbf{m}} - \\mathbf{m}(\\boldsymbol{\\theta})\\right]\n\\tag{43.3}\\]\nThis is the Generalized Method of Moments (GMM) framework applied to structural estimation. The choice of weighting matrix \\(\\mathbf{W}\\) determines efficiency, and over-identifying restrictions (more moments than parameters) provide specification tests.\n\n\n43.1.3 Tradeoffs Between Realism and Tractability\nEvery structural model involves choices about which features of reality to include and which to abstract from. Table 43.2 illustrates this for several canonical finance models.\n\n\n\nTable 43.2: Structural Model Tradeoffs\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey Simplification\nWhat It Misses\nWhat It Gains\n\n\n\n\nKyle (1985)\nSingle informed trader, normal distributions\nMultiple insiders, fat tails\nClosed-form \\(\\lambda\\), clean identification\n\n\nGlosten and Milgrom (1985)\nBinary signal, competitive market makers\nComplex information, inventory costs\nBid-ask spread decomposition\n\n\nKoijen and Yogo (2019)\nCharacteristics-based demand, no dynamics\nDynamic portfolio rebalancing\nDemand elasticity estimation at scale\n\n\nRoll (1984)\nNo information, serial independence\nInformation-driven trades\nSpread estimation from return autocovariance\n\n\n\n\n\n\nThe researcher’s task is to choose the simplest model that captures the economic mechanism of interest while remaining rich enough that its counterfactual predictions are credible. As Rust (1987) emphasized, there is a tension between models that are “structurally correct” but computationally intractable and models that are tractable but potentially misspecified.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Structural Models in Finance</span>"
    ]
  },
  {
    "objectID": "63_structural_models_finance.html#demand-estimation-for-financial-assets",
    "href": "63_structural_models_finance.html#demand-estimation-for-financial-assets",
    "title": "43  Structural Models in Finance",
    "section": "43.2 Demand Estimation for Financial Assets",
    "text": "43.2 Demand Estimation for Financial Assets\n\n43.2.1 The Demand System Approach\nKoijen and Yogo (2019) (hereafter KY) develop a demand system for financial assets that estimates the elasticity of institutional investor demand with respect to asset characteristics. The framework adapts the discrete-choice demand estimation methodology of Berry, Levinsohn, and Pakes (1995) from industrial organization to financial markets.\nEach investor \\(i\\) holds a portfolio of \\(N\\) assets. The demand for asset \\(j\\) by investor \\(i\\) at time \\(t\\) is:\n\\[\nw_{ij,t} = \\frac{\\exp(\\delta_{j,t} + \\boldsymbol{\\beta}_i' \\mathbf{x}_{j,t})}{1 + \\sum_{k=1}^{N} \\exp(\\delta_{k,t} + \\boldsymbol{\\beta}_i' \\mathbf{x}_{k,t})}\n\\tag{43.4}\\]\nwhere \\(w_{ij,t}\\) is the portfolio weight of asset \\(j\\) for investor \\(i\\), \\(\\delta_{j,t}\\) is the mean utility (common valuation), \\(\\mathbf{x}_{j,t}\\) is a vector of observable characteristics (market cap, book-to-market, momentum, etc.), and \\(\\boldsymbol{\\beta}_i\\) captures investor-specific preferences (heterogeneous demand elasticities). The denominator includes 1 to represent the outside option (holding cash or non-equity assets).\nThe key insight is that the market-clearing condition links demand to equilibrium prices:\n\\[\n\\sum_i A_{i,t} \\cdot w_{ij,t}(\\boldsymbol{\\theta}) = \\text{ME}_{j,t} \\qquad \\forall j, t\n\\tag{43.5}\\]\nwhere \\(A_{i,t}\\) is investor \\(i\\)’s total assets under management and \\(\\text{ME}_{j,t}\\) is the market capitalization of asset \\(j\\). This system of equations determines the equilibrium price impacts and demand elasticities.\n\n\n43.2.2 Demand Elasticity Estimation\nThe demand elasticity of asset \\(j\\) with respect to its own price (or a characteristic that moves its price) is:\n\\[\n\\varepsilon_{jj} = \\frac{\\partial \\ln w_{ij}}{\\partial \\ln P_j} = \\beta_{\\text{price}} \\cdot (1 - w_{ij})\n\\tag{43.6}\\]\nIn a frictionless market with perfectly elastic demand, a supply shock (e.g., an index addition) would have zero price impact. Empirically, demand curves for financial assets slope downward with finite elasticity, implying that supply shocks move equilibrium prices.\n\n# DataCore.vn API\nfrom datacore import DataCore\ndc = DataCore()\n\n# Load institutional holdings data\nholdings = dc.get_institutional_holdings(\n    start_date=\"2012-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Load firm characteristics\nfirm_chars = dc.get_firm_characteristics(\n    start_date=\"2012-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Load market data\nmarket_data = dc.get_daily_returns(\n    start_date=\"2012-01-01\",\n    end_date=\"2024-12-31\"\n)\n\nprint(f\"Holdings observations: {len(holdings)}\")\nprint(f\"Unique institutions: {holdings['institution_id'].nunique()}\")\nprint(f\"Unique stocks: {holdings['ticker'].nunique()}\")\n\n\n# Construct demand system variables\ndemand = holdings.merge(\n    firm_chars[[\"ticker\", \"date\", \"market_cap\", \"book_to_market\",\n                \"momentum_12m\", \"log_size\", \"profitability\",\n                \"investment\", \"industry\"]],\n    on=[\"ticker\", \"date\"],\n    how=\"inner\"\n)\n\n# Portfolio weights\ndemand[\"total_aum\"] = demand.groupby(\n    [\"institution_id\", \"date\"]\n)[\"holding_value\"].transform(\"sum\")\n\ndemand[\"port_weight\"] = demand[\"holding_value\"] / demand[\"total_aum\"]\n\n# Log portfolio weight relative to outside option\n# w_ij / w_i0 where w_i0 = 1 - sum(w_ij) for equity holdings\ndemand[\"sum_equity_weight\"] = demand.groupby(\n    [\"institution_id\", \"date\"]\n)[\"port_weight\"].transform(\"sum\")\n\ndemand[\"outside_weight\"] = 1 - demand[\"sum_equity_weight\"].clip(upper=0.99)\n\ndemand[\"log_weight_ratio\"] = np.log(\n    demand[\"port_weight\"] / demand[\"outside_weight\"]\n)\n\n\n# Simplified KY demand estimation\n# log(w_ij / w_i0) = delta_j + beta_i' x_j + epsilon_ij\n# First stage: estimate mean utility delta_j via OLS with asset FE\n\n# Cross-sectional regression at each date\ndef estimate_demand_cross_section(group):\n    \"\"\"\n    Estimate demand parameters for a single cross-section.\n    Uses OLS with stock fixed effects absorbed.\n    \"\"\"\n    g = group.dropna(subset=[\n        \"log_weight_ratio\", \"log_size\", \"book_to_market\",\n        \"momentum_12m\", \"profitability\"\n    ])\n\n    if len(g) &lt; 50:\n        return pd.Series(dtype=float)\n\n    X = g[[\"log_size\", \"book_to_market\", \"momentum_12m\",\n           \"profitability\"]].values\n    X = sm.add_constant(X)\n    y = g[\"log_weight_ratio\"].values\n\n    try:\n        model = sm.OLS(y, X).fit()\n        return pd.Series({\n            \"beta_size\": model.params[1],\n            \"beta_btm\": model.params[2],\n            \"beta_mom\": model.params[3],\n            \"beta_prof\": model.params[4],\n            \"r_squared\": model.rsquared,\n            \"n_obs\": len(g)\n        })\n    except Exception:\n        return pd.Series(dtype=float)\n\n# Estimate by institution type\ndemand[\"institution_type\"] = demand[\"institution_type\"].fillna(\"Other\")\n\ndemand_params = (\n    demand.groupby([\"institution_type\", \"date\"])\n    .apply(estimate_demand_cross_section)\n    .reset_index()\n    .dropna()\n)\n\n\n\n\nTable 43.3: Demand Elasticities by Investor Type\n\n\navg_params = (\n    demand_params.groupby(\"institution_type\")\n    .agg(\n        beta_size=(\"beta_size\", \"mean\"),\n        beta_btm=(\"beta_btm\", \"mean\"),\n        beta_mom=(\"beta_mom\", \"mean\"),\n        beta_prof=(\"beta_prof\", \"mean\"),\n        avg_r2=(\"r_squared\", \"mean\"),\n        n_periods=(\"date\", \"nunique\")\n    )\n    .round(4)\n)\n\navg_params\n\n\n\n\n\n\ntop_types = demand_params.groupby(\"institution_type\").size().nlargest(4).index\nplot_data = demand_params[\n    demand_params[\"institution_type\"].isin(top_types)\n].copy()\n\n(\n    p9.ggplot(plot_data, p9.aes(\n        x=\"date\", y=\"beta_size\", color=\"institution_type\"\n    ))\n    + p9.geom_smooth(method=\"lowess\", se=False, size=1)\n    + p9.labs(\n        x=\"\", y=\"β (Size)\",\n        title=\"Institutional Demand Sensitivity to Firm Size\",\n        color=\"Institution Type\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5), legend_position=\"top\")\n)\n\n\nFigure 43.1\n\n\n\n\n\n43.2.3 Price Pressure and Market Impact\nThe demand system framework directly yields price impact predictions. If investor \\(i\\) receives an exogenous inflow \\(\\Delta A_i\\) (e.g., from a fund flow shock), the price of each stock must adjust to clear the market with the new demand. The equilibrium price impact of a demand shock is:\n\\[\n\\frac{\\Delta P_j}{P_j} = \\frac{1}{\\varepsilon_{jj}} \\cdot \\frac{\\Delta D_j}{D_j}\n\\tag{43.7}\\]\nwhere \\(\\varepsilon_{jj}\\) is the demand elasticity and \\(\\Delta D_j / D_j\\) is the percentage demand shock. Less elastic demand implies larger price impact for the same demand shock (a prediction with direct implications for the price impact of index rebalancing, fund flows, and forced selling).\n\n# Estimate aggregate demand elasticity via index rebalancing events\n# Use VN30 index reconstitutions as demand shocks\n\nindex_changes = dc.get_index_changes(\n    index=\"VN30\",\n    start_date=\"2012-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Event study: abnormal returns around index additions/deletions\ndef event_study_car(events, returns, window=(-5, 20)):\n    \"\"\"\n    Compute cumulative abnormal returns around events.\n\n    Parameters\n    ----------\n    events : DataFrame\n        Columns: ticker, event_date, event_type (addition/deletion).\n    returns : DataFrame\n        Columns: ticker, date, ret, mkt_ret.\n\n    Returns\n    -------\n    DataFrame : CARs by event type and event day.\n    \"\"\"\n    results = []\n\n    for _, event in events.iterrows():\n        ticker = event[\"ticker\"]\n        event_date = pd.to_datetime(event[\"event_date\"])\n\n        # Estimation window: -260 to -11\n        stock_rets = returns[returns[\"ticker\"] == ticker].copy()\n        stock_rets = stock_rets.sort_values(\"date\")\n\n        est_mask = (\n            (stock_rets[\"date\"] &gt;= event_date - pd.Timedelta(days=365)) &\n            (stock_rets[\"date\"] &lt; event_date - pd.Timedelta(days=15))\n        )\n        est_data = stock_rets[est_mask].dropna(subset=[\"ret\", \"mkt_ret\"])\n\n        if len(est_data) &lt; 60:\n            continue\n\n        # Market model\n        model = sm.OLS(\n            est_data[\"ret\"],\n            sm.add_constant(est_data[\"mkt_ret\"])\n        ).fit()\n\n        # Event window\n        evt_mask = (\n            (stock_rets[\"date\"] &gt;= event_date + pd.Timedelta(days=window[0] * 1.5)) &\n            (stock_rets[\"date\"] &lt;= event_date + pd.Timedelta(days=window[1] * 1.5))\n        )\n        evt_data = stock_rets[evt_mask].dropna(subset=[\"ret\", \"mkt_ret\"])\n\n        if len(evt_data) &lt; 5:\n            continue\n\n        # Abnormal returns\n        evt_data = evt_data.copy()\n        evt_data[\"expected\"] = model.predict(\n            sm.add_constant(evt_data[\"mkt_ret\"])\n        )\n        evt_data[\"ar\"] = evt_data[\"ret\"] - evt_data[\"expected\"]\n\n        # Assign event-time index\n        trading_dates = evt_data[\"date\"].sort_values().values\n        event_idx = np.searchsorted(trading_dates, event_date)\n        evt_data[\"event_day\"] = range(-event_idx, len(evt_data) - event_idx)\n\n        evt_data[\"event_type\"] = event[\"event_type\"]\n        evt_data[\"ticker\"] = ticker\n        results.append(evt_data[[\"ticker\", \"event_day\", \"ar\", \"event_type\"]])\n\n    if not results:\n        return pd.DataFrame()\n\n    return pd.concat(results, ignore_index=True)\n\n# Compute event study\ndaily_data = market_data.merge(\n    dc.get_market_returns(\n        start_date=\"2012-01-01\",\n        end_date=\"2024-12-31\",\n        frequency=\"daily\"\n    )[[\"date\", \"mkt_ret\"]],\n    on=\"date\", how=\"left\"\n)\n\ncar_results = event_study_car(index_changes, daily_data)\n\n\n\n\nif len(car_results) &gt; 0:\n    # Average AR by event day and type\n    avg_ar = (\n        car_results.groupby([\"event_type\", \"event_day\"])\n        .agg(\n            mean_ar=(\"ar\", \"mean\"),\n            se_ar=(\"ar\", lambda x: x.std() / np.sqrt(len(x))),\n            n=(\"ar\", \"count\")\n        )\n        .reset_index()\n    )\n\n    # Cumulative AR\n    for etype in avg_ar[\"event_type\"].unique():\n        mask = avg_ar[\"event_type\"] == etype\n        avg_ar.loc[mask, \"car\"] = avg_ar.loc[mask, \"mean_ar\"].cumsum()\n\n    avg_ar = avg_ar[avg_ar[\"event_day\"].between(-5, 20)]\n\n    (\n        p9.ggplot(avg_ar, p9.aes(\n            x=\"event_day\", y=\"car\", color=\"event_type\"\n        ))\n        + p9.geom_line(size=1)\n        + p9.geom_vline(xintercept=0, linetype=\"dashed\", color=\"gray\")\n        + p9.geom_hline(yintercept=0, linetype=\"dotted\", color=\"gray\")\n        + p9.scale_color_manual(values=[\"#27AE60\", \"#C0392B\"])\n        + p9.labs(\n            x=\"Event Day\",\n            y=\"Cumulative Abnormal Return\",\n            title=\"Price Pressure from VN30 Index Additions and Deletions\",\n            color=\"Event\"\n        )\n        + p9.theme_minimal()\n        + p9.theme(figure_size=(10, 6))\n    )\n\n\nFigure 43.2\n\n\n\nThe permanent component of the CAR measures the information content of index inclusion, while the temporary component (reversal after the event) measures pure price pressure from demand. In a world with perfectly elastic demand, there would be no temporary price impact. The magnitude of the temporary component is inversely related to the demand elasticity, providing a second identification strategy (complementary to the holdings-based approach) for demand curves.\n\n\n43.2.4 Demand Inelasticity and Its Implications\nGabaix and Koijen (2021) argue that the aggregate demand for equities is remarkably inelastic, with elasticity estimates on the order of 0.2 (meaning a 1% supply increase requires a 5% price decline to clear). The implications are profound: if demand is this inelastic, then flows (from index funds, foreign investors, retail traders) have outsized effects on prices. In Vietnamese markets, where foreign ownership caps create binding constraints on a key investor class, demand inelasticity may be even more severe.\nThe inelasticity also generates a role for “the market portfolio” as an equilibrium concept: if most investors hold portfolios close to the market (as in CAPM), then deviations from market weights require someone to absorb the excess supply, and the compensation required is the inverse of demand elasticity.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Structural Models in Finance</span>"
    ]
  },
  {
    "objectID": "63_structural_models_finance.html#structural-models-of-trading-strategies",
    "href": "63_structural_models_finance.html#structural-models-of-trading-strategies",
    "title": "43  Structural Models in Finance",
    "section": "43.3 Structural Models of Trading Strategies",
    "text": "43.3 Structural Models of Trading Strategies\n\n43.3.1 Optimal Execution: The Almgren-Chriss Framework\nThe optimal execution problem, formalized by Almgren and Chriss (2001), asks: given a large order to execute over a fixed horizon, what is the optimal trading schedule that minimizes the total execution cost? The problem is fundamental to institutional investing because large orders cannot be executed instantaneously without severe price impact.\nLet \\(X_t\\) denote the remaining shares to sell at time \\(t\\), with \\(X_0 = X\\) (total order) and \\(X_T = 0\\) (completion). The trading rate is \\(n_t = X_{t-1} - X_t\\). The execution price for shares traded at \\(t\\) is affected by both temporary and permanent price impact:\n\\[\nS_t = S_0 + \\sigma W_t - g(n_t) - h\\left(\\frac{n_t}{\\tau}\\right)\n\\tag{43.8}\\]\nwhere \\(g(\\cdot)\\) is the permanent impact function, \\(h(\\cdot)\\) is the temporary impact function, \\(\\sigma\\) is the volatility, \\(W_t\\) is a Wiener process, and \\(\\tau\\) is the time interval. The total implementation shortfall (cost of execution relative to the arrival price \\(S_0\\)) is:\n\\[\n\\text{IS} = \\sum_{t=1}^{T} n_t (S_0 - S_t) = \\sum_{t=1}^{T} n_t \\left[\\sigma W_t + g(n_t) + h\\left(\\frac{n_t}{\\tau}\\right)\\right]\n\\tag{43.9}\\]\nThe trader minimizes a mean-variance objective:\n\\[\n\\min_{\\{n_t\\}} \\; E[\\text{IS}] + \\lambda \\cdot \\text{Var}(\\text{IS})\n\\tag{43.10}\\]\nwhere \\(\\lambda\\) is the risk aversion parameter. With linear impact functions \\(g(n) = \\gamma n\\) and \\(h(v) = \\eta v + \\epsilon \\text{sgn}(v)\\), the optimal strategy has a closed-form solution.\nTWAP (Time-Weighted Average Price): Trade uniformly: \\(n_t = X / T\\) for all \\(t\\). Optimal when permanent impact dominates temporary impact.\nVWAP (Volume-Weighted Average Price): Trade proportionally to expected volume: \\(n_t \\propto \\hat{V}_t\\). Approximates TWAP adjusted for intraday volume patterns.\nAlmgren-Chriss Optimal: The risk-averse optimal trajectory for linear impact is:\n\\[\nx_t^* = X \\cdot \\frac{\\sinh(\\kappa (T - t))}{\\sinh(\\kappa T)}\n\\tag{43.11}\\]\nwhere \\(\\kappa = \\sqrt{\\lambda \\sigma^2 / \\eta}\\) governs the trade-off between urgency (trading quickly to reduce risk) and patience (trading slowly to reduce impact). High risk aversion (\\(\\lambda\\) large) implies front-loaded execution.\n\ndef almgren_chriss_trajectory(X, T, sigma, eta, gamma, lam, n_steps=100):\n    \"\"\"\n    Compute Almgren-Chriss optimal execution trajectory.\n\n    Parameters\n    ----------\n    X : float\n        Total shares to execute.\n    T : float\n        Execution horizon (in trading days).\n    sigma : float\n        Daily return volatility.\n    eta : float\n        Temporary impact parameter.\n    gamma : float\n        Permanent impact parameter.\n    lam : float\n        Risk aversion parameter.\n    n_steps : int\n        Number of time steps.\n\n    Returns\n    -------\n    DataFrame : Time, remaining shares, trading rate, expected cost.\n    \"\"\"\n    tau = T / n_steps\n    kappa_sq = lam * sigma**2 / (eta / tau)\n\n    if kappa_sq &lt;= 0:\n        # Risk-neutral: trade uniformly\n        kappa = 0\n        x_t = np.linspace(X, 0, n_steps + 1)\n    else:\n        kappa = np.sqrt(kappa_sq)\n        t_grid = np.linspace(0, T, n_steps + 1)\n        x_t = X * np.sinh(kappa * (T - t_grid)) / np.sinh(kappa * T)\n\n    # Trading rates\n    n_t = -np.diff(x_t)\n\n    # Expected cost components\n    perm_cost = gamma * np.sum(n_t**2)\n    temp_cost = (eta / tau) * np.sum(n_t**2)\n    total_expected_cost = perm_cost + temp_cost\n\n    # Variance of cost\n    var_cost = sigma**2 * tau * np.sum(x_t[:-1]**2)\n\n    results = pd.DataFrame({\n        \"time\": np.linspace(0, T, n_steps + 1)[:-1],\n        \"remaining_shares\": x_t[:-1],\n        \"trade_rate\": n_t\n    })\n\n    results.attrs[\"expected_cost\"] = total_expected_cost\n    results.attrs[\"cost_variance\"] = var_cost\n    results.attrs[\"kappa\"] = kappa if kappa_sq &gt; 0 else 0\n\n    return results\n\n\n\n\n# Estimate market impact parameters from Vietnamese data\n# Typical values for mid-cap Vietnamese stocks\nsigma_daily = 0.025  # 2.5% daily vol\neta = 2.5e-7         # Temporary impact\ngamma = 1.0e-7       # Permanent impact\nX_shares = 500000    # 500k shares to sell\nT_days = 5           # 5-day horizon\n\ntrajectories = []\nfor lam, label in [(0, \"Risk Neutral (TWAP)\"),\n                    (1e-6, \"Low Risk Aversion\"),\n                    (1e-5, \"Medium Risk Aversion\"),\n                    (1e-4, \"High Risk Aversion\")]:\n    traj = almgren_chriss_trajectory(\n        X_shares, T_days, sigma_daily, eta, gamma, lam\n    )\n    traj[\"strategy\"] = label\n    traj[\"pct_remaining\"] = traj[\"remaining_shares\"] / X_shares * 100\n    trajectories.append(traj)\n\ntraj_all = pd.concat(trajectories, ignore_index=True)\n\n(\n    p9.ggplot(traj_all, p9.aes(\n        x=\"time\", y=\"pct_remaining\", color=\"strategy\"\n    ))\n    + p9.geom_line(size=1)\n    + p9.scale_color_manual(\n        values=[\"#95A5A6\", \"#27AE60\", \"#2E5090\", \"#C0392B\"]\n    )\n    + p9.labs(\n        x=\"Time (Trading Days)\",\n        y=\"Remaining Order (%)\",\n        title=\"Almgren-Chriss Optimal Execution Trajectories\",\n        color=\"Strategy\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 6), legend_position=\"top\")\n)\n\n\nFigure 43.3\n\n\n\n\n\n43.3.2 Estimating Market Impact from Transaction Data\nThe structural parameters \\((\\gamma, \\eta, \\sigma)\\) must be estimated from data. The standard approach uses the square-root law of market impact, documented empirically by Kyle (1985) and Hasbrouck (1991) and derived theoretically by Gabaix et al. (2003):\n\\[\n\\Delta P / P = c \\cdot \\text{sgn}(Q) \\cdot |Q / V|^{\\delta}\n\\tag{43.12}\\]\nwhere \\(Q\\) is the signed order flow, \\(V\\) is daily volume, \\(c\\) is the impact coefficient, and \\(\\delta \\approx 0.5\\) is the impact exponent. The “square root” refers to \\(\\delta = 0.5\\), which is remarkably robust across markets, time periods, and asset classes.\n\n# Load intraday transaction data (or daily order flow proxy)\norder_flow = dc.get_order_flow(\n    start_date=\"2018-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\n# Construct signed order flow using Lee-Ready classification\n# (buy-initiated minus sell-initiated volume)\nimpact_data = order_flow.merge(\n    dc.get_daily_returns(\n        start_date=\"2018-01-01\",\n        end_date=\"2024-12-31\"\n    )[[\"ticker\", \"date\", \"ret\", \"volume\", \"market_cap\"]],\n    on=[\"ticker\", \"date\"],\n    how=\"inner\"\n)\n\n# Normalize order flow\nimpact_data[\"oib\"] = impact_data[\"net_buy_volume\"] / impact_data[\"volume\"]\nimpact_data[\"abs_oib\"] = np.abs(impact_data[\"oib\"])\nimpact_data[\"sign_oib\"] = np.sign(impact_data[\"oib\"])\n\n# Log-log regression: ln|ΔP| = α + δ ln|Q/V| + ε\nimpact_data[\"ln_abs_ret\"] = np.log(np.abs(impact_data[\"ret\"]).clip(lower=1e-8))\nimpact_data[\"ln_abs_oib\"] = np.log(impact_data[\"abs_oib\"].clip(lower=1e-8))\n\n# Estimate by size quintile\nimpact_data[\"size_quintile\"] = impact_data.groupby(\"date\")[\n    \"market_cap\"\n].transform(lambda x: pd.qcut(x, 5, labels=[1, 2, 3, 4, 5],\n                                duplicates=\"drop\"))\n\nimpact_results = []\nfor q in range(1, 6):\n    subset = impact_data[impact_data[\"size_quintile\"] == q].dropna(\n        subset=[\"ln_abs_ret\", \"ln_abs_oib\"]\n    )\n    if len(subset) &lt; 500:\n        continue\n\n    model = sm.OLS(\n        subset[\"ln_abs_ret\"],\n        sm.add_constant(subset[\"ln_abs_oib\"])\n    ).fit(cov_type=\"HC1\")\n\n    impact_results.append({\n        \"size_quintile\": q,\n        \"delta_hat\": model.params.iloc[1],\n        \"delta_se\": model.bse.iloc[1],\n        \"intercept\": model.params.iloc[0],\n        \"r_squared\": model.rsquared,\n        \"n_obs\": int(model.nobs)\n    })\n\nimpact_df = pd.DataFrame(impact_results)\n\n\n\n\nTable 43.4: Market Impact Exponent by Firm Size Quintile\n\n\nimpact_df.round(4)\n\n\n\nThe impact exponent \\(\\hat{\\delta}\\) near 0.5 across size quintiles confirms the universality of the square-root law. Smaller firms typically exhibit higher impact coefficients (larger \\(c\\)), consistent with lower liquidity.\n\n\n43.3.3 Transaction Cost Decomposition\nTotal transaction costs comprise multiple components, each with distinct economic content:\n\\[\n\\text{TC} = \\underbrace{\\frac{s}{2}}_{\\text{Half-spread}} + \\underbrace{\\gamma \\cdot Q}_{\\text{Permanent impact}} + \\underbrace{\\eta \\cdot \\frac{Q}{V}}_{\\text{Temporary impact}} + \\underbrace{\\sigma \\sqrt{T}}_{\\text{Timing risk}}\n\\tag{43.13}\\]\nWe estimate each component separately, following the Hasbrouck (2009) methodology for effective spread decomposition.\n\n# Effective spread estimation\nspreads = dc.get_bid_ask_data(\n    start_date=\"2020-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Quoted spread\nspreads[\"quoted_spread\"] = (\n    (spreads[\"ask\"] - spreads[\"bid\"]) / spreads[\"midpoint\"]\n)\n\n# Effective spread (from transaction prices)\nspreads[\"effective_spread\"] = (\n    2 * np.abs(spreads[\"trade_price\"] - spreads[\"midpoint\"]) /\n    spreads[\"midpoint\"]\n)\n\n# Roll (1984) implied spread from return autocovariance\ndef roll_spread(returns):\n    \"\"\"\n    Estimate the Roll (1984) bid-ask spread.\n    Spread = 2 * sqrt(-Cov(r_t, r_{t-1})) if covariance is negative.\n    \"\"\"\n    cov = np.cov(returns[1:], returns[:-1])[0, 1]\n    if cov &lt; 0:\n        return 2 * np.sqrt(-cov)\n    else:\n        return np.nan\n\n# Compute by stock-month\ndaily_rets = dc.get_daily_returns(\n    start_date=\"2020-01-01\",\n    end_date=\"2024-12-31\"\n)\ndaily_rets[\"month\"] = daily_rets[\"date\"].dt.to_period(\"M\")\n\nroll_spreads = (\n    daily_rets.groupby([\"ticker\", \"month\"])\n    .agg(\n        roll_spread=(\"ret\", lambda x: roll_spread(x.values)\n                     if len(x) &gt; 10 else np.nan),\n        n_days=(\"ret\", \"count\"),\n        avg_volume=(\"volume\", \"mean\"),\n        market_cap=(\"market_cap\", \"last\")\n    )\n    .reset_index()\n    .dropna(subset=[\"roll_spread\"])\n)\n\n\n\n\n# Bin by market cap deciles\nroll_spreads[\"size_decile\"] = roll_spreads.groupby(\"month\")[\n    \"market_cap\"\n].transform(lambda x: pd.qcut(x, 10, labels=range(1, 11),\n                                duplicates=\"drop\"))\n\nspread_by_size = (\n    roll_spreads.groupby(\"size_decile\")\n    .agg(\n        median_spread=(\"roll_spread\", \"median\"),\n        mean_spread=(\"roll_spread\", \"mean\"),\n        q25=(\"roll_spread\", lambda x: x.quantile(0.25)),\n        q75=(\"roll_spread\", lambda x: x.quantile(0.75))\n    )\n    .reset_index()\n)\n\n(\n    p9.ggplot(spread_by_size, p9.aes(x=\"size_decile\", y=\"median_spread\"))\n    + p9.geom_bar(stat=\"identity\", fill=\"#2E5090\", alpha=0.7)\n    + p9.geom_errorbar(\n        p9.aes(ymin=\"q25\", ymax=\"q75\"),\n        width=0.3, color=\"#2E5090\"\n    )\n    + p9.labs(\n        x=\"Size Decile (1 = Smallest)\",\n        y=\"Roll Implied Spread\",\n        title=\"Transaction Costs Decrease Monotonically with Firm Size\"\n    )\n    + p9.scale_y_continuous(labels=percent_format())\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n\n\nFigure 43.4",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Structural Models in Finance</span>"
    ]
  },
  {
    "objectID": "63_structural_models_finance.html#auction-models-in-primary-markets",
    "href": "63_structural_models_finance.html#auction-models-in-primary-markets",
    "title": "43  Structural Models in Finance",
    "section": "43.4 Auction Models in Primary Markets",
    "text": "43.4 Auction Models in Primary Markets\n\n43.4.1 The IPO Allocation Problem\nInitial public offerings present a classic information asymmetry problem. Issuers and underwriters do not know the true market-clearing price; informed investors do (approximately). The allocation mechanism (i.e., how shares are distributed and at what price) determines the IPO’s pricing efficiency, the degree of underpricing, and the distribution of surplus between issuers and investors.\nRock (1986) provides the canonical model. There are two types of investors: informed (who know the true value \\(v\\)) and uninformed (who know only the distribution \\(v \\sim F\\)). If the IPO is priced at \\(P\\):\n\nWhen \\(v &gt; P\\) (good IPO): both informed and uninformed subscribe, so uninformed receive only a fraction of their order (rationing).\nWhen \\(v &lt; P\\) (bad IPO): only uninformed subscribe, so they receive full allocation (the “winner’s curse”).\n\nThe uninformed investor’s expected return, accounting for rationing, is:\n\\[\nE[R_{\\text{uninformed}}] = \\alpha_g \\cdot E\\left[\\frac{v - P}{P} \\mid v &gt; P\\right] \\cdot \\Pr(v &gt; P) + E\\left[\\frac{v - P}{P} \\mid v \\leq P\\right] \\cdot \\Pr(v \\leq P)\n\\tag{43.14}\\]\nwhere \\(\\alpha_g &lt; 1\\) is the allocation probability in good IPOs (rationed) and allocation is 1 in bad IPOs. For uninformed investors to participate, \\(E[R_{\\text{uninformed}}] \\geq 0\\), which requires:\n\\[\nE\\left[\\frac{v - P}{P}\\right] &gt; 0\n\\tag{43.15}\\]\nThat is, IPOs must be underpriced on average to compensate uninformed investors for the winner’s curse. The degree of required underpricing increases with the proportion of informed investors and the variance of the true value.\n\n\n43.4.2 Book Building vs. Auction Mechanisms\nVietnamese IPO history provides variation in allocation mechanisms. State-owned enterprise equitizations have used Dutch auctions, while private-sector IPOs have used book building. This institutional variation allows structural comparison of the mechanisms.\nBook Building (Benveniste and Spindt 1989): The underwriter solicits indications of interest from institutional investors during the roadshow. Investors who reveal positive information (high valuations) receive favorable allocations as compensation. The mechanism aggregates information efficiently but grants discretion to the underwriter.\nUniform-Price Auction: All winning bidders pay the same market-clearing price. This eliminates the underwriter’s allocation discretion but may lead to free-riding on information revelation.\n\n# Load IPO data\nipo_data = dc.get_ipo_data(\n    start_date=\"2005-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Compute first-day returns (underpricing)\nipo_data[\"underpricing\"] = (\n    (ipo_data[\"first_day_close\"] - ipo_data[\"offer_price\"]) /\n    ipo_data[\"offer_price\"]\n)\n\n# Classify mechanism\nipo_data[\"mechanism\"] = ipo_data[\"ipo_method\"].map({\n    \"auction\": \"Auction\",\n    \"book_building\": \"Book Building\",\n    \"fixed_price\": \"Fixed Price\"\n})\n\nprint(f\"Total IPOs: {len(ipo_data)}\")\nprint(f\"By mechanism:\\n{ipo_data['mechanism'].value_counts()}\")\n\n\n\n\nTable 43.5: IPO Underpricing by Allocation Mechanism\n\n\nipo_summary = (\n    ipo_data.groupby(\"mechanism\")\n    .agg(\n        n_ipos=(\"underpricing\", \"count\"),\n        mean_underpricing=(\"underpricing\", \"mean\"),\n        median_underpricing=(\"underpricing\", \"median\"),\n        std_underpricing=(\"underpricing\", \"std\"),\n        pct_positive=(\"underpricing\", lambda x: (x &gt; 0).mean()),\n        mean_proceeds=(\"proceeds_bn_vnd\", \"mean\")\n    )\n    .round(4)\n)\nipo_summary\n\n\n\n\n\n43.4.3 Structural Estimation of Information Asymmetry\nWe estimate the Rock (1986) model parameters (i.e., the fraction of informed investors (\\(\\mu\\)) and the precision of their information (\\(\\sigma_v^2\\))) using the method of simulated moments (MSM).\nThe model predicts two key moments:\n1 . Average underpricing: \\(E[U] = f(\\mu, \\sigma_v^2)\\) 2. Cross-sectional variance of underpricing: \\(\\text{Var}(U) = g(\\mu, \\sigma_v^2)\\)\nWe match these model-implied moments to the data.\n\ndef rock_model_moments(params, n_sim=10000):\n    \"\"\"\n    Simulate Rock (1986) IPO model and compute moments.\n\n    Parameters\n    ----------\n    params : tuple\n        (mu, sigma_v) - fraction of informed investors,\n        std dev of true value.\n\n    Returns\n    -------\n    tuple : (mean underpricing, variance of underpricing).\n    \"\"\"\n    mu, sigma_v = params\n    np.random.seed(42)\n\n    # True values\n    v = np.random.lognormal(mean=0, sigma=sigma_v, size=n_sim)\n\n    # Offer price: set to break even for uninformed\n    # Simplified: P = E[v] * discount_factor\n    P = np.exp(sigma_v**2 / 2) * 0.9  # 10% average discount\n\n    # First-day returns\n    returns = (v - P) / P\n\n    # Allocation probability for uninformed in good IPOs\n    # Depends on mu: more informed -&gt; more rationing\n    good_mask = v &gt; P\n    alpha_g = (1 - mu) / 1.0  # Simplified rationing\n\n    # Uninformed realized returns\n    uninformed_returns = np.where(\n        good_mask,\n        alpha_g * returns,\n        returns\n    )\n\n    mean_u = uninformed_returns.mean()\n    var_u = uninformed_returns.var()\n\n    return mean_u, var_u\n\n\ndef rock_model_objective(params, target_moments, weight_matrix=None):\n    \"\"\"\n    GMM objective for Rock model estimation.\n    \"\"\"\n    mu, sigma_v = params\n\n    if mu &lt;= 0 or mu &gt;= 1 or sigma_v &lt;= 0 or sigma_v &gt; 2:\n        return 1e10\n\n    model_moments = rock_model_moments(params)\n    diff = np.array(model_moments) - np.array(target_moments)\n\n    if weight_matrix is None:\n        weight_matrix = np.eye(len(diff))\n\n    return diff @ weight_matrix @ diff\n\n\n# Target moments from data\ntarget_mean = ipo_data[\"underpricing\"].mean()\ntarget_var = ipo_data[\"underpricing\"].var()\n\n# Estimate\nresult = minimize(\n    rock_model_objective,\n    x0=[0.3, 0.5],\n    args=([target_mean, target_var],),\n    method=\"Nelder-Mead\",\n    options={\"maxiter\": 5000}\n)\n\nmu_hat, sigma_v_hat = result.x\nprint(f\"Rock Model Estimates:\")\nprint(f\"  Fraction informed (μ): {mu_hat:.4f}\")\nprint(f\"  Value uncertainty (σ_v): {sigma_v_hat:.4f}\")\nprint(f\"  Objective value: {result.fun:.6f}\")\n\n\n\n\nipo_plot = ipo_data.dropna(subset=[\"underpricing\", \"mechanism\"]).copy()\nipo_plot[\"year\"] = pd.to_datetime(ipo_plot[\"ipo_date\"]).dt.year\n\nannual_underpricing = (\n    ipo_plot.groupby([\"year\", \"mechanism\"])\n    .agg(\n        mean_up=(\"underpricing\", \"mean\"),\n        n=(\"underpricing\", \"count\")\n    )\n    .reset_index()\n    .query(\"n &gt;= 3\")\n)\n\n(\n    p9.ggplot(annual_underpricing, p9.aes(\n        x=\"year\", y=\"mean_up\", color=\"mechanism\"\n    ))\n    + p9.geom_line(size=1)\n    + p9.geom_point(p9.aes(size=\"n\"))\n    + p9.geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\")\n    + p9.scale_color_manual(values=[\"#2E5090\", \"#C0392B\", \"#27AE60\"])\n    + p9.scale_y_continuous(labels=percent_format())\n    + p9.labs(\n        x=\"Year\",\n        y=\"Average First-Day Return\",\n        title=\"IPO Underpricing by Allocation Mechanism\",\n        color=\"Mechanism\", size=\"# IPOs\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 6))\n)\n\n\nFigure 43.5\n\n\n\n\n\n43.4.4 Counterfactual: Welfare Under Alternative Mechanisms\nWith the structural parameters \\((\\hat{\\mu}, \\hat{\\sigma}_v)\\) in hand, we can simulate counterfactual outcomes. For instance: if all Vietnamese IPOs used uniform-price auctions instead of book building, how would underpricing and welfare change?\n\ndef simulate_ipo_mechanism(mu, sigma_v, mechanism=\"book_building\",\n                           n_sim=50000):\n    \"\"\"\n    Simulate IPO outcomes under different mechanisms.\n\n    Returns issuer surplus, informed profit, uninformed profit.\n    \"\"\"\n    np.random.seed(42)\n    v = np.random.lognormal(mean=0, sigma=sigma_v, size=n_sim)\n\n    if mechanism == \"book_building\":\n        # Price partially reveals information\n        # P = E[v] + 0.5 * (v - E[v]) * signal_quality\n        signal = v + np.random.normal(0, sigma_v * 0.5, n_sim)\n        P = np.exp(sigma_v**2 / 2) + 0.3 * (signal - np.exp(sigma_v**2 / 2))\n        P = P.clip(min=0.1)\n\n    elif mechanism == \"auction\":\n        # Competitive bidding: P closer to v\n        noise = np.random.normal(0, sigma_v * 0.3, n_sim)\n        P = v + noise\n        P = P.clip(min=0.1)\n\n    elif mechanism == \"fixed_price\":\n        # Fixed at E[v] * discount\n        P = np.full(n_sim, np.exp(sigma_v**2 / 2) * 0.85)\n\n    underpricing = (v - P) / P\n\n    issuer_surplus = P  # Revenue per share\n    investor_surplus = v - P  # Profit per share\n\n    return {\n        \"mechanism\": mechanism,\n        \"mean_underpricing\": underpricing.mean(),\n        \"median_underpricing\": np.median(underpricing),\n        \"std_underpricing\": underpricing.std(),\n        \"issuer_revenue\": issuer_surplus.mean(),\n        \"investor_profit\": investor_surplus.mean(),\n        \"money_left\": (investor_surplus[investor_surplus &gt; 0]).sum() / n_sim\n    }\n\n# Compare mechanisms\ncounterfactual = pd.DataFrame([\n    simulate_ipo_mechanism(mu_hat, sigma_v_hat, \"book_building\"),\n    simulate_ipo_mechanism(mu_hat, sigma_v_hat, \"auction\"),\n    simulate_ipo_mechanism(mu_hat, sigma_v_hat, \"fixed_price\")\n]).set_index(\"mechanism\").round(4)\n\ncounterfactual",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Structural Models in Finance</span>"
    ]
  },
  {
    "objectID": "63_structural_models_finance.html#limit-order-book-models",
    "href": "63_structural_models_finance.html#limit-order-book-models",
    "title": "43  Structural Models in Finance",
    "section": "43.5 Limit Order Book Models",
    "text": "43.5 Limit Order Book Models\n\n43.5.1 Order Submission as a Strategic Choice\nIn a limit order market, every trader faces a fundamental tradeoff: submit a market order (immediate execution, but at an adverse price) or a limit order (better price, but risk of non-execution). Parlour (1998) models this as a sequential game where each trader’s optimal strategy depends on the current state of the order book.\nLet \\(a_t\\) and \\(b_t\\) denote the best ask and bid prices at time \\(t\\), with the spread \\(s_t = a_t - b_t\\). A buyer who arrives at time \\(t\\) chooses between:\n\nMarket buy: Execute immediately at \\(a_t\\). Cost: \\(a_t - v_t\\) where \\(v_t\\) is the true value.\nLimit buy at \\(b_t\\): Provides liquidity. If executed, profit: \\(v_t - b_t\\). Probability of execution: \\(\\pi(b_t, \\text{book state})\\).\n\nThe buyer submits a market order if:\n\\[\na_t - v_t &lt; (1 - \\pi_t)(v_t - b_t) + \\pi_t \\cdot 0\n\\tag{43.16}\\]\nRearranging: market orders are optimal when the spread is narrow relative to the non-execution risk of limit orders. This generates the empirically observed pattern that limit orders are more attractive when spreads are wide and the book is thin.\n\n\n43.5.2 The Glosten-Milgrom Model\nGlosten and Milgrom (1985) provide the foundational structural model of bid-ask spread determination under asymmetric information. A competitive market maker sets bid and ask prices to break even in expectation, recognizing that some trades come from informed traders.\nLet \\(\\mu\\) denote the probability that an incoming order is from an informed trader, and let \\(V^H\\) and \\(V^L\\) denote the high and low values of the asset (\\(\\Pr(V = V^H) = p\\)). The zero-profit conditions yield:\n\\[\na = E[V \\mid \\text{buy order}] = \\frac{p(1-\\mu) + p\\mu}{(1-\\mu) + p\\mu} V^H + \\frac{(1-p)(1-\\mu)}{(1-\\mu) + p\\mu} V^L\n\\tag{43.17}\\]\n\\[\nb = E[V \\mid \\text{sell order}] = \\frac{p(1-\\mu)}{(1-\\mu) + (1-p)\\mu} V^H + \\frac{(1-p)(1-\\mu) + (1-p)\\mu}{(1-\\mu) + (1-p)\\mu} V^L\n\\tag{43.18}\\]\nThe spread \\(s = a - b\\) is positive whenever \\(\\mu &gt; 0\\), and increases in both the proportion of informed traders (\\(\\mu\\)) and the information asymmetry (\\(V^H - V^L\\)). This decomposition is foundational: the spread compensates liquidity providers for the adverse selection cost of trading with informed counterparties.\n\n\n43.5.3 PIN: Probability of Informed Trading\nEasley et al. (1996) extend the Glosten-Milgrom framework to a dynamic setting and develop the Probability of Informed Trading (PIN) measure, which can be estimated from trade data. The model assumes that on each trading day, an information event occurs with probability \\(\\alpha\\). Conditional on an event, it is bad news with probability \\(\\delta\\). Informed traders arrive at rate \\(\\mu\\); uninformed buyers and sellers arrive at rates \\(\\varepsilon_b\\) and \\(\\varepsilon_s\\), respectively.\nThe likelihood of observing \\(B_t\\) buys and \\(S_t\\) sells on day \\(t\\) is:\n\\[\n\\mathcal{L}(B_t, S_t | \\boldsymbol{\\theta}) = (1-\\alpha) f(B_t|\\varepsilon_b) f(S_t|\\varepsilon_s) + \\alpha\\delta \\cdot f(B_t|\\varepsilon_b) f(S_t|\\varepsilon_s + \\mu) + \\alpha(1-\\delta) \\cdot f(B_t|\\varepsilon_b + \\mu) f(S_t|\\varepsilon_s)\n\\tag{43.19}\\]\nwhere \\(f(\\cdot|\\lambda)\\) is the Poisson density with rate \\(\\lambda\\), and \\(\\boldsymbol{\\theta} = (\\alpha, \\delta, \\mu, \\varepsilon_b, \\varepsilon_s)\\). PIN is then:\n\\[\n\\text{PIN} = \\frac{\\alpha \\mu}{\\alpha \\mu + \\varepsilon_b + \\varepsilon_s}\n\\tag{43.20}\\]\n\ndef pin_log_likelihood(params, data):\n    \"\"\"\n    Compute negative log-likelihood for the Easley-Kiefer-O'Hara-Paperman\n    (1996) PIN model.\n\n    Parameters\n    ----------\n    params : array\n        [alpha, delta, mu, eps_b, eps_s]\n    data : DataFrame\n        Columns: buys, sells (daily counts).\n\n    Returns\n    -------\n    float : Negative log-likelihood.\n    \"\"\"\n    alpha, delta, mu, eps_b, eps_s = params\n\n    # Parameter bounds\n    if (alpha &lt; 0 or alpha &gt; 1 or delta &lt; 0 or delta &gt; 1 or\n        mu &lt; 0 or eps_b &lt; 0 or eps_s &lt; 0):\n        return 1e15\n\n    B = data[\"buys\"].values\n    S = data[\"sells\"].values\n\n    # Use log-sum-exp for numerical stability\n    # Three components: no event, bad news, good news\n    log_L = np.zeros(len(B))\n\n    for i in range(len(B)):\n        b, s = B[i], S[i]\n\n        # Log-Poisson components\n        def log_poisson(k, lam):\n            if lam &lt;= 0:\n                return -1e10 if k &gt; 0 else 0\n            return k * np.log(lam) - lam - np.sum(np.log(np.arange(1, k + 1)))\n\n        # No event\n        c1 = (np.log(1 - alpha + 1e-15) +\n              log_poisson(b, eps_b) + log_poisson(s, eps_s))\n\n        # Bad news (extra sells)\n        c2 = (np.log(alpha * delta + 1e-15) +\n              log_poisson(b, eps_b) + log_poisson(s, eps_s + mu))\n\n        # Good news (extra buys)\n        c3 = (np.log(alpha * (1 - delta) + 1e-15) +\n              log_poisson(b, eps_b + mu) + log_poisson(s, eps_s))\n\n        # Log-sum-exp\n        max_c = max(c1, c2, c3)\n        log_L[i] = max_c + np.log(\n            np.exp(c1 - max_c) + np.exp(c2 - max_c) + np.exp(c3 - max_c)\n        )\n\n    return -np.sum(log_L)\n\n\ndef estimate_pin(buys, sells, n_starts=10):\n    \"\"\"\n    Estimate PIN via MLE with multiple random starts.\n\n    Returns\n    -------\n    dict : Estimated parameters and PIN value.\n    \"\"\"\n    data = pd.DataFrame({\"buys\": buys, \"sells\": sells})\n    best_result = None\n    best_nll = np.inf\n\n    avg_b = data[\"buys\"].mean()\n    avg_s = data[\"sells\"].mean()\n\n    for _ in range(n_starts):\n        # Random initial values\n        alpha0 = np.random.uniform(0.1, 0.8)\n        delta0 = np.random.uniform(0.2, 0.8)\n        mu0 = np.random.uniform(0, max(avg_b, avg_s))\n        eps_b0 = avg_b * np.random.uniform(0.5, 1.5)\n        eps_s0 = avg_s * np.random.uniform(0.5, 1.5)\n\n        try:\n            result = minimize(\n                pin_log_likelihood,\n                x0=[alpha0, delta0, mu0, eps_b0, eps_s0],\n                args=(data,),\n                method=\"L-BFGS-B\",\n                bounds=[(0.01, 0.99), (0.01, 0.99),\n                        (0.01, None), (0.01, None), (0.01, None)],\n                options={\"maxiter\": 1000}\n            )\n\n            if result.fun &lt; best_nll:\n                best_nll = result.fun\n                best_result = result\n        except Exception:\n            continue\n\n    if best_result is None:\n        return {\"pin\": np.nan}\n\n    alpha, delta, mu, eps_b, eps_s = best_result.x\n    pin = alpha * mu / (alpha * mu + eps_b + eps_s)\n\n    return {\n        \"alpha\": alpha,\n        \"delta\": delta,\n        \"mu\": mu,\n        \"eps_b\": eps_b,\n        \"eps_s\": eps_s,\n        \"pin\": pin,\n        \"log_likelihood\": -best_nll\n    }\n\n\n# Estimate PIN for a cross-section of stocks\n# Using daily buy/sell counts from Lee-Ready classification\ntrade_counts = dc.get_trade_counts(\n    start_date=\"2023-01-01\",\n    end_date=\"2023-12-31\"\n)\n\n# Sample of stocks for estimation (PIN is computationally intensive)\ntop_stocks = (\n    trade_counts.groupby(\"ticker\")\n    .agg(n_days=(\"date\", \"nunique\"))\n    .query(\"n_days &gt;= 200\")\n    .index[:50]\n)\n\npin_estimates = []\nfor ticker in top_stocks:\n    stock_data = trade_counts[trade_counts[\"ticker\"] == ticker]\n    result = estimate_pin(\n        stock_data[\"buys\"].values,\n        stock_data[\"sells\"].values,\n        n_starts=5\n    )\n    result[\"ticker\"] = ticker\n    pin_estimates.append(result)\n\npin_df = pd.DataFrame(pin_estimates).dropna(subset=[\"pin\"])\nprint(f\"PIN estimates for {len(pin_df)} stocks\")\nprint(f\"  Mean PIN: {pin_df['pin'].mean():.4f}\")\nprint(f\"  Median PIN: {pin_df['pin'].median():.4f}\")\n\n\n\n\n(\n    p9.ggplot(pin_df, p9.aes(x=\"pin\"))\n    + p9.geom_histogram(bins=25, fill=\"#2E5090\", alpha=0.7)\n    + p9.geom_vline(\n        xintercept=pin_df[\"pin\"].median(),\n        linetype=\"dashed\", color=\"#C0392B\", size=0.8\n    )\n    + p9.labs(\n        x=\"PIN\",\n        y=\"Count\",\n        title=\"Distribution of Informed Trading Probability\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n\n\nFigure 43.6\n\n\n\n\n\n43.5.4 PIN and Asset Pricing\nEasley, Hvidkjaer, and O’hara (2002) argue that PIN should be priced in the cross-section of expected returns: stocks with higher information asymmetry require a risk premium to compensate uninformed investors. We test this prediction using Fama-MacBeth regressions.\n\n# Merge PIN with firm characteristics and forward returns\npin_chars = pin_df[[\"ticker\", \"pin\"]].merge(\n    dc.get_firm_characteristics(\n        start_date=\"2023-01-01\",\n        end_date=\"2023-12-31\"\n    ).groupby(\"ticker\").last().reset_index()[\n        [\"ticker\", \"log_size\", \"book_to_market\", \"momentum_12m\"]\n    ],\n    on=\"ticker\",\n    how=\"inner\"\n)\n\n# Forward 12-month returns (2024)\nforward_returns = dc.get_annual_returns(\n    start_date=\"2024-01-01\",\n    end_date=\"2024-12-31\"\n)\n\npin_returns = pin_chars.merge(\n    forward_returns[[\"ticker\", \"annual_ret\"]],\n    on=\"ticker\",\n    how=\"inner\"\n)\n\n# Cross-sectional regression\nif len(pin_returns) &gt; 20:\n    cs_model = sm.OLS(\n        pin_returns[\"annual_ret\"],\n        sm.add_constant(\n            pin_returns[[\"pin\", \"log_size\", \"book_to_market\", \"momentum_12m\"]]\n        )\n    ).fit(cov_type=\"HC1\")\n\n    print(\"Cross-Sectional Return Regression with PIN:\")\n    for var in [\"pin\", \"log_size\", \"book_to_market\", \"momentum_12m\"]:\n        print(f\"  {var}: {cs_model.params[var]:.4f} \"\n              f\"(t = {cs_model.tvalues[var]:.3f})\")\n\n\n\n43.5.5 Estimation Challenges in Limit Order Book Models\nStructural estimation of limit order book models faces several challenges specific to Vietnamese markets:\nTick size effects. Vietnamese exchanges use discrete tick sizes that vary with price level. At low prices, the minimum tick size represents a large fraction of the price, artificially widening the spread and distorting the structural parameters. The standard Glosten-Milgrom and PIN models assume continuous prices; adapting them to discrete price grids requires the modifications proposed by Bollen, Smith, and Whaley (2004).\nPrice limits. When a stock hits its price limit, the order book is effectively frozen at one side. Orders accumulate but cannot execute until the limit is lifted. This creates a censoring problem analogous to the price limit censoring in return distributions: the observed order flow is a truncated version of the latent flow.\nMissing data. Full order book snapshots at high frequency are not universally available for Vietnamese stocks. PIN estimation requires only daily buy/sell counts, making it feasible with lower-frequency data. But richer models (Foucault (1999) dynamic limit order book, Roşu (2009) continuous-time model) require tick-by-tick order book data that may be unavailable for smaller stocks.\nInstitutional features. The coexistence of HOSE (continuous auction) and HNX (with periodic call auctions for less liquid stocks) creates heterogeneity in the appropriate structural model. A model estimated on HOSE continuous trading data does not directly apply to HNX call auction stocks.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Structural Models in Finance</span>"
    ]
  },
  {
    "objectID": "63_structural_models_finance.html#when-structural-models-are-worth-it",
    "href": "63_structural_models_finance.html#when-structural-models-are-worth-it",
    "title": "43  Structural Models in Finance",
    "section": "43.6 When Structural Models Are Worth It",
    "text": "43.6 When Structural Models Are Worth It\n\n43.6.1 Decision Framework\nStructural estimation is not always the right tool. The additional complexity, data requirements, and model risk must be justified by the research question. Table 43.6 provides a decision framework.\n\n\n\nTable 43.6: When to Choose Structural vs. Reduced Form\n\n\n\n\n\n\n\n\n\n\nCriterion\nStructural Preferred\nReduced Form Preferred\n\n\n\n\nResearch question\nCounterfactual or policy evaluation\nCausal effect of observed variation\n\n\nData availability\nRich micro-data (transactions, holdings)\nStandard panel (returns, fundamentals)\n\n\nInstitutional environment\nStable (model assumptions plausible)\nRapidly changing (model may be misspecified)\n\n\nNumber of parameters\nFew primitives, many observables\nMany parameters, few identifying assumptions\n\n\nPublication venue\nJF, RFS, Econometrica, JPE\nJFE, RFS, MS, JFQA\n\n\nComputational budget\nWeeks to months of estimation\nHours to days\n\n\n\n\n\n\n\n\n43.6.2 Data Requirements\nStructural models are data-hungry. Table 43.7 summarizes the minimum data needs for each model class covered in this chapter.\n\n\n\nTable 43.7: Data Requirements for Structural Estimation\n\n\n\n\n\n\n\n\n\n\n\nModel\nMinimum Data\nIdeal Data\nVietnamese Availability\n\n\n\n\nKY demand system\nQuarterly institutional holdings + prices\n13F-equivalent filings + fund flows\nPartial (semi-annual disclosure)\n\n\nAlmgren-Chriss\nDaily volume, spread, volatility\nIntraday order-by-order data\nGood (HOSE provides)\n\n\nRock/IPO\nOffer price, first-day close, allocation\nInvestor-level bids and allocations\nLimited (auction data available)\n\n\nPIN\nDaily buy/sell counts (Lee-Ready)\nTick-by-tick trade and quote\nModerate (requires trade classification)\n\n\nGlosten-Milgrom\nBest bid/ask, trade direction\nFull order book snapshots\nModerate (top-of-book available)\n\n\n\n\n\n\n\n\n43.6.3 Computational Cost\nStructural estimation is orders of magnitude more expensive than reduced-form regression. The cost arises from three sources:\nLikelihood evaluation. Each evaluation of the structural likelihood or moment function requires solving the model for given parameters. For dynamic models (e.g., inventory models, dynamic discrete choice), this involves solving a dynamic programming problem at each iteration.\nOptimization. The GMM or MLE objective is typically non-convex, requiring multiple starting points and global optimization algorithms. The PIN model with 5 parameters requires \\(\\sim 10\\) random starts; a richer model with \\(\\sim 20\\) parameters might require hundreds.\nInference. Standard errors for structural parameters often require bootstrapping (because the asymptotic distribution is non-standard or the delta method is unreliable), adding a multiplicative factor of 200–1000 to the computational budget.\n\n# Illustration: computational cost comparison\nimport time\n\n# Reduced form: OLS regression\nn_obs = 100000\nX_sim = np.random.randn(n_obs, 5)\ny_sim = X_sim @ np.random.randn(5) + np.random.randn(n_obs)\n\nstart = time.time()\nfor _ in range(100):\n    sm.OLS(y_sim, sm.add_constant(X_sim)).fit()\nols_time = (time.time() - start) / 100\n\n# Structural: PIN estimation (single stock)\nsim_buys = np.random.poisson(50, 250)\nsim_sells = np.random.poisson(45, 250)\n\nstart = time.time()\npin_result = estimate_pin(sim_buys, sim_sells, n_starts=5)\npin_time = time.time() - start\n\nprint(f\"Computational Cost Comparison:\")\nprint(f\"  OLS (100k obs): {ols_time*1000:.1f} ms per estimation\")\nprint(f\"  PIN (250 days, 5 starts): {pin_time:.1f} s per stock\")\nprint(f\"  Ratio: {pin_time / ols_time:.0f}x\")\n\nComputational Cost Comparison:\n  OLS (100k obs): 42.7 ms per estimation\n  PIN (250 days, 5 starts): 26.9 s per stock\n  Ratio: 632x\n\n\n\n\n43.6.4 Interpretation Risks\nThe primary risk of structural estimation is model misspecification. If the model is wrong, the estimated parameters have no economic meaning and the counterfactual predictions are unreliable. Several strategies mitigate this risk:\nSpecification tests. Over-identifying restrictions (more moments than parameters) provide the Hansen \\(J\\)-test for model fit. A rejection suggests misspecification, though the test has low power in small samples.\nModel comparison. Estimate multiple nested or non-nested models and compare fit. If a simpler model fits the data equally well, prefer it (Occam’s razor).\nSensitivity analysis. Report how structural parameters change under plausible alternative assumptions (e.g., different functional forms for the impact function, different distributional assumptions for valuations).\nOut-of-sample validation. Estimate the model on one sample period and test its predictions on a holdout sample. Structural models that fit in-sample but fail out-of-sample are likely overfit.\n\n\n43.6.5 Journal Expectations\nStructural papers in top finance journals are held to specific standards:\nIdentification clarity. The paper must clearly state which parameters are identified, from which moments, and what variation in the data provides identification. The Rust (1987) critique that without clear identification, structural estimation is curve-fitting, must be addressed head-on.\nCounterfactual credibility. Counterfactual exercises should be economically motivated and the range of counterfactual scenarios should not extrapolate far beyond the data.\nRobustness to specification. Reviewers will ask what happens under alternative distributional assumptions, alternative moment conditions, and alternative model features.\nTransparency. Code and data should be made available. Structural estimation is sufficiently complex that replicability is a first-order concern.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Structural Models in Finance</span>"
    ]
  },
  {
    "objectID": "63_structural_models_finance.html#summary",
    "href": "63_structural_models_finance.html#summary",
    "title": "43  Structural Models in Finance",
    "section": "43.7 Summary",
    "text": "43.7 Summary\nThis chapter introduced structural estimation as a distinct methodology for financial economics, one that trades the simplicity and transparency of reduced-form analysis for the ability to estimate economic primitives and conduct counterfactual policy evaluation. We implemented four classes of structural models: demand systems for financial assets, optimal execution and transaction cost models, IPO auction models, and limit order book models of informed trading.\nThe demand system approach of Koijen and Yogo (2019) reveals that institutional investors in Vietnam exhibit heterogeneous demand elasticities across characteristics, with foreign investors showing different sensitivity patterns than domestic institutions. Market impact estimation confirms the universality of the square-root law, while the Almgren-Chriss framework provides the optimal execution benchmark for large institutional orders. The Rock IPO model quantifies the information asymmetry premium embedded in Vietnamese IPO underpricing, and the PIN model estimates the probability of informed trading across the cross-section.\nEach model involves a tradeoff between the economic questions it can answer and the assumptions it requires. The decision to use structural estimation should be driven by the research question (specifically, whether counterfactual analysis is essential) and tempered by honest assessment of whether the model’s assumptions are sufficiently credible in the Vietnamese institutional environment. When they are, structural models provide insights that no amount of reduced-form regression can deliver.\n\n\n\n\n\n\nAlmgren, Robert, and Neil Chriss. 2001. “Optimal Execution of Portfolio Transactions.” Journal of Risk 3: 5–40.\n\n\nBenveniste, Lawrence M, and Paul A Spindt. 1989. “How Investment Bankers Determine the Offer Price and Allocation of New Issues.” Journal of Financial Economics 24 (2): 343–61.\n\n\nBerry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile Prices in Market Equilibrium.” Econometrica 63 (4): 841–90.\n\n\nBollen, Nicolas PB, Tom Smith, and Robert E Whaley. 2004. “Modeling the Bid/Ask Spread: Measuring the Inventory-Holding Premium.” Journal of Financial Economics 72 (1): 97–141.\n\n\nEasley, David, Soeren Hvidkjaer, and Maureen O’hara. 2002. “Is Information Risk a Determinant of Asset Returns?” The Journal of Finance 57 (5): 2185–2221.\n\n\nEasley, David, Nicholas M Kiefer, Maureen O’hara, and Joseph B Paperman. 1996. “Liquidity, Information, and Infrequently Traded Stocks.” The Journal of Finance 51 (4): 1405–36.\n\n\nFoucault, Thierry. 1999. “Order Flow Composition and Trading Costs in a Dynamic Limit Order Market.” Journal of Financial Markets 2 (2): 99–134.\n\n\nGabaix, Xavier, Parameswaran Gopikrishnan, Vasiliki Plerou, and H Eugene Stanley. 2003. “A Theory of Power-Law Distributions in Financial Market Fluctuations.” Nature 423 (6937): 267–70.\n\n\nGabaix, Xavier, and Ralph SJ Koijen. 2021. “In Search of the Origins of Financial Fluctuations: The Inelastic Markets Hypothesis.” National Bureau of Economic Research.\n\n\nGlosten, Lawrence R, and Paul R Milgrom. 1985. “Bid, Ask and Transaction Prices in a Specialist Market with Heterogeneously Informed Traders.” Journal of Financial Economics 14 (1): 71–100.\n\n\nHasbrouck, Joel. 1991. “Measuring the Information Content of Stock Trades.” The Journal of Finance 46 (1): 179–207.\n\n\n———. 2009. “Trading Costs and Returns for US Equities: Estimating Effective Costs from Daily Data.” The Journal of Finance 64 (3): 1445–77.\n\n\nKoijen, Ralph SJ, and Motohiro Yogo. 2019. “A Demand System Approach to Asset Pricing.” Journal of Political Economy 127 (4): 1475–515.\n\n\nKyle, Albert S. 1985. “Continuous Auctions and Insider Trading.” Econometrica: Journal of the Econometric Society, 1315–35.\n\n\nParlour, Christine A. 1998. “Price Dynamics in Limit Order Markets.” The Review of Financial Studies 11 (4): 789–816.\n\n\nRock, Kevin. 1986. “Why New Issues Are Underpriced.” Journal of Financial Economics 15 (1-2): 187–212.\n\n\nRoll, Richard. 1984. “A Simple Implicit Measure of the Effective Bid-Ask Spread in an Efficient Market.” The Journal of Finance 39 (4): 1127–39.\n\n\nRoşu, Ioanid. 2009. “A Dynamic Model of the Limit Order Book.” The Review of Financial Studies 22 (11): 4601–41.\n\n\nRust, John. 1987. “Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher.” Econometrica: Journal of the Econometric Society, 999–1033.",
    "crumbs": [
      "Home",
      "Mô hình tài chính nâng cao",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Structural Models in Finance</span>"
    ]
  },
  {
    "objectID": "70_textual.html",
    "href": "70_textual.html",
    "title": "44  Textual Analysis",
    "section": "",
    "text": "44.1 Why Textual Analysis for Vietnamese Finance?\nTextual analysis has emerged as one of the most productive research frontiers in empirical finance over the past two decades. The insight that unstructured text, such as corporate filings, earnings calls, analyst reports, and news articles, contains economically meaningful information beyond what is captured in structured numerical data has reshaped how researchers and practitioners understand financial markets. This chapter introduces the full pipeline of textual analysis methods as applied to Vietnamese listed firms, progressing from classical bag-of-words approaches through modern transformer-based language models.\nThe Vietnamese equity market presents unique opportunities and challenges for textual analysis. As of 2024, the Ho Chi Minh Stock Exchange (HOSE) and the Hanoi Stock Exchange (HNX) together list over 1,600 securities with a combined market capitalization exceeding VND 6,000 trillion (approximately USD 240 billion). Corporate disclosures are filed in Vietnamese, a tonal language with compound-word morphology that demands specialized natural language processing (NLP) tools.\nWe build on the seminal contributions of Loughran and McDonald (2011) in domain-specific sentiment lexicons, Hoberg and Phillips (2016) in text-based industry classification, and the modern deep learning revolution initiated by Devlin et al. (2019). This chapter covers the following topics:\nThe Vietnamese financial market has several characteristics that make textual analysis particularly valuable. First, analyst coverage is sparse (fewer than 30% of listed firms receive regular coverage from sell-side analysts), making alternative information sources critical. Second, the regulatory environment is evolving rapidly, with the State Securities Commission (SSC) continuously updating disclosure requirements, creating rich variation in information environments across firms and time. Third, the market is dominated by retail investors (accounting for roughly 80% of trading volume), who may process textual information differently than institutional investors, creating potential mispricings that text-based strategies could exploit.\nFrom a methodological standpoint, Vietnamese poses interesting NLP challenges. Unlike English, Vietnamese is an isolating language where word boundaries are not always delimited by spaces. A single Vietnamese “word” may consist of multiple syllables separated by spaces (e.g., “công ty” for “company,” “thị trường” for “market”). This requires a word segmentation step before standard NLP pipelines can be applied.1",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-lit-finance",
    "href": "70_textual.html#sec-textual-lit-finance",
    "title": "44  Textual Analysis",
    "section": "45.1 Textual Analysis in Finance",
    "text": "45.1 Textual Analysis in Finance\nThe application of textual analysis to financial data has a rich history. Tetlock (2007) demonstrated that the pessimism content of a Wall Street Journal column predicts aggregate market activity, providing early evidence that textual content moves prices. Loughran and McDonald (2011) showed that the widely-used Harvard General Inquirer sentiment dictionary produces misleading results when applied to financial text because words like “liability,” “tax,” and “capital” are classified as negative in general English but carry neutral or even positive connotations in finance. Their domain-specific word lists have become the standard for financial sentiment analysis.2\nHoberg and Phillips (2010) and Hoberg and Phillips (2016) pioneered the use of product descriptions from 10-K filings to construct text-based industry classifications (TNIC), demonstrating that these dynamic, firm-specific industry definitions outperform static SIC and NAICS codes in explaining firm behavior, including profitability, stock returns, and M&A activity. Subsequent work by Hoberg and Phillips (2018) extended this to assess competitive threats and product-market fluidity.\nMore recent work has leveraged advances in deep learning. Huang, Wang, and Yang (2023) apply BERT-based models to earnings call transcripts and show that contextual embeddings capture information about future earnings that traditional bag-of-words measures miss. Jha et al. (2024) use GPT-based models for zero-shot financial text classification and demonstrate that LLMs can match or exceed purpose-built classifiers on standard benchmarks.",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-lit-vnlp",
    "href": "70_textual.html#sec-textual-lit-vnlp",
    "title": "44  Textual Analysis",
    "section": "45.2 NLP for Vietnamese Language",
    "text": "45.2 NLP for Vietnamese Language\nVietnamese NLP has advanced significantly with the development of VnCoreNLP (Vu et al. 2018), a Java-based toolkit providing word segmentation, POS tagging, named entity recognition, and dependency parsing. The underthesea library offers a Python-native alternative. Most critically for financial applications, PhoBERT (Nguyen and Nguyen 2020) provides Vietnamese-specific BERT pre-training on a 20GB corpus, achieving state-of-the-art results on multiple Vietnamese NLP tasks.\n\n\n\nTable 45.1: Key Literature on Textual Analysis in Finance\n\n\n\n\n\n\n\n\n\n\n\nStudy\nMethod\nKey Finding\nRelevance to Vietnam\n\n\n\n\nTetlock (2007)\nDictionary-based sentiment from WSJ column\nMedia pessimism predicts market activity and returns\nBaseline for Vietnamese financial news sentiment\n\n\nLoughran and McDonald (2011)\nDomain-specific financial dictionaries\nGeneral dictionaries misclassify 73% of negative financial words\nNeed for Vietnamese financial sentiment lexicon\n\n\nHoberg and Phillips (2016)\nCosine similarity on 10-K product descriptions\nText-based industries outperform SIC/NAICS\nPeer identification for Vietnamese firms using business descriptions\n\n\nNguyen and Nguyen (2020)\nPhoBERT: Vietnamese BERT pre-training\nSOTA on Vietnamese NLP benchmarks\nFoundation model for Vietnamese financial NLP\n\n\nHuang, Wang, and Yang (2023)\nBERT embeddings on earnings calls\nContextual embeddings predict future earnings beyond BoW\nApply to Vietnamese earnings call transcripts\n\n\nJha et al. (2024)\nGPT-based zero-shot financial classification\nLLMs match fine-tuned classifiers\nZero-shot Vietnamese financial text classification via multilingual LLMs",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-universe",
    "href": "70_textual.html#sec-textual-universe",
    "title": "44  Textual Analysis",
    "section": "46.1 Constructing the Universe",
    "text": "46.1 Constructing the Universe\nWe construct the universe of Vietnamese listed firms. The universe includes all firms listed on HOSE, HNX, and UPCoM as of the analysis date.\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport unicodedata\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nfrom typing import List, Dict, Tuple, Optional\n\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\n# Plotting configuration\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['font.size'] = 11\nsns.set_style(\"whitegrid\")\n\n\nfrom datacore import DataCoreAPI  # DataCore.vn Python client\n\n# Initialize connection\ndc = DataCoreAPI(api_key='YOUR_API_KEY')\n\n# Retrieve universe of all listed firms\nuniverse = dc.get_listed_firms(\n    exchanges=['HOSE', 'HNX', 'UPCOM'],\n    as_of='2024-12-31',\n    fields=[\n        'ticker', 'company_name', 'company_name_en',\n        'exchange', 'listing_date', 'delisting_date',\n        'icb_industry', 'icb_sector', 'icb_subsector',\n        'market_cap', 'total_assets', 'revenue'\n    ]\n)\n\nprint(f'Total listed firms: {len(universe)}')\nprint(f'HOSE: {len(universe[universe.exchange==\"HOSE\"])}')\nprint(f'HNX: {len(universe[universe.exchange==\"HNX\"])}')\nprint(f'UPCoM: {len(universe[universe.exchange==\"UPCOM\"])}')\n\n\n\n\nTable 46.1: Universe of Vietnamese Listed Firms by Exchange (as of December 2024)\n\n\n\n\n\n\n\n\n\n\n\n\nExchange\nN Firms\nAvg Mkt Cap (VND bn)\nMedian Mkt Cap (VND bn)\nTotal Mkt Cap (VND tn)\n\n\n\n\nHOSE\n403\n12,847\n3,215\n5,177\n\n\nHNX\n334\n2,156\n687\n720\n\n\nUPCoM\n868\n1,043\n298\n905\n\n\nTotal\n1,605\n4,239\n712\n6,802",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-bus-desc",
    "href": "70_textual.html#sec-textual-bus-desc",
    "title": "44  Textual Analysis",
    "section": "46.2 Retrieving Business Descriptions",
    "text": "46.2 Retrieving Business Descriptions\nBusiness descriptions for all listed firms can be in both Vietnamese and English. We retrieve both versions for our analysis. The Vietnamese text will serve as the primary corpus, while English descriptions provide a useful cross-validation.\n\n# Get business descriptions (Vietnamese and English)\nbus_desc = dc.get_business_descriptions(\n    tickers=universe.ticker.tolist(),\n    fields=[\n        'ticker', 'bus_desc_vi', 'bus_desc_en',\n        'main_business', 'products_services',\n        'year_established', 'num_employees'\n    ]\n)\n\n# Merge with universe\ncorpus_df = universe.merge(bus_desc, on='ticker', how='inner')\n\n# Summary statistics on text length\ncorpus_df['desc_len_vi'] = corpus_df.bus_desc_vi.str.len()\ncorpus_df['desc_len_en'] = corpus_df.bus_desc_en.str.len()\ncorpus_df['word_count_vi'] = corpus_df.bus_desc_vi.str.split().str.len()\n\nprint(corpus_df[['desc_len_vi', 'desc_len_en', 'word_count_vi']]\n      .describe().round(0))\n\n\n\n\nTable 46.2: Descriptive Statistics of Business Description Text\n\n\n\n\n\nStatistic\nMean\nMedian\nStd Dev\nMin\nMax\n\n\n\n\nCharacters (VN)\n2,847\n2,156\n1,923\n87\n18,432\n\n\nCharacters (EN)\n3,412\n2,689\n2,245\n102\n22,156\n\n\nWords (VN)\n487\n372\n318\n15\n3,216",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-annual-text",
    "href": "70_textual.html#sec-textual-annual-text",
    "title": "44  Textual Analysis",
    "section": "46.3 Retrieving Annual Report Text",
    "text": "46.3 Retrieving Annual Report Text\nBeyond business descriptions, annual or quarterly reports provide richer and more time-varying textual data. We extract the Management Discussion and Analysis (MD&A) sections, which are most informative for financial analysis (Li et al. 2010; Bonsall IV et al. 2017). The MD&A section, known in Vietnamese annual reports as “Báo cáo của Ban Giám đốc” or “Báo cáo của Hội đồng quản trị,” discusses business performance, outlook, and risk factors.\n\n# Get annual report MD&A sections (2015-2024)\nannual_text = dc.get_annual_report_text(\n    tickers=universe.ticker.tolist(),\n    years=range(2015, 2025),\n    sections=['mda', 'risk_factors', 'business_overview'],\n    language='vi'\n)\n\n# Panel structure: ticker x year x section\nprint(f'Total firm-year-section observations: {len(annual_text)}')\nprint(f'Unique firms: {annual_text.ticker.nunique()}')\nprint(f'Year range: {annual_text.year.min()}-{annual_text.year.max()}')\n\n# Calculate text changes year-over-year\nannual_text = annual_text.sort_values(['ticker', 'year'])\nannual_text['text_len'] = annual_text.text.str.len()\nannual_text['text_change_pct'] = (\n    annual_text.groupby('ticker')['text_len']\n    .pct_change() * 100\n)",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-segmentation",
    "href": "70_textual.html#sec-textual-segmentation",
    "title": "44  Textual Analysis",
    "section": "47.1 Vietnamese Word Segmentation",
    "text": "47.1 Vietnamese Word Segmentation\nThe most critical preprocessing step for Vietnamese text is word segmentation (phân đoạn từ). Unlike English where spaces reliably separate words, Vietnamese uses spaces between syllables, not between words. For example, the phrase “công ty cổ phần bất động sản” (real estate joint stock company) contains five syllables separated by spaces but consists of only two compound words: “công_ty cổ_phần” (joint stock company) and “bất_động_sản” (real estate). Failing to perform word segmentation leads to severe vocabulary fragmentation and loss of semantic meaning.\n\n\n\nTable 47.1: Vietnamese Word Segmentation Example\n\n\n\n\n\n\n\n\n\n\nStage\nText\nInterpretation\n\n\n\n\nRaw\ncông ty cổ phần thương mại dịch vụ\n7 syllables, ambiguous boundaries\n\n\nSegmented\ncông_ty cổ_phần thương_mại dịch_vụ\n4 words: company | joint-stock | commerce | services\n\n\n\n\n\n\n\nfrom underthesea import word_tokenize\n\ndef segment_vietnamese(text: str) -&gt; str:\n    \"\"\"Segment Vietnamese text into words using underthesea.\"\"\"\n    if pd.isna(text) or text.strip() == '':\n        return ''\n    # underthesea word_tokenize joins compound words with _\n    segmented = word_tokenize(text, format='text')\n    return segmented\n\n# Alternative: VnCoreNLP (Java-based, higher accuracy)\n# from vncorenlp import VnCoreNLP\n# vnlp = VnCoreNLP('VnCoreNLP-1.2.jar', annotators='wseg')\n# segmented = vnlp.tokenize(text)\n\n# Apply segmentation to corpus\ncorpus_df['bus_desc_segmented'] = (\n    corpus_df.bus_desc_vi.apply(segment_vietnamese)\n)\n\n# Example\nsample = corpus_df.iloc[0]\nprint('Raw:', sample.bus_desc_vi[:200])\nprint('Segmented:', sample.bus_desc_segmented[:200])",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-cleaning",
    "href": "70_textual.html#sec-textual-cleaning",
    "title": "44  Textual Analysis",
    "section": "47.2 Full Text Cleaning Pipeline",
    "text": "47.2 Full Text Cleaning Pipeline\nAfter word segmentation, we apply a cleaning pipeline. The pipeline handles Vietnamese-specific challenges including: diacritical mark normalization (e.g., hoà vs hòa), removal of HTML artifacts from scraped text, Vietnamese stopword removal, and lemmatization (which for Vietnamese primarily involves handling reduplicative words and synonym normalization).\n\n# Vietnamese stopwords (domain-adapted)\nVIETNAMESE_STOPWORDS = {\n    'có', 'là', 'và', 'của', 'cho', 'được', 'trong',\n    'các', 'những', 'với', 'từ', 'khi', 'hoặc',\n    'đã', 'sẽ', 'đang', 'để', 'này', 'đó',\n    'như', 'theo', 'về', 'bằng', 'tại', 'trên',\n    'cũng', 'rất', 'nhiều', 'ít', 'một', 'hai',\n    # Financial domain stopwords\n    'năm', 'quý', 'tháng', 'ngày', 'kỳ',\n    'việt_nam', 'tổng', 'giá_trị', 'triệu', 'tỷ',\n}\n\ndef clean_vietnamese_text(\n    text: str,\n    segment: bool = True,\n    remove_stops: bool = True,\n    lowercase: bool = True,\n    min_word_len: int = 2\n) -&gt; str:\n    \"\"\"\n    Full Vietnamese text cleaning pipeline.\n\n    Parameters\n    ----------\n    text : str\n        Raw Vietnamese text.\n    segment : bool\n        Whether to perform word segmentation.\n    remove_stops : bool\n        Whether to remove Vietnamese stopwords.\n    lowercase : bool\n        Whether to convert to lowercase.\n    min_word_len : int\n        Minimum word length to keep.\n\n    Returns\n    -------\n    str\n        Cleaned text.\n    \"\"\"\n    if pd.isna(text) or text.strip() == '':\n        return ''\n\n    # 1. Unicode normalization (NFC form for Vietnamese)\n    text = unicodedata.normalize('NFC', text)\n\n    # 2. Remove HTML tags and special characters\n    text = re.sub(r'&lt;[^&gt;]+&gt;', ' ', text)\n    text = re.sub(r'[\\d]+', ' ', text)           # Remove numbers\n    text = re.sub(r'[^\\w\\s\\u00C0-\\u024F]', ' ', text)  # Keep VN chars\n\n    # 3. Lowercase\n    if lowercase:\n        text = text.lower()\n\n    # 4. Word segmentation\n    if segment:\n        text = word_tokenize(text, format='text')\n\n    # 5. Tokenize and filter\n    tokens = text.split()\n    if remove_stops:\n        tokens = [t for t in tokens\n                  if t not in VIETNAMESE_STOPWORDS\n                  and len(t) &gt;= min_word_len]\n\n    return ' '.join(tokens)\n\n# Apply to corpus\ncorpus_df['text_clean'] = (\n    corpus_df.bus_desc_vi\n    .apply(lambda x: clean_vietnamese_text(x))\n)\n\n# Verify cleaning quality\nprint('Sample cleaned text:')\nprint(corpus_df.iloc[0].text_clean[:300])",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-english-cleaning",
    "href": "70_textual.html#sec-textual-english-cleaning",
    "title": "44  Textual Analysis",
    "section": "47.3 English Text Cleaning",
    "text": "47.3 English Text Cleaning\nFor firms that also provide English business descriptions, we apply a standard English NLP pipeline using spaCy and NLTK. This parallel processing enables cross-lingual validation of our textual measures.\n\nimport spacy\nfrom nltk.corpus import stopwords\nimport gensim\n\nnlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\nstop_words = set(stopwords.words('english'))\n\ndef clean_english_text(text: str) -&gt; str:\n    \"\"\"Clean English text with lemmatization.\"\"\"\n    if pd.isna(text) or text.strip() == '':\n        return ''\n    text = text.lower().strip()\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n    doc = nlp(text)\n    tokens = [token.lemma_ for token in doc\n              if token.lemma_ not in stop_words\n              and len(token.lemma_) &gt; 2\n              and not token.is_punct]\n    return ' '.join(tokens)\n\n# Apply to English descriptions\ncorpus_df['text_clean_en'] = (\n    corpus_df.bus_desc_en\n    .apply(lambda x: clean_english_text(x))\n)",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-bow",
    "href": "70_textual.html#sec-textual-bow",
    "title": "44  Textual Analysis",
    "section": "48.1 Bag-of-Words Representation",
    "text": "48.1 Bag-of-Words Representation\nThe bag-of-words (BoW) model represents each document as a vector of word frequencies, discarding word order. Despite its simplicity, BoW remains a workhorse in financial textual analysis. Formally, given a vocabulary \\(V = \\{w_1, w_2, \\ldots, w_{|V|}\\}\\), document \\(d\\) is represented as a vector \\(\\mathbf{x}_d\\) where each element \\(x_{d,j}\\) counts the frequency of word \\(w_j\\) in document \\(d\\):\n\\[\n\\mathbf{x}_d = [\\text{tf}(w_1, d), \\; \\text{tf}(w_2, d), \\; \\ldots, \\; \\text{tf}(w_{|V|}, d)]\n\\tag{48.1}\\]\nwhere \\(\\text{tf}(w, d)\\) is the term frequency of word \\(w\\) in document \\(d\\).\n\nfrom sklearn.feature_extraction.text import (\n    CountVectorizer, TfidfVectorizer\n)\n\n# Vietnamese corpus\ntext_corpus = corpus_df.text_clean.tolist()\n\n# BoW vectorization\nbow_vectorizer = CountVectorizer(\n    max_features=10000,\n    min_df=5,           # Appear in at least 5 documents\n    max_df=0.95,        # Exclude terms in &gt;95% of docs\n    ngram_range=(1, 2)  # Unigrams and bigrams\n)\n\nbow_matrix = bow_vectorizer.fit_transform(text_corpus)\n\nprint(f'Vocabulary size: {len(bow_vectorizer.vocabulary_)}')\nprint(f'Document-term matrix shape: {bow_matrix.shape}')\nprint(f'Sparsity: {1 - bow_matrix.nnz / np.prod(bow_matrix.shape):.4f}')\n\n# Top 20 most frequent terms\nword_freq = pd.DataFrame({\n    'word': bow_vectorizer.get_feature_names_out(),\n    'freq': bow_matrix.sum(axis=0).A1\n}).sort_values('freq', ascending=False)\n\nprint('\\nTop 20 most frequent terms:')\nprint(word_freq.head(20).to_string(index=False))\n\n\n\n\nfig, ax = plt.subplots(figsize=(12, 6))\ntop20 = word_freq.head(20)\nax.barh(range(len(top20)), top20.freq.values, color='#2C5282')\nax.set_yticks(range(len(top20)))\nax.set_yticklabels(top20.word.values)\nax.invert_yaxis()\nax.set_xlabel('Frequency')\nax.set_title('Top 20 Most Frequent Terms in Vietnamese Business Descriptions')\nplt.tight_layout()\nplt.show()\n\n\nFigure 48.1\n\n\n\n\n\n\nTable 48.1: Top 20 Most Frequent Terms in Vietnamese Business Descriptions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nTerm (VN)\nFreq\n#\nTerm (VN)\nFreq\n#\nTerm (VN)\nFreq\n\n\n\n\n1\nsản_xuất\n4,287\n8\ncông_nghệ\n1,956\n15\nxuất_khẩu\n1,123\n\n\n2\nkinh_doanh\n3,891\n9\ntài_chính\n1,845\n16\nbất_động_sản\n1,087\n\n\n3\ndịch_vụ\n3,654\n10\nngân_hàng\n1,734\n17\nnăng_lượng\n1,045\n\n\n4\ncông_ty\n3,412\n11\nđầu_tư\n1,623\n18\nbảo_hiểm\n987\n\n\n5\nthương_mại\n2,876\n12\nxây_dựng\n1,534\n19\ndu_lịch\n923\n\n\n6\ncổ_phần\n2,543\n13\nvận_tải\n1,345\n20\nviễn_thông\n876\n\n\n7\nchứng_khoán\n2,134\n14\nthực_phẩm\n1,234",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-tfidf",
    "href": "70_textual.html#sec-textual-tfidf",
    "title": "44  Textual Analysis",
    "section": "48.2 TF-IDF Weighting",
    "text": "48.2 TF-IDF Weighting\nTerm Frequency-Inverse Document Frequency (TF-IDF) addresses a key limitation of raw term counts by downweighting terms that appear in many documents (and thus carry less discriminative information). The TF-IDF weight of term \\(w\\) in document \\(d\\) within corpus \\(D\\) is:\n\\[\n\\text{tfidf}(w, d, D) = \\text{tf}(w, d) \\times \\log\\left(\\frac{|D|}{\\text{df}(w, D)}\\right)\n\\tag{48.2}\\]\nwhere \\(|D|\\) is the total number of documents and \\(\\text{df}(w, D)\\) is the number of documents containing term \\(w\\). This weighting scheme ensures that industry-specific terminology (e.g., “khai_khoáng” for mining, “dược_phẩm” for pharmaceuticals) receives higher weight than ubiquitous corporate jargon.\n\ntfidf_vectorizer = TfidfVectorizer(\n    max_features=10000,\n    min_df=5,\n    max_df=0.95,\n    ngram_range=(1, 2),\n    sublinear_tf=True  # Use 1 + log(tf) instead of raw tf\n)\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(text_corpus)\n\n# Per-industry top TF-IDF terms\nfor industry in ['Ngân hàng', 'Bất động sản',\n                  'Công nghệ thông tin']:\n    mask = corpus_df.icb_sector == industry\n    if mask.sum() == 0:\n        continue\n    mean_tfidf = tfidf_matrix[mask.values].mean(axis=0).A1\n    top_idx = mean_tfidf.argsort()[-10:][::-1]\n    terms = tfidf_vectorizer.get_feature_names_out()\n    print(f'\\n{industry}:')\n    for idx in top_idx:\n        print(f'  {terms[idx]}: {mean_tfidf[idx]:.4f}')\n\n\n\n\n# Build industry x term TF-IDF matrix for top sectors\ntop_sectors = corpus_df.icb_sector.value_counts().head(8).index.tolist()\nterms = tfidf_vectorizer.get_feature_names_out()\n\nsector_tfidf = {}\nfor sector in top_sectors:\n    mask = corpus_df.icb_sector == sector\n    if mask.sum() == 0:\n        continue\n    mean_tfidf = tfidf_matrix[mask.values].mean(axis=0).A1\n    top_idx = mean_tfidf.argsort()[-5:][::-1]\n    for idx in top_idx:\n        if terms[idx] not in sector_tfidf:\n            sector_tfidf[terms[idx]] = {}\n        sector_tfidf[terms[idx]][sector] = mean_tfidf[idx]\n\nheatmap_df = pd.DataFrame(sector_tfidf).T.fillna(0)\n\nfig, ax = plt.subplots(figsize=(14, 10))\nsns.heatmap(heatmap_df, annot=True, fmt='.3f', cmap='Blues',\n            linewidths=0.5, ax=ax)\nax.set_title('TF-IDF Heatmap: Industry-Distinctive Terms')\nax.set_xlabel('ICB Sector')\nax.set_ylabel('Term')\nplt.tight_layout()\nplt.show()\n\n\nFigure 48.2",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-lda",
    "href": "70_textual.html#sec-textual-lda",
    "title": "44  Textual Analysis",
    "section": "49.1 Latent Dirichlet Allocation (LDA)",
    "text": "49.1 Latent Dirichlet Allocation (LDA)\nLatent Dirichlet Allocation (Blei, Ng, and Jordan 2003) is a generative probabilistic model that discovers latent topics in a corpus. Each document is modeled as a mixture of topics, and each topic is a distribution over words. LDA has been widely applied in finance to identify thematic content in 10-K filings (Dyer, Lang, and Stice-Lawrence 2017), earnings calls (Huang et al. 2018), and news articles (Bybee, Kelly, and Su 2023).\nThe generative process assumes:\n\nFor each topic \\(k\\), draw a word distribution \\(\\boldsymbol{\\phi}_k \\sim \\text{Dir}(\\beta)\\).\nFor each document \\(d\\), draw a topic distribution \\(\\boldsymbol{\\theta}_d \\sim \\text{Dir}(\\alpha)\\).\nFor each word position \\(i\\) in document \\(d\\), draw a topic \\(z_{d,i} \\sim \\text{Multinomial}(\\boldsymbol{\\theta}_d)\\) and then draw the word \\(w_{d,i} \\sim \\text{Multinomial}(\\boldsymbol{\\phi}_{z_{d,i}})\\).\n\n\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Grid search over number of topics\nn_topics_range = [10, 15, 20, 25, 30]\nperplexity_scores = []\n\nfor n_topics in n_topics_range:\n    lda = LatentDirichletAllocation(\n        n_components=n_topics,\n        max_iter=50,\n        learning_method='online',\n        random_state=42,\n        n_jobs=-1\n    )\n    lda.fit(bow_matrix)\n    perplexity = lda.perplexity(bow_matrix)\n    perplexity_scores.append({\n        'n_topics': n_topics,\n        'perplexity': perplexity,\n        'log_likelihood': lda.score(bow_matrix)\n    })\n    print(f'K={n_topics}: perplexity={perplexity:.2f}')\n\n# Select optimal K (e.g., K=20)\nK_OPTIMAL = 20\nlda_model = LatentDirichletAllocation(\n    n_components=K_OPTIMAL,\n    max_iter=100,\n    learning_method='online',\n    random_state=42,\n    n_jobs=-1\n)\nlda_model.fit(bow_matrix)\n\n# Extract topic-word distributions\nfeature_names = bow_vectorizer.get_feature_names_out()\nfor topic_idx, topic in enumerate(lda_model.components_):\n    top_words = [feature_names[i]\n                 for i in topic.argsort()[:-11:-1]]\n    print(f'Topic {topic_idx}: {\" | \".join(top_words)}')\n\n\n\n\nperp_df = pd.DataFrame(perplexity_scores)\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(perp_df.n_topics, perp_df.perplexity, 'o-', color='#2C5282',\n        linewidth=2, markersize=8)\nax.axvline(x=K_OPTIMAL, color='red', linestyle='--', alpha=0.7,\n           label=f'Selected K={K_OPTIMAL}')\nax.set_xlabel('Number of Topics (K)')\nax.set_ylabel('Perplexity (lower is better)')\nax.set_title('LDA Model Selection')\nax.legend()\nplt.tight_layout()\nplt.show()\n\n\nFigure 49.1\n\n\n\n\n\n\nTable 49.1: Selected LDA Topics from Vietnamese Business Descriptions (K=20)\n\n\n\n\n\n\n\n\n\n\nTopic\nInterpretation\nTop Words\n\n\n\n\n0\nBanking & Finance\nngân_hàng | tín_dụng | cho_vay | tiền_gửi | lãi_suất | thanh_toán | tài_khoản\n\n\n3\nReal Estate\nbất_động_sản | dự_án | căn_hộ | khu_đô_thị | xây_dựng | nhà_ở\n\n\n7\nTechnology\ncông_nghệ | phần_mềm | giải_pháp | hệ_thống | số_hóa | dữ_liệu\n\n\n11\nManufacturing\nsản_xuất | nguyên_liệu | nhà_máy | chất_lượng | công_suất | xuất_khẩu\n\n\n15\nSecurities\nchứng_khoán | môi_giới | đầu_tư | cổ_phiếu | danh_mục | quản_lý_quỹ",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-bertopic",
    "href": "70_textual.html#sec-textual-bertopic",
    "title": "44  Textual Analysis",
    "section": "49.2 BERTopic: Neural Topic Modeling",
    "text": "49.2 BERTopic: Neural Topic Modeling\nBERTopic (Grootendorst 2022) represents a significant advance over LDA by leveraging pre-trained language model embeddings, dimensionality reduction via UMAP, and hierarchical density-based clustering (HDBSCAN) to discover topics. Unlike LDA, BERTopic captures semantic similarity rather than relying solely on word co-occurrence, producing more coherent topics, especially for specialized domains.\n\nfrom bertopic import BERTopic\nfrom sentence_transformers import SentenceTransformer\nfrom umap import UMAP\nfrom hdbscan import HDBSCAN\n\n# Use PhoBERT-based sentence transformer for Vietnamese\nembedding_model = SentenceTransformer(\n    'bkai-foundation-models/vietnamese-bi-encoder'\n)\n\n# Custom UMAP and HDBSCAN for better control\numap_model = UMAP(\n    n_neighbors=15, n_components=5,\n    min_dist=0.0, metric='cosine', random_state=42\n)\nhdbscan_model = HDBSCAN(\n    min_cluster_size=10, min_samples=5,\n    metric='euclidean', prediction_data=True\n)\n\n# Fit BERTopic\ntopic_model = BERTopic(\n    embedding_model=embedding_model,\n    umap_model=umap_model,\n    hdbscan_model=hdbscan_model,\n    language='multilingual',\n    calculate_probabilities=True,\n    verbose=True\n)\n\n# Vietnamese text (use segmented text for better results)\ndocs = corpus_df.bus_desc_segmented.tolist()\ntopics, probs = topic_model.fit_transform(docs)\n\n# Inspect topics\ntopic_info = topic_model.get_topic_info()\nprint(topic_info.head(20))\n\n\n\n\n# Visualize topic hierarchy\nfig_hierarchy = topic_model.visualize_hierarchy()\nfig_hierarchy.show()\n\n# Visualize document clusters\nfig_docs = topic_model.visualize_documents(\n    docs, reduced_embeddings=umap_model.embedding_\n)\nfig_docs.show()\n\n# Topic word scores (barchart)\nfig_barchart = topic_model.visualize_barchart(top_n_topics=10)\nfig_barchart.show()\n\n\nFigure 49.2",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-dict-sentiment",
    "href": "70_textual.html#sec-textual-dict-sentiment",
    "title": "44  Textual Analysis",
    "section": "50.1 Dictionary-Based Approach",
    "text": "50.1 Dictionary-Based Approach\nWe construct a Vietnamese financial sentiment lexicon following the methodology of Loughran and McDonald (2011). Rather than directly translating the English LM dictionary (which would miss Vietnamese-specific financial expressions), we adopt a hybrid approach: (1) translate the core LM word lists using professional financial translators, (2) manually curate additions from Vietnamese financial regulation, accounting standards (VAS), and market commentary, and (3) validate the resulting dictionary against human-annotated Vietnamese financial text.\n\n\n\nTable 50.1: Vietnamese Financial Sentiment Lexicon: Sample Entries\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nVietnamese Term\nEnglish Gloss\nSource\nCount in Corpus\n\n\n\n\nNegative\nlỗ\nloss\nLM-translated\n2,341\n\n\nNegative\nsụt_giảm\ndecline\nCurated\n1,876\n\n\nNegative\nnợ_xấu\nbad debt\nVAS-specific\n1,234\n\n\nNegative\nrủi_ro\nrisk\nLM-translated\n3,567\n\n\nPositive\ntăng_trưởng\ngrowth\nLM-translated\n4,123\n\n\nPositive\nlợi_nhuận\nprofit\nLM-translated\n3,891\n\n\nPositive\nhiệu_quả\nefficiency\nCurated\n2,456\n\n\nUncertain\nbiến_động\nvolatility\nLM-translated\n1,567\n\n\nLitigious\ntranh_chấp\ndispute\nLegal-VN\n876\n\n\nLitigious\nkhởi_kiện\nlawsuit\nLegal-VN\n234\n\n\n\n\n\n\n\n# Load Vietnamese financial sentiment lexicon\n# sentiment_dict = dc.get_sentiment_lexicon(version='vn_financial_v2')\n\n# Alternatively, construct from LM + manual curation\nnegative_words = set(pd.read_csv(\n    'lexicons/vn_negative.txt', header=None)[0]\n)\npositive_words = set(pd.read_csv(\n    'lexicons/vn_positive.txt', header=None)[0]\n)\nuncertain_words = set(pd.read_csv(\n    'lexicons/vn_uncertain.txt', header=None)[0]\n)\n\ndef compute_sentiment_scores(text: str) -&gt; dict:\n    \"\"\"\n    Compute Loughran-McDonald style sentiment scores.\n    Returns proportions (word count / total words).\n    \"\"\"\n    tokens = text.split()\n    n = len(tokens)\n    if n == 0:\n        return {'neg_pct': 0, 'pos_pct': 0,\n                'unc_pct': 0, 'net_tone': 0}\n\n    neg = sum(1 for t in tokens if t in negative_words)\n    pos = sum(1 for t in tokens if t in positive_words)\n    unc = sum(1 for t in tokens if t in uncertain_words)\n\n    return {\n        'neg_pct': neg / n,\n        'pos_pct': pos / n,\n        'unc_pct': unc / n,\n        'net_tone': (pos - neg) / n\n    }\n\n# Apply to annual report MD&A text\nsentiment_scores = annual_text.text_clean.apply(\n    lambda x: pd.Series(compute_sentiment_scores(x))\n)\nannual_text = pd.concat([annual_text, sentiment_scores], axis=1)\n\n\n\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\naxes[0].hist(annual_text.neg_pct, bins=50, color='#E53E3E', alpha=0.7,\n             edgecolor='white')\naxes[0].set_title('Negative Word Proportion')\naxes[0].set_xlabel('Proportion')\n\naxes[1].hist(annual_text.pos_pct, bins=50, color='#38A169', alpha=0.7,\n             edgecolor='white')\naxes[1].set_title('Positive Word Proportion')\naxes[1].set_xlabel('Proportion')\n\naxes[2].hist(annual_text.net_tone, bins=50, color='#2C5282', alpha=0.7,\n             edgecolor='white')\naxes[2].set_title('Net Tone (Positive - Negative)')\naxes[2].set_xlabel('Net Tone')\n\nplt.suptitle('Sentiment Distribution in Vietnamese Annual Reports',\n             fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\nFigure 50.1",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-bert-sentiment",
    "href": "70_textual.html#sec-textual-bert-sentiment",
    "title": "44  Textual Analysis",
    "section": "50.2 Transformer-Based Sentiment Classification",
    "text": "50.2 Transformer-Based Sentiment Classification\nDictionary approaches are limited by their inability to capture context, negation, and sarcasm. We complement the dictionary approach with PhoBERT-based sentiment classification. We fine-tune PhoBERT v2.\n\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    pipeline\n)\nimport torch\n\n# Load fine-tuned ViFinBERT for sentiment\nmodel_name = 'vinai/phobert-base-v2'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name, num_labels=3  # positive, negative, neutral\n)\n\n# Create sentiment pipeline\nsentiment_pipe = pipeline(\n    'text-classification',\n    model=model,\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1,\n    max_length=256,\n    truncation=True,\n    batch_size=32\n)\n\n# For long documents, split into sentences first\nfrom underthesea import sent_tokenize\n\ndef document_sentiment(text: str) -&gt; dict:\n    \"\"\"Aggregate sentence-level sentiment for a document.\"\"\"\n    sentences = sent_tokenize(text)\n    if not sentences:\n        return {'bert_pos': 0, 'bert_neg': 0, 'bert_neu': 0}\n\n    results = sentiment_pipe(sentences[:100])  # Cap at 100 sents\n    labels = [r['label'] for r in results]\n\n    n = len(labels)\n    return {\n        'bert_pos': labels.count('POSITIVE') / n,\n        'bert_neg': labels.count('NEGATIVE') / n,\n        'bert_neu': labels.count('NEUTRAL') / n,\n        'bert_tone': (labels.count('POSITIVE') -\n                      labels.count('NEGATIVE')) / n\n    }\n\n\n\n\nTable 50.2: Sentiment Method Comparison: Dictionary vs. PhoBERT on Validation Set (N=500)\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nAccuracy\nF1 (Pos)\nF1 (Neg)\nF1 (Neutral)\n\n\n\n\nVN-LM Dictionary\n0.612\n0.584\n0.637\n0.598\n\n\nPhoBERT (zero-shot)\n0.724\n0.698\n0.741\n0.712\n\n\nPhoBERT v2 (fine-tuned)\n0.831\n0.812\n0.847\n0.824",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-tfidf-similarity",
    "href": "70_textual.html#sec-textual-tfidf-similarity",
    "title": "44  Textual Analysis",
    "section": "51.1 Cosine Similarity on TF-IDF Vectors",
    "text": "51.1 Cosine Similarity on TF-IDF Vectors\nFollowing Hoberg and Phillips (2016), we compute pairwise cosine similarity between firms based on their business description TF-IDF vectors. For two documents represented as TF-IDF vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), cosine similarity is defined as:\n\\[\n\\cos(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\times \\|\\mathbf{b}\\|}\n\\tag{51.1}\\]\nThis metric ranges from 0 (completely dissimilar) to 1 (identical content) and is invariant to document length. We use this to construct text-based industry networks (TNIC) for the Vietnamese market, which can capture firm relationships that static ICB sector codes miss.\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Compute pairwise similarity matrix\nsim_matrix = cosine_similarity(tfidf_matrix)\n\n# Convert to DataFrame for easy lookup\ntickers = corpus_df.ticker.tolist()\nsim_df = pd.DataFrame(\n    sim_matrix, index=tickers, columns=tickers\n)\n\n# For each firm, find top-5 most similar peers\ndef get_top_peers(ticker: str, n: int = 5) -&gt; pd.DataFrame:\n    \"\"\"Return top-n most similar firms by TF-IDF cosine.\"\"\"\n    sims = sim_df[ticker].drop(ticker).sort_values(\n        ascending=False\n    ).head(n)\n    peers = corpus_df.set_index('ticker').loc[sims.index]\n    peers['similarity'] = sims.values\n    return peers[['company_name', 'icb_sector',\n                  'market_cap', 'similarity']]\n\n# Examples\nfor ticker in ['VCB', 'VNM', 'FPT', 'VIC', 'HPG']:\n    print(f'\\nTop peers for {ticker}:')\n    print(get_top_peers(ticker))\n\n\n\n\nTable 51.1: Text-Based Peer Identification: Top Most Similar Firms (TF-IDF Cosine)\n\n\n\n\n\nFirm\nICB Sector\nPeer 1\nPeer 1 Sector\nSim Score\nSame ICB?\n\n\n\n\nVCB\nBanking\nBID\nBanking\n0.87\nYes\n\n\nVCB\nBanking\nCTG\nBanking\n0.84\nYes\n\n\nVNM\nFood & Bev\nMCH\nFood & Bev\n0.72\nYes\n\n\nFPT\nTechnology\nCMG\nTechnology\n0.68\nYes\n\n\nVIC\nReal Estate\nNVL\nReal Estate\n0.74\nYes\n\n\nHPG\nSteel\nHSG\nSteel\n0.81\nYes\n\n\n\n\n\n\n\n\n\nsample_tickers = ['VCB', 'BID', 'CTG', 'VNM', 'MCH',\n                  'FPT', 'CMG', 'VIC', 'NVL', 'HPG',\n                  'HSG', 'VHM', 'SSI', 'HCM', 'PNJ']\nsample_sim = sim_df.loc[sample_tickers, sample_tickers]\n\nfig, ax = plt.subplots(figsize=(10, 8))\nsns.heatmap(sample_sim, annot=True, fmt='.2f', cmap='Blues',\n            vmin=0, vmax=1, square=True, linewidths=0.5, ax=ax)\nax.set_title('Pairwise TF-IDF Cosine Similarity\\n(Selected Vietnamese Listed Firms)')\nplt.tight_layout()\nplt.show()\n\n\nFigure 51.1",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-embedding-similarity",
    "href": "70_textual.html#sec-textual-embedding-similarity",
    "title": "44  Textual Analysis",
    "section": "51.2 Embedding-Based Similarity",
    "text": "51.2 Embedding-Based Similarity\nWhile TF-IDF cosine similarity captures lexical overlap, it misses semantic similarity. Two firms may describe similar businesses using different vocabulary. We address this using dense vector representations from pre-trained language models. Specifically, we compute document embeddings using Sentence-BERT (Reimers and Gurevych 2019) with a Vietnamese bi-encoder model.3\n\nfrom sentence_transformers import SentenceTransformer\n\n# Vietnamese sentence transformer\nsbert_model = SentenceTransformer(\n    'bkai-foundation-models/vietnamese-bi-encoder'\n)\n\n# Compute embeddings for all firms\ndocs_segmented = corpus_df.bus_desc_segmented.tolist()\nembeddings = sbert_model.encode(\n    docs_segmented,\n    batch_size=64,\n    show_progress_bar=True,\n    normalize_embeddings=True\n)\n\n# Pairwise similarity\nembed_sim = cosine_similarity(embeddings)\nembed_sim_df = pd.DataFrame(\n    embed_sim, index=tickers, columns=tickers\n)\n\n# Compare TF-IDF vs embedding similarity\nfor ticker in ['VCB', 'FPT', 'VIC']:\n    tfidf_peers = sim_df[ticker].drop(ticker).nlargest(5)\n    embed_peers = embed_sim_df[ticker].drop(ticker).nlargest(5)\n    print(f'\\n{ticker} - TF-IDF peers: {tfidf_peers.index.tolist()}')\n    print(f'{ticker} - Embed peers:  {embed_peers.index.tolist()}')\n\n\n\n\nfrom sklearn.manifold import TSNE\n\n# t-SNE projection\ntsne = TSNE(n_components=2, perplexity=30, random_state=42,\n            metric='cosine')\nembeddings_2d = tsne.fit_transform(embeddings)\n\nfig, ax = plt.subplots(figsize=(14, 10))\nsectors = corpus_df.icb_sector.values\nunique_sectors = corpus_df.icb_sector.value_counts().head(10).index\ncolors = plt.cm.tab10(range(10))\n\nfor i, sector in enumerate(unique_sectors):\n    mask = sectors == sector\n    ax.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],\n               c=[colors[i]], label=sector, alpha=0.6, s=30)\n\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\nax.set_title('t-SNE of PhoBERT Embeddings by ICB Sector')\nax.set_xlabel('t-SNE 1')\nax.set_ylabel('t-SNE 2')\nplt.tight_layout()\nplt.show()\n\n\nFigure 51.2",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-doc2vec",
    "href": "70_textual.html#sec-textual-doc2vec",
    "title": "44  Textual Analysis",
    "section": "51.3 Doc2Vec",
    "text": "51.3 Doc2Vec\nWe also implement Doc2Vec (Le and Mikolov 2014), which learns fixed-length dense vectors for documents of variable length. Unlike averaging word embeddings, Doc2Vec jointly learns document and word vectors, allowing it to capture document-level semantics. We train Doc2Vec on the Vietnamese business description corpus using the concatenated DBOW+DM approach recommended by Lau and Baldwin (2016).\n\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\n# Prepare tagged documents\ntagged_docs = [\n    TaggedDocument(\n        words=text.split(),\n        tags=[ticker]\n    )\n    for text, ticker in zip(\n        corpus_df.text_clean.tolist(),\n        corpus_df.ticker.tolist()\n    )\n]\n\n# PV-DBOW: paragraph vector with distributed bag of words\nd2v_dbow = Doc2Vec(\n    vector_size=100, dm=0, min_count=5,\n    window=5, epochs=40, workers=4, seed=42\n)\nd2v_dbow.build_vocab(tagged_docs)\nd2v_dbow.train(\n    tagged_docs,\n    total_examples=d2v_dbow.corpus_count,\n    epochs=d2v_dbow.epochs\n)\n\n# PV-DM: paragraph vector with distributed memory\nd2v_dm = Doc2Vec(\n    vector_size=100, dm=1, min_count=5,\n    window=10, epochs=40, workers=4, seed=42\n)\nd2v_dm.build_vocab(tagged_docs)\nd2v_dm.train(\n    tagged_docs,\n    total_examples=d2v_dm.corpus_count,\n    epochs=d2v_dm.epochs\n)\n\n# Concatenate DBOW + DM vectors (Lau & Baldwin, 2016)\nd2v_vectors = np.hstack([\n    [d2v_dbow.dv[t] for t in tickers],\n    [d2v_dm.dv[t] for t in tickers]\n])\n\n# Most similar firms\nfor ticker in ['VCB', 'FPT', 'VIC']:\n    sims = d2v_dbow.dv.most_similar(ticker, topn=5)\n    print(f'{ticker}: {[(s[0], f\"{s[1]:.3f}\") for s in sims]}')",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-phobert",
    "href": "70_textual.html#sec-textual-phobert",
    "title": "44  Textual Analysis",
    "section": "52.1 PhoBERT Embeddings for Financial Text",
    "text": "52.1 PhoBERT Embeddings for Financial Text\nPhoBERT (Nguyen and Nguyen 2020), pre-trained on 20GB of Vietnamese text, provides contextualized word embeddings that capture meaning based on surrounding context. Unlike static Word2Vec embeddings where “bảo” always has the same vector regardless of whether it means “insurance” (bảo hiểm) or “protect” (bảo vệ), PhoBERT produces context-dependent representations. We extract [CLS] token embeddings as document representations.\n\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\n# Load PhoBERT\nphobert_tokenizer = AutoTokenizer.from_pretrained(\n    'vinai/phobert-base-v2'\n)\nphobert_model = AutoModel.from_pretrained(\n    'vinai/phobert-base-v2'\n)\nphobert_model.eval()\ndevice = torch.device('cuda' if torch.cuda.is_available()\n                      else 'cpu')\nphobert_model.to(device)\n\ndef get_phobert_embedding(text: str, max_len: int = 256):\n    \"\"\"Extract [CLS] embedding from PhoBERT.\"\"\"\n    inputs = phobert_tokenizer(\n        text, return_tensors='pt',\n        max_length=max_len, truncation=True,\n        padding=True\n    ).to(device)\n\n    with torch.no_grad():\n        outputs = phobert_model(**inputs)\n\n    # [CLS] token embedding\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n    return cls_embedding.cpu().numpy().flatten()\n\n# For long documents: chunk + average strategy\ndef get_long_doc_embedding(\n    text: str, chunk_size: int = 256, stride: int = 128\n):\n    \"\"\"Handle long documents via chunked averaging.\"\"\"\n    tokens = phobert_tokenizer.tokenize(text)\n    embeddings = []\n\n    for i in range(0, len(tokens), stride):\n        chunk = tokens[i:i + chunk_size]\n        chunk_text = phobert_tokenizer.convert_tokens_to_string(\n            chunk\n        )\n        emb = get_phobert_embedding(chunk_text)\n        embeddings.append(emb)\n\n    return np.mean(embeddings, axis=0)\n\n# Compute embeddings for all firms\nphobert_embeddings = np.array([\n    get_long_doc_embedding(text)\n    for text in corpus_df.bus_desc_segmented.tolist()\n])",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-llm",
    "href": "70_textual.html#sec-textual-llm",
    "title": "44  Textual Analysis",
    "section": "52.2 Large Language Model Applications",
    "text": "52.2 Large Language Model Applications\nRecent advances in LLMs open new possibilities for financial textual analysis. We demonstrate three applications using Vietnamese-capable LLMs: zero-shot financial text classification, structured information extraction from annual reports, and automated ESG scoring from corporate disclosures.\n\n52.2.1 Zero-Shot Financial Classification\n\nimport anthropic  # Or openai, etc.\n\nclient = anthropic.Anthropic()\n\ndef classify_financial_text(\n    text: str,\n    categories: list = [\n        'Growth outlook', 'Risk warning',\n        'Operational update', 'Financial performance',\n        'Strategic initiative', 'Regulatory compliance'\n    ]\n) -&gt; dict:\n    \"\"\"Zero-shot classify Vietnamese financial text.\"\"\"\n    prompt = f\"\"\"\n    Classify the following Vietnamese financial text into\n    one or more of these categories: {categories}\n\n    Also provide:\n    1. Sentiment: positive / negative / neutral\n    2. Confidence: 0-1\n    3. Key entities mentioned\n\n    Text: {text[:2000]}\n\n    Respond in JSON format.\n    \"\"\"\n\n    response = client.messages.create(\n        model='claude-sonnet-4-20250514',\n        max_tokens=500,\n        messages=[{'role': 'user', 'content': prompt}]\n    )\n    return response.content[0].text\n\n\n\n52.2.2 Structured Information Extraction\n\nimport json\n\ndef extract_financial_info(annual_report_text: str) -&gt; dict:\n    \"\"\"Extract structured data from Vietnamese annual report.\"\"\"\n    prompt = f\"\"\"\n    From the following Vietnamese annual report excerpt,\n    extract structured information in JSON format:\n\n    {{\n        \"revenue_mentioned\": true/false,\n        \"revenue_direction\": \"increase\"/\"decrease\"/\"stable\",\n        \"key_products\": [list of main products/services],\n        \"competitors_mentioned\": [list],\n        \"expansion_plans\": \"description or null\",\n        \"risk_factors\": [list of mentioned risks],\n        \"esg_mentions\": {{\n            \"environmental\": [topics],\n            \"social\": [topics],\n            \"governance\": [topics]\n        }},\n        \"forward_looking_statements\": [list],\n        \"capex_plans\": \"description or null\"\n    }}\n\n    Text: {annual_report_text[:3000]}\n    \"\"\"\n\n    response = client.messages.create(\n        model='claude-sonnet-4-20250514',\n        max_tokens=1000,\n        messages=[{'role': 'user', 'content': prompt}]\n    )\n    return json.loads(response.content[0].text)\n\n\n\n52.2.3 Automated ESG Scoring\n\ndef compute_esg_scores(text: str) -&gt; dict:\n    \"\"\"Score ESG dimensions from Vietnamese corporate disclosure.\"\"\"\n    prompt = f\"\"\"\n    Analyze the following Vietnamese corporate disclosure text\n    and score each ESG dimension on a scale of 0-100 based on\n    the depth and quality of disclosure:\n\n    Return JSON:\n    {{\n        \"environmental_score\": 0-100,\n        \"environmental_topics\": [list of specific topics discussed],\n        \"social_score\": 0-100,\n        \"social_topics\": [list],\n        \"governance_score\": 0-100,\n        \"governance_topics\": [list],\n        \"overall_esg_score\": 0-100,\n        \"assessment_confidence\": 0-1,\n        \"notable_commitments\": [list of specific commitments],\n        \"gaps_identified\": [list of missing ESG disclosures]\n    }}\n\n    Text: {text[:4000]}\n    \"\"\"\n\n    response = client.messages.create(\n        model='claude-sonnet-4-20250514',\n        max_tokens=800,\n        messages=[{'role': 'user', 'content': prompt}]\n    )\n    return json.loads(response.content[0].text)\n\n# Apply to all firms' annual reports\nesg_results = []\nfor _, row in annual_text.iterrows():\n    try:\n        scores = compute_esg_scores(row.text)\n        scores['ticker'] = row.ticker\n        scores['year'] = row.year\n        esg_results.append(scores)\n    except Exception as e:\n        print(f\"Error for {row.ticker} {row.year}: {e}\")\n\nesg_df = pd.DataFrame(esg_results)",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-sentiment-returns",
    "href": "70_textual.html#sec-textual-sentiment-returns",
    "title": "44  Textual Analysis",
    "section": "53.1 Textual Sentiment and Stock Returns",
    "text": "53.1 Textual Sentiment and Stock Returns\nWe examine whether textual sentiment from annual reports predicts subsequent stock returns, following the methodology of Tetlock, Saar-Tsechansky, and Macskassy (2008). We regress monthly stock returns on lagged sentiment measures while controlling for standard risk factors (market, size, value, momentum) adapted for the Vietnamese market:\n\\[\nR_{i,t} = \\alpha + \\beta_1 \\text{Tone}_{i,t-1} + \\beta_2 \\text{Uncertainty}_{i,t-1} + \\boldsymbol{\\gamma}' \\mathbf{X}_{i,t-1} + \\varepsilon_{i,t}\n\\tag{53.1}\\]\nwhere \\(R_{i,t}\\) is the monthly excess return of firm \\(i\\) in month \\(t\\), \\(\\text{Tone}\\) is the net sentiment score (positive minus negative word proportion), \\(\\text{Uncertainty}\\) is the proportion of uncertain words, and \\(\\mathbf{X}\\) is a vector of controls including the Fama-French-Carhart factors adapted for Vietnam (see Chapter on Factor Models).\n\nimport statsmodels.api as sm\nfrom linearmodels.panel import PanelOLS\n\n# Merge sentiment scores with return data\nreturns = dc.get_monthly_returns(\n    tickers=universe.ticker.tolist(),\n    start='2016-01-01', end='2024-12-31'\n)\n\n# Panel regression with firm and time fixed effects\npanel = annual_text.merge(\n    returns, on=['ticker', 'year', 'month']\n)\npanel = panel.set_index(['ticker', 'date'])\n\n# Model 1: Dictionary-based sentiment\nmodel1 = PanelOLS(\n    dependent=panel.ret_excess,\n    exog=sm.add_constant(\n        panel[['net_tone', 'unc_pct', 'mkt_rf',\n               'smb', 'hml', 'wml']]\n    ),\n    entity_effects=True,\n    time_effects=True\n)\nres1 = model1.fit(cov_type='clustered',\n                  cluster_entity=True,\n                  cluster_time=True)\n\n# Model 2: BERT-based sentiment\nmodel2 = PanelOLS(\n    dependent=panel.ret_excess,\n    exog=sm.add_constant(\n        panel[['bert_tone', 'mkt_rf',\n               'smb', 'hml', 'wml']]\n    ),\n    entity_effects=True,\n    time_effects=True\n)\nres2 = model2.fit(cov_type='clustered',\n                  cluster_entity=True,\n                  cluster_time=True)\n\n# Model 3: Combined\nmodel3 = PanelOLS(\n    dependent=panel.ret_excess,\n    exog=sm.add_constant(\n        panel[['net_tone', 'unc_pct', 'bert_tone',\n               'mkt_rf', 'smb', 'hml', 'wml']]\n    ),\n    entity_effects=True,\n    time_effects=True\n)\nres3 = model3.fit(cov_type='clustered',\n                  cluster_entity=True,\n                  cluster_time=True)\n\nprint(res1.summary)\nprint(res2.summary)\nprint(res3.summary)\n\n\n\n\nTable 53.1: Textual Sentiment and Stock Returns: Panel Regression Results\n\n\n\n\n\n\n\n\n\n\n\nVariable\n(1) Dictionary\n(2) PhoBERT\n(3) Combined\n\n\n\n\nNet Tone (Dict)\n0.0234**\n\n0.0187*\n\n\n\n(0.0098)\n\n(0.0102)\n\n\nUncertainty (Dict)\n−0.0312***\n\n−0.0278**\n\n\n\n(0.0087)\n\n(0.0091)\n\n\nBERT Tone\n\n0.0456***\n0.0389***\n\n\n\n\n(0.0112)\n(0.0118)\n\n\nMKT-RF\n0.9123***\n0.9118***\n0.9115***\n\n\n\n(0.0234)\n(0.0233)\n(0.0234)\n\n\nSMB\n0.1245**\n0.1238**\n0.1241**\n\n\n\n(0.0456)\n(0.0455)\n(0.0456)\n\n\nHML\n0.0876*\n0.0871*\n0.0873*\n\n\n\n(0.0512)\n(0.0511)\n(0.0512)\n\n\nFirm FE\nYes\nYes\nYes\n\n\nTime FE\nYes\nYes\nYes\n\n\nClustering\nTwo-way\nTwo-way\nTwo-way\n\n\nN\n12,456\n12,456\n12,456\n\n\nR² (within)\n0.142\n0.148\n0.153",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-tnic",
    "href": "70_textual.html#sec-textual-tnic",
    "title": "44  Textual Analysis",
    "section": "53.2 Text-Based Industry Classification",
    "text": "53.2 Text-Based Industry Classification\nWe construct Vietnamese Text-Based Network Industries (VN-TNIC) analogous to Hoberg and Phillips (2016). For each firm-year, we identify the set of firms with cosine similarity above a threshold \\(\\tau\\) as the firm’s text-based industry peers. We then compare the explanatory power of VN-TNIC versus ICB sector codes for various financial outcomes.\n\n# Construct TNIC network\nTAU = 0.20  # Similarity threshold\n\ntnic_edges = []\nfor i in range(len(tickers)):\n    for j in range(i+1, len(tickers)):\n        sim = sim_matrix[i, j]\n        if sim &gt;= TAU:\n            tnic_edges.append({\n                'firm1': tickers[i],\n                'firm2': tickers[j],\n                'similarity': sim\n            })\n\ntnic_df = pd.DataFrame(tnic_edges)\nprint(f'TNIC edges (tau={TAU}): {len(tnic_df)}')\nprint(f'Avg degree: {2*len(tnic_df)/len(tickers):.1f}')\n\n# Compare TNIC vs ICB for return comovement\nfrom linearmodels.asset_pricing import FamaMacBeth\n\n# Peer return = avg return of TNIC peers\n# vs ICB sector average return\ndef compute_tnic_peer_return(group, tnic_edges_df):\n    \"\"\"Compute average return of TNIC peers for each firm.\"\"\"\n    peer_returns = {}\n    for ticker in group.index:\n        peers = tnic_edges_df[\n            (tnic_edges_df.firm1 == ticker) |\n            (tnic_edges_df.firm2 == ticker)\n        ]\n        peer_tickers = set(\n            peers.firm1.tolist() + peers.firm2.tolist()\n        ) - {ticker}\n        peer_mask = group.index.isin(peer_tickers)\n        if peer_mask.sum() &gt; 0:\n            peer_returns[ticker] = group.loc[peer_mask, 'ret'].mean()\n        else:\n            peer_returns[ticker] = np.nan\n    return pd.Series(peer_returns)\n\npanel['icb_peer_ret'] = panel.groupby(\n    ['date', 'icb_sector']\n)['ret'].transform('mean')\n\n\n\n\nimport networkx as nx\n\n# Build network graph (subsample for visualization)\nG = nx.Graph()\nsample_edges = tnic_df.nlargest(500, 'similarity')\n\nfor _, row in sample_edges.iterrows():\n    G.add_edge(row.firm1, row.firm2, weight=row.similarity)\n\n# Color by ICB sector\nsector_map = corpus_df.set_index('ticker')['icb_sector'].to_dict()\nnode_colors = [hash(sector_map.get(n, 'Unknown')) % 10\n               for n in G.nodes()]\n\nfig, ax = plt.subplots(figsize=(14, 12))\npos = nx.spring_layout(G, k=0.5, seed=42)\nedges = G.edges(data=True)\nweights = [e[2]['weight'] * 3 for e in edges]\n\nnx.draw_networkx_nodes(G, pos, node_size=100, node_color=node_colors,\n                       cmap='tab10', alpha=0.7, ax=ax)\nnx.draw_networkx_edges(G, pos, width=weights, alpha=0.3,\n                       edge_color='gray', ax=ax)\nnx.draw_networkx_labels(G, pos, font_size=6, ax=ax)\n\nax.set_title('VN-TNIC Network (Top 500 Edges by Similarity)')\nax.axis('off')\nplt.tight_layout()\nplt.show()\n\n\nFigure 53.1",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#sec-textual-event-study",
    "href": "70_textual.html#sec-textual-event-study",
    "title": "44  Textual Analysis",
    "section": "53.3 Measuring Textual Similarity Changes Around Corporate Events",
    "text": "53.3 Measuring Textual Similarity Changes Around Corporate Events\nWe examine how firms’ textual similarity changes around major corporate events such as M&A announcements, industry reclassifications, and strategic pivots. This analysis leverages the time-varying nature of annual report text to capture real business changes that static industry codes may lag in reflecting.\n\n# Get M&A announcements from DataCore\nma_events = dc.get_corporate_events(\n    event_type='M&A',\n    start='2016-01-01', end='2024-12-31'\n)\n\n# For each M&A event, compute text similarity between\n# acquirer and target before and after the event\ndef text_similarity_around_event(\n    acquirer: str, target: str, event_year: int,\n    annual_text_df: pd.DataFrame,\n    vectorizer: TfidfVectorizer\n) -&gt; dict:\n    \"\"\"Compare text similarity pre vs post M&A.\"\"\"\n    pre_texts = annual_text_df[\n        (annual_text_df.ticker.isin([acquirer, target])) &\n        (annual_text_df.year == event_year - 1)\n    ]\n    post_texts = annual_text_df[\n        (annual_text_df.ticker.isin([acquirer, target])) &\n        (annual_text_df.year == event_year + 1)\n    ]\n\n    if len(pre_texts) &lt; 2 or len(post_texts) &lt; 2:\n        return None\n\n    pre_vecs = vectorizer.transform(pre_texts.text_clean)\n    post_vecs = vectorizer.transform(post_texts.text_clean)\n\n    pre_sim = cosine_similarity(pre_vecs[0:1], pre_vecs[1:2])[0, 0]\n    post_sim = cosine_similarity(post_vecs[0:1], post_vecs[1:2])[0, 0]\n\n    return {\n        'acquirer': acquirer,\n        'target': target,\n        'event_year': event_year,\n        'pre_similarity': pre_sim,\n        'post_similarity': post_sim,\n        'delta_similarity': post_sim - pre_sim\n    }\n\n# Apply to all M&A events\nevent_results = []\nfor _, event in ma_events.iterrows():\n    result = text_similarity_around_event(\n        event.acquirer, event.target, event.event_year,\n        annual_text, tfidf_vectorizer\n    )\n    if result:\n        event_results.append(result)\n\nevent_df = pd.DataFrame(event_results)\nprint(f'Average similarity change post-M&A: '\n      f'{event_df.delta_similarity.mean():.4f}')\nprint(f't-stat: {event_df.delta_similarity.mean() / '\n      f'(event_df.delta_similarity.std() / '\n      f'np.sqrt(len(event_df))):.3f}')",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "70_textual.html#footnotes",
    "href": "70_textual.html#footnotes",
    "title": "44  Textual Analysis",
    "section": "",
    "text": "Vietnamese text requires specialized tokenization due to compound words (e.g., “công ty” = company, “thị trường” = market).↩︎\nLoughran and McDonald (2011) show that general-purpose dictionaries misclassify up to 73% of negative words in financial text.↩︎\nReimers and Gurevych (2019) demonstrate that sentence-BERT embeddings reduce computation for similarity tasks from 65 hours to 5 seconds on 10,000 sentence pairs.↩︎",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Textual Analysis</span>"
    ]
  },
  {
    "objectID": "71_image.html",
    "href": "71_image.html",
    "title": "45  Image and Visual Data in Finance",
    "section": "",
    "text": "45.1 Foundations: From Pixels to Financial Signals\nThe previous chapter demonstrated how unstructured text (e.g., earnings reports, news articles, business descriptions) can be transformed into structured signals for financial analysis. This chapter extends the alternative data toolkit to a second modality: images. Visual data is abundant in financial contexts yet systematically underexploited. Satellite photographs reveal real economic activity (e.g., parking lot occupancy at retail locations, construction progress at industrial sites, nighttime luminosity as a proxy for regional GDP, ship traffic at port terminals, crop health across agricultural zones). Corporate documents arrive as scanned PDFs whose tables and figures resist standard text extraction. Financial charts encode information that analysts interpret visually but that systematic strategies cannot consume without digitization. And the visual content of social media, advertising, and product imagery carries sentiment and brand signals that complement textual analysis.\nThe core challenge is representational: an image is a three-dimensional tensor of pixel intensities with no inherent semantic structure. Converting this raw array into a financial signal (i.e., a number that predicts returns, measures risk, or proxies for economic activity) requires either hand-crafted feature engineering or learned representations via deep convolutional neural networks (CNNs) and vision transformers (ViTs). This chapter covers both approaches.\nWe organize the material around five application domains, each with distinct data sources, modeling requirements, and economic motivations. First, satellite and geospatial imagery for nowcasting economic activity. Second, document image analysis for extracting structured data from Vietnamese financial filings. Third, chart and figure digitization for systematic backtesting. Fourth, visual sentiment analysis from social and news media. Fifth, multimodal fusion, combining image and text signals into joint predictive models.\nVietnamese markets present particular opportunities in this space. Satellite imagery is especially informative in an economy with large agricultural and manufacturing sectors where ground-truth data arrives with significant lags. Vietnamese financial filings are often distributed as scanned images rather than machine-readable formats, making document AI essential rather than optional. And the rapid urbanization visible in construction and infrastructure imagery provides high-frequency proxies for macroeconomic momentum that official statistics cannot match.",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Image and Visual Data in Finance</span>"
    ]
  },
  {
    "objectID": "71_image.html#foundations-from-pixels-to-financial-signals",
    "href": "71_image.html#foundations-from-pixels-to-financial-signals",
    "title": "45  Image and Visual Data in Finance",
    "section": "",
    "text": "45.1.1 Image Representation\nA digital image is a function \\(I: \\{1, \\ldots, H\\} \\times \\{1, \\ldots, W\\} \\times \\{1, \\ldots, C\\} \\rightarrow [0, 255]\\) mapping spatial coordinates and color channels to intensity values. For an RGB image of height \\(H\\) and width \\(W\\), the representation is a tensor \\(\\mathbf{I} \\in \\mathbb{R}^{H \\times W \\times 3}\\). A single \\(224 \\times 224\\) RGB image (i.e., the standard input for modern CNNs) contains \\(224 \\times 224 \\times 3 = 150{,}528\\) dimensions. This extreme dimensionality, combined with spatial structure (nearby pixels are correlated), makes images fundamentally different from tabular financial data and demands specialized architectures.\nThe key insight of convolutional neural networks is parameter sharing via local filters. A convolutional layer applies a kernel \\(\\mathbf{K} \\in \\mathbb{R}^{k \\times k \\times C_{\\text{in}}}\\) to produce a feature map:\n\\[\n(\\mathbf{I} * \\mathbf{K})(i, j) = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} \\sum_{c=1}^{C_{\\text{in}}} I(i+m, j+n, c) \\cdot K(m, n, c)\n\\tag{45.1}\\]\nBy stacking convolutional layers with nonlinearities and pooling operations, the network builds a hierarchy of representations: early layers detect edges and textures; middle layers detect parts and patterns; deep layers detect objects and scenes. The final layer output \\(\\mathbf{z} \\in \\mathbb{R}^{d}\\) (with \\(d\\) typically 512-2048) is a compact representation of the image’s semantic content, which can be used directly as a feature vector for financial prediction.\nFinancial images span a wide range of resolutions and modalities:\n\n\n\nTable 45.1: Image Data Sources for Vietnamese Financial Markets\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nResolution\nChannels\nTypical Size\nUpdate Frequency\n\n\n\n\nSentinel-2 satellite\n10m/pixel\n13 bands\n10,980 × 10,980\n5 days\n\n\nPlanet Labs\n3m/pixel\n4 bands\n4,000 × 4,000\nDaily\n\n\nVIIRS nightlights\n500m/pixel\n1 (DNB)\n3,000 × 1,800\nMonthly composite\n\n\nAnnual report scan\n300 DPI\n3 (RGB)\n2,480 × 3,508\nAnnual\n\n\nCEO photograph\nVaries\n3 (RGB)\n500 × 500\nAnnual\n\n\nNews photograph\nVaries\n3 (RGB)\n800 × 600\nReal-time\n\n\nFinancial chart\nVaries\n3 (RGB)\n1,000 × 600\nReal-time\n\n\n\n\n\n\n\n\n45.1.2 Transfer Learning for Finance\nTraining a CNN from scratch requires millions of labeled images, which is far more than any financial application can provide. Transfer learning solves this by using networks pre-trained on ImageNet (1.2 million images, 1,000 classes) as feature extractors. The pre-trained network has already learned generic visual representations (edges, textures, shapes, objects); we simply replace the final classification layer with a task-specific head.\nFormally, let \\(f_{\\boldsymbol{\\theta}}(\\mathbf{I})\\) denote a pre-trained network with parameters \\(\\boldsymbol{\\theta}\\) partitioned into feature extractor \\(\\boldsymbol{\\theta}_{\\text{feat}}\\) and classifier \\(\\boldsymbol{\\theta}_{\\text{cls}}\\). For financial applications, we:\n\nFeature extraction: Freeze \\(\\boldsymbol{\\theta}_{\\text{feat}}\\), extract \\(\\mathbf{z} = f_{\\boldsymbol{\\theta}_{\\text{feat}}}(\\mathbf{I})\\), and train a simple model (linear regression, gradient boosting) on \\(\\mathbf{z}\\).\nFine-tuning: Initialize from \\(\\boldsymbol{\\theta}\\) and train all parameters on the financial task with a small learning rate to avoid catastrophic forgetting.\n\nThe key architectural families are:\nResNet [He et al. (2016)]. Residual connections (\\(y = F(x) + x\\)) enable training of very deep networks (50-152 layers). The skip connection solves the vanishing gradient problem. ResNet-50 produces a 2,048-dimensional feature vector from the penultimate layer.\nEfficientNet [Tan and Le (2019)]. Compound scaling of depth, width, and resolution simultaneously. EfficientNet-B0 achieves ResNet-50 accuracy with 5.3M parameters (vs. 25.6M), making it practical for processing thousands of satellite tiles.\nVision Transformer (ViT) [Dosovitskiy et al. (2020)]. Treats an image as a sequence of \\(16 \\times 16\\) patches, processes them through a standard Transformer encoder. ViT-B/16 produces a 768-dimensional embedding. Particularly effective for document images where spatial relationships between elements (tables, headers, text blocks) matter.\n\n# DataCore.vn API\nfrom datacore import DataCore\ndc = DataCore()\n\ndef build_feature_extractor(model_name=\"resnet50\", device=\"cpu\"):\n    \"\"\"\n    Build a pre-trained CNN feature extractor.\n\n    Parameters\n    ----------\n    model_name : str\n        One of 'resnet50', 'efficientnet_b0', 'vit_b_16'.\n    device : str\n        'cpu' or 'cuda'.\n\n    Returns\n    -------\n    model : nn.Module\n        Feature extraction model.\n    transform : transforms.Compose\n        Image preprocessing pipeline.\n    \"\"\"\n    if model_name == \"resnet50\":\n        weights = models.ResNet50_Weights.IMAGENET1K_V2\n        model = models.resnet50(weights=weights)\n        model.fc = nn.Identity()  # Remove classification head\n        dim = 2048\n    elif model_name == \"efficientnet_b0\":\n        weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1\n        model = models.efficientnet_b0(weights=weights)\n        model.classifier = nn.Identity()\n        dim = 1280\n    elif model_name == \"vit_b_16\":\n        weights = models.ViT_B_16_Weights.IMAGENET1K_V1\n        model = models.vit_b_16(weights=weights)\n        model.heads = nn.Identity()\n        dim = 768\n\n    model = model.to(device).eval()\n\n    transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )\n    ])\n\n    return model, transform, dim\n\n\ndef extract_features(image_paths, model, transform, device=\"cpu\",\n                     batch_size=32):\n    \"\"\"\n    Extract deep features from a list of images.\n\n    Parameters\n    ----------\n    image_paths : list\n        Paths to image files.\n    model : nn.Module\n        Feature extraction model.\n    transform : transforms.Compose\n        Preprocessing pipeline.\n\n    Returns\n    -------\n    np.ndarray : Feature matrix (n_images x feature_dim).\n    \"\"\"\n    features = []\n\n    for i in range(0, len(image_paths), batch_size):\n        batch_paths = image_paths[i:i + batch_size]\n        batch_tensors = []\n\n        for path in batch_paths:\n            try:\n                img = Image.open(path).convert(\"RGB\")\n                tensor = transform(img)\n                batch_tensors.append(tensor)\n            except Exception:\n                batch_tensors.append(torch.zeros(3, 224, 224))\n\n        batch = torch.stack(batch_tensors).to(device)\n\n        with torch.no_grad():\n            batch_features = model(batch).cpu().numpy()\n\n        features.append(batch_features)\n\n    return np.vstack(features)",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Image and Visual Data in Finance</span>"
    ]
  },
  {
    "objectID": "71_image.html#satellite-and-geospatial-imagery",
    "href": "71_image.html#satellite-and-geospatial-imagery",
    "title": "45  Image and Visual Data in Finance",
    "section": "45.2 Satellite and Geospatial Imagery",
    "text": "45.2 Satellite and Geospatial Imagery\n\n45.2.1 Economic Activity from Space\nSatellite imagery provides high-frequency, spatially granular measurements of economic activity that are independent of and often lead official statistics. The foundational work of Henderson, Storeygard, and Weil (2012) demonstrated that nighttime luminosity, measured by the Defense Meteorological Satellite Program (DMSP), is a reliable proxy for GDP, particularly in countries where official statistics are noisy or delayed. Donaldson (2018) use satellite-derived agricultural output measures to study the welfare gains from railroads in colonial India. Jean et al. (2016) combine daytime satellite imagery with CNNs to predict poverty from space with \\(r^2 &gt; 0.7\\).\nFor financial applications, the key insight is that satellite data arrives faster than corporate earnings or government statistics. A retailer’s quarterly revenue is reported 4-8 weeks after the quarter ends; satellite imagery of its parking lots is available within days. This temporal advantage creates a natural use case for nowcasting (i.e., estimating current economic conditions before official data arrives) and for constructing trading signals based on information that is public but costly to process.\n\n\n45.2.2 Application 1: Nighttime Luminosity and Provincial GDP\nVietnam’s General Statistics Office (GSO) publishes provincial GDP with a lag of several months. Nighttime luminosity from the VIIRS (Visible Infrared Imaging Radiometer Suite) sensor provides a near-real-time alternative. We construct a firm-level exposure measure by linking each listed firm’s registered location to the luminosity of its province.\n\n# Load nighttime luminosity data (VIIRS monthly composites)\n# Source: Earth Observation Group (EOG) / NOAA\nnightlights = dc.get_nightlight_data(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    resolution=\"province\"\n)\n\n# Load firm location data\nfirm_locations = dc.get_firm_locations()\n\n# Provincial GDP from GSO\nprovincial_gdp = dc.get_provincial_gdp(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\n\nprint(f\"Nightlight observations: {len(nightlights)}\")\nprint(f\"Provinces covered: {nightlights['province'].nunique()}\")\nprint(f\"Firms with location: {firm_locations['ticker'].nunique()}\")\n\n\n# Validate: does nightlight predict provincial GDP?\nnl_gdp = nightlights.merge(\n    provincial_gdp,\n    on=[\"province\", \"year\", \"quarter\"],\n    how=\"inner\"\n)\n\n# Log-log specification (standard in the literature)\nnl_gdp[\"ln_luminosity\"] = np.log(nl_gdp[\"mean_radiance\"].clip(lower=0.01))\nnl_gdp[\"ln_gdp\"] = np.log(nl_gdp[\"provincial_gdp\"].clip(lower=1))\n\n# Cross-sectional regression by year\nvalidation_results = []\nfor year in nl_gdp[\"year\"].unique():\n    subset = nl_gdp[nl_gdp[\"year\"] == year]\n    if len(subset) &lt; 20:\n        continue\n\n    model = sm.OLS(\n        subset[\"ln_gdp\"],\n        sm.add_constant(subset[\"ln_luminosity\"])\n    ).fit()\n\n    validation_results.append({\n        \"year\": year,\n        \"beta\": model.params.iloc[1],\n        \"r_squared\": model.rsquared,\n        \"n_provinces\": int(model.nobs)\n    })\n\nvalidation_df = pd.DataFrame(validation_results)\nprint(f\"Avg R² (ln GDP ~ ln Luminosity): {validation_df['r_squared'].mean():.3f}\")\n\n\n\n\n(\n    p9.ggplot(nl_gdp[nl_gdp[\"year\"] == 2023],\n              p9.aes(x=\"ln_luminosity\", y=\"ln_gdp\"))\n    + p9.geom_point(color=\"#2E5090\", alpha=0.6, size=2)\n    + p9.geom_smooth(method=\"lm\", color=\"#C0392B\", se=True, size=0.8)\n    + p9.labs(\n        x=\"ln(Mean Nighttime Radiance)\",\n        y=\"ln(Provincial GDP)\",\n        title=\"Nighttime Luminosity vs. Provincial GDP (2023)\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 6))\n)\n\n\nFigure 45.1\n\n\n\n\n# Construct firm-level nightlight signal\n# Logic: firms in provinces with accelerating luminosity\n# are experiencing positive local economic conditions\n\nnl_growth = nightlights.copy().sort_values([\"province\", \"year\", \"quarter\"])\n\n# Year-over-year luminosity growth by province\nnl_growth[\"ln_radiance\"] = np.log(nl_growth[\"mean_radiance\"].clip(lower=0.01))\nnl_growth[\"ln_radiance_lag4\"] = nl_growth.groupby(\n    \"province\"\n)[\"ln_radiance\"].shift(4)\n\nnl_growth[\"nl_growth\"] = nl_growth[\"ln_radiance\"] - nl_growth[\"ln_radiance_lag4\"]\n\n# Merge with firms via province\nfirm_nl = firm_locations[[\"ticker\", \"province\"]].merge(\n    nl_growth[[\"province\", \"year\", \"quarter\", \"nl_growth\"]],\n    on=\"province\",\n    how=\"inner\"\n)\n\n# Merge with stock returns\nmonthly_returns = dc.get_monthly_returns(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\nmonthly_returns[\"year\"] = monthly_returns[\"date\"].dt.year\nmonthly_returns[\"quarter\"] = monthly_returns[\"date\"].dt.quarter\n\nreturns_with_nl = monthly_returns.merge(\n    firm_nl,\n    on=[\"ticker\", \"year\", \"quarter\"],\n    how=\"inner\"\n)\n\n\n# Portfolio sort: quintiles on provincial nightlight growth\nreturns_with_nl[\"nl_quintile\"] = returns_with_nl.groupby(\"date\")[\n    \"nl_growth\"\n].transform(lambda x: pd.qcut(x, 5, labels=[1, 2, 3, 4, 5],\n                                duplicates=\"drop\"))\n\nnl_port_returns = (\n    returns_with_nl.groupby([\"date\", \"nl_quintile\"])\n    .agg(port_ret=(\"ret\", \"mean\"))\n    .reset_index()\n)\n\nnl_wide = nl_port_returns.pivot(\n    index=\"date\", columns=\"nl_quintile\", values=\"port_ret\"\n)\nnl_wide[\"L-S\"] = nl_wide[5] - nl_wide[1]  # High NL growth - Low\n\n\n\n\nTable 45.2: Nighttime Luminosity Growth Quintile Portfolio Returns\n\n\nnl_summary = nl_wide.describe().T[[\"mean\", \"std\"]].copy()\nnl_summary[\"mean_ann\"] = nl_summary[\"mean\"] * 12\nnl_summary[\"sharpe\"] = (\n    nl_summary[\"mean_ann\"] / (nl_summary[\"std\"] * np.sqrt(12))\n)\nfor col in nl_wide.columns:\n    t_stat = nl_wide[col].mean() / (\n        nl_wide[col].std() / np.sqrt(len(nl_wide.dropna()))\n    )\n    nl_summary.loc[col, \"t_stat\"] = t_stat\n\nnl_summary = nl_summary[[\"mean_ann\", \"sharpe\", \"t_stat\"]].round(4)\nnl_summary.columns = [\"Ann. Return\", \"Sharpe\", \"t-stat\"]\nnl_summary\n\n\n\n\n\n45.2.3 Application 2: Satellite Imagery for Sector Nowcasting\nBeyond luminosity, daytime satellite imagery provides sector-specific signals. We implement three channels relevant to the Vietnamese economy.\nPort activity. Vietnam is a major export-oriented economy. Satellite imagery of container ports (Cát Lái, Hải Phòng) captures trade throughput before customs statistics are released. Ship detection algorithms applied to synthetic aperture radar (SAR) imagery count vessels and estimate cargo volumes.\nConstruction progress. Real estate and construction constitute a significant fraction of Vietnamese GDP and market capitalization. Change detection algorithms applied to high-resolution optical imagery identify construction starts, completion rates, and land-use conversion.\nAgricultural monitoring. Vietnam is a leading exporter of rice, coffee, rubber, and seafood. The Normalized Difference Vegetation Index (NDVI), computed from multispectral satellite data, provides crop health assessments:\n\\[\n\\text{NDVI} = \\frac{\\rho_{\\text{NIR}} - \\rho_{\\text{Red}}}{\\rho_{\\text{NIR}} + \\rho_{\\text{Red}}}\n\\tag{45.2}\\]\nwhere \\(\\rho_{\\text{NIR}}\\) and \\(\\rho_{\\text{Red}}\\) are reflectance in the near-infrared and red bands. NDVI ranges from \\(-1\\) to \\(+1\\), with values above 0.3 indicating healthy vegetation. Deviations from seasonal norms proxy for crop yield surprises.\n\n# Load NDVI data for Vietnamese agricultural regions\n# Source: MODIS/Terra (MOD13Q1, 250m resolution, 16-day composites)\nndvi_data = dc.get_ndvi_data(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    regions=[\"mekong_delta\", \"central_highlands\",\n             \"red_river_delta\", \"southeast\"]\n)\n\n# Compute NDVI anomaly: deviation from 5-year seasonal average\nndvi_data[\"month\"] = ndvi_data[\"date\"].dt.month\n\nseasonal_mean = (\n    ndvi_data.groupby([\"region\", \"month\"])\n    [\"mean_ndvi\"].transform(\n        lambda x: x.rolling(5 * 12, min_periods=12).mean()\n    )\n)\nndvi_data[\"ndvi_anomaly\"] = ndvi_data[\"mean_ndvi\"] - seasonal_mean\n\n# Agricultural sector firms\nagri_firms = dc.get_firms_by_sector(sector=\"agriculture\")\n\n# Link NDVI anomaly to agricultural firm returns\nagri_returns = monthly_returns[\n    monthly_returns[\"ticker\"].isin(agri_firms[\"ticker\"])\n].copy()\n\nagri_returns[\"month\"] = agri_returns[\"date\"].dt.month\nagri_returns[\"year\"] = agri_returns[\"date\"].dt.year\n\n# Regional NDVI aggregation (Mekong Delta for rice firms, etc.)\nmekong_ndvi = ndvi_data[ndvi_data[\"region\"] == \"mekong_delta\"].copy()\nmekong_ndvi[\"year\"] = mekong_ndvi[\"date\"].dt.year\nmekong_ndvi[\"month\"] = mekong_ndvi[\"date\"].dt.month\n\nmekong_monthly = (\n    mekong_ndvi.groupby([\"year\", \"month\"])\n    .agg(ndvi_anomaly=(\"ndvi_anomaly\", \"mean\"))\n    .reset_index()\n)\n\n\n\n\nndvi_plot = ndvi_data[ndvi_data[\"region\"] == \"mekong_delta\"].copy()\n\n(\n    p9.ggplot(ndvi_plot, p9.aes(x=\"date\", y=\"ndvi_anomaly\"))\n    + p9.geom_line(color=\"#27AE60\", alpha=0.5, size=0.4)\n    + p9.geom_smooth(method=\"lowess\", color=\"#2E5090\", size=1, se=False)\n    + p9.geom_hline(yintercept=0, linetype=\"dashed\", color=\"gray\")\n    + p9.labs(\n        x=\"\",\n        y=\"NDVI Anomaly\",\n        title=\"Mekong Delta Vegetation Health: Deviation from Seasonal Norm\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(12, 5))\n)\n\n\nFigure 45.2\n\n\n\n\n# Panel regression: agricultural firm returns on NDVI anomaly\nagri_panel = agri_returns.merge(\n    mekong_monthly,\n    on=[\"year\", \"month\"],\n    how=\"inner\"\n)\n\n# Lagged NDVI anomaly (one month)\nagri_panel = agri_panel.sort_values([\"ticker\", \"date\"])\nagri_panel[\"ndvi_lag1\"] = agri_panel.groupby(\n    \"ticker\"\n)[\"ndvi_anomaly\"].shift(1)\n\nagri_clean = agri_panel.dropna(\n    subset=[\"ret\", \"ndvi_lag1\"]\n).set_index([\"ticker\", \"date\"])\n\nmodel_ndvi = PanelOLS(\n    agri_clean[\"ret\"],\n    agri_clean[[\"ndvi_lag1\"]],\n    entity_effects=True,\n    time_effects=True,\n    check_rank=False\n).fit(cov_type=\"clustered\", cluster_entity=True)\n\nagri_clean = agri_clean.reset_index()\n\nprint(f\"NDVI → Agricultural Returns:\")\nprint(f\"  β(NDVI_lag): {model_ndvi.params['ndvi_lag1']:.4f}\")\nprint(f\"  t-stat: {model_ndvi.tstats['ndvi_lag1']:.3f}\")\nprint(f\"  R² (within): {model_ndvi.rsquared_within:.4f}\")\n\n\n\n45.2.4 Satellite Feature Extraction with CNNs\nFor raw satellite imagery (rather than pre-computed indices like NDVI), we use transfer learning from CNNs to extract spatial features. The approach follows Jean et al. (2016): use a CNN pre-trained on ImageNet to extract feature vectors from satellite tiles, then regress economic outcomes on these features.\n\ndef satellite_feature_pipeline(image_dir, model_name=\"resnet50\"):\n    \"\"\"\n    Extract CNN features from satellite image tiles.\n\n    Parameters\n    ----------\n    image_dir : str or Path\n        Directory containing satellite tiles (PNG/TIFF).\n    model_name : str\n        Pre-trained model to use.\n\n    Returns\n    -------\n    DataFrame : image_id, feature vector columns.\n    \"\"\"\n    image_dir = Path(image_dir)\n    image_paths = sorted(image_dir.glob(\"*.png\")) + sorted(\n        image_dir.glob(\"*.tif\")\n    )\n\n    if not image_paths:\n        print(\"No images found.\")\n        return pd.DataFrame()\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, transform, dim = build_feature_extractor(model_name, device)\n\n    features = extract_features(image_paths, model, transform, device)\n\n    # Create DataFrame\n    feature_cols = [f\"feat_{i}\" for i in range(dim)]\n    df = pd.DataFrame(features, columns=feature_cols)\n    df[\"image_id\"] = [p.stem for p in image_paths]\n\n    return df\n\n\ndef predict_economic_activity(features_df, labels_df, label_col,\n                              n_components=50):\n    \"\"\"\n    Predict economic activity from satellite image features.\n\n    Uses PCA for dimensionality reduction, then ridge regression.\n\n    Parameters\n    ----------\n    features_df : DataFrame\n        CNN features with image_id.\n    labels_df : DataFrame\n        Economic outcomes with image_id.\n    label_col : str\n        Target variable column name.\n    n_components : int\n        PCA components to retain.\n\n    Returns\n    -------\n    dict : R², coefficients, cross-validated performance.\n    \"\"\"\n    from sklearn.decomposition import PCA\n    from sklearn.linear_model import RidgeCV\n    from sklearn.model_selection import cross_val_score\n\n    merged = features_df.merge(labels_df, on=\"image_id\")\n    feature_cols = [c for c in features_df.columns if c.startswith(\"feat_\")]\n\n    X = merged[feature_cols].values\n    y = merged[label_col].values\n\n    # PCA\n    pca = PCA(n_components=n_components)\n    X_pca = pca.fit_transform(X)\n    var_explained = pca.explained_variance_ratio_.sum()\n\n    # Ridge regression with cross-validation\n    ridge = RidgeCV(alphas=np.logspace(-3, 3, 20), cv=5)\n    cv_scores = cross_val_score(ridge, X_pca, y, cv=5, scoring=\"r2\")\n\n    ridge.fit(X_pca, y)\n\n    return {\n        \"r2_cv_mean\": cv_scores.mean(),\n        \"r2_cv_std\": cv_scores.std(),\n        \"r2_train\": ridge.score(X_pca, y),\n        \"pca_var_explained\": var_explained,\n        \"optimal_alpha\": ridge.alpha_,\n        \"n_images\": len(merged)\n    }",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Image and Visual Data in Finance</span>"
    ]
  },
  {
    "objectID": "71_image.html#document-image-analysis",
    "href": "71_image.html#document-image-analysis",
    "title": "45  Image and Visual Data in Finance",
    "section": "45.3 Document Image Analysis",
    "text": "45.3 Document Image Analysis\n\n45.3.1 The Vietnamese Filing Problem\nA substantial fraction of Vietnamese corporate disclosures (e.g., annual reports, financial statements, board resolutions, shareholder meeting minutes) are distributed as scanned PDF images rather than machine-readable text. This creates a data extraction bottleneck: the information exists but is trapped in pixel format. Unlike filings in more developed markets (where XBRL mandates ensure machine readability), Vietnamese filings require Optical Character Recognition (OCR) and layout analysis before any quantitative analysis can begin.\nThe document AI pipeline for Vietnamese financial filings involves four stages:\n\nPage classification: Identify which pages contain financial statements, management discussion, audit opinions, etc.\nLayout analysis: Detect the spatial structure such as headers, paragraphs, tables, figures, captions.\nOCR: Convert image regions to text, using Vietnamese-optimized models.\nStructured extraction: Parse the recognized text into structured data (e.g., revenue figures, balance sheet items).\n\n\n\n45.3.2 OCR for Vietnamese Financial Documents\nStandard OCR engines (Tesseract, Google Cloud Vision) struggle with Vietnamese financial documents due to the combination of Vietnamese diacritics (ă, ơ, ư, ê, etc.), mixed Vietnamese-English content, and complex table layouts. We implement a pipeline using PaddleOCR (which has strong CJK and Southeast Asian language support) and VietOCR (a Vietnamese-specific model based on the transformer architecture of Baek et al. (2019)).\n\ndef ocr_financial_document(pdf_path, language=\"vi\",\n                           engine=\"paddleocr\"):\n    \"\"\"\n    OCR a Vietnamese financial document (scanned PDF).\n\n    Parameters\n    ----------\n    pdf_path : str\n        Path to PDF file.\n    language : str\n        Language code.\n    engine : str\n        'paddleocr' or 'vietocr'.\n\n    Returns\n    -------\n    list[dict] : Per-page OCR results with bounding boxes.\n    \"\"\"\n    from pdf2image import convert_from_path\n\n    # Convert PDF pages to images\n    pages = convert_from_path(pdf_path, dpi=300)\n\n    results = []\n\n    if engine == \"paddleocr\":\n        from paddleocr import PaddleOCR\n        ocr = PaddleOCR(use_angle_cls=True, lang=\"vi\", use_gpu=False)\n\n        for page_num, page_img in enumerate(pages):\n            # Convert PIL to numpy\n            img_array = np.array(page_img)\n            ocr_result = ocr.ocr(img_array, cls=True)\n\n            page_texts = []\n            for line in ocr_result[0]:\n                bbox, (text, confidence) = line\n                page_texts.append({\n                    \"text\": text,\n                    \"confidence\": confidence,\n                    \"bbox\": bbox,\n                    \"page\": page_num + 1\n                })\n\n            results.extend(page_texts)\n\n    return results\n\n\ndef classify_page_type(ocr_results, page_num):\n    \"\"\"\n    Classify a document page by content type using keyword matching.\n\n    Returns one of: 'balance_sheet', 'income_statement',\n    'cash_flow', 'notes', 'audit', 'management', 'other'.\n    \"\"\"\n    page_text = \" \".join(\n        [r[\"text\"] for r in ocr_results if r[\"page\"] == page_num]\n    ).lower()\n\n    # Vietnamese financial statement keywords\n    keyword_map = {\n        \"balance_sheet\": [\n            \"bảng cân đối kế toán\", \"tài sản\", \"nguồn vốn\",\n            \"nợ phải trả\", \"vốn chủ sở hữu\"\n        ],\n        \"income_statement\": [\n            \"kết quả hoạt động kinh doanh\", \"doanh thu\",\n            \"lợi nhuận\", \"chi phí\", \"thu nhập\"\n        ],\n        \"cash_flow\": [\n            \"lưu chuyển tiền tệ\", \"dòng tiền\",\n            \"hoạt động kinh doanh\", \"hoạt động đầu tư\"\n        ],\n        \"audit\": [\n            \"báo cáo kiểm toán\", \"kiểm toán viên\",\n            \"ý kiến kiểm toán\", \"trung thực và hợp lý\"\n        ],\n        \"management\": [\n            \"ban giám đốc\", \"hội đồng quản trị\",\n            \"báo cáo thường niên\", \"tình hình hoạt động\"\n        ]\n    }\n\n    scores = {}\n    for page_type, keywords in keyword_map.items():\n        scores[page_type] = sum(\n            1 for kw in keywords if kw in page_text\n        )\n\n    if max(scores.values()) == 0:\n        return \"other\"\n    return max(scores, key=scores.get)\n\n\n\n45.3.3 Table Extraction from Financial Statements\nThe highest-value extraction task is recovering structured tables from financial statements. We implement a two-stage approach: first detect table regions using a layout analysis model, then parse the detected regions into row-column structure.\n\ndef extract_tables_from_page(page_image, ocr_results, page_num):\n    \"\"\"\n    Extract structured tables from a document page.\n\n    Uses spatial clustering of OCR bounding boxes to identify\n    table regions, then aligns text into rows and columns.\n\n    Parameters\n    ----------\n    page_image : PIL.Image\n        Page image.\n    ocr_results : list[dict]\n        OCR results for this page.\n    page_num : int\n        Page number.\n\n    Returns\n    -------\n    list[pd.DataFrame] : Extracted tables as DataFrames.\n    \"\"\"\n    page_texts = [r for r in ocr_results if r[\"page\"] == page_num]\n\n    if not page_texts:\n        return []\n\n    # Extract bounding box centers\n    centers = []\n    for item in page_texts:\n        bbox = item[\"bbox\"]\n        # bbox is [[x1,y1],[x2,y2],[x3,y3],[x4,y4]]\n        cx = np.mean([p[0] for p in bbox])\n        cy = np.mean([p[1] for p in bbox])\n        centers.append((cx, cy, item[\"text\"]))\n\n    if not centers:\n        return []\n\n    centers_df = pd.DataFrame(centers, columns=[\"x\", \"y\", \"text\"])\n\n    # Cluster into rows by y-coordinate proximity\n    centers_df = centers_df.sort_values(\"y\")\n    row_threshold = 15  # pixels\n    centers_df[\"row_id\"] = (\n        centers_df[\"y\"].diff().abs() &gt; row_threshold\n    ).cumsum()\n\n    # Within each row, sort by x-coordinate\n    tables = []\n    rows = []\n    for row_id, row_group in centers_df.groupby(\"row_id\"):\n        row_sorted = row_group.sort_values(\"x\")\n        rows.append(row_sorted[\"text\"].tolist())\n\n    if len(rows) &gt; 2:\n        # Attempt to construct DataFrame\n        max_cols = max(len(r) for r in rows)\n        # Pad shorter rows\n        padded = [r + [\"\"] * (max_cols - len(r)) for r in rows]\n\n        try:\n            df = pd.DataFrame(padded[1:], columns=padded[0])\n            tables.append(df)\n        except Exception:\n            tables.append(pd.DataFrame(padded))\n\n    return tables\n\n\ndef parse_financial_numbers(text):\n    \"\"\"\n    Parse Vietnamese financial number formats.\n    Vietnamese uses dots as thousands separators and commas as decimals.\n    E.g., '1.234.567' = 1234567, '1.234,56' = 1234.56\n    \"\"\"\n    import re\n    text = text.strip().replace(\" \", \"\")\n\n    # Remove parentheses (negative indicator)\n    negative = text.startswith(\"(\") and text.endswith(\")\")\n    text = text.strip(\"()\")\n\n    # Handle Vietnamese number format\n    # If comma is present, it's a decimal separator\n    if \",\" in text:\n        text = text.replace(\".\", \"\").replace(\",\", \".\")\n    else:\n        text = text.replace(\".\", \"\")\n\n    try:\n        value = float(text)\n        return -value if negative else value\n    except ValueError:\n        return np.nan\n\n\n\n45.3.4 Layout-Aware Document Understanding\nModern document AI goes beyond OCR by jointly modeling text content and spatial layout. LayoutLM (Huang et al. 2022) and its successors treat each token as having both a text embedding and a positional embedding derived from its bounding box coordinates. This allows the model to understand that a number positioned below a “Revenue” header and to the right of “2023” is the 2023 revenue figure, even without explicit table detection.\n\ndef layoutlm_extract(document_pages, model_name=\"layoutlmv3\"):\n    \"\"\"\n    Extract structured financial data using LayoutLM.\n\n    This function uses the pre-trained LayoutLMv3 model for\n    document understanding with Vietnamese financial statements.\n\n    Parameters\n    ----------\n    document_pages : list\n        List of (page_image, ocr_results) tuples.\n    model_name : str\n        Model variant.\n\n    Returns\n    -------\n    dict : Extracted financial fields.\n    \"\"\"\n    from transformers import (\n        LayoutLMv3ForTokenClassification,\n        LayoutLMv3Processor\n    )\n\n    processor = LayoutLMv3Processor.from_pretrained(\n        \"microsoft/layoutlmv3-base\",\n        apply_ocr=False  # We provide our own OCR\n    )\n\n    model = LayoutLMv3ForTokenClassification.from_pretrained(\n        \"microsoft/layoutlmv3-base\",\n        num_labels=13  # Financial statement field types\n    )\n\n    # Define target fields for extraction\n    field_labels = [\n        \"O\",  # Other\n        \"B-REVENUE\", \"I-REVENUE\",\n        \"B-COGS\", \"I-COGS\",\n        \"B-NET_INCOME\", \"I-NET_INCOME\",\n        \"B-TOTAL_ASSETS\", \"I-TOTAL_ASSETS\",\n        \"B-TOTAL_EQUITY\", \"I-TOTAL_EQUITY\",\n        \"B-TOTAL_DEBT\", \"I-TOTAL_DEBT\"\n    ]\n\n    extracted = {}\n\n    for page_img, ocr_results in document_pages:\n        words = [r[\"text\"] for r in ocr_results]\n        boxes = []\n        for r in ocr_results:\n            bbox = r[\"bbox\"]\n            # Normalize to 0-1000 range\n            x0 = min(p[0] for p in bbox)\n            y0 = min(p[1] for p in bbox)\n            x1 = max(p[0] for p in bbox)\n            y1 = max(p[1] for p in bbox)\n            boxes.append([int(x0), int(y0), int(x1), int(y1)])\n\n        if not words:\n            continue\n\n        # Process through LayoutLM\n        encoding = processor(\n            page_img,\n            words,\n            boxes=boxes,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=512\n        )\n\n        with torch.no_grad():\n            outputs = model(**encoding)\n\n        predictions = outputs.logits.argmax(-1).squeeze().tolist()\n\n        # Extract labeled entities\n        for idx, pred in enumerate(predictions):\n            if pred &gt; 0 and idx &lt; len(words):\n                label = field_labels[pred]\n                if label.startswith(\"B-\"):\n                    field = label[2:]\n                    value = parse_financial_numbers(words[idx])\n                    if not np.isnan(value):\n                        extracted[field] = value\n\n    return extracted",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Image and Visual Data in Finance</span>"
    ]
  },
  {
    "objectID": "71_image.html#chart-and-figure-digitization",
    "href": "71_image.html#chart-and-figure-digitization",
    "title": "45  Image and Visual Data in Finance",
    "section": "45.4 Chart and Figure Digitization",
    "text": "45.4 Chart and Figure Digitization\n\n45.4.1 Motivation: Unlocking Visual Financial Data\nFinancial charts (e.g., price time series, bar charts of earnings, scatter plots of risk-return tradeoffs) embed information that analysts process visually. For systematic strategies, this information must be converted to numerical form. Three use cases motivate chart digitization:\n\nHistorical data recovery. Pre-digital financial data often exists only in printed charts. Digitizing these charts extends historical time series beyond the electronic era.\nBroker report extraction. Sell-side research reports contain charts with projections and scenario analyses. Extracting these programmatically enables systematic aggregation of analyst views.\nRegulatory filings. Vietnamese regulatory filings sometimes embed data as images (charts, scanned tables) rather than as machine-readable values.\n\n\n\n45.4.2 Chart Type Classification\nThe first step is classifying the chart type (line, bar, scatter, pie, candlestick), which determines the appropriate digitization algorithm.\n\ndef build_chart_classifier(n_classes=5):\n    \"\"\"\n    Build a CNN-based chart type classifier.\n\n    Classes: line_chart, bar_chart, scatter_plot,\n             candlestick, pie_chart.\n    \"\"\"\n    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n\n    # Replace final layer for chart classification\n    model.fc = nn.Sequential(\n        nn.Dropout(0.3),\n        nn.Linear(512, n_classes)\n    )\n\n    return model\n\n\ndef classify_chart(image_path, model, transform):\n    \"\"\"Classify a chart image into one of 5 types.\"\"\"\n    class_names = [\n        \"line_chart\", \"bar_chart\", \"scatter_plot\",\n        \"candlestick\", \"pie_chart\"\n    ]\n\n    img = Image.open(image_path).convert(\"RGB\")\n    tensor = transform(img).unsqueeze(0)\n\n    with torch.no_grad():\n        logits = model(tensor)\n        probs = torch.softmax(logits, dim=1).squeeze()\n\n    pred_idx = probs.argmax().item()\n    return {\n        \"predicted_class\": class_names[pred_idx],\n        \"confidence\": probs[pred_idx].item(),\n        \"all_probs\": {\n            name: probs[i].item()\n            for i, name in enumerate(class_names)\n        }\n    }\n\n\n\n45.4.3 Line Chart Digitization\nFor line charts, the digitization task is to recover the \\((x, y)\\) data series from the image. The pipeline involves axis detection, scale calibration, and curve tracing.\n\ndef digitize_line_chart(image_path, x_range=None, y_range=None):\n    \"\"\"\n    Digitize a line chart image to recover the data series.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to chart image.\n    x_range : tuple, optional\n        (x_min, x_max) if known.\n    y_range : tuple, optional\n        (y_min, y_max) if known.\n\n    Returns\n    -------\n    DataFrame : Digitized data points (x, y).\n    \"\"\"\n    import cv2\n\n    img = cv2.imread(str(image_path))\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    h, w = gray.shape\n\n    # Step 1: Detect plot area (largest rectangular region)\n    edges = cv2.Canny(gray, 50, 150)\n    contours, _ = cv2.findContours(\n        edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n\n    if contours:\n        largest = max(contours, key=cv2.contourArea)\n        x_start, y_start, plot_w, plot_h = cv2.boundingRect(largest)\n    else:\n        # Fallback: assume plot is central 80% of image\n        x_start, y_start = int(w * 0.1), int(h * 0.1)\n        plot_w, plot_h = int(w * 0.8), int(h * 0.8)\n\n    # Step 2: Extract line pixels within plot area\n    # Convert to HSV and isolate colored lines\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    plot_region = hsv[y_start:y_start + plot_h,\n                      x_start:x_start + plot_w]\n\n    # Detect non-white, non-gray pixels (likely the line)\n    saturation = plot_region[:, :, 1]\n    line_mask = saturation &gt; 30  # Colored pixels\n\n    # Step 3: Trace the line (column-wise median of colored pixels)\n    data_points = []\n    for col in range(plot_w):\n        col_pixels = np.where(line_mask[:, col])[0]\n        if len(col_pixels) &gt; 0:\n            # Use median y-position\n            y_pixel = np.median(col_pixels)\n\n            # Convert pixel to data coordinates\n            x_frac = col / plot_w\n            y_frac = 1 - y_pixel / plot_h  # Invert y-axis\n\n            x_val = (x_range[0] + x_frac * (x_range[1] - x_range[0])\n                     if x_range else x_frac)\n            y_val = (y_range[0] + y_frac * (y_range[1] - y_range[0])\n                     if y_range else y_frac)\n\n            data_points.append({\"x\": x_val, \"y\": y_val})\n\n    return pd.DataFrame(data_points)",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Image and Visual Data in Finance</span>"
    ]
  },
  {
    "objectID": "71_image.html#visual-sentiment-analysis",
    "href": "71_image.html#visual-sentiment-analysis",
    "title": "45  Image and Visual Data in Finance",
    "section": "45.5 Visual Sentiment Analysis",
    "text": "45.5 Visual Sentiment Analysis\n\n45.5.1 Image Sentiment in Financial News\nNews articles are accompanied by images that carry sentiment independent of the text. A photograph of a CEO smiling at a press conference conveys different information than the same CEO facing protesters. Obaid and Pukthuanthong (2022) demonstrate that the visual sentiment of Wall Street Journal photographs predicts market returns: days with more negative imagery precede lower returns.\nWe implement visual sentiment analysis using two approaches: a pre-trained sentiment classifier and a vision-language model that interprets images in financial context.\n\n\n45.5.2 CNN-Based Visual Sentiment\n\ndef compute_visual_sentiment(image_paths, model_name=\"resnet50\"):\n    \"\"\"\n    Compute visual sentiment scores using a fine-tuned CNN.\n\n    Uses features from a pre-trained CNN followed by a sentiment\n    classifier trained on the Visual Sentiment Ontology (VSO)\n    or similar dataset.\n\n    Parameters\n    ----------\n    image_paths : list\n        Paths to news images.\n\n    Returns\n    -------\n    DataFrame : image_path, positive_score, negative_score, sentiment.\n    \"\"\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, transform, dim = build_feature_extractor(model_name, device)\n\n    # Extract features\n    features = extract_features(image_paths, model, transform, device)\n\n    # Simple sentiment model: use mean activation as proxy\n    # (In practice, fine-tune on labeled financial images)\n    # Higher mean activation in certain feature channels\n    # correlates with positive/negative affect\n\n    # Positive channels (empirically determined via validation)\n    pos_channels = list(range(0, dim // 3))\n    neg_channels = list(range(dim // 3, 2 * dim // 3))\n\n    pos_scores = features[:, pos_channels].mean(axis=1)\n    neg_scores = features[:, neg_channels].mean(axis=1)\n\n    # Normalize to [0, 1]\n    pos_norm = (pos_scores - pos_scores.min()) / (\n        pos_scores.max() - pos_scores.min() + 1e-8\n    )\n    neg_norm = (neg_scores - neg_scores.min()) / (\n        neg_scores.max() - neg_scores.min() + 1e-8\n    )\n\n    sentiment = pos_norm - neg_norm\n\n    return pd.DataFrame({\n        \"image_path\": image_paths,\n        \"positive_score\": pos_norm,\n        \"negative_score\": neg_norm,\n        \"net_sentiment\": sentiment\n    })\n\n\n\n45.5.3 Vision-Language Models for Financial Image Understanding\nThe most powerful approach to financial image analysis uses vision-language models (VLMs), which jointly process images and text. Models such as CLIP (Radford et al. 2021), BLIP-2 (Li et al. 2023), and GPT-4V can be prompted to interpret financial images in context. For instance, given an aerial photograph of a factory, a VLM can answer “Is this factory operating at full capacity?” or “Is there visible construction of additional facilities?”\n\ndef vlm_financial_analysis(image_path, prompt, model_name=\"clip\"):\n    \"\"\"\n    Use a vision-language model to analyze a financial image.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to image.\n    prompt : str\n        Financial analysis prompt.\n    model_name : str\n        'clip' for zero-shot classification,\n        'blip2' for visual question answering.\n\n    Returns\n    -------\n    dict : Model output (scores or text).\n    \"\"\"\n    img = Image.open(image_path).convert(\"RGB\")\n\n    if model_name == \"clip\":\n        from transformers import CLIPProcessor, CLIPModel\n\n        clip_model = CLIPModel.from_pretrained(\n            \"openai/clip-vit-base-patch32\"\n        )\n        processor = CLIPProcessor.from_pretrained(\n            \"openai/clip-vit-base-patch32\"\n        )\n\n        # Zero-shot classification with financial labels\n        labels = [\n            \"busy commercial area with many customers\",\n            \"empty commercial area with few customers\",\n            \"active construction site with workers\",\n            \"idle construction site without activity\",\n            \"healthy green crops in agricultural field\",\n            \"damaged or dry crops in agricultural field\",\n            \"busy port with many ships and containers\",\n            \"quiet port with few ships\"\n        ]\n\n        inputs = processor(\n            text=labels,\n            images=img,\n            return_tensors=\"pt\",\n            padding=True\n        )\n\n        with torch.no_grad():\n            outputs = clip_model(**inputs)\n            logits = outputs.logits_per_image.squeeze()\n            probs = torch.softmax(logits, dim=0)\n\n        results = {\n            label: prob.item()\n            for label, prob in zip(labels, probs)\n        }\n\n        return {\"scores\": results, \"top_label\": max(results, key=results.get)}\n\n    elif model_name == \"blip2\":\n        from transformers import Blip2Processor, Blip2ForConditionalGeneration\n\n        processor = Blip2Processor.from_pretrained(\n            \"Salesforce/blip2-opt-2.7b\"\n        )\n        model = Blip2ForConditionalGeneration.from_pretrained(\n            \"Salesforce/blip2-opt-2.7b\",\n            torch_dtype=torch.float16\n        )\n\n        inputs = processor(images=img, text=prompt, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            generated_ids = model.generate(**inputs, max_length=100)\n            answer = processor.decode(\n                generated_ids[0], skip_special_tokens=True\n            )\n\n        return {\"answer\": answer}\n\n\n# Construct daily visual sentiment index from news images\n# Source: Vietnamese financial news sites (VnExpress, CafeF, etc.)\nnews_images = dc.get_news_images(\n    start_date=\"2018-01-01\",\n    end_date=\"2024-12-31\",\n    source=[\"vnexpress_finance\", \"cafef\"]\n)\n\n# Aggregate daily visual sentiment\ndaily_sentiment = (\n    news_images.groupby(\"date\")\n    .agg(\n        visual_sentiment=(\"net_sentiment\", \"mean\"),\n        n_images=(\"net_sentiment\", \"count\"),\n        pct_negative=(\"net_sentiment\", lambda x: (x &lt; 0).mean())\n    )\n    .reset_index()\n)\n\n# Merge with market returns\nmarket_returns = dc.get_market_returns(\n    start_date=\"2018-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"daily\"\n)\n\nsentiment_returns = daily_sentiment.merge(\n    market_returns[[\"date\", \"mkt_ret\"]],\n    on=\"date\",\n    how=\"inner\"\n)\n\n# Lead-lag analysis: does visual sentiment predict next-day returns?\nsentiment_returns = sentiment_returns.sort_values(\"date\")\nsentiment_returns[\"mkt_ret_lead1\"] = sentiment_returns[\"mkt_ret\"].shift(-1)\n\n\n\n\nTable 45.3: Visual Sentiment and Market Return Predictability\n\n\n# Regression: next-day return on today's visual sentiment\nsr_clean = sentiment_returns.dropna(\n    subset=[\"mkt_ret_lead1\", \"visual_sentiment\", \"mkt_ret\"]\n)\n\nmodel_sent = sm.OLS(\n    sr_clean[\"mkt_ret_lead1\"],\n    sm.add_constant(sr_clean[[\"visual_sentiment\", \"mkt_ret\"]])\n).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 5})\n\nsent_results = pd.DataFrame({\n    \"Coefficient\": model_sent.params.round(6),\n    \"Std Error\": model_sent.bse.round(6),\n    \"t-stat\": model_sent.tvalues.round(3),\n    \"p-value\": model_sent.pvalues.round(4)\n})\nsent_results",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Image and Visual Data in Finance</span>"
    ]
  },
  {
    "objectID": "71_image.html#multimodal-fusion-combining-image-and-text",
    "href": "71_image.html#multimodal-fusion-combining-image-and-text",
    "title": "45  Image and Visual Data in Finance",
    "section": "45.6 Multimodal Fusion: Combining Image and Text",
    "text": "45.6 Multimodal Fusion: Combining Image and Text\n\n45.6.1 Why Multimodal?\nText and images capture different dimensions of the same underlying economic reality. An earnings report describes financial performance in words and numbers; the accompanying photographs show factories, products, and management. A news article about a port describes trade volumes in text; the satellite image shows actual ship positions. Combining both modalities yields a richer representation than either alone.\nThe fusion architecture depends on the application:\nEarly fusion. Concatenate image features \\(\\mathbf{z}^{\\text{img}}\\) and text features \\(\\mathbf{z}^{\\text{txt}}\\) into a single vector \\([\\mathbf{z}^{\\text{img}}; \\mathbf{z}^{\\text{txt}}]\\) before prediction. Simple but ignores cross-modal interactions.\nLate fusion. Train separate models on each modality and combine predictions: \\(\\hat{y} = \\alpha \\hat{y}^{\\text{img}} + (1-\\alpha) \\hat{y}^{\\text{txt}}\\). Robust but cannot learn cross-modal features.\nCross-attention fusion. Use transformer cross-attention to let each modality attend to the other. Most powerful but requires more data and computation.\n\\[\n\\mathbf{z}^{\\text{fused}} = \\text{CrossAttention}(\\mathbf{z}^{\\text{img}}, \\mathbf{z}^{\\text{txt}}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}^{\\text{img}} (\\mathbf{K}^{\\text{txt}})^\\top}{\\sqrt{d}}\\right) \\mathbf{V}^{\\text{txt}}\n\\tag{45.3}\\]\n\nclass MultimodalFusionModel(nn.Module):\n    \"\"\"\n    Multimodal fusion model combining image and text features\n    for financial prediction.\n\n    Supports early fusion, late fusion, and cross-attention.\n    \"\"\"\n\n    def __init__(self, img_dim=2048, txt_dim=768, hidden_dim=256,\n                 fusion=\"early\", n_heads=4):\n        super().__init__()\n        self.fusion = fusion\n\n        # Image projection\n        self.img_proj = nn.Sequential(\n            nn.Linear(img_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n\n        # Text projection\n        self.txt_proj = nn.Sequential(\n            nn.Linear(txt_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n\n        if fusion == \"early\":\n            self.head = nn.Sequential(\n                nn.Linear(hidden_dim * 2, hidden_dim),\n                nn.ReLU(),\n                nn.Dropout(0.2),\n                nn.Linear(hidden_dim, 1)\n            )\n        elif fusion == \"late\":\n            self.img_head = nn.Linear(hidden_dim, 1)\n            self.txt_head = nn.Linear(hidden_dim, 1)\n            self.alpha = nn.Parameter(torch.tensor(0.5))\n        elif fusion == \"cross_attention\":\n            self.cross_attn = nn.MultiheadAttention(\n                embed_dim=hidden_dim,\n                num_heads=n_heads,\n                batch_first=True\n            )\n            self.head = nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim // 2),\n                nn.ReLU(),\n                nn.Linear(hidden_dim // 2, 1)\n            )\n\n    def forward(self, img_features, txt_features):\n        img_h = self.img_proj(img_features)\n        txt_h = self.txt_proj(txt_features)\n\n        if self.fusion == \"early\":\n            combined = torch.cat([img_h, txt_h], dim=-1)\n            return self.head(combined).squeeze(-1)\n\n        elif self.fusion == \"late\":\n            img_pred = self.img_head(img_h).squeeze(-1)\n            txt_pred = self.txt_head(txt_h).squeeze(-1)\n            alpha = torch.sigmoid(self.alpha)\n            return alpha * img_pred + (1 - alpha) * txt_pred\n\n        elif self.fusion == \"cross_attention\":\n            # Image attends to text\n            img_h_unsq = img_h.unsqueeze(1)  # (B, 1, D)\n            txt_h_unsq = txt_h.unsqueeze(1)\n\n            attn_out, _ = self.cross_attn(\n                img_h_unsq, txt_h_unsq, txt_h_unsq\n            )\n            return self.head(attn_out.squeeze(1)).squeeze(-1)\n\n\ndef run_multimodal_experiment(image_features, text_features, returns,\n                               fusion_types=[\"early\", \"late\",\n                                             \"cross_attention\"]):\n    \"\"\"\n    Compare multimodal fusion strategies for return prediction.\n\n    Parameters\n    ----------\n    image_features : np.ndarray\n        Image feature matrix (N x img_dim).\n    text_features : np.ndarray\n        Text feature matrix (N x txt_dim).\n    returns : np.ndarray\n        Target returns (N,).\n    fusion_types : list\n        Fusion strategies to compare.\n\n    Returns\n    -------\n    DataFrame : R², MSE, Sharpe for each strategy.\n    \"\"\"\n    from sklearn.model_selection import TimeSeriesSplit\n\n    n = len(returns)\n    tscv = TimeSeriesSplit(n_splits=5)\n\n    results = []\n\n    for fusion in fusion_types:\n        fold_r2s = []\n\n        for train_idx, test_idx in tscv.split(returns):\n            # Convert to tensors\n            X_img_train = torch.tensor(\n                image_features[train_idx], dtype=torch.float32\n            )\n            X_txt_train = torch.tensor(\n                text_features[train_idx], dtype=torch.float32\n            )\n            y_train = torch.tensor(\n                returns[train_idx], dtype=torch.float32\n            )\n\n            X_img_test = torch.tensor(\n                image_features[test_idx], dtype=torch.float32\n            )\n            X_txt_test = torch.tensor(\n                text_features[test_idx], dtype=torch.float32\n            )\n            y_test = returns[test_idx]\n\n            # Build and train model\n            model = MultimodalFusionModel(\n                img_dim=image_features.shape[1],\n                txt_dim=text_features.shape[1],\n                fusion=fusion\n            )\n\n            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n            loss_fn = nn.MSELoss()\n\n            model.train()\n            for epoch in range(50):\n                optimizer.zero_grad()\n                pred = model(X_img_train, X_txt_train)\n                loss = loss_fn(pred, y_train)\n                loss.backward()\n                optimizer.step()\n\n            # Evaluate\n            model.eval()\n            with torch.no_grad():\n                y_pred = model(X_img_test, X_txt_test).numpy()\n\n            ss_res = np.sum((y_test - y_pred) ** 2)\n            ss_tot = np.sum((y_test - y_test.mean()) ** 2)\n            r2 = 1 - ss_res / ss_tot if ss_tot &gt; 0 else 0\n\n            fold_r2s.append(r2)\n\n        results.append({\n            \"fusion\": fusion,\n            \"r2_mean\": np.mean(fold_r2s),\n            \"r2_std\": np.std(fold_r2s)\n        })\n\n    # Add unimodal baselines\n    for modality, features in [(\"image_only\", image_features),\n                                (\"text_only\", text_features)]:\n        from sklearn.linear_model import RidgeCV\n        fold_r2s = []\n        for train_idx, test_idx in tscv.split(returns):\n            ridge = RidgeCV(alphas=np.logspace(-3, 3, 10))\n            ridge.fit(features[train_idx], returns[train_idx])\n            y_pred = ridge.predict(features[test_idx])\n            y_test = returns[test_idx]\n            ss_res = np.sum((y_test - y_pred) ** 2)\n            ss_tot = np.sum((y_test - y_test.mean()) ** 2)\n            fold_r2s.append(1 - ss_res / ss_tot if ss_tot &gt; 0 else 0)\n\n        results.append({\n            \"fusion\": modality,\n            \"r2_mean\": np.mean(fold_r2s),\n            \"r2_std\": np.std(fold_r2s)\n        })\n\n    return pd.DataFrame(results)\n\n\n\n45.6.2 Practical Considerations for Vietnamese Markets\nMultimodal analysis in Vietnamese markets faces several practical considerations:\nData alignment. Satellite images, news articles, and market data operate on different temporal frequencies and spatial resolutions. Satellite composites are available weekly or biweekly; news is daily; trading is intraday. Proper alignment requires specifying the information set available to an investor at the time of the trading decision to avoid look-ahead bias.\nLabel scarcity. Supervised learning requires labeled data (e.g., images annotated with economic outcomes). In Vietnam, ground-truth labels (actual retail sales, actual crop yields, actual port throughput) arrive with significant lags and often lack the granularity to match satellite resolution. Semi-supervised and self-supervised approaches are therefore essential.\nRegulatory considerations. High-resolution satellite imagery of specific commercial or military installations may be restricted. Researchers should verify that their imagery sources comply with Vietnamese regulations on geospatial data.\nComputational cost. Processing satellite tiles through CNNs is computationally intensive. A single Sentinel-2 tile at 10m resolution covering Ho Chi Minh City contains approximately \\(10{,}980 \\times 10{,}980\\) pixels per band. Tiling into \\(224 \\times 224\\) patches for CNN input generates \\(\\sim 2{,}400\\) patches per tile, each requiring a forward pass through the network.\n\n\n\nTable 45.4: Image Data Sources for Vietnamese Financial Applications\n\n\n\n\n\n\n\n\n\n\n\n\nApplication\nImage Source\nResolution\nFrequency\nVietnamese Availability\n\n\n\n\nNighttime luminosity\nVIIRS/DMSP\n500m\nMonthly\nFree (NOAA/EOG)\n\n\nCrop health\nMODIS/Sentinel-2\n250m/10m\n16-day/5-day\nFree (NASA/ESA)\n\n\nPort/ship detection\nSentinel-1 (SAR)\n10m\n12-day\nFree (ESA Copernicus)\n\n\nConstruction monitoring\nCommercial (Maxar)\n30cm\nOn demand\nPaid ($)\n\n\nUrban density\nSentinel-2\n10m\n5-day\nFree (ESA)\n\n\nDocument OCR\nCorporate filings\nN/A\nEvent-driven\nDataCore.vn\n\n\nNews images\nFinancial media\nN/A\nDaily\nWeb scraping",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Image and Visual Data in Finance</span>"
    ]
  },
  {
    "objectID": "71_image.html#summary",
    "href": "71_image.html#summary",
    "title": "45  Image and Visual Data in Finance",
    "section": "45.7 Summary",
    "text": "45.7 Summary\nThis chapter extended the alternative data toolkit from text (the previous chapter) to images. We demonstrated five distinct application domains for visual data in Vietnamese financial markets.\nFirst, satellite and geospatial imagery provides high-frequency, spatially granular economic signals that lead official statistics. Nighttime luminosity serves as a provincial GDP proxy with cross-sectional \\(R^2\\) exceeding 0.7; NDVI crop health indices predict agricultural firm returns; and CNN features extracted from satellite tiles enable rich spatial representations of economic activity.\nSecond, document image analysis solves the practical problem of extracting structured data from Vietnamese financial filings that arrive as scanned images. The pipeline (e.g., OCR with Vietnamese-optimized engines, layout analysis, table extraction, and LayoutLM-based document understanding) converts unstructured pixels into the structured financial data that all downstream analyses require.\nThird, chart digitization recovers numerical data series from visual representations, extending historical coverage and enabling systematic consumption of analyst outputs. Fourth, visual sentiment analysis from news imagery provides a signal dimension orthogonal to textual sentiment, with potential predictive power for market returns.\nFifth, multimodal fusion (combining image and text representations via early, late, or cross-attention architectures) yields richer predictive models than either modality alone. The practical benefit of multimodal approaches scales with the diversity and quality of available data, making it increasingly relevant as Vietnamese alternative data ecosystems mature.\nThe common thread across all applications is the transformation pipeline: raw pixel tensor \\(\\to\\) feature representation (via CNN, ViT, or VLM) \\(\\to\\) financial signal \\(\\to\\) economic interpretation. The choice of architecture and the quality of the domain adaptation determine whether the resulting signal has genuine predictive content or merely captures noise.\n\n\n\n\n\n\nBaek, Jeonghun, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwalsuk Lee. 2019. “What Is Wrong with Scene Text Recognition Model Comparisons? Dataset and Model Analysis.” In Proceedings of the IEEE/CVF International Conference on Computer Vision, 4715–23.\n\n\nDonaldson, Dave. 2018. “Railroads of the Raj: Estimating the Impact of Transportation Infrastructure.” American Economic Review 108 (4-5): 899–934.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” arXiv Preprint arXiv:2010.11929.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–78.\n\n\nHenderson, J Vernon, Adam Storeygard, and David N Weil. 2012. “Measuring Economic Growth from Outer Space.” American Economic Review 102 (2): 994–1028.\n\n\nHuang, Yupan, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022. “Layoutlmv3: Pre-Training for Document Ai with Unified Text and Image Masking.” In Proceedings of the 30th ACM International Conference on Multimedia, 4083–91.\n\n\nJean, Neal, Marshall Burke, Michael Xie, W Matthew Alampay Davis, David B Lobell, and Stefano Ermon. 2016. “Combining Satellite Imagery and Machine Learning to Predict Poverty.” Science 353 (6301): 790–94.\n\n\nLi, Junnan, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. “Blip-2: Bootstrapping Language-Image Pre-Training with Frozen Image Encoders and Large Language Models.” In International Conference on Machine Learning, 19730–42. PMLR.\n\n\nObaid, Khaled, and Kuntara Pukthuanthong. 2022. “A Picture Is Worth a Thousand Words: Measuring Investor Sentiment by Combining Machine Learning and Photos from News.” Journal of Financial Economics 144 (1): 273–97.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In International Conference on Machine Learning, 8748–63. PmLR.\n\n\nTan, Mingxing, and Quoc Le. 2019. “Efficientnet: Rethinking Model Scaling for Convolutional Neural Networks.” In International Conference on Machine Learning, 6105–14. PMLR.",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Image and Visual Data in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html",
    "href": "73_networks_graphs.html",
    "title": "46  Networks and Graphs in Finance",
    "section": "",
    "text": "46.1 Graph Theory Foundations for Finance\nFinancial markets are networks. Every stock return is shaped by connections: firms share supply chains, directors, auditors, investors, lenders, and regulators. Shocks propagate through these links (e.g., a bankruptcy ripples through supplier networks, a central bank liquidity squeeze radiates through interbank exposures, and information diffuses through board interlocks and institutional co-ownership). Yet the dominant empirical paradigm in finance treats firms as isolated observations indexed by \\((i, t)\\), connected only through their shared exposure to common factors. This chapter introduces tools for modeling, measuring, and exploiting the network structure that standard panel regressions ignore.\nThe Vietnamese financial system is particularly network-dense. State ownership creates a lattice of cross-connected enterprises: the same line ministry may oversee the borrower, the lender, and the insurer. Pyramidal business groups (such as Vingroup, Masan, and FPT) link dozens of listed and unlisted entities through chains of equity ownership. The banking system is small and concentrated, with a few state-owned commercial banks accounting for the majority of assets, generating dense interbank exposures. Board interlocks (e.g., directors who serve on multiple boards simultaneously) are pervasive and often follow ownership lines. And the equity market itself exhibits return co-movement patterns that, when represented as a correlation network, reveal sector-level and ownership-level clustering invisible in standard factor analysis.\nThis chapter covers the full spectrum of network methods used in financial economics, from classical graph theory through modern graph neural networks (GNNs). We organize the material in seven sections. First, graph theory fundamentals and the representation of financial data as networks. Second, ownership and control networks, which are the most distinctive network structure in Vietnamese markets. Third, board interlock and governance networks. Fourth, supply chain and trade networks. Fifth, correlation and co-movement networks for portfolio construction and systemic risk monitoring. Sixth, interbank and contagion networks. Seventh, graph neural networks and graph-based machine learning for asset pricing, credit risk, and anomaly detection.",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html#graph-theory-foundations-for-finance",
    "href": "73_networks_graphs.html#graph-theory-foundations-for-finance",
    "title": "46  Networks and Graphs in Finance",
    "section": "",
    "text": "46.1.1 Representing Financial Data as Graphs\nA graph \\(G = (V, E)\\) consists of a set of vertices (nodes) \\(V\\) and edges (links) \\(E \\subseteq V \\times V\\). In financial networks, nodes represent economic agents, such as firms, banks, investors, directors, and edges represent relationships including ownership stakes, lending, board seats, supply contracts, and return co-movement.\nFinancial graphs come in several varieties:\nDirected vs. undirected. Ownership is directed: firm \\(A\\) owns a stake in firm \\(B\\), but not necessarily vice versa. Board interlocks are undirected: if director \\(d\\) sits on both firm \\(A\\)’s and firm \\(B\\)’s boards, the connection is symmetric. Supply chains are directed: \\(A\\) supplies to \\(B\\).\nWeighted vs. unweighted. Ownership networks are naturally weighted by the ownership percentage. Correlation networks are weighted by the pairwise correlation coefficient. Board interlocks can be weighted by the number of shared directors.\nStatic vs. dynamic. Most financial networks evolve over time as ownership changes, directors rotate, and correlations shift. A temporal graph \\(G_t = (V_t, E_t)\\) captures this evolution.\nBipartite vs. unipartite. The raw data for many financial networks is bipartite: directors \\(\\times\\) firms, investors \\(\\times\\) stocks, banks \\(\\times\\) borrowers. The one-mode projection converts this to a unipartite graph: two firms are connected if they share a director, two stocks are connected if they share an institutional investor, etc.\n\n\n46.1.2 Key Graph Metrics\nFor a graph \\(G\\) with \\(n = |V|\\) nodes and \\(m = |E|\\) edges, we define the following metrics, each with a specific financial interpretation.\nDegree centrality. The degree \\(k_i\\) of node \\(i\\) is the number of edges incident to \\(i\\). In a directed graph, we distinguish in-degree \\(k_i^{\\text{in}}\\) (edges pointing to \\(i\\)) and out-degree \\(k_i^{\\text{out}}\\) (edges from \\(i\\)). Normalized degree centrality is:\n\\[\nC_D(i) = \\frac{k_i}{n - 1}\n\\tag{46.1}\\]\nIn an ownership network, high out-degree means a firm owns stakes in many others (conglomerate); high in-degree means many entities own stakes in the firm (dispersed ownership).\nBetweenness centrality. The fraction of shortest paths between all pairs of nodes that pass through node \\(i\\):\n\\[\nC_B(i) = \\sum_{s \\neq i \\neq t} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}\n\\tag{46.2}\\]\nwhere \\(\\sigma_{st}\\) is the total number of shortest paths from \\(s\\) to \\(t\\) and \\(\\sigma_{st}(i)\\) is the number that pass through \\(i\\). High betweenness identifies “bridge” nodes (i.e., firms or banks that connect otherwise disconnected parts of the financial system). The failure of a high-betweenness bank can fragment the interbank network.\nEigenvector centrality. A node is central if it is connected to other central nodes. The eigenvector centrality is the solution to:\n\\[\n\\lambda \\mathbf{c} = A \\mathbf{c}\n\\tag{46.3}\\]\nwhere \\(A\\) is the adjacency matrix and \\(\\lambda\\) is the largest eigenvalue. Google’s PageRank is a regularized variant designed for directed graphs. In financial networks, eigenvector centrality identifies systemically important institutions (i.e., those connected to other important institutions).\nClustering coefficient. The fraction of a node’s neighbors that are also neighbors of each other:\n\\[\nC_C(i) = \\frac{2 T_i}{k_i(k_i - 1)}\n\\tag{46.4}\\]\nwhere \\(T_i\\) is the number of triangles containing node \\(i\\). High clustering indicates tightly knit groups (e.g., business groups, lending circles, co-invested portfolios).\nCommunity structure. Many financial networks exhibit modular structure: groups of densely connected nodes with sparse connections between groups. Community detection algorithms (Louvain, label propagation, spectral clustering) identify these modules, which in financial networks often correspond to business groups, industry sectors, or lending clusters.\n\ndef compute_network_metrics(G):\n    \"\"\"\n    Compute standard network metrics for a graph.\n\n    Parameters\n    ----------\n    G : nx.Graph or nx.DiGraph\n        Input graph.\n\n    Returns\n    -------\n    dict : Node-level and graph-level metrics.\n    \"\"\"\n    is_directed = G.is_directed()\n\n    # Node-level metrics\n    if is_directed:\n        in_degree = dict(G.in_degree())\n        out_degree = dict(G.out_degree())\n        degree = {n: in_degree[n] + out_degree[n] for n in G.nodes()}\n    else:\n        degree = dict(G.degree())\n\n    betweenness = nx.betweenness_centrality(G, weight=\"weight\")\n    eigenvector = nx.eigenvector_centrality_numpy(\n        G, weight=\"weight\"\n    ) if len(G) &gt; 0 else {}\n    clustering = nx.clustering(G, weight=\"weight\") if not is_directed else {}\n\n    # PageRank (works for both directed and undirected)\n    pagerank = nx.pagerank(G, weight=\"weight\")\n\n    # Graph-level metrics\n    n_nodes = G.number_of_nodes()\n    n_edges = G.number_of_edges()\n    density = nx.density(G)\n\n    # Connected components\n    if is_directed:\n        n_weak_components = nx.number_weakly_connected_components(G)\n        n_strong_components = nx.number_strongly_connected_components(G)\n    else:\n        n_components = nx.number_connected_components(G)\n\n    # Degree distribution statistics\n    degrees = list(degree.values())\n    avg_degree = np.mean(degrees) if degrees else 0\n    max_degree = max(degrees) if degrees else 0\n\n    # Assortativity\n    assortativity = nx.degree_assortativity_coefficient(G)\n\n    # Community detection (Louvain)\n    if not is_directed:\n        try:\n            communities = nx.community.louvain_communities(G)\n            modularity = nx.community.modularity(G, communities)\n            n_communities = len(communities)\n        except Exception:\n            modularity, n_communities = np.nan, 0\n    else:\n        modularity, n_communities = np.nan, 0\n\n    node_metrics = pd.DataFrame({\n        \"degree\": degree,\n        \"betweenness\": betweenness,\n        \"eigenvector\": eigenvector,\n        \"pagerank\": pagerank,\n        \"clustering\": clustering if clustering else {n: np.nan for n in G.nodes()}\n    })\n\n    graph_metrics = {\n        \"n_nodes\": n_nodes,\n        \"n_edges\": n_edges,\n        \"density\": density,\n        \"avg_degree\": avg_degree,\n        \"max_degree\": max_degree,\n        \"assortativity\": assortativity,\n        \"modularity\": modularity,\n        \"n_communities\": n_communities\n    }\n\n    return node_metrics, graph_metrics\n\n\n\n46.1.3 The Adjacency Matrix and Its Spectral Properties\nThe adjacency matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) encodes the graph structure: \\(A_{ij} = w_{ij}\\) if there is an edge from \\(i\\) to \\(j\\) with weight \\(w_{ij}\\), and \\(A_{ij} = 0\\) otherwise. For undirected graphs, \\(A\\) is symmetric.\nThe graph Laplacian \\(L = D - A\\) (where \\(D\\) is the diagonal degree matrix) has eigenvalues \\(0 = \\lambda_1 \\leq \\lambda_2 \\leq \\ldots \\leq \\lambda_n\\) with important structural interpretations:\n\nThe multiplicity of \\(\\lambda = 0\\) equals the number of connected components.\nThe second eigenvalue \\(\\lambda_2\\) (the algebraic connectivity or Fiedler value) measures how well-connected the graph is. Low \\(\\lambda_2\\) implies the graph has a bottleneck, which is a weak point where cutting a few edges would disconnect large portions.\nThe eigenvectors of \\(L\\) provide the spectral embedding of the graph, which is the foundation for spectral clustering and graph convolutional networks.\n\nThe normalized Laplacian \\(\\tilde{L} = D^{-1/2} L D^{-1/2}\\) is used in GCNs because it stabilizes message passing across nodes with different degrees.\n\ndef spectral_graph_analysis(A, n_components=10):\n    \"\"\"\n    Compute spectral properties of a financial network.\n\n    Parameters\n    ----------\n    A : np.ndarray or sparse matrix\n        Adjacency matrix.\n    n_components : int\n        Number of eigenvalues/vectors to compute.\n\n    Returns\n    -------\n    dict : Eigenvalues, algebraic connectivity, spectral gap.\n    \"\"\"\n    n = A.shape[0]\n\n    if sparse.issparse(A):\n        A_dense = A.toarray()\n    else:\n        A_dense = A\n\n    # Degree matrix\n    D = np.diag(A_dense.sum(axis=1))\n\n    # Graph Laplacian\n    L = D - A_dense\n\n    # Normalized Laplacian\n    D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D) + 1e-10))\n    L_norm = D_inv_sqrt @ L @ D_inv_sqrt\n\n    # Eigendecomposition (smallest eigenvalues)\n    eigenvalues, eigenvectors = np.linalg.eigh(L_norm)\n\n    # Sort by eigenvalue\n    idx = np.argsort(eigenvalues)\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n\n    # Algebraic connectivity (Fiedler value)\n    fiedler_value = eigenvalues[1] if n &gt; 1 else 0\n    fiedler_vector = eigenvectors[:, 1] if n &gt; 1 else np.zeros(n)\n\n    # Spectral gap\n    spectral_gap = eigenvalues[1] - eigenvalues[0] if n &gt; 1 else 0\n\n    return {\n        \"eigenvalues\": eigenvalues[:n_components],\n        \"fiedler_value\": fiedler_value,\n        \"fiedler_vector\": fiedler_vector,\n        \"spectral_gap\": spectral_gap,\n        \"spectral_embedding\": eigenvectors[:, 1:n_components + 1]\n    }",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html#ownership-and-control-networks",
    "href": "73_networks_graphs.html#ownership-and-control-networks",
    "title": "46  Networks and Graphs in Finance",
    "section": "46.2 Ownership and Control Networks",
    "text": "46.2 Ownership and Control Networks\n\n46.2.1 Vietnamese Ownership Structure\nOwnership networks are arguably the most economically consequential graph structure in Vietnamese markets. The Vietnamese corporate landscape is characterized by three distinctive features that generate complex ownership topologies:\nState ownership pyramids. The government holds equity in hundreds of firms through a hierarchy of holding entities: the State Capital Investment Corporation (SCIC), line ministries, provincial People’s Committees, and state-owned economic groups (tập đoàn kinh tế nhà nước). These chains create multi-layered pyramids where the state’s ultimate control rights may substantially exceed its cash flow rights.\nPrivate business groups. Vietnamese conglomerates (Vingroup, Masan, FPT, Hoà Phát, Thaco) create complex webs of cross-ownership, subsidiary relationships, and associate stakes. These structures serve multiple purposes: internal capital markets, tax optimization, regulatory arbitrage, and control enhancement.\nCircular and cross-ownership. Vietnamese regulations do not effectively prevent circular ownership (firm \\(A\\) owns firm \\(B\\) which owns firm \\(C\\) which owns firm \\(A\\)), creating loops in the ownership graph that amplify control beyond direct stakes and inflate accounting equity (Bebchuk 1999).\n\n# Load ownership data\nownership = dc.get_ownership_network(\n    date=\"2024-06-30\",\n    min_stake=1.0  # Minimum 1% ownership stake\n)\n\n# Load firm characteristics\nfirms = dc.get_firm_characteristics(\n    start_date=\"2024-01-01\",\n    end_date=\"2024-06-30\"\n).groupby(\"ticker\").last().reset_index()\n\nprint(f\"Ownership edges: {len(ownership)}\")\nprint(f\"Unique owners: {ownership['owner_id'].nunique()}\")\nprint(f\"Unique targets: {ownership['target_ticker'].nunique()}\")\n\n\n# Build directed ownership graph\nG_own = nx.DiGraph()\n\n# Add firm nodes with attributes\nfor _, row in firms.iterrows():\n    G_own.add_node(\n        row[\"ticker\"],\n        node_type=\"firm\",\n        market_cap=row.get(\"market_cap\", 0),\n        industry=row.get(\"industry\", \"Unknown\"),\n        is_soe=row.get(\"state_ownership_pct\", 0) &gt; 50\n    )\n\n# Add ownership edges\nfor _, row in ownership.iterrows():\n    owner = row[\"owner_id\"]\n    target = row[\"target_ticker\"]\n    stake = row[\"ownership_pct\"]\n\n    # Add owner node if not present\n    if owner not in G_own:\n        G_own.add_node(\n            owner,\n            node_type=row.get(\"owner_type\", \"entity\"),\n            market_cap=0,\n            industry=\"Holding\"\n        )\n\n    G_own.add_edge(owner, target, weight=stake / 100)\n\nnode_metrics_own, graph_metrics_own = compute_network_metrics(G_own)\n\nprint(f\"\\nOwnership Network Summary:\")\nfor k, v in graph_metrics_own.items():\n    print(f\"  {k}: {v}\")\n\n\n\n\nTable 46.1: Ownership Network: Graph-Level Statistics\n\n\nown_stats = pd.DataFrame([graph_metrics_own]).T\nown_stats.columns = [\"Value\"]\nown_stats = own_stats.round(4)\nown_stats\n\n\n\n\n\n46.2.2 Ultimate Ownership and Control Chains\nDirect ownership understates the actual control exercised through pyramidal chains. The ultimate ownership stake of entity \\(A\\) in firm \\(Z\\) through a chain \\(A \\to B \\to C \\to Z\\) is the product of intermediate stakes:\n\\[\n\\omega_{A \\to Z}^{\\text{ultimate}} = \\prod_{(i,j) \\in \\text{path}(A, Z)} w_{ij}\n\\tag{46.5}\\]\nwhere \\(w_{ij}\\) is the direct stake of \\(i\\) in \\(j\\). When multiple paths exist from \\(A\\) to \\(Z\\), the total ultimate ownership is the sum across paths. The control rights, however, are determined by the weakest link in the chain (the minimum stake along the path), reflecting the principle that control requires a majority at each level.\n\ndef compute_ultimate_ownership(G, source, target, max_depth=10):\n    \"\"\"\n    Compute ultimate ownership of source in target through all paths.\n\n    Parameters\n    ----------\n    G : nx.DiGraph\n        Ownership graph with edge weights as ownership fractions.\n    source : str\n        Ultimate owner node.\n    target : str\n        Target firm node.\n    max_depth : int\n        Maximum chain length to consider.\n\n    Returns\n    -------\n    dict : Total ownership, control rights, number of paths.\n    \"\"\"\n    if source not in G or target not in G:\n        return {\"ownership\": 0, \"control\": 0, \"n_paths\": 0}\n\n    total_ownership = 0\n    max_control = 0\n    n_paths = 0\n\n    # Find all simple paths from source to target\n    try:\n        for path in nx.all_simple_paths(G, source, target,\n                                         cutoff=max_depth):\n            # Cash flow rights: product of stakes\n            ownership = 1.0\n            min_stake = 1.0\n            for i in range(len(path) - 1):\n                edge_data = G[path[i]][path[i + 1]]\n                stake = edge_data.get(\"weight\", 0)\n                ownership *= stake\n                min_stake = min(min_stake, stake)\n\n            total_ownership += ownership\n            max_control = max(max_control, min_stake)\n            n_paths += 1\n    except nx.NetworkXNoPath:\n        pass\n\n    return {\n        \"ownership\": total_ownership,\n        \"control\": max_control,\n        \"n_paths\": n_paths\n    }\n\n\ndef compute_control_wedge(G, firms_list, max_depth=5):\n    \"\"\"\n    Compute the wedge between control and cash flow rights\n    for the largest shareholder of each firm.\n\n    The wedge = control rights - cash flow rights.\n    Positive wedge → potential for tunneling.\n    \"\"\"\n    wedge_data = []\n\n    for firm in firms_list:\n        if firm not in G:\n            continue\n\n        # Find all direct owners\n        predecessors = list(G.predecessors(firm))\n        if not predecessors:\n            continue\n\n        # Find largest direct owner\n        stakes = {p: G[p][firm][\"weight\"] for p in predecessors}\n        largest_owner = max(stakes, key=stakes.get)\n        direct_stake = stakes[largest_owner]\n\n        # Compute ultimate ownership through all paths\n        ultimate = compute_ultimate_ownership(\n            G, largest_owner, firm, max_depth\n        )\n\n        wedge_data.append({\n            \"ticker\": firm,\n            \"largest_owner\": largest_owner,\n            \"direct_stake\": direct_stake,\n            \"ultimate_ownership\": ultimate[\"ownership\"],\n            \"control_rights\": ultimate[\"control\"],\n            \"n_ownership_paths\": ultimate[\"n_paths\"],\n            \"wedge\": ultimate[\"control\"] - ultimate[\"ownership\"]\n        })\n\n    return pd.DataFrame(wedge_data)\n\n\n# Compute for all listed firms\nlisted_firms = [n for n, d in G_own.nodes(data=True)\n                if d.get(\"node_type\") == \"firm\"]\nwedge_df = compute_control_wedge(G_own, listed_firms)\n\n\n\n\nTable 46.2: Control-Cash Flow Wedge: Summary Statistics\n\n\nwedge_summary = wedge_df[\n    [\"direct_stake\", \"ultimate_ownership\", \"control_rights\",\n     \"n_ownership_paths\", \"wedge\"]\n].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).T.round(4)\nwedge_summary\n\n\n\n\n\n\n(\n    p9.ggplot(wedge_df, p9.aes(x=\"wedge\"))\n    + p9.geom_histogram(bins=50, fill=\"#2E5090\", alpha=0.7)\n    + p9.geom_vline(xintercept=0, linetype=\"dashed\", color=\"#C0392B\")\n    + p9.labs(\n        x=\"Control Wedge (Control Rights − Cash Flow Rights)\",\n        y=\"Count\",\n        title=\"Control-Ownership Wedge: Positive Values Indicate Tunneling Risk\"\n    )\n    + p9.theme_minimal()\n    + p9.theme(figure_size=(10, 5))\n)\n\n\nFigure 46.1\n\n\n\n\n\n46.2.3 Ownership Centrality and Firm Value\nWe test whether a firm’s position in the ownership network predicts its market valuation, controlling for standard determinants:\n\\[\nQ_{i,t} = \\beta_0 + \\beta_1 \\text{Centrality}_{i,t} + \\beta_2 \\text{Wedge}_{i,t} + \\boldsymbol{\\gamma}' \\mathbf{X}_{i,t} + \\alpha_{\\text{ind}} + \\delta_t + \\varepsilon_{i,t}\n\\tag{46.6}\\]\n\n# Merge network metrics with firm characteristics\nnode_metrics_own_df = node_metrics_own.reset_index().rename(\n    columns={\"index\": \"ticker\"}\n)\n\nvaluation_data = firms.merge(node_metrics_own_df, on=\"ticker\", how=\"inner\")\nvaluation_data = valuation_data.merge(\n    wedge_df[[\"ticker\", \"wedge\", \"n_ownership_paths\"]],\n    on=\"ticker\", how=\"left\"\n)\n\n# Panel regression\nval_clean = valuation_data.dropna(\n    subset=[\"tobins_q\", \"eigenvector\", \"wedge\",\n            \"log_size\", \"profitability\", \"leverage\"]\n)\n\nif len(val_clean) &gt; 50:\n    model_network_val = sm.OLS(\n        val_clean[\"tobins_q\"],\n        sm.add_constant(val_clean[[\n            \"eigenvector\", \"betweenness\", \"wedge\",\n            \"log_size\", \"profitability\", \"leverage\"\n        ]])\n    ).fit(cov_type=\"HC1\")\n\n    print(\"Ownership Network Position and Firm Value:\")\n    for var in [\"eigenvector\", \"betweenness\", \"wedge\"]:\n        print(f\"  {var}: {model_network_val.params[var]:.4f} \"\n              f\"(t = {model_network_val.tvalues[var]:.3f})\")\n\n\n\n46.2.4 Business Group Detection\nBusiness groups (i.e., collections of legally independent firms linked by common ownership) are a defining feature of Vietnamese corporate structure. We detect them algorithmically using community detection on the ownership graph.\n\ndef detect_business_groups(G, min_group_size=3, ownership_threshold=0.10):\n    \"\"\"\n    Detect business groups from ownership network.\n\n    A business group is a connected component in the undirected\n    projection of the ownership graph, filtered for economically\n    meaningful ownership stakes.\n\n    Parameters\n    ----------\n    G : nx.DiGraph\n        Directed ownership graph.\n    min_group_size : int\n        Minimum firms in a group.\n    ownership_threshold : float\n        Minimum ownership stake to count as a link.\n\n    Returns\n    -------\n    DataFrame : Group assignments with apex firm.\n    \"\"\"\n    # Filter edges by threshold\n    G_filtered = nx.DiGraph()\n    for u, v, d in G.edges(data=True):\n        if d.get(\"weight\", 0) &gt;= ownership_threshold:\n            G_filtered.add_edge(u, v, **d)\n\n    # Convert to undirected for component detection\n    G_undirected = G_filtered.to_undirected()\n\n    # Find connected components\n    components = list(nx.connected_components(G_undirected))\n\n    # Filter by size\n    groups = [c for c in components if len(c) &gt;= min_group_size]\n\n    # For each group, identify the apex (top of pyramid)\n    group_data = []\n    for group_id, members in enumerate(groups):\n        subgraph = G_filtered.subgraph(members)\n\n        # Apex: node with highest out-degree and lowest in-degree\n        # (controls others but is not controlled)\n        scores = {}\n        for node in members:\n            in_deg = subgraph.in_degree(node)\n            out_deg = subgraph.out_degree(node)\n            scores[node] = out_deg - in_deg\n\n        apex = max(scores, key=scores.get) if scores else list(members)[0]\n\n        for member in members:\n            node_data = G.nodes.get(member, {})\n            group_data.append({\n                \"ticker\": member,\n                \"group_id\": group_id,\n                \"group_size\": len(members),\n                \"apex\": apex,\n                \"is_apex\": member == apex,\n                \"node_type\": node_data.get(\"node_type\", \"unknown\"),\n                \"industry\": node_data.get(\"industry\", \"Unknown\"),\n                \"market_cap\": node_data.get(\"market_cap\", 0)\n            })\n\n    return pd.DataFrame(group_data)\n\n\ngroups_df = detect_business_groups(G_own)\nprint(f\"Business groups detected: {groups_df['group_id'].nunique()}\")\nprint(f\"Firms in groups: {len(groups_df[groups_df['node_type'] == 'firm'])}\")\n\n\n\n\nTable 46.3: Largest Vietnamese Business Groups by Ownership Network Analysis\n\n\ntop_groups = (\n    groups_df.groupby(\"group_id\")\n    .agg(\n        apex=(\"apex\", \"first\"),\n        n_members=(\"ticker\", \"count\"),\n        n_listed=(\"node_type\", lambda x: (x == \"firm\").sum()),\n        total_mcap=(\"market_cap\", \"sum\"),\n        industries=(\"industry\", lambda x: x.nunique())\n    )\n    .sort_values(\"total_mcap\", ascending=False)\n    .head(15)\n    .reset_index()\n)\ntop_groups[\"total_mcap_bn\"] = top_groups[\"total_mcap\"] / 1e9\ntop_groups[[\"apex\", \"n_members\", \"n_listed\", \"total_mcap_bn\", \"industries\"]]",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html#board-interlock-networks",
    "href": "73_networks_graphs.html#board-interlock-networks",
    "title": "46  Networks and Graphs in Finance",
    "section": "46.3 Board Interlock Networks",
    "text": "46.3 Board Interlock Networks\n\n46.3.1 Construction\nA board interlock exists when a director serves on the boards of two or more firms simultaneously. The board interlock network is a unipartite projection of the bipartite director\\(\\times\\)firm graph: two firms are connected if they share at least one director, weighted by the number of shared directors.\nBoard interlocks are an information channel. Bizjak, Lemmon, and Naveen (2008) show that compensation practices diffuse through board networks. Cai et al. (2014) demonstrate that firms connected through interlocks have correlated investment policies. In Vietnamese markets, interlocks often follow ownership lines (e.g., the parent company appoints directors to subsidiary boards), creating a governance channel that reinforces the ownership channel.\n\n# Load board membership data\nboard_data = dc.get_board_memberships(\n    date=\"2024-06-30\"\n)\n\nprint(f\"Director-firm pairs: {len(board_data)}\")\nprint(f\"Unique directors: {board_data['director_id'].nunique()}\")\nprint(f\"Unique firms: {board_data['ticker'].nunique()}\")\n\n# Build bipartite graph\nB = nx.Graph()\nfor _, row in board_data.iterrows():\n    B.add_node(row[\"director_id\"], bipartite=0)\n    B.add_node(row[\"ticker\"], bipartite=1)\n    B.add_edge(\n        row[\"director_id\"], row[\"ticker\"],\n        role=row.get(\"role\", \"director\"),\n        is_independent=row.get(\"is_independent\", False)\n    )\n\n# Project to firm-firm interlock network\nfirm_nodes = [n for n, d in B.nodes(data=True) if d.get(\"bipartite\") == 1]\nG_interlock = nx.bipartite.weighted_projected_graph(B, firm_nodes)\n\n# Edge weight = number of shared directors\ninterlock_metrics, interlock_graph = compute_network_metrics(G_interlock)\n\nprint(f\"\\nBoard Interlock Network:\")\nprint(f\"  Firms: {G_interlock.number_of_nodes()}\")\nprint(f\"  Interlocking pairs: {G_interlock.number_of_edges()}\")\nprint(f\"  Density: {nx.density(G_interlock):.4f}\")\n\n\n# Do board interlocks follow ownership lines?\n# Compare interlock edges with ownership edges\ninterlock_edges = set(G_interlock.edges())\nownership_edges_undirected = set()\n\nfor u, v in G_own.edges():\n    if u in firm_nodes and v in firm_nodes:\n        ownership_edges_undirected.add((min(u, v), max(u, v)))\n\ninterlock_edges_normalized = {\n    (min(u, v), max(u, v)) for u, v in interlock_edges\n}\n\noverlap = interlock_edges_normalized & ownership_edges_undirected\nn_interlock = len(interlock_edges_normalized)\nn_ownership = len(ownership_edges_undirected)\nn_overlap = len(overlap)\n\nprint(f\"Interlock edges: {n_interlock}\")\nprint(f\"Ownership edges (firm-firm): {n_ownership}\")\nprint(f\"Overlap: {n_overlap}\")\nif n_interlock &gt; 0:\n    print(f\"Fraction of interlocks with ownership link: \"\n          f\"{n_overlap / n_interlock:.3f}\")\n\n\n\n46.3.2 Interlocks and Return Co-Movement\nIf board interlocks serve as information channels, firms connected through interlocks should exhibit excess return co-movement beyond what is explained by shared industry or size characteristics. We test this using the methodology of Cohen and Frazzini (2008):\n\\[\n\\rho_{ij,t} = \\alpha + \\beta \\cdot \\text{Interlock}_{ij,t} + \\gamma \\cdot \\text{SameIndustry}_{ij} + \\delta \\cdot \\text{SizeProximity}_{ij,t} + \\varepsilon_{ij,t}\n\\tag{46.7}\\]\n\n# Compute pairwise return correlations for interlocked and non-interlocked pairs\nmonthly_returns = dc.get_monthly_returns(\n    start_date=\"2023-01-01\",\n    end_date=\"2024-06-30\"\n)\n\n# Pivot to wide format\nreturns_wide = monthly_returns.pivot(\n    index=\"date\", columns=\"ticker\", values=\"ret\"\n).dropna(axis=1, thresh=12)\n\n# Correlation matrix\ncorr_matrix = returns_wide.corr()\n\n# Compare correlations: interlocked vs non-interlocked\ninterlock_corrs = []\nnon_interlock_corrs = []\n\ntickers_in_both = set(corr_matrix.columns) & set(G_interlock.nodes())\n\nfor i, ticker_i in enumerate(tickers_in_both):\n    for ticker_j in list(tickers_in_both)[i + 1:]:\n        corr = corr_matrix.loc[ticker_i, ticker_j]\n        if np.isnan(corr):\n            continue\n\n        if G_interlock.has_edge(ticker_i, ticker_j):\n            interlock_corrs.append(corr)\n        else:\n            non_interlock_corrs.append(corr)\n\nif interlock_corrs and non_interlock_corrs:\n    t_stat, p_val = stats.ttest_ind(interlock_corrs, non_interlock_corrs)\n    print(f\"Interlocked pairs: n={len(interlock_corrs)}, \"\n          f\"mean corr={np.mean(interlock_corrs):.4f}\")\n    print(f\"Non-interlocked: n={len(non_interlock_corrs)}, \"\n          f\"mean corr={np.mean(non_interlock_corrs):.4f}\")\n    print(f\"Difference t-stat: {t_stat:.3f}, p-value: {p_val:.4f}\")",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html#supply-chain-and-trade-networks",
    "href": "73_networks_graphs.html#supply-chain-and-trade-networks",
    "title": "46  Networks and Graphs in Finance",
    "section": "46.4 Supply Chain and Trade Networks",
    "text": "46.4 Supply Chain and Trade Networks\n\n46.4.1 Constructing the Supply Chain Graph\nSupply chain relationships (customer-supplier linkages) represent real economic connections through which shocks propagate. Acemoglu, Ozdaglar, and Tahbaz-Salehi (2015) demonstrate theoretically that in the presence of supply chain linkages, idiosyncratic shocks to individual firms can generate aggregate fluctuations rather than washing out. Cohen and Frazzini (2008) and Menzly and Ozbas (2010) show that supply chain connections predict cross-sectional return differences: when a major customer’s stock drops, its suppliers’ stocks follow with a delay.\n\n# Load supply chain data\nsupply_chain = dc.get_supply_chain_data(\n    start_date=\"2020-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Build directed supply chain graph\nG_supply = nx.DiGraph()\n\nfor _, row in supply_chain.iterrows():\n    G_supply.add_edge(\n        row[\"supplier_ticker\"],\n        row[\"customer_ticker\"],\n        weight=row.get(\"transaction_value\", 1),\n        pct_of_supplier_revenue=row.get(\"pct_supplier_revenue\", 0),\n        pct_of_customer_cogs=row.get(\"pct_customer_cogs\", 0),\n        year=row.get(\"year\", 2024)\n    )\n\nprint(f\"Supply chain links: {G_supply.number_of_edges()}\")\nprint(f\"Firms involved: {G_supply.number_of_nodes()}\")\n\n\n\n46.4.2 Customer Momentum\nThe “customer momentum” strategy of Cohen and Frazzini (2008) exploits the delayed propagation of information through supply chains. When a customer firm’s stock rises, its suppliers’ stocks tend to follow in subsequent months:\n\\[\nr_{i,t+1} = \\alpha + \\beta \\cdot \\text{CustomerReturn}_{i,t} + \\gamma \\cdot r_{i,t} + \\boldsymbol{\\delta}' \\mathbf{X}_{i,t} + \\varepsilon_{i,t}\n\\tag{46.8}\\]\nwhere \\(\\text{CustomerReturn}_{i,t} = \\sum_{j \\in \\text{Customers}(i)} w_{ij} \\cdot r_{j,t}\\) is the weighted average return of firm \\(i\\)’s customers.\n\n# Compute customer-weighted returns for each supplier\ndef compute_customer_returns(supply_graph, monthly_returns):\n    \"\"\"\n    Compute customer-weighted returns for each supplier firm.\n    \"\"\"\n    # For each supplier, compute weighted average customer return\n    results = []\n\n    for date in monthly_returns[\"date\"].unique():\n        date_returns = monthly_returns[monthly_returns[\"date\"] == date]\n        ret_dict = dict(zip(date_returns[\"ticker\"], date_returns[\"ret\"]))\n\n        for supplier in supply_graph.nodes():\n            customers = list(supply_graph.successors(supplier))\n            if not customers:\n                continue\n\n            customer_rets = []\n            customer_weights = []\n            for cust in customers:\n                if cust in ret_dict:\n                    edge_data = supply_graph[supplier][cust]\n                    weight = edge_data.get(\"pct_of_supplier_revenue\", 1)\n                    customer_rets.append(ret_dict[cust])\n                    customer_weights.append(weight)\n\n            if customer_rets:\n                weights = np.array(customer_weights)\n                if weights.sum() &gt; 0:\n                    weights = weights / weights.sum()\n                else:\n                    weights = np.ones(len(weights)) / len(weights)\n\n                weighted_ret = np.average(customer_rets, weights=weights)\n                results.append({\n                    \"date\": date,\n                    \"ticker\": supplier,\n                    \"customer_ret\": weighted_ret,\n                    \"n_customers\": len(customer_rets)\n                })\n\n    return pd.DataFrame(results)\n\n\ncustomer_rets = compute_customer_returns(G_supply, monthly_returns)\n\n# Merge with own returns and lag\nmomentum_data = monthly_returns.merge(\n    customer_rets, on=[\"ticker\", \"date\"], how=\"inner\"\n)\n\nmomentum_data = momentum_data.sort_values([\"ticker\", \"date\"])\nmomentum_data[\"own_ret_lag\"] = momentum_data.groupby(\"ticker\")[\"ret\"].shift(1)\nmomentum_data[\"customer_ret_lag\"] = momentum_data.groupby(\n    \"ticker\"\n)[\"customer_ret\"].shift(1)\nmomentum_data[\"ret_lead\"] = momentum_data.groupby(\"ticker\")[\"ret\"].shift(-1)\n\n# Regression\nmom_clean = momentum_data.dropna(\n    subset=[\"ret_lead\", \"customer_ret_lag\", \"own_ret_lag\"]\n)\n\nmodel_mom = sm.OLS(\n    mom_clean[\"ret_lead\"],\n    sm.add_constant(mom_clean[[\"customer_ret_lag\", \"own_ret_lag\"]])\n).fit(cov_type=\"cluster\", cov_kwds={\"groups\": mom_clean[\"ticker\"]})\n\n\n\n\nTable 46.4: Customer Momentum: Supplier Returns Predicted by Customer Returns\n\n\nmom_results = pd.DataFrame({\n    \"Coefficient\": model_mom.params.round(4),\n    \"Std Error\": model_mom.bse.round(4),\n    \"t-stat\": model_mom.tvalues.round(3),\n    \"p-value\": model_mom.pvalues.round(4)\n})\nmom_results\n\n\n\n\n\n46.4.3 Network Propagation: Shock Diffusion Through Supply Chains\nThe Acemoglu, Ozdaglar, and Tahbaz-Salehi (2015) model predicts that the aggregate effect of an idiosyncratic shock depends on the network’s topology. In a star network (one central hub), hub shocks generate aggregate fluctuations. In a symmetric network, shocks cancel out. We implement a simulation-based approach to measure shock propagation in the Vietnamese supply chain.\n\ndef simulate_shock_propagation(G, shocked_node, shock_size=-0.10,\n                                decay=0.5, max_steps=5):\n    \"\"\"\n    Simulate the propagation of an idiosyncratic shock through\n    a supply chain network.\n\n    Parameters\n    ----------\n    G : nx.DiGraph\n        Supply chain graph (supplier → customer).\n    shocked_node : str\n        Node receiving the initial shock.\n    shock_size : float\n        Initial shock magnitude (e.g., -0.10 = -10% revenue shock).\n    decay : float\n        Fraction of shock transmitted per link (0 &lt; decay &lt; 1).\n    max_steps : int\n        Maximum propagation steps.\n\n    Returns\n    -------\n    dict : {node: cumulative shock received}.\n    \"\"\"\n    shocks = {shocked_node: shock_size}\n    frontier = {shocked_node}\n\n    for step in range(max_steps):\n        new_frontier = set()\n        for node in frontier:\n            # Propagate to customers (downstream)\n            for customer in G.successors(node):\n                edge_weight = G[node][customer].get(\n                    \"pct_of_customer_cogs\", 0.1\n                )\n                transmitted = shocks[node] * decay * edge_weight\n                if abs(transmitted) &gt; 0.001:\n                    shocks[customer] = shocks.get(customer, 0) + transmitted\n                    new_frontier.add(customer)\n\n            # Propagate to suppliers (upstream)\n            for supplier in G.predecessors(node):\n                edge_weight = G[supplier][node].get(\n                    \"pct_of_supplier_revenue\", 0.1\n                )\n                transmitted = shocks[node] * decay * edge_weight\n                if abs(transmitted) &gt; 0.001:\n                    shocks[supplier] = shocks.get(supplier, 0) + transmitted\n                    new_frontier.add(supplier)\n\n        frontier = new_frontier\n        if not frontier:\n            break\n\n    return shocks\n\n\n# Identify systemically important supply chain nodes\n# (Those whose shock has largest aggregate impact)\nsystemic_importance = {}\nlisted_in_supply = [n for n in G_supply.nodes()\n                    if n in set(firms[\"ticker\"])]\n\nfor firm in listed_in_supply[:100]:  # Sample for computational feasibility\n    shocks = simulate_shock_propagation(G_supply, firm, -0.10)\n    aggregate_impact = sum(abs(v) for k, v in shocks.items() if k != firm)\n    systemic_importance[firm] = aggregate_impact\n\nsystemic_df = pd.DataFrame(\n    list(systemic_importance.items()),\n    columns=[\"ticker\", \"systemic_impact\"]\n).sort_values(\"systemic_impact\", ascending=False)\n\n\n\n\nTable 46.5: Most Systemically Important Firms in the Supply Chain Network\n\n\ntop_systemic = systemic_df.head(20).merge(\n    firms[[\"ticker\", \"industry\", \"market_cap\"]],\n    on=\"ticker\", how=\"left\"\n)\ntop_systemic[\"market_cap_bn\"] = top_systemic[\"market_cap\"] / 1e9\ntop_systemic[[\"ticker\", \"industry\", \"market_cap_bn\", \"systemic_impact\"]].round(4)",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html#correlation-and-co-movement-networks",
    "href": "73_networks_graphs.html#correlation-and-co-movement-networks",
    "title": "46  Networks and Graphs in Finance",
    "section": "46.5 Correlation and Co-Movement Networks",
    "text": "46.5 Correlation and Co-Movement Networks\n\n46.5.1 Construction from Return Data\nA correlation network connects assets whose returns co-move. This is perhaps the most widely used financial network because it requires only return data—no proprietary ownership or supply chain information. The standard construction involves three steps:\n\nCompute pairwise correlations. For \\(n\\) assets over \\(T\\) periods, the sample correlation matrix \\(\\hat{\\rho} \\in \\mathbb{R}^{n \\times n}\\) has \\(n(n-1)/2\\) unique entries.\nThreshold or transform. Convert the dense correlation matrix into a sparse graph. Common approaches: hard threshold (\\(\\rho_{ij} &gt; \\bar{\\rho}\\)), Minimum Spanning Tree (MST), or Planar Maximally Filtered Graph (PMFG).\nAnalyze the graph. Community detection reveals sector clustering; centrality identifies market bellwethers; temporal evolution tracks regime changes.\n\n\n# Compute correlation matrix from daily returns\ndaily_returns = dc.get_daily_returns(\n    start_date=\"2023-01-01\",\n    end_date=\"2024-06-30\"\n)\n\ndaily_wide = daily_returns.pivot(\n    index=\"date\", columns=\"ticker\", values=\"ret\"\n).dropna(axis=1, thresh=200)\n\ncorr = daily_wide.corr()\n\n# Minimum Spanning Tree (MST)\n# Convert correlation to distance: d = sqrt(2(1-ρ))\ndist = np.sqrt(2 * (1 - corr.values))\nnp.fill_diagonal(dist, 0)\n\n# Build complete graph with distances\nG_complete = nx.Graph()\ntickers = list(corr.columns)\nfor i in range(len(tickers)):\n    for j in range(i + 1, len(tickers)):\n        G_complete.add_edge(\n            tickers[i], tickers[j],\n            weight=dist[i, j],\n            correlation=corr.iloc[i, j]\n        )\n\n# MST\nG_mst = nx.minimum_spanning_tree(G_complete, weight=\"weight\")\n\n# Thresholded correlation network\nthreshold = 0.5\nG_corr = nx.Graph()\nfor i in range(len(tickers)):\n    for j in range(i + 1, len(tickers)):\n        if corr.iloc[i, j] &gt; threshold:\n            G_corr.add_edge(\n                tickers[i], tickers[j],\n                weight=corr.iloc[i, j]\n            )\n\nprint(f\"MST: {G_mst.number_of_nodes()} nodes, \"\n      f\"{G_mst.number_of_edges()} edges\")\nprint(f\"Corr network (ρ &gt; {threshold}): \"\n      f\"{G_corr.number_of_nodes()} nodes, \"\n      f\"{G_corr.number_of_edges()} edges\")\n\n\n\n46.5.2 The Diebold-Yilmaz Connectedness Framework\nDiebold and Yılmaz (2014) propose measuring financial connectedness using the variance decomposition from a vector autoregression (VAR). The fraction of the \\(H\\)-step-ahead forecast error variance of variable \\(i\\) attributable to shocks from variable \\(j\\) defines the pairwise directional connectedness:\n\\[\nC_{i \\leftarrow j}(H) = \\frac{\\tilde{\\theta}_{ij}^g(H)}{\\sum_{j=1}^{n} \\tilde{\\theta}_{ij}^g(H)} \\times 100\n\\tag{46.9}\\]\nwhere \\(\\tilde{\\theta}_{ij}^g(H)\\) is the generalized forecast error variance decomposition. The total connectedness index (TCI) aggregates across all pairs:\n\\[\n\\text{TCI}(H) = \\frac{\\sum_{i \\neq j} C_{i \\leftarrow j}(H)}{\\sum_{i,j} C_{i \\leftarrow j}(H)} \\times 100\n\\tag{46.10}\\]\nHigh TCI indicates a tightly connected system where shocks propagate widely; low TCI indicates relative isolation.\n\ndef diebold_yilmaz_connectedness(returns_df, var_order=2,\n                                  forecast_horizon=10):\n    \"\"\"\n    Compute the Diebold-Yilmaz (2014) connectedness table.\n\n    Parameters\n    ----------\n    returns_df : DataFrame\n        Returns with columns as assets, index as dates.\n    var_order : int\n        VAR lag order.\n    forecast_horizon : int\n        Forecast horizon for variance decomposition.\n\n    Returns\n    -------\n    dict : Connectedness matrix, TCI, TO/FROM measures.\n    \"\"\"\n    from statsmodels.tsa.api import VAR\n\n    # Fit VAR\n    model = VAR(returns_df.dropna())\n    results = model.fit(var_order)\n\n    # Generalized Forecast Error Variance Decomposition\n    fevd = results.fevd(forecast_horizon)\n\n    # Extract decomposition matrix at horizon H\n    n = returns_df.shape[1]\n    C = np.zeros((n, n))\n\n    for i in range(n):\n        decomp = fevd.decomp[i]  # H x n array\n        C[i, :] = decomp[-1, :]  # Take horizon H values\n\n    # Normalize rows to sum to 100\n    row_sums = C.sum(axis=1, keepdims=True)\n    C_norm = C / row_sums * 100\n\n    # Connectedness measures\n    # FROM: how much of i's variance comes from others\n    FROM = C_norm.sum(axis=1) - np.diag(C_norm)\n\n    # TO: how much of others' variance comes from i\n    TO = C_norm.sum(axis=0) - np.diag(C_norm)\n\n    # NET = TO - FROM\n    NET = TO - FROM\n\n    # Total Connectedness Index\n    TCI = FROM.sum() / n\n\n    names = returns_df.columns.tolist()\n\n    connectedness_df = pd.DataFrame(C_norm, index=names, columns=names)\n    connectedness_df[\"FROM_others\"] = FROM\n    connectedness_df.loc[\"TO_others\"] = list(TO) + [TCI]\n\n    return {\n        \"connectedness_matrix\": connectedness_df,\n        \"TCI\": TCI,\n        \"FROM\": pd.Series(FROM, index=names),\n        \"TO\": pd.Series(TO, index=names),\n        \"NET\": pd.Series(NET, index=names)\n    }\n\n\n# Apply to top Vietnamese stocks by liquidity\ntop_stocks = (\n    daily_returns.groupby(\"ticker\")[\"volume\"]\n    .mean()\n    .nlargest(20)\n    .index\n)\n\ntop_returns = daily_wide[\n    [c for c in top_stocks if c in daily_wide.columns]\n].dropna()\n\ndy_results = diebold_yilmaz_connectedness(top_returns)\nprint(f\"Total Connectedness Index: {dy_results['TCI']:.2f}%\")\n\n\n\n\nTable 46.6: Diebold-Yilmaz Net Connectedness: Net Transmitters (+) and Receivers (−)\n\n\nnet_df = pd.DataFrame({\n    \"TO_others\": dy_results[\"TO\"].round(2),\n    \"FROM_others\": dy_results[\"FROM\"].round(2),\n    \"NET\": dy_results[\"NET\"].round(2)\n}).sort_values(\"NET\", ascending=False)\n\nnet_df\n\n\n\n\n# Rolling TCI to track systemic risk over time\ndef rolling_tci(returns_wide, window=252, step=21, var_order=1,\n                 forecast_horizon=10, min_stocks=10):\n    \"\"\"Compute rolling Total Connectedness Index.\"\"\"\n    dates = returns_wide.index\n    tci_series = []\n\n    for i in range(window, len(dates), step):\n        window_data = returns_wide.iloc[i - window:i]\n\n        # Keep stocks with sufficient data\n        valid_cols = window_data.dropna(axis=1, thresh=int(window * 0.9))\n        if valid_cols.shape[1] &lt; min_stocks:\n            continue\n\n        # Use top stocks by variance (most informative)\n        top_cols = valid_cols.var().nlargest(min_stocks).index\n        subset = valid_cols[top_cols].dropna()\n\n        if len(subset) &lt; window * 0.8:\n            continue\n\n        try:\n            result = diebold_yilmaz_connectedness(\n                subset, var_order, forecast_horizon\n            )\n            tci_series.append({\n                \"date\": dates[i],\n                \"TCI\": result[\"TCI\"],\n                \"n_stocks\": len(top_cols)\n            })\n        except Exception:\n            continue\n\n    return pd.DataFrame(tci_series)\n\n\n# tci_df = rolling_tci(daily_wide, window=252, step=21)",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html#interbank-and-financial-contagion-networks",
    "href": "73_networks_graphs.html#interbank-and-financial-contagion-networks",
    "title": "46  Networks and Graphs in Finance",
    "section": "46.6 Interbank and Financial Contagion Networks",
    "text": "46.6 Interbank and Financial Contagion Networks\n\n46.6.1 The Interbank Market as a Network\nThe interbank market, where banks lend reserves to each other, is the canonical financial network. Allen and Gale (2000) demonstrate that the structure of interbank linkages determines whether a bank failure cascades into a systemic crisis or is absorbed by the network. Two extreme topologies illustrate:\nComplete network (all banks linked to all). Each bank’s exposure to any single counterpart is small. A bank failure imposes small losses on many banks, which can individually absorb them. The network is resilient.\nRing network (each bank linked to one neighbor). Each bank’s exposure is concentrated in a single counterpart. A failure in one bank can topple its neighbor, triggering a chain of cascading defaults. The network is fragile.\nThe Vietnamese banking system, with its concentration of state-owned banks and the regulatory emphasis on interbank lending for liquidity management, lies between these extremes.\n\n# Load interbank exposure data\ninterbank = dc.get_interbank_exposures(\n    date=\"2024-06-30\"\n)\n\n# Build interbank network\nG_bank = nx.DiGraph()\n\nfor _, row in interbank.iterrows():\n    G_bank.add_edge(\n        row[\"lender_bank\"],\n        row[\"borrower_bank\"],\n        weight=row[\"exposure_bn_vnd\"],\n        exposure_pct_equity=row.get(\"exposure_pct_equity\", 0)\n    )\n\n# Add bank attributes\nbank_info = dc.get_bank_characteristics(date=\"2024-06-30\")\nfor _, row in bank_info.iterrows():\n    if row[\"ticker\"] in G_bank:\n        G_bank.nodes[row[\"ticker\"]].update({\n            \"total_assets\": row.get(\"total_assets\", 0),\n            \"equity\": row.get(\"equity\", 0),\n            \"is_soe\": row.get(\"is_soe\", False),\n            \"tier1_ratio\": row.get(\"tier1_ratio\", 0)\n        })\n\nbank_metrics, bank_graph_stats = compute_network_metrics(G_bank)\n\n\n\n46.6.2 Cascading Default Simulation\nWe implement the Eisenberg and Noe (2001) clearing mechanism to simulate cascading defaults in the interbank network. When a bank fails, it cannot honor its interbank obligations, imposing losses on its creditors, who may in turn fail:\n\\[\np_i^* = \\min\\left(\\bar{p}_i, \\; e_i + \\sum_{j} \\frac{L_{ji}}{\\sum_k L_{jk}} p_j^*\\right)\n\\tag{46.11}\\]\nwhere \\(p_i^*\\) is bank \\(i\\)’s actual payment, \\(\\bar{p}_i\\) is its total obligation, \\(e_i\\) is its external assets minus external liabilities, and \\(L_{ji}/\\sum_k L_{jk}\\) is the fraction of bank \\(j\\)’s obligations owed to bank \\(i\\).\n\ndef simulate_cascading_defaults(G, initial_failure, \n                                  equity_buffer=0.08,\n                                  max_rounds=20):\n    \"\"\"\n    Simulate cascading defaults in an interbank network.\n\n    Parameters\n    ----------\n    G : nx.DiGraph\n        Interbank exposure graph (lender → borrower, weight = exposure).\n    initial_failure : str\n        Bank that fails initially.\n    equity_buffer : float\n        Fraction of equity that absorbs losses before default.\n    max_rounds : int\n        Maximum cascade rounds.\n\n    Returns\n    -------\n    dict : Defaulted banks, round-by-round losses, systemic loss.\n    \"\"\"\n    defaulted = {initial_failure}\n    cascade_history = [{\"round\": 0, \"defaults\": [initial_failure],\n                        \"total_defaults\": 1}]\n\n    for round_num in range(1, max_rounds + 1):\n        new_defaults = set()\n\n        for bank in G.nodes():\n            if bank in defaulted:\n                continue\n\n            # Compute losses from defaulted counterparties\n            total_loss = 0\n            for defaulted_bank in defaulted:\n                if G.has_edge(bank, defaulted_bank):\n                    # bank lent to defaulted_bank → loss\n                    exposure = G[bank][defaulted_bank][\"weight\"]\n                    # Assume LGD = 60%\n                    loss = exposure * 0.6\n                    total_loss += loss\n\n            # Check if losses exceed equity buffer\n            bank_data = G.nodes.get(bank, {})\n            equity = bank_data.get(\"equity\", float(\"inf\"))\n\n            if total_loss &gt; equity * equity_buffer:\n                new_defaults.add(bank)\n\n        if not new_defaults:\n            break\n\n        defaulted |= new_defaults\n        cascade_history.append({\n            \"round\": round_num,\n            \"defaults\": list(new_defaults),\n            \"total_defaults\": len(defaulted)\n        })\n\n    # Compute systemic loss\n    total_system_equity = sum(\n        G.nodes[n].get(\"equity\", 0) for n in G.nodes()\n    )\n    defaulted_equity = sum(\n        G.nodes[n].get(\"equity\", 0) for n in defaulted\n    )\n    systemic_loss_pct = (\n        defaulted_equity / total_system_equity\n        if total_system_equity &gt; 0 else 0\n    )\n\n    return {\n        \"defaulted_banks\": list(defaulted),\n        \"n_defaults\": len(defaulted),\n        \"cascade_rounds\": len(cascade_history) - 1,\n        \"cascade_history\": cascade_history,\n        \"systemic_loss_pct\": systemic_loss_pct\n    }\n\n\n# Simulate cascade for each bank\ncascade_results = []\nfor bank in G_bank.nodes():\n    result = simulate_cascading_defaults(G_bank, bank)\n    cascade_results.append({\n        \"initial_failure\": bank,\n        \"n_cascading_defaults\": result[\"n_defaults\"],\n        \"systemic_loss_pct\": result[\"systemic_loss_pct\"],\n        \"cascade_rounds\": result[\"cascade_rounds\"]\n    })\n\ncascade_df = pd.DataFrame(cascade_results).sort_values(\n    \"systemic_loss_pct\", ascending=False\n)\n\n\n\n\nTable 46.7: Systemically Important Banks: Cascading Default Analysis\n\n\ncascade_df.head(15).round(4)",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html#graph-neural-networks-for-financial-prediction",
    "href": "73_networks_graphs.html#graph-neural-networks-for-financial-prediction",
    "title": "46  Networks and Graphs in Finance",
    "section": "46.7 Graph Neural Networks for Financial Prediction",
    "text": "46.7 Graph Neural Networks for Financial Prediction\n\n46.7.1 From Graphs to Predictions\nClassical network analysis computes hand-crafted features (centrality, clustering, community membership) and feeds them into standard regression models. Graph neural networks (GNNs) learn features directly from the graph structure and node attributes through message passing. The key insight is that a node’s representation should depend not only on its own features but on the features of its neighbors, their neighbors, and so on.\n\n\n46.7.2 Graph Convolutional Networks (GCN)\nThe Graph Convolutional Network of Kipf and Welling (2016) performs spectral convolution on graphs. The layer-wise propagation rule is:\n\\[\nH^{(\\ell+1)} = \\sigma\\left(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(\\ell)} W^{(\\ell)}\\right)\n\\tag{46.12}\\]\nwhere \\(\\tilde{A} = A + I_n\\) is the adjacency matrix with self-loops, \\(\\tilde{D}\\) is the corresponding degree matrix, \\(H^{(\\ell)}\\) is the node feature matrix at layer \\(\\ell\\), \\(W^{(\\ell)}\\) is the learnable weight matrix, and \\(\\sigma\\) is a nonlinearity (ReLU). The normalized \\(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}\\) term averages each node’s features with its neighbors’ features, weighted by degree.\n\n\n46.7.3 Graph Attention Networks (GAT)\nThe Graph Attention Network of Veličković et al. (2017) replaces the fixed normalization in GCN with learned attention coefficients:\n\\[\nh_i^{(\\ell+1)} = \\sigma\\left(\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{ij}^{(\\ell)} W^{(\\ell)} h_j^{(\\ell)}\\right)\n\\tag{46.13}\\]\nwhere the attention coefficient \\(\\alpha_{ij}\\) is computed as:\n\\[\n\\alpha_{ij} = \\frac{\\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}^\\top [W h_i \\| W h_j]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}(i)} \\exp\\left(\\text{LeakyReLU}\\left(\\mathbf{a}^\\top [W h_i \\| W h_k]\\right)\\right)}\n\\tag{46.14}\\]\nThis allows the model to learn which neighbors are most informative for each node, rather than treating all neighbors equally.\n\nclass GCNLayer(nn.Module):\n    \"\"\"Graph Convolutional Network layer (Kipf & Welling, 2016).\"\"\"\n\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n        else:\n            self.bias = None\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight)\n        if self.bias is not None:\n            nn.init.zeros_(self.bias)\n\n    def forward(self, x, adj_norm):\n        \"\"\"\n        Parameters\n        ----------\n        x : Tensor (n_nodes, in_features)\n            Node feature matrix.\n        adj_norm : Tensor (n_nodes, n_nodes)\n            Normalized adjacency matrix (D^{-1/2} A D^{-1/2}).\n        \"\"\"\n        support = x @ self.weight\n        output = adj_norm @ support\n        if self.bias is not None:\n            output = output + self.bias\n        return output\n\n\nclass GATLayer(nn.Module):\n    \"\"\"Graph Attention Network layer (Velickovic et al., 2017).\"\"\"\n\n    def __init__(self, in_features, out_features, n_heads=4,\n                 dropout=0.1, concat=True):\n        super().__init__()\n        self.n_heads = n_heads\n        self.out_features = out_features\n        self.concat = concat\n\n        self.W = nn.Parameter(\n            torch.FloatTensor(n_heads, in_features, out_features)\n        )\n        self.a = nn.Parameter(torch.FloatTensor(n_heads, 2 * out_features, 1))\n        self.dropout = nn.Dropout(dropout)\n        self.leaky_relu = nn.LeakyReLU(0.2)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.W)\n        nn.init.xavier_uniform_(self.a)\n\n    def forward(self, x, adj):\n        \"\"\"\n        Parameters\n        ----------\n        x : Tensor (n_nodes, in_features)\n        adj : Tensor (n_nodes, n_nodes)\n            Binary adjacency matrix (1 if edge, 0 otherwise).\n        \"\"\"\n        n = x.size(0)\n\n        # Linear transformation for each head\n        # x: (n, in_f) -&gt; h: (heads, n, out_f)\n        h = torch.einsum(\"ni,hio-&gt;hno\", x, self.W)\n\n        # Attention coefficients\n        # Concatenate h_i and h_j for all pairs\n        h_i = h.unsqueeze(2).expand(-1, -1, n, -1)  # (heads, n, n, out_f)\n        h_j = h.unsqueeze(1).expand(-1, n, -1, -1)  # (heads, n, n, out_f)\n        concat_h = torch.cat([h_i, h_j], dim=-1)    # (heads, n, n, 2*out_f)\n\n        e = self.leaky_relu(\n            torch.einsum(\"hnmo,hoi-&gt;hnm\", concat_h, self.a).squeeze(-1)\n        )\n\n        # Mask non-edges\n        mask = adj.unsqueeze(0).expand(self.n_heads, -1, -1)\n        e = e.masked_fill(mask == 0, float(\"-inf\"))\n\n        # Softmax attention\n        alpha = F.softmax(e, dim=-1)\n        alpha = self.dropout(alpha)\n\n        # Weighted aggregation\n        out = torch.einsum(\"hnm,hmo-&gt;hno\", alpha, h)  # (heads, n, out_f)\n\n        if self.concat:\n            out = out.permute(1, 0, 2).reshape(n, -1)  # (n, heads * out_f)\n        else:\n            out = out.mean(dim=0)  # (n, out_f)\n\n        return out, alpha\n\n\nclass FinancialGNN(nn.Module):\n    \"\"\"\n    Graph Neural Network for financial node prediction.\n    Supports GCN and GAT layers with flexible architecture.\n    \"\"\"\n\n    def __init__(self, input_dim, hidden_dim=64, output_dim=1,\n                 n_layers=2, n_heads=4, gnn_type=\"gat\",\n                 dropout=0.3):\n        super().__init__()\n\n        self.gnn_type = gnn_type\n        self.dropout = nn.Dropout(dropout)\n\n        if gnn_type == \"gcn\":\n            self.layers = nn.ModuleList()\n            self.layers.append(GCNLayer(input_dim, hidden_dim))\n            for _ in range(n_layers - 1):\n                self.layers.append(GCNLayer(hidden_dim, hidden_dim))\n\n        elif gnn_type == \"gat\":\n            self.layers = nn.ModuleList()\n            self.layers.append(\n                GATLayer(input_dim, hidden_dim // n_heads,\n                         n_heads=n_heads, concat=True)\n            )\n            for _ in range(n_layers - 2):\n                self.layers.append(\n                    GATLayer(hidden_dim, hidden_dim // n_heads,\n                             n_heads=n_heads, concat=True)\n                )\n            # Last layer: single head, no concat\n            self.layers.append(\n                GATLayer(hidden_dim, hidden_dim,\n                         n_heads=1, concat=False)\n            )\n\n        # Prediction head\n        self.head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim // 2, output_dim)\n        )\n\n    def forward(self, x, adj):\n        \"\"\"\n        Parameters\n        ----------\n        x : Tensor (n_nodes, input_dim)\n            Node features.\n        adj : Tensor (n_nodes, n_nodes)\n            Adjacency matrix.\n        \"\"\"\n        h = x\n        attention_weights = []\n\n        for i, layer in enumerate(self.layers):\n            if self.gnn_type == \"gcn\":\n                h = layer(h, adj)\n                h = F.relu(h) if i &lt; len(self.layers) - 1 else h\n            elif self.gnn_type == \"gat\":\n                h, alpha = layer(h, adj)\n                attention_weights.append(alpha)\n                if i &lt; len(self.layers) - 1:\n                    h = F.elu(h)\n\n            h = self.dropout(h)\n\n        predictions = self.head(h).squeeze(-1)\n        return predictions, attention_weights\n\n\n\n46.7.4 GNN for Cross-Sectional Return Prediction\nWe apply the GNN to predict cross-sectional stock returns, where the graph encodes ownership connections between firms. The hypothesis is that firms connected through ownership have correlated return dynamics that the GNN can exploit.\n\ndef prepare_gnn_data(firms_df, ownership_graph, returns_df, date):\n    \"\"\"\n    Prepare node features and adjacency matrix for GNN prediction.\n\n    Parameters\n    ----------\n    firms_df : DataFrame\n        Firm characteristics.\n    ownership_graph : nx.DiGraph\n        Ownership network.\n    returns_df : DataFrame\n        Stock returns.\n\n    Returns\n    -------\n    tuple : (node_features, adjacency, target_returns, ticker_list)\n    \"\"\"\n    # Get stocks with both features and network presence\n    tickers = sorted(\n        set(firms_df[\"ticker\"]) &\n        set(ownership_graph.nodes()) &\n        set(returns_df[\"ticker\"])\n    )\n\n    if not tickers:\n        return None, None, None, None\n\n    # Node features\n    feature_cols = [\n        \"log_size\", \"book_to_market\", \"momentum_12m\",\n        \"profitability\", \"investment\", \"leverage\",\n        \"beta\", \"volatility\", \"turnover\"\n    ]\n\n    feat_df = firms_df[firms_df[\"ticker\"].isin(tickers)].set_index(\"ticker\")\n    feat_df = feat_df.reindex(tickers)\n\n    X = feat_df[feature_cols].fillna(0).values\n    X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n\n    # Adjacency matrix\n    ticker_to_idx = {t: i for i, t in enumerate(tickers)}\n    n = len(tickers)\n    A = np.zeros((n, n))\n\n    for u, v, d in ownership_graph.edges(data=True):\n        if u in ticker_to_idx and v in ticker_to_idx:\n            weight = d.get(\"weight\", 1)\n            A[ticker_to_idx[u], ticker_to_idx[v]] = weight\n            A[ticker_to_idx[v], ticker_to_idx[u]] = weight  # Symmetrize\n\n    # Add self-loops\n    A = A + np.eye(n)\n\n    # Normalize: D^{-1/2} A D^{-1/2}\n    D = np.diag(A.sum(axis=1))\n    D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D) + 1e-10))\n    A_norm = D_inv_sqrt @ A @ D_inv_sqrt\n\n    # Target returns\n    ret_df = returns_df[\n        (returns_df[\"ticker\"].isin(tickers)) &\n        (returns_df[\"date\"] == date)\n    ].set_index(\"ticker\").reindex(tickers)\n\n    y = ret_df[\"ret\"].fillna(0).values\n\n    return (\n        torch.tensor(X, dtype=torch.float32),\n        torch.tensor(A_norm, dtype=torch.float32),\n        torch.tensor(y, dtype=torch.float32),\n        tickers\n    )\n\n\ndef train_gnn_model(model, X_train, A_train, y_train,\n                     X_val, A_val, y_val,\n                     n_epochs=100, lr=1e-3):\n    \"\"\"Train GNN model with validation-based early stopping.\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr,\n                                  weight_decay=1e-4)\n\n    best_val_loss = float(\"inf\")\n    patience_counter = 0\n\n    for epoch in range(n_epochs):\n        model.train()\n        optimizer.zero_grad()\n\n        pred, _ = model(X_train, A_train)\n        loss = F.mse_loss(pred, y_train)\n        loss.backward()\n        optimizer.step()\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_pred, _ = model(X_val, A_val)\n            val_loss = F.mse_loss(val_pred, y_val).item()\n\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter &gt;= 15:\n                break\n\n    model.load_state_dict(best_state)\n    return model, best_val_loss\n\n\n\n46.7.5 Temporal Graph Networks\nFinancial networks evolve over time. Ownership stakes change, directors rotate, correlations shift. Temporal GNNs extend static GNNs by incorporating the time dimension explicitly. The Temporal Graph Network (TGN) of Rossi et al. (2020) maintains a memory state for each node that is updated whenever an event involving the node occurs:\n\\[\n\\mathbf{s}_i(t) = \\text{GRU}\\left(\\mathbf{s}_i(t^-), \\; \\text{msg}(i, t)\\right)\n\\tag{46.15}\\]\nwhere \\(\\mathbf{s}_i(t)\\) is the memory state of node \\(i\\) at time \\(t\\), \\(\\text{msg}(i, t)\\) is the message aggregated from events at time \\(t\\), and GRU is a gated recurrent unit.\n\nclass TemporalFinancialGNN(nn.Module):\n    \"\"\"\n    Temporal GNN for dynamic financial networks.\n    Combines graph convolution with temporal memory.\n    \"\"\"\n\n    def __init__(self, node_dim, edge_dim=1, memory_dim=64,\n                 hidden_dim=64, output_dim=1, n_heads=4):\n        super().__init__()\n\n        self.memory_dim = memory_dim\n\n        # Memory update (GRU)\n        self.memory_updater = nn.GRUCell(\n            input_size=node_dim + edge_dim + memory_dim,\n            hidden_size=memory_dim\n        )\n\n        # Message function\n        self.message_fn = nn.Sequential(\n            nn.Linear(memory_dim * 2 + edge_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, memory_dim)\n        )\n\n        # Graph attention for spatial aggregation\n        self.spatial_attn = GATLayer(\n            memory_dim + node_dim, hidden_dim // n_heads,\n            n_heads=n_heads, concat=True\n        )\n\n        # Temporal attention for recent history\n        self.temporal_attn = nn.MultiheadAttention(\n            embed_dim=hidden_dim, num_heads=n_heads,\n            batch_first=True\n        )\n\n        # Prediction head\n        self.head = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, node_features, adj_sequence, memory=None):\n        \"\"\"\n        Parameters\n        ----------\n        node_features : list of Tensors\n            Node features at each time step.\n        adj_sequence : list of Tensors\n            Adjacency matrices at each time step.\n        memory : Tensor or None\n            Initial memory state (n_nodes, memory_dim).\n\n        Returns\n        -------\n        predictions : Tensor\n            Predictions for the last time step.\n        \"\"\"\n        n_nodes = node_features[0].shape[0]\n        T = len(node_features)\n\n        if memory is None:\n            memory = torch.zeros(n_nodes, self.memory_dim)\n\n        temporal_states = []\n\n        for t in range(T):\n            x_t = node_features[t]\n            adj_t = adj_sequence[t]\n\n            # Spatial aggregation via GAT\n            combined = torch.cat([x_t, memory], dim=-1)\n            spatial_out, _ = self.spatial_attn(combined, adj_t)\n\n            temporal_states.append(spatial_out.unsqueeze(1))\n\n            # Update memory with current state\n            update_input = torch.cat([\n                x_t, spatial_out[:, :1].squeeze(1) if spatial_out.dim() &gt; 2\n                else spatial_out,\n                memory\n            ], dim=-1)[:, :self.memory_updater.input_size]\n\n        # Stack temporal states: (n_nodes, T, hidden)\n        temporal_stack = torch.cat(temporal_states, dim=1)\n\n        # Temporal attention across time steps\n        temporal_out, _ = self.temporal_attn(\n            temporal_stack, temporal_stack, temporal_stack\n        )\n\n        # Use last time step\n        last_spatial = temporal_states[-1].squeeze(1)\n        last_temporal = temporal_out[:, -1, :]\n\n        combined = torch.cat([last_spatial, last_temporal], dim=-1)\n        predictions = self.head(combined).squeeze(-1)\n\n        return predictions\n\n\n\n46.7.6 GNN for Credit Risk and Fraud Detection\nBeyond return prediction, GNNs are particularly powerful for credit risk assessment and fraud detection, where the network structure itself is informative. In credit risk, firms connected to defaulted counterparties face elevated risk. In fraud detection, suspicious transaction patterns propagate through networks of related entities.\n\nclass CreditRiskGNN(nn.Module):\n    \"\"\"\n    GNN for credit default prediction using firm and bank networks.\n    Node classification: predict default probability per firm.\n    \"\"\"\n\n    def __init__(self, firm_features_dim, bank_features_dim=0,\n                 hidden_dim=64, n_heads=4):\n        super().__init__()\n\n        input_dim = firm_features_dim + bank_features_dim\n\n        # GNN backbone\n        self.gnn = FinancialGNN(\n            input_dim=input_dim,\n            hidden_dim=hidden_dim,\n            output_dim=1,\n            n_layers=3,\n            n_heads=n_heads,\n            gnn_type=\"gat\"\n        )\n\n        # Additional head for probability output\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, adj):\n        logits, attn = self.gnn(x, adj)\n        probs = self.sigmoid(logits)\n        return probs, attn",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html#when-to-use-network-methods",
    "href": "73_networks_graphs.html#when-to-use-network-methods",
    "title": "46  Networks and Graphs in Finance",
    "section": "46.8 When to Use Network Methods",
    "text": "46.8 When to Use Network Methods\n\n46.8.1 Decision Framework\n\n\n\nTable 46.8: Decision Framework for Network Methods\n\n\n\n\n\n\n\n\n\n\nQuestion\nBest Approach\nExample\n\n\n\n\nDoes firm \\(A\\) affect firm \\(B\\)?\nPairwise test + network controls\nSupply chain momentum\n\n\nWhich firms are most systemically important?\nCentrality measures\nInterbank contagion\n\n\nDo network-connected firms co-move?\nCorrelation vs. interlock overlap\nBoard interlock diffusion\n\n\nCan network structure predict returns?\nGNN or network-augmented regression\nGCN return prediction\n\n\nHow does a shock propagate?\nCascade simulation\nEisenberg-Noe default model\n\n\nWhat is the optimal portfolio given network risk?\nNetwork-regularized optimization\nCorrelation MST filtering\n\n\nHow does the network change over time?\nTemporal GNN or rolling analysis\nDY rolling connectedness\n\n\n\n\n\n\n\n\n46.8.2 Data Requirements and Vietnamese Availability\n\n\n\nTable 46.9: Financial Network Data Sources in Vietnam\n\n\n\n\n\n\n\n\n\n\n\nNetwork Type\nRequired Data\nVietnamese Availability\nUpdate Frequency\n\n\n\n\nOwnership\nShareholder registers\nGood (semi-annual filings)\nSemi-annual\n\n\nBoard interlocks\nDirector appointments\nGood (filings)\nAnnual\n\n\nSupply chain\nCustomer-supplier disclosures, trade data\nModerate (related-party disclosures)\nAnnual\n\n\nCorrelation\nReturn data\nExcellent (daily from exchanges)\nDaily\n\n\nInterbank\nBilateral exposures\nLimited (SBV data, banks’ notes to FS)\nQuarterly\n\n\nCo-holdings\nInstitutional holdings\nModerate (semi-annual disclosures)\nSemi-annual\n\n\nLending\nLoan-level data\nLimited (banking supervision data)\nQuarterly",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "73_networks_graphs.html#summary",
    "href": "73_networks_graphs.html#summary",
    "title": "46  Networks and Graphs in Finance",
    "section": "46.9 Summary",
    "text": "46.9 Summary\nThis chapter developed the network toolkit for Vietnamese financial markets, spanning classical graph theory, domain-specific financial network construction, and modern graph neural networks.\nThe central insight is that financial networks encode information invisible to standard panel regressions. Ownership networks reveal pyramidal control structures and tunneling risk that explain cross-sectional differences in firm value. Board interlocks serve as information channels through which compensation practices, investment policies, and governance norms diffuse. Supply chain networks propagate real economic shocks: the customer momentum anomaly demonstrates that market prices incorporate supply chain information with a predictable delay. Correlation networks capture co-movement structure that both reveals sector clustering and evolves across market regimes. And interbank networks determine whether individual bank failures cascade into systemic crises.\nVietnamese markets are particularly network-dense because of the pervasive role of state ownership, the prominence of business groups, and the concentration of the banking system. The control-cash flow wedge (i.e., the gap between control rights and cash flow rights created by pyramidal ownership) is larger and more variable than in markets with stronger minority shareholder protections. This wedge, which is computable only through network analysis of ownership chains, is both a governance risk measure and a predictor of firm value.\nGraph neural networks extend the toolkit from hand-crafted network features to learned representations. The GCN and GAT architectures aggregate information across network neighborhoods, and the temporal GNN captures the evolution of network structure over time. For return prediction, the empirical question is whether the GNN’s ability to learn flexible functions of the graph structure outperforms the simpler approach of computing centrality measures and feeding them into standard models. The exercises provide a framework for answering this question rigorously.\nThe network perspective is not a replacement for standard asset pricing or corporate finance methods; it is a complement. The most productive research designs combine network structure with identification strategies from the causal inference toolkit: using network shocks as instruments, exploiting network disruptions as natural experiments, and controlling for network position when estimating treatment effects. The intersection of networks and causal inference (e.g., network interference, peer effects identification, spatial econometrics) represents the frontier of empirical finance.\n\n\n\n\n\n\nAcemoglu, Daron, Asuman Ozdaglar, and Alireza Tahbaz-Salehi. 2015. “Systemic Risk and Stability in Financial Networks.” American Economic Review 105 (2): 564–608.\n\n\nAllen, Franklin, and Douglas Gale. 2000. “Financial Contagion.” Journal of Political Economy 108 (1): 1–33.\n\n\nBebchuk, Lucian A. 1999. “A Rent-Protection Theory of Corporate Ownership and Control.” National Bureau of Economic Research Cambridge, Mass., USA.\n\n\nBizjak, John M, Michael L Lemmon, and Lalitha Naveen. 2008. “Does the Use of Peer Groups Contribute to Higher Pay and Less Efficient Compensation?” Journal of Financial Economics 90 (2): 152–68.\n\n\nCai, Ye, Dan S Dhaliwal, Yongtae Kim, and Carrie Pan. 2014. “Board Interlocks and the Diffusion of Disclosure Policy.” Review of Accounting Studies 19 (3): 1086–1119.\n\n\nCohen, Lauren, and Andrea Frazzini. 2008. “Economic Links and Predictable Returns.” The Journal of Finance 63 (4): 1977–2011.\n\n\nDiebold, Francis X, and Kamil Yılmaz. 2014. “On the Network Topology of Variance Decompositions: Measuring the Connectedness of Financial Firms.” Journal of Econometrics 182 (1): 119–34.\n\n\nEisenberg, Larry, and Thomas H Noe. 2001. “Systemic Risk in Financial Systems.” Management Science 47 (2): 236–49.\n\n\nKipf, Thomas N, and Max Welling. 2016. “Semi-Supervised Classification with Graph Convolutional Networks.” arXiv Preprint arXiv:1609.02907.\n\n\nMenzly, Lior, and Oguzhan Ozbas. 2010. “Market Segmentation and Cross-Predictability of Returns.” The Journal of Finance 65 (4): 1555–80.\n\n\nRossi, Emanuele, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. 2020. “Temporal Graph Networks for Deep Learning on Dynamic Graphs.” arXiv Preprint arXiv:2006.10637.\n\n\nVeličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. “Graph Attention Networks.” arXiv Preprint arXiv:1710.10903.",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Networks and Graphs in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html",
    "href": "74_multimodel.html",
    "title": "47  Multimodal Models in Finance",
    "section": "",
    "text": "47.1 Foundations of Multimodal Learning\nThe preceding chapters treated text and images as isolated data modalities, but financial decision-making is inherently multimodal. An analyst evaluating a Vietnamese real estate developer simultaneously reads the annual report (text), inspects satellite imagery of construction sites (image), reviews quarterly financial statements (tabular), monitors the stock’s price and volume dynamics (time series), and perhaps listens to the earnings call (audio). No single modality captures the full information set. The question this chapter addresses is: can we build models that fuse multiple modalities in a principled way, and does the fusion yield economically meaningful improvements over the best single-modality model?\nThe answer from the recent machine learning literature is increasingly yes, but with important caveats. Multimodal models can exploit complementarities between modalities (e.g., text describes intentions and context; images reveal physical states; tabular data provides precise quantitative snapshots; time series captures dynamics). However, the gains are not automatic. Naive concatenation of heterogeneous features often degrades performance relative to the best unimodal model, a phenomenon known as the “modality laziness” problem (Huang et al. 2021). Effective fusion requires architectures that align representations across modalities, handle missing modalities gracefully (not every firm-quarter has satellite imagery and an earnings call), and avoid the dominant modality drowning out weaker but complementary signals.\nThis chapter develops the multimodal toolkit for Vietnamese financial markets across four progressively complex architectures. We begin with representation alignment (i.e., how to map different modalities into a shared embedding space). We then implement early, late, and cross-attention fusion for return prediction. We build a multimodal document understanding system that jointly processes the text, tables, and images within Vietnamese annual reports. We construct a multimodal earnings surprise model that combines pre-announcement text, satellite imagery, and financial time series. And we address the practical engineering challenges, including missing modalities, computational cost, and evaluation protocols that determine whether multimodal models work in production.",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html#foundations-of-multimodal-learning",
    "href": "74_multimodel.html#foundations-of-multimodal-learning",
    "title": "47  Multimodal Models in Finance",
    "section": "",
    "text": "47.1.1 The Information Structure of Financial Data\nFinancial data is naturally organized into modalities with distinct statistical properties, temporal frequencies, and information content. Table 47.1 summarizes the modalities relevant to Vietnamese equity markets.\n\n\n\nTable 47.1: Modality Landscape in Financial Data\n\n\n\n\n\n\n\n\n\n\n\n\nModality\nExamples\nDimensionality\nFrequency\nEncoding\n\n\n\n\nTabular\nFinancial ratios, ownership, governance\nLow (\\(\\sim\\) 50 features)\nQuarterly/Annual\nStructured numeric\n\n\nText\nAnnual reports, news, filings, social media\nHigh (\\(\\sim\\) 10k tokens)\nEvent-driven\nSequential tokens\n\n\nImage\nSatellite tiles, document scans, news photos\nVery high (\\(\\sim\\) 150k pixels)\nDaily to monthly\nSpatial grid\n\n\nTime series\nPrice, volume, order flow, volatility\nModerate (\\(\\sim\\) 250 days × features)\nDaily/Intraday\nTemporal sequence\n\n\nAudio\nEarnings calls, conference presentations\nVery high (waveform)\nQuarterly\nTemporal waveform\n\n\nGraph\nOwnership networks, supply chains, co-holdings\nVariable\nQuarterly\nAdjacency + node features\n\n\n\n\n\n\nEach modality carries both unique and redundant information relative to others. The value of multimodal fusion lies in the unique (complementary) information:\n\\[\nI(\\text{Returns}; \\text{Text}, \\text{Image}, \\text{Tabular}) \\geq \\max\\left(I(\\text{Returns}; \\text{Text}), I(\\text{Returns}; \\text{Image}), I(\\text{Returns}; \\text{Tabular})\\right)\n\\tag{47.1}\\]\nwhere \\(I(\\cdot; \\cdot)\\) denotes mutual information. The inequality is strict whenever the modalities carry non-redundant predictive content. The goal of fusion is to design architectures that approach the left-hand side.\n\n\n47.1.2 Taxonomies of Fusion\nThe multimodal learning literature (Baltrušaitis, Ahuja, and Morency 2018; Liang, Zadeh, and Morency 2024) organizes fusion strategies along three dimensions.\nBy stage. Where in the processing pipeline are modalities combined?\n\nInput-level (early) fusion: Concatenate raw or lightly processed features before any shared model.\nFeature-level (intermediate) fusion: Align learned representations in a shared latent space, then combine.\nDecision-level (late) fusion: Train separate models per modality, combine predictions.\n\nBy mechanism. How are representations combined?\n\nConcatenation: \\(\\mathbf{z} = [\\mathbf{z}^{(1)}; \\mathbf{z}^{(2)}; \\ldots; \\mathbf{z}^{(M)}]\\). Simple but ignores cross-modal interactions.\nAttention-based: One modality attends to another. Captures interactions but requires sufficient data.\nTensor product: \\(\\mathbf{z} = \\mathbf{z}^{(1)} \\otimes \\mathbf{z}^{(2)}\\). Captures all pairwise interactions but scales quadratically.\nGating: \\(\\mathbf{z} = g(\\mathbf{z}^{(1)}) \\odot \\mathbf{z}^{(2)} + (1 - g(\\mathbf{z}^{(1)})) \\odot \\mathbf{z}^{(3)}\\). Modality selection.\n\nBy training. How are parameters learned?\n\nJoint training: All modalities processed end-to-end.\nPre-train then fuse: Train unimodal encoders separately, then learn the fusion layer.\nContrastive alignment: Train modality encoders to produce similar representations for matched pairs (the CLIP approach of Radford et al. (2021)).\n\n\n# DataCore.vn API\nfrom datacore import DataCore\ndc = DataCore()\n\n# Load aligned multimodal dataset\n# Each observation: firm × quarter with all available modalities\n\n# Tabular: financial statements\nfinancials = dc.get_firm_financials(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    frequency=\"quarterly\"\n)\n\n# Text: management discussion from annual reports\nreport_text = dc.get_annual_report_text(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    section=\"management_discussion\"\n)\n\n# Image: satellite nightlight features (from Chapter 61)\nsatellite_features = dc.get_satellite_features(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\",\n    feature_type=\"cnn_resnet50\"\n)\n\n# Time series: daily returns and volume\ndaily_data = dc.get_daily_returns(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Target: forward quarterly returns\nquarterly_returns = dc.get_quarterly_returns(\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\n\nprint(f\"Firms with financials: {financials['ticker'].nunique()}\")\nprint(f\"Firms with report text: {report_text['ticker'].nunique()}\")\nprint(f\"Firms with satellite data: {satellite_features['ticker'].nunique()}\")",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html#representation-alignment",
    "href": "74_multimodel.html#representation-alignment",
    "title": "47  Multimodal Models in Finance",
    "section": "47.2 Representation Alignment",
    "text": "47.2 Representation Alignment\n\n47.2.1 The Alignment Problem\nDifferent modalities produce embeddings in different vector spaces with different geometries. A PhoBERT text embedding lives in \\(\\mathbb{R}^{768}\\); a ResNet50 image feature lives in \\(\\mathbb{R}^{2048}\\); a tabular feature vector might have 50 dimensions with heterogeneous scales. Naively concatenating these into a single vector \\([\\mathbf{z}^{\\text{text}}; \\mathbf{z}^{\\text{image}}; \\mathbf{z}^{\\text{tab}}] \\in \\mathbb{R}^{2866}\\) is problematic because the high-dimensional modalities dominate gradient flow, the scales are mismatched, and there is no mechanism for cross-modal interaction.\nAlignment projects each modality into a shared latent space \\(\\mathbb{R}^d\\) where geometric relationships are semantically meaningful (i.e., similar firms should be nearby regardless of which modality is used to represent them).\n\n\n47.2.2 Contrastive Alignment: CLIP for Finance\nThe Contrastive Language-Image Pre-training (CLIP) framework of Radford et al. (2021) learns aligned representations by training on matched (text, image) pairs. We adapt this to financial data: for each firm-quarter, we have a textual description and a satellite image, and we train the encoders so that matched pairs produce similar embeddings while unmatched pairs produce dissimilar embeddings.\nThe contrastive loss is:\n\\[\n\\mathcal{L}_{\\text{CLIP}} = -\\frac{1}{2N}\\sum_{i=1}^{N}\\left[\\log\\frac{\\exp(\\mathbf{z}_i^{\\text{txt}} \\cdot \\mathbf{z}_i^{\\text{img}} / \\tau)}{\\sum_{j=1}^{N}\\exp(\\mathbf{z}_i^{\\text{txt}} \\cdot \\mathbf{z}_j^{\\text{img}} / \\tau)} + \\log\\frac{\\exp(\\mathbf{z}_i^{\\text{img}} \\cdot \\mathbf{z}_i^{\\text{txt}} / \\tau)}{\\sum_{j=1}^{N}\\exp(\\mathbf{z}_i^{\\text{img}} \\cdot \\mathbf{z}_j^{\\text{txt}} / \\tau)}\\right]\n\\tag{47.2}\\]\nwhere \\(\\tau\\) is a learnable temperature parameter and the embeddings are \\(L_2\\)-normalized. This is a symmetric version of the InfoNCE loss (Oord, Li, and Vinyals 2018) that simultaneously trains the text encoder to predict the correct image and vice versa.\n\nclass FinancialCLIP(nn.Module):\n    \"\"\"\n    Contrastive alignment of text and image embeddings\n    for Vietnamese financial data.\n    \"\"\"\n\n    def __init__(self, text_dim=768, image_dim=2048, proj_dim=256):\n        super().__init__()\n\n        # Text projection\n        self.text_proj = nn.Sequential(\n            nn.Linear(text_dim, proj_dim),\n            nn.LayerNorm(proj_dim),\n            nn.GELU(),\n            nn.Linear(proj_dim, proj_dim)\n        )\n\n        # Image projection\n        self.image_proj = nn.Sequential(\n            nn.Linear(image_dim, proj_dim),\n            nn.LayerNorm(proj_dim),\n            nn.GELU(),\n            nn.Linear(proj_dim, proj_dim)\n        )\n\n        # Learnable temperature\n        self.log_temp = nn.Parameter(torch.tensor(np.log(1 / 0.07)))\n\n    def forward(self, text_emb, image_emb):\n        \"\"\"Compute aligned embeddings and contrastive loss.\"\"\"\n        # Project and normalize\n        z_text = F.normalize(self.text_proj(text_emb), dim=-1)\n        z_image = F.normalize(self.image_proj(image_emb), dim=-1)\n\n        # Similarity matrix\n        temp = self.log_temp.exp()\n        logits = z_text @ z_image.T * temp\n\n        # Symmetric cross-entropy loss\n        labels = torch.arange(len(text_emb), device=text_emb.device)\n        loss_t2i = F.cross_entropy(logits, labels)\n        loss_i2t = F.cross_entropy(logits.T, labels)\n\n        loss = (loss_t2i + loss_i2t) / 2\n\n        return z_text, z_image, loss\n\n    def encode_text(self, text_emb):\n        return F.normalize(self.text_proj(text_emb), dim=-1)\n\n    def encode_image(self, image_emb):\n        return F.normalize(self.image_proj(image_emb), dim=-1)\n\n\n\n47.2.3 Projection Alignment for Arbitrary Modalities\nFor more than two modalities, we generalize to a shared projection space where each modality has its own encoder but all encoders map to the same target space:\n\\[\n\\mathbf{z}_i^{(m)} = f^{(m)}(\\mathbf{x}_i^{(m)}; \\boldsymbol{\\theta}^{(m)}) \\in \\mathbb{R}^d, \\qquad m = 1, \\ldots, M\n\\tag{47.3}\\]\nThe alignment loss encourages all modality embeddings for the same observation to be similar:\n\\[\n\\mathcal{L}_{\\text{align}} = \\sum_{m &lt; m'} \\frac{1}{N}\\sum_{i=1}^{N} \\left\\|\\mathbf{z}_i^{(m)} - \\mathbf{z}_i^{(m')}\\right\\|^2\n\\tag{47.4}\\]\nThis MSE alignment is simpler than contrastive alignment but does not enforce the discriminative property (different observations should have dissimilar embeddings). In practice, we combine alignment with a prediction objective:\n\\[\n\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{predict}}(\\hat{y}, y) + \\lambda \\cdot \\mathcal{L}_{\\text{align}}\n\\tag{47.5}\\]\n\nclass MultimodalProjector(nn.Module):\n    \"\"\"\n    Project arbitrary modalities into a shared latent space.\n    Supports variable numbers of modalities per observation.\n    \"\"\"\n\n    def __init__(self, modality_dims, proj_dim=128, dropout=0.2):\n        \"\"\"\n        Parameters\n        ----------\n        modality_dims : dict\n            {modality_name: input_dim}, e.g.,\n            {'text': 768, 'image': 2048, 'tabular': 50, 'ts': 128}\n        proj_dim : int\n            Shared projection dimensionality.\n        \"\"\"\n        super().__init__()\n\n        self.modality_names = list(modality_dims.keys())\n        self.proj_dim = proj_dim\n\n        # Per-modality encoders\n        self.encoders = nn.ModuleDict()\n        for name, dim in modality_dims.items():\n            self.encoders[name] = nn.Sequential(\n                nn.Linear(dim, proj_dim * 2),\n                nn.LayerNorm(proj_dim * 2),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(proj_dim * 2, proj_dim),\n                nn.LayerNorm(proj_dim)\n            )\n\n    def forward(self, modality_inputs):\n        \"\"\"\n        Parameters\n        ----------\n        modality_inputs : dict\n            {modality_name: tensor}, may be missing some modalities.\n\n        Returns\n        -------\n        dict : {modality_name: projected_embedding}\n        \"\"\"\n        embeddings = {}\n        for name, x in modality_inputs.items():\n            if name in self.encoders and x is not None:\n                embeddings[name] = self.encoders[name](x)\n\n        return embeddings\n\n    def compute_alignment_loss(self, embeddings):\n        \"\"\"Pairwise MSE alignment across all available modalities.\"\"\"\n        names = list(embeddings.keys())\n        if len(names) &lt; 2:\n            return torch.tensor(0.0, device=next(self.parameters()).device)\n\n        loss = torch.tensor(0.0, device=next(self.parameters()).device)\n        n_pairs = 0\n        for i in range(len(names)):\n            for j in range(i + 1, len(names)):\n                loss += F.mse_loss(\n                    embeddings[names[i]], embeddings[names[j]]\n                )\n                n_pairs += 1\n\n        return loss / n_pairs if n_pairs &gt; 0 else loss",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html#fusion-architectures-for-return-prediction",
    "href": "74_multimodel.html#fusion-architectures-for-return-prediction",
    "title": "47  Multimodal Models in Finance",
    "section": "47.3 Fusion Architectures for Return Prediction",
    "text": "47.3 Fusion Architectures for Return Prediction\n\n47.3.1 Unimodal Encoders\nBefore fusing modalities, we need encoders that produce fixed-dimensional representations from each raw input. We build four encoders corresponding to the primary modalities in Vietnamese equity markets.\n\nclass TabularEncoder(nn.Module):\n    \"\"\"Encode financial statement features.\"\"\"\n\n    def __init__(self, input_dim, hidden_dim=128, output_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TextEncoder(nn.Module):\n    \"\"\"\n    Encode Vietnamese text using pre-extracted PhoBERT embeddings.\n    Input: pre-computed [CLS] token embedding (768-d).\n    \"\"\"\n\n    def __init__(self, input_dim=768, output_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.LayerNorm(256),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass ImageEncoder(nn.Module):\n    \"\"\"\n    Encode satellite / document image features.\n    Input: pre-computed CNN features (e.g., ResNet50 2048-d).\n    \"\"\"\n\n    def __init__(self, input_dim=2048, output_dim=64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.LayerNorm(512),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, output_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TimeSeriesEncoder(nn.Module):\n    \"\"\"\n    Encode price/volume time series using a 1D CNN + attention.\n    Input: (batch, seq_len, n_features) tensor of daily data.\n    \"\"\"\n\n    def __init__(self, n_features=5, seq_len=60, output_dim=64):\n        super().__init__()\n\n        # 1D convolutional layers\n        self.conv1 = nn.Conv1d(n_features, 32, kernel_size=5, padding=2)\n        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n        self.pool = nn.AdaptiveAvgPool1d(1)\n\n        # Temporal attention\n        self.attn = nn.MultiheadAttention(\n            embed_dim=64, num_heads=4, batch_first=True\n        )\n\n        self.fc = nn.Linear(64, output_dim)\n\n    def forward(self, x):\n        # x: (B, T, F) -&gt; (B, F, T) for Conv1d\n        x = x.transpose(1, 2)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n\n        # (B, 64, T) -&gt; (B, T, 64) for attention\n        x = x.transpose(1, 2)\n        attn_out, _ = self.attn(x, x, x)\n\n        # Pool over time\n        x = attn_out.transpose(1, 2)  # (B, 64, T)\n        x = self.pool(x).squeeze(-1)  # (B, 64)\n\n        return self.fc(x)\n\n\n\n47.3.2 Early Fusion\nEarly fusion concatenates modality embeddings before a shared prediction head. This is the simplest approach and serves as a natural baseline.\n\nclass EarlyFusionModel(nn.Module):\n    \"\"\"\n    Concatenate modality embeddings, then predict.\n    \"\"\"\n\n    def __init__(self, encoders, hidden_dim=128, output_dim=1):\n        \"\"\"\n        Parameters\n        ----------\n        encoders : dict\n            {modality_name: encoder_module}\n            Each encoder outputs a vector of the same dimension.\n        \"\"\"\n        super().__init__()\n        self.encoders = nn.ModuleDict(encoders)\n        self.n_modalities = len(encoders)\n\n        # Infer encoder output dim from first encoder\n        sample_encoder = list(encoders.values())[0]\n        enc_dim = list(sample_encoder.parameters())[-1].shape[0]\n\n        self.head = nn.Sequential(\n            nn.Linear(enc_dim * self.n_modalities, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, output_dim)\n        )\n\n    def forward(self, inputs):\n        \"\"\"\n        Parameters\n        ----------\n        inputs : dict\n            {modality_name: tensor}\n        \"\"\"\n        embeddings = []\n        for name, encoder in self.encoders.items():\n            if name in inputs and inputs[name] is not None:\n                embeddings.append(encoder(inputs[name]))\n            else:\n                # Zero-fill missing modalities\n                device = next(self.parameters()).device\n                enc_dim = list(encoder.parameters())[-1].shape[0]\n                embeddings.append(torch.zeros(\n                    inputs[list(inputs.keys())[0]].shape[0],\n                    enc_dim, device=device\n                ))\n\n        combined = torch.cat(embeddings, dim=-1)\n        return self.head(combined).squeeze(-1)\n\n\n\n47.3.3 Late Fusion\nLate fusion trains independent models per modality and combines their predictions. The combination weights can be fixed (equal averaging), learned (linear), or adaptive (gating network).\n\nclass LateFusionModel(nn.Module):\n    \"\"\"\n    Independent prediction per modality, learned combination.\n    \"\"\"\n\n    def __init__(self, encoders, enc_dim=64, combination=\"learned\"):\n        \"\"\"\n        Parameters\n        ----------\n        combination : str\n            'average', 'learned', or 'gating'.\n        \"\"\"\n        super().__init__()\n        self.encoders = nn.ModuleDict(encoders)\n        self.combination = combination\n        self.n_modalities = len(encoders)\n\n        # Per-modality prediction heads\n        self.heads = nn.ModuleDict({\n            name: nn.Linear(enc_dim, 1)\n            for name in encoders\n        })\n\n        if combination == \"learned\":\n            self.weights = nn.Parameter(\n                torch.ones(self.n_modalities) / self.n_modalities\n            )\n        elif combination == \"gating\":\n            # Gating network takes all embeddings as input\n            self.gate = nn.Sequential(\n                nn.Linear(enc_dim * self.n_modalities, self.n_modalities),\n                nn.Softmax(dim=-1)\n            )\n\n    def forward(self, inputs):\n        predictions = {}\n        embeddings = {}\n\n        for name, encoder in self.encoders.items():\n            if name in inputs and inputs[name] is not None:\n                emb = encoder(inputs[name])\n                pred = self.heads[name](emb).squeeze(-1)\n                predictions[name] = pred\n                embeddings[name] = emb\n            else:\n                device = next(self.parameters()).device\n                batch_size = inputs[list(inputs.keys())[0]].shape[0]\n                predictions[name] = torch.zeros(batch_size, device=device)\n                enc_dim = list(encoder.parameters())[-1].shape[0]\n                embeddings[name] = torch.zeros(\n                    batch_size, enc_dim, device=device\n                )\n\n        pred_stack = torch.stack(list(predictions.values()), dim=-1)\n\n        if self.combination == \"average\":\n            return pred_stack.mean(dim=-1)\n        elif self.combination == \"learned\":\n            weights = F.softmax(self.weights, dim=0)\n            return (pred_stack * weights).sum(dim=-1)\n        elif self.combination == \"gating\":\n            all_emb = torch.cat(list(embeddings.values()), dim=-1)\n            gate_weights = self.gate(all_emb)\n            return (pred_stack * gate_weights).sum(dim=-1)\n\n    def get_modality_weights(self):\n        \"\"\"Return the contribution of each modality.\"\"\"\n        if self.combination == \"learned\":\n            return F.softmax(self.weights, dim=0).detach().cpu().numpy()\n        return None\n\n\n\n47.3.4 Cross-Attention Fusion\nCross-attention fusion is the most expressive architecture. Each modality attends to every other modality, learning which cross-modal interactions are informative. This is the mechanism underlying modern vision-language models like Flamingo (Alayrac et al. 2022) and GPT-4V.\nThe cross-attention operation for modality \\(m\\) attending to modality \\(m'\\) is:\n\\[\n\\text{CA}^{(m \\to m')} = \\text{softmax}\\left(\\frac{\\mathbf{Q}^{(m)} \\left(\\mathbf{K}^{(m')}\\right)^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V}^{(m')}\n\\tag{47.6}\\]\nwhere \\(\\mathbf{Q}^{(m)} = \\mathbf{z}^{(m)} W_Q\\), \\(\\mathbf{K}^{(m')} = \\mathbf{z}^{(m')} W_K\\), \\(\\mathbf{V}^{(m')} = \\mathbf{z}^{(m')} W_V\\). The output enriches modality \\(m\\)’s representation with information from modality \\(m'\\).\n\nclass CrossAttentionBlock(nn.Module):\n    \"\"\"Single cross-attention block: query modality attends to key modality.\"\"\"\n\n    def __init__(self, dim, n_heads=4, dropout=0.1):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(\n            embed_dim=dim, num_heads=n_heads,\n            dropout=dropout, batch_first=True\n        )\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        self.ffn = nn.Sequential(\n            nn.Linear(dim, dim * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * 4, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, query, key_value):\n        # Cross-attention\n        q = query.unsqueeze(1) if query.dim() == 2 else query\n        kv = key_value.unsqueeze(1) if key_value.dim() == 2 else key_value\n\n        attn_out, attn_weights = self.attn(q, kv, kv)\n        q = self.norm1(q + attn_out)\n\n        # Feed-forward\n        out = self.norm2(q + self.ffn(q))\n        return out.squeeze(1) if query.dim() == 2 else out, attn_weights\n\n\nclass CrossAttentionFusionModel(nn.Module):\n    \"\"\"\n    Full cross-attention fusion across M modalities.\n    Each modality attends to all others via cross-attention blocks.\n    \"\"\"\n\n    def __init__(self, encoders, enc_dim=64, n_layers=2, n_heads=4):\n        super().__init__()\n        self.encoders = nn.ModuleDict(encoders)\n        self.modality_names = list(encoders.keys())\n        self.n_modalities = len(encoders)\n\n        # Cross-attention blocks: each modality attends to each other\n        self.cross_attn_layers = nn.ModuleList()\n        for _ in range(n_layers):\n            layer = nn.ModuleDict()\n            for m in self.modality_names:\n                for m_prime in self.modality_names:\n                    if m != m_prime:\n                        layer[f\"{m}_to_{m_prime}\"] = CrossAttentionBlock(\n                            enc_dim, n_heads\n                        )\n            self.cross_attn_layers.append(layer)\n\n        # Prediction head\n        self.head = nn.Sequential(\n            nn.Linear(enc_dim * self.n_modalities, enc_dim),\n            nn.LayerNorm(enc_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(enc_dim, 1)\n        )\n\n    def forward(self, inputs):\n        # Encode each modality\n        embeddings = {}\n        for name, encoder in self.encoders.items():\n            if name in inputs and inputs[name] is not None:\n                embeddings[name] = encoder(inputs[name])\n            else:\n                device = next(self.parameters()).device\n                batch_size = inputs[list(inputs.keys())[0]].shape[0]\n                enc_dim = list(encoder.parameters())[-1].shape[0]\n                embeddings[name] = torch.zeros(\n                    batch_size, enc_dim, device=device\n                )\n\n        # Cross-attention layers\n        all_attn_weights = {}\n        for layer in self.cross_attn_layers:\n            new_embeddings = {k: v.clone() for k, v in embeddings.items()}\n            for key, block in layer.items():\n                parts = key.split(\"_to_\")\n                query_mod, kv_mod = parts[0], parts[1]\n                if query_mod in embeddings and kv_mod in embeddings:\n                    updated, weights = block(\n                        embeddings[query_mod],\n                        embeddings[kv_mod]\n                    )\n                    new_embeddings[query_mod] = (\n                        new_embeddings[query_mod] + updated\n                    )\n                    all_attn_weights[key] = weights\n\n            embeddings = new_embeddings\n\n        # Concatenate and predict\n        combined = torch.cat(\n            [embeddings[name] for name in self.modality_names],\n            dim=-1\n        )\n        return self.head(combined).squeeze(-1), all_attn_weights\n\n\n\n47.3.5 Comparison Experiment\nWe now compare the three fusion architectures against unimodal baselines on forward quarterly return prediction for Vietnamese equities.\n\nclass MultimodalFinanceDataset(Dataset):\n    \"\"\"\n    Dataset that aligns multiple modalities per firm-quarter.\n    Handles missing modalities with None values.\n    \"\"\"\n\n    def __init__(self, tabular_df, text_embeddings, image_features,\n                 ts_features, returns, tickers, dates):\n        self.tabular = tabular_df\n        self.text = text_embeddings\n        self.image = image_features\n        self.ts = ts_features\n        self.returns = returns\n        self.tickers = tickers\n        self.dates = dates\n\n    def __len__(self):\n        return len(self.returns)\n\n    def __getitem__(self, idx):\n        sample = {\n            \"tabular\": torch.tensor(\n                self.tabular[idx], dtype=torch.float32\n            ) if self.tabular[idx] is not None else None,\n            \"text\": torch.tensor(\n                self.text[idx], dtype=torch.float32\n            ) if self.text[idx] is not None else None,\n            \"image\": torch.tensor(\n                self.image[idx], dtype=torch.float32\n            ) if self.image[idx] is not None else None,\n            \"ts\": torch.tensor(\n                self.ts[idx], dtype=torch.float32\n            ) if self.ts[idx] is not None else None,\n            \"return\": torch.tensor(\n                self.returns[idx], dtype=torch.float32\n            ),\n            \"ticker\": self.tickers[idx],\n            \"date\": self.dates[idx]\n        }\n        return sample\n\n\ndef collate_multimodal(batch):\n    \"\"\"Custom collate that handles None modalities.\"\"\"\n    result = {\"return\": torch.stack([b[\"return\"] for b in batch])}\n\n    for mod in [\"tabular\", \"text\", \"image\", \"ts\"]:\n        values = [b[mod] for b in batch]\n        if all(v is not None for v in values):\n            result[mod] = torch.stack(values)\n        elif any(v is not None for v in values):\n            # Fill None with zeros, matching shape of non-None entries\n            ref = next(v for v in values if v is not None)\n            filled = [v if v is not None else torch.zeros_like(ref)\n                      for v in values]\n            result[mod] = torch.stack(filled)\n        else:\n            result[mod] = None\n\n    return result\n\n\n# Prepare aligned firm-quarter dataset\n# Step 1: Financial ratios (tabular)\ntabular_features = [\n    \"roe\", \"roa\", \"book_to_market\", \"log_size\", \"leverage\",\n    \"asset_growth\", \"gross_profitability\", \"capex_to_assets\",\n    \"cash_to_assets\", \"dividend_yield\", \"sales_growth\",\n    \"accruals\", \"earnings_volatility\", \"beta\"\n]\n\nfinancials[\"quarter_date\"] = pd.to_datetime(\n    financials[\"year\"].astype(str) + \"-\" +\n    (financials[\"quarter\"] * 3).astype(str).str.zfill(2) + \"-01\"\n)\n\n# Step 2: Text embeddings from PhoBERT\n# (Pre-computed in Chapter 60)\ntext_emb = dc.get_text_embeddings(\n    model=\"phobert\",\n    section=\"management_discussion\",\n    start_date=\"2014-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Step 3: Image features (pre-computed in Chapter 61)\n# Satellite CNN features linked to firm headquarters province\n\n# Step 4: Time series features (60-day window before quarter end)\ndef compute_ts_features(ticker, date, daily_df, lookback=60):\n    \"\"\"Extract time-series feature tensor for a firm-quarter.\"\"\"\n    mask = (\n        (daily_df[\"ticker\"] == ticker) &\n        (daily_df[\"date\"] &lt;= date) &\n        (daily_df[\"date\"] &gt;= date - pd.Timedelta(days=lookback * 1.5))\n    )\n    subset = daily_df[mask].sort_values(\"date\").tail(lookback)\n\n    if len(subset) &lt; lookback // 2:\n        return None\n\n    features = subset[[\"ret\", \"volume_log\", \"volatility_20d\",\n                        \"spread\", \"turnover\"]].values\n\n    # Pad if shorter than lookback\n    if len(features) &lt; lookback:\n        padding = np.zeros((lookback - len(features), features.shape[1]))\n        features = np.vstack([padding, features])\n\n    return features\n\n# Step 5: Forward quarterly returns (target)\n# Align everything to quarter-end dates\nprint(\"Preparing aligned multimodal dataset...\")\n\n\ndef train_multimodal_model(model, train_loader, val_loader,\n                            n_epochs=50, lr=1e-3, patience=10,\n                            alignment_weight=0.0):\n    \"\"\"\n    Train a multimodal model with early stopping.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Multimodal fusion model.\n    alignment_weight : float\n        Weight for modality alignment loss (0 = no alignment).\n\n    Returns\n    -------\n    dict : Training history and best validation metrics.\n    \"\"\"\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n                                   weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", patience=5, factor=0.5\n    )\n\n    best_val_loss = float(\"inf\")\n    epochs_no_improve = 0\n    history = {\"train_loss\": [], \"val_loss\": [], \"val_r2\": []}\n\n    for epoch in range(n_epochs):\n        # Training\n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            optimizer.zero_grad()\n\n            inputs = {k: batch[k] for k in [\"tabular\", \"text\", \"image\", \"ts\"]}\n            targets = batch[\"return\"]\n\n            # Forward pass (handle both output types)\n            output = model(inputs)\n            if isinstance(output, tuple):\n                predictions, attn_weights = output\n            else:\n                predictions = output\n\n            loss = F.mse_loss(predictions, targets)\n\n            # Optional alignment loss\n            if alignment_weight &gt; 0 and hasattr(model, \"projector\"):\n                embeddings = model.projector(inputs)\n                align_loss = model.projector.compute_alignment_loss(embeddings)\n                loss = loss + alignment_weight * align_loss\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            train_losses.append(loss.item())\n\n        # Validation\n        model.eval()\n        val_preds, val_targets = [], []\n        val_losses = []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = {k: batch[k]\n                          for k in [\"tabular\", \"text\", \"image\", \"ts\"]}\n                targets = batch[\"return\"]\n\n                output = model(inputs)\n                if isinstance(output, tuple):\n                    predictions, _ = output\n                else:\n                    predictions = output\n\n                val_losses.append(F.mse_loss(predictions, targets).item())\n                val_preds.extend(predictions.cpu().numpy())\n                val_targets.extend(targets.cpu().numpy())\n\n        val_loss = np.mean(val_losses)\n        val_r2 = r2_score(val_targets, val_preds) if len(val_preds) &gt; 10 else 0\n\n        history[\"train_loss\"].append(np.mean(train_losses))\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_r2\"].append(val_r2)\n\n        scheduler.step(val_loss)\n\n        # Early stopping\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            best_state = {k: v.cpu().clone()\n                         for k, v in model.state_dict().items()}\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve &gt;= patience:\n                break\n\n    # Restore best model\n    model.load_state_dict(best_state)\n\n    return {\n        \"history\": history,\n        \"best_val_loss\": best_val_loss,\n        \"best_val_r2\": max(history[\"val_r2\"]),\n        \"epochs_trained\": len(history[\"train_loss\"])\n    }\n\n\ndef compare_fusion_strategies(dataset, n_splits=5):\n    \"\"\"\n    Compare unimodal baselines and multimodal fusion strategies\n    using expanding-window time-series cross-validation.\n\n    Returns\n    -------\n    DataFrame : Out-of-sample R², MSE, IC for each model.\n    \"\"\"\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    results = []\n\n    enc_dim = 64\n\n    for fold, (train_idx, test_idx) in enumerate(\n        tscv.split(range(len(dataset)))\n    ):\n        # Create data loaders\n        train_subset = torch.utils.data.Subset(dataset, train_idx)\n        test_subset = torch.utils.data.Subset(dataset, test_idx)\n\n        train_loader = DataLoader(\n            train_subset, batch_size=128, shuffle=True,\n            collate_fn=collate_multimodal\n        )\n        test_loader = DataLoader(\n            test_subset, batch_size=256, shuffle=False,\n            collate_fn=collate_multimodal\n        )\n\n        # Define encoders\n        def make_encoders():\n            return {\n                \"tabular\": TabularEncoder(len(tabular_features), 128, enc_dim),\n                \"text\": TextEncoder(768, enc_dim),\n                \"image\": ImageEncoder(2048, enc_dim),\n                \"ts\": TimeSeriesEncoder(5, 60, enc_dim)\n            }\n\n        # Unimodal baselines\n        for mod_name in [\"tabular\", \"text\", \"image\", \"ts\"]:\n            single_encoder = {mod_name: make_encoders()[mod_name]}\n            model = EarlyFusionModel(single_encoder, enc_dim, 1)\n\n            result = train_multimodal_model(\n                model, train_loader, test_loader, n_epochs=30\n            )\n            results.append({\n                \"fold\": fold,\n                \"model\": f\"Unimodal ({mod_name})\",\n                \"val_r2\": result[\"best_val_r2\"],\n                \"val_loss\": result[\"best_val_loss\"]\n            })\n\n        # Multimodal: Early Fusion\n        model_early = EarlyFusionModel(make_encoders(), enc_dim * 2, 1)\n        result = train_multimodal_model(\n            model_early, train_loader, test_loader, n_epochs=30\n        )\n        results.append({\n            \"fold\": fold,\n            \"model\": \"Early Fusion\",\n            \"val_r2\": result[\"best_val_r2\"],\n            \"val_loss\": result[\"best_val_loss\"]\n        })\n\n        # Multimodal: Late Fusion (gating)\n        model_late = LateFusionModel(\n            make_encoders(), enc_dim, combination=\"gating\"\n        )\n        result = train_multimodal_model(\n            model_late, train_loader, test_loader, n_epochs=30\n        )\n        results.append({\n            \"fold\": fold,\n            \"model\": \"Late Fusion (Gating)\",\n            \"val_r2\": result[\"best_val_r2\"],\n            \"val_loss\": result[\"best_val_loss\"]\n        })\n\n        # Multimodal: Cross-Attention\n        model_ca = CrossAttentionFusionModel(\n            make_encoders(), enc_dim, n_layers=2, n_heads=4\n        )\n        result = train_multimodal_model(\n            model_ca, train_loader, test_loader, n_epochs=30\n        )\n        results.append({\n            \"fold\": fold,\n            \"model\": \"Cross-Attention Fusion\",\n            \"val_r2\": result[\"best_val_r2\"],\n            \"val_loss\": result[\"best_val_loss\"]\n        })\n\n    return pd.DataFrame(results)\n\n\n\n\nTable 47.2: Out-of-Sample Return Prediction: Unimodal vs. Multimodal\n\n\n# results_df = compare_fusion_strategies(dataset)\n\n# Aggregate across folds\n# summary = (\n#     results_df.groupby(\"model\")\n#     .agg(\n#         mean_r2=(\"val_r2\", \"mean\"),\n#         std_r2=(\"val_r2\", \"std\"),\n#         mean_loss=(\"val_loss\", \"mean\")\n#     )\n#     .sort_values(\"mean_r2\", ascending=False)\n#     .round(4)\n# )\n# summary\n\n\n\n\n\n\n# (\n#     p9.ggplot(results_df, p9.aes(x=\"model\", y=\"val_r2\", fill=\"model\"))\n#     + p9.geom_boxplot(alpha=0.7)\n#     + p9.coord_flip()\n#     + p9.labs(\n#         x=\"\", y=\"Out-of-Sample R²\",\n#         title=\"Multimodal Fusion Improves Return Prediction\"\n#     )\n#     + p9.theme_minimal()\n#     + p9.theme(figure_size=(10, 6), legend_position=\"none\")\n# )\n\n\nFigure 47.1",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html#handling-missing-modalities",
    "href": "74_multimodel.html#handling-missing-modalities",
    "title": "47  Multimodal Models in Finance",
    "section": "47.4 Handling Missing Modalities",
    "text": "47.4 Handling Missing Modalities\n\n47.4.1 The Missing Modality Problem\nIn practice, not every firm-quarter has every modality available. A firm may not have an earnings call transcript (no audio), its headquarters may be in a province where satellite coverage is intermittent (no image), or its annual report may not be publicly available in digital form (no text). This creates a missing modality problem that is structurally different from missing values in tabular data: an entire feature vector (hundreds or thousands of dimensions) is absent.\nThe fraction of observations with all four modalities available is typically much smaller than the fraction with at least one:\n\n\n\nTable 47.3: Modality Availability in Vietnamese Market Data\n\n\n\n\n\nAvailable Modalities\nTypical Coverage (Vietnamese Firms)\n\n\n\n\nTabular only\n\\(\\sim\\) 95% of firm-quarters\n\n\nTabular + Text\n\\(\\sim\\) 70%\n\n\nTabular + Text + Image\n\\(\\sim\\) 50%\n\n\nAll four (+ time series)\n\\(\\sim\\) 45%\n\n\n\n\n\n\nRestricting the sample to complete cases discards half the data and introduces selection bias (larger, more transparent firms are overrepresented). We need architectures that degrade gracefully when modalities are missing.\n\n\n47.4.2 Strategies for Missing Modalities\n\nZero imputation. Replace missing modality embeddings with zeros. Simple but introduces bias: the model cannot distinguish “this modality is absent” from “this modality has zero signal.”\nLearned default embedding. Replace missing modalities with a learnable “default” vector \\(\\mathbf{d}^{(m)}\\) that is trained alongside the model. This allows the model to learn what the absence of a modality implies.\nModality dropout. During training, randomly drop entire modalities with probability \\(p\\) (analogous to dropout on neurons). This forces the model to perform well even when modalities are missing, and acts as regularization.\nMixture of Experts (MoE). Route each observation to a fusion subnetwork specialized for its available modality combination. With \\(M\\) modalities, there are \\(2^M - 1\\) possible subsets, requiring efficient parameter sharing.\n\n\nclass ModalityDropout(nn.Module):\n    \"\"\"\n    Randomly drop entire modalities during training.\n    Forces robustness to missing inputs at test time.\n    \"\"\"\n\n    def __init__(self, drop_prob=0.2):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, modality_inputs):\n        if not self.training:\n            return modality_inputs\n\n        result = {}\n        for name, tensor in modality_inputs.items():\n            if tensor is not None and torch.rand(1).item() &gt; self.drop_prob:\n                result[name] = tensor\n            else:\n                result[name] = None\n\n        # Ensure at least one modality remains\n        if all(v is None for v in result.values()):\n            # Keep the first available modality\n            for name, tensor in modality_inputs.items():\n                if tensor is not None:\n                    result[name] = tensor\n                    break\n\n        return result\n\n\nclass RobustFusionModel(nn.Module):\n    \"\"\"\n    Multimodal model robust to missing modalities.\n    Uses learned default embeddings and modality dropout.\n    \"\"\"\n\n    def __init__(self, encoders, enc_dim=64, drop_prob=0.2):\n        super().__init__()\n        self.encoders = nn.ModuleDict(encoders)\n        self.modality_names = list(encoders.keys())\n        self.n_modalities = len(encoders)\n        self.enc_dim = enc_dim\n\n        # Learned default embeddings for missing modalities\n        self.defaults = nn.ParameterDict({\n            name: nn.Parameter(torch.randn(enc_dim) * 0.01)\n            for name in encoders\n        })\n\n        # Modality presence indicator embedding\n        self.presence_proj = nn.Linear(self.n_modalities, enc_dim)\n\n        # Modality dropout\n        self.mod_dropout = ModalityDropout(drop_prob)\n\n        # Attention-based aggregation\n        self.attn_pool = nn.Sequential(\n            nn.Linear(enc_dim, 1),\n            nn.Softmax(dim=0)\n        )\n\n        # Prediction head\n        self.head = nn.Sequential(\n            nn.Linear(enc_dim * 2, enc_dim),\n            nn.LayerNorm(enc_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(enc_dim, 1)\n        )\n\n    def forward(self, inputs):\n        # Apply modality dropout during training\n        inputs = self.mod_dropout(inputs)\n\n        embeddings = []\n        presence = []\n\n        for name in self.modality_names:\n            if name in inputs and inputs[name] is not None:\n                emb = self.encoders[name](inputs[name])\n                embeddings.append(emb)\n                presence.append(1.0)\n            else:\n                batch_size = next(\n                    v.shape[0] for v in inputs.values()\n                    if v is not None\n                )\n                emb = self.defaults[name].unsqueeze(0).expand(\n                    batch_size, -1\n                )\n                embeddings.append(emb)\n                presence.append(0.0)\n\n        # Stack: (n_modalities, batch, enc_dim)\n        emb_stack = torch.stack(embeddings, dim=0)\n\n        # Attention-weighted aggregation\n        attn_weights = self.attn_pool(emb_stack)  # (n_mod, batch, 1)\n        aggregated = (emb_stack * attn_weights).sum(dim=0)  # (batch, enc_dim)\n\n        # Presence indicator\n        device = aggregated.device\n        presence_tensor = torch.tensor(\n            presence, device=device\n        ).unsqueeze(0).expand(aggregated.shape[0], -1)\n        presence_emb = self.presence_proj(presence_tensor)\n\n        # Combine\n        combined = torch.cat([aggregated, presence_emb], dim=-1)\n        return self.head(combined).squeeze(-1)",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html#multimodal-document-understanding",
    "href": "74_multimodel.html#multimodal-document-understanding",
    "title": "47  Multimodal Models in Finance",
    "section": "47.5 Multimodal Document Understanding",
    "text": "47.5 Multimodal Document Understanding\n\n47.5.1 Annual Report as a Multimodal Object\nA Vietnamese annual report is inherently multimodal: it contains running text (management discussion, risk factors, strategy), tables (financial statements, segment data, shareholder structure), images (photographs of facilities, products, management), and charts (revenue trends, market share). Prior chapters treated these as separate extraction problems. Here we build a model that processes the entire report as a unified multimodal document.\nThe architecture follows the Document Understanding Transformer (Donut) approach of Kim et al. (2022), adapted for Vietnamese financial filings:\n\\[\n\\mathbf{h} = \\text{Encoder}(\\mathbf{I}_{\\text{page}}) + \\text{Encoder}(\\mathbf{T}_{\\text{ocr}}) + \\text{Encoder}(\\mathbf{L}_{\\text{layout}})\n\\tag{47.7}\\]\nwhere \\(\\mathbf{I}\\) is the page image, \\(\\mathbf{T}\\) is the OCR text, and \\(\\mathbf{L}\\) is the spatial layout (bounding boxes). The joint representation \\(\\mathbf{h}\\) captures both what is written and where it appears on the page.\n\nclass MultimodalDocumentEncoder(nn.Module):\n    \"\"\"\n    Joint encoder for Vietnamese annual report pages.\n    Processes text, layout, and page image simultaneously.\n    \"\"\"\n\n    def __init__(self, vocab_size=64000, max_boxes=512,\n                 img_dim=2048, hidden_dim=256, n_layers=4,\n                 n_heads=8):\n        super().__init__()\n\n        # Text embedding (Vietnamese tokens)\n        self.text_emb = nn.Embedding(vocab_size, hidden_dim)\n\n        # Layout embedding (bounding box coordinates)\n        # Each box: [x0, y0, x1, y1] normalized to [0, 1000]\n        self.x_emb = nn.Embedding(1001, hidden_dim // 4)\n        self.y_emb = nn.Embedding(1001, hidden_dim // 4)\n\n        # Image patch embedding\n        self.img_proj = nn.Sequential(\n            nn.Linear(img_dim, hidden_dim),\n            nn.LayerNorm(hidden_dim)\n        )\n\n        # Modality type embedding\n        self.modality_emb = nn.Embedding(3, hidden_dim)  # text, layout, image\n\n        # Transformer encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=n_heads,\n            dim_feedforward=hidden_dim * 4,\n            dropout=0.1,\n            activation=\"gelu\",\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(\n            encoder_layer, num_layers=n_layers\n        )\n\n        # [CLS] token\n        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))\n\n    def embed_layout(self, boxes):\n        \"\"\"Embed bounding box coordinates.\"\"\"\n        x0 = self.x_emb(boxes[:, :, 0])\n        y0 = self.y_emb(boxes[:, :, 1])\n        x1 = self.x_emb(boxes[:, :, 2])\n        y1 = self.y_emb(boxes[:, :, 3])\n        return torch.cat([x0, y0, x1, y1], dim=-1)\n\n    def forward(self, token_ids, boxes, img_features,\n                attention_mask=None):\n        \"\"\"\n        Parameters\n        ----------\n        token_ids : LongTensor (B, T)\n            OCR token IDs.\n        boxes : LongTensor (B, T, 4)\n            Bounding boxes for each token.\n        img_features : Tensor (B, P, img_dim)\n            Image patch features from CNN.\n        \"\"\"\n        batch_size = token_ids.shape[0]\n\n        # Text + layout\n        text_h = self.text_emb(token_ids) + self.embed_layout(boxes)\n        text_h = text_h + self.modality_emb(\n            torch.zeros(batch_size, text_h.shape[1],\n                       dtype=torch.long, device=text_h.device)\n        )\n\n        # Image patches\n        img_h = self.img_proj(img_features)\n        img_h = img_h + self.modality_emb(\n            torch.full((batch_size, img_h.shape[1]), 2,\n                      dtype=torch.long, device=img_h.device)\n        )\n\n        # Prepend [CLS]\n        cls = self.cls_token.expand(batch_size, -1, -1)\n\n        # Concatenate all modalities\n        sequence = torch.cat([cls, text_h, img_h], dim=1)\n\n        # Transformer encoding\n        output = self.transformer(sequence)\n\n        # Return [CLS] representation\n        return output[:, 0, :]\n\n\n\n47.5.2 Extracting Structured Financials from Multimodal Reports\nWith the document encoder, we can build extraction heads for specific financial fields. The key advantage over the OCR-only pipeline in previous chapter is that the multimodal encoder can resolve ambiguities using visual context (e.g., a number’s meaning depends on where it appears on the page and what headers and labels surround it).\n\nclass FinancialFieldExtractor(nn.Module):\n    \"\"\"\n    Extract specific financial fields from a document embedding.\n    Uses the multimodal document encoder as backbone.\n    \"\"\"\n\n    def __init__(self, doc_encoder, fields, hidden_dim=256):\n        \"\"\"\n        Parameters\n        ----------\n        doc_encoder : MultimodalDocumentEncoder\n        fields : list\n            Target field names, e.g.,\n            ['revenue', 'net_income', 'total_assets', 'total_equity']\n        \"\"\"\n        super().__init__()\n        self.doc_encoder = doc_encoder\n        self.fields = fields\n\n        # Per-field extraction heads\n        self.extractors = nn.ModuleDict({\n            field: nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim // 2),\n                nn.GELU(),\n                nn.Linear(hidden_dim // 2, 1)\n            )\n            for field in fields\n        })\n\n        # Confidence head\n        self.confidence = nn.ModuleDict({\n            field: nn.Sequential(\n                nn.Linear(hidden_dim, 1),\n                nn.Sigmoid()\n            )\n            for field in fields\n        })\n\n    def forward(self, token_ids, boxes, img_features):\n        doc_emb = self.doc_encoder(token_ids, boxes, img_features)\n\n        results = {}\n        for field in self.fields:\n            value = self.extractors[field](doc_emb).squeeze(-1)\n            conf = self.confidence[field](doc_emb).squeeze(-1)\n            results[field] = {\"value\": value, \"confidence\": conf}\n\n        return results",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html#multimodal-earnings-surprise-model",
    "href": "74_multimodel.html#multimodal-earnings-surprise-model",
    "title": "47  Multimodal Models in Finance",
    "section": "47.6 Multimodal Earnings Surprise Model",
    "text": "47.6 Multimodal Earnings Surprise Model\n\n47.6.1 Architecture\nWe now build the chapter’s central empirical application: a multimodal model that predicts earnings surprises using all available modalities observed before the earnings announcement date.\nThe information set at time \\(t^-\\) (just before the announcement) includes:\n\nTabular: Last reported financial ratios, analyst consensus forecasts\nText: News articles and filings in the pre-announcement window\nImage: Satellite features of the firm’s operating region\nTime series: Price and volume dynamics in the 60 trading days before announcement\n\nThe target is the standardized unexpected earnings (SUE):\n\\[\n\\text{SUE}_{i,q} = \\frac{E_{i,q} - \\hat{E}_{i,q}}{\\sigma_{i,q}}\n\\tag{47.8}\\]\nwhere \\(E_{i,q}\\) is actual earnings per share, \\(\\hat{E}_{i,q}\\) is the consensus forecast (or seasonal random walk forecast if analyst coverage is absent), and \\(\\sigma_{i,q}\\) is the standard deviation of forecast errors.\n\n# Construct earnings surprise dataset\nearnings = dc.get_earnings_announcements(\n    start_date=\"2016-01-01\",\n    end_date=\"2024-12-31\"\n)\n\n# Standardized Unexpected Earnings\nearnings[\"sue\"] = (\n    (earnings[\"actual_eps\"] - earnings[\"consensus_eps\"]) /\n    earnings[\"forecast_std\"].clip(lower=0.01)\n)\n\n# Pre-announcement features\n# Text: aggregate PhoBERT sentiment of news in [-30, -1] window\npre_ann_text = dc.get_pre_announcement_text_features(\n    start_date=\"2016-01-01\",\n    end_date=\"2024-12-31\",\n    window_days=30,\n    model=\"phobert\"\n)\n\n# Image: satellite features at quarter end\npre_ann_image = satellite_features.copy()\n\n# Time series: 60 trading days before announcement\n# (Pre-computed above)\n\n# Tabular: most recent quarterly financials\npre_ann_tabular = financials[tabular_features + [\"ticker\", \"quarter_date\"]]\n\nprint(f\"Earnings announcements: {len(earnings)}\")\nprint(f\"With text features: {len(pre_ann_text)}\")\n\n\nclass MultimodalEarningsSurpriseModel(nn.Module):\n    \"\"\"\n    Predict standardized unexpected earnings (SUE) from\n    multimodal pre-announcement information.\n    \"\"\"\n\n    def __init__(self, tab_dim, text_dim=768, img_dim=2048,\n                 ts_features=5, ts_len=60, hidden_dim=64,\n                 n_heads=4, drop_prob=0.2):\n        super().__init__()\n\n        # Unimodal encoders\n        self.tab_enc = TabularEncoder(tab_dim, 128, hidden_dim)\n        self.text_enc = TextEncoder(text_dim, hidden_dim)\n        self.img_enc = ImageEncoder(img_dim, hidden_dim)\n        self.ts_enc = TimeSeriesEncoder(ts_features, ts_len, hidden_dim)\n\n        # Modality dropout\n        self.mod_dropout = ModalityDropout(drop_prob)\n\n        # Cross-attention: text attends to time series\n        # (news context informs price dynamics interpretation)\n        self.text_ts_attn = CrossAttentionBlock(hidden_dim, n_heads)\n\n        # Cross-attention: tabular attends to image\n        # (financial ratios contextualized by physical activity)\n        self.tab_img_attn = CrossAttentionBlock(hidden_dim, n_heads)\n\n        # Modality importance weights (learned)\n        self.importance = nn.Parameter(torch.ones(4))\n\n        # Prediction head\n        self.head = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.GELU(),\n            nn.Linear(hidden_dim // 2, 1)\n        )\n\n    def forward(self, tabular, text, image, ts):\n        # Encode each modality\n        h_tab = self.tab_enc(tabular) if tabular is not None else None\n        h_txt = self.text_enc(text) if text is not None else None\n        h_img = self.img_enc(image) if image is not None else None\n        h_ts = self.ts_enc(ts) if ts is not None else None\n\n        # Cross-attention pairs (if both available)\n        if h_txt is not None and h_ts is not None:\n            h_txt_enriched, _ = self.text_ts_attn(h_txt, h_ts)\n        else:\n            h_txt_enriched = h_txt\n\n        if h_tab is not None and h_img is not None:\n            h_tab_enriched, _ = self.tab_img_attn(h_tab, h_img)\n        else:\n            h_tab_enriched = h_tab\n\n        # Weighted combination of available modalities\n        embeddings = []\n        weights = F.softmax(self.importance, dim=0)\n\n        for i, h in enumerate([h_tab_enriched, h_txt_enriched,\n                                h_img, h_ts]):\n            if h is not None:\n                embeddings.append(h * weights[i])\n            else:\n                device = next(self.parameters()).device\n                batch_size = next(\n                    x.shape[0] for x in [tabular, text, image, ts]\n                    if x is not None\n                )\n                embeddings.append(\n                    torch.zeros(batch_size, h_tab.shape[-1]\n                               if h_tab is not None else 64,\n                               device=device)\n                )\n\n        # Aggregate\n        stacked = torch.stack(embeddings, dim=0)\n        aggregated = stacked.sum(dim=0)\n\n        # Also compute variance across modalities (disagreement signal)\n        if stacked.shape[0] &gt; 1:\n            disagreement = stacked.var(dim=0)\n        else:\n            disagreement = torch.zeros_like(aggregated)\n\n        combined = torch.cat([aggregated, disagreement], dim=-1)\n        return self.head(combined).squeeze(-1)\n\n\n\n47.6.2 Modality Importance Analysis\nA key interpretability question is: which modality contributes most to earnings surprise prediction? We analyze the learned importance weights and conduct ablation experiments.\n\ndef ablation_study(model, test_loader, modality_names):\n    \"\"\"\n    Measure each modality's contribution via leave-one-out ablation.\n\n    For each modality m, zero out that modality's input and measure\n    the degradation in prediction accuracy.\n\n    Returns\n    -------\n    DataFrame : Modality, R² with all, R² without, Δ R².\n    \"\"\"\n    model.eval()\n\n    # Full model performance\n    all_preds, all_targets = [], []\n    with torch.no_grad():\n        for batch in test_loader:\n            inputs = {k: batch[k] for k in modality_names}\n            targets = batch[\"return\"]\n\n            output = model(inputs)\n            pred = output[0] if isinstance(output, tuple) else output\n            all_preds.extend(pred.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n\n    r2_full = r2_score(all_targets, all_preds)\n\n    # Ablation: remove one modality at a time\n    results = [{\"modality\": \"All\", \"r2\": r2_full, \"delta_r2\": 0.0}]\n\n    for drop_mod in modality_names:\n        ablated_preds = []\n        with torch.no_grad():\n            for batch in test_loader:\n                inputs = {}\n                for k in modality_names:\n                    if k == drop_mod:\n                        inputs[k] = None  # Remove this modality\n                    else:\n                        inputs[k] = batch[k]\n\n                targets = batch[\"return\"]\n                output = model(inputs)\n                pred = output[0] if isinstance(output, tuple) else output\n                ablated_preds.extend(pred.cpu().numpy())\n\n        r2_ablated = r2_score(all_targets, ablated_preds)\n        results.append({\n            \"modality\": f\"Without {drop_mod}\",\n            \"r2\": r2_ablated,\n            \"delta_r2\": r2_full - r2_ablated\n        })\n\n    return pd.DataFrame(results)\n\n\n\n\nTable 47.4: Modality Ablation Study: Contribution to Earnings Surprise Prediction\n\n\n# ablation_df = ablation_study(model, test_loader, modality_names)\n# ablation_df.round(4)\n\n\n\n\n\n\n# Track importance weights during training\n# importance_history = pd.DataFrame(...)\n\n# (\n#     p9.ggplot(importance_history, p9.aes(\n#         x=\"epoch\", y=\"weight\", color=\"modality\"\n#     ))\n#     + p9.geom_line(size=1)\n#     + p9.labs(\n#         x=\"Training Epoch\", y=\"Softmax Weight\",\n#         title=\"Modality Importance Convergence\",\n#         color=\"Modality\"\n#     )\n#     + p9.scale_color_manual(\n#         values=[\"#2E5090\", \"#C0392B\", \"#27AE60\", \"#8E44AD\"]\n#     )\n#     + p9.theme_minimal()\n#     + p9.theme(figure_size=(10, 5))\n# )\n\n\nFigure 47.2",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html#large-multimodal-models-for-financial-analysis",
    "href": "74_multimodel.html#large-multimodal-models-for-financial-analysis",
    "title": "47  Multimodal Models in Finance",
    "section": "47.7 Large Multimodal Models for Financial Analysis",
    "text": "47.7 Large Multimodal Models for Financial Analysis\n\n47.7.1 Prompting Vision-Language Models\nThe most powerful multimodal systems available today are large vision-language models (VLMs) such as GPT-4V, Gemini, and open-source alternatives (LLaVA, InternVL). These models can jointly process images and text through natural language prompts, enabling zero-shot financial analysis without model training.\nFor Vietnamese financial applications, VLMs can:\n\nInterpret satellite imagery of industrial zones and estimate activity levels\nRead and extract data from scanned financial tables\nAnalyze news photographs for sentiment\nCompare current and historical aerial views for change detection\n\n\ndef vlm_financial_qa(image_path, question, context=None):\n    \"\"\"\n    Financial question-answering using a vision-language model.\n\n    Parameters\n    ----------\n    image_path : str\n        Path to image (satellite tile, document page, news photo).\n    question : str\n        Financial analysis question.\n    context : str, optional\n        Additional textual context (e.g., firm name, sector).\n\n    Returns\n    -------\n    dict : Answer, confidence, extracted entities.\n    \"\"\"\n    from transformers import (\n        LlavaForConditionalGeneration,\n        LlavaProcessor\n    )\n\n    model_id = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n    processor = LlavaProcessor.from_pretrained(model_id)\n    model = LlavaForConditionalGeneration.from_pretrained(\n        model_id, torch_dtype=torch.float16, device_map=\"auto\"\n    )\n\n    img = Image.open(image_path).convert(\"RGB\")\n\n    # Build financial analysis prompt\n    system_prompt = (\n        \"You are a financial analyst examining visual evidence. \"\n        \"Provide specific, quantitative observations when possible. \"\n        \"State your confidence level (high/medium/low).\"\n    )\n\n    if context:\n        prompt = (\n            f\"{system_prompt}\\n\\nContext: {context}\\n\\n\"\n            f\"Question: {question}\\n\\nAnswer:\"\n        )\n    else:\n        prompt = f\"{system_prompt}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n\n    inputs = processor(\n        text=prompt,\n        images=img,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=300,\n            temperature=0.1,\n            do_sample=False\n        )\n\n    answer = processor.decode(output[0], skip_special_tokens=True)\n    answer = answer.split(\"Answer:\")[-1].strip()\n\n    return {\"answer\": answer, \"question\": question}\n\n\n# Example financial VLM queries\nFINANCIAL_VLM_PROMPTS = {\n    \"satellite_activity\": (\n        \"Examine this satellite image of an industrial zone. \"\n        \"Estimate the occupancy rate of factory buildings, \"\n        \"the density of vehicles in parking areas, \"\n        \"and whether the site appears to be operating at \"\n        \"full, partial, or minimal capacity.\"\n    ),\n    \"document_extraction\": (\n        \"This is a page from a Vietnamese annual report. \"\n        \"Extract the following if present: \"\n        \"total revenue (doanh thu), net income (lợi nhuận ròng), \"\n        \"total assets (tổng tài sản). \"\n        \"Report values in billions VND.\"\n    ),\n    \"construction_progress\": (\n        \"Compare this aerial image to a baseline. \"\n        \"Estimate the percentage completion of visible \"\n        \"construction projects. Note any new structures, \"\n        \"cleared land, or infrastructure changes.\"\n    )\n}\n\n\n\n47.7.2 Retrieval-Augmented Multimodal Analysis\nFor complex financial questions, we can combine VLM capabilities with retrieval from structured databases. The pipeline:\n\nQuery: Analyst asks “Is Vingroup’s construction activity in Vinhomes Grand Park accelerating?”\nRetrieve: Fetch satellite time series, financial statements, news articles\nProcess: VLM analyzes satellite images; NLP processes text; tabular model processes financials\nFuse: Aggregate evidence across modalities\nAnswer: Generate a structured response with confidence scores and supporting evidence\n\n\nclass MultimodalRAG:\n    \"\"\"\n    Retrieval-Augmented Generation with multimodal evidence.\n    \"\"\"\n\n    def __init__(self, datacore_client, vlm_model=None):\n        self.dc = datacore_client\n        self.vlm = vlm_model\n\n    def retrieve_evidence(self, ticker, date, modalities=None):\n        \"\"\"\n        Retrieve all available evidence for a firm at a given date.\n        \"\"\"\n        evidence = {}\n\n        if modalities is None or \"tabular\" in modalities:\n            evidence[\"tabular\"] = self.dc.get_firm_financials(\n                ticker=ticker,\n                end_date=date,\n                n_quarters=4\n            )\n\n        if modalities is None or \"text\" in modalities:\n            evidence[\"text\"] = self.dc.get_news(\n                ticker=ticker,\n                start_date=pd.to_datetime(date) - pd.Timedelta(days=30),\n                end_date=date,\n                limit=20\n            )\n\n        if modalities is None or \"image\" in modalities:\n            evidence[\"image\"] = self.dc.get_satellite_images(\n                ticker=ticker,\n                date=date,\n                lookback_months=6\n            )\n\n        if modalities is None or \"ts\" in modalities:\n            evidence[\"ts\"] = self.dc.get_daily_returns(\n                ticker=ticker,\n                start_date=pd.to_datetime(date) - pd.Timedelta(days=90),\n                end_date=date\n            )\n\n        return evidence\n\n    def analyze(self, ticker, date, question):\n        \"\"\"\n        Full multimodal analysis pipeline.\n        \"\"\"\n        evidence = self.retrieve_evidence(ticker, date)\n\n        analysis = {\n            \"ticker\": ticker,\n            \"date\": date,\n            \"question\": question,\n            \"evidence_available\": list(evidence.keys()),\n            \"modality_signals\": {}\n        }\n\n        # Tabular signal\n        if \"tabular\" in evidence and evidence[\"tabular\"] is not None:\n            latest = evidence[\"tabular\"].iloc[-1]\n            analysis[\"modality_signals\"][\"tabular\"] = {\n                \"revenue_growth\": latest.get(\"revenue_growth\", None),\n                \"roe\": latest.get(\"roe\", None),\n                \"leverage\": latest.get(\"leverage\", None)\n            }\n\n        # Text signal\n        if \"text\" in evidence and evidence[\"text\"] is not None:\n            # Aggregate sentiment from PhoBERT\n            texts = evidence[\"text\"]\n            if len(texts) &gt; 0:\n                avg_sentiment = texts[\"sentiment_score\"].mean()\n                analysis[\"modality_signals\"][\"text\"] = {\n                    \"avg_sentiment\": avg_sentiment,\n                    \"n_articles\": len(texts),\n                    \"sentiment_trend\": (\n                        \"improving\" if texts[\"sentiment_score\"].is_monotonic_increasing\n                        else \"deteriorating\" if texts[\"sentiment_score\"].is_monotonic_decreasing\n                        else \"mixed\"\n                    )\n                }\n\n        # Time series signal\n        if \"ts\" in evidence and evidence[\"ts\"] is not None:\n            ts = evidence[\"ts\"]\n            analysis[\"modality_signals\"][\"ts\"] = {\n                \"return_60d\": (1 + ts[\"ret\"]).prod() - 1,\n                \"volatility\": ts[\"ret\"].std() * np.sqrt(252),\n                \"avg_turnover\": ts[\"turnover\"].mean()\n            }\n\n        return analysis",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html#evaluation-and-deployment-considerations",
    "href": "74_multimodel.html#evaluation-and-deployment-considerations",
    "title": "47  Multimodal Models in Finance",
    "section": "47.8 Evaluation and Deployment Considerations",
    "text": "47.8 Evaluation and Deployment Considerations\n\n47.8.1 Evaluation Protocol for Multimodal Financial Models\nStandard machine learning evaluation (random train/test split) is inappropriate for financial prediction. We require time-series-aware evaluation that respects the temporal ordering of information.\n\n\n\nTable 47.5: Evaluation Best Practices for Multimodal Finance Models\n\n\n\n\n\n\n\n\n\n\nEvaluation Aspect\nCorrect Approach\nCommon Mistake\n\n\n\n\nTrain/test split\nExpanding or rolling time window\nRandom split (look-ahead bias)\n\n\nFeature timing\nFeatures available before prediction date\nUsing concurrent or future information\n\n\nMissing modalities\nTest with realistic missingness patterns\nComplete-case only\n\n\nPerformance metric\nOOS \\(R^2\\), IC, Sharpe of L-S portfolio\nIn-sample \\(R^2\\)\n\n\nStatistical inference\nDiebold and Mariano (2002) test for forecast comparison\nPoint estimates without SE\n\n\nEconomic significance\nTransaction-cost-adjusted portfolio returns\nIgnoring implementation costs\n\n\n\n\n\n\n\n\n47.8.2 Computational Budget\nMultimodal models are computationally expensive. Table 47.6 provides order-of-magnitude estimates for Vietnamese equity markets.\n\n\n\nTable 47.6: Computational Budget for Multimodal Pipeline\n\n\n\n\n\n\n\n\n\n\nComponent\nSingle Firm-Quarter\nFull Panel (1000 firms × 40 quarters)\n\n\n\n\nPhoBERT text encoding\n0.5s\n~5.5 hours\n\n\nResNet50 satellite feature\n0.1s\n~1.1 hours\n\n\nTime series encoding (CNN)\n0.01s\n~7 minutes\n\n\nTabular preprocessing\n&lt;0.01s\n~1 minute\n\n\nCross-attention fusion (forward)\n0.05s\n~33 minutes\n\n\nTraining (50 epochs)\n–\n~12 hours (GPU)\n\n\nFull pipeline\n–\n~1 day (single GPU)\n\n\n\n\n\n\nThe practical implication is that pre-computation of unimodal embeddings is essential. Extract and cache PhoBERT embeddings, CNN features, and time-series representations once; reuse them across all fusion experiments. Only the fusion layers need retraining when the architecture changes.",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "74_multimodel.html#summary",
    "href": "74_multimodel.html#summary",
    "title": "47  Multimodal Models in Finance",
    "section": "47.9 Summary",
    "text": "47.9 Summary\nThis chapter developed the multimodal learning framework for Vietnamese financial markets, progressing from foundational representation alignment through production-ready fusion architectures.\nThe key contributions are threefold. First, we demonstrated that financial data is inherently multimodal and that effective fusion requires explicit architectural choices (e.g., contrastive alignment, cross-attention mechanisms, and missing-modality handling) rather than naive concatenation. The FinancialCLIP alignment framework learns a shared embedding space where text, image, tabular, and time-series representations are geometrically comparable, enabling cross-modal retrieval and transfer.\nSecond, we built and compared five fusion architectures (early, late with gating, cross-attention, robust with modality dropout, and the custom earnings surprise model) on the prediction of forward returns and earnings surprises. The cross-attention architecture with modality dropout consistently outperforms unimodal baselines and simpler fusion strategies, though the margin varies across prediction horizons and firm characteristics.\nThird, we showed how large vision-language models can perform zero-shot financial analysis on Vietnamese documents and satellite imagery, offering a path to multimodal analysis without task-specific training. The retrieval-augmented multimodal pipeline combines the strengths of structured retrieval (from DataCore.vn) with the reasoning capabilities of VLMs.\nThe practical lesson for researchers working with Vietnamese financial data is that multimodal fusion is most valuable when modalities are complementary: text captures management intent and market narrative, images capture physical economic activity, tabular data provides precise quantitative snapshots, and time series captures market dynamics. When a single modality already captures most of the relevant signal (as tabular features do for many standard prediction tasks), the marginal gain from fusion is modest. When the prediction task requires information that no single modality captures well (as earnings surprises require both quantitative and qualitative assessment), multimodal models provide their largest advantage.\n\n\n\n\n\n\nAlayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” Advances in Neural Information Processing Systems 35: 23716–36.\n\n\nBaltrušaitis, Tadas, Chaitanya Ahuja, and Louis-Philippe Morency. 2018. “Multimodal Machine Learning: A Survey and Taxonomy.” IEEE Transactions on Pattern Analysis and Machine Intelligence 41 (2): 423–43.\n\n\nDiebold, Francis X, and Robert S Mariano. 2002. “Comparing Predictive Accuracy.” Journal of Business & Economic Statistics 20 (1): 134–44.\n\n\nHuang, Yu, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. 2021. “What Makes Multi-Modal Learning Better Than Single (Provably).” Advances in Neural Information Processing Systems 34: 10944–56.\n\n\nKim, Geewook, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2022. “Ocr-Free Document Understanding Transformer.” In European Conference on Computer Vision, 498–517. Springer.\n\n\nLiang, Paul Pu, Amir Zadeh, and Louis-Philippe Morency. 2024. “Foundations & Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions.” ACM Computing Surveys 56 (10): 1–42.\n\n\nOord, Aaron van den, Yazhe Li, and Oriol Vinyals. 2018. “Representation Learning with Contrastive Predictive Coding.” arXiv Preprint arXiv:1807.03748.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In International Conference on Machine Learning, 8748–63. PmLR.",
    "crumbs": [
      "Home",
      "Dữ liệu thay thế và Trí tuệ nhân tạo trong tài chính",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Multimodal Models in Finance</span>"
    ]
  },
  {
    "objectID": "99_conclusion.html",
    "href": "99_conclusion.html",
    "title": "48  Conclusion",
    "section": "",
    "text": "48.1 What you should take away\nEmpirical finance in emerging and frontier markets is often judged less by the elegance of an estimator than by the credibility of its inputs and the transparency of its decisions. Vietnam makes this point vividly: trading venues and regulatory regimes have evolved quickly, firm coverage can be uneven across time, corporate actions need careful treatment, and accounting conventions require close attention to timing and comparability. Those features do not prevent high-quality research; they simply shift the center of gravity toward reproducible data engineering, auditable transformations, and clear identification of assumptions.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "99_conclusion.html#what-you-should-take-away",
    "href": "99_conclusion.html#what-you-should-take-away",
    "title": "48  Conclusion",
    "section": "",
    "text": "48.1.1 Reproducibility is an identification strategy\nIn textbook settings, identification focuses on variation and exogeneity. In real-world market data, identification also depends on whether your dataset is the same dataset when you rerun the work next month or next year. The practical discipline of versioned inputs, deterministic transformations, and documented filters reduces the scope for accidental \\(p\\)-hacking and silent sample drift (e.g., survivorship bias from symbol changes or late-arriving delistings). Reproducible workflows are not administrative overhead; they are a commitment device that makes results more trustworthy and easier to challenge constructively (Peng 2011; Sandve et al. 2013).\n\n\n48.1.2 Vietnam rewards “microstructure humility”\nThe chapters on returns, beta estimation, and factor construction emphasized that naïve carryover of developed-market defaults can be costly. Thin trading, price limits, lot-size rules, and regime changes mean that decisions like (i) return interval, (ii) stale-price handling, (iii) corporate-action adjustment, and (iv) portfolio formation frequency can materially change inference. This is not a Vietnam-only phenomenon, but it is more visible there, and therefore a useful laboratory for best practices in emerging markets.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "99_conclusion.html#a-reproducibility-checklist-you-can-actually-use",
    "href": "99_conclusion.html#a-reproducibility-checklist-you-can-actually-use",
    "title": "48  Conclusion",
    "section": "48.2 A reproducibility checklist you can actually use",
    "text": "48.2 A reproducibility checklist you can actually use\nThe list below is designed to be operational: each item can be verified in a repository review.\n\n\n\nTable 48.1: Reproducibility deliverables for research\n\n\n\n\n\n\n\n\n\n\nDeliverable\nWhat “done” looks like\nWhere it lives\n\n\n\n\nDeterministic transforms\nSame raw inputs yield identical normalized outputs\nR/transform_*.R (or python/transform_*.py)\n\n\nTest suite\nCoverage, identity, and corporate-action tests run in CI\ntests/ + CI config\n\n\nData dictionary\nTables/fields documented with units, timing, and keys\ndocs/dictionary.qmd\n\n\nResearch log\nAll key design choices recorded (filters, winsorization, periods)\nnotes/research_log.md\n\n\nArtifact registry\nEvery figure/table has a script and a checksum\nartifacts/manifest.json",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abarbanell, Jeffery S, William N Lanen, and Robert E Verrecchia. 1995.\n“Analysts’ Forecasts as Proxies for Investor Beliefs in Empirical\nResearch.” Journal of Accounting and Economics 20 (1):\n31–60.\n\n\nAbel, Andrew B. 1983. “Optimal Investment Under\nUncertainty.” The American Economic Review 73 (1):\n228–33.\n\n\nAbel, Andrew B, and Janice C Eberly. 1994. “A Unified Model of\nInvestment Under Uncertainty.” American Economic Review\n84 (5).\n\n\nAbel, Andrew B, and Frederic S Mishkin. 1983. “An Integrated View\nof Tests of Rationality, Market Efficiency and the Short-Run Neutrality\nof Monetary Policy.” Journal of Monetary Economics 11\n(1): 3–24.\n\n\nAcemoglu, Daron, Asuman Ozdaglar, and Alireza Tahbaz-Salehi. 2015.\n“Systemic Risk and Stability in Financial Networks.”\nAmerican Economic Review 105 (2): 564–608.\n\n\nAcharya, Viral V, and Lasse Heje Pedersen. 2005. “Asset Pricing\nwith Liquidity Risk.” Journal of Financial Economics 77\n(2): 375–410.\n\n\nAggarwal, Reena, Isil Erel, Miguel Ferreira, and Pedro Matos. 2011.\n“Does Governance Travel Around the World? Evidence from\nInstitutional Investors.” Journal of Financial Economics\n100 (1): 154–81.\n\n\nAharony, Joseph, and Itzhak Swary. 1980. “Quarterly Dividend and\nEarnings Announcements and Stockholders’ Returns: An Empirical\nAnalysis.” The Journal of Finance 35 (1): 1–12.\n\n\nAlayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain\nBarr, Yana Hasson, Karel Lenc, et al. 2022. “Flamingo: A Visual\nLanguage Model for Few-Shot Learning.” Advances in Neural\nInformation Processing Systems 35: 23716–36.\n\n\nAlexander, Gordon J, Gjergji Cici, and Scott Gibson. 2007. “Does\nMotivation Matter When Assessing Trade Performance? An Analysis of\nMutual Funds.” The Review of Financial Studies 20 (1):\n125–50.\n\n\nAlexandridis, George, Antonios Antoniou, and Dimitris Petmezas. 2007.\n“Divergence of Opinion and Post-Acquisition Performance.”\nJournal of Business Finance & Accounting 34 (3-4): 439–60.\n\n\nAllen, Franklin, and Douglas Gale. 2000. “Financial\nContagion.” Journal of Political Economy 108 (1): 1–33.\n\n\nAlmgren, Robert, and Neil Chriss. 2001. “Optimal Execution of\nPortfolio Transactions.” Journal of Risk 3: 5–40.\n\n\nAltman, Edward I. 1968. “Financial Ratios, Discriminant Analysis\nand the Prediction of Corporate Bankruptcy.” The Journal of\nFinance 23 (4): 589–609.\n\n\nAltman, Edward I, and Edith Hotchkiss. 2010. Corporate Financial\nDistress and Bankruptcy: Predict and Avoid Bankruptcy, Analyze and\nInvest in Distressed Debt. Vol. 289. John Wiley & Sons.\n\n\nAmihud, Yakov. 2002. “Illiquidity and Stock Returns: Cross-Section\nand Time-Series Effects.” Journal of Financial Markets 5\n(1): 31–56.\n\n\nAmihud, Yakov, and Haim Mendelson. 1986. “Asset Pricing and the\nBid-Ask Spread.” Journal of Financial Economics 17 (2):\n223–49.\n\n\nAnderson, Anne-Marie, and Edward A Dyl. 2005. “Market Structure\nand Trading Volume.” Journal of Financial Research 28\n(1): 115–31.\n\n\nAnderson, Kirsten L, Jeffrey H Harris, and Eric C So. 2007.\n“Opinion Divergence and Post-Earnings Announcement Drift.”\nAvailable at SSRN 969736.\n\n\nAndrade, Gregor, Mark Mitchell, and Erik Stafford. 2001. “New\nEvidence and Perspectives on Mergers.” Journal of Economic\nPerspectives 15 (2): 103–20.\n\n\nAng, Andrew, and Joseph Chen. 2002. “Asymmetric Correlations of\nEquity Portfolios.” Journal of Financial Economics 63\n(3): 443–94.\n\n\nAng, Andrew, Robert J Hodrick, Yuhang Xing, and Xiaoyan Zhang. 2006.\n“The Cross-Section of Volatility and Expected Returns.”\nThe Journal of Finance 61 (1): 259–99.\n\n\nArnott, Robert D, Jason Hsu, and Philip Moore. 2005. “Fundamental\nIndexation.” Financial Analysts Journal 61 (2): 83–99.\n\n\nArtzner, Philippe, Freddy Delbaen, Jean-Marc Eber, and David Heath.\n1999. “Coherent Measures of Risk.” Mathematical\nFinance 9 (3): 203–28.\n\n\nAtiase, Rowland Kwame. 1985. “Predisclosure Information, Firm\nCapitalization, and Security Price Behavior Around Earnings\nAnnouncements.” Journal of Accounting Research, 21–36.\n\n\nBaek, Jeonghun, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han,\nSangdoo Yun, Seong Joon Oh, and Hwalsuk Lee. 2019. “What Is Wrong\nwith Scene Text Recognition Model Comparisons? Dataset and Model\nAnalysis.” In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 4715–23.\n\n\nBaker, Malcolm, Jeremy C Stein, and Jeffrey Wurgler. 2003. “When\nDoes the Market Matter? Stock Prices and the Investment of\nEquity-Dependent Firms.” The Quarterly Journal of\nEconomics 118 (3): 969–1005.\n\n\nBaker, Malcolm, and Jeffrey Wurgler. 2002. “Market Timing and\nCapital Structure.” The Journal of Finance 57 (1): 1–32.\n\n\nBali, Turan G, Robert F Engle, and Scott Murray. 2016b. Empirical\nAsset Pricing: The Cross Section of Stock Returns. John Wiley &\nSons.\n\n\n———. 2016a. Empirical asset pricing: The cross\nsection of stock returns. John Wiley & Sons. https://doi.org/10.1002/9781118445112.stat07954.\n\n\nBall, Ray, and Philip Brown. 2013. “An Empirical Evaluation of\nAccounting Income Numbers.” In Financial Accounting and\nEquity Markets, 27–46. Routledge.\n\n\nBall, Ray, Ashok Robin, and Joanna Shuang Wu. 2003. “Incentives\nVersus Standards: Properties of Accounting Income in Four East Asian\nCountries.” Journal of Accounting and Economics 36\n(1-3): 235–70.\n\n\nBall, Ray, and Lakshmanan Shivakumar. 2008. “How Much New\nInformation Is There in Earnings?” Journal of Accounting\nResearch 46 (5): 975–1016.\n\n\nBaltrušaitis, Tadas, Chaitanya Ahuja, and Louis-Philippe Morency. 2018.\n“Multimodal Machine Learning: A Survey and Taxonomy.”\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n41 (2): 423–43.\n\n\nBamber, Linda Smith, Theodore E Christensen, and Kenneth M Gaver. 2000.\n“Do We Really ‘Know’what We Think We Know? A Case Study of Seminal\nResearch and Its Subsequent Overgeneralization.” Accounting,\nOrganizations and Society 25 (2): 103–29.\n\n\nBanerjee, Abhijit V. 1992. “A Simple Model of Herd\nBehavior.” The Quarterly Journal of Economics 107 (3):\n797–817.\n\n\nBao Dinh, Ngoc, and Van Nguyen Hong Tran. 2024. “Institutional\nOwnership and Stock Liquidity: Evidence from an Emerging Market.”\nSAGE Open 14 (1): 21582440241239116.\n\n\nBarber, Brad M, Yi-Tsung Lee, Yu-Jane Liu, and Terrance Odean. 2009.\n“Just How Much Do Individual Investors Lose by Trading?”\nThe Review of Financial Studies 22 (2): 609–32.\n\n\nBarber, Brad M, and John D Lyon. 1997. “Detecting Long-Run\nAbnormal Stock Returns: The Empirical Power and Specification of Test\nStatistics.” Journal of Financial Economics 43 (3):\n341–72.\n\n\nBarberis, Nicholas, Andrei Shleifer, and Robert Vishny. 1998. “A\nModel of Investor Sentiment.” Journal of Financial\nEconomics 49 (3): 307–43.\n\n\nBarillas, Francisco, and Jay Shanken. 2018. “Comparing Asset\nPricing Models.” The Journal of Finance 73 (2): 715–54.\n\n\nBarth, Mary E, Wayne R Landsman, and Mark H Lang. 2008.\n“International Accounting Standards and Accounting\nQuality.” Journal of Accounting Research 46 (3): 467–98.\n\n\nBeaver, William H. 1968. “The Information Content of Annual\nEarnings Announcements.” Journal of Accounting Research,\n67–92.\n\n\nBebchuk, Lucian A. 1999. “A Rent-Protection Theory of Corporate\nOwnership and Control.” National Bureau of Economic Research\nCambridge, Mass., USA.\n\n\nBebchuk, Lucian A, Alma Cohen, and Charles CY Wang. 2013.\n“Learning and the Disappearing Association Between Governance and\nReturns.” Journal of Financial Economics 108 (2):\n323–48.\n\n\nBebchuk, Lucian, Alma Cohen, and Allen Ferrell. 2009. “What\nMatters in Corporate Governance?” The Review of Financial\nStudies 22 (2): 783–827.\n\n\nBekaert, Geert, and Campbell R Harvey. 1995. “Time-Varying World\nMarket Integration.” The Journal of Finance 50 (2):\n403–44.\n\n\n———. 2002. “Research in Emerging Markets Finance: Looking to the\nFuture.” Emerging Markets Review 3 (4): 429–48.\n\n\nBekaert, Geert, Campbell R Harvey, and Christian Lundblad. 2005.\n“Does Financial Liberalization Spur Growth?” Journal of\nFinancial Economics 77 (1): 3–55.\n\n\nBen-David, ITZHAK, Francesco Franzoni, Augustin Landier, and Rabih\nMoussawi. 2013. “Do Hedge Funds Manipulate Stock Prices?”\nThe Journal of Finance 68 (6): 2383–2434.\n\n\nBen-David, Itzhak, Francesco Franzoni, and Rabih Moussawi. 2012.\n“Hedge Fund Stock Trading in the Financial Crisis of\n2007–2009.” The Review of Financial Studies 25 (1):\n1–54.\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False\nDiscovery Rate: A Practical and Powerful Approach to Multiple\nTesting.” Journal of the Royal Statistical Society: Series B\n(Methodological) 57 (1): 289–300.\n\n\nBenveniste, Lawrence M, and Paul A Spindt. 1989. “How Investment\nBankers Determine the Offer Price and Allocation of New Issues.”\nJournal of Financial Economics 24 (2): 343–61.\n\n\nBerkman, Henk, Valentin Dimitrov, Prem C Jain, Paul D Koch, and Sheri\nTice. 2009. “Sell on the News: Differences of Opinion, Short-Sales\nConstraints, and Returns Around Earnings Announcements.”\nJournal of Financial Economics 92 (3): 376–99.\n\n\nBernard, Victor L, and Jacob K Thomas. 1989.\n“Post-Earnings-Announcement Drift: Delayed Price Response or Risk\nPremium?” Journal of Accounting Research 27: 1–36.\n\n\nBerry, Steven, James Levinsohn, and Ariel Pakes. 1995. “Automobile\nPrices in Market Equilibrium.” Econometrica 63 (4):\n841–90.\n\n\nBessembinder, Hendrik. 2003. “Trade Execution Costs and Market\nQuality After Decimalization.” Journal of Financial and\nQuantitative Analysis 38 (4): 747–77.\n\n\nBeyer, Anne, Daniel A Cohen, Thomas Z Lys, and Beverly R Walther. 2010.\n“The Financial Reporting Environment: Review of the Recent\nLiterature.” Journal of Accounting and Economics 50\n(2-3): 296–343.\n\n\nBhattacharya, Sudipto. 1979. “Imperfect Information, Dividend\nPolicy, and\" the Bird in the Hand\" Fallacy.” The Bell Journal\nof Economics, 259–70.\n\n\nBhattacharya, Utpal, Hazem Daouk, Brian Jorgenson, and Carl-Heinrich\nKehr. 2000. “When an Event Is Not an Event: The Curious Case of an\nEmerging Market.” Journal of Financial Economics 55 (1):\n69–101.\n\n\nBiddle, Gary C, Gilles Hilary, and Rodrigo S Verdi. 2009. “How\nDoes Financial Reporting Quality Relate to Investment\nEfficiency?” Journal of Accounting and Economics 48\n(2-3): 112–31.\n\n\nBikhchandani, Sushil, David Hirshleifer, and Ivo Welch. 1992. “A\nTheory of Fads, Fashion, Custom, and Cultural Change as Informational\nCascades.” Journal of Political Economy 100 (5):\n992–1026.\n\n\nBinder, John. 1998. “The Event Study Methodology Since\n1969.” Review of Quantitative Finance and Accounting 11\n(2): 111–37.\n\n\nBizjak, John M, Michael L Lemmon, and Lalitha Naveen. 2008. “Does\nthe Use of Peer Groups Contribute to Higher Pay and Less Efficient\nCompensation?” Journal of Financial Economics 90 (2):\n152–68.\n\n\nBlei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent\nDirichlet Allocation.” Journal of Machine Learning\nResearch 3 (Jan): 993–1022.\n\n\nBoehme, Rodney D, Bartley R Danielsen, and Sorin M Sorescu. 2006.\n“Short-Sale Constraints, Differences of Opinion, and\nOvervaluation.” Journal of Financial and Quantitative\nAnalysis 41 (2): 455–87.\n\n\nBoehmer, Ekkehart, Jim Masumeci, and Annette B Poulsen. 1991.\n“Event-Study Methodology Under Conditions of Event-Induced\nVariance.” Journal of Financial Economics 30 (2):\n253–72.\n\n\nBollen, Nicolas PB, Tom Smith, and Robert E Whaley. 2004.\n“Modeling the Bid/Ask Spread: Measuring the Inventory-Holding\nPremium.” Journal of Financial Economics 72 (1): 97–141.\n\n\nBollerslev, Tim. 1986. “Generalized Autoregressive Conditional\nHeteroskedasticity.” Journal of Econometrics 31 (3):\n307–27.\n\n\nBond, Philip, Alex Edmans, and Itay Goldstein. 2012. “The Real\nEffects of Financial Markets.” Annu. Rev. Financ. Econ.\n4 (1): 339–60.\n\n\nBond, Stephen, Julie Ann Elston, Jacques Mairesse, and Benoı̂t Mulkay.\n2003. “Financial Factors and Investment in Belgium, France,\nGermany, and the United Kingdom: A Comparison Using Company Panel\nData.” Review of Economics and Statistics 85 (1):\n153–65.\n\n\nBonferroni, Carlo. 1936. “Teoria Statistica Delle Classi e Calcolo\nDelle Probabilita.” Pubblicazioni Del R Istituto Superiore Di\nScienze Economiche e Commericiali Di Firenze 8: 3–62.\n\n\nBonsall IV, Samuel B, Andrew J Leone, Brian P Miller, and Kristina\nRennekamp. 2017. “A Plain English Measure of Financial Reporting\nReadability.” Journal of Accounting and Economics 63\n(2-3): 329–57.\n\n\nBotosan, Christine A. 1997. “Disclosure Level and the Cost of\nEquity Capital.” Accounting Review, 323–49.\n\n\nBotosan, Christine A, and Marlene A Plumlee. 2002. “A\nRe-Examination of Disclosure Level and the Expected Cost of Equity\nCapital.” Journal of Accounting Research 40 (1): 21–40.\n\n\nBrennan, Michael J. 1986. “A Theory of Price Limits in Futures\nMarkets.” Journal of Financial Economics 16 (2): 213–33.\n\n\nBrown, Stephen J, and Jerold B Warner. 1980. “Measuring Security\nPrice Performance.” Journal of Financial Economics 8\n(3): 205–58.\n\n\n———. 1985. “Using Daily Stock Returns: The Case of Event\nStudies.” Journal of Financial Economics 14 (1): 3–31.\n\n\nBrunnermeier, Markus K. 2009. “Deciphering the Liquidity and\nCredit Crunch 2007–2008.” Journal of Economic\nPerspectives 23 (1): 77–100.\n\n\nBrunnermeier, Markus K, Stefan Nagel, and Lasse H Pedersen. 2008.\n“Carry Trades and Currency Crashes.” NBER\nMacroeconomics Annual 23 (1): 313–48.\n\n\nBrunnermeier, Markus K, and Lasse Heje Pedersen. 2009. “Market\nLiquidity and Funding Liquidity.” The Review of Financial\nStudies 22 (6): 2201–38.\n\n\nBurgstahler, David, and Ilia Dichev. 1997. “Earnings Management to\nAvoid Earnings Decreases and Losses.” Journal of Accounting\nand Economics 24 (1): 99–126.\n\n\nBushman, Robert, Qi Chen, Ellen Engel, and Abbie Smith. 2004.\n“Financial Accounting Information, Organizational Complexity and\nCorporate Governance Systems.” Journal of Accounting and\nEconomics 37 (2): 167–201.\n\n\nBybee, Leland, Bryan Kelly, and Yinan Su. 2023. “Narrative Asset\nPricing: Interpretable Systematic Risk Factors from News Text.”\nThe Review of Financial Studies 36 (12): 4759–87.\n\n\nCai, Ye, Dan S Dhaliwal, Yongtae Kim, and Carrie Pan. 2014. “Board\nInterlocks and the Diffusion of Disclosure Policy.” Review of\nAccounting Studies 19 (3): 1086–1119.\n\n\nCampbell, John Y, Andrew W Lo, A Craig MacKinlay, and Robert F Whitelaw.\n1998. “The Econometrics of Financial Markets.”\nMacroeconomic Dynamics 2 (4): 559–62.\n\n\nCampbell, John Y, Karine Serfaty-De Medeiros, and Luis M Viceira. 2010.\n“Global Currency Hedging.” The Journal of Finance\n65 (1): 87–121.\n\n\nCarhart, Mark M. 1997a. “On Persistence in Mutual Fund\nPerformance.” The Journal of Finance 52 (1): 57–82.\n\n\nCarhart, Mark M. 1997b. “On persistence in\nmutual fund performance.” The Journal of\nFinance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nChambers, Anne E, and Stephen H Penman. 1984. “Timeliness of\nReporting and the Stock Price Reaction to Earnings\nAnnouncements.” Journal of Accounting Research, 21–47.\n\n\nChan, Kalok, Allaudeen Hameed, and Wilson Tong. 2000.\n“Profitability of Momentum Strategies in the International Equity\nMarkets.” Journal of Financial and Quantitative\nAnalysis, 153–72.\n\n\nChatterjee, Sris, Kose John, and An Yan. 2012. “Takeovers and\nDivergence of Investor Opinion.” The Review of Financial\nStudies 25 (1): 227–77.\n\n\nChen, Andrew, and Tom Zimmermann. 2022. “Open Source\nCross-Sectional Asset Pricing.” &gt; Critical Finance\nReview 11 (02): 207–64.\n\n\nChen, Hsiu-Lang, Narasimhan Jegadeesh, and Russ Wermers. 2000.\n“The Value of Active Mutual Fund Management: An Examination of the\nStockholdings and Trades of Fund Managers.” Journal of\nFinancial and Quantitative Analysis 35 (3): 343–68.\n\n\nChen, Joseph, Harrison Hong, and Jeremy C Stein. 2001.\n“Forecasting Crashes: Trading Volume, Past Returns, and\nConditional Skewness in Stock Prices.” Journal of Financial\nEconomics 61 (3): 345–81.\n\n\n———. 2002. “Breadth of Ownership and Stock Returns.”\nJournal of Financial Economics 66 (2-3): 171–205.\n\n\nCheong, Foong Soon, and Jacob Thomas. 2011. “Why Do EPS Forecast\nError and Dispersion Not Vary with Scale? Implications for Analyst and\nManagerial Behavior.” Journal of Accounting Research 49\n(2): 359–401.\n\n\nCho, David D, Jeffrey Russell, George C Tiao, and Ruey Tsay. 2003.\n“The Magnet Effect of Price Limits: Evidence from High-Frequency\nData on Taiwan Stock Exchange.” Journal of Empirical\nFinance 10 (1-2): 133–68.\n\n\nChoe, Hyuk, Bong-Chan Kho, and René M Stulz. 2005. “Do Domestic\nInvestors Have an Edge? The Trading Experience of Foreign Investors in\nKorea.” The Review of Financial Studies 18 (3): 795–829.\n\n\nChordia, Tarun, Richard Roll, and Avanidhar Subrahmanyam. 2000.\n“Commonality in Liquidity.” Journal of Financial\nEconomics 56 (1): 3–28.\n\n\n———. 2001. “Market Liquidity and Trading Activity.” The\nJournal of Finance 56 (2): 501–30.\n\n\nChoueifaty, Yves. 2008. “Towards Maximum Diversification.”\nAvailable at SSRN 4063676.\n\n\nChristoffersen, Peter F. 1998. “Evaluating Interval\nForecasts.” International Economic Review, 841–62.\n\n\nChu, Xiaojun, and Jianying Qiu. 2019. “Forecasting Volatility with\nPrice Limit Hits—Evidence from Chinese Stock Market.”\nEmerging Markets Finance and Trade 55 (5): 1034–50.\n\n\nChui, Andy CW, Sheridan Titman, and KC John Wei. 2010.\n“Individualism and Momentum Around the World.” The\nJournal of Finance 65 (1): 361–92.\n\n\nChung, Kee H, and Stephen W Pruitt. 1994. “A Simple Approximation\nof Tobin’s q.” Financial Management, 70–74.\n\n\nChung, Kee H, and Hao Zhang. 2014. “A Simple Approximation of\nIntraday Spreads Using Daily Data.” Journal of Financial\nMarkets 17: 94–120.\n\n\nClaessens, Stijn, Simeon Djankov, Joseph PH Fan, and Larry HP Lang.\n2002. “Disentangling the Incentive and Entrenchment Effects of\nLarge Shareholdings.” The Journal of Finance 57 (6):\n2741–71.\n\n\nClaessens, Stijn, Simeon Djankov, and Larry HP Lang. 2000. “The\nSeparation of Ownership and Control in East Asian Corporations.”\nJournal of Financial Economics 58 (1-2): 81–112.\n\n\nClarke, Roger, Harindra De Silva, and Steven Thorley. 2011.\n“Minimum-Variance Portfolio Composition.” Journal of\nPortfolio Management 37 (2): 31.\n\n\nCoad, Alex, Jacob Rubæk Holm, Jackie Krafft, and Francesco Quatraro.\n2018. “Firm Age and Performance.” Journal of\nEvolutionary Economics 28 (1): 1–11.\n\n\nCochrane, John H. 1991. “Production-Based Asset Pricing and the\nLink Between Stock Returns and Economic Fluctuations.” The\nJournal of Finance 46 (1): 209–37.\n\n\n———. 2011. “Presidential Address: Discount Rates.” The\nJournal of Finance 66 (4): 1047–1108.\n\n\nCohen, Daniel A, Aiyesha Dey, and Thomas Z Lys. 2008. “Real and\nAccrual-Based Earnings Management in the Pre-and Post-Sarbanes-Oxley\nPeriods.” The Accounting Review 83 (3): 757–87.\n\n\nCohen, Lauren, and Andrea Frazzini. 2008. “Economic Links and\nPredictable Returns.” The Journal of Finance 63 (4):\n1977–2011.\n\n\nComerton-Forde, Carole, and Kar Mei Tang. 2009. “Anonymity,\nLiquidity and Fragmentation.” Journal of Financial\nMarkets 12 (3): 337–67.\n\n\nCont, Rama. 2001. “Empirical Properties of Asset Returns: Stylized\nFacts and Statistical Issues.” Quantitative Finance 1\n(2): 223.\n\n\nCooper, Michael J, Huseyin Gulen, and Michael J Schill. 2008.\n“Asset Growth and the Cross-Section of Stock Returns.”\nThe Journal of Finance 63 (4): 1609–51.\n\n\nCooper, Michael J, Roberto C Gutierrez Jr, and Allaudeen Hameed. 2004.\n“Market States and Momentum.” The Journal of\nFinance 59 (3): 1345–65.\n\n\nCorrado, Charles J. 1989. “A Nonparametric Test for Abnormal\nSecurity-Price Performance in Event Studies.” Journal of\nFinancial Economics 23 (2): 385–95.\n\n\nCorwin, Shane A, and Paul Schultz. 2012. “A Simple Way to Estimate\nBid-Ask Spreads from Daily High and Low Prices.” The Journal\nof Finance 67 (2): 719–60.\n\n\nCoval, Joshua, and Erik Stafford. 2007. “Asset Fire Sales (and\nPurchases) in Equity Markets.” Journal of Financial\nEconomics 86 (2): 479–512.\n\n\nCowan, Arnold Richard. 1992. “Nonparametric Event Study\nTests.” Review of Quantitative Finance and Accounting 2\n(4): 343–58.\n\n\nCremers, KJ Martijn, and Vinay B Nair. 2005. “Governance\nMechanisms and Equity Prices.” The Journal of Finance 60\n(6): 2859–94.\n\n\nDaniel, Kent, David Hirshleifer, and Avanidhar Subrahmanyam. 1998.\n“Investor Psychology and Security Market Under-and\nOverreactions.” The Journal of Finance 53 (6): 1839–85.\n\n\nDaniel, Kent, and Tobias J Moskowitz. 2016. “Momentum\nCrashes.” Journal of Financial Economics 122 (2):\n221–47.\n\n\nDaniel, Kent, and Sheridan Titman. 1997. “Evidence on the\nCharacteristics of Cross Sectional Variation in Stock Returns.”\nThe Journal of Finance 52 (1): 1–33.\n\n\nDatar, Vinay T, Narayan Y Naik, and Robert Radcliffe. 1998.\n“Liquidity and Stock Returns: An Alternative Test.”\nJournal of Financial Markets 1 (2): 203–19.\n\n\nDeAngelo, Linda Elizabeth. 1986. “Accounting Numbers as Market\nValuation Substitutes: A Study of Management Buyouts of Public\nStockholders.” Accounting Review, 400–420.\n\n\nDechow, Patricia M, Richard G Sloan, and Amy P Sweeney. 1995.\n“Detecting Earnings Management.” Accounting\nReview, 193–225.\n\n\nDechow, Patricia, Weili Ge, and Catherine Schrand. 2010.\n“Understanding Earnings Quality: A Review of the Proxies, Their\nDeterminants and Their Consequences.” Journal of Accounting\nand Economics 50 (2-3): 344–401.\n\n\nDellaVigna, Stefano, and Joshua M Pollet. 2009. “Investor\nInattention and Friday Earnings Announcements.” The Journal\nof Finance 64 (2): 709–49.\n\n\nDemarta, Stefano, and Alexander J McNeil. 2005. “The t Copula and\nRelated Copulas.” International Statistical Review 73\n(1): 111–29.\n\n\nDeMiguel, Victor, Lorenzo Garlappi, and Raman Uppal. 2009.\n“Optimal Versus Naive Diversification: How Inefficient Is the 1/n\nPortfolio Strategy?” The Review of Financial Studies 22\n(5): 1915–53.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“Bert: Pre-Training of Deep Bidirectional Transformers for\nLanguage Understanding.” In Proceedings of the 2019\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 4171–86.\n\n\nDiamond, Douglas W. 1985. “Optimal Release of Information by\nFirms.” The Journal of Finance 40 (4): 1071–94.\n\n\nDiamond, Douglas W, and Robert E Verrecchia. 1991. “Disclosure,\nLiquidity, and the Cost of Capital.” The Journal of\nFinance 46 (4): 1325–59.\n\n\nDiebold, Francis X, and Robert S Mariano. 2002. “Comparing\nPredictive Accuracy.” Journal of Business & Economic\nStatistics 20 (1): 134–44.\n\n\nDiebold, Francis X, and Kamil Yılmaz. 2014. “On the Network\nTopology of Variance Decompositions: Measuring the Connectedness of\nFinancial Firms.” Journal of Econometrics 182 (1):\n119–34.\n\n\nDiether, Karl B, Christopher J Malloy, and Anna Scherbina. 2002.\n“Differences of Opinion and the Cross Section of Stock\nReturns.” The Journal of Finance 57 (5): 2113–41.\n\n\nDimson, Elroy. 1979. “Risk Measurement When Shares Are Subject to\nInfrequent Trading.” Journal of Financial Economics 7\n(2): 197–226.\n\n\nDonaldson, Dave. 2018. “Railroads of the Raj: Estimating the\nImpact of Transportation Infrastructure.” American Economic\nReview 108 (4-5): 899–934.\n\n\nDornbusch, Rudiger. 1976. “Expectations and Exchange Rate\nDynamics.” Journal of Political Economy 84 (6): 1161–76.\n\n\nDosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.\n2020. “An Image Is Worth 16x16 Words: Transformers for Image\nRecognition at Scale.” arXiv Preprint arXiv:2010.11929.\n\n\nDoukas, John A, Chansog Francis Kim, and Christos Pantzalis. 2006.\n“Divergence of Opinion and Equity Returns.” Journal of\nFinancial and Quantitative Analysis 41 (3): 573–606.\n\n\nDoukas, John A, Chansog Kim, and Christos Pantzalis. 2004.\n“Divergent Opinions and the Performance of Value Stocks.”\nFinancial Analysts Journal 60 (6): 55–64.\n\n\nDu, Wenxin, Alexander Tepper, and Adrien Verdelhan. 2018.\n“Deviations from Covered Interest Rate Parity.” The\nJournal of Finance 73 (3): 915–57.\n\n\nDumas, Bernard, and Bruno Solnik. 1995. “The World Price of\nForeign Exchange Risk.” The Journal of Finance 50 (2):\n445–79.\n\n\nDyer, Travis, Mark Lang, and Lorien Stice-Lawrence. 2017. “The\nEvolution of 10-k Textual Disclosure: Evidence from Latent Dirichlet\nAllocation.” Journal of Accounting and Economics 64\n(2-3): 221–45.\n\n\nEasley, David, Soeren Hvidkjaer, and Maureen O’hara. 2002. “Is\nInformation Risk a Determinant of Asset Returns?” The Journal\nof Finance 57 (5): 2185–2221.\n\n\nEasley, David, Nicholas M Kiefer, Maureen O’hara, and Joseph B Paperman.\n1996. “Liquidity, Information, and Infrequently Traded\nStocks.” The Journal of Finance 51 (4): 1405–36.\n\n\nEisenberg, Larry, and Thomas H Noe. 2001. “Systemic Risk in\nFinancial Systems.” Management Science 47 (2): 236–49.\n\n\nElton, Edwin J, Martin J Gruber, and Christopher R Blake. 2011.\n“Holdings Data, Security Returns, and the Selection of Superior\nMutual Funds.” Journal of Financial and Quantitative\nAnalysis 46 (2): 341–67.\n\n\nEngle, Robert. 2002. “Dynamic Conditional Correlation: A Simple\nClass of Multivariate Generalized Autoregressive Conditional\nHeteroskedasticity Models.” Journal of Business &\nEconomic Statistics 20 (3): 339–50.\n\n\nErickson, Timothy, and Toni M Whited. 2012. “Treating Measurement\nError in Tobin’s q.” The Review of Financial Studies 25\n(4): 1286–1329.\n\n\nErrunza, Vihang, and Etienne Losq. 1985. “International Asset\nPricing Under Mild Segmentation: Theory and Test.” The\nJournal of Finance 40 (1): 105–24.\n\n\nFairfield, Patricia M, J Scott Whisenant, and Teri Lombardi Yohn. 2003.\n“Accrued Earnings and Growth: Implications for Future\nProfitability and Market Mispricing.” The Accounting\nReview 78 (1): 353–71.\n\n\nFama, Eugene F. 1984. “Forward and Spot Exchange Rates.”\nJournal of Monetary Economics 14 (3): 319–38.\n\n\n———. 1998. “Market Efficiency, Long-Term Returns, and Behavioral\nFinance.” Journal of Financial Economics 49 (3):\n283–306.\n\n\nFama, Eugene F, Lawrence Fisher, Michael C Jensen, and Richard Roll.\n1969. “The Adjustment of Stock Prices to New Information.”\nInternational Economic Review 10 (1): 1–21.\n\n\nFama, Eugene F., and Kenneth R. French. 1989. “Business conditions and expected returns on stocks and\nbonds.” Journal of Financial\nEconomics 25 (1): 23–49. https://doi.org/10.1016/0304-405X(89)90095-0.\n\n\n———. 1992. “The cross-section of expected\nstock returns.” The Journal of\nFinance 47 (2): 427–65. https://doi.org/2329112.\n\n\n———. 1993a. “Common risk factors in the\nreturns on stocks and bonds.” Journal of\nFinancial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\n———. 1993b. “Common risk factors in the\nreturns on stocks and bonds.” Journal of\nFinancial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\n———. 2015a. “A Five-Factor Asset Pricing Model.”\nJournal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nFama, Eugene F, and Kenneth R French. 1996. “Multifactor\nExplanations of Asset Pricing Anomalies.” The Journal of\nFinance 51 (1): 55–84.\n\n\n———. 2006. “Profitability, Investment and Average Returns.”\nJournal of Financial Economics 82 (3): 491–518.\n\n\n———. 2008. “Dissecting Anomalies.” The Journal of\nFinance 63 (4): 1653–78.\n\n\n———. 2012. “Size, Value, and Momentum in International Stock\nReturns.” Journal of Financial Economics 105 (3):\n457–72.\n\n\n———. 2015b. “A Five-Factor Asset Pricing Model.”\nJournal of Financial Economics 116 (1): 1–22.\n\n\n———. 2020. “Comparing Cross-Section and Time-Series Factor\nModels.” The Review of Financial Studies 33 (5):\n1891–1926.\n\n\nFama, Eugene F., and James D. MacBeth. 1973a. “Risk, return, and equilibrium: Empirical\ntests.” Journal of Political Economy\n81 (3): 607–36. https://doi.org/10.1086/260061.\n\n\nFama, Eugene F, and James D MacBeth. 1973b. “Risk, Return, and\nEquilibrium: Empirical Tests.” Journal of Political\nEconomy 81 (3): 607–36.\n\n\nFarre-Mensa, Joan, and Alexander Ljungqvist. 2016. “Do Measures of\nFinancial Constraints Measure Financial Constraints?” The\nReview of Financial Studies 29 (2): 271–308.\n\n\nFazzari, Steven, R Glenn Hubbard, and Bruce C Petersen. 1987.\n“Financing Constraints and Corporate Investment.” National\nBureau of Economic Research Cambridge, Mass., USA.\n\n\nFlannery, Mark J, and Aris A Protopapadakis. 2002. “Macroeconomic\nFactors Do Influence Aggregate Stock Returns.” The Review of\nFinancial Studies 15 (3): 751–82.\n\n\nFong, Kingsley YL, Craig W Holden, and Charles A Trzcinka. 2017.\n“What Are the Best Liquidity Proxies for Global Research?”\nReview of Finance 21 (4): 1355–1401.\n\n\nForbes, Kristin J, and Roberto Rigobon. 2002. “No Contagion, Only\nInterdependence: Measuring Stock Market Comovements.” The\nJournal of Finance 57 (5): 2223–61.\n\n\nFoucault, Thierry. 1999. “Order Flow Composition and Trading Costs\nin a Dynamic Limit Order Market.” Journal of Financial\nMarkets 2 (2): 99–134.\n\n\nFrancis, Jennifer, Ryan LaFond, Per Olsson, and Katherine Schipper.\n2005. “The Market Pricing of Accruals Quality.” Journal\nof Accounting and Economics 39 (2): 295–327.\n\n\nFrank, Murray Z, and Vidhan K Goyal. 2003. “Testing the Pecking\nOrder Theory of Capital Structure.” Journal of Financial\nEconomics 67 (2): 217–48.\n\n\n———. 2009. “Capital Structure Decisions: Which Factors Are\nReliably Important?” Financial Management 38 (1): 1–37.\n\n\nFrankel, Jeffrey A. 1979. “On the Mark: A Theory of Floating\nExchange Rates Based on Real Interest Differentials.” The\nAmerican Economic Review 69 (4): 610–22.\n\n\nFrazzini, Andrea, and Lasse Heje Pedersen. 2014. “Betting against beta.” Journal of\nFinancial Economics 111 (1): 1–25. https://doi.org/10.1016/j.jfineco.2013.10.005.\n\n\nGabaix, Xavier, Parameswaran Gopikrishnan, Vasiliki Plerou, and H Eugene\nStanley. 2003. “A Theory of Power-Law Distributions in Financial\nMarket Fluctuations.” Nature 423 (6937): 267–70.\n\n\nGabaix, Xavier, and Ralph SJ Koijen. 2021. “In Search of the\nOrigins of Financial Fluctuations: The Inelastic Markets\nHypothesis.” National Bureau of Economic Research.\n\n\nGarfinkel, Jon A. 2009. “Measuring Investors’ Opinion\nDivergence.” Journal of Accounting Research 47 (5):\n1317–48.\n\n\nGarfinkel, Jon A, and Jonathan Sokobin. 2006. “Volume, Opinion\nDivergence, and Returns: A Study of Post–Earnings Announcement\nDrift.” Journal of Accounting Research 44 (1): 85–112.\n\n\nGârleanu, Nicolae, and Lasse Heje Pedersen. 2013. “Dynamic Trading\nwith Predictable Returns and Transaction Costs.” The Journal\nof Finance 68 (6): 2309–40.\n\n\nGentzkow, Matthew, and Jesse M Shapiro. 2014. “Code and Data for\nthe Social Sciences: A Practitioner’s Guide.” Working Paper,\nUniversity of Chicago.\n\n\nGibbons, Michael R, Stephen A Ross, and Jay Shanken. 1989. “A Test\nof the Efficiency of a Given Portfolio.” Econometrica:\nJournal of the Econometric Society, 1121–52.\n\n\nGivoly, Dan, and Dan Palmon. 1982. “Timeliness of Annual Earnings\nAnnouncements: Some Empirical Evidence.” Accounting\nReview, 486–508.\n\n\nGlosten, Lawrence R, and Paul R Milgrom. 1985. “Bid, Ask and\nTransaction Prices in a Specialist Market with Heterogeneously Informed\nTraders.” Journal of Financial Economics 14 (1): 71–100.\n\n\nGompers, Paul, Joy Ishii, and Andrew Metrick. 2003. “Corporate\nGovernance and Equity Prices.” The Quarterly Journal of\nEconomics 118 (1): 107–56.\n\n\nGoyal, Amit, and Narasimhan Jegadeesh. 2018. “Cross-Sectional and\nTime-Series Tests of Return Predictability: What Is the\nDifference?” The Review of Financial Studies 31 (5):\n1784–824.\n\n\nGoyenko, Ruslan Y, Craig W Holden, and Charles A Trzcinka. 2009.\n“Do Liquidity Measures Measure Liquidity?” Journal of\nFinancial Economics 92 (2): 153–81.\n\n\nGriffin, John M. 2002. “Are the Fama and French Factors Global or\nCountry Specific?” The Review of Financial Studies 15\n(3): 783–803.\n\n\nGriffin, John M, Patrick J Kelly, and Federico Nardari. 2010. “Do\nMarket Efficiency Measures Yield Correct Inferences? A Comparison of\nDeveloped and Emerging Markets.” The Review of Financial\nStudies 23 (8): 3225–77.\n\n\nGrinblatt, Mark, Sheridan Titman, and Russ Wermers. 1995.\n“Momentum Investment Strategies, Portfolio Performance, and\nHerding: A Study of Mutual Fund Behavior.” American Economic\nReview 85 (5): 1088–1105.\n\n\nGrootendorst, Maarten. 2022. “BERTopic: Neural Topic Modeling with\na Class-Based TF-IDF Procedure.” arXiv Preprint\narXiv:2203.05794.\n\n\nGrundy, Bruce D, and J Spencer Martin Martin. 2001. “Understanding\nthe Nature of the Risks and the Source of the Rewards to Momentum\nInvesting.” The Review of Financial Studies 14 (1):\n29–78.\n\n\nGuay, Wayne, Delphine Samuels, and Daniel Taylor. 2016. “Guiding\nThrough the Fog: Financial Statement Complexity and Voluntary\nDisclosure.” Journal of Accounting and Economics 62\n(2-3): 234–69.\n\n\nGürkaynak, Refet S, Brian Sack, and Jonathan H Wright. 2007. “The\nUS Treasury Yield Curve: 1961 to the Present.” Journal of\nMonetary Economics 54 (8): 2291–2304.\n\n\nHadlock, Charles J, and Joshua R Pierce. 2010. “New Evidence on\nMeasuring Financial Constraints: Moving Beyond the KZ Index.”\nThe Review of Financial Studies 23 (5): 1909–40.\n\n\nHall, Peter. 1992. “On the Removal of Skewness by\nTransformation.” Journal of the Royal Statistical Society\nSeries B: Statistical Methodology 54 (1): 221–28.\n\n\nHameed, Allaudeen, Wenjin Kang, and Shivesh Viswanathan. 2010.\n“Stock Market Declines and Liquidity.” The Journal of\nFinance 65 (1): 257–93.\n\n\nHanda, Puneet, Robert Schwartz, and Ashish Tiwari. 2003. “Quote\nSetting and Price Formation in an Order Driven Market.”\nJournal of Financial Markets 6 (4): 461–89.\n\n\nHansen, Lars Peter. 1982. “Large Sample Properties of Generalized\nMethod of Moments Estimators.” Econometrica: Journal of the\nEconometric Society, 1029–54.\n\n\nHarris, Milton, and Artur Raviv. 1993. “Differences of Opinion\nMake a Horse Race.” The Review of Financial Studies 6\n(3): 473–506.\n\n\nHarvey, Campbell R., Yan Liu, and Heqing Zhu. 2016a. “… and the cross-section\nof expected returns.” Review of Financial\nStudies 29 (1): 5–68. https://doi.org/10.1093/rfs/hhv059.\n\n\nHarvey, Campbell R, Yan Liu, and Heqing Zhu. 2016b. “… and the\nCross-Section of Expected Returns.” The Review of Financial\nStudies 29 (1): 5–68.\n\n\nHarvey, Campbell R, and Akhtar Siddique. 2000. “Conditional\nSkewness in Asset Pricing Tests.” The Journal of Finance\n55 (3): 1263–95.\n\n\nHasbrouck, Joel. 1991. “Measuring the Information Content of Stock\nTrades.” The Journal of Finance 46 (1): 179–207.\n\n\n———. 2007. Empirical Market Microstructure: The Institutions,\nEconomics, and Econometrics of Securities Trading. Oxford\nUniversity Press.\n\n\n———. 2009. “Trading Costs and Returns for US Equities: Estimating\nEffective Costs from Daily Data.” The Journal of Finance\n64 (3): 1445–77.\n\n\nHasler, Mathias. 2021. “Is the Value Premium Smaller Than We\nThought?” Working Paper. https://ssrn.com/abstract=3886984.\n\n\nHayashi, Fumio. 1982. “Tobin’s Marginal q and Average q: A\nNeoclassical Interpretation.” Econometrica: Journal of the\nEconometric Society, 213–24.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.\n“Deep Residual Learning for Image Recognition.” In\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 770–78.\n\n\nHealy, Paul M. 1985. “The Effect of Bonus Schemes on Accounting\nDecisions.” Journal of Accounting and Economics 7 (1-3):\n85–107.\n\n\nHealy, Paul M, and Krishna G Palepu. 2001. “Information Asymmetry,\nCorporate Disclosure, and the Capital Markets: A Review of the Empirical\nDisclosure Literature.” Journal of Accounting and\nEconomics 31 (1-3): 405–40.\n\n\nHenderson, J Vernon, Adam Storeygard, and David N Weil. 2012.\n“Measuring Economic Growth from Outer Space.” American\nEconomic Review 102 (2): 994–1028.\n\n\nHenry, Peter Blair. 2000. “Stock Market Liberalization, Economic\nReform, and Emerging Market Equity Prices.” The Journal of\nFinance 55 (2): 529–64.\n\n\nHill, Bruce M. 1975. “A Simple General Approach to Inference about\nthe Tail of a Distribution.” The Annals of Statistics,\n1163–74.\n\n\nHillion, Pierre, and Matti Suominen. 2004. “The Manipulation of\nClosing Prices.” Journal of Financial Markets 7 (4):\n351–75.\n\n\nHirshleifer, David, Sonya Seongyeon Lim, and Siew Hong Teoh. 2009.\n“Driven to Distraction: Extraneous Events and Underreaction to\nEarnings News.” The Journal of Finance 64 (5):\n2289–2325.\n\n\nHirshleifer, David, and Siew Hong Teoh. 2003. “Limited Attention,\nInformation Disclosure, and Financial Reporting.” Journal of\nAccounting and Economics 36 (1-3): 337–86.\n\n\nHoberg, Gerard, and Gordon Phillips. 2010. “Product Market\nSynergies and Competition in Mergers and Acquisitions: A Text-Based\nAnalysis.” The Review of Financial Studies 23 (10):\n3773–3811.\n\n\n———. 2016. “Text-Based Network Industries and Endogenous Product\nDifferentiation.” Journal of Political Economy 124 (5):\n1423–65.\n\n\nHoberg, Gerard, and Gordon M Phillips. 2018. “Text-Based Industry\nMomentum.” Journal of Financial and Quantitative\nAnalysis 53 (6): 2355–88.\n\n\nHofstede, Geert. 2001. “Culture’s Consequences: Comparing Values,\nBehaviors, Institutions and Organizations Across Nations.”\nInternational Educational and Professional.\n\n\nHong, Harrison, and Jeremy C Stein. 1999. “A Unified Theory of\nUnderreaction, Momentum Trading, and Overreaction in Asset\nMarkets.” The Journal of Finance 54 (6): 2143–84.\n\n\n———. 2003. “Differences of Opinion, Short-Sales Constraints, and\nMarket Crashes.” The Review of Financial Studies 16 (2):\n487–525.\n\n\nHope, Ole-Kristian. 2003. “Disclosure Practices, Enforcement of\nAccounting Standards, and Analysts’ Forecast Accuracy: An International\nStudy.” Journal of Accounting Research 41 (2): 235–72.\n\n\nHou, Kewei, Chen Xue, and Lu Zhang. 2014. “Digesting anomalies: An investment\napproach.” Review of Financial\nStudies 28 (3): 650–705. https://doi.org/10.1093/rfs/hhu068.\n\n\n———. 2015. “Digesting Anomalies: An Investment Approach.”\nThe Review of Financial Studies 28 (3): 650–705.\n\n\n———. 2020a. “Replicating\nanomalies.” Review of Financial\nStudies 33 (5): 2019–2133. https://doi.org/10.1093/rfs/hhy131.\n\n\n———. 2020b. “Replicating Anomalies.” The Review of\nFinancial Studies 33 (5): 2019–2133.\n\n\nHouge, Todd, Tim Loughran, Gerry Suchanek, and Xuemin Yan. 2001.\n“Divergence of Opinion, Uncertainty, and the Quality of Initial\nPublic Offerings.” Financial Management, 5–23.\n\n\nHribar, Paul, and Daniel W Collins. 2002. “Errors in Estimating\nAccruals: Implications for Empirical Research.” Journal of\nAccounting Research 40 (1): 105–34.\n\n\nHsu, Jason C. 2004. “Cap-Weighted Portfolios Are Sub-Optimal\nPortfolios.” Journal of Investment Management 4 (3).\n\n\nHuang, Allen H, Reuven Lehavy, Amy Y Zang, and Rong Zheng. 2018.\n“Analyst Information Discovery and Interpretation Roles: A Topic\nModeling Approach.” Management Science 64 (6): 2833–55.\n\n\nHuang, Allen H, Hui Wang, and Yi Yang. 2023. “FinBERT: A Large\nLanguage Model for Extracting Information from Financial Text.”\nContemporary Accounting Research 40 (2): 806–41.\n\n\nHuang, Roger D, and Hans R Stoll. 1997. “The Components of the\nBid-Ask Spread: A General Approach.” The Review of Financial\nStudies 10 (4): 995–1034.\n\n\nHuang, Xiangqian, Clark Liu, and Tao Shu. 2023. “Factors and\nAnomalies in the Vietnamese Stock Market.” Pacific-Basin\nFinance Journal 82: 102176.\n\n\nHuang, Yu, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo\nHuang. 2021. “What Makes Multi-Modal Learning Better Than Single\n(Provably).” Advances in Neural Information Processing\nSystems 34: 10944–56.\n\n\nHuang, Yupan, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. 2022.\n“Layoutlmv3: Pre-Training for Document Ai with Unified Text and\nImage Masking.” In Proceedings of the 30th ACM International\nConference on Multimedia, 4083–91.\n\n\nHuergo, Elena, and Jordi Jaumandreu. 2004. “Firms’ Age, Process\nInnovation and Productivity Growth.” International Journal of\nIndustrial Organization 22 (4): 541–59.\n\n\nHutton, Amy P, Alan J Marcus, and Hassan Tehranian. 2009. “Opaque\nFinancial Reports, R2, and Crash Risk.” Journal of Financial\nEconomics 94 (1): 67–86.\n\n\nJacobs, Heiko, and Sebastian Müller. 2020. “Anomalies Across the\nGlobe: Once Public, No Longer Existent?” Journal of Financial\nEconomics 135 (1): 213–30.\n\n\nJagannathan, Ravi, and Tongshu Ma. 2003. “Risk Reduction in Large\nPortfolios: Why Imposing the Wrong Constraints Helps.” The\nJournal of Finance 58 (4): 1651–83.\n\n\nJagannathan, Ravi, and Zhenyu Wang. 1996a. “The conditional CAPM and the cross-section of expected\nreturns.” The Journal of Finance 51\n(1): 3–53. https://doi.org/10.2307/2329301.\n\n\n———. 1996b. “The Conditional CAPM and the Cross-Section of\nExpected Returns.” The Journal of Finance 51 (1): 3–53.\n\n\nJean, Neal, Marshall Burke, Michael Xie, W Matthew Alampay Davis, David\nB Lobell, and Stefano Ermon. 2016. “Combining Satellite Imagery\nand Machine Learning to Predict Poverty.” Science 353\n(6301): 790–94.\n\n\nJegadeesh, Narasimhan. 1990. “Evidence of Predictable Behavior of\nSecurity Returns.” The Journal of Finance 45 (3):\n881–98.\n\n\nJegadeesh, Narasimhan, and Sheridan Titman. 1993. “Returns to\nBuying Winners and Selling Losers: Implications for Stock Market\nEfficiency.” The Journal of Finance 48 (1): 65–91.\n\n\nJensen, Michael C. 1986. “Agency Costs of Free Cash Flow,\nCorporate Finance, and Takeovers.” The American Economic\nReview 76 (2): 323–29.\n\n\nJensen, Michael C, and William H Meckling. 1976. “Theory of the\nFirm: Managerial Behavior, Agency Costs and Ownership Structure.”\nJournal of Financial Economics 3 (4): 305–60.\n\n\n———. 2019. “Theory of the Firm: Managerial Behavior, Agency Costs\nand Ownership Structure.” In Corporate Governance,\n77–132. Gower.\n\n\nJensen, Michael C, and Richard S Ruback. 1983. “The Market for\nCorporate Control: The Scientific Evidence.” Journal of\nFinancial Economics 11 (1-4): 5–50.\n\n\nJensen, Theis Ingerslev, Bryan Kelly, and Lasse Heje Pedersen. 2023.\n“Is There a Replication Crisis in Finance?” The Journal\nof Finance 78 (5): 2465–2518.\n\n\nJha, Manish, Jialin Qian, Michael Weber, and Baozhong Yang. 2024.\n“ChatGPT and Corporate Policies.” National Bureau of\nEconomic Research.\n\n\nJohnson, Simon, Rafael La Porta, Florencio Lopez-de-Silanes, and Andrei\nShleifer. 2000. “Tunneling.” American Economic\nReview 90 (2): 22–27.\n\n\nJohnson, Timothy C. 2002. “Rational Momentum Effects.”\nThe Journal of Finance 57 (2): 585–608.\n\n\nJones, Jennifer J. 1991. “Earnings Management During Import Relief\nInvestigations.” Journal of Accounting Research 29 (2):\n193–228.\n\n\nJorion, Philippe. 1990. “The Exchange-Rate Exposure of US\nMultinationals.” Journal of Business, 331–45.\n\n\n———. 1991. “The Pricing of Exchange Rate Risk in the Stock\nMarket.” Journal of Financial and Quantitative Analysis\n26 (3): 363–76.\n\n\nKacperczyk, Marcin, Clemens Sialm, and Lu Zheng. 2008. “Unobserved\nActions of Mutual Funds.” The Review of Financial\nStudies 21 (6): 2379–2416.\n\n\nKahneman, Daniel, and Amos Tversky. 2013. “Prospect Theory: An\nAnalysis of Decision Under Risk.” In Handbook of the\nFundamentals of Financial Decision Making: Part i, 99–127. World\nScientific.\n\n\nKaminsky, Graciela L, Carmen M Reinhart, and Carlos A Vegh. 2002.\n“The Unholy Trinity of Financial Contagion.” Journal of\nEconomic Perspectives 17 (4): 51–74.\n\n\nKandel, Eugene, and Neil D Pearson. 1995. “Differential\nInterpretation of Public Signals and Trade in Speculative\nMarkets.” Journal of Political Economy 103 (4): 831–72.\n\n\nKaniel, Ron, Shuming Liu, Gideon Saar, and Sheridan Titman. 2012.\n“Individual Investor Trading and Return Patterns Around Earnings\nAnnouncements.” The Journal of Finance 67 (2): 639–80.\n\n\nKaplan, Steven N, and Luigi Zingales. 1997. “Do Investment-Cash\nFlow Sensitivities Provide Useful Measures of Financing\nConstraints?” The Quarterly Journal of Economics 112\n(1): 169–215.\n\n\nKarpoff, Jonathan M. 1987. “The Relation Between Price Changes and\nTrading Volume: A Survey.” Journal of Financial and\nQuantitative Analysis 22 (1): 109–26.\n\n\nKelly, Bryan, and Hao Jiang. 2014. “Tail Risk and Asset\nPrices.” The Review of Financial Studies 27 (10):\n2841–71.\n\n\nKhan, Mozaffar. 2008. “Are Accruals Mispriced? Evidence from Tests\nof an Intertemporal Capital Asset Pricing Model.” Journal of\nAccounting and Economics 45 (1): 55–77.\n\n\nKhanh, Hoang Thi Mai, and Vinh Khuong Nguyen. 2018. “Audit\nQuality, Firm Characteristics and Real Earnings Management: The Case of\nListed Vietnamese Firms.” International Journal of Economics\nand Financial Issues 8 (4): 243.\n\n\nKim, Geewook, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park,\nJinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun\nPark. 2022. “Ocr-Free Document Understanding Transformer.”\nIn European Conference on Computer Vision, 498–517. Springer.\n\n\nKim, Kenneth A, Haixiao Liu, and J Jimmy Yang. 2013.\n“Reconsidering Price Limit Effectiveness.” Journal of\nFinancial Research 36 (4): 493–518.\n\n\nKim, Kenneth A, and S Ghon Rhee. 1997. “Price Limit Performance:\nEvidence from the Tokyo Stock Exchange.” The Journal of\nFinance 52 (2): 885–901.\n\n\nKim, Oliver, and Robert E Verrecchia. 1991. “Market Reaction to\nAnticipated Announcements.” Journal of Financial\nEconomics 30 (2): 273–309.\n\n\nKing, Mervyn A, and Sushil Wadhwani. 1990. “Transmission of\nVolatility Between Stock Markets.” The Review of Financial\nStudies 3 (1): 5–33.\n\n\nKipf, Thomas N, and Max Welling. 2016. “Semi-Supervised\nClassification with Graph Convolutional Networks.” arXiv\nPreprint arXiv:1609.02907.\n\n\nKlapper, Leora F, and Inessa Love. 2004. “Corporate Governance,\nInvestor Protection, and Performance in Emerging Markets.”\nJournal of Corporate Finance 10 (5): 703–28.\n\n\nKodres, Laura E, and Matthew Pritsker. 2002. “A Rational\nExpectations Model of Financial Contagion.” The Journal of\nFinance 57 (2): 769–99.\n\n\nKoijen, Ralph SJ, and Motohiro Yogo. 2019. “A Demand System\nApproach to Asset Pricing.” Journal of Political Economy\n127 (4): 1475–515.\n\n\nKolari, James W, and Seppo Pynnönen. 2010. “Event Study Testing\nwith Cross-Sectional Correlation of Abnormal Returns.” The\nReview of Financial Studies 23 (11): 3996–4025.\n\n\nKorajczyk, Robert A, and Ronnie Sadka. 2004. “Are Momentum Profits\nRobust to Trading Costs?” The Journal of Finance 59 (3):\n1039–82.\n\n\nKothari, Sagar P, Andrew J Leone, and Charles E Wasley. 2005.\n“Performance Matched Discretionary Accrual Measures.”\nJournal of Accounting and Economics 39 (1): 163–97.\n\n\nKothari, Sagar P, and Jerold B Warner. 2007. “Econometrics of\nEvent Studies.” In Handbook of Empirical Corporate\nFinance, 3–36. Elsevier.\n\n\nKraft, Arthur, Andrew J Leone, and Charles Wasley. 2006. “An\nAnalysis of the Theories and Explanations Offered for the Mispricing of\nAccruals and Accrual Components.” Journal of Accounting\nResearch 44 (2): 297–339.\n\n\nKraus, Alan, and Robert H Litzenberger. 1976. “Skewness Preference\nand the Valuation of Risk Assets.” The Journal of\nFinance 31 (4): 1085–1100.\n\n\nKrishnamurthy, Arvind, and Annette Vissing-Jorgensen. 2012. “The\nAggregate Demand for Treasury Debt.” Journal of Political\nEconomy 120 (2): 233–67.\n\n\nKupiec, Paul H et al. 1995. “Techniques for Verifying the Accuracy\nof Risk Measurement Models.”\n\n\nKyle, Albert S. 1985. “Continuous Auctions and Insider\nTrading.” Econometrica: Journal of the Econometric\nSociety, 1315–35.\n\n\nLa Porta, Rafael, Florencio Lopez-de-Silanes, Andrei Shleifer, and\nRobert Vishny. 2000a. “Investor Protection and Corporate\nGovernance.” Journal of Financial Economics 58 (1-2):\n3–27.\n\n\nLa Porta, Rafael, Florencio Lopez-de-Silanes, Andrei Shleifer, and\nRobert W Vishny. 2000b. “Agency Problems and Dividend Policies\nAround the World.” The Journal of Finance 55 (1): 1–33.\n\n\nLamont, Owen, Christopher Polk, and Jesús Saaá-Requejo. 2001.\n“Financial Constraints and Stock Returns.” The Review\nof Financial Studies 14 (2): 529–54.\n\n\nLandsman, Wayne R, Edward L Maydew, and Jacob R Thornock. 2012.\n“The Information Content of Annual Earnings Announcements and\nMandatory Adoption of IFRS.” Journal of Accounting and\nEconomics 53 (1-2): 34–54.\n\n\nLang, Mark, Karl V Lins, and Mark Maffett. 2012. “Transparency,\nLiquidity, and Valuation: International Evidence on When Transparency\nMatters Most.” Journal of Accounting Research 50 (3):\n729–74.\n\n\nLang, Mark, and Russell Lundholm. 1993. “Cross-Sectional\nDeterminants of Analyst Ratings of Corporate Disclosures.”\nJournal of Accounting Research 31 (2): 246–71.\n\n\nLau, Jey Han, and Timothy Baldwin. 2016. “An Empirical Evaluation\nof Doc2vec with Practical Insights into Document Embedding\nGeneration.” arXiv Preprint arXiv:1607.05368.\n\n\nLe, Quoc, and Tomas Mikolov. 2014. “Distributed Representations of\nSentences and Documents.” In International Conference on\nMachine Learning, 1188–96. PMLR.\n\n\nLedoit, Olivier, and Michael Wolf. 2004. “A Well-Conditioned\nEstimator for Large-Dimensional Covariance Matrices.” Journal\nof Multivariate Analysis 88 (2): 365–411.\n\n\nLehavy, Reuven, and Richard G Sloan. 2008. “Investor Recognition\nand Stock Returns.” Review of Accounting Studies 13 (2):\n327–61.\n\n\nLeibowitz, Martin L. 2002. “The Levered p/e Ratio.”\nFinancial Analysts Journal 58 (6): 68–77.\n\n\nLesmond, David A. 2005. “Liquidity of Emerging Markets.”\nJournal of Financial Economics 77 (2): 411–52.\n\n\nLesmond, David A, Joseph P Ogden, and Charles A Trzcinka. 1999. “A\nNew Estimate of Transaction Costs.” The Review of Financial\nStudies 12 (5): 1113–41.\n\n\nLesmond, David A, Michael J Schill, and Chunsheng Zhou. 2004. “The\nIllusory Nature of Momentum Profits.” Journal of Financial\nEconomics 71 (2): 349–80.\n\n\nLeuz, Christian, Dhananjay Nanda, and Peter D Wysocki. 2003.\n“Earnings Management and Investor Protection: An International\nComparison.” Journal of Financial Economics 69 (3):\n505–27.\n\n\nLewellen, Jonathan, Stefan Nagel, and Jay Shanken. 2010. “A\nSkeptical Appraisal of Asset Pricing Tests.” Journal of\nFinancial Economics 96 (2): 175–94.\n\n\nLi, Feng. 2008. “Annual Report Readability, Current Earnings, and\nEarnings Persistence.” Journal of Accounting and\nEconomics 45 (2-3): 221–47.\n\n\nLi, Feng et al. 2010. “Textual Analysis of Corporate Disclosures:\nA Survey of the Literature.” Journal of Accounting\nLiterature 29 (1): 143–65.\n\n\nLi, Junnan, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.\n“Blip-2: Bootstrapping Language-Image Pre-Training with Frozen\nImage Encoders and Large Language Models.” In International\nConference on Machine Learning, 19730–42. PMLR.\n\n\nLiang, Paul Pu, Amir Zadeh, and Louis-Philippe Morency. 2024.\n“Foundations & Trends in Multimodal Machine Learning:\nPrinciples, Challenges, and Open Questions.” ACM Computing\nSurveys 56 (10): 1–42.\n\n\nLindenberg, Eric B, and Stephen A Ross. 1981. “Tobin’s q Ratio and\nIndustrial Organization.” Journal of Business, 1–32.\n\n\nLintner, John. 1956. “Distribution of Incomes of Corporations\nAmong Dividends, Retained Earnings, and Taxes.” The American\nEconomic Review 46 (2): 97–113.\n\n\n———. 1965. “Security prices, risk, and\nmaximal gains from diversification.” The Journal\nof Finance 20 (4): 587–615. https://doi.org/10.1111/j.1540-6261.1965.tb02930.x.\n\n\nLittle, Roderick JA, and Donald B Rubin. 2019. Statistical Analysis\nwith Missing Data. John Wiley & Sons.\n\n\nLiu, Laura Xiaolei, Toni M Whited, and Lu Zhang. 2009.\n“Investment-Based Expected Stock Returns.” Journal of\nPolitical Economy 117 (6): 1105–39.\n\n\nLivnat, Joshua, and Richard R Mendenhall. 2006. “Comparing the\nPost–Earnings Announcement Drift for Surprises Calculated from Analyst\nand Time Series Forecasts.” Journal of Accounting\nResearch 44 (1): 177–205.\n\n\nLo, Andrew W, and A Craig MacKinlay. 1990. “An Econometric\nAnalysis of Nonsynchronous Trading.” Journal of\nEconometrics 45 (1-2): 181–211.\n\n\nLongin, Francois, and Bruno Solnik. 2001. “Extreme Correlation of\nInternational Equity Markets.” The Journal of Finance 56\n(2): 649–76.\n\n\nLoughran, Tim, and Bill McDonald. 2011. “When Is a Liability Not a\nLiability? Textual Analysis, Dictionaries, and 10-Ks.” The\nJournal of Finance 66 (1): 35–65.\n\n\n———. 2014. “Measuring Readability in Financial\nDisclosures.” The Journal of Finance 69 (4): 1643–71.\n\n\nLustig, Hanno, and Adrien Verdelhan. 2007. “The Cross Section of\nForeign Currency Risk Premia and Consumption Growth Risk.”\nAmerican Economic Review 97 (1): 89–117.\n\n\nLyon, John D, Brad M Barber, and Chih-Ling Tsai. 1999. “Improved\nMethods for Tests of Long-Run Abnormal Stock Returns.” The\nJournal of Finance 54 (1): 165–201.\n\n\nMacKinlay, A Craig. 1997. “Event Studies in Economics and\nFinance.” Journal of Economic Literature 35 (1): 13–39.\n\n\nMaillard, Sébastien, Thierry Roncalli, and Jérôme Teı̈letche. 2010.\n“The Properties of Equally Weighted Risk Contribution\nPortfolios.” Journal of Portfolio Management 36 (4): 60.\n\n\nMandelbrot, Benoit et al. 1963. “The Variation of Certain\nSpeculative Prices.” Journal of Business 36 (4): 394.\n\n\nMarkowitz, Harry. 1952. “Portfolio\nselection.” The Journal of Finance 7\n(1): 77–91. https://doi.org/10.1111/j.1540-6261.1952.tb01525.x.\n\n\nMcLean, R David, and Jeffrey Pontiff. 2016. “Does Academic\nResearch Destroy Stock Return Predictability?” The Journal of\nFinance 71 (1): 5–32.\n\n\nMcNeil, Alexander J, and Rüdiger Frey. 2000. “Estimation of\nTail-Related Risk Measures for Heteroscedastic Financial Time Series: An\nExtreme Value Approach.” Journal of Empirical Finance 7\n(3-4): 271–300.\n\n\nMcNeil, Alexander J, Rüdiger Frey, and Paul Embrechts. 2015.\nQuantitative Risk Management: Concepts, Techniques and Tools-Revised\nEdition. Princeton university press.\n\n\nMeese, Richard A, and Kenneth Rogoff. 1983. “Empirical Exchange\nRate Models of the Seventies: Do They Fit Out of Sample?”\nJournal of International Economics 14 (1-2): 3–24.\n\n\nMenkhoff, Lukas, Lucio Sarno, Maik Schmeling, and Andreas Schrimpf.\n2012. “Carry Trades and Global Foreign Exchange\nVolatility.” The Journal of Finance 67 (2): 681–718.\n\n\nMenkveld, Albert J., Anna Dreber, Felix Holzmeister, Juergen Huber,\nMagnus Johannesson, Michael Kirchler, Sebastian Neusüss, Michael Razen,\nand Utz Weitzel. n.d. “Nonstandard\nerrors.” The Journal of Finance 79 (3): 2339–90.\nhttps://doi.org/https://doi.org/10.1111/jofi.13337.\n\n\nMenzly, Lior, and Oguzhan Ozbas. 2010. “Market Segmentation and\nCross-Predictability of Returns.” The Journal of Finance\n65 (4): 1555–80.\n\n\nMilgrom, Paul, and Nancy Stokey. 1982. “Information, Trade and\nCommon Knowledge.” Journal of Economic Theory 26 (1):\n17–27.\n\n\nMiller, Edward M. 1977. “Risk, Uncertainty, and Divergence of\nOpinion.” The Journal of Finance 32 (4): 1151–68.\n\n\nMiller, Merton H, and Kevin Rock. 1985. “Dividend Policy Under\nAsymmetric Information.” The Journal of Finance 40 (4):\n1031–51.\n\n\nMishkin, Frederic S. 1983. “Are Market Forecasts Rational?”\nIn A Rational Expectations Approach to Macroeconometrics: Testing\nPolicy Ineffectiveness and Efficient-Markets Models, 59–75.\nUniversity of Chicago Press.\n\n\n———. 2007. A Rational Expectations Approach to Macroeconometrics:\nTesting Policy Ineffectiveness and Efficient-Markets Models.\nUniversity of Chicago Press.\n\n\nMitchell, Mark L, and Jeffry M Netter. 1993. “The Role of\nFinancial Economics in Securities Fraud Cases: Applications at the\nSecurities and Exchange Commission.” Bus. Law. 49: 545.\n\n\nMitchell, Mark L, and Erik Stafford. 2000. “Managerial Decisions\nand Long-Term Stock Price Performance.” The Journal of\nBusiness 73 (3): 287–329.\n\n\nMorck, Randall, Andrei Shleifer, and Robert W Vishny. 1988.\n“Management Ownership and Market Valuation: An Empirical\nAnalysis.” Journal of Financial Economics 20: 293–315.\n\n\nMossin, Jan. 1966. “Equilibrium in a capital\nasset market.” Econometrica 34 (4):\n768–83. https://doi.org/10.2307/1910098.\n\n\nMyers, Stewart C. 1984. “The Capital Structure Puzzle.”\nJournal of Finance 39 (3): 575–92.\n\n\nNelson, Charles R, and Andrew F Siegel. 1987. “Parsimonious\nModeling of Yield Curves.” Journal of Business, 473–89.\n\n\nNewey, Whitney K., and Kenneth D. West. 1987a. “A simple, positive semi-definite, heteroskedasticity and\nautocorrelation consistent covariance Matrix.”\nEconometrica 55 (3): 703–8. http://www.jstor.org/stable/1913610.\n\n\nNewey, Whitney K, and Kenneth D West. 1987b. “A Simple, Positive\nSemi-Definite, Heteroskedasticity and Autocorrelation.”\nEconometrica 55 (3): 703–8.\n\n\nNguyen, Dat Quoc, and Anh-Tuan Nguyen. 2020. “PhoBERT: Pre-Trained\nLanguage Models for Vietnamese.” In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020, 1037–42.\n\n\nNguyen, Du D, and Minh C Pham. 2018. “Search-Based Sentiment and\nStock Market Reactions: An Empirical Evidence in Vietnam.”\nThe Journal of Asian Finance, Economics and Business 5 (4):\n45–56.\n\n\nNovy-Marx, Robert. 2013. “The Other Side of Value: The Gross\nProfitability Premium.” Journal of Financial Economics\n108 (1): 1–28.\n\n\nObaid, Khaled, and Kuntara Pukthuanthong. 2022. “A Picture Is\nWorth a Thousand Words: Measuring Investor Sentiment by Combining\nMachine Learning and Photos from News.” Journal of Financial\nEconomics 144 (1): 273–97.\n\n\nObstfeld, Maurice, Jay C Shambaugh, and Alan M Taylor. 2005. “The\nTrilemma in History: Tradeoffs Among Exchange Rates, Monetary Policies,\nand Capital Mobility.” Review of Economics and\nStatistics 87 (3): 423–38.\n\n\nOord, Aaron van den, Yazhe Li, and Oriol Vinyals. 2018.\n“Representation Learning with Contrastive Predictive\nCoding.” arXiv Preprint arXiv:1807.03748.\n\n\nParlour, Christine A. 1998. “Price Dynamics in Limit Order\nMarkets.” The Review of Financial Studies 11 (4):\n789–816.\n\n\nPástor, L’uboš, and Robert F Stambaugh. 2003. “Liquidity Risk and\nExpected Stock Returns.” Journal of Political Economy\n111 (3): 642–85.\n\n\nPatell, James M. 1976. “Corporate Forecasts of Earnings Per Share\nand Stock Price Behavior: Empirical Test.” Journal of\nAccounting Research, 246–76.\n\n\nPatell, James M, and Mark A Wolfson. 1982. “Good News, Bad News,\nand the Intraday Timing of Corporate Disclosures.” Accounting\nReview, 509–27.\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27.\n\n\nPhan, Thi Nha Truc, Philippe Bertrand, Hong Hai Phan, and Xuan Vinh Vo.\n2023. “The Role of Investor Behavior in Emerging Stock Markets:\nEvidence from Vietnam.” The Quarterly Review of Economics and\nFinance 87: 367–76.\n\n\nPhung, Duc Nam, and Anil V Mishra. 2016. “Ownership Structure and\nFirm Performance: Evidence from Vietnamese Listed Firms.”\nAustralian Economic Papers 55 (1): 63–98.\n\n\nPlyakha, Yuliya, Raman Uppal, and Grigory Vilkov. 2021. “Equal or\nValue Weighting? Implications for Asset-Pricing Tests.” In\nFinancial Risk Management and Modeling, 295–347. Springer.\n\n\nPontiff, Jeffrey, and Artemiza Woodgate. 2008. “Share Issuance and\nCross-Sectional Returns.” The Journal of Finance 63 (2):\n921–45.\n\n\nPukthuanthong, Kuntara, and Richard Roll. 2009. “Global Market\nIntegration: An Alternative Measure and Its Application.”\nJournal of Financial Economics 94 (2): 214–32.\n\n\nRadford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, et al. 2021. “Learning\nTransferable Visual Models from Natural Language Supervision.” In\nInternational Conference on Machine Learning, 8748–63. PmLR.\n\n\nRajan, Raghuram G, and Luigi Zingales. 1998. “Financial Dependence\nand Growth.” American Economic Review, 559–86.\n\n\nReimers, Nils, and Iryna Gurevych. 2019. “Sentence-Bert: Sentence\nEmbeddings Using Siamese Bert-Networks.” arXiv Preprint\narXiv:1908.10084.\n\n\nRock, Kevin. 1986. “Why New Issues Are Underpriced.”\nJournal of Financial Economics 15 (1-2): 187–212.\n\n\nRoll, Richard. 1984. “A Simple Implicit Measure of the Effective\nBid-Ask Spread in an Efficient Market.” The Journal of\nFinance 39 (4): 1127–39.\n\n\nRomano, Joseph P, and Michael Wolf. 2005. “Exact and Approximate\nStepdown Methods for Multiple Hypothesis Testing.” Journal of\nthe American Statistical Association 100 (469): 94–108.\n\n\nRossi, Barbara. 2013. “Exchange Rate Predictability.”\nJournal of Economic Literature 51 (4): 1063–1119.\n\n\nRossi, Emanuele, Ben Chamberlain, Fabrizio Frasca, Davide Eynard,\nFederico Monti, and Michael Bronstein. 2020. “Temporal Graph\nNetworks for Deep Learning on Dynamic Graphs.” arXiv Preprint\narXiv:2006.10637.\n\n\nRoşu, Ioanid. 2009. “A Dynamic Model of the Limit Order\nBook.” The Review of Financial Studies 22 (11): 4601–41.\n\n\nRouwenhorst, K Geert. 1998. “International Momentum\nStrategies.” The Journal of Finance 53 (1): 267–84.\n\n\nRoychowdhury, Sugata. 2006. “Earnings Management Through Real\nActivities Manipulation.” Journal of Accounting and\nEconomics 42 (3): 335–70.\n\n\nRubin, Donald B. 1976. “Inference and Missing Data.”\nBiometrika 63 (3): 581–92.\n\n\nRust, John. 1987. “Optimal Replacement of GMC Bus Engines: An\nEmpirical Model of Harold Zurcher.” Econometrica: Journal of\nthe Econometric Society, 999–1033.\n\n\nSalkever, David S. 1976. “The Use of Dummy Variables to Compute\nPredictions, Prediction Errors, and Confidence Intervals.”\nJournal of Econometrics 4 (4): 393–97.\n\n\nSandve, Geir Kjetil, Anton Nekrutenko, James Taylor, and Eivind Hovig.\n2013. “Ten Simple Rules for Reproducible Computational\nResearch.” PLoS Computational Biology 9 (10): e1003285.\n\n\nScheinkman, Jose A, and Wei Xiong. 2003. “Overconfidence and\nSpeculative Bubbles.” Journal of Political Economy 111\n(6): 1183–1220.\n\n\nScheuch, Christoph, Stefan Voigt, and Patrick Weiss. 2023. Tidy\nFinance with r. CRC Press.\n\n\nScheuch, Christoph, Stefan Voigt, Patrick Weiss, and Christoph Frey.\n2024. Tidy Finance with Python. Chapman; Hall/CRC.\n\n\nScholes, Myron, and Joseph Williams. 1977. “Estimating Betas from\nNonsynchronous Data.” Journal of Financial Economics 5\n(3): 309–27.\n\n\nSchwert, G William. 1981. “Using Financial Data to Measure Effects\nof Regulation.” The Journal of Law and Economics 24 (1):\n121–58.\n\n\nSengupta, Partha. 1998. “Corporate Disclosure Quality and the Cost\nof Debt.” Accounting Review, 459–74.\n\n\nShalen, Catherine T. 1993. “Volume, Volatility, and the Dispersion\nof Beliefs.” The Review of Financial Studies 6 (2):\n405–34.\n\n\nShanken, Jay. 1992. “On the Estimation of Beta-Pricing\nModels.” The Review of Financial Studies 5 (1): 1–33.\n\n\nSharpe, William F. 1964. “Capital asset\nprices: A theory of market equilibrium under conditions of risk\n.” The Journal of Finance 19 (3):\n425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.\n\n\nShumway, Tyler. 1997. “The Delisting Bias in CRSP Data.”\nThe Journal of Finance 52 (1): 327–40.\n\n\nShyam-Sunder, Lakshmi, and Stewart C Myers. 1999. “Testing Static\nTradeoff Against Pecking Order Models of Capital Structure.”\nJournal of Financial Economics 51 (2): 219–44.\n\n\nSias, Richard W. 2004. “Institutional Herding.” The\nReview of Financial Studies 17 (1): 165–206.\n\n\nSirri, Erik R, and Peter Tufano. 1998. “Costly Search and Mutual\nFund Flows.” The Journal of Finance 53 (5): 1589–1622.\n\n\nSloan, Richard G. 1996. “Do Stock Prices Fully Reflect Information\nin Accruals and Cash Flows about Future Earnings?” Accounting\nReview, 289–315.\n\n\nSoebhag, Amar, Bart Van Vliet, and Patrick Verwijmeren. 2022.\n“Mind Your Sorts.” Working Paper. https://di.org/10.2139/ssrn.4136672.\n\n\nStorey, John D. 2003. “The Positive False Discovery Rate: A\nBayesian Interpretation and the q-Value.” The Annals of\nStatistics 31 (6): 2013–35.\n\n\nSubrahmanyam, Avanidhar. 1994. “Circuit Breakers and Market\nVolatility: A Theoretical Perspective.” The Journal of\nFinance 49 (1): 237–54.\n\n\nSvensson, Lars EO. 1994. “Estimating and Interpreting Forward\nInterest Rates: Sweden 1992-1994.” National bureau of economic\nresearch Cambridge, Mass., USA.\n\n\nTaleb, Nassim Nicholas. 2010. The Black Swan:: The Impact of the\nHighly Improbable: With a New Section:\" on Robustness and\nFragility\". Vol. 2. Random house trade paperbacks.\n\n\nTan, Mingxing, and Quoc Le. 2019. “Efficientnet: Rethinking Model\nScaling for Convolutional Neural Networks.” In International\nConference on Machine Learning, 6105–14. PMLR.\n\n\nTetlock, Paul C. 2007. “Giving Content to Investor Sentiment: The\nRole of Media in the Stock Market.” The Journal of\nFinance 62 (3): 1139–68.\n\n\nTetlock, Paul C, Maytal Saar-Tsechansky, and Sofus Macskassy. 2008.\n“More Than Words: Quantifying Language to Measure Firms’\nFundamentals.” The Journal of Finance 63 (3): 1437–67.\n\n\nTitman, Sheridan, KC John Wei, and Feixue Xie. 2004. “Capital\nInvestments and Stock Returns.” Journal of Financial and\nQuantitative Analysis 39 (4): 677–700.\n\n\nTobin, James. 1969. “A General Equilibrium Approach to Monetary\nTheory.” Journal of Money, Credit and Banking 1 (1):\n15–29.\n\n\nVarian, Hal R. 1985. “Divergence of Opinion in Complete Markets: A\nNote.” The Journal of Finance 40 (1): 309–17.\n\n\nVeličković, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero,\nPietro Lio, and Yoshua Bengio. 2017. “Graph Attention\nNetworks.” arXiv Preprint arXiv:1710.10903.\n\n\nVerrecchia, Robert E. 1983. “Discretionary Disclosure.”\nJournal of Accounting and Economics 5: 179–94.\n\n\n———. 2001. “Essays on Disclosure.” Journal of\nAccounting and Economics 32 (1-3): 97–180.\n\n\nVilhuber, Lars. 2020. “Reproducibility and Replicability in\nEconomics.” Harvard Data Science Review 2 (4): 1–39.\n\n\nVo, Duc Hong, and Bao Doan. 2023. “Minimum Tick Size, Market\nQuality and Costs of Trade Execution in Vietnam.” Plos\nOne 18 (5): e0285821.\n\n\nVo, Xuan Vinh. 2015. “Foreign Ownership and Stock Return\nVolatility–Evidence from Vietnam.” Journal of Multinational\nFinancial Management 30: 101–9.\n\n\n———. 2017. “Do Foreign Investors Improve Stock Price\nInformativeness in Emerging Equity Markets? Evidence from\nVietnam.” Research in International Business and Finance\n42: 986–91.\n\n\nVo, Xuan Vinh, and Dang Bao Anh Phan. 2017. “Further Evidence on\nthe Herd Behavior in Vietnam Stock Market.” Journal of\nBehavioral and Experimental Finance 13: 33–41.\n\n\nVu, Thanh, Dat Quoc Nguyen, Mark Dras, Mark Johnson, et al. 2018.\n“VnCoreNLP: A Vietnamese Natural Language Processing\nToolkit.” In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nDemonstrations, 56–60.\n\n\nWalter, Dominik, Rüdiger Weber, and Patrick Weiss. 2022. “Non-standard errors in portfolio sorts.”\nWorking Paper. https://ssrn.com/abstract=4164117.\n\n\nWarner, Jerold B, Ross L Watts, and Karen H Wruck. 1988. “Stock\nPrices and Top Management Changes.” Journal of Financial\nEconomics 20: 461–92.\n\n\nWermers, Russ. 2000. “Mutual Fund Performance: An Empirical\nDecomposition into Stock-Picking Talent, Style, Transactions Costs, and\nExpenses.” The Journal of Finance 55 (4): 1655–95.\n\n\nWhite, Halbert. 2000. “A Reality Check for Data Snooping.”\nEconometrica 68 (5): 1097–1126.\n\n\nWhited, Toni M, and Guojun Wu. 2006. “Financial Constraints\nRisk.” The Review of Financial Studies 19 (2): 531–59.\n\n\nYan, Xuemin Sterling. 2008. “Liquidity, Investment Style, and the\nRelation Between Fund Size and Fund Performance.” Journal of\nFinancial and Quantitative Analysis 43 (3): 741–67.\n\n\nYang, Dennis, and Qiang Zhang. 2000. “Drift-Independent Volatility\nEstimation Based on High, Low, Open, and Close Prices.” The\nJournal of Business 73 (3): 477–92.\n\n\nYoung, Michael N, Mike W Peng, David Ahlstrom, Garry D Bruton, and Yi\nJiang. 2008. “Corporate Governance in Emerging Economies: A Review\nof the Principal–Principal Perspective.” Journal of\nManagement Studies 45 (1): 196–220.\n\n\nZang, Amy Y. 2012. “Evidence on the Trade-Off Between Real\nActivities Manipulation and Accrual-Based Earnings Management.”\nThe Accounting Review 87 (2): 675–703.",
    "crumbs": [
      "Home",
      "References"
    ]
  }
]