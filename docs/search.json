[
  {
    "objectID": "01_working_with_stock_returns.html",
    "href": "01_working_with_stock_returns.html",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "",
    "text": "2.1 Data Access and Preparation\nThis chapter develops a practical framework for transforming raw equity price records into return series suitable for empirical financial analysis. The focus is on methodological clarity and reproducibility, with particular attention to data issues that are prevalent in emerging equity markets such as Vietnam.\nThe discussion proceeds from individual stocks to a broad market cross-section, using constituents of the VN30 index as the primary empirical setting.\nWe begin by loading the core numerical and data manipulation libraries. These provide all functionality required for return construction without relying on specialized financial wrappers.\nimport pandas as pd\nimport numpy as np\nFor this project, we retrieve our historical price data using the DataCore API. If you wish to replicate this analysis or use the dataset for your own work, you will need to access the data through their platform.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#data-access-and-preparation",
    "href": "01_working_with_stock_returns.html#data-access-and-preparation",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "",
    "text": "2.1.1 Prerequisites for API Access\nTo run the code below, you need to configure a few things first:\n\nObtain an API Key: You must subscribe to the relevant dataset on the DataCore platform to receive a unique API key.\nWhitelist Your IP Address: The API requires your IP address to be whitelisted for security.\n\nLocal Machine: If you are running this code on your personal computer, you generally need to whitelist your public IP address.\nCloud or Remote Sessions (e.g., HPC Open OnDemand): If you are using a remote server such as those provided by DataCore, the server’s IP address will change with each new session. You must retrieve the server’s private/public IP for that specific session and whitelist it in your DataCore account settings before running the script.\n\nSet Environment Variables: To keep your credentials secure, do not hardcode your API key into your scripts. Instead, save it as an environment variable named datacore_api on your machine.\n\nNote: If you only want to test the code performance, DataCore provides a preview endpoint that does not require an API key, though the data returned is limited.\n\nimport requests\nimport pandas as pd\n\nurl = \"https://gateway.datacore.vn/data/ds/preview\"\nparams = {\n    \"dataSetCode\": \"fundamental_annual\",\n    \"pageSize\": 10000 \n}\nheaders = {\n    \"Accept\": \"application/json\",\n    \"Origin\": \"https://datacore.vn\",\n    \"Referer\": \"https://datacore.vn/\"\n}\n\nresponse = requests.get(url, params=params, headers=headers)\ndata = response.json()\n\ncolumns = data['data']['fields']\nrows = data['data']['dataDetail']\n\ndf = pd.DataFrame(rows, columns=columns)\nprint(df.head())\nprint(\"Total rows:\", len(df))\n\n  symbol  year  total_current_asset ca_fin        ca_cce       ca_cash  \\\n0    CLL  2011         1.078338e+11   None  8.313178e+10  4.131776e+09   \n1    CLL  2012         2.360575e+10   None  8.003560e+09  4.003560e+09   \n2    CLL  2013         5.764370e+10   None  3.496426e+10  9.964256e+09   \n3    CLL  2014         4.973590e+10   None  1.718744e+10  1.718744e+10   \n4    CLL  2015         2.389115e+11   None  1.790364e+11  2.403638e+10   \n\n  ca_cash_inbank ca_cash_attransit  ca_cash_equivalent  ca_fin_invest  ...  \\\n0           None              None        7.900000e+10   0.000000e+00  ...   \n1           None              None        4.000000e+09   0.000000e+00  ...   \n2           None              None        2.500000e+10   0.000000e+00  ...   \n3           None              None        0.000000e+00   1.000000e+09  ...   \n4           None              None        1.550000e+11   1.000000e+09  ...   \n\n   operating_margin      roe      roa sector_pe sector_pb  sector_ps  \\\n0           0.35473  0.19560  0.10649   3.00213   0.26108    0.22170   \n1           0.45369  0.20337  0.13018   2.40335   0.26211    0.21745   \n2           0.45997  0.23459  0.16452   3.11089   0.41013    0.32436   \n3           0.40554  0.19984  0.14747   3.25886   0.46823    0.42146   \n4           0.33774  0.16525  0.12633   6.77337   0.81110    0.68401   \n\n   sector_eps  sector_ros  sector_roe  sector_roa  \n0  3341.54903     0.07385     0.08753     0.05039  \n1  4296.47005     0.09048     0.11392     0.06346  \n2  4024.74648     0.10427     0.13645     0.07415  \n3  5100.81019     0.12933     0.14956     0.08087  \n4  5216.38499     0.10098     0.11815     0.06234  \n\n[5 rows x 308 columns]\nTotal rows: 10\n\n\n\n\n2.1.2 Checking Your IP Address\nIf you need to verify the IP address of the machine running your code (to whitelist it), you can use the following Python snippets.\nTo find your Public IP:\n\nimport requests\n\ntry:\n    public_ip = requests.get(\"https://api.ipify.org\").text\n    print(f\"Public IP: {public_ip}\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"Could not retrieve Public IP: {e}\")\n\nTo find your Private IP (useful for specific remote server setups):\n\nimport socket\n\ndef get_private_ip():\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        s.connect((\"8.8.8.8\", 80))\n        private_ip = s.getsockname()[0]\n        s.close()\n        return private_ip\n    except Exception as e:\n        return f\"Error: {e}\"\n\nprint(f\"Private IP: {get_private_ip()}\")\n\n\n\n2.1.3 Fetching the Dataset\nThe following script demonstrates how to securely authenticate and paginate through the DataCore API to retrieve the full dataset_historical_price dataset.\n\n\n\n# Convert the date column to proper datetime objects\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\n# Ensure price and ratio columns are numeric before calculation\nprices[\"close_price\"] = pd.to_numeric(prices[\"close_price\"])\nprices[\"adj_ratio\"] = pd.to_numeric(prices[\"adj_ratio\"])\n\n# Calculate the adjusted close price\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\n# Rename columns to match standard conventions\nprices = prices.rename(\n    columns={\n        \"vol_total\": \"volume\",\n        \"open_price\": \"open\",\n        \"low_price\": \"low\",\n        \"high_price\": \"high\",\n        \"close_price\": \"close\",\n    }\n)\n\n# Sort the dataset logically by symbol and date\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nprint(\"Data manipulation complete. The dataset is ready for analysis.\")\n\nData manipulation complete. The dataset is ready for analysis.\n\n\n\nprices[\"date\"] = pd.to_datetime(prices[\"date\"])\n\n\nprices[\"adjusted_close\"] = prices[\"close_price\"] * prices[\"adj_ratio\"]\n\n\nprices = prices.rename(\n    columns={\n        \"vol_total\": \"volume\",\n        \"open_price\": \"open\",\n        \"low_price\": \"low\",\n        \"high_price\": \"high\",\n        \"close_price\": \"close\",\n    }\n)\n\nprices = prices.sort_values([\"symbol\", \"date\"])\n\nAdjusted closing prices incorporate mechanical changes due to corporate actions such as cash dividends and stock splits. Using adjusted prices ensures that subsequent return calculations reflect investor-relevant performance rather than accounting artifacts.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#examining-a-single-equity",
    "href": "01_working_with_stock_returns.html#examining-a-single-equity",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.2 Examining a Single Equity",
    "text": "2.2 Examining a Single Equity\nTo ground the discussion, we isolate the trading history of a single large-cap stock, FPT, over a long sample period.\n\nimport datetime as dt\n\nstart = pd.Timestamp(\"2000-01-01\")\nend = pd.Timestamp(dt.date.today().year - 1, 12, 31)\n\n\nfpt = prices.loc[\n    (prices[\"symbol\"] == \"FPT\")\n    & (prices[\"date\"] &gt;= start)\n    & (prices[\"date\"] &lt;= end),\n    [\"date\", \"symbol\", \"volume\", \"open\", \"low\", \"high\", \"close\", \"adjusted_close\"],\n].copy()\n\nThis subset contains the standard daily market variables required for most empirical studies. Before computing returns, it is good practice to visually inspect the price series.\n\nfrom plotnine import ggplot, aes, geom_line, labs\n\n\n(\n    ggplot(fpt, aes(x=\"date\", y=\"adjusted_close\"))\n    + geom_line()\n    + labs(title=\"Adjusted price path of FPT\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 2.1: Prices are in VND, adjusted for dividend payments and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#from-prices-to-returns",
    "href": "01_working_with_stock_returns.html#from-prices-to-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.3 From Prices to Returns",
    "text": "2.3 From Prices to Returns\nMost empirical asset pricing models are formulated in terms of returns rather than price levels. The simple daily return is defined as\n\\[\nr_t = \\frac{p_t}{p_t - 1} - 1,\n\\]\nwhere \\(p_t\\) denotes the adjusted closing price at the end of trading day \\(t\\).\nBefore computing returns, we must address invalid price observations. In Vietnamese equity data, adjusted prices occasionally take the value zero. These entries typically arise from IPO placeholders, trading suspensions, or historical backfilling conventions and cannot be used to compute meaningful returns.\n\nprices.loc[prices[\"adjusted_close\"] &lt;= 0, [\"symbol\", \"date\", \"adjusted_close\"]].head()\n\n\n\n\n\n\n\n\nsymbol\ndate\nadjusted_close\n\n\n\n\n33886\nADP\n2010-02-09\n0.0\n\n\n33887\nADP\n2010-02-24\n0.0\n\n\n33888\nADP\n2010-03-01\n0.0\n\n\n33889\nADP\n2010-03-03\n0.0\n\n\n33890\nADP\n2010-03-12\n0.0\n\n\n\n\n\n\n\nWe therefore exclude non-positive adjusted prices and compute returns stock by stock. Correct chronological ordering is essential.\n\nreturns = (\n    prices\n    .loc[prices[\"adjusted_close\"] &gt; 0]\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n)\nreturns = returns.dropna(subset=[\"ret\"])\n\nThe initial return for each stock is missing by construction, since no lagged price is available. These observations are mechanical and can safely be removed in most applications.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "href": "01_working_with_stock_returns.html#limiting-the-influence-of-extreme-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.4 Limiting the Influence of Extreme Returns",
    "text": "2.4 Limiting the Influence of Extreme Returns\nDaily return series often contain extreme observations driven by data errors, thin trading, or abrupt price adjustments. A common approach is to winsorize returns using cross-sectional percentile cutoffs.\n\ndef winsorize_cs(df, column=\"ret\", lower_q=0.01, upper_q=0.99):\n    lo = df[column].quantile(lower_q)\n    hi = df[column].quantile(upper_q)\n    out = df.copy()\n    out[column] = out[column].clip(lo, hi)\n    return out\n\nreturns = winsorize_cs(returns)\n\nApplying winsorization across the full cross-section limits the impact of extreme market-wide observations while preserving relative differences between firms. Winsorizing within each stock is rarely appropriate in panel settings and can severely distort illiquid securities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#distributional-features-of-returns",
    "href": "01_working_with_stock_returns.html#distributional-features-of-returns",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.5 Distributional Features of Returns",
    "text": "2.5 Distributional Features of Returns\nWe next examine the empirical distribution of daily returns for FPT. The figure below also marks the historical 5 percent quantile, which provides a simple, non-parametric measure of downside risk.\n\nfrom mizani.formatters import percent_format\nfrom plotnine import geom_histogram, geom_vline, scale_x_continuous\n\n\nfpt_ret = returns.loc[returns[\"symbol\"] == \"FPT\"].copy()\nq05 = fpt_ret[\"ret\"].quantile(0.05)\n\n\n(\n    ggplot(fpt_ret, aes(x=\"ret\"))\n    + geom_histogram(bins=100)\n    + geom_vline(xintercept=q05, linetype=\"dashed\")\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"Distribution of daily FPT returns\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\nFigure 2.2: The dotted vertical line indicates the historical five percent quantile.\n\n\n\n\n\nSummary statistics offer a compact description of return behavior and should always be inspected before formal modeling.\n\nreturns[\"ret\"].describe().round(3)\n\ncount    4305063.000\nmean           0.000\nstd            0.035\nmin           -0.125\n25%           -0.004\n50%            0.000\n75%            0.003\nmax            0.130\nName: ret, dtype: float64\n\n\nComputing these statistics by calendar year can reveal periods of elevated volatility or structural change.\n\n(\n    returns\n    .assign(year=lambda x: x[\"date\"].dt.year)\n    .groupby(\"year\")[\"ret\"]\n    .describe()\n    .round(3)\n)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n\n\n2010\n131548.0\n-0.001\n0.036\n-0.125\n-0.021\n0.0\n0.018\n0.13\n\n\n2011\n166826.0\n-0.003\n0.033\n-0.125\n-0.020\n0.0\n0.011\n0.13\n\n\n2012\n177938.0\n0.000\n0.033\n-0.125\n-0.012\n0.0\n0.015\n0.13\n\n\n2013\n180417.0\n0.001\n0.033\n-0.125\n-0.004\n0.0\n0.008\n0.13\n\n\n2014\n181907.0\n0.001\n0.034\n-0.125\n-0.008\n0.0\n0.011\n0.13\n\n\n2015\n197881.0\n0.000\n0.033\n-0.125\n-0.006\n0.0\n0.005\n0.13\n\n\n2016\n227896.0\n0.000\n0.035\n-0.125\n-0.005\n0.0\n0.003\n0.13\n\n\n2017\n283642.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.001\n0.13\n\n\n2018\n329887.0\n0.000\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2019\n352754.0\n0.000\n0.033\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2020\n369367.0\n0.001\n0.035\n-0.125\n0.000\n0.0\n0.000\n0.13\n\n\n2021\n379415.0\n0.002\n0.038\n-0.125\n-0.005\n0.0\n0.007\n0.13\n\n\n2022\n387050.0\n-0.001\n0.038\n-0.125\n-0.008\n0.0\n0.004\n0.13\n\n\n2023\n391605.0\n0.001\n0.034\n-0.125\n-0.002\n0.0\n0.002\n0.13\n\n\n2024\n400379.0\n0.000\n0.031\n-0.125\n-0.002\n0.0\n0.000\n0.13\n\n\n2025\n146551.0\n0.000\n0.037\n-0.125\n-0.004\n0.0\n0.002\n0.13",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "href": "01_working_with_stock_returns.html#expanding-to-a-market-cross-section",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.6 Expanding to a Market Cross-Section",
    "text": "2.6 Expanding to a Market Cross-Section\nThe same procedures apply naturally to a larger universe of stocks. We now restrict attention to the constituents of the VN30 index.\n\nvn30 = [\n    \"ACB\",\"BCM\",\"BID\",\"BVH\",\"CTG\",\"FPT\",\"GAS\",\"GVR\",\"HDB\",\"HPG\",\n    \"MBB\",\"MSN\",\"MWG\",\"PLX\",\"POW\",\"SAB\",\"SHB\",\"SSB\",\"STB\",\"TCB\",\n    \"TPB\",\"VCB\",\"VHM\",\"VIB\",\"VIC\",\"VJC\",\"VNM\",\"VPB\",\"VRE\",\"EIB\",\n]\n\n\nprices_vn30 = prices.loc[prices[\"symbol\"].isin(vn30)]\nfrom plotnine import theme\n\n\n(\n    ggplot(prices_vn30, aes(x=\"date\", y=\"adjusted_close\", color=\"symbol\"))\n    + geom_line()\n    + labs(title=\"Adjusted prices of VN30 constituents\", x=\"\", y=\"\")\n    + theme(legend_position=\"none\")\n)\n\n\n\n\n\n\n\nFigure 2.3: Prices in VND, adjusted for dividend payments and stock splits.\n\n\n\n\n\nReturns for the VN30 universe are computed analogously.\n\nreturns_vn30 = (\n    prices_vn30\n    .sort_values([\"symbol\", \"date\"])\n    .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted_close\"].pct_change())\n    [[\"symbol\", \"date\", \"ret\"]]\n    .dropna()\n)\n\n\nreturns_vn30.groupby(\"symbol\")[\"ret\"].describe().round(3)\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nsymbol\n\n\n\n\n\n\n\n\n\n\n\n\nACB\n3822.0\n-0.000\n0.023\n-0.407\n-0.006\n0.0\n0.007\n0.097\n\n\nBCM\n1795.0\n0.001\n0.027\n-0.136\n-0.010\n0.0\n0.010\n0.159\n\n\nBID\n2811.0\n0.000\n0.024\n-0.369\n-0.010\n0.0\n0.011\n0.070\n\n\nBVH\n3825.0\n0.000\n0.024\n-0.097\n-0.012\n0.0\n0.012\n0.070\n\n\nCTG\n3825.0\n0.000\n0.024\n-0.376\n-0.010\n0.0\n0.010\n0.070\n\n\nEIB\n3825.0\n-0.000\n0.022\n-0.302\n-0.008\n0.0\n0.008\n0.070\n\n\nFPT\n3825.0\n-0.000\n0.024\n-0.439\n-0.008\n0.0\n0.009\n0.070\n\n\nGAS\n3236.0\n0.000\n0.022\n-0.289\n-0.009\n0.0\n0.010\n0.070\n\n\nGVR\n1775.0\n0.001\n0.030\n-0.137\n-0.014\n0.0\n0.016\n0.169\n\n\nHDB\n1828.0\n-0.001\n0.028\n-0.391\n-0.009\n0.0\n0.010\n0.070\n\n\nHPG\n3825.0\n-0.001\n0.032\n-0.581\n-0.010\n0.0\n0.011\n0.070\n\n\nMBB\n3371.0\n-0.000\n0.023\n-0.473\n-0.008\n0.0\n0.008\n0.069\n\n\nMSN\n3825.0\n0.000\n0.024\n-0.553\n-0.010\n0.0\n0.010\n0.070\n\n\nMWG\n2701.0\n-0.000\n0.035\n-0.751\n-0.009\n0.0\n0.011\n0.070\n\n\nPLX\n2009.0\n-0.000\n0.021\n-0.140\n-0.010\n0.0\n0.010\n0.070\n\n\nPOW\n1784.0\n0.000\n0.023\n-0.071\n-0.012\n0.0\n0.011\n0.102\n\n\nSAB\n2100.0\n-0.000\n0.024\n-0.745\n-0.008\n0.0\n0.007\n0.070\n\n\nSHB\n3824.0\n-0.000\n0.028\n-0.338\n-0.013\n0.0\n0.013\n0.100\n\n\nSSB\n1029.0\n-0.000\n0.023\n-0.292\n-0.005\n0.0\n0.004\n0.070\n\n\nSTB\n3825.0\n0.000\n0.024\n-0.321\n-0.010\n0.0\n0.010\n0.070\n\n\nTCB\n1732.0\n-0.000\n0.035\n-0.884\n-0.009\n0.0\n0.010\n0.070\n\n\nTPB\n1761.0\n-0.001\n0.029\n-0.477\n-0.009\n0.0\n0.009\n0.070\n\n\nVCB\n3825.0\n-0.000\n0.024\n-0.539\n-0.009\n0.0\n0.009\n0.070\n\n\nVHM\n1744.0\n-0.000\n0.024\n-0.419\n-0.009\n0.0\n0.008\n0.070\n\n\nVIB\n2072.0\n-0.000\n0.031\n-0.489\n-0.009\n0.0\n0.010\n0.109\n\n\nVIC\n3825.0\n-0.000\n0.027\n-0.673\n-0.008\n0.0\n0.008\n0.070\n\n\nVJC\n2046.0\n-0.000\n0.020\n-0.455\n-0.007\n0.0\n0.006\n0.070\n\n\nVNM\n3825.0\n-0.000\n0.023\n-0.547\n-0.007\n0.0\n0.007\n0.070\n\n\nVPB\n1927.0\n-0.000\n0.033\n-0.678\n-0.010\n0.0\n0.010\n0.070\n\n\nVRE\n1871.0\n-0.000\n0.024\n-0.295\n-0.012\n0.0\n0.011\n0.070",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#aggregating-returns-across-time",
    "href": "01_working_with_stock_returns.html#aggregating-returns-across-time",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.7 Aggregating Returns Across Time",
    "text": "2.7 Aggregating Returns Across Time\nFinancial variables are observed at different frequencies. While equity prices are recorded daily, many empirical questions require monthly or annual returns. Lower-frequency returns are constructed by compounding higher-frequency observations.\n\nreturns_monthly = (\n    returns_vn30\n    .assign(month=lambda x: x[\"date\"].dt.to_period(\"M\").dt.to_timestamp())\n    .groupby([\"symbol\", \"month\"], as_index=False)\n    .agg(ret=(\"ret\", lambda x: np.prod(1 + x) - 1))\n)\n\nComparing daily and monthly return distributions illustrates how aggregation dampens volatility and alters tail behavior.\n\nfrom plotnine import facet_wrap\n\nfpt_d = returns_vn30.loc[returns_vn30[\"symbol\"] == \"FPT\"].assign(freq=\"Daily\")\nfpt_m = returns_monthly.loc[returns_monthly[\"symbol\"] == \"FPT\"].assign(freq=\"Monthly\")\n\n\nfpt_both = pd.concat([\n    fpt_d[[\"ret\", \"freq\"]],\n    fpt_m[[\"ret\", \"freq\"]],\n])\n\n\n(\n    ggplot(fpt_both, aes(x=\"ret\"))\n    + geom_histogram(bins=50)\n    + scale_x_continuous(labels=percent_format())\n    + labs(title=\"FPT returns at different frequencies\", x=\"\", y=\"\")\n    + facet_wrap(\"freq\", scales=\"free\")\n)\n\n\n\n\n\n\n\nFigure 2.4: Returns are based on prices adjusted for dividend payments and stock splits.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "href": "01_working_with_stock_returns.html#aggregation-across-firms-trading-activity",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.8 Aggregation Across Firms: Trading Activity",
    "text": "2.8 Aggregation Across Firms: Trading Activity\nAggregation is not limited to time. In some settings, it is informative to aggregate variables across firms. As an illustration, we compute total daily trading value for VN30 stocks by multiplying share volume by adjusted prices and summing across firms.\n\ntrading_value = (\n    prices_vn30\n    .assign(value=lambda x: x[\"volume\"] * x[\"adjusted_close\"] / 1e9)\n    .groupby(\"date\")[\"value\"]\n    .sum()\n    .reset_index()\n    .assign(value_lag=lambda x: x[\"value\"].shift(1))\n)\n(\n    ggplot(trading_value, aes(x=\"date\", y=\"value\"))\n    + geom_line()\n    + labs(title=\"Aggregate VN30 trading value (billion VND)\", x=\"\", y=\"\")\n)\n\n\n\n\n\n\n\n\nFinally, we assess persistence in trading activity by comparing trading value on consecutive days.\n\nfrom plotnine import geom_point, geom_abline\n\n\n(\n    ggplot(trading_value, aes(x=\"value_lag\", y=\"value\"))\n    + geom_point()\n    + geom_abline(intercept=0, slope=1, linetype=\"dashed\")\n    + labs(\n        title=\"Persistence in VN30 trading value\",\n        x=\"Previous day\",\n        y=\"Current day\",\n    )\n)\n\n\n\n\n\n\n\nFigure 2.5: Total daily trading volume.\n\n\n\n\n\nA strong alignment along the 45-degree line indicates that high-activity trading days tend to be followed by similarly active days, a common empirical regularity in equity markets.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "01_working_with_stock_returns.html#summary",
    "href": "01_working_with_stock_returns.html#summary",
    "title": "2  Constructing and Analyzing Equity Return Series",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nThis chapter established a reproducible workflow for transforming raw price data into return series, diagnosing common data issues, and aggregating information across time and firms. These steps provide the empirical foundation for subsequent analyses of risk, return predictability, and market dynamics in Vietnam’s equity market.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Constructing and Analyzing Equity Return Series</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html",
    "href": "16_institutional_ownership.html",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "",
    "text": "13.1 Institutional Ownership in Vietnam: A Distinct Landscape\nVietnam’s equity market presents a fundamentally different institutional ownership landscape from the mature markets of the US, Europe, or Japan. Since the Ho Chi Minh City Securities Trading Center (now HOSE) opened on July 28, 2000 with just two listed stocks, the market has grown to over 1,700 listed companies across three exchanges (HOSE, HNX, and UPCOM) with a combined market capitalization exceeding 200 billion USD. Yet the ownership structure remains distinctive in several critical ways:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#institutional-ownership-in-vietnam-a-distinct-landscape",
    "href": "16_institutional_ownership.html#institutional-ownership-in-vietnam-a-distinct-landscape",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "",
    "text": "Retail dominance. Individual investors account for approximately 85% of trading value on Vietnamese exchanges, far exceeding the institutional share. This contrasts sharply with the US, where institutional investors dominate both ownership and trading (Bao Dinh and Tran 2024). The implications for market efficiency, price discovery, and volatility are profound.\nState ownership legacy. Vietnam’s equitization (privatization) program, initiated under Đổi Mới reforms in 1986, means that the state remains a significant or controlling shareholder in many listed companies. As of 2022, SOEs (firms with state ownership &gt; 50%) account for approximately 30% of total market capitalization despite representing less than 10% of listed firms (Huang, Liu, and Shu 2023). State ownership introduces unique agency problems, governance dynamics, and liquidity constraints.\nForeign Ownership Limits (FOLs). Vietnam imposes sector-specific caps on aggregate foreign ownership, typically 49% for most sectors, 30% for banking, and varying limits for aviation, media, and telecommunications. When a stock reaches its FOL, foreign investors can only buy from other foreign sellers, creating a segmented market with distinct pricing dynamics and a well-documented “FOL premium” (Vo 2015).\nDisclosure regime. Unlike the US quarterly 13F filing system, Vietnam’s ownership disclosure is event-driven and periodic. Major shareholders (≥5%) must disclose within 7 business days of crossing thresholds. Annual reports contain detailed shareholder registers. Semi-annual fund reports provide portfolio snapshots. This creates a patchwork of disclosure frequencies that require careful handling.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-datacore",
    "href": "16_institutional_ownership.html#sec-datacore",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.2 Data Infrastructure: DataCore.vn",
    "text": "13.2 Data Infrastructure: DataCore.vn\nDataCore.vn is a comprehensive Vietnamese financial data platform that provides academic-grade datasets for the Vietnamese market. Throughout this chapter, we assume all data is sourced exclusively from DataCore.vn, which provides:\n\n\n\nTable 13.1: DataCore.vn Data Tables Used in This Chapter\n\n\n\n\n\n\n\n\n\n\nDataCore.vn Dataset\nContent\nKey Variables\n\n\n\n\nStock Prices\nDaily/monthly OHLCV for HOSE, HNX, UPCOM\nticker, date, close, adjusted_close, volume, shares_outstanding\n\n\nOwnership Structure\nShareholder composition snapshots\nticker, date, shareholder_name, shares_held, ownership_pct, shareholder_type\n\n\nMajor Shareholders\nDetailed ≥5% holders\nticker, date, shareholder_name, shares_held, is_foreign, is_state, is_institution\n\n\nCorporate Actions\nDividends, stock splits, bonus shares, rights issues\nticker, ex_date, action_type, ratio, record_date\n\n\nCompany Profile\nSector, exchange, listing date, charter capital\nticker, exchange, industry_code, listing_date, fol_limit\n\n\nFinancial Statements\nQuarterly/annual financials\nticker, period, revenue, net_income, total_assets, equity\n\n\nForeign Ownership\nDaily foreign ownership tracking\nticker, date, foreign_shares, foreign_pct, fol_limit, foreign_room\n\n\nFund Holdings\nSemi-annual fund portfolio disclosures\nfund_name, report_date, ticker, shares_held, market_value\n\n\n\n\n\n\n\nclass DataCoreReader:\n    \"\"\"\n    Unified data reader for DataCore.vn datasets.\n    \n    Assumes data has been downloaded from DataCore.vn and stored locally.\n    Supports both Parquet (recommended for performance) and CSV formats.\n    \n    Parameters\n    ----------\n    data_dir : str or Path\n        Root directory containing DataCore.vn data files\n    file_format : str\n        'parquet' or 'csv' (default: 'parquet')\n    \"\"\"\n    \n    # Expected file names in the data directory\n    FILE_MAP = {\n        'prices': 'stock_prices',\n        'ownership': 'ownership_structure',\n        'major_shareholders': 'major_shareholders',\n        'corporate_actions': 'corporate_actions',\n        'company_profile': 'company_profile',\n        'financials': 'financial_statements',\n        'foreign_ownership': 'foreign_ownership_daily',\n        'fund_holdings': 'fund_holdings',\n    }\n    \n    def __init__(self, data_dir: Union[str, Path], file_format: str = 'parquet'):\n        self.data_dir = Path(data_dir)\n        self.fmt = file_format\n        self._cache = {}\n        \n        # Verify data directory exists\n        if not self.data_dir.exists():\n            raise FileNotFoundError(\n                f\"Data directory not found: {self.data_dir}\\n\"\n                f\"Please download data from DataCore.vn and place it in this directory.\"\n            )\n        \n        print(f\"DataCore.vn reader initialized: {self.data_dir}\")\n        available = [f.stem for f in self.data_dir.glob(f'*.{self.fmt}')]\n        print(f\"Available datasets: {available}\")\n    \n    def _read(self, key: str) -&gt; pd.DataFrame:\n        \"\"\"Read and cache a dataset.\"\"\"\n        if key in self._cache:\n            return self._cache[key]\n        \n        fname = self.FILE_MAP.get(key, key)\n        filepath = self.data_dir / f\"{fname}.{self.fmt}\"\n        \n        if not filepath.exists():\n            raise FileNotFoundError(\n                f\"Dataset not found: {filepath}\\n\"\n                f\"Expected file: {fname}.{self.fmt} in {self.data_dir}\"\n            )\n        \n        if self.fmt == 'parquet':\n            df = pd.read_parquet(filepath)\n        else:\n            df = pd.read_csv(filepath, parse_dates=True)\n        \n        # Auto-detect and parse date columns\n        for col in df.columns:\n            if 'date' in col.lower() or col.lower() in ['period', 'ex_date', 'record_date']:\n                try:\n                    df[col] = pd.to_datetime(df[col])\n                except (ValueError, TypeError):\n                    pass\n        \n        self._cache[key] = df\n        print(f\"Loaded {key}: {len(df):,} rows, {len(df.columns)} columns\")\n        return df\n    \n    @property\n    def prices(self) -&gt; pd.DataFrame:\n        return self._read('prices')\n    \n    @property\n    def ownership(self) -&gt; pd.DataFrame:\n        return self._read('ownership')\n    \n    @property\n    def major_shareholders(self) -&gt; pd.DataFrame:\n        return self._read('major_shareholders')\n    \n    @property\n    def corporate_actions(self) -&gt; pd.DataFrame:\n        return self._read('corporate_actions')\n    \n    @property\n    def company_profile(self) -&gt; pd.DataFrame:\n        return self._read('company_profile')\n    \n    @property\n    def financials(self) -&gt; pd.DataFrame:\n        return self._read('financials')\n    \n    @property\n    def foreign_ownership(self) -&gt; pd.DataFrame:\n        return self._read('foreign_ownership')\n    \n    @property\n    def fund_holdings(self) -&gt; pd.DataFrame:\n        return self._read('fund_holdings')\n    \n    def clear_cache(self):\n        \"\"\"Clear all cached datasets to free memory.\"\"\"\n        self._cache.clear()\n\n# Initialize reader — adjust path to your local DataCore.vn data\n# dc = DataCoreReader('/path/to/datacore_data', file_format='parquet')\n\nThis chapter proceeds as follows. Section 13.3 builds the complete data pipeline from raw DataCore.vn extracts to clean, analysis-ready datasets, with particular attention to corporate action adjustments. Section 13.4 defines Vietnam’s unique ownership taxonomy. Section 13.5 computes institutional ownership ratios, concentration, and breadth for the Vietnamese market. Section 13.6 develops specialized foreign ownership analytics including FOL utilization and room premium. Section 13.7 derives institutional trades from ownership disclosure snapshots. Section 13.8 computes fund-level flows and turnover. Section 13.9 analyzes state ownership dynamics. Section 13.10 introduces network analysis, ML classification, and event-study frameworks. Section 13.11 presents complete empirical applications, and Section 13.12 concludes.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-data-pipeline",
    "href": "16_institutional_ownership.html#sec-data-pipeline",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.3 Data Pipeline",
    "text": "13.3 Data Pipeline\n\n13.3.1 Stock Price Data and Corporate Action Adjustments\nVietnam’s equity market is notorious for frequent corporate actions, particularly stock dividends and bonus share issuances, that dramatically alter share counts. A company issuing a 30% stock dividend means every 100 shares become 130 shares, and the reference price adjusts downward proportionally. Failure to properly adjust historical shares and prices for these events is the single most common source of error in Vietnamese equity research.\n\n# ============================================================================\n# Step 1: Corporate Action Adjustment Factors\n# ============================================================================\n\ndef build_adjustment_factors(corporate_actions: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Build cumulative adjustment factors from the corporate actions history.\n    \n    In Vietnam, the most common share-altering corporate actions are:\n    1. Stock dividends (cổ tức bằng cổ phiếu): e.g., 30% → ratio = 0.30\n       Effect: shares × (1 + 0.30), price × (1 / 1.30)\n    2. Bonus shares (thưởng cổ phiếu): mechanically identical to stock dividends\n    3. Stock splits (chia tách): e.g., 2:1 → ratio = 2.0\n       Effect: shares × 2, price × 0.5\n    4. Rights issues (phát hành thêm): dilutive, but not all shareholders exercise\n       We approximate with the subscription ratio\n    5. Reverse splits (gộp cổ phiếu): rare in Vietnam\n       Effect: shares ÷ ratio, price × ratio\n    \n    We construct a FORWARD-LOOKING cumulative adjustment factor such that:\n       adjusted_shares = raw_shares × cum_adj_factor(from_date, to_date)\n       adjusted_price = raw_price / cum_adj_factor(from_date, to_date)\n    \n    This is analogous to CRSP's cfacshr in the US context.\n    \n    Parameters\n    ----------\n    corporate_actions : pd.DataFrame\n        DataCore.vn corporate actions with columns:\n        ticker, ex_date, action_type, ratio\n        \n        action_type values:\n        - 'stock_dividend': ratio = dividend rate (e.g., 0.30 for 30%)\n        - 'bonus_shares': ratio = bonus rate (e.g., 0.20 for 20%)\n        - 'stock_split': ratio = split factor (e.g., 2.0 for 2:1)\n        - 'reverse_split': ratio = merge factor (e.g., 5.0 for 5:1 merge)\n        - 'rights_issue': ratio = subscription rate (e.g., 0.10 for 10:1)\n        - 'cash_dividend': ratio = VND per share (no share adjustment needed)\n    \n    Returns\n    -------\n    pd.DataFrame\n        Adjustment factors: ticker, ex_date, point_factor, cum_factor\n    \"\"\"\n    # Filter to share-altering events only\n    share_events = ['stock_dividend', 'bonus_shares', 'stock_split', \n                    'reverse_split', 'rights_issue']\n    ca = corporate_actions[\n        corporate_actions['action_type'].isin(share_events)\n    ].copy()\n    \n    if len(ca) == 0:\n        print(\"No share-altering corporate actions found.\")\n        return pd.DataFrame(columns=['ticker', 'ex_date', 'point_factor', 'cum_factor'])\n    \n    # Compute point adjustment factor for each event\n    def compute_point_factor(row):\n        atype = row['action_type']\n        ratio = row['ratio']\n        \n        if atype in ['stock_dividend', 'bonus_shares']:\n            # 30% stock dividend: 100 shares → 130 shares\n            return 1 + ratio\n        elif atype == 'stock_split':\n            # 2:1 split: 100 shares → 200 shares\n            return ratio\n        elif atype == 'reverse_split':\n            # 5:1 reverse: 500 shares → 100 shares\n            return 1.0 / ratio\n        elif atype == 'rights_issue':\n            # Approximate: assume all rights exercised\n            # In practice, this overestimates the adjustment\n            return 1 + ratio\n        else:\n            return 1.0\n    \n    ca['point_factor'] = ca.apply(compute_point_factor, axis=1)\n    \n    # Sort chronologically within each ticker\n    ca = ca.sort_values(['ticker', 'ex_date']).reset_index(drop=True)\n    \n    # Cumulative factor: product of all point factors from listing to date\n    # This gives us a running \"total adjustment\" for each ticker\n    ca['cum_factor'] = ca.groupby('ticker')['point_factor'].cumprod()\n    \n    # Summary statistics\n    n_tickers = ca['ticker'].nunique()\n    n_events = len(ca)\n    avg_events = n_events / n_tickers if n_tickers &gt; 0 else 0\n    \n    print(f\"Corporate action adjustment factors built:\")\n    print(f\"  Tickers with adjustments: {n_tickers:,}\")\n    print(f\"  Total share-altering events: {n_events:,}\")\n    print(f\"  Average events per ticker: {avg_events:.1f}\")\n    print(f\"\\nEvent type distribution:\")\n    print(ca['action_type'].value_counts().to_string())\n    \n    return ca[['ticker', 'ex_date', 'action_type', 'ratio', \n               'point_factor', 'cum_factor']]\n\n\ndef adjust_shares(shares: float, ticker: str, from_date, to_date, \n                  adj_factors: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Adjust a share count from one date to another for corporate actions.\n    \n    Example: If a company had a 30% stock dividend with ex_date between\n    from_date and to_date, then 1000 shares at from_date = 1300 shares \n    at to_date.\n    \n    Parameters\n    ----------\n    shares : float\n        Number of shares at from_date\n    ticker : str\n        Stock ticker\n    from_date, to_date : pd.Timestamp\n        Period for adjustment\n    adj_factors : pd.DataFrame\n        Output of build_adjustment_factors()\n    \n    Returns\n    -------\n    float\n        Adjusted shares at to_date\n    \"\"\"\n    events = adj_factors[\n        (adj_factors['ticker'] == ticker) &\n        (adj_factors['ex_date'] &gt; pd.Timestamp(from_date)) &\n        (adj_factors['ex_date'] &lt;= pd.Timestamp(to_date))\n    ]\n    \n    if len(events) == 0:\n        return shares\n    \n    total_factor = events['point_factor'].prod()\n    return shares * total_factor\n\n\n# Example usage:\n# adj_factors = build_adjustment_factors(dc.corporate_actions)\n\n\n\n\n\n\n\nImportantThe Stock Dividend Problem in Vietnam\n\n\n\nVietnamese companies issue stock dividends with remarkable frequency, many growth companies do so 2-3 times per year. Consider Vinhomes (VHM) or FPT Corporation: their share counts may double or triple over a 5-year period purely from stock dividends. If you compare raw ownership shares from 2019 to 2024 without adjustment, you will obtain nonsensical ownership ratios. Every time-series analysis of Vietnamese ownership data must use adjusted shares. This is the Vietnamese equivalent of the CRSP cfacshr adjustment factor problem in US data, but more severe because the events are more frequent and larger in magnitude.\n\n\n\n# ============================================================================\n# Step 2: Process Stock Price Data\n# ============================================================================\n\ndef process_price_data(prices: pd.DataFrame, \n                       adj_factors: pd.DataFrame,\n                       company_profile: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Process DataCore.vn stock price data:\n    1. Align dates to month-end and quarter-end\n    2. Merge company metadata (exchange, sector, FOL limit)\n    3. Compute adjusted prices and shares outstanding\n    4. Compute market capitalization\n    5. Create quarter-end snapshots\n    \n    Parameters\n    ----------\n    prices : pd.DataFrame\n        Daily/monthly price data from DataCore.vn\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors\n    company_profile : pd.DataFrame\n        Company metadata including exchange, sector, FOL\n    \n    Returns\n    -------\n    pd.DataFrame\n        Quarter-end processed stock data\n    \"\"\"\n    df = prices.copy()\n    \n    # Standardize date\n    df['date'] = pd.to_datetime(df['date'])\n    df['month_end'] = df['date'] + pd.offsets.MonthEnd(0)\n    df['quarter_end'] = df['date'] + pd.offsets.QuarterEnd(0)\n    \n    # Merge company profile\n    profile_cols = ['ticker', 'exchange', 'industry_code', 'fol_limit', \n                    'listing_date', 'company_name']\n    profile_cols = [c for c in profile_cols if c in company_profile.columns]\n    df = df.merge(company_profile[profile_cols], on='ticker', how='left')\n    \n    # Build cumulative adjustment factor for each ticker-date\n    # For each observation, compute the total adjustment from listing to that date\n    df = df.sort_values(['ticker', 'date'])\n    \n    # Merge adjustment events\n    # For each ticker-date, find the cumulative factor as of that date\n    def get_cum_factor_at_date(group):\n        ticker = group.name\n        ticker_adj = adj_factors[adj_factors['ticker'] == ticker].copy()\n        \n        if len(ticker_adj) == 0:\n            group['cum_adj_factor'] = 1.0\n            return group\n        \n        # For each date, find cumulative factor (product of all events up to that date)\n        group = group.sort_values('date')\n        group['cum_adj_factor'] = 1.0\n        \n        for _, event in ticker_adj.iterrows():\n            mask = group['date'] &gt;= event['ex_date']\n            group.loc[mask, 'cum_adj_factor'] *= event['point_factor']\n        \n        return group\n    \n    df = df.groupby('ticker', group_keys=False).apply(get_cum_factor_at_date)\n    \n    # Adjusted price and shares\n    # adjusted_close should already be provided by DataCore.vn\n    # But we compute our own for consistency\n    if 'adjusted_close' not in df.columns:\n        df['adjusted_close'] = df['close'] / df['cum_adj_factor']\n    \n    # Adjusted shares outstanding\n    df['adjusted_shares'] = df['shares_outstanding'] * df['cum_adj_factor']\n    \n    # Market capitalization (in billion VND)\n    df['market_cap'] = df['close'] * df['shares_outstanding'] / 1e9\n    \n    # Monthly returns\n    df = df.sort_values(['ticker', 'date'])\n    df['ret'] = df.groupby('ticker')['adjusted_close'].pct_change()\n    \n    # Keep quarter-end observations\n    # For daily data: keep last trading day of each quarter\n    df_quarterly = (df.sort_values(['ticker', 'quarter_end', 'date'])\n                      .groupby(['ticker', 'quarter_end'])\n                      .last()\n                      .reset_index())\n    \n    print(f\"Processed price data:\")\n    print(f\"  Total records (daily): {len(df):,}\")\n    print(f\"  Quarter-end records: {len(df_quarterly):,}\")\n    print(f\"  Unique tickers: {df_quarterly['ticker'].nunique():,}\")\n    print(f\"  Date range: {df_quarterly['quarter_end'].min()} to \"\n          f\"{df_quarterly['quarter_end'].max()}\")\n    print(f\"\\nExchange distribution:\")\n    print(df_quarterly.groupby('exchange')['ticker'].nunique().to_string())\n    \n    return df_quarterly\n\n# prices_q = process_price_data(dc.prices, adj_factors, dc.company_profile)\n\n\n\n13.3.2 Ownership Structure Data\nVietnamese ownership data from DataCore.vn captures the composition of shareholders as disclosed in annual reports, semi-annual reports, and event-driven disclosures. The key distinction from US 13F data is that Vietnamese disclosures provide a complete ownership decomposition, not just institutional long positions, but the full breakdown into state, institutional, foreign, and individual ownership.\n\n# ============================================================================\n# Step 3: Process Ownership Structure Data\n# ============================================================================\n\nclass OwnershipType:\n    \"\"\"\n    Vietnam's ownership taxonomy.\n    \n    Unlike the US where 13F captures only institutional long positions,\n    Vietnamese disclosure provides a complete ownership decomposition.\n    We classify shareholders into five mutually exclusive categories.\n    \"\"\"\n    STATE = 'state'                    # Nhà nước (government entities, SOE parents)\n    FOREIGN_INST = 'foreign_inst'      # Tổ chức nước ngoài\n    DOMESTIC_INST = 'domestic_inst'    # Tổ chức trong nước (non-state)\n    INDIVIDUAL = 'individual'          # Cá nhân\n    TREASURY = 'treasury'              # Cổ phiếu quỹ\n    \n    ALL_TYPES = [STATE, FOREIGN_INST, DOMESTIC_INST, INDIVIDUAL, TREASURY]\n    INSTITUTIONAL = [STATE, FOREIGN_INST, DOMESTIC_INST]\n    FOREIGN = [FOREIGN_INST]  # Can be expanded if foreign individuals are tracked\n\n\ndef classify_shareholders(ownership: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Classify shareholders into Vietnam's ownership taxonomy.\n    \n    DataCore.vn may provide a `shareholder_type` field, but naming \n    conventions vary. This function standardizes the classification \n    using a combination of provided flags and name-based heuristics.\n    \n    The classification challenge in Vietnam (noted by @huang2023factors):\n    DataCore.vn may not always cleanly separate institution types, so we \n    use a cascading approach:\n    1. Use explicit flags (is_state, is_foreign, is_institution) if available\n    2. Apply name-based heuristics for Vietnamese entity names\n    3. Default to 'individual' for unclassified shareholders\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Raw ownership data from DataCore.vn\n    \n    Returns\n    -------\n    pd.DataFrame\n        Ownership data with standardized `owner_type` column\n    \"\"\"\n    df = ownership.copy()\n    \n    # --- Method 1: Use explicit flags if available ---\n    if all(col in df.columns for col in ['is_state', 'is_foreign', 'is_institution']):\n        conditions = [\n            (df['is_state'] == True),\n            (df['is_foreign'] == True) & (df['is_institution'] == True),\n            (df['is_foreign'] == True) & (df['is_institution'] != True),\n            (df['is_institution'] == True) & (df['is_state'] != True) & \n                (df['is_foreign'] != True),\n        ]\n        choices = [\n            OwnershipType.STATE,\n            OwnershipType.FOREIGN_INST,\n            OwnershipType.FOREIGN_INST,  # Foreign individuals often grouped\n            OwnershipType.DOMESTIC_INST,\n        ]\n        df['owner_type'] = np.select(conditions, choices, \n                                      default=OwnershipType.INDIVIDUAL)\n    \n    # --- Method 2: Name-based heuristics ---\n    elif 'shareholder_name' in df.columns:\n        name = df['shareholder_name'].str.lower().fillna('')\n        \n        # State entities: government ministries, SCIC, state corporations\n        state_keywords = [\n            'bộ tài chính', 'tổng công ty đầu tư', 'scic', \n            'ủy ban nhân dân', 'nhà nước', 'state capital',\n            'tổng công ty', 'vốn nhà nước', 'bộ công thương',\n            'bộ quốc phòng', 'bộ giao thông', 'vinashin',\n        ]\n        is_state = name.apply(\n            lambda x: any(kw in x for kw in state_keywords)\n        )\n        \n        # Foreign entities: common fund names, foreign company patterns\n        foreign_keywords = [\n            'fund', 'investment', 'capital', 'limited', 'ltd', 'inc',\n            'corporation', 'holdings', 'asset management', 'pte',\n            'gmbh', 'management', 'partners', 'advisors',\n            'dragon capital', 'vinacapital', 'templeton', \n            'blackrock', 'jpmorgan', 'samsung', 'mirae',\n        ]\n        # Also check for non-Vietnamese characters as a heuristic\n        is_foreign_name = name.apply(\n            lambda x: any(kw in x for kw in foreign_keywords)\n        )\n        \n        # Domestic institutions: Vietnamese bank, securities, insurance names\n        domestic_inst_keywords = [\n            'ngân hàng', 'chứng khoán', 'bảo hiểm', 'quỹ đầu tư',\n            'công ty quản lý', 'bảo việt', 'techcombank', 'vietcombank',\n            'bidv', 'vietinbank', 'vpbank', 'mb bank', 'ssi', 'hsc',\n            'vcsc', 'vndirect', 'fpt capital', 'manulife',\n        ]\n        is_domestic_inst = name.apply(\n            lambda x: any(kw in x for kw in domestic_inst_keywords)\n        )\n        \n        # Treasury shares\n        is_treasury = name.str.contains('cổ phiếu quỹ|treasury', case=False)\n        \n        # Apply classification cascade\n        df['owner_type'] = OwnershipType.INDIVIDUAL  # Default\n        df.loc[is_domestic_inst, 'owner_type'] = OwnershipType.DOMESTIC_INST\n        df.loc[is_foreign_name, 'owner_type'] = OwnershipType.FOREIGN_INST\n        df.loc[is_state, 'owner_type'] = OwnershipType.STATE\n        df.loc[is_treasury, 'owner_type'] = OwnershipType.TREASURY\n    \n    # --- Method 3: Use shareholder_type directly ---\n    elif 'shareholder_type' in df.columns:\n        type_map = {\n            'state': OwnershipType.STATE,\n            'foreign_institution': OwnershipType.FOREIGN_INST,\n            'foreign_individual': OwnershipType.FOREIGN_INST,\n            'domestic_institution': OwnershipType.DOMESTIC_INST,\n            'individual': OwnershipType.INDIVIDUAL,\n            'treasury': OwnershipType.TREASURY,\n        }\n        df['owner_type'] = df['shareholder_type'].str.lower().map(type_map)\n        df['owner_type'] = df['owner_type'].fillna(OwnershipType.INDIVIDUAL)\n    \n    else:\n        raise ValueError(\n            \"Cannot classify shareholders. Expected one of:\\n\"\n            \"  1. Columns: is_state, is_foreign, is_institution\\n\"\n            \"  2. Column: shareholder_name (for heuristic classification)\\n\"\n            \"  3. Column: shareholder_type (pre-classified)\"\n        )\n    \n    # Summary\n    print(\"Ownership classification results:\")\n    print(df['owner_type'].value_counts().to_string())\n    \n    return df\n\n# ownership_classified = classify_shareholders(dc.ownership)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-ownership-taxonomy",
    "href": "16_institutional_ownership.html#sec-ownership-taxonomy",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.4 Vietnam’s Ownership Taxonomy",
    "text": "13.4 Vietnam’s Ownership Taxonomy\n\n13.4.1 The Five Ownership Categories\nVietnam’s ownership structure is decomposed into five mutually exclusive categories that together sum to 100% of shares outstanding:\n\n\n\nTable 13.2: Vietnam’s Ownership Taxonomy\n\n\n\n\n\n\n\n\n\n\n\nCategory\nVietnamese Term\nDescription\nTypical Share (2020s)\n\n\n\n\nState\nSở hữu Nhà nước\nGovernment entities, SCIC, SOE parent companies\n~15-25% of market cap\n\n\nForeign Institutional\nTổ chức nước ngoài\nForeign funds, banks, corporations\n~15-20%\n\n\nDomestic Institutional\nTổ chức trong nước\nVietnamese funds, banks, insurance, securities firms\n~5-10%\n\n\nIndividual\nCá nhân\nRetail investors (both Vietnamese and foreign individuals)\n~55-65%\n\n\nTreasury\nCổ phiếu quỹ\nCompany’s own repurchased shares\n~0-2%\n\n\n\n\n\n\nThis taxonomy differs fundamentally from the US 13F framework in several ways:\n\nCompleteness: We observe 100% of ownership, not just institutional long positions above $100 million AUM.\nState as a category: State ownership is a first-class analytical category, not subsumed under “All Others” as in the LSEG type code system.\nIndividual visibility: We observe aggregate individual ownership directly, whereas in the US, individual ownership is merely the residual (100% − institutional ownership).\nNo short position ambiguity: Vietnam’s market has very limited short-selling infrastructure, so ownership data genuinely represents long positions.\n\n\n# ============================================================================\n# Step 4: Compute Ownership Decomposition\n# ============================================================================\n\ndef compute_ownership_decomposition(ownership: pd.DataFrame,\n                                     prices_q: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the full ownership decomposition for each stock at each \n    disclosure date.\n    \n    For each stock-date combination, aggregates shares held by each \n    ownership category and computes ownership ratios relative to \n    total shares outstanding.\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership data (output of classify_shareholders)\n    prices_q : pd.DataFrame\n        Quarter-end price data with shares_outstanding\n    \n    Returns\n    -------\n    pd.DataFrame\n        Stock-period level ownership decomposition with columns for\n        each ownership type's share count and percentage\n    \"\"\"\n    # Aggregate shares by ticker, date, and owner type\n    agg = (ownership.groupby(['ticker', 'date', 'owner_type'])['shares_held']\n                    .sum()\n                    .reset_index())\n    \n    # Pivot to wide format: one column per ownership type\n    wide = agg.pivot_table(\n        index=['ticker', 'date'],\n        columns='owner_type',\n        values='shares_held',\n        fill_value=0\n    ).reset_index()\n    \n    # Rename columns\n    type_cols = [c for c in wide.columns if c in OwnershipType.ALL_TYPES]\n    rename_map = {t: f'shares_{t}' for t in type_cols}\n    wide = wide.rename(columns=rename_map)\n    \n    # Total institutional shares\n    inst_cols = [f'shares_{t}' for t in OwnershipType.INSTITUTIONAL \n                 if f'shares_{t}' in wide.columns]\n    wide['shares_institutional'] = wide[inst_cols].sum(axis=1)\n    \n    # Total foreign shares (for FOL tracking)\n    foreign_cols = [f'shares_{t}' for t in OwnershipType.FOREIGN \n                    if f'shares_{t}' in wide.columns]\n    wide['shares_foreign_total'] = wide[foreign_cols].sum(axis=1)\n    \n    # Align with quarter-end dates for merging with price data\n    wide['quarter_end'] = wide['date'] + pd.offsets.QuarterEnd(0)\n    \n    # Merge with price data to get shares outstanding\n    merged = wide.merge(\n        prices_q[['ticker', 'quarter_end', 'shares_outstanding', \n                  'adjusted_shares', 'market_cap', 'exchange', \n                  'industry_code', 'fol_limit', 'close']],\n        on=['ticker', 'quarter_end'],\n        how='left'\n    )\n    \n    # Compute ownership ratios\n    tso = merged['shares_outstanding']\n    for col in merged.columns:\n        if col.startswith('shares_') and col != 'shares_outstanding':\n            ratio_col = col.replace('shares_', 'pct_')\n            merged[ratio_col] = merged[col] / tso\n            merged.loc[tso &lt;= 0, ratio_col] = np.nan\n    \n    # Derived measures\n    merged['pct_free_float'] = 1 - merged.get('pct_state', 0) - merged.get('pct_treasury', 0)\n    \n    # SOE flag: state ownership &gt; 50%\n    merged['is_soe'] = (merged.get('pct_state', 0) &gt; 0.50).astype(int)\n    \n    # FOL utilization\n    if 'fol_limit' in merged.columns and 'pct_foreign_total' in merged.columns:\n        merged['fol_utilization'] = merged['pct_foreign_total'] / merged['fol_limit']\n        merged['foreign_room'] = merged['fol_limit'] - merged['pct_foreign_total']\n        merged.loc[merged['fol_limit'] &lt;= 0, ['fol_utilization', 'foreign_room']] = np.nan\n    \n    # Number of institutional owners (breadth)\n    n_owners = (ownership[ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)]\n                .groupby(['ticker', 'date'])['shareholder_name']\n                .nunique()\n                .reset_index()\n                .rename(columns={'shareholder_name': 'n_inst_owners'}))\n    \n    n_foreign_owners = (ownership[ownership['owner_type'] == OwnershipType.FOREIGN_INST]\n                        .groupby(['ticker', 'date'])['shareholder_name']\n                        .nunique()\n                        .reset_index()\n                        .rename(columns={'shareholder_name': 'n_foreign_owners'}))\n    \n    merged = merged.merge(n_owners, on=['ticker', 'date'], how='left')\n    merged = merged.merge(n_foreign_owners, on=['ticker', 'date'], how='left')\n    merged[['n_inst_owners', 'n_foreign_owners']] = (\n        merged[['n_inst_owners', 'n_foreign_owners']].fillna(0)\n    )\n    \n    print(f\"Ownership decomposition computed:\")\n    print(f\"  Stock-period observations: {len(merged):,}\")\n    print(f\"  Unique tickers: {merged['ticker'].nunique():,}\")\n    print(f\"\\nMean ownership structure:\")\n    pct_cols = [c for c in merged.columns if c.startswith('pct_')]\n    print(merged[pct_cols].mean().round(4).to_string())\n    \n    return merged\n\n# ownership_decomp = compute_ownership_decomposition(\n#     ownership_classified, prices_q\n# )",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-ownership-metrics",
    "href": "16_institutional_ownership.html#sec-ownership-metrics",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.5 Institutional Ownership Measures",
    "text": "13.5 Institutional Ownership Measures\n\n13.5.1 Ownership Ratio\nThe Institutional Ownership Ratio (IOR) for stock \\(i\\) at time \\(t\\) in Vietnam is:\n\\[\nIOR_{i,t} = \\frac{S_{i,t}^{state} + S_{i,t}^{foreign\\_inst} + S_{i,t}^{domestic\\_inst}}{TSO_{i,t}}\n\\tag{13.1}\\]\nwhere \\(S_{i,t}^{type}\\) denotes adjusted shares held by each ownership category and \\(TSO_{i,t}\\) is total shares outstanding. Unlike the US where the IOR can exceed 100% due to long-only reporting and short selling, the Vietnamese IOR is bounded by construction in \\([0, 1]\\) because we observe the complete ownership decomposition.\nWe also compute category-specific ownership ratios:\n\\[\nIOR_{i,t}^{foreign} = \\frac{S_{i,t}^{foreign\\_inst}}{TSO_{i,t}}, \\quad\nIOR_{i,t}^{state} = \\frac{S_{i,t}^{state}}{TSO_{i,t}}, \\quad\nIOR_{i,t}^{domestic} = \\frac{S_{i,t}^{domestic\\_inst}}{TSO_{i,t}}\n\\tag{13.2}\\]\n\n\n13.5.2 Concentration: Herfindahl-Hirschman Index\nThe Institutional Ownership Concentration via the Herfindahl-Hirschman Index is:\n\\[\nIOC_{i,t}^{HHI} = \\sum_{j=1}^{N_{i,t}} \\left(\\frac{S_{i,j,t}}{\\sum_{k=1}^{N_{i,t}} S_{i,k,t}}\\right)^2\n\\tag{13.3}\\]\nIn Vietnam, the HHI is particularly informative because it captures the dominance of state shareholders. A company where the government holds 65% will have a mechanically high HHI even if the remaining 35% is diversely held.\nWe therefore compute separate HHI measures for different ownership categories:\n\\[\nHHI_{i,t}^{total} = \\sum_{j} w_{i,j,t}^2, \\quad\nHHI_{i,t}^{non-state} = \\sum_{j \\notin state} \\left(\\frac{S_{i,j,t}}{\\sum_{k \\notin state} S_{i,k,t}}\\right)^2\n\\tag{13.4}\\]\nThe non-state HHI is more comparable to the US institutional HHI, as it captures concentration among market-driven investors.\n\n\n13.5.3 Breadth of Ownership\nFollowing Chen, Hong, and Stein (2002), Institutional Breadth (\\(N_{i,t}\\)) is the number of institutional investors holding stock \\(i\\) in period \\(t\\). The Change in Breadth is:\n\\[\n\\Delta Breadth_{i,t} = \\frac{N_{i,t}^{cont} - N_{i,t-1}^{cont}}{TotalInstitutions_{t-1}}\n\\tag{13.5}\\]\nwhere \\(N_{i,t}^{cont}\\) counts only institutions that appear in the disclosure universe in both periods \\(t\\) and \\(t-1\\), following the Lehavy and Sloan (2008) algorithm. This adjustment is particularly important in Vietnam where:\n\nNew funds launch frequently (especially ETFs tracking VN30)\nForeign funds enter and exit the market\nDomestic securities firms consolidate or spin off asset management divisions\n\n\n# ============================================================================\n# Step 5: Compute All IO Metrics\n# ============================================================================\n\ndef compute_io_metrics_vietnam(ownership: pd.DataFrame,\n                                ownership_decomp: pd.DataFrame,\n                                adj_factors: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute security-level institutional ownership metrics adapted for Vietnam.\n    \n    Computes:\n    1. Ownership ratios by category (state, foreign, domestic inst, individual)\n    2. HHI concentration (total, non-state, foreign-only)\n    3. Number of institutional owners (total, foreign, domestic)\n    4. Change in breadth (Lehavy-Sloan adjusted)\n    5. FOL-related metrics (utilization, room, near-cap indicator)\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership data with individual shareholder records\n    ownership_decomp : pd.DataFrame\n        Aggregated ownership decomposition (output of compute_ownership_decomposition)\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors\n    \n    Returns\n    -------\n    pd.DataFrame\n        Stock-period level metrics\n    \"\"\"\n    # Start with the ownership decomposition\n    metrics = ownership_decomp.copy()\n    \n    # --- HHI Concentration ---\n    # Total HHI: across all institutional shareholders\n    inst_ownership = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)\n    ].copy()\n    \n    def compute_hhi_group(group):\n        \"\"\"Compute HHI for a group of shareholders.\"\"\"\n        total = group['shares_held'].sum()\n        if total &lt;= 0:\n            return np.nan\n        weights = group['shares_held'] / total\n        return (weights ** 2).sum()\n    \n    # Total institutional HHI\n    hhi_total = (inst_ownership.groupby(['ticker', 'date'])\n                               .apply(compute_hhi_group)\n                               .reset_index(name='hhi_institutional'))\n    metrics = metrics.merge(hhi_total, on=['ticker', 'date'], how='left')\n    \n    # Non-state HHI (exclude state shareholders)\n    non_state = ownership[\n        ownership['owner_type'].isin([OwnershipType.FOREIGN_INST, \n                                       OwnershipType.DOMESTIC_INST])\n    ]\n    hhi_nonstate = (non_state.groupby(['ticker', 'date'])\n                             .apply(compute_hhi_group)\n                             .reset_index(name='hhi_non_state'))\n    metrics = metrics.merge(hhi_nonstate, on=['ticker', 'date'], how='left')\n    \n    # Foreign-only HHI\n    foreign_only = ownership[ownership['owner_type'] == OwnershipType.FOREIGN_INST]\n    hhi_foreign = (foreign_only.groupby(['ticker', 'date'])\n                               .apply(compute_hhi_group)\n                               .reset_index(name='hhi_foreign'))\n    metrics = metrics.merge(hhi_foreign, on=['ticker', 'date'], how='left')\n    \n    # --- Change in Breadth (Lehavy-Sloan Algorithm) ---\n    metrics = metrics.sort_values(['ticker', 'date'])\n    \n    # Get list of all institutions filing in each period\n    inst_by_period = (inst_ownership.groupby('date')['shareholder_name']\n                                     .apply(set)\n                                     .to_dict())\n    \n    # For each stock-period: count continuing institutions\n    def compute_breadth_change(group):\n        group = group.sort_values('date').reset_index(drop=True)\n        group['dbreadth'] = np.nan\n        \n        for i in range(1, len(group)):\n            current_date = group.loc[i, 'date']\n            prev_date = group.loc[i-1, 'date']\n            \n            # Institutions in universe for both periods\n            current_universe = inst_by_period.get(current_date, set())\n            prev_universe = inst_by_period.get(prev_date, set())\n            continuing_universe = current_universe & prev_universe\n            \n            if len(prev_universe) == 0:\n                continue\n            \n            # Count continuing institutions holding this stock in each period\n            ticker = group.loc[i, 'ticker']\n            \n            current_holders = set(\n                inst_ownership[\n                    (inst_ownership['ticker'] == ticker) & \n                    (inst_ownership['date'] == current_date)\n                ]['shareholder_name']\n            )\n            prev_holders = set(\n                inst_ownership[\n                    (inst_ownership['ticker'] == ticker) & \n                    (inst_ownership['date'] == prev_date)\n                ]['shareholder_name']\n            )\n            \n            # Count only continuing institutions\n            n_current_cont = len(current_holders & continuing_universe)\n            n_prev_cont = len(prev_holders & continuing_universe)\n            \n            group.loc[i, 'dbreadth'] = (\n                (n_current_cont - n_prev_cont) / len(prev_universe)\n            )\n        \n        return group\n    \n    metrics = metrics.groupby('ticker', group_keys=False).apply(compute_breadth_change)\n    \n    # --- FOL Indicators ---\n    if 'fol_utilization' in metrics.columns:\n        metrics['near_fol_cap'] = (metrics['fol_utilization'] &gt; 0.90).astype(int)\n        metrics['at_fol_cap'] = (metrics['fol_utilization'] &gt; 0.98).astype(int)\n    \n    print(f\"IO metrics computed for Vietnam:\")\n    print(f\"  Observations: {len(metrics):,}\")\n    print(f\"\\nKey metric distributions:\")\n    summary_cols = ['pct_institutional', 'pct_state', 'pct_foreign_total',\n                    'hhi_institutional', 'n_inst_owners', 'dbreadth']\n    summary_cols = [c for c in summary_cols if c in metrics.columns]\n    print(metrics[summary_cols].describe().round(4).to_string())\n    \n    return metrics\n\n# io_metrics = compute_io_metrics_vietnam(\n#     ownership_classified, ownership_decomp, adj_factors\n# )\n\n\n\n13.5.4 Time Series Visualization\n\n\n\ndef plot_ownership_timeseries_vietnam(metrics: pd.DataFrame):\n    \"\"\"\n    Create publication-quality time series plots of Vietnamese \n    ownership structure evolution.\n    \"\"\"\n    fig, axes = plt.subplots(3, 1, figsize=(12, 14))\n    \n    # Aggregate across all stocks (market-cap weighted)\n    ts = metrics.groupby('quarter_end').apply(\n        lambda g: pd.Series({\n            'pct_state': np.average(g['pct_state'].fillna(0), \n                                     weights=g['market_cap'].fillna(1)),\n            'pct_foreign': np.average(g['pct_foreign_total'].fillna(0), \n                                       weights=g['market_cap'].fillna(1)),\n            'pct_domestic_inst': np.average(g['pct_domestic_inst'].fillna(0), \n                                             weights=g['market_cap'].fillna(1)),\n            'pct_individual': np.average(g['pct_individual'].fillna(0), \n                                          weights=g['market_cap'].fillna(1)),\n            'n_stocks': g['ticker'].nunique(),\n            'total_mktcap': g['market_cap'].sum(),\n            'median_n_inst': g['n_inst_owners'].median(),\n            'median_hhi': g['hhi_institutional'].median(),\n            'pct_soe': g['is_soe'].mean(),\n        })\n    ).reset_index()\n    \n    # ---- Panel A: Ownership Composition (Stacked Area) ----\n    ax = axes[0]\n    dates = ts['quarter_end']\n    ax.stackplot(dates,\n                 ts['pct_state'] * 100,\n                 ts['pct_foreign'] * 100,\n                 ts['pct_domestic_inst'] * 100,\n                 ts['pct_individual'] * 100,\n                 labels=['State', 'Foreign Institutional', \n                         'Domestic Institutional', 'Individual'],\n                 colors=[OWNER_COLORS['State'], OWNER_COLORS['Foreign Institutional'],\n                         OWNER_COLORS['Domestic Institutional'], OWNER_COLORS['Individual']],\n                 alpha=0.8)\n    ax.set_ylabel('Ownership Share (%)')\n    ax.set_title('Panel A: Ownership Composition of Vietnamese Listed Companies '\n                 '(Market-Cap Weighted)')\n    ax.legend(loc='upper right', frameon=True, framealpha=0.9)\n    ax.set_ylim(0, 100)\n    \n    # ---- Panel B: Institutional Ownership by Component ----\n    ax = axes[1]\n    ax.plot(dates, ts['pct_state'] * 100, label='State',\n            color=OWNER_COLORS['State'], linewidth=2)\n    ax.plot(dates, ts['pct_foreign'] * 100, label='Foreign Institutional',\n            color=OWNER_COLORS['Foreign Institutional'], linewidth=2)\n    ax.plot(dates, ts['pct_domestic_inst'] * 100, label='Domestic Institutional',\n            color=OWNER_COLORS['Domestic Institutional'], linewidth=2)\n    total_inst = (ts['pct_state'] + ts['pct_foreign'] + ts['pct_domestic_inst']) * 100\n    ax.plot(dates, total_inst, label='Total Institutional',\n            color=OWNER_COLORS['Total Institutional'], linewidth=2.5, linestyle='--')\n    ax.set_ylabel('Ownership Ratio (%)')\n    ax.set_title('Panel B: Institutional Ownership Components')\n    ax.legend(loc='upper left', frameon=True, framealpha=0.9)\n    \n    # ---- Panel C: Market Structure ----\n    ax = axes[2]\n    ax2 = ax.twinx()\n    ax.plot(dates, ts['n_stocks'], color='#1f77b4', linewidth=2, label='# Listed Stocks')\n    ax2.plot(dates, ts['total_mktcap'] / 1000, color='#d62728', linewidth=2, \n             label='Total Market Cap (Trillion VND)')\n    ax.set_ylabel('Number of Listed Stocks', color='#1f77b4')\n    ax2.set_ylabel('Market Cap (Trillion VND)', color='#d62728')\n    ax.set_title('Panel C: Vietnamese Stock Market Development')\n    \n    # Combine legends\n    lines1, labels1 = ax.get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left', framealpha=0.9)\n    \n    plt.tight_layout()\n    plt.savefig('fig_ownership_timeseries_vn.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_ownership_timeseries_vietnam(io_metrics)\n\n\nFigure 13.1\n\n\n\n\n\n\ndef plot_io_by_exchange_size(metrics: pd.DataFrame):\n    \"\"\"Plot IO ratios by exchange and size quintile.\"\"\"\n    df = metrics[metrics['market_cap'].notna() & (metrics['market_cap'] &gt; 0)].copy()\n    \n    # Size quintiles within each quarter\n    df['size_quintile'] = df.groupby('quarter_end')['market_cap'].transform(\n        lambda x: pd.qcut(x, 5, labels=['Q1\\n(Small)', 'Q2', 'Q3', 'Q4', 'Q5\\n(Large)'],\n                          duplicates='drop')\n    )\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n    \n    metrics_to_plot = [\n        ('pct_institutional', 'Total Institutional'),\n        ('pct_foreign_total', 'Foreign Institutional'),\n        ('pct_state', 'State'),\n    ]\n    \n    for ax, (col, title) in zip(axes, metrics_to_plot):\n        for exchange, color in EXCHANGE_COLORS.items():\n            data = df[df['exchange'] == exchange]\n            if len(data) == 0:\n                continue\n            means = data.groupby('size_quintile')[col].mean() * 100\n            ax.bar(np.arange(len(means)) + list(EXCHANGE_COLORS.keys()).index(exchange) * 0.25,\n                   means, width=0.25, label=exchange, color=color, alpha=0.8)\n        \n        ax.set_title(title)\n        ax.set_xlabel('Size Quintile')\n        if ax == axes[0]:\n            ax.set_ylabel('Mean Ownership (%)')\n        ax.legend()\n        ax.set_xticks(np.arange(5) + 0.25)\n        ax.set_xticklabels(['Q1\\n(Small)', 'Q2', 'Q3', 'Q4', 'Q5\\n(Large)'])\n    \n    plt.tight_layout()\n    plt.savefig('fig_io_by_exchange_size.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_io_by_exchange_size(io_metrics)\n\n\nFigure 13.2\n\n\n\n\n\n\nTable 13.3: Summary Statistics of Ownership Structure in Vietnam by Size Quintile and Exchange (Pooled 2010-2024)\n\n\ndef tabulate_io_summary(metrics: pd.DataFrame, start_year: int = 2010) -&gt; pd.DataFrame:\n    \"\"\"\n    Create publication-quality summary table of Vietnamese ownership\n    structure by firm size.\n    \"\"\"\n    df = metrics[\n        (metrics['quarter_end'].dt.year &gt;= start_year) &\n        (metrics['market_cap'].notna()) & (metrics['market_cap'] &gt; 0)\n    ].copy()\n    \n    df['size_quintile'] = df.groupby('quarter_end')['market_cap'].transform(\n        lambda x: pd.qcut(x, 5, labels=['Q1 (Small)', 'Q2', 'Q3', 'Q4', 'Q5 (Large)'],\n                          duplicates='drop')\n    )\n    \n    table = df.groupby('size_quintile').agg(\n        N=('ticker', 'count'),\n        Mean_MktCap=('market_cap', 'mean'),\n        Mean_IO_Total=('pct_institutional', 'mean'),\n        Mean_State=('pct_state', 'mean'),\n        Mean_Foreign=('pct_foreign_total', 'mean'),\n        Mean_Domestic_Inst=('pct_domestic_inst', 'mean'),\n        Mean_Individual=('pct_individual', 'mean'),\n        Median_N_Owners=('n_inst_owners', 'median'),\n        Median_HHI=('hhi_institutional', 'median'),\n        Pct_SOE=('is_soe', 'mean'),\n        Mean_FOL_Util=('fol_utilization', 'mean'),\n    ).round(4)\n    \n    # Format\n    table['N'] = table['N'].apply(lambda x: f\"{x:,.0f}\")\n    table['Mean_MktCap'] = table['Mean_MktCap'].apply(lambda x: f\"{x:,.0f}B VND\")\n    for col in ['Mean_IO_Total', 'Mean_State', 'Mean_Foreign', \n                'Mean_Domestic_Inst', 'Mean_Individual', 'Pct_SOE', 'Mean_FOL_Util']:\n        table[col] = table[col].apply(lambda x: f\"{x:.1%}\" if pd.notna(x) else \"—\")\n    table['Median_N_Owners'] = table['Median_N_Owners'].apply(lambda x: f\"{x:.0f}\")\n    table['Median_HHI'] = table['Median_HHI'].apply(lambda x: f\"{x:.3f}\" if pd.notna(x) else \"—\")\n    \n    table.columns = ['N', 'Mean Mkt Cap', 'IO Total', 'State', 'Foreign', \n                      'Dom. Inst.', 'Individual', 'Med. # Owners', \n                      'Med. HHI', '% SOE', 'FOL Util.']\n    \n    return table\n\n# io_summary = tabulate_io_summary(io_metrics)\n# print(io_summary.to_string())",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-foreign-ownership",
    "href": "16_institutional_ownership.html#sec-foreign-ownership",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.6 Foreign Ownership Dynamics",
    "text": "13.6 Foreign Ownership Dynamics\n\n13.6.1 Foreign Ownership Limits and the FOL Premium\nVietnam’s Foreign Ownership Limits create a unique market segmentation. When a stock reaches its FOL, the only way for a new foreign investor to buy is if an existing foreign holder sells. This creates a de facto “foreign-only” market for FOL-constrained stocks, with documented price premiums (Vo 2015).\nThe FOL Utilization Ratio for stock \\(i\\) at time \\(t\\) is:\n\\[\nFOL\\_Util_{i,t} = \\frac{ForeignOwnership_{i,t}}{FOL\\_Limit_i}\n\\tag{13.6}\\]\nStocks are classified by FOL proximity (Table 13.4).\n\n\n\nTable 13.4: FOL Proximity Zones\n\n\n\n\n\n\n\n\n\n\nFOL Zone\nUtilization Range\nMarket Implication\n\n\n\n\nGreen\n&lt; 50%\nAmple foreign room; normal trading\n\n\nYellow\n50-80%\nModerate room; some foreign interest pressure\n\n\nOrange\n80-95%\nLimited room; foreign premium emerging\n\n\nRed\n95-100%\nNear cap; significant foreign premium\n\n\nCapped\n≈ 100%\nAt limit; foreign-only secondary market\n\n\n\n\n\n\n\n# ============================================================================\n# Step 6: Foreign Ownership Limit Analysis\n# ============================================================================\n\nclass FOLAnalyzer:\n    \"\"\"\n    Analyze Foreign Ownership Limit dynamics in the Vietnamese market.\n    \n    Key analyses:\n    1. FOL utilization tracking and classification\n    2. FOL premium estimation (price impact of being near cap)\n    3. Foreign room dynamics (opening/closing events)\n    4. Cross-sectional determinants of foreign ownership\n    \"\"\"\n    \n    FOL_ZONES = {\n        'Green': (0, 0.50),\n        'Yellow': (0.50, 0.80),\n        'Orange': (0.80, 0.95),\n        'Red': (0.95, 1.00),\n        'Capped': (1.00, 1.50),\n    }\n    \n    def __init__(self, io_metrics: pd.DataFrame,\n                 foreign_daily: Optional[pd.DataFrame] = None):\n        \"\"\"\n        Parameters\n        ----------\n        io_metrics : pd.DataFrame\n            Full ownership metrics from compute_io_metrics_vietnam()\n        foreign_daily : pd.DataFrame, optional\n            Daily foreign ownership tracking from DataCore.vn\n        \"\"\"\n        self.metrics = io_metrics.copy()\n        self.foreign_daily = foreign_daily\n    \n    def classify_fol_zones(self) -&gt; pd.DataFrame:\n        \"\"\"Classify stocks into FOL proximity zones.\"\"\"\n        df = self.metrics.copy()\n        \n        if 'fol_utilization' not in df.columns:\n            print(\"FOL utilization not available in metrics.\")\n            return df\n        \n        conditions = []\n        choices = []\n        for zone, (lo, hi) in self.FOL_ZONES.items():\n            conditions.append(\n                (df['fol_utilization'] &gt;= lo) & (df['fol_utilization'] &lt; hi)\n            )\n            choices.append(zone)\n        \n        df['fol_zone'] = np.select(conditions, choices, default='Unknown')\n        \n        # Summary\n        zone_dist = df.groupby('fol_zone')['ticker'].nunique()\n        print(\"FOL Zone Distribution (unique stocks):\")\n        print(zone_dist.to_string())\n        \n        return df\n    \n    def estimate_fol_premium(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Estimate the FOL premium using a cross-sectional approach.\n        \n        For each period, regress stock valuations (P/B or P/E) on FOL \n        utilization, controlling for fundamentals. The coefficient on \n        FOL utilization captures the premium investors pay for stocks \n        near their foreign ownership cap.\n        \n        Alternative: Compare returns of stocks transitioning between \n        FOL zones as a natural experiment.\n        \"\"\"\n        df = self.metrics.copy()\n        df = df[df['fol_utilization'].notna() & df['market_cap'].notna()].copy()\n        \n        # FOL zone dummies\n        df['near_cap'] = (df['fol_utilization'] &gt; 0.90).astype(int)\n        df['at_cap'] = (df['fol_utilization'] &gt; 0.98).astype(int)\n        \n        # Price-to-book as valuation measure\n        # (Assumes 'equity' is available from financial data)\n        if 'equity' in df.columns:\n            df['pb_ratio'] = df['market_cap'] * 1e9 / df['equity']\n        else:\n            # Use market cap as proxy for cross-sectional analysis\n            df['log_mktcap'] = np.log(df['market_cap'])\n        \n        # Fama-MacBeth style: run cross-sectional regressions each period\n        results = []\n        for quarter, group in df.groupby('quarter_end'):\n            group = group.dropna(subset=['fol_utilization', 'log_mktcap'])\n            if len(group) &lt; 50:\n                continue\n            \n            y = group['log_mktcap']\n            X = sm.add_constant(group[['fol_utilization', 'pct_state', \n                                        'n_inst_owners']])\n            try:\n                model = sm.OLS(y, X).fit()\n                results.append({\n                    'quarter': quarter,\n                    'beta_fol': model.params.get('fol_utilization', np.nan),\n                    'tstat_fol': model.tvalues.get('fol_utilization', np.nan),\n                    'r2': model.rsquared,\n                    'n': len(group),\n                })\n            except Exception:\n                continue\n        \n        if results:\n            results_df = pd.DataFrame(results)\n            print(\"FOL Premium (Fama-MacBeth Regression):\")\n            print(f\"  Mean β(FOL_util): {results_df['beta_fol'].mean():.4f}\")\n            print(f\"  t-statistic: {results_df['beta_fol'].mean() / \"\n                  f\"(results_df['beta_fol'].std() / np.sqrt(len(results_df))):.2f}\")\n            return results_df\n        \n        return pd.DataFrame()\n    \n    def analyze_foreign_room_events(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Analyze events where foreign room opens or closes.\n        \n        Room-opening events (FOL cap raised, foreign seller exits) can\n        trigger significant price movements as pent-up foreign demand \n        is released. Room-closing events (approaching cap) can create\n        selling pressure as foreign investors anticipate illiquidity.\n        \"\"\"\n        if self.foreign_daily is None:\n            print(\"Daily foreign ownership data required for event analysis.\")\n            return pd.DataFrame()\n        \n        df = self.foreign_daily.copy()\n        df = df.sort_values(['ticker', 'date'])\n        \n        # Compute daily change in foreign room\n        df['foreign_room_change'] = df.groupby('ticker')['foreign_room'].diff()\n        \n        # Identify room-opening events (room increases by &gt; 1 percentage point)\n        df['room_open_event'] = (df['foreign_room_change'] &gt; 0.01).astype(int)\n        \n        # Identify room-closing events (room decreases to &lt; 2%)\n        df['room_close_event'] = (\n            (df['foreign_room'] &lt; 0.02) & \n            (df.groupby('ticker')['foreign_room'].shift(1) &gt;= 0.02)\n        ).astype(int)\n        \n        events = df[\n            (df['room_open_event'] == 1) | (df['room_close_event'] == 1)\n        ].copy()\n        \n        print(f\"Foreign room events identified:\")\n        print(f\"  Room-opening events: {df['room_open_event'].sum():,}\")\n        print(f\"  Room-closing events: {df['room_close_event'].sum():,}\")\n        \n        return events\n\n# fol_analyzer = FOLAnalyzer(io_metrics, dc.foreign_ownership)\n# fol_classified = fol_analyzer.classify_fol_zones()\n# fol_premium = fol_analyzer.estimate_fol_premium()\n\n\n\n\ndef plot_fol_utilization(metrics: pd.DataFrame):\n    \"\"\"Plot FOL utilization distribution by sector.\"\"\"\n    df = metrics[metrics['fol_utilization'].notna()].copy()\n    \n    # Assign broad sectors\n    sector_map = {\n        'Banking': ['VCB', 'BID', 'CTG', 'TCB', 'VPB', 'MBB', 'ACB', 'HDB', 'STB', 'TPB'],\n        'Real Estate': ['VHM', 'VIC', 'NVL', 'KDH', 'DXG', 'HDG', 'VRE'],\n        'Technology': ['FPT', 'CMG', 'FOX'],\n        'Consumer': ['VNM', 'MSN', 'SAB', 'MWG', 'PNJ'],\n    }\n    \n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    for sector, tickers in sector_map.items():\n        data = df[df['ticker'].isin(tickers)]['fol_utilization']\n        if len(data) &gt; 0:\n            ax.hist(data * 100, bins=30, alpha=0.4, label=sector, density=True)\n    \n    ax.axvline(x=30, color='red', linestyle='--', alpha=0.7, label='Banking FOL (30%)')\n    ax.axvline(x=49, color='blue', linestyle='--', alpha=0.7, label='Standard FOL (49%)')\n    ax.set_xlabel('FOL Utilization (%)')\n    ax.set_ylabel('Density')\n    ax.set_title('Foreign Ownership Limit Utilization Distribution')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig('fig_fol_utilization.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_fol_utilization(io_metrics)\n\n\nFigure 13.3",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-trades",
    "href": "16_institutional_ownership.html#sec-trades",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.7 Institutional Trades",
    "text": "13.7 Institutional Trades\n\n13.7.1 Trade Inference in Vietnam\nIn the US, institutional trades are inferred from quarterly 13F holding snapshots. In Vietnam, the challenge is more acute because disclosure frequency varies:\n\nMajor shareholders (\\(\\ge\\) 5%): Must disclose within 7 business days of crossing ownership thresholds (5%, 10%, 15%, 20%, 25%, 50%, 65%, 75%)\nFund portfolio reports: Semi-annual disclosure required; some funds report quarterly\nAnnual reports: Provide complete shareholder register but only once per year\nDaily foreign ownership: HOSE/HNX publish aggregate daily foreign buy/sell data\n\nWe derive trades from the change in ownership between consecutive disclosure dates, applying the same logic as the US Ben-David et al. (2013) algorithm but adapted for Vietnam’s irregular disclosure intervals.\n\n# ============================================================================\n# Step 7: Derive Institutional Trades\n# ============================================================================\n\ndef derive_trades_vietnam(ownership: pd.DataFrame,\n                           adj_factors: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Derive institutional trades from changes in ownership disclosures.\n    \n    Adapted from Ben-David, Franzoni, and Moussawi (2012) for \n    Vietnam's irregular disclosure frequency.\n    \n    Key differences from US approach:\n    1. Disclosure intervals are irregular (not always quarterly)\n    2. We observe ALL institutional types, not just 13F filers\n    3. No $100M AUM threshold (we see all institutional holders)\n    4. Must adjust for corporate actions between disclosure dates\n    \n    Trade types:\n    +1: Initiating Buy (new position)\n    +2: Incremental Buy (increased existing position)\n    -1: Terminating Sale (fully exited position)\n    -2: Incremental Sale (reduced existing position)\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership with: ticker, date, shareholder_name, \n        shares_held, owner_type\n    adj_factors : pd.DataFrame\n        Corporate action adjustment factors\n    \n    Returns\n    -------\n    pd.DataFrame\n        Trade-level data: date, shareholder_name, ticker, trade, \n        buysale, owner_type\n    \"\"\"\n    # Focus on institutional shareholders only\n    inst = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)\n    ].copy()\n    \n    inst = inst.sort_values(['shareholder_name', 'ticker', 'date']).reset_index(drop=True)\n    \n    trades_list = []\n    \n    for (shareholder, ticker), group in inst.groupby(['shareholder_name', 'ticker']):\n        group = group.reset_index(drop=True)\n        \n        for i in range(len(group)):\n            current = group.iloc[i]\n            current_date = current['date']\n            current_shares = current['shares_held']\n            owner_type = current['owner_type']\n            \n            if i == 0:\n                # First observation: if institution appears, it's an initiating buy\n                # (we don't know if they held before our data starts)\n                # Skip the very first observation to avoid false initiating buys\n                continue\n            \n            prev = group.iloc[i - 1]\n            prev_date = prev['date']\n            prev_shares = prev['shares_held']\n            \n            # Adjust previous shares for corporate actions between dates\n            prev_shares_adj = adjust_shares(\n                prev_shares, ticker, prev_date, current_date, adj_factors\n            )\n            \n            # Compute trade (in adjusted shares)\n            trade = current_shares - prev_shares_adj\n            \n            # Classify trade type\n            if abs(trade) &lt; 1:  # De minimis threshold\n                continue\n            \n            if prev_shares_adj &lt;= 0 and current_shares &gt; 0:\n                buysale = 1  # Initiating buy\n            elif prev_shares_adj &gt; 0 and current_shares &lt;= 0:\n                buysale = -1  # Terminating sale\n            elif trade &gt; 0:\n                buysale = 2  # Incremental buy\n            else:\n                buysale = -2  # Incremental sale\n            \n            trades_list.append({\n                'date': current_date,\n                'shareholder_name': shareholder,\n                'ticker': ticker,\n                'trade': trade,\n                'prev_shares_adj': prev_shares_adj,\n                'current_shares': current_shares,\n                'buysale': buysale,\n                'owner_type': owner_type,\n                'days_between': (current_date - prev_date).days,\n            })\n    \n    trades = pd.DataFrame(trades_list)\n    \n    if len(trades) &gt; 0:\n        print(f\"Trades derived: {len(trades):,}\")\n        print(f\"\\nTrade type distribution:\")\n        labels = {1: 'Initiating Buy', 2: 'Incremental Buy',\n                  -1: 'Terminating Sale', -2: 'Incremental Sale'}\n        for bs, label in sorted(labels.items()):\n            n = (trades['buysale'] == bs).sum()\n            print(f\"  {label}: {n:,} ({n/len(trades):.1%})\")\n        \n        print(f\"\\nBy owner type:\")\n        print(trades.groupby('owner_type')['trade'].agg(['count', 'mean', 'median'])\n              .round(0).to_string())\n    \n    return trades\n\n# trades = derive_trades_vietnam(ownership_classified, adj_factors)\n\n\n\n\n\n\n\nWarningCorporate Action Adjustment in Trade Derivation\n\n\n\nWhen computing trades as \\(\\Delta Shares = Shares_t - Shares_{t-1}\\), the previous period’s shares must be adjusted for any corporate actions between \\(t-1\\) and \\(t\\). If VNM issued a 20% stock dividend between the two disclosure dates, then 1,000 shares at \\(t-1\\) should be compared to 1,200 adjusted shares, not 1,000 raw shares. Failing to make this adjustment would create a phantom “buy” of 200 shares that never actually occurred.\n\n\n\ndef derive_trades_vectorized_vietnam(ownership: pd.DataFrame,\n                                      adj_factors: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Vectorized version of Vietnamese trade derivation.\n    \n    Uses pandas groupby and vectorized operations instead of Python loops.\n    Approximately 20-50x faster for large datasets.\n    \n    Note: Corporate action adjustment is applied per-group, which still\n    requires some iteration but is much faster than row-by-row.\n    \"\"\"\n    inst = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL) &\n        (ownership['shares_held'] &gt; 0)\n    ].copy()\n    \n    inst = inst.sort_values(['shareholder_name', 'ticker', 'date']).reset_index(drop=True)\n    \n    # Lagged values\n    inst['prev_date'] = inst.groupby(['shareholder_name', 'ticker'])['date'].shift(1)\n    inst['prev_shares'] = inst.groupby(['shareholder_name', 'ticker'])['shares_held'].shift(1)\n    inst['is_first'] = inst['prev_date'].isna()\n    \n    # Remove first observations (no prior to compare)\n    inst = inst[~inst['is_first']].copy()\n    \n    # Adjust previous shares for corporate actions\n    # Vectorized: for each row, apply adjustment between prev_date and date\n    def adjust_row(row):\n        return adjust_shares(\n            row['prev_shares'], row['ticker'], \n            row['prev_date'], row['date'], adj_factors\n        )\n    \n    inst['prev_shares_adj'] = inst.apply(adjust_row, axis=1)\n    \n    # Compute trade\n    inst['trade'] = inst['shares_held'] - inst['prev_shares_adj']\n    inst['days_between'] = (inst['date'] - inst['prev_date']).dt.days\n    \n    # Classify trade type\n    inst['buysale'] = np.select(\n        [\n            (inst['prev_shares_adj'] &lt;= 0) & (inst['shares_held'] &gt; 0),\n            (inst['prev_shares_adj'] &gt; 0) & (inst['shares_held'] &lt;= 0),\n            inst['trade'] &gt; 0,\n            inst['trade'] &lt; 0,\n        ],\n        [1, -1, 2, -2],\n        default=0\n    )\n    \n    # Remove zero trades\n    trades = inst[inst['buysale'] != 0].copy()\n    \n    trades = trades[['date', 'shareholder_name', 'ticker', 'trade', \n                     'buysale', 'owner_type', 'days_between',\n                     'prev_shares_adj', 'shares_held']].copy()\n    trades = trades.rename(columns={'shares_held': 'current_shares'})\n    \n    print(f\"Vectorized trades: {len(trades):,}\")\n    return trades\n\n# trades = derive_trades_vectorized_vietnam(ownership_classified, adj_factors)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-flows-turnover",
    "href": "16_institutional_ownership.html#sec-flows-turnover",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.8 Fund-Level Flows and Turnover",
    "text": "13.8 Fund-Level Flows and Turnover\n\n13.8.1 Portfolio Assets and Returns from Fund Holdings\nUsing DataCore.vn’s fund holdings data, we compute fund-level portfolio analytics analogous to the US 13F approach:\n\\[\nAssets_{j,t} = \\sum_{i=1}^{N_{j,t}} S_{i,j,t} \\times P_{i,t}\n\\tag{13.7}\\]\n\\[\nR_{j,t \\to t+1}^{holdings} = \\frac{\\sum_{i} S_{i,j,t} \\times P_{i,t} \\times R_{i,t \\to t+1}}{\\sum_{i} S_{i,j,t} \\times P_{i,t}}\n\\tag{13.8}\\]\n\\[\nNetFlows_{j,t} = Assets_{j,t} - Assets_{j,t-1} \\times (1 + R_{j,t-1 \\to t}^{holdings})\n\\tag{13.9}\\]\n\n\n13.8.2 Turnover Measures\nFollowing Carhart (1997), adapted for Vietnam’s fund reporting:\n\\[\nTurnover_{j,t}^{Carhart} = \\frac{\\min(TotalBuys_{j,t}, TotalSales_{j,t})}{\\overline{Assets}_{j,t}}\n\\tag{13.10}\\]\n\n# ============================================================================\n# Step 8: Fund-Level Portfolio Analytics\n# ============================================================================\n\ndef compute_fund_analytics(fund_holdings: pd.DataFrame,\n                            prices_q: pd.DataFrame,\n                            adj_factors: pd.DataFrame) -&gt; Dict:\n    \"\"\"\n    Compute fund-level portfolio analytics from DataCore.vn fund holdings.\n    \n    Vietnamese fund disclosure is typically semi-annual (some quarterly),\n    which limits the frequency of these analytics compared to the US\n    quarterly approach.\n    \n    Returns\n    -------\n    dict with keys:\n        'fund_assets': pd.DataFrame of fund-level assets and returns\n        'fund_trades': pd.DataFrame of fund-level derived trades\n        'fund_aggregates': pd.DataFrame of flows and turnover\n    \"\"\"\n    fh = fund_holdings.copy()\n    fh = fh[fh['shares_held'] &gt; 0].copy()\n    \n    # Merge with prices\n    fh = fh.merge(\n        prices_q[['ticker', 'quarter_end', 'close', 'adjusted_close', 'ret']],\n        left_on=['ticker', 'report_date'],\n        right_on=['ticker', 'quarter_end'],\n        how='inner'\n    )\n    \n    # Portfolio value\n    fh['holding_value'] = fh['shares_held'] * fh['close']\n    \n    # --- Fund-Level Assets ---\n    fund_assets = fh.groupby(['fund_name', 'report_date']).agg(\n        total_assets=('holding_value', lambda x: x.sum() / 1e9),  # Billion VND\n        n_stocks=('ticker', 'nunique'),\n    ).reset_index()\n    \n    # Holdings return (value-weighted)\n    fh['weight'] = fh.groupby(['fund_name', 'report_date'])['holding_value'].transform(\n        lambda x: x / x.sum()\n    )\n    fund_hret = (fh.groupby(['fund_name', 'report_date'])\n                   .apply(lambda g: np.average(g['ret'].fillna(0), weights=g['weight']))\n                   .reset_index(name='holdings_return'))\n    \n    fund_assets = fund_assets.merge(fund_hret, on=['fund_name', 'report_date'])\n    \n    # --- Fund-Level Trades ---\n    # Derive trades from changes in holdings\n    fh_sorted = fh.sort_values(['fund_name', 'ticker', 'report_date'])\n    fh_sorted['prev_shares'] = fh_sorted.groupby(['fund_name', 'ticker'])['shares_held'].shift(1)\n    fh_sorted['prev_date'] = fh_sorted.groupby(['fund_name', 'ticker'])['report_date'].shift(1)\n    \n    # Adjust for corporate actions\n    fh_sorted['prev_shares_adj'] = fh_sorted.apply(\n        lambda r: adjust_shares(r['prev_shares'], r['ticker'], \n                                r['prev_date'], r['report_date'], adj_factors)\n        if pd.notna(r['prev_shares']) else np.nan,\n        axis=1\n    )\n    \n    fh_sorted['trade'] = fh_sorted['shares_held'] - fh_sorted['prev_shares_adj']\n    fh_sorted['trade_value'] = fh_sorted['trade'] * fh_sorted['close'] / 1e9  # Billion VND\n    \n    # Aggregate buys and sells per fund-period\n    fund_trades = fh_sorted[fh_sorted['trade'].notna()].copy()\n    fund_flows = fund_trades.groupby(['fund_name', 'report_date']).agg(\n        total_buys=('trade_value', lambda x: x[x &gt; 0].sum()),\n        total_sales=('trade_value', lambda x: -x[x &lt; 0].sum()),\n    ).reset_index()\n    \n    # --- Fund-Level Aggregates ---\n    fund_agg = fund_assets.merge(fund_flows, on=['fund_name', 'report_date'], how='left')\n    fund_agg[['total_buys', 'total_sales']] = fund_agg[['total_buys', 'total_sales']].fillna(0)\n    \n    fund_agg = fund_agg.sort_values(['fund_name', 'report_date'])\n    fund_agg['lag_assets'] = fund_agg.groupby('fund_name')['total_assets'].shift(1)\n    fund_agg['lag_hret'] = fund_agg.groupby('fund_name')['holdings_return'].shift(1)\n    \n    # Net flows\n    fund_agg['net_flows'] = (fund_agg['total_assets'] - \n                              fund_agg['lag_assets'] * (1 + fund_agg['holdings_return']))\n    \n    # Turnover (Carhart definition)\n    fund_agg['avg_assets'] = (fund_agg['total_assets'] + fund_agg['lag_assets']) / 2\n    fund_agg['turnover'] = (\n        fund_agg[['total_buys', 'total_sales']].min(axis=1) / fund_agg['avg_assets']\n    )\n    \n    # Annualize (approximate, since disclosure may be semi-annual)\n    fund_agg['periods_per_year'] = 365 / fund_agg.groupby('fund_name')['report_date'].diff().dt.days\n    fund_agg['turnover_annual'] = fund_agg['turnover'] * fund_agg['periods_per_year'].fillna(2)\n    \n    print(f\"Fund analytics computed:\")\n    print(f\"  Unique funds: {fund_agg['fund_name'].nunique():,}\")\n    print(f\"  Fund-period observations: {len(fund_agg):,}\")\n    print(f\"\\nTurnover statistics:\")\n    print(fund_agg[['turnover', 'turnover_annual']].describe().round(4))\n    \n    return {\n        'fund_assets': fund_assets,\n        'fund_trades': fund_trades,\n        'fund_aggregates': fund_agg,\n    }\n\n# fund_analytics = compute_fund_analytics(dc.fund_holdings, prices_q, adj_factors)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-state-ownership",
    "href": "16_institutional_ownership.html#sec-state-ownership",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.9 State Ownership Analysis",
    "text": "13.9 State Ownership Analysis\n\n13.9.1 Equitization and the Decline of State Ownership\nVietnam’s equitization (cổ phần hóa) program has been a defining feature of the market since the early 2000s. The program converts state-owned enterprises into joint-stock companies, typically with the state retaining a controlling or significant minority stake that is then gradually reduced through secondary offerings.\n\n# ============================================================================\n# Step 9: State Ownership Analysis\n# ============================================================================\n\ndef analyze_state_ownership(metrics: pd.DataFrame) -&gt; Dict:\n    \"\"\"\n    Comprehensive analysis of state ownership in Vietnam.\n    \n    Computes:\n    1. Aggregate state ownership trends\n    2. SOE population dynamics (entry/exit from SOE classification)\n    3. Equitization event detection (large drops in state ownership)\n    4. State ownership by sector and size\n    5. Governance implications (state as blockholder)\n    \"\"\"\n    df = metrics.copy()\n    \n    # --- 1. Aggregate Trends ---\n    ts = df.groupby('quarter_end').agg(\n        n_soe=('is_soe', 'sum'),\n        n_total=('ticker', 'nunique'),\n        pct_soe=('is_soe', 'mean'),\n        mean_state_pct=('pct_state', 'mean'),\n        median_state_pct=('pct_state', 'median'),\n        # Market cap share of SOEs\n        soe_mktcap=('market_cap', lambda x: x[df.loc[x.index, 'is_soe'] == 1].sum()),\n        total_mktcap=('market_cap', 'sum'),\n    ).reset_index()\n    ts['soe_mktcap_share'] = ts['soe_mktcap'] / ts['total_mktcap']\n    \n    # --- 2. Equitization Events ---\n    # Detect large drops in state ownership (&gt;10 percentage points)\n    df_sorted = df.sort_values(['ticker', 'quarter_end'])\n    df_sorted['state_change'] = df_sorted.groupby('ticker')['pct_state'].diff()\n    \n    equitization_events = df_sorted[\n        df_sorted['state_change'] &lt; -0.10  # &gt; 10pp drop\n    ][['ticker', 'quarter_end', 'pct_state', 'state_change', 'market_cap']].copy()\n    \n    # --- 3. By Sector ---\n    if 'industry_code' in df.columns:\n        by_sector = df.groupby('industry_code').agg(\n            mean_state=('pct_state', 'mean'),\n            pct_soe=('is_soe', 'mean'),\n            n_firms=('ticker', 'nunique'),\n        ).sort_values('mean_state', ascending=False)\n    else:\n        by_sector = None\n    \n    print(f\"State Ownership Analysis:\")\n    print(f\"  Current SOE count: {ts.iloc[-1]['n_soe']:.0f} / {ts.iloc[-1]['n_total']:.0f}\")\n    print(f\"  SOE market cap share: {ts.iloc[-1]['soe_mktcap_share']:.1%}\")\n    print(f\"  Mean state ownership: {ts.iloc[-1]['mean_state_pct']:.1%}\")\n    print(f\"\\nEquitization events detected: {len(equitization_events):,}\")\n    \n    return {\n        'trends': ts,\n        'equitization_events': equitization_events,\n        'by_sector': by_sector,\n    }\n\n# state_analysis = analyze_state_ownership(io_metrics)\n\n\n\n\ndef plot_state_ownership(state_analysis: Dict, metrics: pd.DataFrame):\n    \"\"\"Plot state ownership dynamics.\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n    ts = state_analysis['trends']\n    \n    # Panel A: SOE trends\n    ax = axes[0]\n    ax.plot(ts['quarter_end'], ts['pct_soe'] * 100, \n            label='% of Firms that are SOEs', linewidth=2, color='#d62728')\n    ax.plot(ts['quarter_end'], ts['soe_mktcap_share'] * 100,\n            label='SOE Market Cap Share (%)', linewidth=2, color='#1f77b4')\n    ax.plot(ts['quarter_end'], ts['mean_state_pct'] * 100,\n            label='Mean State Ownership (%)', linewidth=2, color='#2ca02c', linestyle='--')\n    ax.set_ylabel('Percentage')\n    ax.set_title('Panel A: State Ownership and SOE Prevalence Over Time')\n    ax.legend(frameon=True, framealpha=0.9)\n    \n    # Panel B: Distribution\n    ax = axes[1]\n    # Use most recent period\n    latest = metrics[metrics['quarter_end'] == metrics['quarter_end'].max()]\n    state_pct = latest['pct_state'].dropna() * 100\n    \n    ax.hist(state_pct, bins=50, color='#d62728', alpha=0.7, edgecolor='black')\n    ax.axvline(x=50, color='black', linestyle='--', alpha=0.7, label='50% (SOE threshold)')\n    ax.set_xlabel('State Ownership (%)')\n    ax.set_ylabel('Number of Companies')\n    ax.set_title('Panel B: Distribution of State Ownership (Most Recent Quarter)')\n    ax.legend()\n    \n    plt.tight_layout()\n    plt.savefig('fig_state_ownership.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# plot_state_ownership(state_analysis, io_metrics)\n\n\nFigure 13.4",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-modern-extensions",
    "href": "16_institutional_ownership.html#sec-modern-extensions",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.10 Modern Extensions",
    "text": "13.10 Modern Extensions\n\n13.10.1 Network Analysis of Co-Ownership\nInstitutional co-ownership networks capture how stocks are connected through shared investors. In Vietnam, these networks reveal the influence structure of major domestic conglomerates (e.g., Vingroup, Masan, FPT) and the overlap between foreign fund portfolios.\n\ndef construct_stock_coownership_network(ownership: pd.DataFrame,\n                                         period: str,\n                                         min_overlap: int = 3) -&gt; Dict:\n    \"\"\"\n    Construct a stock-level co-ownership network.\n    \n    Two stocks are connected if they share institutional investors.\n    Edge weight = number of shared institutional investors.\n    \n    This is particularly informative in Vietnam where:\n    - Foreign fund portfolios concentrate on the same blue-chips\n    - Conglomerate cross-holdings create explicit linkages\n    - State ownership creates implicit connections (SCIC holds multiple stocks)\n    \n    Parameters\n    ----------\n    ownership : pd.DataFrame\n        Classified ownership data\n    period : str\n        Analysis date\n    min_overlap : int\n        Minimum shared investors to create an edge\n    \n    Returns\n    -------\n    dict with network statistics and adjacency data\n    \"\"\"\n    import networkx as nx\n    \n    date = pd.Timestamp(period)\n    \n    # Get institutional holders for this period\n    inst = ownership[\n        (ownership['date'] == date) &\n        (ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL))\n    ][['ticker', 'shareholder_name', 'owner_type']].copy()\n    \n    # Create bipartite mapping: institution → set of stocks held\n    inst_to_stocks = inst.groupby('shareholder_name')['ticker'].apply(set).to_dict()\n    \n    # Stock → set of institutions\n    stock_to_inst = inst.groupby('ticker')['shareholder_name'].apply(set).to_dict()\n    \n    # Build stock-level network\n    stocks = list(stock_to_inst.keys())\n    G = nx.Graph()\n    \n    for i in range(len(stocks)):\n        for j in range(i + 1, len(stocks)):\n            shared = stock_to_inst[stocks[i]] & stock_to_inst[stocks[j]]\n            if len(shared) &gt;= min_overlap:\n                G.add_edge(stocks[i], stocks[j], weight=len(shared),\n                           shared_investors=list(shared)[:5])  # Store sample\n    \n    # Add node attributes\n    for stock in stocks:\n        if stock in G.nodes:\n            G.nodes[stock]['n_inst_holders'] = len(stock_to_inst[stock])\n    \n    # Network statistics\n    stats = {\n        'n_nodes': G.number_of_nodes(),\n        'n_edges': G.number_of_edges(),\n        'density': nx.density(G) if G.number_of_nodes() &gt; 1 else 0,\n        'avg_clustering': nx.average_clustering(G, weight='weight') if G.number_of_nodes() &gt; 0 else 0,\n        'n_components': nx.number_connected_components(G),\n    }\n    \n    # Centrality measures\n    if G.number_of_nodes() &gt; 0:\n        degree_cent = nx.degree_centrality(G)\n        stats['most_connected'] = sorted(degree_cent.items(), \n                                          key=lambda x: x[1], reverse=True)[:10]\n        \n        if G.number_of_nodes() &gt; 2:\n            try:\n                eigen_cent = nx.eigenvector_centrality_numpy(G, weight='weight')\n                stats['most_central'] = sorted(eigen_cent.items(),\n                                                key=lambda x: x[1], reverse=True)[:10]\n            except Exception:\n                stats['most_central'] = []\n    \n    print(f\"Co-Ownership Network ({period}):\")\n    for k, v in stats.items():\n        if k not in ['most_connected', 'most_central']:\n            print(f\"  {k}: {v}\")\n    \n    if 'most_connected' in stats:\n        print(f\"\\nMost connected stocks:\")\n        for stock, cent in stats['most_connected'][:5]:\n            print(f\"  {stock}: {cent:.3f}\")\n    \n    return {'graph': G, 'stats': stats}\n\n# network = construct_stock_coownership_network(\n#     ownership_classified, '2024-06-30'\n# )\n\n\n\n13.10.2 ML-Enhanced Investor Classification\nVietnam’s investor classification challenge is distinct from the US. While the US has the Bushee typology based on portfolio turnover and concentration, Vietnam requires classification of both investor type (when not explicitly labeled) and investor behavior (active vs passive, short-term vs long-term).\n\ndef classify_investors_vietnam(ownership: pd.DataFrame,\n                                prices_q: pd.DataFrame,\n                                n_clusters: int = 4) -&gt; pd.DataFrame:\n    \"\"\"\n    ML-based classification of Vietnamese institutional investors.\n    \n    Features adapted for Vietnam's market:\n    1. Portfolio concentration (HHI of holdings)\n    2. Holding duration (average time in positions)\n    3. Size preference (average market cap of holdings)\n    4. Sector concentration\n    5. Foreign/domestic indicator\n    6. Trading frequency (inverse of average days between disclosures)\n    \n    Expected clusters for Vietnam:\n    - Passive State Holders: SOE parents, SCIC - low turnover, concentrated\n    - Active Foreign Funds: Dragon Capital, VinaCapital - moderate turnover\n    - Domestic Securities Firms: SSI, VNDirect - high turnover, diversified\n    - Long-Term Foreign: Pension funds, sovereign wealth - low turnover\n    \"\"\"\n    from sklearn.cluster import KMeans\n    from sklearn.preprocessing import StandardScaler\n    \n    inst = ownership[\n        ownership['owner_type'].isin(OwnershipType.INSTITUTIONAL)\n    ].copy()\n    \n    # Merge with price data\n    inst = inst.merge(\n        prices_q[['ticker', 'quarter_end', 'close', 'market_cap']],\n        left_on=['ticker', 'date'],\n        right_on=['ticker', 'quarter_end'],\n        how='left'\n    )\n    \n    inst['holding_value'] = inst['shares_held'] * inst['close'].fillna(0)\n    \n    # Compute features per investor-period\n    features = inst.groupby(['shareholder_name', 'date']).agg(\n        n_stocks=('ticker', 'nunique'),\n        total_value=('holding_value', 'sum'),\n        hhi_portfolio=('holding_value', \n                        lambda x: ((x/x.sum())**2).sum() if x.sum() &gt; 0 else np.nan),\n        avg_mktcap=('market_cap', 'mean'),\n        is_foreign=('owner_type', \n                     lambda x: (x == OwnershipType.FOREIGN_INST).any().astype(int)),\n        is_state=('owner_type', \n                   lambda x: (x == OwnershipType.STATE).any().astype(int)),\n    ).reset_index()\n    \n    # Average across all periods per investor\n    investor_features = features.groupby('shareholder_name').agg(\n        avg_n_stocks=('n_stocks', 'mean'),\n        avg_hhi=('hhi_portfolio', 'mean'),\n        avg_mktcap=('avg_mktcap', 'mean'),\n        avg_total_value=('total_value', 'mean'),\n        is_foreign=('is_foreign', 'max'),\n        is_state=('is_state', 'max'),\n        n_periods=('date', 'nunique'),\n    ).dropna()\n    \n    # Feature matrix\n    feature_cols = ['avg_n_stocks', 'avg_hhi', 'avg_mktcap', 'avg_total_value']\n    X = investor_features[feature_cols].copy()\n    \n    # Log-transform\n    for col in feature_cols:\n        X[col] = np.log1p(X[col].clip(lower=0))\n    \n    # Add binary features\n    X['is_foreign'] = investor_features['is_foreign']\n    X['is_state'] = investor_features['is_state']\n    \n    # Standardize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # K-means\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=20)\n    investor_features['cluster'] = kmeans.fit_predict(X_scaled)\n    \n    # Label clusters\n    cluster_profiles = investor_features.groupby('cluster').agg({\n        'avg_n_stocks': 'mean',\n        'avg_hhi': 'mean',\n        'avg_total_value': 'mean',\n        'is_foreign': 'mean',\n        'is_state': 'mean',\n        'shareholder_name': 'count',\n    }).rename(columns={'shareholder_name': 'n_investors'})\n    \n    print(\"Investor Clusters:\")\n    print(cluster_profiles.round(3).to_string())\n    \n    return investor_features\n\n# investor_classes = classify_investors_vietnam(ownership_classified, prices_q)\n\n\n\n13.10.3 Event Study: Ownership Disclosure Shocks\nVietnam’s threshold-based major shareholder disclosure creates natural events for studying the price impact of ownership changes.\n\ndef ownership_event_study(major_shareholders: pd.DataFrame,\n                           prices: pd.DataFrame,\n                           event_window: Tuple[int, int] = (-5, 20),\n                           estimation_window: int = 120) -&gt; pd.DataFrame:\n    \"\"\"\n    Event study of ownership disclosure announcements.\n    \n    Vietnam requires major shareholders (≥5%) to disclose within 7 \n    business days of crossing ownership thresholds. These disclosures \n    can be informationally significant, especially:\n    1. Foreign fund accumulation (signal of quality)\n    2. State divestiture (equitization signal)\n    3. Insider purchases (management confidence signal)\n    \n    Uses market model for expected returns:\n    E[R_i,t] = α_i + β_i × R_m,t\n    \n    Parameters\n    ----------\n    major_shareholders : pd.DataFrame\n        Disclosure events from DataCore.vn\n    prices : pd.DataFrame\n        Daily stock prices\n    event_window : tuple\n        (pre_event_days, post_event_days)\n    estimation_window : int\n        Days before event window for market model estimation\n    \"\"\"\n    events = major_shareholders.copy()\n    events = events.sort_values(['ticker', 'date'])\n    \n    # Identify significant ownership changes\n    events['ownership_change'] = events.groupby(\n        ['ticker', 'shareholder_name']\n    )['ownership_pct'].diff()\n    \n    significant_events = events[\n        events['ownership_change'].abs() &gt; 0.01  # &gt; 1 percentage point\n    ].copy()\n    \n    significant_events['event_type'] = np.where(\n        significant_events['ownership_change'] &gt; 0, 'accumulation', 'divestiture'\n    )\n    \n    # Merge with daily prices\n    prices_daily = prices[['ticker', 'date', 'ret']].copy()\n    prices_daily = prices_daily.sort_values(['ticker', 'date'])\n    \n    # VN-Index as market return (ticker code depends on data provider)\n    if 'VNINDEX' in prices_daily['ticker'].values:\n        market_ret = prices_daily[prices_daily['ticker'] == 'VNINDEX'][['date', 'ret']].copy()\n        market_ret = market_ret.rename(columns={'ret': 'mkt_ret'})\n    else:\n        # Use equal-weighted market return as proxy\n        market_ret = (prices_daily.groupby('date')['ret']\n                                  .mean()\n                                  .reset_index()\n                                  .rename(columns={'ret': 'mkt_ret'}))\n    \n    # For each event, compute abnormal returns\n    results = []\n    pre, post = event_window\n    \n    for _, event in significant_events.iterrows():\n        ticker = event['ticker']\n        event_date = event['date']\n        \n        # Get stock returns around the event\n        stock_ret = prices_daily[prices_daily['ticker'] == ticker].copy()\n        stock_ret = stock_ret.merge(market_ret, on='date', how='left')\n        stock_ret = stock_ret.sort_values('date').reset_index(drop=True)\n        \n        # Find event date index\n        event_idx = stock_ret[stock_ret['date'] &gt;= event_date].index\n        if len(event_idx) == 0:\n            continue\n        event_idx = event_idx[0]\n        \n        # Estimation window\n        est_start = max(0, event_idx - estimation_window + pre)\n        est_end = event_idx + pre\n        est_data = stock_ret.iloc[est_start:est_end].dropna(subset=['ret', 'mkt_ret'])\n        \n        if len(est_data) &lt; 30:\n            continue\n        \n        # Market model\n        X = sm.add_constant(est_data['mkt_ret'])\n        y = est_data['ret']\n        try:\n            model = sm.OLS(y, X).fit()\n        except Exception:\n            continue\n        \n        # Event window abnormal returns\n        ew_start = event_idx + pre\n        ew_end = min(event_idx + post + 1, len(stock_ret))\n        event_data = stock_ret.iloc[ew_start:ew_end].copy()\n        \n        if len(event_data) == 0:\n            continue\n        \n        event_data['expected_ret'] = (model.params['const'] + \n                                       model.params['mkt_ret'] * event_data['mkt_ret'])\n        event_data['abnormal_ret'] = event_data['ret'] - event_data['expected_ret']\n        event_data['car'] = event_data['abnormal_ret'].cumsum()\n        event_data['event_day'] = range(pre, pre + len(event_data))\n        event_data['ticker'] = ticker\n        event_data['event_date'] = event_date\n        event_data['event_type'] = event['event_type']\n        event_data['ownership_change'] = event['ownership_change']\n        event_data['shareholder_name'] = event['shareholder_name']\n        \n        results.append(event_data)\n    \n    if results:\n        all_results = pd.concat(results, ignore_index=True)\n        \n        # Average CARs by event type\n        avg_car = (all_results.groupby(['event_type', 'event_day'])['car']\n                              .agg(['mean', 'std', 'count'])\n                              .reset_index())\n        avg_car['t_stat'] = avg_car['mean'] / (avg_car['std'] / np.sqrt(avg_car['count']))\n        \n        print(f\"Event Study Results:\")\n        print(f\"  Total events: {significant_events['event_type'].value_counts().to_string()}\")\n        \n        # CAR at event day 0, +5, +10, +20\n        for et in ['accumulation', 'divestiture']:\n            print(f\"\\n  {et.title()} Events:\")\n            subset = avg_car[avg_car['event_type'] == et]\n            for day in [0, 5, 10, 20]:\n                row = subset[subset['event_day'] == day]\n                if len(row) &gt; 0:\n                    print(f\"    CAR({day:+d}): {row.iloc[0]['mean']:.4f} \"\n                          f\"(t={row.iloc[0]['t_stat']:.2f})\")\n        \n        return all_results\n    \n    return pd.DataFrame()\n\n# event_results = ownership_event_study(dc.major_shareholders, dc.prices)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-empirical-applications",
    "href": "16_institutional_ownership.html#sec-empirical-applications",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.11 Empirical Applications",
    "text": "13.11 Empirical Applications\n\n13.11.1 Application 1: Foreign Ownership and Stock Returns in Vietnam\nDoes foreign institutional ownership predict returns in Vietnam? Huang, Liu, and Shu (2023) find evidence consistent with the information advantage hypothesis.\n\ndef test_foreign_io_returns(metrics: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Test whether changes in foreign institutional ownership predict \n    future stock returns in Vietnam.\n    \n    Methodology:\n    1. Sort stocks into quintiles by change in foreign IO\n    2. Compute equal-weighted and VN-Index-adjusted returns\n    3. Report portfolio returns and long-short spread\n    \n    This adapts the Chen, Hong, and Stein (2002) breadth test \n    specifically for Vietnam's foreign ownership component.\n    \"\"\"\n    df = metrics.copy()\n    df = df.sort_values(['ticker', 'quarter_end'])\n    \n    # Change in foreign IO\n    df['delta_foreign'] = df.groupby('ticker')['pct_foreign_total'].diff()\n    \n    # Forward quarterly return\n    df['fwd_ret'] = df.groupby('ticker')['ret'].shift(-1)\n    \n    # Drop missing\n    df = df.dropna(subset=['delta_foreign', 'fwd_ret'])\n    \n    # Quintile portfolios each quarter\n    df['foreign_quintile'] = df.groupby('quarter_end')['delta_foreign'].transform(\n        lambda x: pd.qcut(x, 5, labels=[1, 2, 3, 4, 5], duplicates='drop')\n    )\n    \n    # Portfolio returns\n    port_ret = (df.groupby(['quarter_end', 'foreign_quintile'])['fwd_ret']\n                  .mean()\n                  .reset_index())\n    \n    port_wide = port_ret.pivot(index='quarter_end', columns='foreign_quintile', \n                                values='fwd_ret')\n    port_wide['LS'] = port_wide[5] - port_wide[1]\n    \n    # Test significance\n    results = {}\n    for q in [1, 2, 3, 4, 5, 'LS']:\n        data = port_wide[q].dropna()\n        mean_ret = data.mean()\n        t_stat = mean_ret / (data.std() / np.sqrt(len(data)))\n        results[q] = {\n            'Mean Return (%)': mean_ret * 100,\n            't-statistic': t_stat,\n            'N quarters': len(data),\n        }\n    \n    results_df = pd.DataFrame(results).T\n    results_df.index.name = 'ΔForeign IO Quintile'\n    \n    print(\"Foreign Ownership Change and Future Returns (Vietnam)\")\n    print(\"=\" * 60)\n    print(results_df.round(3).to_string())\n    \n    return results_df\n\n# foreign_return_results = test_foreign_io_returns(io_metrics)\n\n\n\n13.11.2 Application 2: State Divestiture and Value Creation\n\ndef analyze_equitization_value(metrics: pd.DataFrame, \n                                state_analysis: Dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Test whether reductions in state ownership are associated with \n    subsequent value creation (higher returns, improved governance).\n    \n    Hypothesis: State divestiture reduces agency costs, improves \n    operational efficiency, and attracts institutional investors,\n    leading to positive abnormal returns.\n    \n    Uses a difference-in-differences approach:\n    Treatment: Firms experiencing &gt;10pp drop in state ownership\n    Control: Matched firms with stable state ownership\n    \"\"\"\n    df = metrics.copy()\n    events = state_analysis['equitization_events']\n    \n    if len(events) == 0:\n        print(\"No equitization events detected.\")\n        return pd.DataFrame()\n    \n    # Get treated firms and their event quarters\n    treated = events[['ticker', 'quarter_end']].drop_duplicates()\n    treated['treated'] = 1\n    \n    # Merge with metrics\n    df = df.merge(treated, on=['ticker', 'quarter_end'], how='left')\n    df['treated'] = df['treated'].fillna(0)\n    \n    # Pre/post comparison for treated firms\n    treated_tickers = treated['ticker'].unique()\n    \n    results = []\n    for ticker in treated_tickers:\n        firm = df[df['ticker'] == ticker].sort_values('quarter_end')\n        event_row = firm[firm['treated'] == 1]\n        if len(event_row) == 0:\n            continue\n        \n        event_q = event_row.iloc[0]['quarter_end']\n        \n        # Pre-event (4 quarters before)\n        pre = firm[firm['quarter_end'] &lt; event_q].tail(4)\n        # Post-event (4 quarters after)\n        post = firm[firm['quarter_end'] &gt; event_q].head(4)\n        \n        if len(pre) &lt; 2 or len(post) &lt; 2:\n            continue\n        \n        results.append({\n            'ticker': ticker,\n            'event_quarter': event_q,\n            'state_pct_pre': pre['pct_state'].mean(),\n            'state_pct_post': post['pct_state'].mean(),\n            'foreign_pct_pre': pre['pct_foreign_total'].mean(),\n            'foreign_pct_post': post['pct_foreign_total'].mean(),\n            'n_inst_pre': pre['n_inst_owners'].mean(),\n            'n_inst_post': post['n_inst_owners'].mean(),\n            'ret_pre': pre['ret'].mean(),\n            'ret_post': post['ret'].mean(),\n        })\n    \n    if results:\n        results_df = pd.DataFrame(results)\n        \n        # Paired t-tests\n        print(\"Equitization Value Analysis\")\n        print(\"=\" * 60)\n        for metric in ['state_pct', 'foreign_pct', 'n_inst', 'ret']:\n            pre_col = f'{metric}_pre'\n            post_col = f'{metric}_post'\n            diff = results_df[post_col] - results_df[pre_col]\n            t_stat, p_val = stats.ttest_1samp(diff.dropna(), 0)\n            print(f\"  Δ{metric}: {diff.mean():.4f} (t={t_stat:.2f}, p={p_val:.3f})\")\n        \n        return results_df\n    \n    return pd.DataFrame()\n\n# equitization_results = analyze_equitization_value(io_metrics, state_analysis)\n\n\n\n13.11.3 Application 3: Institutional Herding in Vietnam\n\ndef compute_herding_vietnam(trades: pd.DataFrame,\n                             owner_types: Optional[List[str]] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the Lakonishok, Shleifer, and Vishny (1992) herding measure\n    adapted for the Vietnamese market.\n    \n    Can be computed separately for:\n    - All institutional investors\n    - Foreign institutions only\n    - Domestic institutions only\n    \n    The herding measure captures whether institutions systematically\n    trade in the same direction beyond what chance would predict.\n    \"\"\"\n    from scipy.stats import binom\n    \n    t = trades.copy()\n    \n    if owner_types:\n        t = t[t['owner_type'].isin(owner_types)]\n    \n    t['is_buy'] = (t['trade'] &gt; 0).astype(int)\n    \n    # For each stock-period\n    stock_trades = t.groupby(['ticker', 'date']).agg(\n        n_traders=('shareholder_name', 'nunique'),\n        n_buyers=('is_buy', 'sum'),\n    ).reset_index()\n    \n    # Minimum traders threshold\n    stock_trades = stock_trades[stock_trades['n_traders'] &gt;= 3]\n    stock_trades['p_buy'] = stock_trades['n_buyers'] / stock_trades['n_traders']\n    \n    # Expected proportion per period\n    E_p = stock_trades.groupby('date').apply(\n        lambda g: g['n_buyers'].sum() / g['n_traders'].sum()\n    ).reset_index(name='E_p')\n    \n    stock_trades = stock_trades.merge(E_p, on='date')\n    \n    # Adjustment factor\n    def expected_abs_dev(n, p):\n        k = np.arange(0, n + 1)\n        probs = binom.pmf(k, n, p)\n        return np.sum(probs * np.abs(k / n - p))\n    \n    stock_trades['adj_factor'] = stock_trades.apply(\n        lambda r: expected_abs_dev(int(r['n_traders']), r['E_p']), axis=1\n    )\n    \n    stock_trades['hm'] = (np.abs(stock_trades['p_buy'] - stock_trades['E_p']) - \n                           stock_trades['adj_factor'])\n    \n    stock_trades['buy_herd'] = np.where(\n        stock_trades['p_buy'] &gt; stock_trades['E_p'], stock_trades['hm'], np.nan\n    )\n    stock_trades['sell_herd'] = np.where(\n        stock_trades['p_buy'] &lt; stock_trades['E_p'], stock_trades['hm'], np.nan\n    )\n    \n    # Time series of herding\n    ts_herding = stock_trades.groupby('date').agg(\n        mean_hm=('hm', 'mean'),\n        mean_buy_herd=('buy_herd', 'mean'),\n        mean_sell_herd=('sell_herd', 'mean'),\n        pct_herding=('hm', lambda x: (x &gt; 0).mean()),\n        n_stocks=('ticker', 'nunique'),\n    ).reset_index()\n    \n    print(f\"Herding Analysis ({owner_types or 'All Institutions'}):\")\n    print(f\"  Mean HM: {stock_trades['hm'].mean():.4f}\")\n    print(f\"  Mean Buy Herding: {stock_trades['buy_herd'].mean():.4f}\")\n    print(f\"  Mean Sell Herding: {stock_trades['sell_herd'].mean():.4f}\")\n    print(f\"  % stocks with herding: {(stock_trades['hm'] &gt; 0).mean():.1%}\")\n    \n    return stock_trades, ts_herding\n\n# herding_all, herding_ts = compute_herding_vietnam(trades)\n# herding_foreign, _ = compute_herding_vietnam(\n#     trades, owner_types=[OwnershipType.FOREIGN_INST]\n# )",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "16_institutional_ownership.html#sec-conclusion",
    "href": "16_institutional_ownership.html#sec-conclusion",
    "title": "13  Institutional Ownership Analytics in Vietnam",
    "section": "13.12 Conclusion and Practical Recommendations",
    "text": "13.12 Conclusion and Practical Recommendations\n\n13.12.1 Summary of Measures\nTable 13.5 summarizes all institutional ownership measures developed in this chapter for the Vietnamese market.\n\n\n\nTable 13.5: Summary of All Ownership Measures for Vietnam\n\n\n\n\n\n\n\n\n\n\n\nMeasure\nDefinition\nKey Adaptation for Vietnam\nPython Function\n\n\n\n\nIO Ratio\nInst. shares / TSO\nDecomposed into state, foreign, domestic\ncompute_ownership_decomposition()\n\n\nHHI Concentration\n\\(\\sum w_j^2\\)\nSeparate HHI for total, non-state, foreign\ncompute_io_metrics_vietnam()\n\n\nΔBreadth\nLehavy-Sloan adjusted\nApplied to irregular disclosure intervals\ncompute_io_metrics_vietnam()\n\n\nFOL Utilization\nForeign % / FOL limit\nVietnam-specific; no US equivalent\nFOLAnalyzer\n\n\nFOL Premium\nPrice impact of FOL proximity\nCross-sectional regression approach\nFOLAnalyzer.estimate_fol_premium()\n\n\nTrades\nΔShares (corp-action adjusted)\nCritical: adjust for stock dividends\nderive_trades_vectorized_vietnam()\n\n\nFund Turnover\nmin(B,S)/avg(A)\nSemi-annual frequency; annualized\ncompute_fund_analytics()\n\n\nSOE Status\nState ownership &gt; 50%\nTracks equitization program\nanalyze_state_ownership()\n\n\nLSV Herding\n\\(|p - E[p]| - E[|p - E[p]|]\\)\nSeparate foreign vs domestic herding\ncompute_herding_vietnam()\n\n\nCo-Ownership Network\nShared institutional holders\nReveals conglomerate linkages\nconstruct_stock_coownership_network()\n\n\n\n\n\n\n\n\n13.12.2 Data Quality Checklist for Vietnam\n\n\n\n\n\n\nTipVietnam Data Quality Checklist\n\n\n\n\nCorporate actions: Have you built and applied adjustment factors for ALL stock dividends, bonus shares, splits, and rights issues?\nShareholder classification: Have you verified the owner type classification (state vs foreign vs domestic institutional vs individual)?\nFOL limits: Are sector-specific FOL limits correctly assigned (30% for banks, 49% standard, unlimited for some sectors)?\nDisclosure dates: Are you using the actual disclosure date (not the record date or ex-date) for ownership snapshots?\nTreasury shares: Are treasury shares excluded from ownership ratio denominators?\nUPCOM coverage: Does your sample include or exclude UPCOM stocks (which have weaker disclosure requirements)?\nCross-listings: Are you handling NVDR (Non-Voting Depository Receipts) if applicable after market reforms?\nName consistency: Are shareholder names standardized across disclosure periods (Vietnamese names can have multiple romanization forms)?\nTrade adjustment: When deriving trades between periods, have you adjusted previous shares for ALL intervening corporate actions?\nFund mandate changes: For fund analytics, have you accounted for fund mergers, closures, and mandate changes that affect time-series continuity?\n\n\n\n\n\n13.12.3 Comparison with US Framework\n\n\n\nTable 13.6: US vs Vietnam Institutional Ownership Framework Comparison\n\n\n\n\n\n\n\n\n\n\nDimension\nUS (WRDS/13F)\nVietnam (DataCore.vn)\n\n\n\n\nDisclosure\nQuarterly 13F (mandatory)\nAnnual reports + event-driven\n\n\nCoverage\nInstitutions &gt; $100M AUM\nAll shareholders in annual reports\n\n\nOwnership observed\nLong positions only\nComplete decomposition\n\n\nIO can exceed 100%\nYes (short selling)\nNo (by construction)\n\n\nPermanent ID\nCRSP PERMNO\nTicker (with manual tracking of changes)\n\n\nAdjustment factors\nCRSP cfacshr\nMust build from corporate actions\n\n\nInvestor classification\nLSEG typecode / Bushee\nState/Foreign/Domestic/Individual\n\n\nShort selling\nNot in 13F; exists in market\nVery limited; not a concern\n\n\nUnique features\n—\nFOL, SOE ownership, stock dividend frequency\n\n\n\n\n\n\n\n\n\n\n\n\nBao Dinh, Ngoc, and Van Nguyen Hong Tran. 2024. “Institutional Ownership and Stock Liquidity: Evidence from an Emerging Market.” SAGE Open 14 (1): 21582440241239116.\n\n\nBen-David, ITZHAK, Francesco Franzoni, Augustin Landier, and Rabih Moussawi. 2013. “Do Hedge Funds Manipulate Stock Prices?” The Journal of Finance 68 (6): 2383–2434.\n\n\nCarhart, Mark M. 1997. “On Persistence in Mutual Fund Performance.” The Journal of Finance 52 (1): 57–82.\n\n\nChen, Joseph, Harrison Hong, and Jeremy C Stein. 2002. “Breadth of Ownership and Stock Returns.” Journal of Financial Economics 66 (2-3): 171–205.\n\n\nHuang, Xiangqian, Clark Liu, and Tao Shu. 2023. “Factors and Anomalies in the Vietnamese Stock Market.” Pacific-Basin Finance Journal 82: 102176.\n\n\nLehavy, Reuven, and Richard G Sloan. 2008. “Investor Recognition and Stock Returns.” Review of Accounting Studies 13 (2): 327–61.\n\n\nVo, Xuan Vinh. 2015. “Foreign Ownership and Stock Return Volatility–Evidence from Vietnam.” Journal of Multinational Financial Management 30: 101–9.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Institutional Ownership Analytics in Vietnam</span>"
    ]
  },
  {
    "objectID": "18_event_studies.html",
    "href": "18_event_studies.html",
    "title": "15  Event Studies in Finance",
    "section": "",
    "text": "15.0.1 Why Event Studies Matter\nEvent studies constitute one of the most enduring and widely deployed empirical methodologies in financial economics. At their core, event studies measure the impact of a specific event on the value of a firm by examining abnormal security returns around the time the event occurs. The methodology rests on a simple premise: if capital markets are informationally efficient, the effect of an event will be reflected immediately in security prices, and any deviation from “normal” expected returns can be attributed to the event itself.\nSince the pioneering work of Eugene F. Fama et al. (1969), who studied how stock prices adjust to new information around stock splits, event studies have become a cornerstone of empirical research across finance, accounting, economics, and law. Ball and Brown (2013) demonstrated that accounting earnings announcements convey information to the market, a finding that launched decades of research in accounting and disclosure. The methodology has since been refined through contributions by Brown and Warner (1980) and Brown and Warner (1985), who established the statistical properties of event study methods, and MacKinlay (1997) codified best practices that remain standard today.\nThe breadth of applications is remarkable. Event studies have been used to examine the wealth effects of mergers and acquisitions (Jensen and Ruback 1983; Andrade, Mitchell, and Stafford 2001), earnings announcements (Bernard and Thomas 1989), dividend changes (Aharony and Swary 1980), regulatory changes (Schwert 1981), executive turnover (Warner, Watts, and Wruck 1988), and macroeconomic announcements (Flannery and Protopapadakis 2002). In law and economics, event studies serve as the primary tool for measuring damages in securities fraud litigation (Mitchell and Netter 1993) and assessing the impact of regulatory interventions (Binder 1998). Kothari and Warner (2007) documented over 500 published event studies in the top five finance journals alone between 1974 and 2000.\nThe enduring popularity of event studies stems from several compelling properties:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "18_event_studies.html#literature-review-and-methodological-evolution",
    "href": "18_event_studies.html#literature-review-and-methodological-evolution",
    "title": "15  Event Studies in Finance",
    "section": "15.1 Literature Review and Methodological Evolution",
    "text": "15.1 Literature Review and Methodological Evolution\n\n15.1.1 The Classical Framework (1969-1985)\nThe modern event study traces its origins to Eugene F. Fama et al. (1969), hereafter FFJR, who examined monthly stock returns around 940 stock splits between 1927 and 1959. Their key innovation was the use of the market model to decompose returns into expected (normal) and unexpected (abnormal) components.\nBall and Brown (2013) independently developed a similar approach to study earnings announcements, establishing the information content of accounting data. It was a finding with profound implications for both the efficient markets hypothesis and the relevance of financial reporting.\nBrown and Warner (1980) provided the first systematic analysis of event study methodology using simulation. Their study of monthly data established several important results: (i) the simple market model performs at least as well as more complex models, (ii) value-weighted market indices can lead to misspecification when the sample is tilted toward smaller firms, and (iii) the standard cross-sectional test has well-specified size under the null hypothesis. Their follow-up study (Brown and Warner 1985) extended the analysis to daily data, documenting the importance of non-normality in daily returns and the increased power of daily versus monthly studies.\n\n\n15.1.2 Risk Model Refinements (1992-2015)\nThe advent of the Fama-French three-factor model (Eugene F. Fama and French 1993) represented a major advance in modeling expected returns. Adding size (SMB) and value (HML) factors to the market model improved the cross-sectional fit of expected returns considerably. Carhart (1997) augmented this with a momentum factor (UMD), yielding the four-factor model that became standard in event studies through the 2000s. Eugene F. Fama and French (2015) subsequently introduced profitability (RMW) and investment (CMA) factors in their five-factor model.\nThe choice of risk model matters for event studies primarily in long-horizon settings. Kothari and Warner (2007) showed that for short-window studies (3-5 days), the market model and multi-factor models produce virtually identical results because the incremental factors explain very little daily return variation for individual firms. However, for event windows exceeding 20 trading days, model choice can materially affect inferences.\n\n\n15.1.3 Testing for Abnormal Returns (1976-2010)\nThe statistical testing of abnormal returns has evolved considerably:\n\n\n\nTable 15.1: Summary of major event study test statistics\n\n\n\n\n\n\n\n\n\n\n\nTest\nYear\nKey Property\nReference\n\n\n\n\nPatell Z\n1976\nStandardizes by estimation-period \\(\\sigma\\); weights firms inversely by volatility\nPatell (1976)\n\n\nCross-Sectional \\(t\\)\n1980\nAllows event-induced variance change\nBrown and Warner (1980)\n\n\nBMP\n1991\nRobust to event-induced variance\nBoehmer, Masumeci, and Poulsen (1991)\n\n\nCorrado Rank\n1989\nNon-parametric; robust to non-normality\nCorrado (1989)\n\n\nGeneralized Sign\n1992\nNon-parametric; uses estimation-window baseline\nCowan (1992)\n\n\nKolari-Pynnönen\n2010\nAccounts for cross-sectional dependence\nKolari and Pynnönen (2010)\n\n\nSkewness-Adjusted\n1992\nCorrects for BHAR skewness\nHall (1992)\n\n\n\n\n\n\n\n\n15.1.4 CARs versus BHARs\nCumulative abnormal returns (CARs) sum daily abnormal returns, while buy-and-hold abnormal returns (BHARs) compound returns and subtract the compounded benchmark. Barber and Lyon (1997) demonstrated that BHARs better capture the actual investor experience, since investors earn compound, not cumulative, returns. However, Eugene F. Fama (1998) and Mitchell and Stafford (2000) showed that BHARs exhibit severe cross-sectional dependence and positive skewness. For short event windows (under 10 days), the difference between CARs and BHARs is negligible. For longer windows, both should be reported.\n\n\n15.1.5 Emerging Market Considerations\nEvent studies in emerging markets face distinct challenges:\n\nThin trading. Many emerging market securities trade infrequently, inducing bias in market model beta estimates. Scholes and Williams (1977) and Dimson (1979) proposed corrections using leading and lagging market returns.\nFactor availability. While Fama-French factors are readily available for developed markets, emerging market factors must often be constructed locally.\nMarket microstructure. Price limits (\\(\\pm\\) 7% on HOSE, \\(\\pm\\) 10% on HNX, \\(\\pm\\) 15% on UPCOM in Vietnam), T+2 settlement, and the absence of short-selling affect the speed of price adjustment. Researchers should consider wider event windows to accommodate slower information incorporation (Bhattacharya et al. 2000; Griffin, Kelly, and Nardari 2010).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "18_event_studies.html#mathematical-framework",
    "href": "18_event_studies.html#mathematical-framework",
    "title": "15  Event Studies in Finance",
    "section": "15.2 Mathematical Framework",
    "text": "15.2 Mathematical Framework\nThis section presents the complete mathematical specification of the event study methodology. We follow the notation conventions of Campbell et al. (1998) and Kothari and Warner (2007).\n\n15.2.1 Timeline and Windows\nThe event study timeline is defined relative to the event date, denoted \\(\\tau = 0\\). All dates are measured in trading days:\n\\[\n\\underbrace{T_0 + 1, \\ldots, T_1}_{\\text{Estimation Window (L₁ days)}} \\quad \\underbrace{\\quad}_{\\text{Gap (G days)}} \\quad \\underbrace{\\tau_1, \\ldots, 0, \\ldots, \\tau_2}_{\\text{Event Window (L₂ days)}}\n\\]\nwhere:\n\nEstimation window: \\(L_1\\) trading days over which the risk model parameters are estimated\nGap: \\(G\\) trading days separating estimation and event windows, preventing contamination by pre-event information leakage\nEvent window: \\(L_2 = \\tau_2 - \\tau_1 + 1\\) trading days centered around the event date\n\nFor example, with \\(L_1 = 150\\), \\(G = 15\\), \\(\\tau_1 = -10\\), \\(\\tau_2 = +10\\): the estimation window covers trading days \\([-175, -25]\\) relative to the event, and the event window covers \\([-10, +10]\\).\n\n\n15.2.2 Normal Return Models\nLet \\(R_{it}\\) denote the return on security \\(i\\) on trading day \\(t\\), \\(R_{ft}\\) the risk-free rate, and \\(R_{mt}\\) the market return. We implement six models:\nModel 0: Market-Adjusted Returns. Assumes \\(\\beta_i = 1\\) and \\(\\alpha_i = 0\\) for all firms:\n\\[\nAR_{it}^{MA} = R_{it} - R_{mt}\n\\]\nModel 1: Market Model (Sharpe 1964):\n\\[\nR_{it} = \\alpha_i + \\beta_i R_{mt} + \\varepsilon_{it}, \\quad E[\\varepsilon_{it}] = 0, \\quad \\text{Var}[\\varepsilon_{it}] = \\sigma^2_{\\varepsilon_i}\n\\]\n\\[\nAR_{it}^{MM} = R_{it} - \\hat{\\alpha}_i - \\hat{\\beta}_i R_{mt}\n\\]\nModel 2: Fama-French Three-Factor (Eugene F. Fama and French 1993):\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\varepsilon_{it}\n\\]\nModel 3: Carhart Four-Factor (Carhart 1997):\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\beta_{i,4} \\cdot UMD_t + \\varepsilon_{it}\n\\]\nModel 4: Fama-French Five-Factor (Eugene F. Fama and French 2015):\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\beta_{i,1}(R_{mt} - R_{ft}) + \\beta_{i,2} \\cdot SMB_t + \\beta_{i,3} \\cdot HML_t + \\beta_{i,4} \\cdot RMW_t + \\beta_{i,5} \\cdot CMA_t + \\varepsilon_{it}\n\\]\nModel 5: User-Specified Factor Model:\n\\[\nR_{it} - R_{ft} = \\alpha_i + \\sum_{k=1}^{K} \\beta_{i,k} F_{k,t} + \\varepsilon_{it}\n\\]\n\n\n15.2.3 Aggregation: CARs and BHARs\nCumulative Abnormal Returns sum daily abnormal returns:\n\\[\nCAR_i(\\tau_1, \\tau_2) = \\sum_{t=\\tau_1}^{\\tau_2} AR_{it}, \\qquad \\overline{CAR}(\\tau_1, \\tau_2) = \\frac{1}{N} \\sum_{i=1}^{N} CAR_i(\\tau_1, \\tau_2)\n\\]\nBuy-and-Hold Abnormal Returns compound returns:\n\\[\nBHAR_i(\\tau_1, \\tau_2) = \\prod_{t=\\tau_1}^{\\tau_2}(1 + R_{it}) - \\prod_{t=\\tau_1}^{\\tau_2}(1 + \\hat{E}[R_{it}])\n\\]\n\n\n15.2.4 Standardized Returns\nThe standardized abnormal return for firm \\(i\\) on day \\(t\\) is:\n\\[\nSAR_{it} = \\frac{AR_{it}}{\\hat{\\sigma}_{\\varepsilon_i}}\n\\]\nThe standardized cumulative abnormal return is:\n\\[\nSCAR_i(\\tau_1, \\tau_2) = \\frac{CAR_i(\\tau_1, \\tau_2)}{\\hat{\\sigma}_{\\varepsilon_i} \\sqrt{L_2}}\n\\]\n\n\n15.2.5 Test Statistics\nLet \\(N\\) denote the number of firm-event observations.\nTest 1: Cross-Sectional \\(t\\)-Test. Allows event-induced variance; assumes cross-sectional independence:\n\\[\nt_{CS} = \\frac{\\overline{CAR}}{s_{CAR}/\\sqrt{N}}, \\quad s_{CAR} = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(CAR_i - \\overline{CAR})^2}\n\\]\nTest 2: Patell Z-Test (Patell 1976). Weights firms inversely by volatility:\n\\[\nZ_{Patell} = \\frac{\\sum_{i=1}^{N} SCAR_i}{\\sqrt{\\sum_{i=1}^{N} \\frac{K_i - 2}{K_i - 4}}}\n\\]\nTest 3: BMP Test (Boehmer, Masumeci, and Poulsen 1991). Robust to event-induced variance:\n\\[\nt_{BMP} = \\frac{\\overline{SCAR}}{s_{SCAR}/\\sqrt{N}}\n\\]\nTest 4: Kolari-Pynnönen Adjusted BMP (Kolari and Pynnönen 2010). Accounts for cross-sectional dependence:\n\\[\nt_{KP} = t_{BMP} \\times \\sqrt{\\frac{1}{1 + (N-1)\\bar{r}}}\n\\]\nwhere \\(\\bar{r}\\) is the mean pairwise cross-correlation of estimation-period residuals.\nTest 5: Generalized Sign Test (Cowan 1992):\n\\[\nZ_{GSign} = \\frac{\\hat{p} - \\hat{p}_0}{\\sqrt{\\hat{p}_0(1-\\hat{p}_0)/N}}\n\\]\nTest 6: Sign Test:\n\\[\nZ_{Sign} = \\frac{N^{+} - 0.5N}{\\sqrt{0.25N}}\n\\]\nTest 7: Skewness-Adjusted \\(t\\)-Test (Hall 1992):\n\\[\nt_{SA} = \\sqrt{N}\\left(\\bar{z} + \\frac{1}{3}\\hat{\\gamma}\\bar{z}^2 + \\frac{1}{27}\\hat{\\gamma}^2\\bar{z}^3 + \\frac{1}{6N}\\hat{\\gamma}\\right\n\\]\nTest 8: Wilcoxon Signed-Rank Test: A non-parametric test of whether the median CAR differs from zero.\nThe table below summarizes the assumptions of each test:\n\n\n\nTable 15.2: Assumption requirements for event study test statistics\n\n\n\n\n\n\n\n\n\n\n\nTest\nEvent-Induced Variance\nCross-Sectional Independence\nNormality\n\n\n\n\nCross-Sectional \\(t\\)\nRobust\n✗ Assumes\n✗ Assumes\n\n\nPatell Z\n✗ Assumes no change\n✗ Assumes\n✗ Assumes\n\n\nBMP\nRobust\n✗ Assumes\n✗ Assumes\n\n\nKolari-Pynnönen\nRobust\nRobust\n✗ Assumes\n\n\nGeneralized Sign\nRobust\n✗ Assumes\nRobust\n\n\nCorrado Rank\nRobust\n✗ Assumes\nRobust\n\n\nSkewness-Adjusted\nRobust\n✗ Assumes\nPartially\n\n\nWilcoxon\nRobust\n✗ Assumes\nRobust",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "18_event_studies.html#python-implementation",
    "href": "18_event_studies.html#python-implementation",
    "title": "15  Event Studies in Finance",
    "section": "15.3 Python Implementation",
    "text": "15.3 Python Implementation\n\n15.3.1 Design Philosophy\nOur implementation follows these principles:\n\nModularity: Each component (calendar, estimation, AR computation, testing) is a separate function.\nVectorization: All operations use pandas/numpy for performance on large datasets.\nConfigurability: All parameters are user-configurable via a dataclass.\nTransparency: Intermediate outputs are preserved for inspection.\nProduction-ready: Comprehensive input validation, missing data handling, and edge cases.\n\n\n\n15.3.2 Setup and Imports\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Tuple\nfrom enum import Enum\nimport warnings\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.float_format', '{:.6f}'.format)\nprint(\"All libraries loaded.\")\n\nAll libraries loaded.\n\n\n\nimport pandas as pd\nimport sqlite3\n\ntidy_finance = sqlite3.connect(database=\"data/tidy_finance_python.sqlite\")\n\nfactors_ff3_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff3_daily\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff5_daily = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_daily\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff3_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff3_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\nfactors_ff5_monthly = pd.read_sql_query(\n    sql=\"SELECT * FROM factors_ff5_monthly\",\n    con=tidy_finance,\n    parse_dates=[\"date\"]\n)\n\n\nprices_monthly = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_monthly\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\nprices_daily = pd.read_sql_query(\n    sql=\"\"\"\n        SELECT symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\n        FROM prices_daily\n    \"\"\",\n    con=tidy_finance,\n    parse_dates={\"date\"}\n).dropna()\n\n\n\n15.3.3 Configuration\n\nclass RiskModel(Enum):\n    \"\"\"Supported risk models for expected return computation.\"\"\"\n    MARKET_ADJ = \"market_adjusted\"\n    MARKET_MODEL = \"market_model\"\n    FF3 = \"ff3\"\n    CARHART = \"carhart\"\n    FF5 = \"ff5\"\n    CUSTOM = \"custom\"\n\n@dataclass\nclass EventStudyConfig:\n    \"\"\"Complete configuration for an event study.\n    \n    Attributes\n    ----------\n    estimation_window : int\n        Length of estimation period in trading days. Brown and Warner (1985)\n        suggest ≥100 days; MacKinlay (1997) recommends 120 as standard.\n    event_window_start : int\n        Start of event window relative to event date (e.g., -10).\n    event_window_end : int\n        End of event window relative to event date (e.g., +10).\n    gap : int\n        Trading days between estimation and event windows. Prevents\n        contamination from pre-event information leakage.\n    min_estimation_obs : int\n        Minimum non-missing returns required in estimation period.\n    risk_model : RiskModel\n        Risk model for computing expected returns.\n    custom_factors : list\n        Column names for user-specified factors (CUSTOM model only).\n    thin_trading_adj : str or None\n        None, 'scholes_williams', or 'dimson'.\n    dimson_lags : int\n        Number of leads/lags for Dimson (1979) correction.\n    \"\"\"\n    estimation_window: int = 150\n    event_window_start: int = -10\n    event_window_end: int = 10\n    gap: int = 15\n    min_estimation_obs: int = 120\n    risk_model: RiskModel = RiskModel.MARKET_MODEL\n    custom_factors: List[str] = field(default_factory=list)\n    thin_trading_adj: Optional[str] = None\n    dimson_lags: int = 1\n    \n    @property\n    def event_window_length(self) -&gt; int:\n        return self.event_window_end - self.event_window_start + 1\n    \n    def validate(self):\n        assert self.estimation_window &gt; 0\n        assert self.event_window_start &lt;= self.event_window_end\n        assert self.gap &gt;= 0\n        assert self.min_estimation_obs &lt;= self.estimation_window\n        if self.risk_model == RiskModel.CUSTOM:\n            assert len(self.custom_factors) &gt; 0\n        return True\n\n# Demonstrate\nconfig_demo = EventStudyConfig(\n    estimation_window=150, event_window_start=-10, event_window_end=10,\n    gap=15, min_estimation_obs=120, risk_model=RiskModel.FF3\n)\nconfig_demo.validate()\nprint(f\"Event window length: {config_demo.event_window_length} days\")\nprint(f\"Model: {config_demo.risk_model.value}\")\n\nEvent window length: 21 days\nModel: ff3\n\n\n\n\n15.3.4 Step 1: Trading Calendar Construction\nA correct trading calendar is fundamental. It maps any event date to the exact calendar dates for the start/end of estimation and event windows, accounting for weekends, holidays, and non-trading days.\n\ndef build_trading_calendar(trading_dates, config):\n    \"\"\"Build a trading calendar mapping event dates to window boundaries.\n    \n    For each potential event date, identifies the calendar dates for the\n    start/end of the estimation period and event window using only actual\n    trading days.\n    \n    Parameters\n    ----------\n    trading_dates : array-like\n        Sorted unique trading dates in the market.\n    config : EventStudyConfig\n    \n    Returns\n    -------\n    pd.DataFrame with columns: estper_beg, estper_end, evtwin_beg,\n        evtdate, evtwin_end, cal_index\n    \"\"\"\n    dates = pd.Series(sorted(pd.to_datetime(trading_dates).unique()))\n    n = len(dates)\n    \n    L1 = config.estimation_window\n    G = config.gap\n    s = config.event_window_start\n    L2 = config.event_window_length\n    \n    # Offsets (FIRSTOBS logic)\n    o0 = 0                      # estper_beg\n    o1 = L1 - 1                 # estper_end\n    o2 = L1 + G                 # evtwin_beg\n    o3 = L1 + G - s             # evtdate\n    o4 = L1 + G + L2 - 1        # evtwin_end\n    \n    max_offset = o4\n    valid = n - max_offset\n    if valid &lt;= 0:\n        raise ValueError(f\"Need ≥{max_offset+1} trading dates, have {n}\")\n    \n    cal = pd.DataFrame({\n        'estper_beg': dates.iloc[o0:o0+valid].values,\n        'estper_end': dates.iloc[o1:o1+valid].values,\n        'evtwin_beg': dates.iloc[o2:o2+valid].values,\n        'evtdate':    dates.iloc[o3:o3+valid].values,\n        'evtwin_end': dates.iloc[o4:o4+valid].values,\n    })\n    cal['cal_index'] = range(1, len(cal)+1)\n    \n    # Validate window lengths using a sample row\n    idx = min(10, len(cal)-1)\n    row = cal.iloc[idx]\n    est_n = dates[(dates &gt;= row['estper_beg']) & (dates &lt;= row['estper_end'])].shape[0]\n    evt_n = dates[(dates &gt;= row['evtwin_beg']) & (dates &lt;= row['evtwin_end'])].shape[0]\n    assert est_n == L1, f\"Estimation window: {est_n} ≠ {L1}\"\n    assert evt_n == L2, f\"Event window: {evt_n} ≠ {L2}\"\n    \n    return cal\n\n# Demo\ndemo_dates = pd.bdate_range('2018-01-01', '2023-12-31', freq='B')\ndemo_cal = build_trading_calendar(demo_dates, config_demo)\nprint(f\"Calendar: {len(demo_cal)} potential event dates\")\nprint(demo_cal.head(3).to_string(index=False))\n\nCalendar: 1380 potential event dates\nestper_beg estper_end evtwin_beg    evtdate evtwin_end  cal_index\n2018-01-01 2018-07-27 2018-08-20 2018-09-03 2018-09-17          1\n2018-01-02 2018-07-30 2018-08-21 2018-09-04 2018-09-18          2\n2018-01-03 2018-07-31 2018-08-22 2018-09-05 2018-09-19          3\n\n\n\n\n15.3.5 Step 2: Event Date Alignment\nWhen an event occurs on a non-trading day, align to the next available trading day.\n\ndef align_events(events, calendar, id_col='symbol', date_col='event_date'):\n    \"\"\"Align event dates to trading calendar.\n    \n    Non-trading-day events are shifted forward to the next trading day.\n    \n    Parameters\n    ----------\n    events : pd.DataFrame with [id_col, date_col] and optional 'group'\n    calendar : pd.DataFrame from build_trading_calendar()\n    \n    Returns\n    -------\n    pd.DataFrame with window boundaries for each firm-event\n    \"\"\"\n    events = events.copy()\n    events[date_col] = pd.to_datetime(events[date_col])\n    \n    cal_dates = calendar[['evtdate']].drop_duplicates().sort_values('evtdate')\n    \n    merged = pd.merge_asof(\n        events.sort_values(date_col),\n        cal_dates.rename(columns={'evtdate': 'aligned_date'}),\n        left_on=date_col, right_on='aligned_date',\n        direction='forward'\n    )\n    \n    result = merged.merge(calendar, left_on='aligned_date', right_on='evtdate', how='inner')\n    \n    shifted = (result[date_col] != result['evtdate']).sum()\n    if shifted &gt; 0:\n        print(f\"  {shifted} event(s) shifted to next trading day\")\n    \n    result = result.rename(columns={date_col: 'original_date'})\n    result = result.drop_duplicates(subset=[id_col, 'evtdate'])\n    \n    return result\n\n\n\n15.3.6 Step 3: Data Extraction and Factor Merging\nExtract returns for each security-event across the full estimation + event window and merge risk factors.\n\ndef extract_returns(aligned_events, prices, factors, config,\n                    id_col='symbol', date_col='date', ret_col='ret',\n                    mkt_col='mkt_excess', rf_col='risk_free'):\n    \"\"\"Extract stock returns and merge risk factors for each event.\n    \n    For each security-event, retrieves daily returns from estper_beg\n    through evtwin_end and merges appropriate risk factors.\n    \"\"\"\n    prices = prices.copy()\n    factors = factors.copy()\n    prices[date_col] = pd.to_datetime(prices[date_col])\n    factors[date_col] = pd.to_datetime(factors[date_col])\n    \n    # Recover raw return from excess return if needed\n    if ret_col not in prices.columns and 'ret_excess' in prices.columns:\n        if rf_col in factors.columns:\n            prices = prices.merge(factors[[date_col, rf_col]].drop_duplicates(),\n                                  on=date_col, how='left')\n        prices[ret_col] = prices['ret_excess'] + prices[rf_col]\n    \n    # Factor columns based on model\n    model = config.risk_model\n    fac_cols = [mkt_col] if mkt_col in factors.columns else []\n    if rf_col in factors.columns:\n        fac_cols.append(rf_col)\n    \n    model_factors = {\n        RiskModel.FF3: ['smb', 'hml'],\n        RiskModel.CARHART: ['smb', 'hml', 'umd'],\n        RiskModel.FF5: ['smb', 'hml', 'rmw', 'cma'],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n    for f in model_factors.get(model, []):\n        if f in factors.columns:\n            fac_cols.append(f)\n    \n    fac_cols = list(set([date_col] + fac_cols))\n    \n    # Vectorized merge approach: join events with prices on id + date range\n    frames = []\n    for _, evt in aligned_events.iterrows():\n        mask = ((prices[id_col] == evt[id_col]) &\n                (prices[date_col] &gt;= evt['estper_beg']) &\n                (prices[date_col] &lt;= evt['evtwin_end']))\n        fd = prices.loc[mask, [id_col, date_col, ret_col]].copy()\n        if len(fd) == 0:\n            continue\n        fd['evtdate'] = evt['evtdate']\n        fd['estper_beg'] = evt['estper_beg']\n        fd['estper_end'] = evt['estper_end']\n        fd['evtwin_beg'] = evt['evtwin_beg']\n        fd['evtwin_end'] = evt['evtwin_end']\n        if 'group' in evt.index:\n            fd['group'] = evt['group']\n        frames.append(fd)\n    \n    if not frames:\n        raise ValueError(\"No return data found for any events\")\n    \n    result = pd.concat(frames, ignore_index=True)\n    result = result.merge(factors[fac_cols].drop_duplicates(), on=date_col, how='left')\n    \n    # Excess and market-adjusted returns\n    if rf_col in result.columns:\n        result['ret_excess'] = result[ret_col] - result[rf_col]\n    else:\n        result['ret_excess'] = result[ret_col]\n    if mkt_col in result.columns:\n        result['ret_mktadj'] = result['ret_excess'] - result[mkt_col]\n    \n    result = result.sort_values([id_col, 'evtdate', date_col]).reset_index(drop=True)\n    n_evts = result.groupby([id_col, 'evtdate']).ngroups\n    print(f\"  Extracted {len(result):,} obs for {n_evts} firm-events\")\n    return result\n\n\n\n15.3.7 Step 4: Risk Model Estimation\nEstimate risk model parameters over the estimation window.\n\ndef estimate_model(\n    event_returns, config, id_col=\"symbol\", date_col=\"date\", ret_col=\"ret\"\n):\n    \"\"\"Estimate risk model parameters for each firm-event.\n\n    Runs OLS over the estimation window. Returns alpha, betas, sigma,\n    R^2, nobs, and residuals for cross-correlation computation.\n    \"\"\"\n    model = config.risk_model\n\n    # Define regression specification\n    dep_var_map = {\n        RiskModel.MARKET_ADJ: \"ret_mktadj\",\n        RiskModel.MARKET_MODEL: ret_col,\n        RiskModel.FF3: \"ret_excess\",\n        RiskModel.CARHART: \"ret_excess\",\n        RiskModel.FF5: \"ret_excess\",\n        RiskModel.CUSTOM: \"ret_excess\",\n    }\n    indep_var_map = {\n        RiskModel.MARKET_ADJ: [],\n        RiskModel.MARKET_MODEL: [\"mkt_excess\"],\n        RiskModel.FF3: [\"mkt_excess\", \"smb\", \"hml\"],\n        RiskModel.CARHART: [\"mkt_excess\", \"smb\", \"hml\", \"umd\"],\n        RiskModel.FF5: [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n\n    dep_var = dep_var_map[model]\n    indep_vars = indep_var_map[model]\n\n    est = event_returns[\n        (event_returns[date_col] &gt;= event_returns[\"estper_beg\"])\n        & (event_returns[date_col] &lt;= event_returns[\"estper_end\"])\n    ].copy()\n\n    params_list = []\n\n    for (firm, evtdate), grp in est.groupby([id_col, \"evtdate\"]):\n        valid = grp.dropna(subset=[dep_var] + indep_vars)\n        nobs = len(valid)\n        if nobs &lt; config.min_estimation_obs:\n            continue\n\n        y = valid[dep_var].values\n\n        if len(indep_vars) == 0:\n            # Market-adjusted: intercept-only for variance\n            p = {\n                id_col: firm,\n                \"evtdate\": evtdate,\n                \"alpha\": y.mean(),\n                \"sigma\": y.std(ddof=1),\n                \"variance\": y.var(ddof=1),\n                \"nobs\": nobs,\n                \"r_squared\": 0.0,\n                \"_residuals\": y - y.mean(),\n            }\n        else:\n            X = sm.add_constant(valid[indep_vars].values)\n            res = sm.OLS(y, X).fit()\n            p = {\n                id_col: firm,\n                \"evtdate\": evtdate,\n                \"alpha\": res.params[0],\n                \"sigma\": np.sqrt(res.mse_resid),\n                \"variance\": res.mse_resid,\n                \"nobs\": nobs,\n                \"r_squared\": res.rsquared if np.isfinite(res.rsquared) else np.nan,\n                \"_residuals\": res.resid,\n            }\n            for j, var in enumerate(indep_vars):\n                p[f\"beta_{var}\"] = res.params[j + 1]\n\n        # Skip degenerate firms (zero or near-zero variance)\n        if p[\"sigma\"] &lt; 1e-6:\n            continue\n\n        params_list.append(p)\n\n    if not params_list:\n        raise ValueError(\"No firm-events passed minimum observation filter\")\n\n    params_df = pd.DataFrame(params_list)\n    n_total = event_returns.groupby([id_col, \"evtdate\"]).ngroups\n    print(\n        f\"  Estimated {len(params_df)}/{n_total} firm-events \"\n        f\"(mean R^2 = {params_df['r_squared'].dropna().mean():.4f})\"\n    )\n    return params_df\n\n\n\n15.3.8 Step 5: Abnormal Return Computation\nCompute AR, CAR, BHAR, SAR, SCAR for each firm-event-date.\n\ndef compute_abnormal_returns(\n    event_returns, params, config, id_col=\"symbol\", date_col=\"date\", ret_col=\"ret\"\n):\n    \"\"\"Compute abnormal returns and aggregate to CARs/BHARs.\n\n    Returns\n    -------\n    daily_ar : pd.DataFrame - daily AR/SAR/CAR/BHAR per firm-event-date\n    event_ar : pd.DataFrame - event-level CAR/BHAR/SCAR per firm-event\n    \"\"\"\n    model = config.risk_model\n\n    factor_map = {\n        RiskModel.MARKET_ADJ: [],\n        RiskModel.MARKET_MODEL: [\"mkt_excess\"],\n        RiskModel.FF3: [\"mkt_excess\", \"smb\", \"hml\"],\n        RiskModel.CARHART: [\"mkt_excess\", \"smb\", \"hml\", \"umd\"],\n        RiskModel.FF5: [\"mkt_excess\", \"smb\", \"hml\", \"rmw\", \"cma\"],\n        RiskModel.CUSTOM: config.custom_factors,\n    }\n    factor_cols = factor_map[model]\n\n    # Filter to event window\n    evt = event_returns[\n        (event_returns[date_col] &gt;= event_returns[\"evtwin_beg\"])\n        & (event_returns[date_col] &lt;= event_returns[\"evtwin_end\"])\n    ].copy()\n\n    # Merge params (drop residuals column for merge)\n    merge_cols = [c for c in params.columns if c != \"_residuals\"]\n    evt = evt.merge(params[merge_cols], on=[id_col, \"evtdate\"], how=\"inner\")\n\n    # Expected returns\n    if model == RiskModel.MARKET_ADJ:\n        evt[\"expected_ret\"] = evt.get(\"mkt_excess\", 0) + evt.get(\"risk_free\", 0)\n        evt[\"AR\"] = evt[ret_col] - evt[\"expected_ret\"]\n    else:\n        evt[\"expected_ret\"] = evt[\"alpha\"]\n        for fc in factor_cols:\n            bcol = f\"beta_{fc}\"\n            if bcol in evt.columns:\n                evt[\"expected_ret\"] += evt[bcol] * evt[fc]\n\n        if model == RiskModel.MARKET_MODEL:\n            evt[\"AR\"] = evt[ret_col] - evt[\"expected_ret\"]\n        else:\n            evt[\"AR\"] = evt[\"ret_excess\"] - evt[\"expected_ret\"]\n\n    evt[\"SAR\"] = evt[\"AR\"] / evt[\"sigma\"]\n    evt = evt.sort_values([id_col, \"evtdate\", date_col])\n\n    # Compute event time\n    all_dates = sorted(event_returns[date_col].unique())\n    d2i = {d: i for i, d in enumerate(all_dates)}\n    evt[\"evttime\"] = evt[date_col].map(d2i) - evt[\"evtdate\"].map(d2i)\n\n    # Cumulative measures per firm-event\n    daily_recs = []\n    event_recs = []\n\n    for (firm, evtdate), g in evt.groupby([id_col, \"evtdate\"]):\n        g = g.sort_values(date_col).copy()\n        nd = len(g)\n\n        g[\"CAR\"] = g[\"AR\"].cumsum()\n        g[\"cum_ret\"] = (1 + g[ret_col]).cumprod() - 1\n        g[\"cum_expected\"] = (1 + g[\"expected_ret\"]).cumprod() - 1\n        g[\"BHAR\"] = g[\"cum_ret\"] - g[\"cum_expected\"]\n        g[\"SCAR\"] = g[\"CAR\"] / (g[\"sigma\"].iloc[0] * np.sqrt(np.arange(1, nd + 1)))\n\n        daily_recs.append(g)\n\n        last = g.iloc[-1]\n        sigma = g[\"sigma\"].iloc[0]\n        nobs = g[\"nobs\"].iloc[0]\n\n        rec = {\n            id_col: firm,\n            \"evtdate\": evtdate,\n            \"CAR\": last[\"CAR\"],\n            \"BHAR\": last[\"BHAR\"],\n            \"cum_ret\": last[\"cum_ret\"],\n            \"SCAR\": last[\"CAR\"] / (sigma * np.sqrt(nd)),\n            \"sigma\": sigma,\n            \"variance\": g[\"variance\"].iloc[0],\n            \"nobs\": nobs,\n            \"n_event_days\": nd,\n            \"alpha\": g[\"alpha\"].iloc[0],\n            \"pat_scale\": (nobs - 2) / (nobs - 4) if nobs &gt; 4 else np.nan,\n            \"pos_car\": int(last[\"CAR\"] &gt; 0),\n        }\n\n        for fc in factor_cols:\n            bcol = f\"beta_{fc}\"\n            if bcol in g.columns:\n                rec[bcol] = g[bcol].iloc[0]\n        if \"group\" in g.columns:\n            rec[\"group\"] = g[\"group\"].iloc[0]\n\n        event_recs.append(rec)\n\n    daily_ar = pd.concat(daily_recs, ignore_index=True)\n    event_ar = pd.DataFrame(event_recs)\n\n    print(\n        f\"  {len(event_ar)} firm-events | Mean CAR: {event_ar['CAR'].mean():.6f} | \"\n        f\"Mean BHAR: {event_ar['BHAR'].mean():.6f} | \"\n        f\"% positive: {event_ar['pos_car'].mean():.1%}\"\n    )\n    return daily_ar, event_ar\n\n\n\n15.3.9 Step 6: Comprehensive Test Statistics\nEight tests covering parametric, non-parametric, and cross-correlation-robust approaches.\n\ndef compute_test_statistics(event_ar, params=None, group_col=None):\n    \"\"\"Compute comprehensive test statistics for abnormal returns.\n    \n    Implements 8 tests with varying assumptions about variance,\n    cross-dependence, and distributional form.\n    \"\"\"\n    def _stats(data, label=None):\n        N = len(data)\n        if N &lt; 3:\n            return None\n        \n        cars = data['CAR'].values\n        bhars = data['BHAR'].values\n        scars = data['SCAR'].values\n        pos = data['pos_car'].values\n        \n        m_car, s_car = np.mean(cars), np.std(cars, ddof=1)\n        m_scar, s_scar = np.mean(scars), np.std(scars, ddof=1)\n        \n        r = {'group': label or 'All', 'N': N,\n             'mean_CAR': m_car, 'median_CAR': np.median(cars),\n             'std_CAR': s_car, 'mean_BHAR': np.mean(bhars),\n             'pct_positive': np.mean(pos)}\n        \n        # 1. Cross-sectional t\n        t1 = m_car / (s_car / np.sqrt(N)) if s_car &gt; 0 else np.nan\n        r['t_CS'] = t1\n        r['p_CS'] = 2 * (1 - stats.t.cdf(abs(t1), N-1)) if np.isfinite(t1) else np.nan\n        \n        # 2. Patell Z\n        if 'pat_scale' in data.columns:\n            ps = data['pat_scale'].dropna().values\n            z2 = np.sum(scars[:len(ps)]) / np.sqrt(np.sum(ps)) if len(ps) &gt; 0 else np.nan\n        else:\n            z2 = m_scar * np.sqrt(N)\n        r['Z_Patell'] = z2\n        r['p_Patell'] = 2*(1-stats.norm.cdf(abs(z2))) if np.isfinite(z2) else np.nan\n        \n        # 3. BMP\n        t3 = m_scar / (s_scar / np.sqrt(N)) if s_scar &gt; 0 else np.nan\n        r['t_BMP'] = t3\n        r['p_BMP'] = 2*(1-stats.t.cdf(abs(t3), N-1)) if np.isfinite(t3) else np.nan\n        \n        # 4. Kolari-Pynnönen\n        rbar = 0.0\n        if params is not None and '_residuals' in params.columns:\n            resids = [row['_residuals'] for _, row in params.iterrows()\n                      if isinstance(row.get('_residuals'), np.ndarray)]\n            if len(resids) &gt; 1:\n                ml = min(len(x) for x in resids)\n                aligned = np.column_stack([x[:ml] for x in resids])\n                cm = np.corrcoef(aligned.T)\n                np.fill_diagonal(cm, 0)\n                rbar = cm.sum() / (len(resids) * (len(resids)-1))\n        \n        adj = np.sqrt(1/(1+(N-1)*rbar)) if (1+(N-1)*rbar) &gt; 0 else 1\n        t4 = t3 * adj if np.isfinite(t3) else np.nan\n        r['t_KP'] = t4\n        r['p_KP'] = 2*(1-stats.t.cdf(abs(t4), N-1)) if np.isfinite(t4) else np.nan\n        r['r_bar'] = rbar\n        \n        # 5. Generalized sign test\n        p_hat = np.mean(pos)\n        z5 = (p_hat - 0.5) / np.sqrt(0.25 / N)\n        r['Z_GSign'] = z5\n        r['p_GSign'] = 2*(1-stats.norm.cdf(abs(z5)))\n        \n        # 6. Sign test\n        r['Z_Sign'] = z5  # Same formula with p0=0.5\n        r['p_Sign'] = r['p_GSign']\n        \n        # 7. Skewness-adjusted t\n        if s_scar &gt; 0:\n            zb = m_scar / s_scar\n            gam = stats.skew(scars)\n            t7 = np.sqrt(N) * (zb + gam*zb**2/3 + gam**2*zb**3/27 + gam/(6*N))\n            r['t_SkAdj'] = t7\n            r['p_SkAdj'] = 2*(1-stats.t.cdf(abs(t7), N-1)) if np.isfinite(t7) else np.nan\n        \n        # 8. Wilcoxon signed-rank\n        try:\n            w, pw = stats.wilcoxon(cars, alternative='two-sided')\n            r['W_Wilcoxon'] = w\n            r['p_Wilcoxon'] = pw\n        except:\n            r['W_Wilcoxon'] = r['p_Wilcoxon'] = np.nan\n        \n        return r\n    \n    results = [_stats(event_ar)]\n    if group_col and group_col in event_ar.columns:\n        for gv, gd in event_ar.groupby(group_col):\n            s = _stats(gd, label=gv)\n            if s:\n                results.append(s)\n    \n    return pd.DataFrame([r for r in results if r is not None])\n\n\ndef compute_daily_stats(daily_ar, id_col='symbol'):\n    \"\"\"Compute test statistics at each event time t.\"\"\"\n    rows = []\n    for t, g in daily_ar.groupby('evttime'):\n        n = g[id_col].nunique()\n        if n &lt; 2:\n            continue\n        m_ar = g['AR'].mean()\n        s_ar = g['AR'].std(ddof=1)\n        t_ar = m_ar / (s_ar/np.sqrt(n)) if s_ar &gt; 0 else np.nan\n        rows.append({'evttime': t, 'N': n, 'mean_AR': m_ar,\n                     'mean_CAR': g['CAR'].mean(), 'mean_BHAR': g['BHAR'].mean(),\n                     'mean_cum_ret': g.get('cum_ret', pd.Series()).mean(),\n                     't_AR': t_ar})\n    return pd.DataFrame(rows).sort_values('evttime')\n\n\n\n15.3.10 Step 7: Publication-Ready Visualization\n\ndef plot_event_study(daily_stats, title=\"Cumulative Abnormal Returns Around Event Date\",\n                     figsize=(12, 7), save_path=None):\n    \"\"\"Publication-ready event study plot with CAR, BHAR, and daily AR panels.\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=figsize, height_ratios=[3, 1],\n                              gridspec_kw={'hspace': 0.05})\n    ds = daily_stats.sort_values('evttime')\n    t = ds['evttime'].values\n    \n    # Top: cumulative returns\n    ax = axes[0]\n    ax.plot(t, ds['mean_CAR']*100, color='#2166AC', lw=2.5, label='Mean CAR')\n    ax.plot(t, ds['mean_BHAR']*100, color='#B2182B', lw=2, ls='--', label='Mean BHAR')\n    if 'mean_cum_ret' in ds.columns:\n        ax.plot(t, ds['mean_cum_ret']*100, color='#666', lw=1.5, ls=':', \n                label='Mean Cum. Return', alpha=0.7)\n    ax.axvline(0, color='k', lw=0.8, alpha=0.5)\n    ax.axhline(0, color='k', lw=0.5, alpha=0.3)\n    ax.set_ylabel('Cumulative Return (%)', fontsize=12)\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.legend(loc='upper left', fontsize=10)\n    ax.grid(True, alpha=0.2)\n    ax.set_xticklabels([])\n    \n    # Bottom: daily AR bars\n    ax2 = axes[1]\n    colors = ['#2166AC' if v &gt;= 0 else '#B2182B' for v in ds['mean_AR']]\n    ax2.bar(t, ds['mean_AR']*100, color=colors, alpha=0.7, width=0.8)\n    if 't_AR' in ds.columns:\n        sig = np.abs(ds['t_AR'].values) &gt; 1.96\n        if sig.any():\n            ax2.scatter(t[sig], ds['mean_AR'].values[sig]*100, \n                       color='gold', s=40, marker='*', zorder=4, label='p&lt;0.05')\n            ax2.legend(fontsize=9)\n    ax2.axvline(0, color='k', lw=0.8, alpha=0.5)\n    ax2.axhline(0, color='k', lw=0.5, alpha=0.3)\n    ax2.set_xlabel('Event Time (Trading Periods)', fontsize=12)\n    ax2.set_ylabel('Mean AR (%)', fontsize=10)\n    ax2.grid(True, alpha=0.2)\n    \n    for a in axes:\n        a.spines['top'].set_visible(False)\n        a.spines['right'].set_visible(False)\n    plt.tight_layout()\n    if save_path:\n        fig.savefig(save_path, dpi=300, bbox_inches='tight')\n    return fig\n\n\ndef plot_car_distribution(event_ar, var='CAR', figsize=(12, 5)):\n    \"\"\"Cross-sectional distribution of CARs with histogram and QQ plot.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n    data = event_ar[var].dropna() * 100\n    \n    ax1.hist(data, bins=50, density=True, alpha=0.6, color='#2166AC', edgecolor='white')\n    ax1.axvline(data.mean(), color='k', ls='--', lw=1.5, \n                label=f'Mean={data.mean():.2f}%')\n    ax1.axvline(data.median(), color='gray', ls=':', lw=1.5,\n                label=f'Median={data.median():.2f}%')\n    ax1.set_xlabel(f'{var} (%)')\n    ax1.set_ylabel('Density')\n    ax1.set_title(f'Distribution of {var}', fontweight='bold')\n    ax1.legend()\n    ax1.spines['top'].set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    \n    # QQ plot\n    (osm, osr), (slope, intercept, r) = stats.probplot(data, dist='norm')\n    ax2.scatter(osm, osr, alpha=0.4, s=10, color='#2166AC')\n    ax2.plot(osm, slope*np.array(osm)+intercept, 'r--', lw=1)\n    ax2.set_xlabel('Theoretical Quantiles')\n    ax2.set_ylabel('Sample Quantiles')\n    ax2.set_title('Q-Q Plot (Normal)', fontweight='bold')\n    ax2.spines['top'].set_visible(False)\n    ax2.spines['right'].set_visible(False)\n    \n    plt.tight_layout()\n    return fig\n\n\n\n15.3.11 The Master Pipeline\nCombine all components into one function:\n\ndef run_event_study(events, prices, factors, config,\n                    id_col='symbol', date_col='date', ret_col='ret',\n                    event_date_col='event_date', mkt_col='mkt_excess',\n                    rf_col='risk_free', group_col=None, verbose=True):\n    \"\"\"Run a complete event study from raw inputs to test statistics.\n    \n    This is the main entry point. Provide your events, price data,\n    factor data, and configuration—get back everything you need.\n    \n    Parameters\n    ----------\n    events : pd.DataFrame\n        Columns: [id_col, event_date_col], optional 'group'.\n    prices : pd.DataFrame\n        Daily returns: [id_col, date_col, ret_col or 'ret_excess', rf_col].\n    factors : pd.DataFrame\n        Factor returns: [date_col, mkt_col, 'smb', 'hml', ...].\n    config : EventStudyConfig\n    \n    Returns\n    -------\n    dict with keys: 'config', 'daily_ar', 'event_ar', 'daily_stats',\n        'test_stats', 'params'\n    \"\"\"\n    config.validate()\n    \n    if verbose:\n        print(f\"═══ Event Study: {config.risk_model.value} model ═══\")\n        print(f\"  Windows: estimation={config.estimation_window}, \"\n              f\"gap={config.gap}, event=({config.event_window_start},{config.event_window_end})\")\n        print(f\"  Min obs: {config.min_estimation_obs}\\n\")\n    \n    # 1. Trading calendar\n    if verbose: print(\"Step 1: Building trading calendar...\")\n    trading_dates = pd.Series(sorted(prices[date_col].unique()))\n    calendar = build_trading_calendar(trading_dates, config)\n    if verbose: print(f\"  {len(calendar)} potential event dates\\n\")\n    \n    # 2. Align events\n    if verbose: print(\"Step 2: Aligning events to trading calendar...\")\n    aligned = align_events(events, calendar, id_col, event_date_col)\n    if verbose: print(f\"  {len(aligned)} aligned events\\n\")\n    \n    # 3. Extract returns\n    if verbose: print(\"Step 3: Extracting returns and merging factors...\")\n    evt_rets = extract_returns(aligned, prices, factors, config,\n                               id_col, date_col, ret_col, mkt_col, rf_col)\n    if verbose: print()\n    \n    # 4. Estimate model\n    if verbose: print(\"Step 4: Estimating risk model parameters...\")\n    params = estimate_model(evt_rets, config, id_col, date_col, ret_col)\n    if verbose: print()\n    \n    # 5. Compute abnormal returns\n    if verbose: print(\"Step 5: Computing abnormal returns...\")\n    daily_ar, event_ar = compute_abnormal_returns(\n        evt_rets, params, config, id_col, date_col, ret_col)\n    if verbose: print()\n    \n    # 6. Test statistics\n    if verbose: print(\"Step 6: Computing test statistics...\")\n    test_stats = compute_test_statistics(event_ar, params, group_col)\n    daily_stats = compute_daily_stats(daily_ar, id_col)\n    if verbose:\n        print(f\"  Done.\\n\")\n        print(\"═══ Results Summary ═══\")\n        cols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 't_BMP', 'p_BMP', 't_KP', 'p_KP']\n        avail = [c for c in cols if c in test_stats.columns]\n        print(test_stats[avail].to_string(index=False))\n    \n    return {\n        'config': config,\n        'params': params,\n        'daily_ar': daily_ar,\n        'event_ar': event_ar,\n        'daily_stats': daily_stats,\n        'test_stats': test_stats,\n        'calendar': calendar,\n    }\n\nprint(\"Master pipeline ready.\")\n\nMaster pipeline ready.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "18_event_studies.html#demonstration-with-simulated-data",
    "href": "18_event_studies.html#demonstration-with-simulated-data",
    "title": "15  Event Studies in Finance",
    "section": "15.4 Demonstration with Simulated Data",
    "text": "15.4 Demonstration with Simulated Data\nSince we are building a general-purpose framework (the actual event data will be supplied later), we demonstrate the full pipeline with realistic simulated data.\n\nnp.random.seed(2024)\n\n# --- Simulated trading calendar (Vietnamese market: ~245 days/year) ---\ndates = pd.bdate_range('2019-01-01', '2023-12-31', freq='B')\n# Remove Tet + national holidays (simplified)\ntet_holidays = pd.to_datetime([\n    '2019-02-04','2019-02-05','2019-02-06','2019-02-07','2019-02-08',\n    '2020-01-23','2020-01-24','2020-01-27','2020-01-28','2020-01-29',\n    '2021-02-10','2021-02-11','2021-02-12','2021-02-15','2021-02-16',\n    '2022-01-31','2022-02-01','2022-02-02','2022-02-03','2022-02-04',\n    '2023-01-20','2023-01-23','2023-01-24','2023-01-25','2023-01-26',\n])\ndates = dates.difference(tet_holidays)\nT = len(dates)\n\n# --- Simulated factors (realistic Vietnamese market parameters) ---\nrf_daily = 0.04 / 252  # ~4% annual risk-free\nmkt_excess = np.random.normal(0.0003, 0.012, T)  # ~7.5% annual, ~19% vol\nsmb = np.random.normal(0.0001, 0.006, T)\nhml = np.random.normal(0.0001, 0.005, T)\nrmw = np.random.normal(0.00005, 0.004, T)\ncma = np.random.normal(0.00005, 0.004, T)\n\nfactors_sim = pd.DataFrame({\n    'date': dates, 'mkt_excess': mkt_excess, 'smb': smb, 'hml': hml,\n    'rmw': rmw, 'cma': cma, 'risk_free': rf_daily\n})\n\n# --- 100 simulated stocks ---\nn_stocks = 100\nsymbols = [f'SIM{i:03d}' for i in range(n_stocks)]\nbetas = np.random.uniform(0.5, 1.5, n_stocks)\nalphas = np.random.normal(0, 0.0002, n_stocks)\nidio_vols = np.random.uniform(0.015, 0.035, n_stocks)\n\nprice_rows = []\nfor i, sym in enumerate(symbols):\n    eps = np.random.normal(0, idio_vols[i], T)\n    rets = alphas[i] + betas[i] * mkt_excess + 0.3*smb + 0.2*hml + eps\n    for j in range(T):\n        price_rows.append({\n            'symbol': sym, 'date': dates[j], 'ret': rets[j],\n            'ret_excess': rets[j] - rf_daily,\n            'risk_free': rf_daily,\n            'mktcap': np.random.uniform(100, 5000),\n        })\n\nprices_sim = pd.DataFrame(price_rows)\n\n# --- Simulated events: 50 random firm-dates with KNOWN positive AR ---\nevent_indices = np.random.choice(range(250, T-50), 50, replace=False)\nevent_firms = np.random.choice(symbols, 50, replace=True)\nevent_dates_sim = [dates[i] for i in event_indices]\n\n# Inject abnormal returns on event date (2% positive shock)\nfor firm, edate in zip(event_firms, event_dates_sim):\n    mask = (prices_sim['symbol'] == firm) & (prices_sim['date'] == edate)\n    prices_sim.loc[mask, 'ret'] += 0.02\n    prices_sim.loc[mask, 'ret_excess'] += 0.02\n\nevents_sim = pd.DataFrame({\n    'symbol': event_firms,\n    'event_date': event_dates_sim,\n    'group': np.random.choice([1, 2], 50)\n})\n\nprint(f\"Simulated data: {n_stocks} stocks × {T} days = {len(prices_sim):,} obs\")\nprint(f\"Events: {len(events_sim)} firm-event pairs\")\nprint(f\"Injected abnormal return: +2% on event date\")\n\nSimulated data: 100 stocks × 1279 days = 127,900 obs\nEvents: 50 firm-event pairs\nInjected abnormal return: +2% on event date\n\n\n\n15.4.1 Running the Full Pipeline\n\nconfig = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\nresults = run_event_study(\n    events=events_sim,\n    prices=prices_sim,\n    factors=factors_sim,\n    config=config,\n    group_col='group'\n)\n\n═══ Event Study: ff3 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  1094 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 9,300 obs for 50 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 50/50 firm-events (mean R^2 = 0.2368)\n\nStep 5: Computing abnormal returns...\n  50 firm-events | Mean CAR: 0.033009 | Mean BHAR: 0.032498 | % positive: 60.0%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\ngroup  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n  All 50  0.033009   0.032498      0.600000 2.288362 0.026468 2.157106 0.035929 2.161291 0.035587\n    1 20  0.030704   0.028326      0.650000 1.568676 0.133227 1.248382 0.227056 1.249320 0.226720\n    2 30  0.034545   0.035280      0.566667 1.688848 0.101975 1.734699 0.093413 1.736689 0.093055\n\n\n\n\n15.4.2 Visualizing Results\n\nfig1 = plot_event_study(\n    results['daily_stats'],\n    title=\"Event Study: FF3 Model — Simulated Vietnamese Market\"\n)\nplt.show()\n\n\n\n\n\n\n\nFigure 15.1: Dynamics of cumulative abnormal returns (CARs) and buy-and-hold abnormal returns (BHARs) around the event date. The positive jump at t=0 reflects the injected 2% abnormal return.\n\n\n\n\n\n\nfig2 = plot_car_distribution(results['event_ar'], 'CAR')\nplt.show()\n\n\n\n\n\n\n\nFigure 15.2: Cross-sectional distribution of cumulative abnormal returns. The rightward shift from zero and positive skewness are consistent with the injected positive event effect.\n\n\n\n\n\n\n\n15.4.3 Complete Test Statistics\n\n\n\nTable 15.3: Event study test statistics for the full sample and by subgroup\n\n\n# Format for display\nts = results['test_stats'].copy()\n\n# Select key columns\ndisplay_cols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 'Z_Patell', 'p_Patell',\n                't_BMP', 'p_BMP', 't_KP', 'p_KP',\n                'Z_GSign', 'p_GSign', 't_SkAdj', 'p_SkAdj']\navail = [c for c in display_cols if c in ts.columns]\ndisplay_df = ts[avail].copy()\n\n# Format\nfor c in display_df.columns:\n    if c in ['N']:\n        display_df[c] = display_df[c].astype(int)\n    elif c.startswith('p_'):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4f}' if pd.notna(x) else '')\n    elif c in ['mean_CAR', 'mean_BHAR']:\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4%}' if pd.notna(x) else '')\n    elif c == 'pct_positive':\n        display_df[c] = display_df[c].map(lambda x: f'{x:.1%}' if pd.notna(x) else '')\n    elif isinstance(display_df[c].iloc[0], (int, float, np.floating)):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(display_df.to_string(index=False))\n\ngroup  N mean_CAR mean_BHAR pct_positive  t_CS   p_CS Z_Patell p_Patell t_BMP  p_BMP  t_KP   p_KP Z_GSign p_GSign t_SkAdj p_SkAdj\n  All 50  3.3009%   3.2498%        60.0% 2.288 0.0265    1.832   0.0669 2.157 0.0359 2.161 0.0356   1.414  0.1573   2.074  0.0434\n    1 20  3.0704%   2.8326%        65.0% 1.569 0.1332    1.020   0.3075 1.248 0.2271 1.249 0.2267   1.342  0.1797   1.103  0.2836\n    2 30  3.4545%   3.5280%        56.7% 1.689 0.1020    1.532   0.1255 1.735 0.0934 1.737 0.0931   0.730  0.4652   1.727  0.0949\n\n\n\n\n\n\n15.4.4 Running Multiple Models for Robustness\nA key best practice is to report results across multiple risk models. If conclusions are robust across models, this strengthens the findings:\n\nmodels_to_run = [\n    (\"Market-Adjusted\", RiskModel.MARKET_ADJ),\n    (\"Market Model\", RiskModel.MARKET_MODEL),\n    (\"Fama-French 3\", RiskModel.FF3),\n    (\"Fama-French 5\", RiskModel.FF5),\n]\n\nrobustness = []\nfor name, mdl in models_to_run:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=-10, event_window_end=10,\n        gap=15, min_estimation_obs=120, risk_model=mdl\n    )\n    res = run_event_study(events_sim, prices_sim, factors_sim, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    robustness.append({\n        'Model': name,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.2f}\",\n        't (BMP)': f\"{full['t_BMP']:.2f}\",\n        't (KP)': f\"{full.get('t_KP', np.nan):.2f}\",\n    })\n\nrob_df = pd.DataFrame(robustness)\nprint(\"Robustness Across Risk Models:\")\nprint(rob_df.to_string(index=False))\n\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.0000)\n  50 firm-events | Mean CAR: 0.029672 | Mean BHAR: 0.026468 | % positive: 62.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2198)\n  50 firm-events | Mean CAR: 0.033785 | Mean BHAR: 0.029974 | % positive: 62.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2368)\n  50 firm-events | Mean CAR: 0.033009 | Mean BHAR: 0.032498 | % positive: 60.0%\n  Extracted 9,300 obs for 50 firm-events\n  Estimated 50/50 firm-events (mean R^2 = 0.2516)\n  50 firm-events | Mean CAR: 0.036479 | Mean BHAR: 0.036000 | % positive: 64.0%\nRobustness Across Risk Models:\n          Model  N Mean CAR Mean BHAR % Positive t (CS) t (BMP) t (KP)\nMarket-Adjusted 50  2.9672%   2.6468%      62.0%   2.12    2.03   2.01\n   Market Model 50  3.3785%   2.9974%      62.0%   2.37    2.26   2.28\n  Fama-French 3 50  3.3009%   3.2498%      60.0%   2.29    2.16   2.16\n  Fama-French 5 50  3.6479%   3.6000%      64.0%   2.51    2.36   2.41",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "18_event_studies.html#sec-usage",
    "href": "18_event_studies.html#sec-usage",
    "title": "15  Event Studies in Finance",
    "section": "15.5 How to Use This Framework with Your Data",
    "text": "15.5 How to Use This Framework with Your Data\n\n15.5.1 Required Data Format\nTo run the event study on real Vietnamese market data, prepare three inputs:\n1. Stock Returns (prices DataFrame):\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nsymbol\nStock ticker\n'VNM'\n\n\ndate\nTrading date\n2023-06-15\n\n\nret or ret_excess\nDaily return (decimal)\n0.0123\n\n\nrisk_free\nDaily risk-free rate\n0.000159\n\n\n\n2. Factor Returns (factors DataFrame):\n\n\n\nColumn\nDescription\n\n\n\n\ndate\nTrading date\n\n\nmkt_excess\nMarket excess return\n\n\nsmb\nSize factor (FF3/FF5)\n\n\nhml\nValue factor (FF3/FF5)\n\n\nrmw\nProfitability factor (FF5)\n\n\ncma\nInvestment factor (FF5)\n\n\nrisk_free\nRisk-free rate\n\n\n\n3. Event File (events DataFrame):\n\n\n\nColumn\nDescription\nExample\n\n\n\n\nsymbol\nStock ticker\n'VNM'\n\n\nevent_date\nEvent date\n2023-03-15\n\n\ngroup\n(Optional) subgroup\n1\n\n\n\n\n\n15.5.2 Minimal Usage Example\n# Load your data\nprices = pd.read_csv('prices_daily.csv', parse_dates=['date'])\nfactors = pd.read_csv('factors_ff3_daily.csv', parse_dates=['date'])\nevents = pd.read_csv('my_events.csv', parse_dates=['event_date'])\n\n# Configure\nconfig = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-5,\n    event_window_end=5,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\n# Run\nresults = run_event_study(events, prices, factors, config)\n\n# Access outputs\nresults['test_stats']    # Test statistics\nresults['event_ar']      # Firm-level CARs/BHARs\nresults['daily_ar']      # Daily abnormal returns\nresults['daily_stats']   # Event-time aggregates\n\n# Plot\nplot_event_study(results['daily_stats'], title=\"My Event Study\")",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "18_event_studies.html#demonstration-with-vietnamese-market-data",
    "href": "18_event_studies.html#demonstration-with-vietnamese-market-data",
    "title": "15  Event Studies in Finance",
    "section": "15.6 Demonstration with Vietnamese Market Data",
    "text": "15.6 Demonstration with Vietnamese Market Data\nWe now demonstrate the full event study pipeline using actual Vietnamese stock market data. The datasets available are:\n\nprices_daily: symbol, date, ret_excess, mktcap, mktcap_lag, risk_free\nprices_monthly: same structure\nfactors_ff3_daily: date, smb, hml, mkt_excess, risk_free\nfactors_ff3_monthly — monthly frequency version\nfactors_ff5_daily: date, smb, hml, mkt_excess, risk_free, rmw, cma\nfactors_ff5_monthly\n\nSince our data provides ret_excess rather than raw returns, we recover raw returns as \\(R_{it} = R^e_{it} + R_{f,t}\\), and the market return as \\(R_{m,t} = R^e_{m,t} + R_{f,t}\\). The extract_event_returns() function handles this automatically.\n\n15.6.1 Loading the Data\n\n# --- Recover raw returns ---\n# ret = ret_excess + risk_free\nprices_daily['ret'] = prices_daily['ret_excess'] + prices_daily['risk_free']\nprices_monthly['ret'] = prices_monthly['ret_excess'] + prices_monthly['risk_free']\n\n# --- Inspect the data ---\nprint(\"=\" * 70)\nprint(\"VIETNAMESE MARKET DATA SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"\\nprices_daily: {prices_daily.shape[0]:,} rows, \"\n      f\"{prices_daily['symbol'].nunique()} stocks, \"\n      f\"{prices_daily['date'].min().date()} to {prices_daily['date'].max().date()}\")\nprint(f\"prices_monthly: {prices_monthly.shape[0]:,} rows, \"\n      f\"{prices_monthly['symbol'].nunique()} stocks\")\nprint(f\"\\nfactors_ff3_daily: {factors_ff3_daily.shape[0]:,} trading days\")\nprint(f\"  Columns: {list(factors_ff3_daily.columns)}\")\nprint(f\"factors_ff5_daily: {factors_ff5_daily.shape[0]:,} trading days\")\nprint(f\"  Columns: {list(factors_ff5_daily.columns)}\")\nprint(f\"\\nSample daily returns:\")\nprint(prices_daily[['symbol', 'date', 'ret_excess', 'ret', 'risk_free', 'mktcap']]\n      .head(5).to_string(index=False))\nprint(f\"\\nSample daily factors:\")\nprint(factors_ff3_daily.head(5).to_string(index=False))\n\n======================================================================\nVIETNAMESE MARKET DATA SUMMARY\n======================================================================\n\nprices_daily: 3,462,157 rows, 1459 stocks, 2010-01-05 to 2023-12-29\nprices_monthly: 165,499 rows, 1457 stocks\n\nfactors_ff3_daily: 3,126 trading days\n  Columns: ['date', 'smb', 'hml', 'mkt_excess', 'risk_free']\nfactors_ff5_daily: 3,126 trading days\n  Columns: ['date', 'smb', 'hml', 'rmw', 'cma', 'mkt_excess', 'risk_free']\n\nSample daily returns:\nsymbol       date  ret_excess      ret  risk_free     mktcap\n   A32 2018-10-24   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-25   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-26   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-29   -0.000159 0.000000   0.000159 176.120000\n   A32 2018-10-30   -0.000159 0.000000   0.000159 176.120000\n\nSample daily factors:\n      date       smb       hml  mkt_excess  risk_free\n2011-07-01  0.008587  0.000967   -0.019862   0.000159\n2011-07-04  0.005099 -0.001099   -0.000633   0.000159\n2011-07-05 -0.009088  0.010152    0.013314   0.000159\n2011-07-06  0.004875 -0.003918   -0.008045   0.000159\n2011-07-07 -0.011239 -0.000584    0.003391   0.000159\n\n\n\n\n15.6.2 Creating Sample Events\nFor this demonstration, we create a sample event file. In practice, events would come from corporate announcements (earnings, M&A, dividends), regulatory changes, or other information shocks. Here we select 50 large-cap Vietnamese stocks and assign random event dates from the most recent two years of data to illustrate the pipeline mechanics.\n\nnp.random.seed(2024)\n\n# Select the 50 largest stocks by median market cap\nlargest = (prices_daily.groupby('symbol')['mktcap']\n           .median()\n           .nlargest(50)\n           .index.tolist())\n\n# Date range for events: last 2 years of data, with buffer for windows\ndate_range = prices_daily['date'].sort_values().unique()\nn_dates = len(date_range)\n# Events from the middle portion (need room for estimation + event windows)\nevent_eligible = date_range[int(n_dates * 0.3):int(n_dates * 0.85)]\n\n# Generate 50 random firm-event pairs\nevent_firms = np.random.choice(largest, 50, replace=True)\nevent_dates = np.random.choice(event_eligible, 50, replace=False)\n\nevents_demo = pd.DataFrame({\n    'symbol': event_firms,\n    'event_date': pd.to_datetime(event_dates),\n    'group': np.random.choice(['Group_A', 'Group_B'], 50)\n})\n\n# Remove any duplicate firm-date pairs\nevents_demo = events_demo.drop_duplicates(subset=['symbol', 'event_date'])\n\nprint(f\"Sample event file: {len(events_demo)} firm-event observations\")\nprint(f\"Unique firms: {events_demo['symbol'].nunique()}\")\nprint(f\"Date range: {events_demo['event_date'].min().date()} to \"\n      f\"{events_demo['event_date'].max().date()}\")\nprint(f\"\\nGroup distribution:\")\nprint(events_demo['group'].value_counts().to_string())\nprint(f\"\\nFirst 10 events:\")\nprint(events_demo.sort_values('event_date').head(10).to_string(index=False))\n\nSample event file: 50 firm-event observations\nUnique firms: 35\nDate range: 2014-06-25 to 2021-10-29\n\nGroup distribution:\ngroup\nGroup_B    26\nGroup_A    24\n\nFirst 10 events:\nsymbol event_date   group\n   MCH 2014-06-25 Group_B\n   SIP 2014-10-23 Group_B\n   VRE 2014-11-14 Group_B\n   QNS 2014-12-25 Group_A\n   FOX 2015-01-16 Group_B\n   THD 2015-01-26 Group_A\n   QNS 2015-02-12 Group_A\n   HNG 2015-05-07 Group_B\n   MML 2015-08-17 Group_B\n   ACV 2015-10-15 Group_A\n\n\n\n\n15.6.3 Daily Event Study: Fama-French 3-Factor Model\n\nconfig_ff3 = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF3\n)\n\nresults_ff3 = run_event_study(\n    events=events_demo,\n    prices=prices_daily,\n    factors=factors_ff3_daily,\n    config=config_ff3,\n    group_col='group'\n)\n\n═══ Event Study: ff3 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  3308 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 5,421 obs for 30 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n\nStep 5: Computing abnormal returns...\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n    All 26  0.021513   0.024885      0.615385 0.774999 0.445609 0.726797 0.474101 0.699973 0.490407\nGroup_A 13  0.008601   0.006783      0.538462 0.211606 0.835966 0.577574 0.574229 0.567041 0.581138\nGroup_B 13  0.034425   0.042987      0.692308 0.879892 0.396197 0.437902 0.669236 0.429917 0.674875\n\n\n\n\n15.6.4 Visualizing Daily Results\n\nfig1 = plot_event_study(\n    results_ff3['daily_stats'],\n    title=\"Event Study: Fama-French 3-Factor Model — Vietnamese Market (Daily)\"\n)\nplt.show()\n\n\n\n\n\n\n\nFigure 15.3: Cumulative abnormal returns around event dates for Vietnamese stocks using the Fama-French 3-factor model. The event window spans [-10, +10] trading days.\n\n\n\n\n\n\nfig2 = plot_car_distribution(results_ff3['event_ar'], 'CAR')\nplt.show()\n\n\n\n\n\n\n\nFigure 15.4: Cross-sectional distribution of cumulative abnormal returns (CARs) across firm-events. The histogram and Q-Q plot assess normality assumptions underlying parametric tests.\n\n\n\n\n\n\n\n15.6.5 Complete Test Statistics (Daily)\n\n\n\nTable 15.4: Event study test statistics for the full sample and by subgroup — Daily frequency, FF3 model\n\n\nts = results_ff3['test_stats'].copy()\n\ndisplay_cols = ['group', 'N', 'mean_CAR', 'median_CAR', 'mean_BHAR', 'pct_positive',\n                't_CS', 'p_CS', 'Z_Patell', 'p_Patell',\n                't_BMP', 'p_BMP', 't_KP', 'p_KP',\n                'Z_GSign', 'p_GSign', 't_SkAdj', 'p_SkAdj',\n                'W_Wilcoxon', 'p_Wilcoxon']\navail = [c for c in display_cols if c in ts.columns]\ndisplay_df = ts[avail].copy()\n\nfor c in display_df.columns:\n    if c in ['N']:\n        display_df[c] = display_df[c].astype(int)\n    elif c == 'group':\n        continue\n    elif c.startswith('p_'):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4f}' if pd.notna(x) else '')\n    elif c in ['mean_CAR', 'median_CAR', 'mean_BHAR']:\n        display_df[c] = display_df[c].map(lambda x: f'{x:.4%}' if pd.notna(x) else '')\n    elif c == 'pct_positive':\n        display_df[c] = display_df[c].map(lambda x: f'{x:.1%}' if pd.notna(x) else '')\n    elif isinstance(display_df[c].iloc[0], (int, float, np.floating)):\n        display_df[c] = display_df[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(display_df.to_string(index=False))\n\n  group  N mean_CAR median_CAR mean_BHAR pct_positive  t_CS   p_CS Z_Patell p_Patell t_BMP  p_BMP  t_KP   p_KP Z_GSign p_GSign t_SkAdj p_SkAdj W_Wilcoxon p_Wilcoxon\n    All 26  2.1513%    1.5701%   2.4885%        61.5% 0.775 0.4456    0.961   0.3364 0.727 0.4741 0.700 0.4904   1.177  0.2393   0.738  0.4676    158.000     0.6710\nGroup_A 13  0.8601%    2.5330%   0.6783%        53.8% 0.212 0.8360    0.739   0.4596 0.578 0.5742 0.567 0.5811   0.277  0.7815   0.597  0.5618     45.000     1.0000\nGroup_B 13  3.4425%    1.4169%   4.2987%        69.2% 0.880 0.3962    0.620   0.5352 0.438 0.6692 0.430 0.6749   1.387  0.1655   0.444  0.6647     35.000     0.4973\n\n\n\n\n\n\n15.6.6 Robustness: Multiple Risk Models (Daily)\n\n\n\nTable 15.5: Robustness of event study results across risk models — Daily frequency\n\n\nmodels_daily = [\n    (\"Market-Adjusted\",  RiskModel.MARKET_ADJ,    factors_ff3_daily),\n    (\"Market Model\",     RiskModel.MARKET_MODEL,   factors_ff3_daily),\n    (\"Fama-French 3\",    RiskModel.FF3,            factors_ff3_daily),\n    (\"Fama-French 5\",    RiskModel.FF5,            factors_ff5_daily),\n]\n\nrobustness_daily = []\nfor name, mdl, facs in models_daily:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=-10, event_window_end=10,\n        gap=15, min_estimation_obs=120, risk_model=mdl\n    )\n    res = run_event_study(events_demo, prices_daily, facs, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    robustness_daily.append({\n        'Model': name,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Median CAR': f\"{full['median_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.3f}\",\n        'p (CS)': f\"{full['p_CS']:.4f}\",\n        't (BMP)': f\"{full['t_BMP']:.3f}\",\n        'p (BMP)': f\"{full['p_BMP']:.4f}\",\n        't (KP)': f\"{full.get('t_KP', np.nan):.3f}\",\n        'p (KP)': f\"{full.get('p_KP', np.nan):.4f}\",\n    })\n\nrob_daily_df = pd.DataFrame(robustness_daily)\nprint(\"Robustness Across Risk Models (Daily Frequency)\")\nprint(\"=\" * 100)\nprint(rob_daily_df.to_string(index=False))\n\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 28/30 firm-events (mean R^2 = 0.0000)\n  28 firm-events | Mean CAR: 0.035338 | Mean BHAR: 0.036936 | % positive: 50.0%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.1960)\n  26 firm-events | Mean CAR: 0.032107 | Mean BHAR: 0.033221 | % positive: 61.5%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2675)\n  26 firm-events | Mean CAR: 0.025684 | Mean BHAR: 0.028399 | % positive: 57.7%\nRobustness Across Risk Models (Daily Frequency)\n====================================================================================================\n          Model  N Mean CAR Median CAR Mean BHAR % Positive t (CS) p (CS) t (BMP) p (BMP) t (KP) p (KP)\nMarket-Adjusted 28  3.5338%    0.2610%   3.6936%      50.0%  1.390 0.1758   1.103  0.2798  1.062 0.2975\n   Market Model 26  3.2107%    0.6925%   3.3221%      61.5%  1.198 0.2422   1.146  0.2625  1.107 0.2789\n  Fama-French 3 26  2.1513%    1.5701%   2.4885%      61.5%  0.775 0.4456   0.727  0.4741  0.700 0.4904\n  Fama-French 5 26  2.5684%    2.3619%   2.8399%      57.7%  0.968 0.3422   0.987  0.3332  0.962 0.3451\n\n\n\n\n\n\n15.6.7 Robustness: Multiple Event Windows\nA key practice is to examine sensitivity to the event window specification:\n\n\n\nTable 15.6: Sensitivity of results to event window specification\n\n\nwindows = [\n    (\"(-1, +1)\",  -1, 1),\n    (\"(-3, +3)\",  -3, 3),\n    (\"(-5, +5)\",  -5, 5),\n    (\"(-10, +10)\", -10, 10),\n    (\"(-1, +5)\",  -1, 5),\n    (\"(-5, +1)\",  -5, 1),\n    (\"(0, 0)\",     0, 0),\n]\n\nwindow_results = []\nfor label, ws, we in windows:\n    cfg = EventStudyConfig(\n        estimation_window=150, event_window_start=ws, event_window_end=we,\n        gap=15, min_estimation_obs=120, risk_model=RiskModel.FF3\n    )\n    res = run_event_study(events_demo, prices_daily, factors_ff3_daily, cfg, verbose=False)\n    ts = res['test_stats']\n    full = ts[ts['group'] == 'All'].iloc[0]\n    window_results.append({\n        'Window': label,\n        'Days': we - ws + 1,\n        'N': int(full['N']),\n        'Mean CAR': f\"{full['mean_CAR']:.4%}\",\n        'Mean BHAR': f\"{full['mean_BHAR']:.4%}\",\n        '% Positive': f\"{full['pct_positive']:.1%}\",\n        't (CS)': f\"{full['t_CS']:.3f}\",\n        't (BMP)': f\"{full['t_BMP']:.3f}\",\n        'p (BMP)': f\"{full['p_BMP']:.4f}\",\n    })\n\nwin_df = pd.DataFrame(window_results)\nprint(\"Sensitivity to Event Window Specification (FF3 Model)\")\nprint(\"=\" * 90)\nprint(win_df.to_string(index=False))\n\n  Extracted 4,899 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2147)\n  26 firm-events | Mean CAR: 0.004074 | Mean BHAR: 0.004648 | % positive: 50.0%\n  Extracted 5,015 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2155)\n  26 firm-events | Mean CAR: 0.003761 | Mean BHAR: 0.004327 | % positive: 42.3%\n  Extracted 5,131 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2217)\n  26 firm-events | Mean CAR: -0.001133 | Mean BHAR: 0.001027 | % positive: 42.3%\n  Extracted 5,421 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2245)\n  26 firm-events | Mean CAR: 0.021513 | Mean BHAR: 0.024885 | % positive: 61.5%\n  Extracted 5,019 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2147)\n  26 firm-events | Mean CAR: -0.005096 | Mean BHAR: -0.005148 | % positive: 42.3%\n  Extracted 5,011 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2217)\n  26 firm-events | Mean CAR: 0.008600 | Mean BHAR: 0.010441 | % positive: 46.2%\n  Extracted 4,841 obs for 30 firm-events\n  Estimated 26/30 firm-events (mean R^2 = 0.2150)\n  26 firm-events | Mean CAR: 0.000344 | Mean BHAR: 0.000502 | % positive: 46.2%\nSensitivity to Event Window Specification (FF3 Model)\n==========================================================================================\n    Window  Days  N Mean CAR Mean BHAR % Positive t (CS) t (BMP) p (BMP)\n  (-1, +1)     3 26  0.4074%   0.4648%      50.0%  0.456   0.601  0.5535\n  (-3, +3)     7 26  0.3761%   0.4327%      42.3%  0.198   0.361  0.7211\n  (-5, +5)    11 26 -0.1133%   0.1027%      42.3% -0.049   0.030  0.9761\n(-10, +10)    21 26  2.1513%   2.4885%      61.5%  0.775   0.727  0.4741\n  (-1, +5)     7 26 -0.5096%  -0.5148%      42.3% -0.385  -0.068  0.9460\n  (-5, +1)     7 26  0.8600%   1.0441%      46.2%  0.439   0.429  0.6715\n    (0, 0)     1 26  0.0344%   0.0502%      46.2%  0.064  -0.020  0.9840\n\n\n\n\n\n\n15.6.8 Monthly Event Study: Fama-French 3-Factor Model\nFor longer-horizon studies, monthly frequency is appropriate. Note that the estimation window is specified in months rather than days:\n\n# Create monthly events aligned to the monthly data\n# Map daily event dates to the corresponding month-end\nevents_monthly = events_demo.copy()\nevents_monthly['event_date'] = events_monthly['event_date'].dt.to_period('M').dt.to_timestamp('M')\n\n# Use month-end dates from monthly prices\nmonthly_dates = prices_monthly['date'].sort_values().unique()\n\n# Filter events to dates present in monthly data\nevents_monthly = events_monthly[events_monthly['event_date'].isin(monthly_dates)]\nevents_monthly = events_monthly.drop_duplicates(subset=['symbol', 'event_date'])\n\nconfig_monthly = EventStudyConfig(\n    estimation_window=36,     # 36 months\n    event_window_start=-3,    # 3 months before\n    event_window_end=3,       # 3 months after\n    gap=3,                    # 3-month gap\n    min_estimation_obs=24,    # At least 24 months\n    risk_model=RiskModel.FF3\n)\n\nif len(events_monthly) &gt; 0:\n    results_monthly = run_event_study(\n        events=events_monthly,\n        prices=prices_monthly,\n        factors=factors_ff3_monthly,\n        config=config_monthly,\n        group_col='group'\n    )\n    \n    print(\"\\n--- Monthly Test Statistics ---\")\n    ts_m = results_monthly['test_stats']\n    mcols = ['group', 'N', 'mean_CAR', 'mean_BHAR', 'pct_positive',\n             't_CS', 'p_CS', 't_BMP', 'p_BMP']\n    mavail = [c for c in mcols if c in ts_m.columns]\n    print(ts_m[mavail].to_string(index=False))\nelse:\n    print(\"No monthly events could be aligned. Skipping monthly study.\")\n\n═══ Event Study: ff3 model ═══\n  Windows: estimation=36, gap=3, event=(-3,3)\n  Min obs: 24\n\nStep 1: Building trading calendar...\n  122 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 1,036 obs for 33 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 18/33 firm-events (mean R^2 = 0.3218)\n\nStep 5: Computing abnormal returns...\n  18 firm-events | Mean CAR: -0.005257 | Mean BHAR: -0.014576 | % positive: 55.6%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive      t_CS     p_CS     t_BMP    p_BMP      t_KP     p_KP\n    All 18 -0.005257  -0.014576      0.555556 -0.081058 0.936342 -0.249547 0.805928 -0.320212 0.752709\nGroup_A  7 -0.141756  -0.135084      0.428571 -1.102110 0.312648 -1.357452 0.223472 -1.462577 0.193905\nGroup_B 11  0.081606   0.062111      0.636364  1.390317 0.194599  1.177372 0.266309  1.342593 0.209085\n\n--- Monthly Test Statistics ---\n  group  N  mean_CAR  mean_BHAR  pct_positive      t_CS     p_CS     t_BMP    p_BMP\n    All 18 -0.005257  -0.014576      0.555556 -0.081058 0.936342 -0.249547 0.805928\nGroup_A  7 -0.141756  -0.135084      0.428571 -1.102110 0.312648 -1.357452 0.223472\nGroup_B 11  0.081606   0.062111      0.636364  1.390317 0.194599  1.177372 0.266309\n\n\n\nif len(events_monthly) &gt; 0 and 'daily_stats' in results_monthly:\n    fig3 = plot_event_study(\n        results_monthly['daily_stats'],\n        title=\"Event Study: FF3 Model — Vietnamese Market (Monthly)\"\n    )\n    plt.show()\n\n\n\n\n\n\n\nFigure 15.5: Monthly cumulative abnormal returns around event dates. Wider windows capture slower information incorporation typical of emerging markets.\n\n\n\n\n\n\n\n15.6.9 Daily Event Study: Fama-French 5-Factor Model\n\nconfig_ff5 = EventStudyConfig(\n    estimation_window=150,\n    event_window_start=-10,\n    event_window_end=10,\n    gap=15,\n    min_estimation_obs=120,\n    risk_model=RiskModel.FF5\n)\n\nresults_ff5 = run_event_study(\n    events=events_demo,\n    prices=prices_daily,\n    factors=factors_ff5_daily,\n    config=config_ff5,\n    group_col='group'\n)\n\n═══ Event Study: ff5 model ═══\n  Windows: estimation=150, gap=15, event=(-10,10)\n  Min obs: 120\n\nStep 1: Building trading calendar...\n  3308 potential event dates\n\nStep 2: Aligning events to trading calendar...\n  50 aligned events\n\nStep 3: Extracting returns and merging factors...\n  Extracted 5,421 obs for 30 firm-events\n\nStep 4: Estimating risk model parameters...\n  Estimated 26/30 firm-events (mean R^2 = 0.2675)\n\nStep 5: Computing abnormal returns...\n  26 firm-events | Mean CAR: 0.025684 | Mean BHAR: 0.028399 | % positive: 57.7%\n\nStep 6: Computing test statistics...\n  Done.\n\n═══ Results Summary ═══\n  group  N  mean_CAR  mean_BHAR  pct_positive     t_CS     p_CS    t_BMP    p_BMP     t_KP     p_KP\n    All 26  0.025684   0.028399      0.576923 0.968332 0.342154 0.986788 0.333201 0.962252 0.345139\nGroup_A 13  0.015352   0.013489      0.461538 0.393655 0.700741 0.786232 0.446980 0.776663 0.452395\nGroup_B 13  0.036016   0.043309      0.692308 0.965100 0.353542 0.585915 0.568789 0.578785 0.573438\n\n\n\n\n15.6.10 Comparing FF3 vs FF5 Estimation Quality\n\n\n\nTable 15.7: Comparison of estimation quality between FF3 and FF5 models\n\n\nparams_ff3 = results_ff3['params']\nparams_ff5 = results_ff5['params']\n\nprint(\"Model Estimation Diagnostics\")\nprint(\"=\" * 60)\nprint(f\"\\n{'Metric':&lt;30} {'FF3':&gt;12} {'FF5':&gt;12}\")\nprint(\"-\" * 54)\nprint(f\"{'Firm-events estimated':&lt;30} {len(params_ff3):&gt;12} {len(params_ff5):&gt;12}\")\nprint(f\"{'Mean R^2':&lt;30} {params_ff3['r_squared'].mean():&gt;12.4f} {params_ff5['r_squared'].mean():&gt;12.4f}\")\nprint(f\"{'Median R^2':&lt;30} {params_ff3['r_squared'].median():&gt;12.4f} {params_ff5['r_squared'].median():&gt;12.4f}\")\nprint(f\"{'Mean σ(ε)':&lt;30} {params_ff3['sigma'].mean():&gt;12.6f} {params_ff5['sigma'].mean():&gt;12.6f}\")\nprint(f\"{'Mean |α|':&lt;30} {params_ff3['alpha'].abs().mean():&gt;12.6f} {params_ff5['alpha'].abs().mean():&gt;12.6f}\")\nprint(f\"{'Mean β(MKT)':&lt;30} {params_ff3['beta_mkt_excess'].mean():&gt;12.4f} {params_ff5['beta_mkt_excess'].mean():&gt;12.4f}\")\nif 'beta_smb' in params_ff3.columns:\n    print(f\"{'Mean β(SMB)':&lt;30} {params_ff3['beta_smb'].mean():&gt;12.4f} {params_ff5['beta_smb'].mean():&gt;12.4f}\")\nif 'beta_hml' in params_ff3.columns:\n    print(f\"{'Mean β(HML)':&lt;30} {params_ff3['beta_hml'].mean():&gt;12.4f} {params_ff5['beta_hml'].mean():&gt;12.4f}\")\nif 'beta_rmw' in params_ff5.columns:\n    print(f\"{'Mean β(RMW)':&lt;30} {'—':&gt;12} {params_ff5['beta_rmw'].mean():&gt;12.4f}\")\nif 'beta_cma' in params_ff5.columns:\n    print(f\"{'Mean β(CMA)':&lt;30} {'—':&gt;12} {params_ff5['beta_cma'].mean():&gt;12.4f}\")\n\nModel Estimation Diagnostics\n============================================================\n\nMetric                                  FF3          FF5\n------------------------------------------------------\nFirm-events estimated                    26           26\nMean R^2                             0.2245       0.2675\nMedian R^2                           0.1943       0.2692\nMean σ(ε)                          0.021753     0.021351\nMean |α|                           0.001022     0.001130\nMean β(MKT)                          0.8867       0.9721\nMean β(SMB)                         -0.0434       0.0265\nMean β(HML)                          0.2489       0.1493\nMean β(RMW)                               —      -0.0934\nMean β(CMA)                               —       0.1070\n\n\n\n\n\n\n15.6.11 Event-Level Detail\n\n\n\nTable 15.8: Event-level detail: CARs and BHARs for each firm-event (FF3 model)\n\n\ndetail = results_ff3['event_ar'].copy()\ndetail_cols = ['symbol', 'evtdate', 'CAR', 'BHAR', 'SCAR', 'sigma',\n               'nobs', 'alpha', 'beta_mkt_excess']\ndetail_avail = [c for c in detail_cols if c in detail.columns]\ndetail_show = detail[detail_avail].copy()\ndetail_show['CAR'] = detail_show['CAR'].map(lambda x: f'{x:.4%}')\ndetail_show['BHAR'] = detail_show['BHAR'].map(lambda x: f'{x:.4%}')\ndetail_show['SCAR'] = detail_show['SCAR'].map(lambda x: f'{x:.3f}')\n\nprint(\"Event-Level Results (first 20 firm-events)\")\nprint(\"=\" * 100)\nprint(detail_show.head(20).to_string(index=False))\n\nEvent-Level Results (first 20 firm-events)\n====================================================================================================\nsymbol    evtdate       CAR      BHAR   SCAR    sigma  nobs     alpha  beta_mkt_excess\n   BVH 2016-10-20 -12.6456% -11.6522% -1.603 0.017219   150  0.001193         1.449182\n   DHG 2016-01-25  16.1151%  17.1149%  2.273 0.015472   150 -0.000224         0.754245\n   DNH 2019-10-14   1.7233%   1.8068%  0.104 0.036035   150 -0.000781         1.861996\n   DPM 2020-07-30  -5.9576%  -6.2025% -0.545 0.023873   150  0.001279         0.313932\n   FOX 2021-01-13   2.7478%   2.9194%  0.329 0.018243   150 -0.000696         0.148058\n   GAS 2020-02-11   0.5371%   0.7801%  0.100 0.011694   150  0.000501         1.851155\n   GEX 2020-08-20  11.9985%  13.5843%  1.197 0.021883   150  0.000944         1.583418\n   IDC 2018-10-01   1.3889%   0.5651%  0.094 0.032120   150 -0.000674         0.809305\n   MML 2021-10-29 -12.0139% -12.3759% -1.141 0.022985   150  0.002976         0.260588\n   MSN 2015-10-23  -5.7205%  -5.5645% -0.718 0.017375   150  0.001177         0.453951\n   PGV 2019-06-26  -7.5892%  -9.1366% -0.359 0.046165   150  0.000502         0.151377\n   PLX 2020-01-07   2.5330%   2.7847%  0.423 0.013067   150 -0.000176         0.917578\n   PLX 2020-06-01  -6.1517%  -6.0085% -0.739 0.018168   150 -0.000465         1.815178\n   POW 2020-12-23   7.7751%   9.1225%  1.130 0.015014   150 -0.000575         0.774423\n   PVD 2020-05-25   4.8827%   5.6674%  0.510 0.020875   150 -0.001522         1.316102\n   PVS 2017-08-07   2.1360%   2.1918%  0.297 0.015715   150 -0.000341         1.136273\n   QNS 2018-01-18   4.3001%   3.4011%  0.530 0.017703   150 -0.003716         0.128328\n   SAB 2017-08-24   6.0347%   6.7732%  0.748 0.017614   150  0.003284         2.123751\n   SNZ 2019-03-12  34.5230%  33.1105%  2.145 0.035121   150  0.000235        -1.015554\n   VCI 2020-01-20  -3.3191%  -2.9072% -0.458 0.015797   150  0.000335        -0.086268\n\n\n\n\n\n\n15.6.12 Daily Abnormal Return Dynamics\n\n\n\nTable 15.9: Daily dynamics of mean abnormal returns and test statistics within the event window\n\n\nds = results_ff3['daily_stats'].copy()\nds_cols = ['evttime', 'N', 'mean_AR', 'mean_CAR', 'mean_BHAR', 't_AR_CS', 't_AR_BMP']\nds_avail = [c for c in ds_cols if c in ds.columns]\nds_show = ds[ds_avail].copy()\n\nfor c in ['mean_AR', 'mean_CAR', 'mean_BHAR']:\n    if c in ds_show.columns:\n        ds_show[c] = ds_show[c].map(lambda x: f'{x:.4%}')\nfor c in ['t_AR_CS', 't_AR_BMP']:\n    if c in ds_show.columns:\n        ds_show[c] = ds_show[c].map(lambda x: f'{x:.3f}' if pd.notna(x) else '')\n\nprint(\"Daily Event-Window Dynamics (FF3 Model)\")\nprint(\"=\" * 80)\nprint(ds_show.to_string(index=False))\n\nDaily Event-Window Dynamics (FF3 Model)\n================================================================================\n evttime  N  mean_AR mean_CAR mean_BHAR\n     -10 23  0.3571%  0.3571%   0.3729%\n      -9 23  0.0337%  0.3907%   0.4209%\n      -8 23 -0.1581%  0.2326%   0.2766%\n      -7 23  0.3691%  0.6018%   0.6581%\n      -6 23  1.3416%  1.9433%   2.0278%\n      -5 23  0.1509%  2.0942%   2.2313%\n      -4 23  0.5512%  2.6454%   2.9187%\n      -3 23 -0.4641%  2.1814%   2.4297%\n      -2 23  0.2412%  2.4225%   2.8028%\n      -1 23  0.5660%  2.9885%   3.3240%\n       0 23 -0.0281%  2.9604%   3.4926%\n       1 23 -0.2564%  2.7040%   3.1039%\n       2 23 -0.4421%  2.2619%   2.5764%\n       3 23  0.6337%  2.8956%   3.4659%\n       4 23 -0.8432%  2.0524%   2.4738%\n       5 23 -0.4174%  1.6350%   2.1339%\n       6 23 -0.2272%  1.4078%   1.8085%\n       7 23 -0.2085%  1.1993%   1.6819%\n       8 23  0.0893%  1.2886%   1.8609%\n       9 23  0.3307%  1.6193%   2.1658%\n      10 23  0.5320%  2.1513%   2.4885%\n\n\n\n\n\n\n15.6.13 Summary of Key Findings\n\nprint(\"=\" * 70)\nprint(\"EVENT STUDY RESULTS SUMMARY\")\nprint(\"=\" * 70)\n\nff3_all = results_ff3['test_stats'][results_ff3['test_stats']['group'] == 'All'].iloc[0]\n\nprint(f\"\\nSample: {int(ff3_all['N'])} firm-event observations\")\nprint(f\"Frequency: Daily\")\nprint(f\"Primary Model: Fama-French 3-Factor\")\nprint(f\"Estimation Window: {config_ff3.estimation_window} trading days\")\nprint(f\"Event Window: ({config_ff3.event_window_start}, {config_ff3.event_window_end})\")\nprint(f\"Gap: {config_ff3.gap} trading days\")\nprint(f\"\\n--- Abnormal Return Measures ---\")\nprint(f\"Mean CAR({config_ff3.event_window_start},{config_ff3.event_window_end}): \"\n      f\"{ff3_all['mean_CAR']:.4%}\")\nprint(f\"Median CAR: {ff3_all['median_CAR']:.4%}\")\nprint(f\"Mean BHAR: {ff3_all['mean_BHAR']:.4%}\")\nprint(f\"Fraction positive CARs: {ff3_all['pct_positive']:.1%}\")\nprint(f\"\\n--- Statistical Significance ---\")\nprint(f\"Cross-Sectional t: {ff3_all['t_CS']:.3f} (p = {ff3_all['p_CS']:.4f})\")\nprint(f\"Patell Z: {ff3_all['Z_Patell']:.3f} (p = {ff3_all['p_Patell']:.4f})\")\nprint(f\"BMP t: {ff3_all['t_BMP']:.3f} (p = {ff3_all['p_BMP']:.4f})\")\nprint(f\"Kolari-Pynnönen t: {ff3_all['t_KP']:.3f} (p = {ff3_all['p_KP']:.4f})\")\nprint(f\"Generalized Sign Z: {ff3_all['Z_GSign']:.3f} (p = {ff3_all['p_GSign']:.4f})\")\n\nsig_005 = sum(1 for k in ['p_CS','p_Patell','p_BMP','p_KP','p_GSign','p_SkAdj','p_Wilcoxon']\n              if k in ff3_all and pd.notna(ff3_all[k]) and ff3_all[k] &lt; 0.05)\ntotal_tests = sum(1 for k in ['p_CS','p_Patell','p_BMP','p_KP','p_GSign','p_SkAdj','p_Wilcoxon']\n                  if k in ff3_all and pd.notna(ff3_all[k]))\nprint(f\"\\n{sig_005}/{total_tests} tests significant at 5% level\")\n\n# Robustness note\nprint(f\"\\nRobustness: Results checked across {len(models_daily)} risk models \"\n      f\"and {len(windows)} event windows\")\n\n======================================================================\nEVENT STUDY RESULTS SUMMARY\n======================================================================\n\nSample: 26 firm-event observations\nFrequency: Daily\nPrimary Model: Fama-French 3-Factor\nEstimation Window: 150 trading days\nEvent Window: (-10, 10)\nGap: 15 trading days\n\n--- Abnormal Return Measures ---\nMean CAR(-10,10): 2.1513%\nMedian CAR: 1.5701%\nMean BHAR: 2.4885%\nFraction positive CARs: 61.5%\n\n--- Statistical Significance ---\nCross-Sectional t: 0.775 (p = 0.4456)\nPatell Z: 0.961 (p = 0.3364)\nBMP t: 0.727 (p = 0.4741)\nKolari-Pynnönen t: 0.700 (p = 0.4904)\nGeneralized Sign Z: 1.177 (p = 0.2393)\n\n0/7 tests significant at 5% level\n\nRobustness: Results checked across 4 risk models and 7 event windows",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  },
  {
    "objectID": "18_event_studies.html#practical-recommendations",
    "href": "18_event_studies.html#practical-recommendations",
    "title": "15  Event Studies in Finance",
    "section": "15.7 Practical Recommendations",
    "text": "15.7 Practical Recommendations\nBased on the literature and our implementation experience:\n\nEstimation window: Use 150 trading days (~7 months) for daily studies. This balances parameter precision against structural breaks. For monthly studies, 60 months is standard (Kothari and Warner 2007).\nGap: 15 trading days is standard. Increase to 30 if information leakage is a concern.\nEvent window: Start with (-1, +1) for short-window tests, then expand to (-5, +5) and (-10, +10) for robustness. Report all windows.\nModel choice: Always report market model as the baseline. Add FF3 or FF5 for robustness. For Vietnam, local factors are preferable to global factors.\nTest statistics: Report at minimum: cross-sectional t (for ease of interpretation), BMP (robust to event-induced variance), and one non-parametric test (sign or Wilcoxon). Report Kolari-Pynnönen if events cluster in calendar time.\nThin trading: For Vietnamese small-caps, consider Dimson (1979) with 1 lead/lag or increase min_estimation_obs to filter out illiquid stocks.\nMultiple testing: If testing multiple event windows or subgroups, apply Bonferroni or Holm corrections to control family-wise error rate.\n\n\n\n\n\n\n\nAharony, Joseph, and Itzhak Swary. 1980. “Quarterly Dividend and Earnings Announcements and Stockholders’ Returns: An Empirical Analysis.” The Journal of Finance 35 (1): 1–12.\n\n\nAndrade, Gregor, Mark Mitchell, and Erik Stafford. 2001. “New Evidence and Perspectives on Mergers.” Journal of Economic Perspectives 15 (2): 103–20.\n\n\nBall, Ray, and Philip Brown. 2013. “An Empirical Evaluation of Accounting Income Numbers.” In Financial Accounting and Equity Markets, 27–46. Routledge.\n\n\nBarber, Brad M, and John D Lyon. 1997. “Detecting Long-Run Abnormal Stock Returns: The Empirical Power and Specification of Test Statistics.” Journal of Financial Economics 43 (3): 341–72.\n\n\nBernard, Victor L, and Jacob K Thomas. 1989. “Post-Earnings-Announcement Drift: Delayed Price Response or Risk Premium?” Journal of Accounting Research 27: 1–36.\n\n\nBhattacharya, Utpal, Hazem Daouk, Brian Jorgenson, and Carl-Heinrich Kehr. 2000. “When an Event Is Not an Event: The Curious Case of an Emerging Market.” Journal of Financial Economics 55 (1): 69–101.\n\n\nBinder, John. 1998. “The Event Study Methodology Since 1969.” Review of Quantitative Finance and Accounting 11 (2): 111–37.\n\n\nBoehmer, Ekkehart, Jim Masumeci, and Annette B Poulsen. 1991. “Event-Study Methodology Under Conditions of Event-Induced Variance.” Journal of Financial Economics 30 (2): 253–72.\n\n\nBrown, Stephen J, and Jerold B Warner. 1980. “Measuring Security Price Performance.” Journal of Financial Economics 8 (3): 205–58.\n\n\n———. 1985. “Using Daily Stock Returns: The Case of Event Studies.” Journal of Financial Economics 14 (1): 3–31.\n\n\nCampbell, John Y, Andrew W Lo, A Craig MacKinlay, and Robert F Whitelaw. 1998. “The Econometrics of Financial Markets.” Macroeconomic Dynamics 2 (4): 559–62.\n\n\nCarhart, Mark M. 1997. “On persistence in mutual fund performance.” The Journal of Finance 52 (1): 57–82. https://doi.org/10.1111/j.1540-6261.1997.tb03808.x.\n\n\nCorrado, Charles J. 1989. “A Nonparametric Test for Abnormal Security-Price Performance in Event Studies.” Journal of Financial Economics 23 (2): 385–95.\n\n\nCowan, Arnold Richard. 1992. “Nonparametric Event Study Tests.” Review of Quantitative Finance and Accounting 2 (4): 343–58.\n\n\nDimson, Elroy. 1979. “Risk Measurement When Shares Are Subject to Infrequent Trading.” Journal of Financial Economics 7 (2): 197–226.\n\n\nFama, Eugene F. 1998. “Market Efficiency, Long-Term Returns, and Behavioral Finance.” Journal of Financial Economics 49 (3): 283–306.\n\n\nFama, Eugene F, Lawrence Fisher, Michael C Jensen, and Richard Roll. 1969. “The Adjustment of Stock Prices to New Information.” International Economic Review 10 (1): 1–21.\n\n\nFama, Eugene F., and Kenneth R. French. 1993. “Common risk factors in the returns on stocks and bonds.” Journal of Financial Economics 33 (1): 3–56. https://doi.org/10.1016/0304-405X(93)90023-5.\n\n\n———. 2015. “A Five-Factor Asset Pricing Model.” Journal of Financial Economics 116 (1): 1–22. https://doi.org/10.1016/j.jfineco.2014.10.010.\n\n\nFlannery, Mark J, and Aris A Protopapadakis. 2002. “Macroeconomic Factors Do Influence Aggregate Stock Returns.” The Review of Financial Studies 15 (3): 751–82.\n\n\nGriffin, John M, Patrick J Kelly, and Federico Nardari. 2010. “Do Market Efficiency Measures Yield Correct Inferences? A Comparison of Developed and Emerging Markets.” The Review of Financial Studies 23 (8): 3225–77.\n\n\nHall, Peter. 1992. “On the Removal of Skewness by Transformation.” Journal of the Royal Statistical Society Series B: Statistical Methodology 54 (1): 221–28.\n\n\nJensen, Michael C, and Richard S Ruback. 1983. “The Market for Corporate Control: The Scientific Evidence.” Journal of Financial Economics 11 (1-4): 5–50.\n\n\nKolari, James W, and Seppo Pynnönen. 2010. “Event Study Testing with Cross-Sectional Correlation of Abnormal Returns.” The Review of Financial Studies 23 (11): 3996–4025.\n\n\nKothari, Sagar P, and Jerold B Warner. 2007. “Econometrics of Event Studies.” In Handbook of Empirical Corporate Finance, 3–36. Elsevier.\n\n\nMacKinlay, A Craig. 1997. “Event Studies in Economics and Finance.” Journal of Economic Literature 35 (1): 13–39.\n\n\nMitchell, Mark L, and Jeffry M Netter. 1993. “The Role of Financial Economics in Securities Fraud Cases: Applications at the Securities and Exchange Commission.” Bus. Law. 49: 545.\n\n\nMitchell, Mark L, and Erik Stafford. 2000. “Managerial Decisions and Long-Term Stock Price Performance.” The Journal of Business 73 (3): 287–329.\n\n\nPatell, James M. 1976. “Corporate Forecasts of Earnings Per Share and Stock Price Behavior: Empirical Test.” Journal of Accounting Research, 246–76.\n\n\nScholes, Myron, and Joseph Williams. 1977. “Estimating Betas from Nonsynchronous Data.” Journal of Financial Economics 5 (3): 309–27.\n\n\nSchwert, G William. 1981. “Using Financial Data to Measure Effects of Regulation.” The Journal of Law and Economics 24 (1): 121–58.\n\n\nSharpe, William F. 1964. “Capital asset prices: A theory of market equilibrium under conditions of risk .” The Journal of Finance 19 (3): 425–42. https://doi.org/10.1111/j.1540-6261.1964.tb02865.x.\n\n\nWarner, Jerold B, Ross L Watts, and Karen H Wruck. 1988. “Stock Prices and Top Management Changes.” Journal of Financial Economics 20: 461–92.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Event Studies in Finance</span>"
    ]
  }
]